crowdsourcing learning as domain adaptation: a case study on namedentity recognition.
xin zhang1, guangwei xu, yueheng sun2, meishan zhang 1∗, pengjun xie1school of new media and communication, tianjin university, china2college of intelligence and computing, tianjin university, china{hsinz,yhs,zhangmeishan}@tju.edu.cn{ahxgwonepiece,xpjandy}@gmail.com.
abstract.
crowdsourcing is regarded as one prospectivesolution for effective supervised learning, aim-ing to build large-scale annotated training databy crowd workers.
previous studies focus onreducing the inﬂuences from the noises of thecrowdsourced annotations for supervised mod-els.
we take a different point in this work, re-garding all crowdsourced annotations as gold-standard with respect to the individual annota-tors.
in this way, we ﬁnd that crowdsourcingcould be highly similar to domain adaptation,and then the recent advances of cross-domainmethods can be almost directly applied tocrowdsourcing.
here we take named entityrecognition (ner) as a study case, suggest-ing an annotator-aware representation learn-ing model that inspired by the domain adap-tation methods which attempt to capture ef-fective domain-aware features.
we investigateboth unsupervised and supervised crowdsourc-ing learning, assuming that no or only small-scale expert annotations are available.
exper-imental results on a benchmark crowdsourcedner dataset show that our method is highly ef-fective, leading to a new state-of-the-art perfor-mance.
in addition, under the supervised set-ting, we can achieve impressive performancegains with only a very small scale of expert an-notations..1.introduction.
crowdsourcing has gained a growing interest inthe natural language processing (nlp) community,which helps hard nlp tasks such as named entityrecognition (finin et al., 2010; derczynski et al.,2016), part-of-speech tagging (hovy et al., 2014),relation extraction (abad et al., 2017), translation(zaidan and callison-burch, 2011), argument re-trieval (mayhew et al., 2020), and others (snowet al., 2008; callison-burch and dredze, 2010) to.
∗corresponding author..figure 1: a ner example with crowdsourced labels, aand exp denote annotator and expert, respectively..collect a large scale dataset for supervised modeltraining.
in contrast to the gold-standard annota-tions labeled by experts, the crowdsourced annota-tions can be constructed quickly at a low cost withmasses of crowd annotators (snow et al., 2008; nyeet al., 2018).
however, these annotations are rela-tively lower-quality with much-unexpected noisesince the crowd annotators are not professionalenough, which can make errors in complex andambiguous contexts (sheng et al., 2008)..previous crowdsourcing learning models strug-gle to reduce the inﬂuences of noises of the crowd-sourced annotations (hsueh et al., 2009; raykarand yu, 2012a; hovy et al., 2013; jamison andgurevych, 2015).
majority voting (mv) is onestraightforward way to aggregate high-quality an-notations, which has been widely adopted (snowet al., 2008; fernandes and brefeld, 2011; ro-drigues et al., 2014), but it requires multiple an-notations for a given input.
recently, the majorityof models concentrate on monitoring the distancesbetween crowdsourced and gold-standard anno-tations, obtaining better performances than mvby considering the annotator information together(nguyen et al., 2017; simpson and gurevych, 2019;li et al., 2020).
most of these studies assumethe crowdsourced annotations as untrustworthy an-swers, proposing sophisticated strategies to recoverthe golden answers from crowdsourced labels..in this work, we take a different view for crowd-sourcing learning, regarding the crowdsourced an-notations as the gold standard in terms of individual.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5558–5570august1–6,2021.©2021associationforcomputationallinguistics5558textandreaferrigatosprintedtohisworldcupwina-1b-peri-perooob-orgi-orgoa-2ob-perooob-misci-miscoa-3b-peri-perooob-orgi-orgoexpb-peri-perooob-misci-miscoannotators.
in other words, we assume that all an-notators (including experts) own their specializedunderstandings towards a speciﬁc task, and theyannotate the task consistently according to theirindividual principles by the understandings, wherethe experts can reach an oracle principle by consen-sus.
the above view indicates that crowdsourcinglearning aims to train a model based on the under-standings of crowd annotators, and then test themodel by the oracle understanding from experts..based on the assumption, we ﬁnd that crowd-sourcing learning is highly similar to domain adap-tation, which is one important topic that has beeninvestigated extensively for decades (ben-davidet al., 2006; daum´e iii, 2007; chu and wang, 2018;jia and zhang, 2020).
we treat each annotatoras one domain speciﬁcally, and then crowdsourc-ing learning is essentially almost a multi-sourcedomain adaptation problem.
thus, one naturalquestion arises: what is the performance when astate-of-the-art domain adaptation model is applieddirectly to crowdsourcing learning..here we take ner as a study case to investi-gate crowdsourcing learning as domain adaptation,considering that ner has been one popular taskfor crowdsourcing learning in the nlp community(finin et al., 2010; rodrigues et al., 2014; der-czynski et al., 2016).
we suggest a state-of-the-artrepresentation learning model that can effectivelycapture annotator(domain)-aware features.
also,we investigate two settings of crowdsourcing learn-ing, one being the unsupervised setting with noexpert annotation, which has been widely studiedbefore, and the other being the supervised settingwhere a certain scale of expert annotations exists,which is inspired by domain adaptation..finally, we conduct experiments on a benchmarkcrowdsourcing ner dataset (tjong kim sang andde meulder, 2003; rodrigues et al., 2014) to evalu-ate our methods.
we take a standard bilstm-crf(lample et al., 2016) model with bert (devlinet al., 2019) word representations as the baseline,and adapt it to our representation learning model.
experimental results show that our method is ableto model crowdsourced annotations effectively.
un-der the unsupervised setting, our model can give astrong performance, outperforming previous worksigniﬁcantly.
in addition, the model performancecan be greatly boosted by feeding with small-scaleexpert annotations, which can be a prospective di-rection for low-resource scenarios..figure 2: illustration of the connection between multi-source domain adaptation and crowdsourcing learning..in summary, we make the following three major.
contributions:.
(1) we present a different view of crowdsourcinglearning, and propose to treat crowdsourcinglearning as domain adaptation, which natu-rally connects the two important topics of ma-chine learning for nlp..(2) we propose a novel method for crowdsourc-ing learning.
although the method is of alimited novelty for domain adaptation, it isthe ﬁrst work to crowdsourcing learning, andcan achieve state-of-the-art performance onner..(3) we introduce supervised crowdsourcing learn-ing for the ﬁrst time, which is borrowed fromdomain adaptation and would be a prospectivesolution for hard nlp tasks in practice..we will release the code and detailed experimen-tal settings at github.com/izhx/clasda under theapache license 2.0 to facilitate future research..2 the basic idea.
here we describe the concepts of the domain adap-tation and crowdsourcing learning in detail, andshow how they are connected together..2.1 domain adaptation.
domain adaptation happens when a supervisedmodel trained on a ﬁxed set of training corpus, in-cluding several speciﬁc domains, is required to teston a different domain (ben-david et al., 2006; man-sour et al., 2009).
the scenario is quite frequent inpractice, and thus has received extensive attentionwith massive investigations (csurka, 2017; ram-poni and plank, 2020).
the major problem lies inthe different input distributions between source andtarget domains, leading to biased predictions overthe inputs with a large gap to the source domains..5559annotator1···annotatormdomain1···domainmxij→yijxij→yijexpertdomaintgt==⇒==⇒xij=ai(xij)crowdsourcinglearningmulti-sourcedomainadaptationhere we focus on multi-source cross-domainadaptation, which would suit our next correspond-ing mostly.
following mansour et al.
(2009); zhaoet al.
(2019), the multi-source domain adaptationassumes a set of labeled examples from m domainsavailable, denoted by dsrc = {(xi, yi)}mi=1,1where xi = {xij=1,2 andwe aim to train a model on dsrc to adapt to a spe-ciﬁc target domain with the help of a large scaleraw corpus xtgt = {xi}nt.
j=1 and yi = {yi.
i=1 of the target domain..j}ni.
j}ni.
note that under this setting, all xs, includingsource and target domains, are generated individu-ally according to their unknown distributions, thusthe abstract representations learned from the sourcedomain dataset dsrc would inevitably be biased tothe target domain, which is the primary reasonfor the degraded performance of the target domain(huang and yates, 2010; ganin et al., 2016).
anumber of domain adaptation models have strug-gled for better transferable high-level representa-tions as domain shifts (ramponi and plank, 2020)..2.2 crowdsourcing learning.
crowdsourcing aims to produce a set of large-scaleannotated examples created by crowd annotators,which is used to train supervised models for agiven task (raykar et al., 2010).
as the majorityof nlp models assume that gold-standard high-quality training corpora are already available (man-ning and schutze, 1999), crowdsourcing learninghas received much less interest than cross-domainadaptation, although the availability of these cor-pora is always not the truth..formally, under the crowdsourcing setting, weusually assume that there are a number of crowd an-notators a = {ai}mi=1 (here we use the same m aswell as later superscripts in order to align with thedomain adaptation), and all annotators should havea sufﬁcient number of training examples by theirdifferent understandings for a given task, whichare referred to as dcrowd = {(xi, yi)}mi=1 wherej=1 and yi = {yixi = {xij=1.
we aim to traina model on dcrowd and adapt it to predict the expertoutputs.
note that all xs do not have signiﬁcantdifferences in their distributions in this paradigm..j}ni.
j}ni.
1a domain is commonly deﬁned as a distribution on theinput data in many works, e.g., ben-david et al.
(2006).
tomake domain adaptation and crowdsourcing learning highlysimilar in formula, we follow zhao et al.
(2019), deﬁninga domain as a joint distribution on the input space x andthe label space y. section 4.5 gives a discussion of theirconnection..2n∗ indicates the number of instances..figure 3: the structure of our representation learningmodel, where the right orange part denotes the annota-tor switcher, and v denotes the generated adapter pa-rameters by pgn.
the transformer layers in gray arekept frozen in training, and other modules are trainable..j)}ni.
j = ai(xi.
crowdsourcing learning as domain adapta-tion by scrutinizing the above formalization,when we set all xs jointly with the annotatorsby using xij), which indicates the con-textualized understanding (a vectorial form is de-sirable here of the neural representations) of xijby the annotator ai, then we would regard thatxi = {ai(xij=1 is generated from different dis-tributions as well.
in this way, we are able to con-nect crowdsourcing learning and domain adapta-tion together, as shown in figure 2, based on theassumption that all y s are gold-standard for crowd-sourced annotations when crowd annotators areunited as joint inputs.
and ﬁnally, we need to per-form predictions by regarding xexpert = expert(x),and in particular, the learning of expert differs fromthat of the target domain in domain adaptation..3 a case study on ner.
in this section, we take ner as a case study, whichhas been investigated most frequently in nlp (ya-dav and bethard, 2018), and propose a representa-tion learning model mainly inspired by the domainadaptation model of (jia et al., 2019) to performcrowdsourcing learning.
in addition, we introducethe unsupervised and supervised settings for crowd-sourcing learning which are directly borrowed fromthe domain adaptation..3.1 the representation learning model.
we convert ner into a standard sequence label-ing problem by using the bio schema, followingthe majority of previous works, and extend a state-of-the-art bert-bilstm-crf model (mayhew.
5560crfya1···yanbilstmtransformerlnadapterspgnv............transformerl1adapterspgnvembeddingx1···xnapgn◦adapter◦bertet al., 2020) to our crowdsourcing learning.
fig-ure 3 shows the overall network structure of ourrepresentation learning model.
by using a sophisti-cated parameter generator module (platanios et al.,2018), it can capture annotator-aware features.
fol-lowing, we introduce the proposed model by fourcomponents: (1) word representation, (2) annota-tor switcher, (3) bilstm encoding, and (4) crfinference and training..word representation given a sentence of nwords x = w1 · · · wn, we ﬁrst convert it to vec-torial representations by bert.
different fromthe standard bert exploration, here we useadapter◦bert (houlsby et al., 2019), where twoextra adapter modules are inside each transformerlayer.
the process can be simply formalized as:.
e1 · · · en = adapter ◦ bert(w1 · · · wn).
(1).
where ◦ indicates an injection operation.
the de-tailed structure of the transformer with adapters isdescribed in appendix a..noticeably, the adapter ◦ bert method nolonger needs ﬁne-tuning the huge bert param-eters and can obtain comparable performance byadjusting the much lightweight adapter parametersinstead.
thus the representation can be more pa-rameter efﬁcient, and in this way we can easilyextend the word representations to annotator-awarerepresentations..annotator switcher our goal is to efﬁcientlylearn annotator-aware word representations, whichcan be regarded as contextualized understandingsof individual annotators.
hence, we introduce anannotator switcher to support adapter ◦ bert withannotator input as well, which is inspired by ¨ust¨unet al.
(2020).
the key idea is to use parameter gen-eration network (pgn) (platanios et al., 2018; jiaet al., 2019) to produce adapter parameters dynam-ically by input annotators.
in this way, our modelcan ﬂexibly switch among different annotators..concretely, assuming that v is the vectorialform of all adapter parameters by a pack operation,which can also be unpacked to recover all adapterparameters as well, the pgn module is to generatev for adapter ◦ bert dynamically according theannotator inputs, as shown in figure 3 by the rightorange part.
the switcher can be formalized as:.
x = r(cid:48).
1 · · · r(cid:48)n.= pgn ◦ adapter ◦ bert(x, a)= adapter ◦ bert(x, v = θ × ea),.
(2).
where θ ∈ r|v |×|ea| , x = r(cid:48)n is theannotator-aware representations of annotator a forx = w1 · · · wn, and ea is the annotator embedding..1 · · · r(cid:48).
bilstm encoding adapter ◦ bert requires anadditional task-oriented module for high-level fea-ture extraction.
here we exploit a single bil-stm layer to achieve it: h1 · · · hn = bilstm(x),which is used for next-step inference and training..crf inference and training we use crf tocalculate the score of a candidate sequential outputy = l1 · · · ln globally:.
oi = w crfhi + bcrfn(cid:88).
score(y|x, a) =.
i=1.
(t [li−1, li] + oi[li]).
(3).
where w crf, bcrf and t are model parameters..given an input (x, a), we perform inference bythe viterbi algorithm.
for training, we deﬁne asentence-level cross-entropy objective:.
p(ya|x, a) =.
exp (cid:0)score(ya|x, a)(cid:1)y exp (cid:0)score(y|x, a)(cid:1)(cid:80).
(4).
l = − log p(ya|x, a).
where ya is the gold-standard output of x from a,y belongs to all possible candidates, and p(ya|x, a)indicates the sentence-level probability..3.2 the unsupervised setting.
here we introduce unsupervised crowdsourcinglearning in alignment with unsupervised domainadaptation, assuming that no expert annotation isavailable, which is the widely-adopted setting ofprevious work of crowdsourcing learning (shenget al., 2008; zhang et al., 2016; sheng and zhang,2019).
this setting has a large divergence with do-main adaptation in target learning.
in the unsuper-vised domain adaptation, the information of the tar-get domain can be learned through a large-scale rawcorpus (ramponi and plank, 2020), where there isno correspondence in the unsupervised crowdsourc-ing learning to learn information of experts..to this end, here we suggest a simple and heuris-tic method to model experts by the specialty ofcrowdsourcing learning.
intuitively, we expect thatexperts should approve the knowledge of the com-mon consensus for a given task, and meanwhile,our model needs the embedding representation ofexperts for inference.
thus, we can estimate the.
5561expert embedding by using the centroid point of allannotator embeddings:.
pannotator-agnostic.
r.f1.
eexpert =.
1|a|.
(cid:88).
ea.
a∈a.
(5).
annotator-aware.
76.35 72.47 74.3683.61 68.47 75.28.
78.59 74.54 76.5174.34 79.41 76.7978.84 75.67 77.95.model.
allmv.
lclc-catthis work.
where a represents all annotators contributed to thetraining corpus.
this expert can be interpreted asthe elected outcome by annotator voting with equalimportance.
in this way, we perform the inferencein unsupervised crowdsourcing learning by feedingeexpert as the annotator input..3.3 the supervised setting.
inspired by the supervised domain adaptation, wealso present the supervised crowdsourcing learning,which has been seldom concerned.
the setting isvery simple, just by assuming that a certain scaleof expert annotations is available.
in this way, wecan learn the expert representation directly by su-pervised learning with our proposed model..the supervised setting could be a more practi-cable scenario in real applications.
intuitively, itshould bring much better performance than the un-supervised setting with few shot expert annotations,which does not increase the overall annotation costmuch.
in fact, during or after the crowdsourcing an-notation process, we usually have a quality controlmodule, which can help to produce silvery qual-ity pseudo-expert annotations (kittur et al., 2008;lease, 2011).
thus, the supervised setting can behighly valuable yet has been ignored mostly..4 experiments.
4.1 setting.
dataset we use the conll-2003 ner englishdataset (tjong kim sang and de meulder, 2003)with crowdsourced annotations provided by ro-drigues and pereira (2018) to investigate our meth-ods in both unsupervised and supervised settings.
the crowdsourced annotations consume 400 newinvolving 5,985 sentences in practice,articles,which are labeled by a total of 47 crowd anno-tators.
the total number of annotations is 16,878.thus the averaged number of annotated sentencesper annotator is 359, which covers 6% of the totalsentences.
the dataset includes golden/expert an-notations on the training sentences and a standardconll-2003 test set for ner evaluation..previous work.
49.40 85.60 62.60(rodrigues et al., 2014)82.38 62.10 70.82lc (nguyen et al., 2017)79.61 62.87 70.26lc-cat (nguyen et al., 2017)(rodrigues and pereira, 2018)66.00 59.30 62.40(simpson and gurevych, 2019)† 80.30 74.80 77.40.table 1: the test results of the unsupervised setting,where the superscript † indicates that there exist differ-ences in the test corpus..mance, reporting the entity-level precision (p), re-call (r), and their f1 value.
all experiments of thesame setting are conducted by ﬁve times, and themedian outputs are used for performance reporting.
we exploit the pair-wise t-test for signiﬁcance test,regarding two results signiﬁcantly different whenthe p-value is below 10−5..baselines we re-implement several methods ofprevious work as baselines, and all the methodsare based on adapter◦bert-bilstm-crf (noannotator switcher inside) for fair comparisons..for both the unsupervised and supervised set-tings, we consider the following baseline models:.
• all: which treats all annotations equally, ig-noring the annotator information no mattercrowd or expert..• mv: which is borrowed from rodrigues et al.
(2014), where aggregated labels are producedby token level majority voting.
in particular,the gold-standard labels are used instead ifthey are available for a speciﬁc sentence dur-ing the supervised crowdsourcing learning..• lc: which is proposed by nguyen et al.
(2017), where the annotator bias to the gold-standard labels is explicitly modeled at thecrf layer for each crowd annotator, andspeciﬁcally, the expert is with zero bias..• lc-cat: which is also presented by nguyenet al.
(2017) as a baseline to lc, where the an-notator bias is modeled at the bilstm layerinstead and also the expert bias is set to zero.3.
evaluation the standard conll-2003 evalua-tion metric is used to calculate the ner perfor-.
3note that although lc-cat is not as expected as lcin (nguyen et al., 2017), our results show that lc-cat isslightly better based on adapter◦bert-bilstm-crf..5562model.
allmvgold.
1%r.p.f1.
p.f1.
p.f1.
p.25%r.100%r.f1.
5%r.75.08 74.82 74.95 76.18 75.71 75.94 78.64 78.93 78.78 86.65 82.29 84.4283.87 67.37 74.72 83.49 69.32 75.75 84.77 79.43 82.0169.52 75.41 72.35 76.70 82.14 79.33 81.32 85.39 83.31.
89.28 89.77 89.52.annotator-agnostic.
annotator-aware.
lclc-catthis work.
78.09 74.10 76.04 79.98 77.18 78.55 77.72 81.06 79.36 87.42 85.64 86.5275.37 78.54 76.92 74.24 81.32 77.62 76.88 81.37 78.96 88.25 86.03 87.1380.06 81.91 80.97 83.25 85.36 84.29 85.19 87.46 86.31 89.62 90.51 90.06.table 2: the test results of the supervised setting, where we add different proportions of the most informativegold-standard (expert) annotations incrementally.
note that mv at 100% is equivalent to the gold model, becauseall voted labels are substituted with gold-standard labels..notice that all and mv are annotator-agnosticmodels, which exploit no information speciﬁc tothe individual annotators, while the other threemodels are all annotator-aware models, where theannotator information is used by different ways..hyper-parameters we offer all detailed settingsof hyper-parameters in appendix b..4.2 unsupervised results.
table 1 shows the test results of the unsupervisedsetting.
as a whole, we can see that our representa-tion learning model (i.e., this work) borrowedfrom domain adaptation can achieve the best per-formance, resulting in an f1 score of 77.95, signif-icantly better than the second-best model lc-cat(i.e., 77.95 − 76.79 = 1.16).
the result indicatesthe advantage of our method over the other models.
by examining the results in-depth, we can ﬁndthat the annotator-aware model is signiﬁcantly bet-ter than the annotator-agnostic models, demonstrat-ing that the annotator information is highly helpfulfor crowdsourcing learning.
the observation fur-ther shows the reasonableness by aligning annota-tors to domains, since domain information is alsouseful for domain adaptation.
in addition, the betterperformance of our representation learning methodamong the annotator-aware models indicates thatour model can capture annotator-aware informationmore effectively because our start point is totallydifferent.
we do not attempt to model the expertlabels based on crowdsourcing annotations..further, we observe that several models showbetter precision values, while others give betterrecall values.
a high precision but low recall in-dicates that the model is conservative in detectingnamed entities, and vice the reverse.
our proposedmodel is able to balance the two directions better,with the least gap between them.
also, the re-.
sults imply that there is still much space for futuredevelopment, and the recent advances of domainadaptation might offer good avenues..finally, we compare our results with previousstudies.
as shown, our model can obtain the bestperformance in the literature.
in particular, by com-paring our results with the original performances re-ported in nguyen et al.
(2017), we can see that ourre-implementation is much better than theirs.
themajor difference lies in the exploration of bert inour model, which brings improvements closed to6% for both lc and lc-cat..4.3 supervised results.
to investigate the supervised setting, we assumethat expert annotations (ground truths) of all crowd-sourcing sentences are available.
besides explor-ing the full expert annotations, we study anotherthree different scenarios by incrementally addingthe expert annotations into the unsupervised setting,aiming to study the effectiveness of our model withsmall expert annotations as well.
concretely, weassume proportions of 1%, 5%, 25%, and 100% ofthe expert annotations available.4 table 2 showsall the results, including our four baselines and angold model based on only expert annotations forcomparisons.
overall, we can see that our repre-sentation learning model can bring the best perfor-mances for all scenarios, demonstrating its effec-tiveness in the supervised learning as well..next, by comparing annotator-agnostic andannotator-aware models, we can see that annotator-aware models are better, which is consistent with.
4intuitively, if expert annotations are involved, we shouldintentionally choose the more informative inputs for anno-tations, which can reduce the overall cost to meet a certainperformance standard.
thus, we can fully demonstrate theeffectiveness of crowdsourced annotations under the semi-supervised setting.
here we try to choose the most informativelabeled instances for the 1%, 5%, and 25% settings..5563(a) 0%.
(b) 5%.
(c) 25%.
(d) 100%.
figure 4: the visualization of annotator embeddingsby dimensionality reduction with pca.
out designedunsupervised (0%) expert is consistent with the well-learned one (100%).
with the expert annotations in-creases, the learned expert becomes more accurate..the unsupervised setting.
more interestingly, theresults show that all is better than gold withvery small-scale expert annotations (1% and 5%),and the tendency is reversed only when there aresufﬁcient expert annotations (25% and 100%).
theobservation indicates that crowdsourced annota-tions are always helpful when golden annotationsare not enough.
in addition, it is easy to understandthat mv is worse than gold since the latter has ahigher-quality of the training corpus..further, we can ﬁnd that even the annotator-aware lc and lc-cat models are unable to obtainany positive inﬂuence compared with gold, whichdemonstrates that distilling ground-truths from thecrowdsourcing annotations might not be the mostpromising solution.
while our representation learn-ing model can give consistently better results thangold, indicating that crowdsourced annotationsare always helpful by our method.
by regardingcrowdsourcing learning as domain adaptation, weno longer take crowdsourced annotations as noise,and on the contrary, they are treated as transferableknowledge, similar to the relationship between thesource domains and the target domain.
thus theycould always be useful in this way..4.4 analysis.
to better understand our idea and model in-depth,we conducted the following ﬁne-grained analyses.5.
visualization of annotator embeddings ourrepresentation learning model is able to learn anno-tator embeddings through the task objective.
it isinteresting to visualize these embeddings to checktheir distributions, which can reﬂect the relation-ships between the individual annotators.
figure 4shows the visualization results after principal com-ponent analysis (pca) dimensionality reduction,.
5in addition, we could not perform the ablation study of.
our model because it is not an incremental work..modelallmvlclc-catthis work.
p67.0272.2472.3472.7680.78.r69.3169.4970.4871.7873.78.f168.1570.8871.3572.2677.12.gold(5%).
79.33.table 3: the performance of training on 85% and test-ing on 15% of the crowdsourced annotations..where the unsupervised and three supervised sce-narios are investigated.6 as shown, we can see thatmost crowd annotators are distributed in a concen-trated area for all scenarios, indicating that theyare able to share certain common characteristics oftask understanding..further, we focus on the relationship betweenexpert and crowd annotators, and the results showtwo interesting ﬁndings.
first, the heuristic expertof our unsupervised learning is almost consistentwith that of the supervised learning of the wholeexpert annotations (100%), which indicates that ourunsupervised expert estimation is perfectly good.
second, the visualization shows that the relation-ship between expert and crowd annotators couldbe biased when expert annotations are not enough.
as the size of expert annotations increases, theirconnection might be more accurate gradually..the predictability of crowdsourcing annota-tions our primary assumption is based on thatall crowdsourced annotations are regarded as thegold-standard with respect to the crowd annotators,which naturally indicates that these annotationsare predictable.
here we conduct analysis to ver-ify the assumption by a new task to predicate thecrowdsourced annotations, concretely, we dividethe annotations into two sections, where 85% ofthem are used as the training and the remaining areused for testing, and then we apply our baselineand proposed models to learn and evaluate..table 3 shows the results.
as shown, our modelcan achieve the best performance by an f1 scoreof 77.12%, and the other models are signiﬁcantlyworse (at least 4.86 drops by f1).
considering thatthe proportion of the averaged training examplesper annotator over the full 5,985 sentences is only5%,7 we exploit the gold model of the 5% ex-pert annotations for reference.
we can see that thegap between them is small (77.12% v.s.
79.33%),.
6the 1% setting is excluded for its incapability to capturethe relationship between the expert and crowd annotators withsuch small expert annotations..7the value can be directly calculated (0.06∗0.85 ≈ 0.05)..5564(a) 0%.
(b) 1%.
(c) 5%.
(d) 25%.
figure 5: comparisons by f1 scores between full andﬁltered crowdsourced annotations (i.e., excluding unre-liable annotators).
we compute f1 values of each anno-tator with respect to the gold-standard labels, and ﬁlterout 10 annotators with lowest scores..which indicates that our assumption is acceptableas a whole.
the other models could be unsuitablefor our assumption due to the poor performanceinduced by their modeling strategies..the impact of unreliable annotators han-dling unreliable annotators, such as spammers,is a practical and common issue in crowdsourc-ing (raykar and yu, 2012b).
obviously, regard-ing crowd annotations as untrustworthy answersis more considerate to this problem.
in contrast,our assumption might be challenged because theseunreliable annotators are discrepant in their ownannotations.
to show the inﬂuence of unreliableannotators, we ﬁlter out several unreliable annota-tors in the corpus, and reevaluate the performancefor the low-resource supervised and unsupervisedscenarios on the remaining annotations..figure 5 shows the comparison results of theoriginal corpus and the ﬁltered corpus.8 first,we can ﬁnd that improved performance can beachieved in all cases, indicating excluding theseunreliable annotations is helpful for crowdsourcing.
second, the lc and lc-cat model give smallerscore differences compared with the all modelbetween these two kinds of results, which veriﬁedthat they are considerate to unreliable annotators.
third, our model also performs robustly, it can copewith this practical issue in a certain degree as well..results on the sampled annotators and anno-tations the above analysis shows the beneﬁt ofremoving unreliable annotators, which reduces asmall number of annotators and annotations.
aproblem arises naturally: will the performance be.
8mv is not included because a proportion of instances are.
unable to obtain aggregated answers..datamodelall74.36lc76.51lc-cat76.79this work 77.95.full excluded part-1 part-2f1.
76.7376.8077.5978.23.
74.6675.2974.8677.41.
75.9276.7076.0277.58.table 4: the unsupervised test results of differentlysampled datasets.
the full is original results in table1. the excluded is the ﬁltered corpus in figure 5. thepart-1 and part-2 are both consist of 13 annotators.
part-1 have 1800 texts with 6275 crowd annotations, eachtext is labeled by at least 3 annotators.
these numbersof part-2 are 2192, 5582, and 2, respectively..consistent if we sample a small proportion of anno-tators?
to verify it, we sampled two sub-set fromthe crowdsourced training corpus and re-train ourmodel as well as baselines.
table 4 shows the eval-uation results of re-trained models on the standardtest set in unsupervised setting.
we also add ourmain result for the comparison.
as shown, all sam-pled datasets demonstrate similar trends with themain result (denoted as full).
the supervisedresults are consistent with our main result as well,which are not listed due to space reasons..4.5 the discussion of domain deﬁnitions.
the most widely used deﬁnition of a domain is thedistribution on the input space x .
zhao et al.
(2019)deﬁne a domain d as the pair of a distributiond on the input space x and a labeling functionf : x → y, i.e., domain d = (cid:104)d, f (cid:105)..in this work, we assume each annotator is aunique labeling function a : x → y. unitingeach annotator and the instances he/she labeled, wecan result in a number of domains {(cid:104)di, ai(cid:105)}|a|i=1,where a represents all annotators.
then the crowd-sourcing learning can be interpreted by the laterdeﬁnition, i.e., learning from these crowd annota-tors/domains and predicting the labels of raw inputs(sampled from the raw data distribution dexpert) inexpert annotator/domain (cid:104)dexpert, expert(cid:105).
to unifythe deﬁnition in a single distribution, we directlydeﬁne a domain as the joint distribution on theinput space x and the label space y..in addition, we can align to the former deﬁnitionby using the representation outputs xi = ai(x) asthe data input, which shows different distributionsfor the same sentence towards different annotators.
thus, each source domain di is the distibution ofxi, and we need learn the expert representationsxexpert to perform inference on the unlabled texts..5565alllclc-catthiswork737679fullfilteredalllclc-catthiswork737883fullfilteredalllclc-catthiswork758085fullfilteredalllclc-catthiswork788388fullfiltered5 related work.
5.3 named entity recognition.
5.1 crowdsourcing learning.
crowdsourcing is a cheap and popular way to col-lect large-scale labeled data, which can facilitatethe model training for hard tasks that require su-pervised learning (wang and zhou, 2016; shengand zhang, 2019).
in particular, crowdsourceddata is often regarded as low-quality, includingmuch noise regarding expert annotations as thegold-standard.
initial studies of crowdsourcinglearning try to arrive at a high-quality corpus bymajority voting or control the quality by sophisti-cated strategies during the crowd annotation pro-cess (khattak and salleb-aouissi, 2011; liu et al.,2017; tang and lease, 2011)..recently, the majority work focuses on full ex-ploration of all annotated corpus by machine learn-ing models, taking the information from crowdannotators into account including annotator relia-bility (rodrigues et al., 2014), annotator accuracy(huang et al., 2015), worker-label confusion ma-trix (nguyen et al., 2017), and sequential confusionmatrix (simpson and gurevych, 2019)..in this work, we present a totally differentviewpoint for crowdsourcing, regarding all crowd-sourced annotations as golden in terms of individ-ual annotators, just like the primitive gold-standardlabels corresponded to the experts, and further pro-pose a domain adaptation paradigm for crowdsourc-ing learning..5.2 domain adaptation.
domain adaptation has been studied extensively toreduce the performance gap between the resource-rich and resource-scarce domains (ben-david et al.,2006; mansour et al., 2009), which has also re-ceived great attention in the nlp community(daum´e iii, 2007; jiang and zhai, 2007; finkel andmanning, 2009; glorot et al., 2011; chu and wang,2018; ramponi and plank, 2020).
typical methodsinclude self-training to produce pseudo training in-stances for the target domain (yu et al., 2015) andrepresentation learning to capture transferable fea-tures across the source and target domains (seneret al., 2016)..in this work, we make correlations between do-main adaptation and crowdsourcing learning, en-abling crowdsourcing learning to beneﬁt from theadvances of domain adaptation, and then present arepresentation learning model borrowed from jiaet al.
(2019) and ¨ust¨un et al.
(2020)..ner is a fundamental and challenging task of nlp(yadav and bethard, 2018).
the bilstm-crf(lample et al., 2016) architecture, as well as bert(devlin et al., 2019), are able to bring state-of-the-art performance in the literature (jia et al., 2019;wang et al., 2020; jia and zhang, 2020).
may-hew et al.
(2020) exploits the bert-bilstm-crfmodel, achieving strong performance on ner..in addition, ner has been widely adopted ascrowdsourcing learning as well (finin et al., 2010;rodrigues et al., 2014; derczynski et al., 2016;yang et al., 2018).
thus, we exploit ner as acase study following these works, and take a bert-bilstm-crf model as the basic model for ourannotator-aware extension..6 conclusion and future work.
we studied the connection between crowdsourc-ing learning and domain adaptation, and then pro-posed to treat crowdsourcing learning as a domainadaptation problem.
following, we took ner asa case study, suggesting a representation learningmodel from recent advances of domain adaptationfor crowdsourcing learning.
by this case study,we introduced unsupervised and supervised crowd-sourcing learning, where the former is a widely-studied setting while the latter has been seldominvestigated.
finally, we conducted experimentson a widely-adopted benchmark dataset for crowd-sourcing ner, and the results show that our rep-resentation learning model is highly effective inunsupervised learning, achieving the best perfor-mance in the literature.
in addition, the supervisedlearning with a very small scale of expert annota-tions can boost the performance signiﬁcantly..our work sheds light on the application of effec-tive domain adaptation models on crowdsourcinglearning.
there are still many other sophisticatedcross-domain models, such as adversarial learn-ing (ganin et al., 2016) and self-training (yu et al.,2015).
future work may include how to apply theseadvances to crowdsourcing learning properly..acknowledgments.
we thank all reviewers for their hard work.
this re-search is supported by grants from the national keyresearch and development program of china (no.
2018yfc0832101) and the fonds of beijing ad-vanced innovation center for language resourcesunder grant tyz19005..5566ethical impact.
we present a different view of crowdsourcing learn-ing and propose to treat it as domain adaptation,showing the connection between these two topicsof machine learning for nlp.
in this view, many so-phisticated cross-domain models could be appliedto crowdsourcing learning.
moreover, the motiva-tion that regarding all crowdsourced annotations asgold-standard to the corresponding annotators, alsosheds light on introducing other transfer learningtechniques in future work..the above idea and our proposed representationlearning model for crowdsourcing sequence label-ing, are totally agnostic to any private informationof annotators.
and we do not use any sensitiveinformation, bu only the id of annotators, in prob-lem modeling and learning.
the crowdsourcedconll english ner data also anonymized anno-tators.
there will be no privacy issues in the future..references.
azad abad, moin nabi, and alessandro moschitti.
2017. self-crowdsourcing training for relation ex-traction.
in proceedings of the acl: short papers..shai ben-david, john blitzer, koby crammer, and fer-nando pereira.
2006. analysis of representations fordomain adaptation.
in proceedings of the twentiethannual conference on neural information process-ing systems, pages 137–144.
mit press..chris callison-burch and mark dredze.
2010. creat-ing speech and language data with amazon’s me-chanical turk.
in proceedings of the naacl-hlt2010 workshop on creating speech and languagedata with amazon’s mechanical turk, pages 1–12..chenhui chu and rui wang.
2018. a survey of do-main adaptation for neural machine translation.
inproceedings of coling, pages 1304–1319..gabriela csurka.
2017. domain adaptation for vi-sual applications: a comprehensive survey.
arxivpreprint arxiv:1702.05374..hal daum´e iii.
2007. frustratingly easy domain adap-.
tation.
in proceedings of acl, pages 256–263..leon derczynski, kalina bontcheva, and ian roberts.
2016. broad twitter corpus: a diverse named entityin proceedings of the col-recognition resource.
ing: technical papers, pages 1169–1179..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of naacl-hlt..eraldo r. fernandes and ulf brefeld.
2011. learn-in ecml-ing from partially annotated sequences.
pkdd, volume 6911 of lecture notes in computerscience, pages 407–422.
springer..tim finin, william murnane, anand karandikar,nicholas keller, justin martineau, and mark dredze.
2010. annotating named entities in twitter datawith crowdsourcing.
in proceedings of the naacl-hlt 2010 workshop on creating speech and lan-guage data with amazon’s mechanical turk..jenny rose finkel and christopher d. manning.
2009.in pro-.
hierarchical bayesian domain adaptation.
ceedings of hlt-naacl, pages 602–610..yaroslav ganin, evgeniya ustinova, hana ajakan, pas-cal germain, hugo larochelle, franc¸ois laviolette,mario marchand, and victor s. lempitsky.
2016.domain-adversarial training of neural networks.
j.mach.
learn.
res., 17:59:1–59:35..xavier glorot, antoine bordes, and yoshua bengio.
2011. domain adaptation for large-scale sentimentin pro-classiﬁcation: a deep learning approach.
ceedings of icml, pages 513–520..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
in proceedings of icml, pages 2790–2799..dirk hovy, taylor berg-kirkpatrick, ashish vaswani,and eduard hovy.
2013. learning whom to trustwith mace.
in proceedings of the naacl-hlt..dirk hovy, barbara plank, and anders søgaard.
2014.experiments with crowdsourced re-annotation of apos tagging data set.
in proceedings of acl..pei-yun hsueh, prem melville, and vikas sindhwani.
2009. data quality from crowdsourcing: a study ofannotation selection criteria.
in proceedings of thenaacl hlt 2009 workshop on active learning fornatural language processing, pages 27–35..fei huang and alexander yates.
2010. exploringrepresentation-learning approaches to domain adap-tation.
in proceedings of the 2010 workshop on do-main adaptation for natural language processing..ziheng huang, jialu zhong, and rebecca j. passon-neau.
2015. estimation of discourse segmentationin proceedings of thelabels from crowd data.
emnlp, pages 2190–2200..emily jamison and iryna gurevych.
2015. noise oradditional information?
leveraging crowdsource an-notation item agreement for natural language tasks.
in proceedings of the emnlp, pages 291–297..chen jia, xiaobo liang, and yue zhang.
2019. cross-domain ner using cross-domain language model-ing.
in proceedings of acl, pages 2464–2474..5567chen jia and yue zhang.
2020. multi-cell composi-in pro-.
tional lstm for ner domain adaptation.
ceedings of acl, pages 5906–5917..jing jiang and chengxiang zhai.
2007..weighting for domain adaptation in nlp.
ceedings of acl, pages 264–271..instancein pro-.
faiza khan khattak and ansaf salleb-aouissi.
2011.quality control of crowd labeling through expertin proceedings of the nips 2nd work-evaluation.
shop on computational social science and the wis-dom of crowds, volume 2, page 5..aniket kittur, ed h. chi, and bongwon suh.
2008.crowdsourcing user studies with mechanical turk.
in proceedings of the 2008 conference on humanfactors in computing systems, chi 2008, 2008, flo-rence, italy, april 5-10, 2008, pages 453–456.
acm..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of naacl-hlt, pages 260–270..matthew lease.
2011. on quality control and machinelearning in crowdsourcing.
in human computation,volume ws-11-11 of aaai workshops.
aaai..maolin li, hiroya takamura, and sophia ananiadou.
2020. a neural model for aggregating coreferencein proceedings ofannotation in crowdsourcing.
coling, pages 5760–5773..mengchen liu, liu jiang, junlin liu, xiting wang,jun zhu, and shixia liu.
2017. improving learning-from-crowds through expert validation.
in proceed-ings of ijcai, pages 2329–2336..christopher manning and hinrich schutze.
1999.foundations of statistical natural language process-ing.
mit press..yishay mansour, mehryar mohri, and afshin ros-tamizadeh.
2009. domain adaptation with multiplein advances in neural information pro-sources.
cessing systems, volume 21, pages 1041–1048..stephen mayhew, nitish gupta, and dan roth.
2020.robust named entity recognition with truecasing pre-training.
in aaai 2020, pages 8480–8487..an thanh nguyen, byron wallace, junyi jessy li, aninenkova, and matthew lease.
2017. aggregatingand predicting sequence labels from crowd annota-tions.
in proceedings of acl, pages 299–309..benjamin nye, junyi jessy li, roma patel, yinfeiyang, iain marshall, ani nenkova, and byron wal-lace.
2018. a corpus with multi-level annotationsof patients, interventions and outcomes to supportlanguage processing for medical literature.
in pro-ceedings of the acl, pages 197–207..emmanouil antonios platanios, mrinmaya sachan,graham neubig, and tom mitchell.
2018. contex-tual parameter generation for universal neural ma-chine translation.
in proceedings of emnlp..alan ramponi and barbara plank.
2020. neural unsu-pervised domain adaptation in nlp—a survey.
inproceedings of the coling, pages 6838–6855..vikas c. raykar and shipeng yu.
2012a.
eliminatingspammers and ranking annotators for crowdsourcedlabeling tasks.
j. mach.
learn.
res., 13:491–518..vikas c. raykar and shipeng yu.
2012b.
eliminatingspammers and ranking annotators for crowdsourcedlabeling tasks.
j. mach.
learn.
res., 13:491–518..vikas c. raykar, shipeng yu, linda h. zhao, ger-ardo hermosillo valadez, charles florin, luca bo-goni, and linda moy.
2010. learning from crowds.
j. mach.
learn.
res., 11:1297–1322..filipe rodrigues and francisco c. pereira.
2018. deeplearning from crowds.
in proceedings of the aaai..filipe rodrigues, francisco c. pereira, and bernardeteribeiro.
2014. sequence labeling with multiple an-notators.
mach.
learn., 95(2):165–181..ozan sener, hyun oh song, ashutosh saxena, and sil-vio savarese.
2016. learning transferrable represen-tations for unsupervised domain adaptation.
in ad-vances in neural information processing systems..victor s. sheng, foster j. provost, and panagiotis g.ipeirotis.
2008. get another label?
improving dataquality and data mining using multiple, noisy label-ers.
in proceedings of the kdd, pages 614–622..victor s. sheng and jing zhang.
2019. machine learn-ing with crowdsourcing: a brief summary of thepast research and future directions.
proceedings ofthe aaai, 33(01):9837–9843..edwin simpson and iryna gurevych.
2019..abayesian approach for sequence tagging withcrowds.
in proceedings of the emnlp-ijcnlp..rion snow, brendan o’connor, daniel jurafsky, andandrew ng.
2008. cheap and fast – but is it good?
evaluating non-expert annotations for natural lan-guage tasks.
in proceedings of the emnlp..wei tang and matthew lease.
2011. semi-supervisedin sigirconsensus labeling for crowdsourcing.
2011 workshop on crowdsourcing for information re-trieval (cir), pages 1–6..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the conll at hlt-naacl 2003..ahmet ¨ust¨un, arianna bisazza, gosse bouma, andgertjan van noord.
2020. udapter: language adap-tation for truly universal dependency parsing.
inproceedings of the emnlp, pages 2302–2315..5568jing wang, mayank kulkarni, and daniel preotiuc-pietro.
2020. multi-domain named entity recogni-tion with genre-aware and agnostic inference.
inproceedings of the acl, pages 8476–8488..lu wang and zhi-hua zhou.
2016. cost-saving effectof crowdsourcing learning.
in proceedings of ijcai,ijcai’16, pages 2111–2117..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the emnlp: system demon-strations, pages 38–45..vikas yadav and steven bethard.
2018. a survey on re-cent advances in named entity recognition from deeplearning models.
in proceedings of the coling..yaosheng yang, meishan zhang, wenliang chen, weizhang, haofen wang, and min zhang.
2018. adver-sarial learning for chinese ner from crowd annota-tions.
in proceedings of the aaai..juntao yu, mohab elkaref, and bernd bohnet.
2015.domain adaptation for dependency parsing via self-in proceedings of the 14th internationaltraining.
conference on parsing technologies, pages 1–10..omar f. zaidan and chris callison-burch.
2011.crowdsourcing translation:professional qualityfrom non-professionals.
in proceedings of the acl-hlt, pages 1220–1229..jing zhang, xindong wu, and victor s. sheng.
2016.learning from crowdsourced labeled data: a survey.
artif.
intell.
rev., 46(4):543–576..han zhao, remi tachet des combes, kun zhang, andgeoffrey j. gordon.
2019. on learning invariant rep-resentations for domain adaptation.
in proceedingsof the icml, pages 7523–7532.
pmlr..a transformer with adapters.
in our adapter ◦ bert word representation, weinsert two adapter modules for each transformerlayer inside bert.
figure 6 shows the detailed net-work structure of transformer with adapters.
morespeciﬁcally, the forward operation of an adapterlayer is computed as follows:hmid = gelu(w aphout = w ap2 , bap1 , w ap.
2 hmid + bap1 and bapwhere w ap2 are adapter param-eters, and the dimension size of hmid is usuallysmaller than that of the corresponding transformer..1 hin + bap1 )2 + hin,.
(6).
figure 6: transformer integrated with adapters inside..model.
all.
mv.
gold.
finetuning.
2 layers4 layers6 layers8 layers10 layersall layers.
89.32.
74.1274.96bert with adapter inside73.8171.8373.3073.1674.8173.7475.3174.2474.5675.0175.2874.36.
89.2089.2689.3389.1389.2189.52.trainableparams size108m.
4.55m5.34m6.14m6.94m7.73m8.53m.
table 5: the comparisons between bert ﬁne-tuningand adapter ◦ bert based on the standard ner with-out annotator as input..here we also give a supplement to illustrate thepack operation from all adapter parameters into asingle vector v :.
(cid:77).
v =.
adapters.
{w ap.
1 ⊕ w ap.
2 ⊕ bap.
1 ⊕ bap.
2 },.
(7).
where ﬁrst all parameters of a single adapter arereshaped and concatenated and then a further con-catenation is performed over all adapters..b hyper-parameters.
we choose the bert-base-cased9, which is for en-glish language and consists of 12-layer transform-ers with the hidden size 768 for all layers.
we loadthe bert weight and implement the adapter injec-tion based on the transformers (wolf et al., 2020) li-brary.
the sizes of the adapter middle hidden statesare set to 128 constantly.
the annotator embeddingsize is 8 to ﬁt the model in one rtx-2080ti gpuof 11gb memory.
the bilstm hidden size is.
9https://github.com/google-research/bert.
5569layernorm+adapter2xfeed-forwardlayerlayernorm+adapterfeed-forwardlayermulti-headedattentionthis work.
model.
mv.
lc-cat.
mv.
gold.
lc-cat.
this work.
ground-truth.
text and entities.
unsupervised.
pace, a junior, helped [ohio state]loc to a 10-1 record and a berth in the rose bowl against[arizona]org state.
pace, a junior, helped [ohio state]org to a 10-1 record and a berth in the [rose bowl]m iscagainst [arizona]org state.
pace, a junior, helped [ohio state]org to a 10-1 record and a berth in the [rose bowl]m iscagainst [arizona state]org..supervised (25%).
pace, a junior, helped [ohio state]loc to a 10-1 record and a berth in the [rose bowl]m iscagainst [arizona state]loc .
[pace]p er, a junior, helped [ohio state]org to a 10-1 record and a berth in the [rosebowl]m isc against [arizona]org state.
pace, a junior, helped [ohio state]org to a 10-1 record and a berth in the [rose bowl]m iscagainst [arizona state]loc .
[pace]p er, a junior, helped [ohio state]org to a 10-1 record and a berth in the [rosebowl]m isc against [arizona state]org.
[pace]p er, a junior, helped [ohio state]org to a 10-1 record and a berth in the [rosebowl]m isc against [arizona state]org..table 6: a case study, where the text with underlines indicates errors..d case study.
here we also offer a case study to understandthe performance in unsupervised and supervisedcrowdsourcing learning, as well as the differentcrowdsourcing models.
we exploit one complexexample in table 6 which involves different out-puts for various models.
as shown, we can seethat supervised models are able to recall the am-biguous entity (i.e., pace, a single word with mul-tiple senses) correctly, while unsupervised mod-els fail, which may be due to the inconsistenciesof the crowdsourced annotations.
by comparingour model with other baselines, we can show thatour representation learning model can capture theglobal text input understanding consistently, e.g.,being able to connect ohio state and arizona statetogether..set to 400. for all models, we inject adapters orswitchers in all 12 layers of bert.
all experimentsare run on the single gpu at an 8-gpu server witha 14 core cpu and 128gb memory..we exploit the stochastic gradient-based onlinelearning, with a batch size of 64, to optimize modelparameters.
we apply the time-step dropout, whichrandomly sets several representations in the se-quence to zeros with a probability of 0.2, on theword representations to avoid overﬁtting.
we usethe adam algorithm to update the parameters witha constant learning rate 1 × 10−3, and apply thegradient clipping by a maximum value of 5.0 toavoid gradient explosion..c the advantage of adapter ◦ bert.
our models are all based on adapter ◦ bert asthe basic representations, which is different fromthe widely-adopted bert ﬁne-tuning architecture.
here we compare the two strategies in detail.
theresults are shown in table 5, where for adapter ◦bert we consider gradually increasing the numberof transformer layers (covering the last n layers)inside the bert.
as shown, it is apparently thatadapter ◦ bert is much more parameter efﬁcient,and when all layers are exploited, the model canbe even better than bert ﬁne-tuning.
thus it ismore desirable to use adapter ◦ bert covering allbert transformers inside..5570