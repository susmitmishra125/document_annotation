learning prototypical functions for physical artifacts.
tianyu jiang and ellen riloffschool of computinguniversity of utahsalt lake city, ut 84112{tianyu, riloff}@cs.utah.edu.
abstract.
humans create things for a reason.
ancientpeople created spears for hunting, knives forcutting meat, pots for preparing food, etc.
theprototypical function of a physical artifact isa kind of commonsense knowledge that werely on to understand natural language.
forexample, if someone says “she borrowed thebook” then you would assume that she intendsto read the book, or if someone asks “cani use your knife?” then you would assumein this pa-that they need to cut something.
per, we introduce a new nlp task of learningthe prototypical uses for human-made physicalobjects.
we use frames from framenet to rep-resent a set of common functions for objects,and describe a manually annotated data set ofphysical objects labeled with their prototypi-cal function.
we also present experimental re-sults for this task, including bert-based mod-els that use language model predictions frommasked patterns as well as artifact sense def-initions from wordnet and frame deﬁnitionsfrom framenet..1.introduction.
humans are a creative species.
new objects areinvented by people every day, and most are cre-ated for a reason.
knives were created for cutting,bicycles were created for transportation, and tele-phones were created for communication.
someobjects can perform multiple functions (e.g., smartphones) and humans are also creative at ﬁndingsecondary uses for objects (e.g., heavy objects areoften used as makeshift paperweights).
but whenwe mention physical objects in conversation or inwriting, people generally infer that the object willbe used in the most prototypical way, unless theyare told otherwise..the prototypical function of human-made physi-cal artifacts is a kind of commonsense knowledge.
that often plays a role in natural language under-standing.
consider the following examples of in-ferences that arise from physical artifacts..example 1a) he killed the mayor with a gun.
b) he killed the mayor with a knife.
c) he killed the mayor with a bomb..example 1 describes a killing with three differ-ent types of instruments.
most readers would as-sume that a) describes a shooting, b) describes astabbing, and c) describes an explosion.
but ex-actly how each instrument was used is implicit.
we make different inferences about how they wereused based on our knowledge of the objects..example 2a) she ﬁnished the cigarette.
b) she ﬁnished the puzzle.
c) she ﬁnished the movie..example 2 illustrates how we infer different ac-tions based on the object when the main action iselided (i.e., “ﬁnished” means that some action hasended but the action itself is implicit).
most peoplewould assume that the cigarette was smoked, thepuzzle was solved, and the movie was watched..example 3a) she put the cake in the box.
b) she put the cake in the oven.
c) she put the cake in the refrigerator..example 3 illustrates second-order inferencesthat can follow from a sentence.
the verb “put”means that the cake was placed somewhere, but theobject of “in” leads to different inferences aboutintention.
putting a cake in an oven implies that itwill be baked, but putting a cake in a refrigeratorimplies that it will be cooled..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6941–6951august1–6,2021.©2021associationforcomputationallinguistics6941example 4a) he ordered a taxi.
b) he ordered a pizza.
c) he ordered a t-shirt..example 4 reveals inferences about motivationsand future plans.
if someone orders a taxi then weinfer that they need transportation, if they order apizza then we expect they will eat it, and if theyorder a t-shirt then we assume it will be worn..we believe that it is essential for nlp systems to“read between the lines” and make the same typesof inferences that people do when reading thesesentences.
the goal of our research is to exploremethods for learning the prototypical functions ofhuman-made physical artifacts so that future nlpsystems can beneﬁt from this knowledge.
first,we deﬁne a new nlp task to associate physicalobjects with frames from framenet as a canonicalrepresentation for their prototypical function.
weintroduce a gold standard data set of 938 physicalartifacts that have each been labeled with a framethat represents its prototypical function based onhuman judgements.
second, we evaluate baselinemodels to assess how well existing resources andsimple methods perform on this task.
third, wepresent transformer-based models for this task thatexploit both masked sentence patterns and the def-initions of physical artifacts and frames.
experi-ments show that our best model yields substantiallybetter results than the baseline methods..2 related work.
researchers have known for a long time that com-monsense knowledge is essential for natural lan-guage understanding (charniak, 1972; schank andabelson, 1977).
some of this work speciﬁcallyargued that commonsense knowledge about physi-cal objects, including functional knowledge, playsan important role in narrative text understanding(burstein, 1979; lehnert and burstein, 1979)..these observations have led to considerablework toward constructing commonsense knowl-edge repositories.
the cyc project (lenat, 1995)built a large ontology of commonsense conceptsand facts over many years.
more recently, con-ceptnet (speer et al., 2017) captures commonsenseknowledge in the form of predeﬁned relations ex-pressed in natural language words and phrases.
it was built from open mind common sense, acrowd-sourced knowledge project (singh, 2002),.
and later enhanced with other sources such as wik-tionary and wordnet (miller, 1995)..within the nlp community, a variety of recentprojects have focused on trying to acquire differenttypes of commonsense knowledge, such as forbesand choi (2017); collell et al.
(2018); rashkin et al.
(2018); yang et al.
(2018).
sap et al.
(2019) pre-sented a crowd-sourced commonsense reasoningdata set called atomic that focuses on inferentialknowledge related to events, which is organizedas if-then relations.
bosselut et al.
(2019) laterproposed comet, a transformer-based frameworkfor automatic construction of commonsense knowl-edge bases that was trained from atomic andconceptnet.
both conceptnet and comet in-clude a usedfor relation that is relevant to our task,and we evaluate their performance on our data setin section 6..of relevance to our work, jiang and riloff (2018)learned the prototypical “functions” of locations byidentifying activities that represent a prototypicalreason why people go to a location.
for example,people go to restaurants to eat, airports to catch aﬂight, and churches to pray.
they referred to the as-sociated activity as a prototypical goal activity andpresented a semi-supervised method to iterativelylearn the goal activities..our work is also related to frame semantics,which studies how we associate words and phraseswith conceptual structures called frames (fillmore,1976), which characterize an abstract scene or sit-uation.
the berkeley framenet project (bakeret al., 1998; ruppenhofer et al., 2016) provides anonline lexical database for frame semantics and acorpus of annotated documents.
there has beensubstantial work on frame semantic parsing (e.g.,das et al., 2014; peng et al., 2018), which is the taskof automatically extracting frame structures fromsentences.
several efforts have enhanced framenetby mapping it to other lexicons, such as wordnet,propbank and verbnet (shi and mihalcea, 2005;palmer, 2009; ferr´andez et al., 2010).
pavlick et al.
(2015) increased the lexical coverage of framenetthrough automatic paraphrasing and manual veri-ﬁcation.
yatskar et al.
(2016) introduced situationrecognition, which is the problem of producing aconcise summary of the situation that an imagedepicts.
similar to our work, they selected a sub-set of frames from framenet to represent possiblesituations depicted in an image.
our work uses asubset of frames from framenet to represent the.
6942prototypical functions for human-made physicalartifacts..that we needed.
overall, it serves as an appropriateplatform for our work..3 motivation.
our work was motivated by observing sentencesthat mention physical objects and realizing that weoften infer a richer meaning for these sentencesthan what they explicitly state.
we came to ap-preciate that the prototypical function of an objectwas the basis for many of our inferences, but wealso recognized that not all objects have a prototyp-ical function.
in particular, naturally occurring ob-jects rarely have a prototypical function (e.g., rock,snake).
in contrast, human-made physical objectsusually do have a prototypical function becausethey were created for a purpose.
consequently,we limited the scope of our work to human-madeartifacts.
of course, some objects are commonlyused for multiple purposes, but in most cases thereseems to be one use that is dominant, so for thesake of tractability we decided to assign a single(most) prototypical function to each artifact for thisresearch.
we had initially planned to include fooditems, but many foods are also naturally occurringplants or animals (e.g., watermelon, shrimp) so weomitted them.
it may be worth re-examining theselimitations in future work..another key decision that we had to make washow to represent the prototypical functions.
somerecent work on commonsense knowledge acquisi-tion has opted to generate words and phrases as ex-pressions of a relation, such as conceptnet (speeret al., 2017) and atomic (sap et al., 2019).
asan example, conceptnet includes a relation calledusedfor that lists the following phrases as usesfor a knife: stabbing, butter, cutting food, carvingwood, slicing, boning..we chose to adopt a different approach.
first,we wanted a canonical representation for each typeof function that represents a general concept, ratherthan a list of phrases.
this approach naturally cap-tures clusters of objects (i.e., those assigned to thesame frame) and avoids evaluation issues arisingfrom differing phrases that may be learned for sim-ilar objects (e.g., cut vs. carve vs. slice).
second,we did not want to reinvent the wheel and develop anew taxonomy of action types ourselves.
for thesereasons, we chose to use the semantic frames inframenet as a canonical representation for our pro-totypical functions.
although framenet is not per-fect nor complete, it contains many of the actions.
this approach also opens up new avenues forresearch down the road.
although it is beyond thescope of this paper, we can imagine that sentencescould trigger frames based on inferences originat-ing from physical objects during semantic parsing.
for example, “she used a pencil” should arguablybe represented as a writing (text creation) event.
however we leave that challenge for future work.
this paper focuses on the speciﬁc task of learningthe prototypical functions for human-made physi-cal artifacts using a subset of framenet frames asthe set of function types..4 creating a gold standard data set.
4.1 artifact selection.
as explained in section 3, our work focuses onartifacts that are 1) physical objects and 2) createdby people.
to acquire a list of objects that meetthese criteria, we extracted all terms in synsetsthat are descendants of the artifact.n.01 synset1in wordnet (miller, 1995).
we then removed aterm from the list if the artifact sense was not itsﬁrst sense deﬁnition.2 this process produced 8,822entries, many of which met our criteria except thatthe list still contained a lot of abstract terms (e.g.,vocabulary, modernism)..to address this issue, we turned to brysbaert et al.
(2014) which presents concreteness ratings basedon crowd sourcing for 37,058 english words and2,896 two-word expressions.
they used a 5-pointrating scale ranging from abstract to concrete, sowe extracted words with the part-of-speech “noun”and a rating ≥ 4.5, which produced a list of 3,462concrete nouns.
we then intersected this list withthe terms extracted from wordnet, producing a setof 1,017 concrete physical artifacts..4.2 frame selection.
framenet 1.7 contains 1,221 frame deﬁnitions.
however, not all of them are suitable for repre-senting typical uses of physical artifacts, whichshould be actions that involve a physical object.
for example, some frames are intended for abstractnominal categories (e.g., calendric unit for tempo-ral terms), high-level abstractions (e.g., intention-ally act which sits above more speciﬁc frames),.
1except we removed synsets for buildings and roads.
2because the ﬁrst sense deﬁnition in wordnet usually,.
though not always, represents the most common meaning..6943artifact function frames.
light movement (16)building (15)dimension (15)removing (14)closure (13)competition (13).
wearing (145)containing (76)self motion (69)protecting (52)supporting (49)cause harm (48)perception experience (44) create representation (13)make noise (37)cause motion (24)cutting (19)cooking creation (18)ingestion (18)reading activity (17)grooming (16).
bringing (12)sleep (12)text creation (12)attaching (11)contacting (10)cure (9)cause temperature change (8) rite (1).
hunting (8)cause ﬂuidic motion (6)eclipse (5)inhibit movement (5)performing arts (5)setting ﬁre (5)cause to fragment (4)education teaching (3)excreting (3)cause to be dry (2)agriculture (1)commercial transaction (1)residence (1).
table 1: frames for prototypical functions of physical artifacts.
the frequency with which they occur in our goldstandard data set is shown in parentheses..and events or states that are not typically associ-ated with physical artifacts (e.g., judgement)..to focus on an appropriate subset of frames, wemanually selected 42 frames in framenet that rep-resent actions that are common functions of human-made physical artifacts.
we intentionally didn’t se-lect frames that categorize nouns in a general way.
for example, framenet contains an artifact framethat includes oven, phone and wheel as its lexicalunits.
this frame only serves to identify terms thatrepresent physical objects, and we wanted framesthat represent a function.
the list of frames that weused is shown in table 1 along with the frequencywith which they occur in our gold standard data set,as described in the next section..4.3 human annotation.
to create a gold standard data set with frame assign-ments for the physical artifacts, we recruited 3 hu-man annotators.
we presented the annotators withthe wordnet deﬁnition for each term and askedthem to select one frame that captures the most pro-totypical use for the artifact.
in addition to the 42function frames, we also gave them a none option ifnone of the frames was a good match, and a not anartifact option if the term was not in fact a human-made physical artifact (because our list extractedfrom wordnet and brysbaert et al.
(2014) was notperfect).
to prepare the annotators, we asked themto read the deﬁnitions of all the frames beforehandand we gave them detailed annotation guidelinesto familiarize them with the task.
we randomly.
frame.
artifact examples.
wearingcontainingself motionprotectingsupportingcause harmperception expmake noisecause motioncutting.
hat, shirtbasket, luggagebicycle, yachtarmor, helmetchair, scaffoldingcannon, spearearphone, eyeglassbell, violinengine, propellerknife, scissors.
table 2: examples of artifacts for the top 10 frames..sorted the artifacts before presenting them to theannotators..when the annotations were ﬁnished, we mea-sured the pair-wise inter-annotator agreement(iaa) using cohen’s kappa.
the iaa scores were0.75, 0.72 and 0.69, with an average of κ = 0.72.given the difﬁculty of this task (44 possible labels),we felt that the human agreement was relativelygood..finally, we created the gold standard data set3by using the majority label from the three humanannotators.
there were 72 artifacts with no major-ity label (i.e., the annotators assigned 3 differentlabels), and 7 terms with the majority label not an.
3the data set is available at: https://github.com/.
tyjiangu/physical_artifacts_function.
6944artifact, so we discarded these 79 terms.
conse-quently, our gold standard data set contains 938physical artifacts that are each labeled with a framerepresenting its most prototypical function, or la-beled as none when none of our 42 frames wasappropriate.4 table 2 shows the 10 most frequentlyassigned frames and a few examples of artifactsassigned to each frame..5 methods.
we explored several approaches for learning theprototypical functions of human-made physical ar-tifacts.
to assess the difﬁculty of this task, we ﬁrstpresent baseline models that 1) exploit informationextracted from existing knowledge bases and 2) useco-occurrence information extracted from a textcorpus.
next, we explore methods that use largeneural language models.
we describe a method thatuses masked pattern predictions, and then presentmodels that also incorporate artifact sense deﬁni-tions and frame deﬁnitions..5.1 notation.
we model our task as a multiclass classiﬁcationproblem.
the artifacts and frames are denotedas ai (i = 1..m) and fj (j = 1..n).
the taskis to select the fj that represents the most pro-totypical use for an artifact ai.
we will denotethe set of lexical units for fj in framenet asluj = {lk|lk evokes fj}.5.
5.2 conceptnet and comet baselines.
conceptnet (speer et al., 2017) is a well-knowncommonsense knowledge resource that contains ausedfor relation, which is potentially relevant toour task (though it should be noted that an objectcan be used in ways that are not prototypical, soour task of identifying the prototypical use is notexactly the same).
comet (bosselut et al., 2019)is a framework that was trained on conceptnetwith the goal of improving upon its coverage.
ourﬁrst experiments apply these resources to see howeffective they can be for this task..for each artifact in conceptnet, we extract theﬁrst word from each phrase listed under its usedforrelation.
these are typically verbs that describe anaction although sometimes they are nouns.
forcomet, we use its beam-10 setting to generate 10phrases of the usedfor relation for each artifact..483 terms were assigned to the none category.
5we merged lexical units from similar frames in framenet..see details in appendix a..xcomp.
dobj.
dobj.
prep.
pobj.
use.
n.to.
v.v.n.v.adp.
n.figure 1: dependency patterns used for co-occurrence..next, we want to use the extracted words to rankcandidate frames.
framenet deﬁnes lexical unitsthat can evoke a speciﬁc frame.
for example, readcan trigger the reading activity frame.
supposeour artifact is a book and one of the extracted wordsis read, then reading activity is a candidate frame.
we then score each frame based on the overlapbetween the words extracted from conceptnet orcomet and the frame’s lexical units.
speciﬁcally,we deﬁne f req(ai, w) as the count of a word woccurring in the usedfor relation of artifact ai, andi(w, fj) = 1 if w ∈ lu j otherwise 0. then ourscore for fj is deﬁned as:.
scn(ai, fj) =.
f req(ai, w) ∗ i(w, fj),.
(1).
(cid:88).
w(cid:15)w.fi-where w is the set of extracted words.
nally, for each ai, we select fj(cid:48) such that j(cid:48) =arg maxj scn(ai, fj) as its prototypical function.
if scn(ai, f (cid:48).
j) equals zero, then we predict none..5.3 co-occurrence baseline.
an intuitive idea for potentially learning commonfunctions associated with physical artifacts is to ex-tract verbs that frequently co-occur with the artifactin a large text corpus.
we assume that if a verb fre-quently co-occurs with an artifact, then the framesassociated with the verb are plausible candidatesfor the artifact’s prototypical function..for this approach, we created 3 dependencyparse patterns to extract <noun, verb> pairs, asdepicted in figure 1. the physical object is thenoun represented by n. the activity is a verb (withan appended particle if one exists) represented by v.we included the verb-dobj pattern because some ar-tifacts and their functions are expressed in this way,such as “read book” or “wear jacket”.
we usedspacy6 to parse the whole english wikipedia cor-pus (as of feb 20, 2020) and extracted over 3.8 mil-lion <n, v> pairs (305,055 distinct pairs) for our938 artifacts.
we deﬁne the function f req(ai, v)as the co-occurrence count of artifact ai and verbv in the corpus.
then we apply the same methoddescribed in section 5.2 to assign a score to each.
6https://spacy.io/.
6945figure 2: overview of the pfmask model.
each pink block that is fed into bert represents a sentence templatefor a given artifact..frame based on the extracted verbs and select thebest frame..template as:.
5.4 masked language model (mlm).
baseline.
co-occurrence in text is a strong signal of correla-tion.
but an activity that is highly correlated withan artifact may not be its prototypical use.
for ex-ample, cut frequently co-occurs with rope, but thepurpose of a rope is not to be cut – its prototypicaluse is for attaching things..recent work has successfully used maskedlanguage models to learn commonsense knowledge(davison et al., 2019), so we explored whethermasked language models could be beneﬁcial forour task.
we use the bert (devlin et al., 2019)masked language model to get prediction scoresfor every (ai, lk) pair, where ai is one of ourphysical artifacts and lk is a lexical unit linkedto one of our 42 candidate frames.
we deﬁned6 sentence templates that represent expressionsdescribing what an object is used for, which areshown below.
the ﬁrst blank space is for artifactai and the second blank space is for action lk......can be used toto.
can be used for.
for.
(1)(2) i used(3)(4) i used(5) the purpose of(6) if i had.
, i could.
..is to..next, we produced a probability distributionover all of the lexical units based on the secondblank position.
speciﬁcally, for the t-th sentencetemplate st, we obtain p r(lk|st, ai) by maskingonly the second blank space (ai is inserted into theﬁrst blank) and we obtain p r(lk|st) by maskingboth blank space.
then we deﬁne the score of lkas the typical use of artifact ai based on the t-th.
u (ai, lk, st) = log p r(lk|st, ai) − log p r(lk|st).
(2)the score u (ai, lk) using all templates is computedas: u (ai, lk) = 1t.t u (ai, lk, st).
finally, we deﬁne the score for fj being the pro-.
(cid:80).
totypical function for ai as:.
smlm(ai, fj) =.
u (ai, lk)..(3).
(cid:88).
lk∈luj.
we select fj(cid:48) where j(cid:48) = arg maxj smlm(ai, fj)as the best frame.
if smlm(ai, f (cid:48)j) ≤ 0, we predictnone..5.5 learning from masked patterns.
our mlm baseline uses the discrete output of themasked language model (i.e., the prediction to-inkens from the vocabulary and their scores).
order to take advantage of a language model’s ﬁne-tuning capability, we use the same architecture asdescribed in section 5.4, except that instead of us-ing the predicted lexical units and their probabilityp r(lk|st, ai), we retrieve the last hidden state vec-tor for the [mask] token as output.
since thereare 6 masked templates, we have 6 output vectorsfor each artifact ai.
we compute the average ofthese vectors and pass it through a linear layer anda softmax layer to produce a probability distribu-tion over all candidate frames plus none.
figure2 shows the overview of this architecture, whichwe will call the pfmask model.
we will refer to theﬁnal score for artifact ai with respect to frame fjas smask(ai, fj).
the loss function is deﬁned as:.
l = −.
log smask(ai, fj∗),.
(4).
n(cid:88).
i=1.
where fj∗ is the gold label for ai..
6946...ℒbertlinear...[cls]aknifecanbeusedto.[sep][mask][cls]ifihadaknife,icould.
[sep][mask]figure 3: overview of the pfdef model.
each green block that is fed into bert represents an artifact and one ofthe candidate frames..5.6 learning from deﬁnitions.
the challenge for our task is obtaining informa-tion about the intended function of a physical ar-tifact.
we observed that this information is oftendescribed in the dictionary deﬁnition of an arti-fact, although it can be expressed in many differentways.
for example, the ﬁrst sense deﬁnition inwordnet for knife is “edge tool used as a cuttinginstrument...”, and for bus it is “a vehicle carryingmany passengers...”.
the deﬁnition often providesa short and precise sentence that describes what theartifact is as well as what it is typically used for..framenet also provides a deﬁnition for eachframe.
for example, the deﬁnition of the cuttingframe is “an agent cuts a item into pieces usingan instrument”.
jiang and riloff (2021) exploitedboth frame and lexical unit deﬁnitions for the frameidentiﬁcation task in a model that assesses the se-mantic coherence between the meaning of a targetword in a sentence and a candidate frame.
simi-larly, we hypothesized that a model could poten-tially learn the semantic relatedness between thedeﬁnitions of a physical artifact and the frame thatdescribes its typical function..to investigate this idea, we used the bertmodel (devlin et al., 2019) as the base of our ar-chitecture and ﬁne-tuned bert for our task usingboth dictionary deﬁnitions of artifacts and framedeﬁnitions from framenet.
figure 3 shows theoverview of this architecture, which we call thepfdef model.
each large green block represents anartifact ai paired with one of the candidate frames.
we encode wordnet’s deﬁnition of the artifact asthe ﬁrst input sequence and the frame’s deﬁnitionfrom framenet as the second input sequence tobert.
we use the last hidden vector of the [cls]token as the output.
for each artifact ai, we haven + 1 such pairs where n is the number of candi-date frames and 1 refers to the none option.
ontop of bert’s output, we apply a linear and asoftmax layer to produce a probability distribution.
figure 4: overview of the pfdef +mask model..over all candidate frames.
we will refer to the ﬁ-nal score for artifact ai with respect to frame fj assdef (ai, fj).
the loss function is deﬁned as:.
l = −.
log sdef (ai, fj∗),.
(5).
n(cid:88).
i=1.
where fj∗ is the gold label for ai..5.7 learning from deﬁnitions plus masked.
patterns.
our ﬁnal model combines the idea of using bothdeﬁnitions and masked sentence patterns.
figure4 depicts the combined pdef +mask model.
the leftpart is the pfdef model which estimates the relat-edness between artifact and frame deﬁnitions.
itsoutput is a matrix of dimension (# of frames, hiddenvector size).
the right part is the pfmask model,which predicts the most probable frame for an arti-fact using our masked patterns.
it produces a singleoutput vector of dimension (1, hidden vector size).
we broadcast it across the rows to have the samedimension as (# of frames, hidden vector size) andthen we concatenate the matrices of both modelsto pass through a linear layer before computing theloss.
the model uses ﬁne-tuning to jointly learn allparameters so that information from both modelswill optimally contribute to the ﬁnal prediction..6947[cls][sep][sep]artifact     definitionframe     definition...[cls][sep][sep]artifact definitionframe     definitionℒlinear...bert+linearℒconcat......model.
acc.
pre rec.
f1.
conceptnetco-occurrencecometmlm.
pfmaskpfdefpfdef +mask.
17.531.930.742.8.
58.574.776.8.
33.624.129.729.5.
35.763.565.2.
13.523.935.633.8.
36.557.661.1.
16.419.928.228.2.
35.459.362.4.table 3: experimental results for different models..6 evaluation.
6.1 experiment settings.
our gold standard data set contains 938 artifactsthat are each paired with one frame that representsits most prototypical use.
we set aside 20% (188)of the data as a development set and used 80%(750) as the test set.
we evaluated all of the learn-ing models by performing 5-fold cross validationon the test set.
we use the pre-trained uncasedbert-base model with the same settings as devlinet al.
(2019) and ﬁne-tuned bert on the trainingdata.
we set the max sequence length as 200, batchsize as 1, learning rate started at 2e-5, and trainfor 10 epochs.
all reported results are averagedover 3 runs.
we report overall accuracy as wellas precision, recall and f1 scores macro-averagedover the 43 class labels (42 frames + none)..6.2 results.
the ﬁrst four rows in table 3 show the performanceof our four baseline methods.
conceptnet andthe co-occurrence model produced the lowest f1scores.
we see that conceptnet has better precisionbut low recall because only about 1/3 of the arti-facts in our data set has a usedfor relation deﬁnedin conceptnet.
we also tried adding the capableofrelation, which is deﬁned as what an item can do,but it is even more sparse than usedfor and combin-ing both relations only marginally increased recall.
the performance of comet shows that cometdoes indeed improve upon the coverage of con-ceptnet, although it sacriﬁces some precision.
wealso tried using the beam-5 and greedy settingsof comet, which produced higher precision butlower recall and f1 scores..compared to comet, the co-occurrence base-line has higher accuracy but a much lower f1 score.
the explanation is that the co-occurrence model.
figure 5: f1 scores for high & low frequency frames..performs much better on frames that are associ-ated with artifacts that are frequently mentionedin the corpus than for frames associated with lessfrequent artifacts.
this is intuitive because, in gen-eral, we expect to extract a more representativesample of activities when we have more data.
thisphenomenon (accuracy much higher than f1) canalso be observed in the mlm model which uses apre-trained language model that learns from largecorpora, so it is not surprising that co-occurrenceand the mlm model behave similarly.
in contrast,conceptnet and comet behave more consistentlyacross the set of frames..the bottom section of table 3 shows the resultsfor our new models, which were trained speciﬁcallyfor this task.
the pfmask model achieves 58.5%accuracy and a 35.4% f1 score, which outperformsall of the baselines.
the pfdef model performssubstantially better, achieving 74.7% accuracy anda 59.3% f1 score.
this result demonstrates thatthe deﬁnitions of the artifacts and the frames pro-vide valuable information that a learner can beneﬁtfrom.
the last row shows the performance of thecombined model, which performed better than theindividual models.
this model saw additional gainsin both precision and recall, increasing the accu-racy from 74.7% to 76.8% and the f1 score from59.3% to 62.4%..6.3 analysis.
to understand the degree to which the number oftraining instances for each frame correlated withperformance, we divided the frames into two sets:high frequency frames assigned to ≥ 15 artifactsand low frequency frames assigned to < 15 arti-facts.
the results are shown in figure 5 with thef1 scores from the pfdef +mask model displayedon the y-axis.
we conclude that frames with moretraining instances generally showed better perfor-mance, so our model would likely further improve.
6948id artifact.
mask (cid:51) def (cid:51) 1mask (cid:51) def (cid:55)2mask (cid:55) def (cid:51) 3mask (cid:55) def (cid:51) 4mask (cid:55) def (cid:55)5.scissorshydrantbedhelmetsnowplow.
pfmask.
cutting.
supportingwearinghunting.
pfdef.
cutting.
sleepprotectingself motion.
cause ﬂuidic motion cause temperature change.
table 4: sample output of pfdef and pfmask models.
the correct predictions are in bold..given more training data..table 4 shows some examples of output from thepfmask and pfdef models to compare their behav-ior.
the correct predictions appear in bold.
bothmodels are correct for example 1. for example 2,only the pfmask model is right, which indicatesthat the masked pattern can be more useful than thedeﬁnition sometimes.
for examples 3 and 4, pfdefwas correct and pfmask was wrong.
the pfmaskmodel sometimes generates frames representingfunctions that are true but tangential.
for exam-ple, beds do support us and helmets are worn, butthese functions do not sufﬁciently characterize theobjects (e.g., chairs also support us but are not typ-ically used for sleeping, and jewelry is also wornbut not used for protection).
for example 5, bothmodels are wrong – the correct frame is removing.
though both are wrong, the pfdef model producesa more reasonable answer than the pfmask model.7we also observed that the mlm baseline some-times produces seemingly random answers that arehard to explain..finally, we investigated the 83 instances thatwere labeled as none to see what kind of artifactsfell into this category.
the biggest cluster of relatedartifacts were 17 types of fabric, such as linen, silkand canvas.
framenet does not include a frame formaterials of this kind, probably because they arean ingredient for making clothes rather than toolsthemselves.
artifacts like toy were also labeled asnone presumably because toys are used in a gen-eral way (for play).
this category also includedsome artifacts not tied to a single prototypical func-tion but commonly used for many purposes (e.g.,computer, laptop)..7 conclusion.
used a subset of frames from framenet to representthe set of common functions.
we also presenteda manually annotated data set of 938 physical ar-tifacts for this task.
our experiments showed thata transformer-based model using both artifact andframe deﬁnitions as well as masked pattern pre-dictions outperforms several baseline methods.
infuture work, we hope to show the value of func-tional knowledge about objects for sentence-levelunderstanding tasks as well as narrative documentunderstanding..acknowledgments.
we thank yuan zhuang for his helpful commentson our work.
we also thank the anonymous review-ers for their valuable suggestions and feedback..references.
collin f. baker, charles j. fillmore, and john b. lowe.
in pro-1998. the berkeley framenet project.
ceedings of the 36th annual meeting of the associ-ation for computational linguistics and 17th inter-national conference on computational linguistics(acl-coling 1998)..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, asli celikyilmaz, and yejin choi.
2019. comet: commonsense transformers for au-tomatic knowledge graph construction.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics (acl 2019)..marc brysbaert, amy beth warriner, and victor ku-perman.
2014. concreteness ratings for 40 thousandgenerally known english word lemmas.
behavior re-search methods, 46(3):904–911..mark h. burstein.
1979. the use of object-speciﬁcknowledge in natural language processing.
in pro-ceeding of the 17th annual meeting on associationfor computational linguistics (acl 1979)..we introduced the new task of learning prototypicalfunctions for human-made physical artifacts, and.
eugene charniak.
1972. toward a model of children’s.
story comprehension.
ph.d. thesis, mit..7in fact, snowplow can also refer to a skiing action, al-.
though wordnet does not contain that word sense..guillem collell, luc van gool, and marie-francinemoens.
2018. acquiring common sense spatial.
6949knowledge through implicit spatial templates.
inproceedings of the thirty-second aaai conferenceon artiﬁcial intelligence (aaai 2018)..dipanjan das, desai chen, andr´e f.t.
martins,nathan schneider, and noah a smith.
2014.frame-semantic parsing.
computational linguistics,40(1):9–56..joe davison, joshua feldman, and alexander rush.
2019. commonsense knowledge mining from pre-in proceedings of the 2019 con-trained models.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp 2019)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl 2019)..´oscar ferr´andez, michael ellsworth, rafael mu˜noz,and collin f. baker.
2010. aligning framenet andwordnet based on semantic neighborhoods.
in pro-ceedings of the seventh international conference onlanguage resources and evaluation (lrec 10)..charles j. fillmore.
1976. frame semantics and thein annals of the new yorknature of language.
academy of sciences: conference on the origin anddevelopment of language and speech, volume 280(1), pages 20–32..maxwell forbes and yejin choi.
2017. verb physics:relative physical knowledge of actions and ob-jects.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (acl2017)..tianyu jiang and ellen riloff.
2018. learning proto-typical goal activities for locations.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (acl 2018)..tianyu jiang and ellen riloff.
2021. exploiting deﬁ-nitions for frame identiﬁcation.
in proceedings ofthe 16th conference of the european chapter of theassociation for computational linguistics (eacl2021)..wendy g. lehnert and mark h. burstein.
1979. therole of object primitives in natural language pro-in proceedings of the 6th internationalcessing.
joint conference on artiﬁcial intelligence (ijcai1979)..douglas b. lenat.
1995. cyc: a large-scale investmentin knowledge infrastructure.
communications of theacm, 38(11):33–38..george a. miller.
1995..a lexicaldatabase for english.
communications of the acm,38(11):39–41..wordnet:.
martha palmer.
2009. semlink: linking propbank,in proceedings of the gen-verbnet and framenet.
erative lexicon conference, pages 9–15.
genlex-09,pisa, italy..ellie pavlick, travis wolfe, pushpendre rastogi,chris callison-burch, mark dredze, and benjaminframenet+: fast paraphras-van durme.
2015.in proceedings of thetic tripling of framenet.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (acl-ijcnlp 2015)..hao peng, sam thomson, swabha swayamdipta, andnoah a. smith.
2018. learning joint semanticin proceedings of theparsers from disjoint data.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies (naacl 2018)..hannah rashkin, maarten sap, emily allaway,noah a. smith, and yejin choi.
2018. event2mind:commonsense inference on events, intents, and reac-tions.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (acl2018)..josef ruppenhofer, michael ellsworth, myriam r. l.petruck, christopher r. johnson, collin f. baker,and jan scheffczyk.
2016. framenet ii: extendedtheory and practice..maarten sap, ronan le bras, emily allaway, chan-dra bhagavatula, nicholas lourie, hannah rashkin,brendan roof, noah a smith, and yejin choi.
2019.atomic: an atlas of machine commonsense for if-then reasoning.
in proceedings of the aaai confer-ence on artiﬁcial intelligence (aaai 2019)..roger c. schank and robert p. abelson.
1977. scripts,plans, goals and understanding.
lawrence erl-baum associates, hillsdale, new jersey..lei shi and rada mihalcea.
2005. putting pieces to-gether: combining framenet, verbnet and wordnetin international con-for robust semantic parsing.
ference on intelligent text processing and computa-tional linguistics..push singh.
2002. the public acquisition of common-in proceedings of aaai springsense knowledge.
symposium: acquiring (and using) linguistic (andworld) knowledge for information access..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-eral knowledge.
in proceedings of the aaai confer-ence on artiﬁcial intelligence (aaai 2017)..yiben yang, larry birnbaum, ji-ping wang, and dougdowney.
2018. extracting commonsense proper-ties from embeddings with limited human guid-ance.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (acl2018)..6950mark yatskar, luke zettlemoyer, and ali farhadi.
2016. situation recognition: visual semantic rolelabeling for image understanding.
in proceedings ofthe ieee conference on computer vision and patternrecognition..a appendix.
when selecting frames to represent the prototypi-cal functions of physical artifacts, we observed thatsome frames in framenet share similar meanings(e.g., reading activity and reading perception) orrelated functions (e.g., create representation andrecording).
however, these frames often have com-plementary sets of lexical units..since our baselines (conceptnet, comet, co-occurrence, and mlm) rely on the lexical unitsof frames to make predictions, increasing the cov-erage of lexical units can be beneﬁcial.
so wemanually clustered frames that share a related deﬁ-nition with our 42 chosen frames and merged theirlexical units.
the table below shows the clusterfor which the lexical units are merged.
our experi-ments showed that merging lexical units from theseframes improved both the precision and recall..primary frame.
clustered frames.
agriculture.
attachingcause ﬂuidic motioncause harm.
cause motion.
food gatheringgrowing foodplantingconnectorscause to be wetattackweaponcause to move in-placegrinding.
inhibit movementlight movementmake noise.
competitioncontainingcooking creationcreate representationcurehunting.
cause to fragmentcommercial transaction commerce buycommerce sellexercisingcontainersapply heatrecordingrecoverytaking captivetrapimmobilizationlocation of lightcause to make noisenoise makersperception activecause to perceiveinformation displayreading perceptionemptyingvehicleride vehicleoperate vehicleposturebody decorationclothingaccoutrementsclothing parts.
reading activityremovingself motion.
supportingwearing.
perception experience.
6951