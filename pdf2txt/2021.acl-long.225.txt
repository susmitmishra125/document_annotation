unsupervised neural machine translation for low-resource domains viameta-learning.
cheonbok park1,∗, yunwon tae2,∗,taehee kim3, soyoung yang3,mohammad azam khan4, lucy park5,†, and jaegul choo31naver corp,cbok.park@navercorp.com, 2korea university, tyj204@korea.ac.kr3kaist, {taeheekim, sy yang, jchoo}@kaist.com4dpdc, azam@dpdc.org.bd, 5upstage ai research, lucy@upstage.ai.
abstract.
unsupervised machine translation, which uti-lizes unpaired monolingual corpora as trainingdata, has achieved comparable performanceagainst supervised machine translation.
how-ever, it still suffers from data-scarce domains.
to address this issue, this paper presents anovel meta-learning algorithm for unsuper-vised neural machine translation (unmt) thattrains the model to adapt to another domainby utilizing only a small amount of trainingdata.
we assume that domain-general knowl-edge is a signiﬁcant factor in handling data-scarce domains.
hence, we extend the meta-learning algorithm, which utilizes knowledgelearned from high-resource domains, to boostthe performance of low-resource unmt.
ourmodel surpasses a transfer learning-based ap-proach by up to 2-3 bleu scores.
extensiveexperimental results show that our proposedalgorithm is pertinent for fast adaptation andconsistently outperforms other baselines..1.introduction.
unsupervised neural machine translation (unmt)leverages unpaired monolingual corpora for itstraining, without requiring an already labeled,parallel corpus.
recently, the state of the art inunmt (conneau and lample, 2019; song et al.,2019; ren et al., 2019) has achieved comparableperformances against supervised neural machinetranslation (nmt) approaches.
in contrast to super-vised nmt, which uses a parallel corpus, trainingthe unmt model requires a signiﬁcant number ofmonolingual sentences (e.g., 1m-3m sentences).
however, the prerequisite limits unmt’s appli-cability to low-resource domains, especially for.
∗ equal contributions† this work done in naver corp.our code is avilable at https://github.com/.
papago-lab/metagumt.
domain-speciﬁc document translation tasks.
sincegathering or creating those documents requires do-main speciﬁc knowledge, the monolingual datathemselves are scarce and expensive.
in addition,the minority languages (e.g., uzbek and nepali)make the problem of data scarcity even worse..yet, unmt for low-resource domains is not anactively explored ﬁeld.
one naive approach is totrain a model on high-resource domains (e.g., econ-omy and sports) while hoping the model will gen-eralize on an unseen low-resource domain (e.g.,medicine).
however, recent studies have shownthat non-trivial domain mismatch can signiﬁcantlycause low translation accuracy on supervised nmttasks (koehn and knowles, 2017)..another.
reasonable.
approach is.
transferlearning—particularly, domain adaptation—whichhas shown performance improvements in the su-pervised nmt literature (freitag and al-onaizan,2016; zeng et al., 2019).
in this approach, themodel is ﬁrst pretrained using data from existingdomains and then ﬁnetuned on a new domain.
however, this approach can suffer from overﬁttingand catastrophic forgetting due to a small amountof training data and a large domain gap..as an effective method for handling a smallamount of training data, meta-learning has shownits superiority in various nlp studies such as di-alog generation, machine translation, and naturallanguage understanding (qian and yu, 2019; guet al., 2018; dou et al., 2019).
in general, the meta-learning approach is strongly affected by the num-ber of different tasks where tasks are deﬁned as lan-guages or domains from the aforementioned stud-ies.
however, in practice, the previous studies maystruggle to gather data to deﬁne tasks because theyrely on a supervised model that requires labeledcorpora.
in this respect, we argue that applying ameta-learning approach to the unsupervised modelis more feasible and achievable than the supervised.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2888–2901august1–6,2021.©2021associationforcomputationallinguistics2888model because it can deﬁne multiple different taskswith unlabeled corpora.
therefore, we introducea new meta-learning approach for unmt, calledmetaumt, for low-resource domains by deﬁningeach task as a domain..the objective of metaumt is to ﬁnd the opti-mal initialization for the model parameters that canquickly adapt to a new domain even with only asmall amount of monolingual data.
as shown infig.
1 (a), we deﬁne two different training phases,a meta-train and a meta-test phase, and simulatethe domain adaption process to obtain optimallyinitialized parameters.
speciﬁcally, the meta-trainphase adapts model parameters to a domain whilethe meta-test phase optimizes the parameters ob-tained from the meta-train phase.
after obtainingoptimally initialized parameters through these twophases, we ﬁne-tune the model using a target do-main (i.e., a low-resource domain)..although the initial parameters optimizedthrough metaumt are suitable for adapting to alow-resource domain, these parameters may notfully maintain the knowledge of high-resourcedomains.
concretely,in the meta-test phase,metaumt optimizes initial parameters using theadapted parameters; however, it discards meta-trainknowledge used to update adapted parameters inthe meta-train phase.
therefore, instead of validat-ing the same domain used in the meta-train phase,we intend to inject generalizable knowledge intothe initial parameters by utilizing another domainin the meta-test phase.
this prevents overﬁttingfrom the data scarcity issue..as shown in fig.
1 (b), we propose an improvedmeta-learning approach called metagumt for low-resource unmt by explicitly infusing commonknowledge across multiple source domains as wellas generalizable knowledge from one particular do-main to another.
in other words, we do not onlyencourage the model to ﬁnd the optimally initial-ized parameters that can quickly adapt to a targetdomain with low-resource data, but also encour-age the model to maintain common knowledge(e.g., general words such as determiners, conjunc-tions, and pronouns) which is obtainable from mul-tiple source domains.
furthermore, due to a smallamount of training data in a low-resource domain,the model can suffer from overﬁtting; however, weattempt to handle overﬁtting by leveraging general-izable knowledge that is available from one domainto another.
our proposed meta-learning approach.
demonstrates consistent improvements over otherbaseline models..overall, our contributions can be summarized as.
follows:.
• we apply a meta-learning approach forunmt.
to the best of our knowledge, this isthe ﬁrst study to use a meta-learning approachfor unmt, where this approach is more suit-able to a unmt task than a supervised one..• we empirically demonstrate that our enhancedmethod, metagumt, shows fast convergenceon both pre-training (i.e., meta-learning withsource domains) and ﬁnetuning (i.e., adaptingto a target domain)..• the model trained with metagumt consis-tently outperforms all baseline models includ-ing metaumt.
this demonstrates that ﬁnd-ing optimally initialized parameters that in-corporate high-resource domain knowledgeand generalizable knowledge is signiﬁcant inhandling a low-resource domain..2 related work.
our study leverages two components from thenatural language processing (nlp) domain: low-resource nmt and meta-learning.
in this section,we discuss previous studies by concentrating onthese two main components..2.1 low-resource neural machine.
translation.
based on the success of attention-based mod-els (luong et al., 2015; vaswani et al., 2017),nmt obtains signiﬁcant improvement in numer-ous language datasets, even showing promisingresults (wu et al.)
in different datasets.
however,the performance of nmt models depends on thesize of the parallel dataset (koehn and knowles,2017).
to address this problem, one conventionalapproach is utilizing monolingual datasets..recent studies point out the difﬁculty of gather-ing parallel data, whereas the monolingual datasetsare relatively easy to collect.
to facilitate mono-lingual corpora, several studies apply dual learn-ing (he et al., 2016), back-translation (sennrichet al., 2016b), and pretraining the model with bilin-gual corpora (hu et al., 2019; wei et al., 2020).
furthermore, as a challenging scenario, recent stud-ies propose the unmt methods without using any.
2889figure 1: an illustration of a high-level training process for both metaumt and metagumt.
in the case ofmetagumt, the training process is divided into two different phases, a meta-train phase and a meta-test phase.
the objective in the meta-train phase is to obtain adapted parameters (i.e., φ) by minimizing a meta-train loss (i.e.,l[dtrn ]) from initial unadapted parameters.
n represents the number of domains; dtr indicates meta-train data.
in the meta-test phase, we optimize initial parameters θ through φ by minimizing the two losses, meta-train andmeta-test losses, i.e., (cid:80) l[dtrother].
dts represents meta-test data; dother is the domain dataother than dn ..n ] and (cid:80) l[dts.
n ; dts.
parallel corpora (lample et al., 2018a; artetxeet al., 2018; yang et al., 2018).
the unmt mod-els show comparable performances by extendingthe back-translation method (conneau et al., 2018)and incorporating methods such as shared bytepair encoding (bpe) (lample et al., 2018b) andcross-lingual representations (conneau and lam-ple, 2019), following those of the supervised nmt.
however, since these approaches require plenty ofmonolingual datasets, they suffer in a low-resourcedomain..transferring the knowledge from high-resourcedomains to a low-resource domain is one alterna-tive way to address this challenge.
a few studiesconcentrate on transferring the knowledge fromthe rich-resource corpora into the low-resourceone.
several models (chu and wang, 2018; huet al., 2019) show better performances than whentrained with the low-resource corpora only.
how-ever, these approaches are applicable in speciﬁcscenarios where one or both of the source and tar-get domains consist of a parallel corpus..to address the issues, we deﬁne a new task asthe unsupervised domain adaptation on the low-resource dataset.
our work is more challengingthan any other previous studies, since we assumethat both the low-resource target domain and thesource domain corpora are monolingual..2.2 meta learning.
given a small amount of training data, most ofthe machine learning models are prone to overﬁt-ting, thus failing to ﬁnd a generalizable solution.
tohandle this issue, meta-learning approaches seekfor how to adapt quickly and accurately to a low-resource task, and show impressive results in var-ious domains (finn et al., 2017; javed and white,.
2019).
the meta-learning approaches aim to ﬁndthe optimal initialization of the model parametersthat adapts the model to a low-resource dataset ina few iterations of training (finn et al., 2017; raviand larochelle, 2016).
owing to the success of themeta learning, recent studies apply the meta learn-ing to low-resource nmt tasks, including multi-lingual nmt (gu et al., 2018) and the domainadaptation (li et al., 2020).
these studies assumethat all the training corpora consist of the paral-lel sentences.
however, a recent work (li et al.,2018) utilizes the meta learning approach to ﬁnd ageneralized model for multiple target tasks.
how-ever, it is not focused on adapting a speciﬁc targettask since its main goal is to handle the target taskwithout using any low-resource data..our study attempts to address the low-resourceunmt by exploiting meta-learning approaches.
moreover, we present two novel losses that encour-age incorporating high-resource knowledge andgeneralizable knowledge into the model parame-ters.
our proposed approaches show signiﬁcantperformance improvements in adapting to a low-resource target domain..3 unsupervised neural machine.
translation.
in this section, we ﬁrst introduce the notation ofthe general unmt models.
we then describe thethree steps for the unmt task: initialization, lan-guage modeling, and back-translation.
on thesethree steps, we illustrate how each step contributesto improving the performance of unmt..notations.
we denote s and t as a source anda target monolingual language dataset.
x and yrepresent the source and the target sentences froms and t .
we assume the nmt model is parame-.
2890meta-traina) metaumt meta-testmeta-trainb) metagumt meta-testfigure 2: overall training process of our proposed metagumt.
(a) a single domain (e.g., law) is ﬁrst chosen tocompute ldiag with model parameters θ in the meta-train phase and ldicd−out(cid:48) with temporary model parameters φiin the meta-test phase.
(b) another domain (e.g., it) is sampled to compute ldicd−other based on φi in the meta-testphase.
(c) temporary model parameters φi is updated from θ to learn the knowledge of high-resource domains.
(d) cross-domain and aggregated meta-train loss functions are computed across all out-domain datasets.
(e) theoptimal initialization θ is obtained by minimizing lag and lcd..terized by θ. we also denote ms→s and mt→t aslanguage models in a source and a target language,respectively, while denoting ms→t and mt→s as themachine translation models from the source to thetarget language and vice versa..initialization.
a recent unmt model (lampleet al., 2018b) is based on a shared encoder and de-coder architecture for the source and the target lan-guage.
due to the shared encoder and decoder foreach language, initializing the model parametersof the shared encoder and decoder is an importantstep for competitive performances (conneau et al.,2018; lample et al., 2018a; artetxe et al., 2018;yang et al., 2018).
conneau and lample (2019)propose the xlm (cross-lingual language model)to initialize parameters, showing signiﬁcantly im-proved performances for unmt.
among variousinitialization methods, we leverage the xlm as ourinitialization method..language modeling.
we use a denoising auto-encoder (vincent et al., 2008) to train the unmtmodel, reconstructing an original sentence froma noisy one in a given language.
the objectivefunction is deﬁned as follows:.
llm =ex∼s[− log ms→s(x|c(x))]+.
ey∼t [− log mt→t(y|c(y))],.
(1).
where c is a noise function described in (lam-ple et al., 2018b), which randomly drops or swapswords in a given sentence.
by reconstructing thesentence from the noisy sentence, the model learnsthe language modeling in each language..back-translation.
back-translation helps themodel learn the mapping functions between thesource and the target language by using only themonolingual sentences.
for example, we samplea sentence x and y from source language s andtarget language t .
to make pseudo-pair sentencesfrom the sampled source sentence, we deduce thetarget sentence from the source sentence, such thaty(cid:48) = ms→t (x), resulting in the pseudo parallelsentence, i.e., (x, y(cid:48)).
similarly, we obtain (x(cid:48), y),where x(cid:48) is the translation of a target sentence, i.e.,mt→s (y).
we do not back-propagate when we gen-erate the pseudo-parallel sentence pairs.
in short,the back-translation objective function is.
lbt =ey∼t [−logms→tex∼s[−logmt→s.
(cid:0)y | x(cid:48)(cid:1)]+(cid:0)x | y(cid:48)(cid:1)]..(2).
4 proposed approach.
this section ﬁrst explains our formulation of a low-resource unsupervised machine translation taskwhere we can apply a meta-learning approach.
afterwards, we elaborate our proposed methods,metaumt and metagumt.
we utilize the meta-learning approach to address a low-resource chal-lenge for unsupervised machine translation.
more-over, we extend metaumt into metagumt toexplicitly incorporate learned knowledge from mul-tiple domains..4.1 problem setup.
finn et al.
(2017) assume multiple different tasks toﬁnd the proper initial parameters that can quicklyadapt to a new task using only a few training.
2891choosing a domain    sampling  another domainenglish sentencesenglish sentencesenglish sentencesenglish sentences meta-test phase meta-train phase(e) update      : (c) obtain        by using one-step gradient :(d) compute each loss :(b)(a)english sentencesenglish sentencesenglish sentencesenglish sentencesenglish sentencesenglish sentencesenglish sentencesenglish sentencesenglish sentencesenglish sentencesout, ..., dn.
examples.
in this paper, we consider tasks inthe meta-learning as domains, where dout ={d0out} represents n out-domain datasets(i.e., source domain datasets), and din indicatesan in-domain dataset (i.e., a target domain dataset),which can be the dataset in an arbitrary domainnot included in dout.
each domain in both doutand din is assumed to be composed of unpairedlanguage corpora, and we create din as a low-resource monolingual dataset 1. to adapt our modelto the low-resource in-domain data, we ﬁnetune theunmt model by minimizing both the losses de-scribed in eqs.
(1) and (2) with din..4.2 metaumt.
in order to obtain an optimal initialization of themodel parameters, allowing the model to quicklyadapt to a new domain with only a small numberof monolingual training data, metaumt uses twotraining phases, the meta-train phase and the meta-test phase.
during the meta-train phase, the modelﬁrst learns domain-speciﬁc knowledge by updatinginitial model parameters θ to temporary model pa-rameters φi, i.e., adapted parameters.
then, in themeta-test phase, the model learns the adaptation byoptimizing θ with respect to φi.
from the domainadaption perspective, two phases simulate the do-main adaption process.
the model ﬁrst adapts to aspeciﬁc domain through the meta-train phase, andthis adaption is evaluated in the meta-test phase..meta-train phase.
we obtain φi for each i-thout-domain dataset by using one-step gradient de-scent, i.e.,.
φi= θ − α∇θlsdi.
(θ),.
out.
where lsdi.
out.
is represented as.
lsdi.
out.
= llmdi.
out.
(θ) + lbtdi.
out.
(θ)..(3).
(4).
diout is the i-th out-domain dataset, and α is thelearning rate for the meta-train phase.
as previ-ously discussed in section 3, the language mod-eling and back-translation losses are essential infacilitating the unsupervised machine translation.
hence, ls consists of llm and lbt, where each lossfunction is computed with di.
out..meta-test phase.
the objective of the meta-testphase is to update θ using each φi learned from the.
meta-train phase by using each lsout(cid:48)this update as a meta-update, deﬁned as.
di.
2. we call.
θ ← θ − β∇θ.
lsdi.
out(cid:48).
(φi),.
(5).
n(cid:88).
i=0.
where β is another learning rate in the meta-testphase.
since eq.
(5) requires the second-order gra-dient, the equation is simpliﬁed with the ﬁrst-ordergradient by replacing the second-order term.
finnet al.
(2017) showed that the ﬁrst-order approxi-mation of the meta-learning maintains the perfor-mance while minimizing the computational cost..4.3 metagumt.
to handle a data scarcity issue from a meta-learningperspective, it is critical to be able to make the ini-tialized model to adapt to a data-scarce domain.
however, since a small amount of training data inthe new domain may cause the model to overﬁt andprevent utilizing high-resource domain knowledge,it is important to incorporate high-resource domainknowledge and generalizable knowledge into themodel parameters.
to address this issue, we extendthe existing meta-learning approach via two novellosses, which we call an aggregated meta-train lossand a cross-domain loss.
the former contributesto incorporating high-resource domain knowledgeinto the model parameters, while the latter encour-ages our model, after trained using a particulardomain, to still generalize well to another domain,i.e., cross-domain generalization..meta-train phase.
as shown in fig.
2 (c), viaeqs.
(3) and (4), we obtain φi from each i-th out-domain datasets.
since this phase is exactly samewith the meta-train phase of metaumt, we leaveout the details..meta-test phase.
the aggregated meta-trainloss, which refers to fig.
2 (d), is computed us-ing all out-domain datasets, i.e.,.
lag =.
lsdi.
out.
(θ)..n(cid:88).
i=0.
(6).
this loss term allows the model to learn the sourcedomain knowledge that is potentially applicable toa target domain.
moreover, to alleviate the overﬁt-ting after adapting to the low-resource domain, we.
1we randomly sample the 5,000 tokens (∼ 300 sentences).
from the in-domain training dataset..and ls.
2ls.
difrom same di..out.
di.
out(cid:48).
indicate different batch sampled data.
2892eub medicalkoran.
gv.
eub subtitlesgv europarl.
eub.
eubgv subtitles medical koran.
gv europarl.
model.
dout.
din.
unadaptedtransfermixedmetaumtmetagumtsupervised nmtunsupervised nmt.
medicalkoran.
lawitsubtitlesde-en en-de7.549.189.9610.5810.892.490.94.
9.4610.9211.7712.9513.452.241.26.epoch-4153285.lawiteuroparlde-en en-de15.8222.3116.7822.9617.0522.9918.5924.5318.9525.131.521.880.761.53.epoch-3522723.lawitmedicalde-en en-de19.2321.3019.7822.7719.9922.9821.8624.622.7925.329.807.712.723.37.epoch-6844119.de-en31.131.6931.6932.5134.2611.296.07.law.
en-de25.3525.5925.7427.2229.3710.074.73.epoch-46321311.table 1: bleu scores on various out-domain (dout) and in-domain (din) combinations for the language pairsof de-en and en-de.
the ”epoch” column indicates the converged number of epochs for each in-domain dataset.
since the unadapted model does not have any additional ﬁnetuning step, we leave the epoch column as blank.
thebold represents the signiﬁcant difference (p < 0.05) with others.
each bleu score represents the average of tentrials..introduce a cross-domain loss, which is in fig.
2(d), as.
lcd =.
lsdicd.
(φi),.
n(cid:88).
i=0.
(7).
di.
di.
out(cid:48).
other.
dicd.
= ls.
(φi) + ls(φi), i.e., com-where lsputing the cross-domain loss with the data fromout(cid:48) as well as those from other domains didiother .
to obtain the optimal initialization θ for modelparameters, we deﬁne our total loss function, whichis fig.
2 (e), as the sum of the two of our losses,i.e.,.
θ ← θ − β∇θ(lcd + lag)..(8).
in summary, our aggregated meta-train and cross-domain losses encourage our model to accuratelyand quickly adapt to an unseen target domain.
theoverall procedure is described in algorithm a.1..5 experiments.
this section ﬁrst introduces experiment settingsand training details.
afterwards, we show empiricalresults in various scenarios..5.1 dataset and preprocessing.
we conduct our experiments on eight differentdomains 3(appendix t.2).
each domain datasetis publicly available on opus 4 (tiedemann,2012).
we utilize the eight domains for out-domain(dout) and in-domain datasets (din).
to build themonolingual corpora of in-domain and out-domaindatasets, we sample data from the parallel corpus.
we made sure to include at most one sentence fromeach pair of parallel sentences.
for instance, wesample the ﬁrst half of the sentences as unpaired.
3acquis (law), emea (medical), it, tanzil (koran), sub-titles, eubookshop (eub), europarl, and globalvoices (gv).
4http://opus.nlpl.eu/.
source data and the other half as truly unpairedtarget data.
consequently, the sampled monolin-gual corpora contain no translated sentence in eachlanguage.
each of the two monolingual corporacontains the equal number of sentences for eachlanguage (e.g., english and german).
for our low-resource scenarios, we sample 5,000 tokens froma selected in-domain corpus for each language.
note that the out-domain dataset represents thefull monolingual corpora..5.2 experimental settings.
as our base model, we use a transformer (vaswaniet al., 2017), which is initialized by a masked lan-guage model from xlm (conneau and lample,2019) using our out-domain datasets.
all the mod-els consist of 6 layers, 1,024 units, and 8 heads..we establish and evaluate various baseline mod-.
els as follows:.
• unmt model is trained with only the in-domain monolingual data, composed of 5,000words for each language..• supervised neural machine translationmodel (nmt) is trained with in-domain par-allel datasets, which we arrange in parallelwith the two in-domain monolingual corpora..• unadapted model is pretrained with only theout-domain datasets and evaluated on the in-domain datasets..• transfer learning model.
is a ﬁnetunedmodel, which is pretrained with the out-domain datasets and then ﬁnetuned with alow-resource in-domain dataset..• mixed ﬁnetuned model (chu et al., 2017)is similar to the transfer learning model, but.
2893figure 3: results of the models that are ﬁrst pretrained on medical, law, eubookshop, koran, it, and globalvoicesdatasets and then ﬁnetuned on a subtitles dataset.
(a) is a performance comparison with respect to the number ofwords for adaptation.
(b) is the number of iterations until the convergence during the ﬁnetuning stage with respectto the number of words.
(c) is the number of iterations until convergence, where the bleu is validating scorescalculated by the average of en-de and de-en..it utilizes both in-domain and out-domaindatasets for ﬁnetuning.
that is, the trainingbatch is sampled evenly from in-domain andout-of-domain datasets..meta-train and the cross-domain losses) help themodel not only to perform well even on the un-seen in-domain dataset but also to accelerate theconvergence speed..5.3 experimental results.
5.4 performances and adaptation speed in.
in order to verify that leveraging the high-resourcedomains (i.e., the source domains) effects to handlethe low-resource domains (i.e., the target domain),we compare the unsupervised and supervised mod-els with ours and other baseline models..as shown in table 1, the unsupervised modeltrained on in-domain data suffers from data scarcitybecause it only uses low-resource in-domain data.
although the unsupervised and supervised mod-els are initialized by xlm, those models show theworst performance in all the cases.
this result in-dicates that when the small size of an in-domaincorpus is given, it is appropriate to utilize the out-domain datasets rather than to train only with low-resource data.
in addition, the performance of theunadapted model is far behind compared to othermodels, such as the mixed ﬁnetuned model, trans-fer learning model, metaumt, and metagumt.
this implies that we need an adequate strategy ofleveraging the high-resource domains to improvethe performance..we further compare the performance betweenour proposed approaches (i.e., metaumt andmetagumt) and the other two ﬁnetuning mod-els (i.e., the transfer learning and the mixed ﬁne-tuned ones).
our methods exhibit the leading per-formances in both directions of translation (en ↔de), and consistently achieve improvements of 2-3 bleu score in most of settings.
furthermore,metagumt consistently obtains better bleuscores and converges faster than metaumt.
weassert that our proposed losses (i.e., the aggregated.
finetuning stage.
as shown in fig.
3 (a), we compare our proposedmethods with the transfer learning approach byvarying the sizes of an in-domain monolingual cor-pus.
the smaller the size of training data is, thewider the performance gap between the two ap-proaches and the transfer learning model becomes.
it means that meta-learning is an effective approachto alleviate the performance degradation, prevent-ing the model from overﬁtting to the low-resourcedata..compared to the transfer.
learning model,metaumt demonstrates a better performancethan other methods in various settings.
however,metagumt exhibits even better performances con-sistently in all settings owing to our proposed losses(eq.
(8)).
the transfer learning approach shows theworst performance except for the unadapted model,even though it exploits the in-domain corpus afterbeing pretrained with the out-domain datasets..additionally, we analyze the number of itera-tions required for a model to converge given anin-domain dataset.
as shown in fig.
3 (b), the meta-learning approaches rapidly converge after only afew iterations, even faster than the transfer learn-ing one does.
as the number of in-domain trainingwords increases, the transfer learning approach re-quires a much larger number of iterations until con-vergence than our meta-learning approaches.
it canbe seen that metaumt and metagumt rapidlyadapt to an unseen domain.
moreover, owing to theencapsulated knowledge from the high-resource do-.
2894(a)(b)(c)                                                                                                                                                   parameter.
d.initial θdout.
meidcal.
law.
koran.
eub.
it.
gv.
transfermixed ﬁnetunedmetaumtmetagumt.
de-en en-de de-en en-de de-en en-de de-en en-de de-en en-de de-en en-de de-en en-de de-en en-de16.5830.9816.92-18.733.019.3137.37.
34.8-27..7442.73.
26.96-23.3931.63.
30.28-15.437.3.
17.4-2.7721.24.
13.72-4.8918.2.
12.32-6.7813.72.
14.25-1.0617.38.
11.59-0.7913.84.
10.01-2.5911.8.
20.98-9.4524.0.
17.74-4.6819.24.
10.9211.7712.9513.45.
9.189.9610.5810.89.
22.3122.8423.9124.44.table 2: bleu scores evaluated on out-domain and in-domain data with initial θ and ﬁnetuned θ, respectively.
”d”denotes the domain, ”unseen” indicates the new domain evaluated with ﬁnetuned θ. since the transfer and mixedﬁnetuned models use the same initial θ, we leave its corresponding row as ”-”..finetuned θ.dinsubtitles.
unseeneuroparl.
mains, metagumt converges within a relativelyearlier iteration than metaumt does..in summary, the meta-learning-based methodsquickly converge in the low-resource domain, im-proving the performances over the transfer learningmethod in various low-resource settings.
this in-dicates that the meta-learning-based approachesare suitable to alleviate the data deﬁciency issue inscarce domains.
furthermore, our losses in eq.
(8)enhance the capabilities of aggregating domain gen-eral knowledge and ﬁnding adequate initialization..5.5 number of iterations until convergence.
in pretraining stage.
an advantage of our meta-learning approaches isthat they can ﬁnd an optimal initialization pointfrom which the model can quickly adapt to a low-resource in-domain dataset.
the transfer learn-ing model requires twice more iterations untilconvergence than ours does.
as shown in fig.
3(c), metaumt and metagumt not only con-verge quickly but also outperform the other base-line methods.
speciﬁcally, compared to metaumt,metagumt is effective in achieving an optimizedinitialization at an earlier iteration.
these resultsindicate that our additional losses (i.e., the cross-domain and aggregated meta-train losses) are ben-eﬁcial in boosting up the ability for ﬁnding an op-timal initialization point when training the modelwith the out-domain datasets..5.6 analysis of metagumt losses.
we assume that the domain generalization abilityand high-resource domain knowledge are helpfulfor the unmt model to translate the low-resourcedomain sentences.
first, to identify whether themodel encapsulates the high-resource knowledgefrom multiple sources, we evaluate our model onout-domain datasets (i.e., dout) with initial θ. asshown in table.
2, metagumt shows remarkableperformances over metaumt in all domains, evenbetter than the transfer learning models.
in otherwords, metaumt demonstrates poor performances.
cross-domain aggregated meta-train de-en en-de average25.8526.06 +0.2126.22 +0.3726.46 +0.61.
27.0927.3727.5427.85.
24.624.7624.9025.06.
(cid:55)(cid:88)(cid:55)(cid:88).
(cid:55)(cid:55)(cid:88)(cid:88).
∆.
table 3: effectiveness of each cross-domain and aggre-gated meta-train loss..in dout, compared to metagumt.
this can beexplained as metagumt uses an aggregated meta-train loss such that metagumt is able to encap-sulate the high-resource domain knowledge.
asshown in table.
1, metagumt achieves superiorperformances, showing that metagumt is capableof leveraging the encapsulated knowledge whenﬁnetuning the low-resource target domain..secondly, our cross-domain loss encouragesthe model to have a generalization capabilityafter adapting to the low-resource target do-main.
as shown in ”unseen” column in table.
2,metagumt outperforms the other models.
it canbe seen that our model has the domain general-ization ability after the ﬁnetuning stage due to thecross-domain loss in the meta-test phase..5.7 performance of unbalanced monolingual.
data in finetuing stage.
in unmt, data unbalancing is often the case inthat source language (e.g., english) data are abun-dant and the target language (e.g., nepali) dataare scarce (kim et al., 2020).
we extend our ex-periment to the unbalanced scenarios to examinewhether our proposed model shows the same ten-dency.
in this scenario, the low-resource target do-main dataset consists of monolingual sentencesfrom one side with two times more tokens than themonolingual sentences from the other.
as shownin table.
4, metagumt outperforms in all unbal-anced data cases.
it shows that metagumt is fea-sible to a practical unmt scenario where the num-ber of sentences is different in the source and targetlanguages.
the only difference against the mainexperiment setting 5.1 is the condition that the in-domain corpus is unbalanced.
we also include the.
2895# tokensdeen10k5k16k8k32k16k64k32k.
mixed.
metagumt.
metaumten-de de-en en-de de-en en-de de-en34.2826.0434.3926.0934.4426.4434.7727.39.
29.4329.6230.1029.83.
28.8027.8427.9228.67.
31.9032.0132.3732.84.
32.6532.9332.9633.52.table 4: results on the unbalanced monolingual lawdomain data during the ﬁnetuning stage, where dout isgv, euorparl, eub, subtitles, medical and koran..# dout.
456.metagumt.
metaumt.
transfer.
mixed.
en-de de-en en-de de-en en-de de-en en-de de-en5.977.5810.89.
7.479.4913.45.
7.228.6811.77.
7.249.0112.95.
5.877.3310.58.
7.178.0810.92.
5.757.179.18.
5.877.209.96.table 5: effectiveness of the different number of sourcedomains between meta-learning based approaches andthe transfer learning approach, where # dout representsthe number of out-domain datasets in the pretrainingstage..result of the transfer learning model in table.
t.4..6 conclusions.
5.8 ablation study.
we empirically show the effectiveness of the cross-domain and aggregated meta-train losses, as shownin table 3 5. first, compared to metaumt whichdoes not use any of the two losses, incorporatingthe cross-domain loss improves the average bleuscore by 0.21. the cross-domain loss acts as a reg-ularization function that prevents the model fromoverﬁtting during the ﬁnetuning stage.
second, theaggregated meta-train loss, another critical compo-nent of our model, allows the model to utilize thehigh-resource domain knowledge in the ﬁnetuningstage.
this also improves the average bleu scoreby 0.37 from metaumt.
lastly, combining bothcross-domain and aggregated meta-train losses sig-niﬁcantly enhances the result in both directions oftranslation (en ↔ de), indicating that they arecomplementary to each other..5.9.impact of the number of source domains.
we examine how the performances change againstthe different number of source domains for eachapproach.
as shown in table.
5 6, metagumtconsistently outperforms the transfer, the mixed-ﬁnetune, and metaumt approaches.
as the sizeof the source domains increases, so does the per-formance gap between ours and the transferringbased models, i.e., transferring and mixed-ﬁnetunemodels.
this indicates that the meta-learning basedapproaches are highly effected by the size of the do-mains in the meta-train phase, and also, if the num-ber of source domains is large enough to capturethe general knowledge, the meta-learning based ap-proaches are suitable to handle the low-resource tar-get task (i.e., machine translation in a low-resourcedomain)..5the models are pretrained on subtitles, law, eub, eu-.
roparl, it, and gv and then ﬁnetuned on the medical data..6the 4 case contains the medical, law, koran and eubdomains.
5 and 6 additionally utilize one more domain(i.e.,it) and two more domains(i.e.,it and gv), respectively..this paper proposes a novel meta-learning ap-proach for low-resource unmt, called metaumt,which leverages multiple source domains to quicklyand effectively adapt the model to the target do-main even with a small amount of training data.
moreover, we introduce an improved method calledmetagumt, which enhances cross-domain gen-eralization and maintains high-resource domainknowledge.
we empirically show that our proposedapproach consistently outperforms the baselinemethods with a nontrivial margin.
we believe thatour proposed methods can be extended to semi-supervised machine translation as well.
in the fu-ture, we will further analyze other languages, suchas uzbek and nepali, instead of languages likeenglish and german..acknowledgments.
this work was partially supported by institute of in-formation & communications technology planning& evaluation (iitp) grant funded by the korea gov-ernment (msit) (no.
2019-0-00075, artiﬁcial in-telligence graduate school program (kaist) andno.
2020-0-00368, a neural-symbolic model forknowledge acquisition and inference techniques)and by the national research foundation of ko-rea (nrf) grant funded by the korean government(msit) (no.
nrf-2019r1a2c4070420).
references.
mikel artetxe, gorka labaka, eneko agirre, andkyunghyun cho.
2018. unsupervised neural ma-chine translation.
proc.
international conferenceon learning representations (iclr).
vancouver,canada..chenhui chu, raj dabre, and sadao kurohashi.
2017.an empirical comparison of domain adaptationmethods for neural machine translation.
in in proc.
the annual meeting of the association for computa-tional linguistics (acl) (volume 2: short papers),pages 385–391, vancouver, canada..2896chenhui chu and rui wang.
2018. a survey ofdomain adaptation for neural machine translation.
in proc.
the international conference on compu-tational linguistics (coling), pages 1304–1319,santa fe, new mexico, usa..alexis conneau and guillaume lample.
2019. cross-in proc.
ad-lingual language model pretraining.
vances in neural information processing systems(neurips), pages 7057–7067, vancouver, canada..alexis conneau, guillaume lample, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2018.in proc.
word translation without parallel data.
international conference on learning representa-tions (iclr).
vancouver, canada..zi-yi dou, keyi yu, and antonios anastasopoulos.
2019.investigating meta-learning algorithms forlow-resource natural language understanding tasks.
in proc.
the conference on empirical methods innatural language processing and the internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1192–1197, hong kong,china..chelsea finn, pieter abbeel, and sergey levine.
2017.model-agnostic meta-learning for fast adaptation ofin proc.
the international confer-deep networks.
ence on machine learning (icml), volume 70, page1126–1135..markus freitag and yaser al-onaizan.
2016. fastdomain adaptation for neural machine translation.
corr, abs/1612.06897..jiatao gu, yong wang, yun chen, victor o. k. li,and kyunghyun cho.
2018. meta-learning forin proc.
low-resource neural machine translation.
the conference on empirical methods in naturallanguage processing (emnlp), pages 3622–3631,brussels, belgium..di he, yingce xia, tao qin, liwei wang, nenghai yu,tie-yan liu, and wei-ying ma.
2016. dual learn-in proc.
advances ining for machine translation.
neural information processing systems (nips), vol-ume 29..junjie hu, mengzhou xia, graham neubig, and jaimecarbonell.
2019. domain adaptation of neural ma-in proc.
chine translation by lexicon induction.
the annual meeting of the association for compu-tational linguistics (acl), pages 2989–3001, flo-rence, italy..khurram javed and martha white.
2019. meta-learning representations for continual learning.
inproc.
advances in neural information processingsystems (neurips), volume 32, vancouver, canada..yunsu kim, miguel grac¸a, and hermann ney.
2020.when and why is unsupervised neural machine trans-lation useless?
in proc.
the annual conference ofthe european association for machine translation(eacl), pages 35–44, lisboa, portugal..diederik p kingma and jimmy ba.
adam: a methodfor stochastic optimization.
corr, abs/1412.6980..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproc.
the annual meeting of the association forcomputational linguistics companion volume pro-ceedings of the demo and poster sessions, pages177–180, prague, czech republic..philipp koehn and rebecca knowles.
2017. six chal-in proc.
oflenges for neural machine translation.
the first workshop on neural machine translation(wmt), pages 28–39, vancouver, canada..guillaume lample, alexis conneau, ludovic denoyer,and marc’aurelio ranzato.
2018a.
unsupervisedmachine translation using monolingual corpora only.
international conference on learning representa-tions.
vancouver, canada..guillaume lample, myle ott, alexis conneau, lu-dovic denoyer, and marc’aurelio ranzato.
2018b.
phrase-based & neural unsupervised machine trans-lation.
in proc.
the conference on empirical meth-ods in natural language processing, pages 5039–5049, brussels, belgium..da li, yongxin yang, yi-zhe song, and timothyhospedales.
2018. learning to generalize: meta-in proc.thelearning for domain generalization.
aaai conference on artiﬁcial intelligence (aaai),volume 32..rumeng li, xun wang, and hong yu.
2020. metamt,a meta learning method leveraging multiple do-main data for low resource machine translation.
inproc.
the aaai conference on artiﬁcial intelligence(aaai), volume 34, pages 8245–8252..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedneural machine translation.
in proc.
the conferenceon empirical methods in natural language process-ing (emnlp), pages 1412–1421, lisbon, portugal..kun qian and zhou yu.
2019. domain adaptive dialogin proc.
the annualgeneration via meta learning.
meeting of the association for computational lin-guistics (acl), pages 2639–2649, florence, italy..sachin ravi and hugo larochelle.
2016. optimiza-tion as a model for few-shot learning.
proc.
inter-national conenference on learning representations(iclr).
vancouver, canada..shuo ren, yu wu, shujie liu, ming zhou, and shuaima.
2019. explicit cross-lingual pre-training for un-supervised machine translation.
pages 770–779..2897rico sennrich, barry haddow, and alexandra birch.
2016a.
improving neural machine translation mod-in proc.
the annualels with monolingual data.
meeting of the association for computational lin-guistics (acl), pages 86–96, berlin, germany..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordsin proc.
the annual meet-with subword units.
ing of the association for computational linguistics(acl), pages 1715–1725, berlin, germany..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
inthe international conference on machineproc.
learning (icml), volume 97, pages 5926–5936..j¨org tiedemann.
2012. parallel data, tools and inter-in proc.
the international con-faces in opus.
ference on language resources and evaluation(lrec), pages 2214–2218, istanbul, turkey..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proc.
advances in neural informationprocessing systems (nips), pages 5998–6008, longbeach, ca, usa..pascal vincent, hugo larochelle, yoshua bengio, andpierre-antoine manzagol.
2008. extracting andcomposing robust features with denoising autoen-in proc.
of the international conferencecoders.
on machine learning (icml), pages 1096–1103,helsinki, finland..hao-ran wei, zhirui zhang, boxing chen, and wei-hua luo.
2020.iterative domain-repaired back-in proc.
the conference on empiricaltranslation.
methods in natural language processing (emnlp),pages 5884–5893, online.
association for computa-tional linguistics..yonghui wu, mike schuster, zhifeng chen, quoc v le,mohammad norouzi, wolfgang macherey, maximkrikun, yuan cao, qin gao, klaus macherey, et al.
google’s neural machine translation system: bridg-ing the gap between human and machine translation.
corr, abs/1609.08144..zhen yang, wei chen, feng wang, and bo xu.
2018. unsupervised neural machine translation withweight sharing.
in proc.
the annual meeting of theassociation for computational linguistics (acl),pages 46–55, melbourne, australia..jiali zeng, yang liu, jinsong su, yubing ge, yaojieiterativelu, yongjing yin, and jiebo luo.
2019.dual domain adaptation for neural machine transla-in proc the conference on empirical meth-tion.
ods in natural language processing and the inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 845–855, hongkong, china..2898a implementation details.
in orderto preprocess datasets, we utilizemoses (koehn et al., 2007) to tokenize the sen-tences.
we then use byte-pair encoding (bpe) (sen-nrich et al., 2016a) to build a shared sub-word vo-cabulary using fastbpe7 with 60,000 bpe codes.
based on this shared sub-word vocabulary, con-structed from the out-domain datasets, we splitwords into sub-word units for the in-domain dataset.
we implement all of the models using pytorch li-brary 8, and then train them in four nvidia v100gpus for pretraining and ﬁnetuning.
we evaluate allthe experiments based on the bleu script 9. thenumber of convergence iteration of each algorithmis deﬁned based on the best validation epoch, whichshows no more improvement on validation scoreafter we run 10 more epochs.
moreover, we haveconducted comprehensive experiments to obtainour main result table (table.
1 and table.
t.1 ) ondifferent domains by training the model with 10different sampled words each time..for optimizing each algorithms, we choose theadam optimizer (kingma and ba) for pretrain-ing stage, as well as the adam warmup opti-mizer (vaswani et al., 2017) for ﬁnetuning stage.
the learning rate is set to 10−4, optimized withinthe range of 10−2 to 10−5.
in all experiments, thenumber of tokens per batch is set as 1,120 andthe dropout rate is set as 0.1. in meta-learning ap-proaches, we set the learning rates of alpha andbeta commonly as 0.0001 in all experiments..in the pretraining stage, we follow the same stop-ping criterion as gu et al.
(2018).
for instance,among different target domains, we randomly se-lect one as a validation domain.
we utilize earlystopping, i.e., stopping training if the validationbleu score does not increase within the ten sub-sequent epochs.
similarly in the ﬁnetuning stage,we apply early stopping using a validation datasetfrom the target domain..b additional results on different.
domain combinations.
since the combination of dout and din can consistdifferently, this section provides additional results.
as shown in table.
t.1, our proposed approaches.
7https://github.com/glample/fastbpe8https://pytorch.org/9https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl.
figure c.1: a performance comparison with respect tothe number of words for adaptation on a law domain..figure c.2: number of iterations until the convergenceduring the ﬁnetuining stage with respect to the numberof words on a law domain..still signiﬁcantly outperform other baseline modelsin different domain combination settings..c perfomances and adaptation speed infinetuning stage for a law domain.
as shown in fig c.1 and fig c.2, metagumt con-sistently outperforms other methods even thoughthe number words are increasing.
through this ex-periment, we attempt to show the robustness of ourmethods (i.e., metaumt and metagumt) againstothers (i.e., transferring and mixed-ﬁnetune mod-els).
the models are pretrained on subtitles, eu-bookshop, europarl, globalvoices, medical, andkoran datasets and then ﬁnetuned on a law dataset..d comparison between metagumt and.
metaumt algorithms.
as shown in algorithms.
a.1, we provide an over-all algorithm of metagumt.
the only difference.
2899                                                                                                                                                                                 model.
dout.
din.
unadaptedtransfermixedmetaumtmetagumtsupervised nmtunsupervised nmt.
medicalsubtitles.
laweub europarlit.
koran medicalsubtitles.
lawkoraneub europarlgv.
de-en en-de14.8918.6216.3519.8016.4919.7518.0521.0818.4221.373.333.480.861.83.epoch-47431522.de-en en-de16.6519.2716.9019.9916.9520.0318.9122.3619.2422.760.850.970.180.51.epoch-35321420.gv europarl.
eubsubtitles medical koraniten-de15.3016.1316.1817.0617.743.590.55.de-en16.1019.3119.3920.520.743.530.51.epoch-5844107.table t.1: extended results on various domain settings.
the column ‘epoch’ indicates the converged number ofepochs for each in-domain dataset.
since the unadapted model does not involve an additional ﬁnetuning step, weleave the epoch column as blank..algorithm a.1 metagumtrequire: α, β: step sizes.
5:.
4:.
out.
(θ) with respecttolanguage sentences.
1: pretrain θ by using xlm2: while not done dofor all diout do3:evaluate ∇θllmdioutsource and targetfrom diback-translation generates source andtarget language sentences using the cur-rent translation modelevaluate ∇θlbtdigenerated sentencessum each gradient:∇θls= ∇θllm(θ)didioutcompute adapted parameters with one-step gradient descent:φi = θ − α∇θlsdi.
(θ) with using pseudo-.
(θ) + ∇θlbtdi.
(θ).
out.
out.
out.
6:.
7:.
8:.
out.
9:.
end forupdate θ ← θ − β∇θ(lcd + lag).
10:11: end while.
between metaumt and metagumt is the meta-test phase in line 10. while metaumt computesthe loss using eq.
(5), metagumt utilizes eq.
(8)..e performance of semi-superivsed.
machine translation in finetuningstage.
proposed.
thealgorithms, metaumt andmetagumt, show promising results on low-resource monolingual data.
however, some mayargue that creating parallel sentences from a smallnumber of unpaired monolingual sentences (e.g.,5k tokens) is also feasible.
hence, we additionally.
corpus.
acquis (law)emea (medical)ittanzil (koran)subtitleseubookshop (eub)europarlglobalvoices (gv).
words.
de8m6.3m1m.
en9.2m7.5m1.7m5.6m 5.3ms92.7m 87.6m115.4m 100m27.3m 25.7m0.6m0.6m.
sentences.
w/s.
0.7m1.1m0.3m0.5m22.5m9.3m1.9m0.05m.
en12.936.819.0810.664.1112.3713.9910.67.de11.305.755.3210.083.8910.7213.1810.88.table t.2: statistics of each corpora..conduct an experiment of semi-supervised machinetranslation in the ﬁnetuning stage.
for instance, wefollow the same pretraining stage, but we utilizeboth monolingual and parallel sentences whileﬁnetuning the model on a low-resource domain.
the number of tokens for each monolingual andparallel data is 5k.
to ﬁnetune the model in thesemi-supervised setting, we compute the loss assum of lct and lbt, where lct is the conventionaltranslation loss in the supervised nmt, i.e.,.
lct =e(x,y)∼p [−logms→t (y | x)]+e(x,y)∼p [−logmt→s (x | y)]..(9).
as shown in table t.3, we observe thatmetagumt demonstrates the promising perfor-mance against others, even if we only utilize themonolingual out-domain datasets to pretrain themodel..f statistics of datasets.
as shown in table.
t.2, we present the overall num-ber of sentences and words for each domain, wherew/s indicates the number of words per sentence ina domain..2900# tokens.
transfer.
metaumt.
metagumt.
parallel monolingual de-en en-de de-en de-en en-de de-en35.905k36.258k37.0516k38.5832k.
28.0428.8629.6230.55.
31.2131.7832.5134.60.
30.3130.5131.4933.25.
32.7433.1433.8835.35.
34.3135.2236.6237.25.
5k8k16k32k.
table t.3: results of semi-supervised machine translation in the ﬁnetuning stage.
“# tokens” indicates the numberof tokens for both monolingual and parallel datasets.
each model is ﬁrst pretrained on medical, law, eubookshop,koran, it, and globalvoices and then ﬁnetuned to the low-resource domain (i.e., law)..# tokensdeen10k5k16k8k32k16k64k32k.
transfer.
mixed.
metaumt.
metagumt.
de-en en-de en-de de-en en-de de-en en-de de-en34.2825.8434.3925.9534.4425.9634.7727.34.
32.6532.9332.9633.52.
31.6731.8332.0332.64.
28.8027.8427.9228.67.
26.0426.0926.4427.39.
31.9032.0132.3732.84.
29.4329.6230.1029.83.table t.4: results on unbalanced monolingual data.
this is the same results of table 4 but included the additionalbaseline model, transfer..2901