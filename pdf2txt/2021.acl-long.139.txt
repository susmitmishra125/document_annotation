a systematic investigation of kb-text embedding alignment at scale.
vardaan pahuja1, yu gu1, wenhu chen2, mehdi bahrami3,lei liu3, wei-peng chen3, yu su1.
1the ohio state university.
2university of california, santa barbara.
3fujitsu laboratories of america.
{pahuja.9, gu.826, su.809}@osu.edu,.
wenhuchen@cs.ucsb.edu.
{mbahrami, lliu, wchen}@fujitsu.com.
abstract.
knowledge bases (kbs) and text often con-tain complementary knowledge: kbs storestructured knowledge that can support long-range reasoning, while text stores more com-prehensive and timely knowledge in an un-structured way.
separately embedding the in-dividual knowledge sources into vector spaceshas demonstrated tremendous successes in en-coding the respective knowledge, but how tojointly embed and reason with both knowledgesources to fully leverage the complementaryinformation is still largely an open problem.
we conduct a large-scale, systematic investiga-tion of aligning kb and text embeddings forjoint reasoning.
we set up a novel evaluationframework with two evaluation tasks, few-shotlink prediction and analogical reasoning, andevaluate an array of kb-text embedding align-ment methods.
we also demonstrate how suchalignment can infuse textual information intokb embeddings for more accurate link pre-diction on emerging entities and events, usingcovid-19 as a case study.1.
1.introduction.
recent years have witnessed a rapid growth ofknowledge bases (kbs) such as freebase (bol-lacker et al., 2007), dbpedia (auer et al., 2007),yago (suchanek et al., 2007) and wikidata(vrandeˇci´c and kr¨otzsch, 2014).
these kbs storefacts about real-world entities (e.g.
people, places,and things) in the form of rdf triples, i.e.
(sub-ject, predicate, object).
today’s kbs are massive inscale.
for instance, freebase contains over 45 mil-lion entities and 3 billion facts involving a large va-riety of relations.
such large-scale multi-relationalknowledge provides a great potential for improv-ing a wide range of tasks, from information re-trieval (castells et al., 2007; shen et al., 2015),.
1code and data are available at https://github..com/dki-lab/joint-kb-text-embedding..figure 1: kbs and text are complementary and embed-ding alignment could help injecting information fromone source to the other and vice versa.
dashed line ismissing link in the kb..question answering (yao and van durme, 2014; yuet al., 2017) to biological data mining (zheng et al.,2020b)..kb embedding models (bordes et al., 2013;dong et al., 2014; lin et al., 2015) embed enti-ties and relations into vector space(s) such thatthe embeddings capture the symbolic knowledgepresent in the kb.
similarly, word embedding mod-els (mikolov et al., 2013b; pennington et al., 2014)learn continuous vector representations that capturethe distributional semantics of words.
experimentson analogical reasoning (mikolov et al., 2013b;gladkova et al., 2016) and multilingual word em-bedding alignment (mikolov et al., 2013a) haveshown that there exists a linear structure in theword embedding space encoding relational infor-mation.
on the other hand, translation-based kbembedding models (bordes et al., 2013; lin et al.,2015; ji et al., 2015), by construction, also presenta linear structure in their embedding space..a natural question then is, can we align thetwo embedding spaces such that they mutually en-hance each other?
such alignment could poten-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1764–1774august1–6,2021.©2021associationforcomputationallinguistics1764article: united nations secretariatheadquartered in new york, the secretariat functions through duty stations in addis ababa, bangkok, beirut, geneva, nairobi, santiago and vienna, in addition to ofﬁces all over the worldunited nationssecretariatunited nations systemunited nationsnew yorkpart ofparentorganizationheadquarterslocationtially inject structured knowledge from kbs intotext embeddings and inject unstructured but moretimely-updated knowledge from text into kb em-beddings, leading to more universal and compre-hensive embeddings (figure 1).
several studieshave attempted at this.
lao et al.
(2012) use thepath-ranking algorithm (lao and cohen, 2010)on combined text and kb to improve binary rela-tion prediction.
gardner et al.
(2014) leverage textdata to enhance kb inference and help address theincompleteness of kbs.
toutanova et al.
(2015)augment the kb with facts and relations from thetext corpus and learn joint embedding for entities,kb relations and textual relations.
enhancement ofkb entity embeddings using using entity descrip-tions has been attempted in (zhong et al., 2015;xie et al., 2016).
wang et al.
(2014) propose tojointly embed entities and words in the same vectorspace.
the alignment of embeddings of words andentities is accomplished using wikipedia anchorsor entity names..however, existing studies are still ad-hoc and amore systematic investigation of kb-text embed-ding alignment is needed to answer an array ofimportant open questions: what is the best way toalign the kb and text embedding spaces?
to whatdegree can such alignment inject information fromone source to another?
how to balance the align-ment loss with the original embedding losses?
inthis work, we conduct a systematic investigation ofkb-text embedding alignment at scale and seek toanswer these questions.
our investigation uses thelatest version of the full wikidata (vrandeˇci´c andkr¨otzsch, 2014) as the kb, the full wikipedia asthe text corpus, and the shared entities as anchorsfor alignment.
we deﬁne two tasks, few-shot linkprediction and analogical reasoning, to evaluatethe effectiveness of injecting text information intokb embeddings and injecting kb information intotext embeddings, respectively, based on which weevaluate and compare an array of embedding align-ment methods.
the results and discussion presentnew insights about this important problem.
finally,using covid-19 as a case study, we also demon-strate that such alignment can effectively inject textinformation into kb embeddings to complete kbson emerging entities and events..in summary, our contributions are three-fold:.
1. we conduct the ﬁrst systematic investigationon kb-text embedding alignment at scale andpropose and compare multiple effective align-.
ment methods..2. we set up a novel evaluation framework withtwo evaluation tasks, few-shot link predictionand analogical reasoning, to facilitate futureresearch on this important problem..3. we have also learned joint kb-text embed-dings on the largest-scale data to date and willrelease the embeddings as a valuable resourceto the community..2 related work.
kb-kb embedding alignment.
most existingknowledge bases are incomplete.
learning of dis-tributed representations for entities and relationsin knowledge bases ﬁnds application in the task oflink prediction i.e.
to infer missing facts in the kbgiven the known facts.
this includes translation-based models (bordes et al., 2013; lin et al., 2015;ji et al., 2015), feed-forward neural network basedapproaches (socher et al., 2013; dong et al., 2014),convolutional neural networks (dettmers et al.,2018; nguyen et al., 2018) and models that lever-age graph neural networks (schlichtkrull et al.,2018; shang et al., 2019; nathani et al., 2019).
recently, many research works have focused onthe alignment of embedding spaces of heteroge-neous data sources such as different kbs.
je (haoet al., 2016) introduces a projection matrix to alignthe embedding spaces of different kbs.
mtranse(chen et al., 2017) ﬁrst learns the embeddings ofentities and relations in each language indepen-dently and then learns the transformation betweenthese embedding spaces.
wang et al.
(2018) usegraph convolutional networks and a set of pre-aligned entities to learn embeddings of entities inmultilingual kbs in a uniﬁed vector space.
in thepresent work, we focus on aligning the kb andtextual embedding spaces.
kb-text joint representation.
many recent ap-proaches have attempted to learn the embeddingsof words and knowledge base entities in the samevector space.
wang et al.
(2014) propose an align-ment technique for kb and text representationsusing entity names and/or anchors.
wikipedia2vec(yamada et al., 2016) extends the skip-gram basedmodel by modeling entity-entity co-occurrencesusing a link graph and word-entity co-occurrencesusing kb anchors.
however, an entity mentioncan be ambiguous i.e.
it can refer to different en-tities in different contexts.
to resolve this, cao.
1765et al.
(2017) propose multi-prototype entity men-tion embedding model to learn representations fordifferent senses of entity mentions.
it includes amention sense embedding model which uses con-text words and a set of reference entities to predictthe actual entity referred to by the mention.
despitethis progress, a comprehensive investigation of themerits of different alignment approaches is missing.
our work takes a step forward in this direction andproposes a novel evaluation framework to comparemultiple alignment approaches for kb-text jointembedding on a large-scale kb and textual corpus..3 model.
in this section, we describe the four alignmentmethods used in our study.
at ﬁrst, we describe thecomponent models used in all alignment methods -the kb embedding model and the skip-gram model..3.1 knowledge base embedding model.
we use the transe model (bordes et al., 2013) tolearn the kb embeddings.
we use the loss functionproposed in sun et al.
(2019) as our kb embeddingobjective..lkb =(cid:88).
(h,r,t)∈s∪s(cid:48).
log(1 + exp(y ∗ (−γ + dr(h, t)))).
here, dr(h, t) = (cid:107)h + r − t(cid:107)2 denotes the scorefunction for the triple (h, r, t), s denotes the set ofpositive triples and s (cid:48)denotes the set of corruptedtriples obtained by replacing the head or tail of apositive triple with a random entity.
γ is a hyper-parameter which denotes the margin and y denotesthe label (+1 for positive triple and -1 for negativetriple)..3.2 skip-gram model.
the skip-gram model learns the embeddings ofwords and entities by modeling the word-word,word-entity and entity-entity co-occurrences.
weuse the skip-gram model proposed in yamada et al.
(2016) for learning the word and entity representa-tions.
let w and e denote the set of all words andentities in the vocabulary respectively and c denotethe size of the context window..• word-word co-occurrence model: theskip-gram model is trained to predict the tar-get word given a context word.
given a se-.
quence of n words w1, w2, · · · , wn , the skip-gram model maximizes the following objec-tive:.
n(cid:88).
(cid:88).
lww =.
n=1.
−c≤j≤c;j(cid:54)=0.
log p (wn+j|wn).
where p(wo|wi ) =.
t.(cid:48)exp(vwiw∈w exp(v(cid:48)wi.
vwo )t.(cid:80).
vw).
.
here,.
v(cid:48)w and vw denote the input and output repre-sentations of the word w respectively.
theinput representations are used as the ﬁnal rep-resentations for both words and entities..• word-entity co-occurrence model: in theword-entity co-occurrence model, the modelis trained to predict the context words of anentity pointed to by the target anchor.
thetraining objective corresponding to the word-entity co-occurrences is.
lwe =.
(cid:88).
(cid:88).
log p(wo|ei).
(ei,cei )∈a.
wo∈cei.
here, a denotes the set of anchors in the cor-pus.
each anchor consists of an entity ei andits context words (represented by cei).
theconditional probability p(wo|ei) is given by:.
p(wo|ei) =.
t.vwo ).
exp(v(cid:48)eiw∈w exp(v(cid:48)ei.
t vw).
(cid:80).
• entity-entity co-occurrence model: theentity-entity co-occurrence model learns topredict incoming links of an entity (denotedby ce) given an entity e..(cid:88).
(cid:88).
lee =.
log p(eo|ei).
ei∈e.
eo∈cei ;ei(cid:54)=eo.
p(eo|ei) =.
veo ).
t.exp(v(cid:48)eie∈e exp(v(cid:48)ei.
t ve).
(cid:80).
in practice, the probabilities involved in the skip-gram model are estimated using negative sampling(mikolov et al., 2013b).
the overall objective isthe sum of the three objectives for each type ofco-occurrence..lsg = lww + lwe + lee.
1766figure 2: schematic representation of different alignment methods.
3.3 alignment methods.
we align the entity pairs in kb and text corpususing a set of seed entity pairs, which are obtainedfrom a mapping between wikidata and wikipedia.
this mapping is constructed from the metadata as-sociated with the wikidata entities.
the set of enti-ties present in the transe model and the skip-grammodel is denoted by et e and esg respectively..(a) alignment using same embedding: in thisapproach, we use the same embedding forthe shared entities in the kb and text corpus.
there is no separate alignment loss for thismethod..(b) alignment using projection:.
inspired bythe multilingual word embedding approaches(mikolov et al., 2013a; faruqui and dyer,2014) which use a linear transformation tomap word embeddings from one space to an-other, we use an afﬁne transformation fromthe skip-gram vector space to the transe vec-tor space to align the entity representations..the alignment loss is calculated as a squaredl2 norm between the transformed skip-gramentity embeddings and the correspondingtranse entity embeddings.
the vectors et eand esg denote the transe and skip-gram.
versions of embeddings of the entity e respec-tively..lalign =(cid:88).
e∈esg∩et e.(cid:107)(w esg + b) − et e(cid:107)22.
(c) alignment using entity names:.
in thisalignment technique inspired by wang et al.
(2014), for a particular triple (h, r, t) in thekb, if an equivalent entity eh exists in the textcorpus, we add an additional triple (eh, r, t)to the kb.
similarly, if an equivalent entity etalso exists for the entity t, we add the triples(h, r, et) and (eh, r, et) to the kb.
the term“name graph” is used to denote this subgraphof additional triples..lalign =(cid:88).
1[h∈esg∧t∈esg]dr(wh, wt)+.
(h,r,t) ∈ kb1[t∈esg]dr(h, wt) + 1[h∈esg]dr(wh, t).
(d) alignment using wikipedia anchors thisalignment technique is motivated by a simi-lar technique proposed in wang et al.
(2014).
here, we introduce an alignment loss term inwhich for word-entity co-occurrences, we sub-stitute the textual entity embedding by its kb.
1767knowledge basebarack obamahonoluluborn_inkb entity nodetextual entity nodetie embedding weights(a) alignment using same embeddinghonolulul2affinetransform(b) alignment using projectionhonolulu[barack obama] was born in [honolulu].textual corpus[entity mention]:  wikipedia anchorborn_inborn_inborn_in(c) alignment using entity namesnew edges in train graphoriginal entity-word co-occurrencesentity-word co-occurrences for alignment(                  , was)(                  , born)(                  , in)(d) alignment using wikipedia anchorshonoluluhonolulubarack obamabarack obamahonoluluhonolulubarack obamahonolulubarack obamabarack obamahonoluluhonoluluhonolulu(                  , was)(                  , born)(                  , in)honoluluhonoluluhonolulualignment losscounterpart in the skip-gram objective.
leteite denote the embedding of the kb entityequivalent to the textual entity ei..lalign =(cid:88).
(ei,cei )∈a.
wo∈cei.
k(cid:88).
i=1.
(cid:88).
log σ(exp(eite.
t.vwo ))+.
ewi∼pn(w)[log σ(−exp(eite.
vwi))].
t.here, pn(w) denotes the noise distributionover words and k is the number of negativesamples..the ﬁnal objective for training these models be-.
comes.
l = lkb + lsg + λlalign.
here, λ denotes the balance parameter which con-trols the extent of inﬂuence of alignment on theembeddings of each of the individual vector spaces.
an illustration of the different alignment methodsused in our study is given in figure 2..4 dataset.
we use wikipedia as the text corpus and wikidata(vrandeˇci´c and kr¨otzsch, 2014) as the knowledgebase.
we use the wikidata version dated 16 de-cember 2020 and the wikipedia version dated 3 de-cember 2020 for all of our experiments.
the termsupport set (as used in the subsequent sections),denoted by s, is used to refer to the intersection setof wikidata entities and entities in wikipedia forwhich an article is present.
dataset preprocessing.
we pre-process the orig-inal set of wikidata triples and ﬁlter out entitiesand relations with frequency less than 10 and 5respectively.
this results in a kb with 14.64 mentities, 1222 relations, and 261 m facts.
similarly,we preprocess wikipedia and ﬁlter out words fromthe vocabulary with frequency less than 10. how-ever, we utilize the entire entity set of wikipediato maximize the size of the support set.
after pro-cessing, the wikipedia vocabulary consists of 2.1m words and 12.3 m entities..5 experiments.
5.1 experimental setup.
we compare the performance of different alignmentmethods using two evaluation tasks - few-shot link.
prediction and analogical reasoning.
the few-shot link prediction task is designed to test thecapability of the alignment model to inject the re-lational information present in text into the knowl-edge base embeddings.
the train-test set for thistask is constructed such that the test set containstriples corresponding to a subset of entities in thesupport set, but each of these entities is observedonly once in the training triples set.
thus, themodel is tasked to do link prediction on entitiesthat occur rarely in the training set (hence the term“few-shot”).
the training and test sets consist of260.1 m and 110.8 k triples respectively.
for thissetting, both entities of each triple in the test set arecontained in the support set..the purpose of the analogical reasoning task is totest the information ﬂow from the knowledge-baseembeddings to the skip-gram embeddings.
thistask was ﬁrst proposed in mikolov et al.
(2013b) totest the syntactic and semantic information presentin learned word embeddings.
we choose the top50 relations from the set of one-to-one and many-to-one relations based on the frequency of occur-rence and construct a dataset of 1000 analogicalreasoning examples for each relation.
the 1st pairof entities is randomly chosen from the trainingtriples set, as the pair of entities involved in thatrelation.
the 2nd pair of entities is obtained fromthe test triples set.
more formally, given a pairof entities (h1, t1) and the head entity of the 2ndpair (h2), the task is to predict the tail entity (t2)of the 2nd pair by comparing the cosine similaritybetween the embedding of candidate entity (et2)and (eh2 + et1 − eh1)..evaluation protocol.
for link prediction evalu-ation on a given test triple (h, r, t), we corrupteither the head entity (by generating triplets like(h(cid:48), r, t)) or the tail entity (by generating tripletslike (h, r, t(cid:48))) of the triple and then rank the scoreof correct entity amongst all entities in the candi-date set.
due to the extremely large entity vocab-ulary size in wikidata, we restrict the size of thecandidate set to a sample of 1000 entities whosetypes lie in the set of permissible domain/rangetypes for that relation (lerer et al., 2019; krompaßet al., 2015).
in cases where the number of suchentities is less than 1000, we choose the entire setof those entities.
in addition, we ﬁlter any positivetriplets (triplets that exist in the kb) from the setof negative triplets for this evaluation, also knownas ﬁltered evaluation setting.
we report results on.
1768standard evaluation metrics - mean rank (mr),hits@1, and hits@10. for this task, we comparethe transe model and the kb-side embeddings ofdifferent alignment methods..for the analogical reasoning task, we reportmean rank (mr), hits@1, and hits@10 by rank-ing the correct entity t2 against the entities in thecandidate set.
the candidate set for the tail entityt2 is a set of 1k entities sampled from the supportset (excluding h1, h2 and t1) according to the nodedegree.
all reported metrics are macro-averagedover the results for different relations.
here, wecompare the skip-gram model embeddings with thetextual embeddings obtained from different align-ment methods..5.2.implementation.
the scale of the training data (both the wikidataknowledge base and the wikipedia corpus) is huge,so the efﬁcient implementation of the model is akey challenge.
for efﬁcient implementation of thetranse model, we used the dgl-ke (zheng et al.,2020a) library.
it uses graph partitioning to trainacross multiple partitions of the knowledge basein parallel and incorporates engineering optimiza-tions like efﬁcient negative sampling to reduce thetraining time by orders of magnitude compared tonaive implementations.
the skip-gram model isimplemented using pytorch (paszke et al., 2019)and wikipedia2vec (yamada et al., 2020) libraries.
for training, we optimize the parameters of thetranse and skip-gram models alternately in eachepoch.
we use the adagrad (duchi et al., 2011)optimizer for the kbe model and sgd for theskip-gram model.
for both models, the training isdone by multiple processes asynchronously usingthe hogwild (niu et al., 2011) approach.
this in-troduces additional challenges like synchronizingthe weights of parameters among different trainingprocesses.
we choose the values of balance param-eter for each of the two evaluation tasks based onthe performance of aligned kb and textual embed-dings on a small set of analogy examples (disjointfrom the analogy test set used in the main evalu-ation).
our implementation can serve as a goodresource to do a similar large-scale analysis of kb-text alignment approaches in the future..5.3 overall results.
the overall results for the two evaluation tasks aregiven in table 1. for the few-shot link predic-tion task, we observe that all the alignment tech-.
niques lead to improved performance of the kbembeddings over the naive transe baseline.
thesame embedding alignment approach performs thebest followed by entity name alignment, projec-tion, and alignment using wikipedia anchors.
theuse of the same embeddings for the shared entitieshelps in propagating the factual knowledge presentin the text to the kb more efﬁciently, so the sameembedding alignment performs better than others.
the entity name alignment approach is worse thanthe same embedding alignment approach since thetest set entities occur less often in the train set (asthe dataset is few-shot).
so, the name graph doesn’tmake a substantial difference here..for the analogical reasoning task, the resultsshow that all alignment approaches obtain an im-provement over the naive skip-gram baseline.
theentity name alignment approach performs the bestfollowed by projection, same embedding align-ment, and alignment using wikipedia anchors.
the good performance of the entity name align-ment approach could be explained by the fact thatfor every test analogy example (eh1, et1, eh2, et2),there is a relation r present between the entitypairs (eh1, et1) and (eh2, et2), although that is un-observed.
since eh and et also occur in the kb,due to the extra added triples, the kb reasoningprocess incorporates the relation r in these embed-dings, just like it does for kb entities h and t. theother approaches viz.
same embedding alignment,projection, and wikipedia anchors don’t have amechanism for explicit kb reasoning like the en-tity name alignment approach.
the projectiontechnique outperforms the same embedding align-ment as the embeddings in the two spaces are lesstightly coupled in the former, so it can take advan-tage of the complementary relational informationin textual as well as the kb embeddings..5.4 fine-grained analysis.
in this section, we present a ﬁne-grained analy-sis of the efﬁcacy of the alignment methods w.r.t.
changes in training data size and whether the testset entities belong to the support set.
we alsostudy the impact of balance parameter on the per-formance of the two evaluation tasks.
due to re-source constraint, we do this analysis on two repre-sentative methods of different nature - projectionalignment and same embedding alignment.
effect of training data size.
to study and differ-entiate the impact of entities present in the support.
1769model.
transeskip-gram.
projectionsame embedding align.
entity name align.
wikipedia anchors align..few-shot link prediction.
analogical reasoning.
mr hits@1 hits@10 mr hits@1 hits@10.
187–.
134102116138.
20.3–.
22.930.723.125.8.
40.4–.
47.251.846.746.2.
–25.
1211814.
–50.6.
65.960.766.556.1.
–78.0.
89.087.591.084.8.table 1: overall results for both evaluation tasks..set on the performance of the few-shot link predic-tion task, we create two versions of the training setwith different sizes:.
(a) full version: in this version of the training set,we include all triples in wikidata which don’tviolate the few-shot property of the dataset.
this is the same as the training set for theevaluation proposed in section 5.1..(b) support version: in this version of the trainingset, we exclude triples from the full versionwhose either head or tail entity isn’t presentin the support set..next, we try to analyze the impact of whetherthe head/tail entity of the test triple is present inthe support set s, on the few-shot link predictionperformance.
to this end, we create two versionsof test sets:.
(a) both in support: both head and tail entity of.
the triple lie in the support set..(b) missing support: atleast one out of thehead/tail entity of the triple doesn’t lie in thesupport set..the statistics for this dataset are given in table 3.the results for the training data size analysis fordifferent alignment methods on test set (both insupport) are shown in table 4. the results showthat for both projection and same embedding align-ment approach, the performance is signiﬁcantlybetter with using the full training set of triples in-stead of just the support set.
this shows that triplesinvolving non-support set entities play a vital rolein helping learn better entity and relation repre-sentations which in turn helps in injecting textualinformation to the kb embeddings via alignment.
effect of support set for test triples.
here, weinvestigate the performance of the few-shot linkprediction task for triples whose entities may not.
lie in the support set.
the results for this evalua-tion are given in table 5. we observe that there isno signiﬁcant gain in performance for any of thealignment methods over the simple transe base-line.
this shows these alignment methods are onlyeffective for triples whose both entities lie in thesupport set..effect of balance parameter.
in this analysis, westudy the role of balance parameter for the projec-tion alignment method.
this parameter controlsthe extent of alignment between the two embed-ding spaces.
the higher the value of the balanceparameter, the more the embedding tries to capturethe entity information from the other embeddingspace, rather than its own.
the results of this studyare shown in table 2. the peak performance forthe few-shot link prediction task is obtained forbalance parameter = 1e0 in terms of hits@1 andhits@10. whereas, for the analogical reasoningtask, the peak performance is obtained for balanceparameter = 1e-3.
this difference in the optimalvalue of the balance parameter can be explained bythe fact that the skip-gram objective relies on co-sine similarity which is more sensitive to changesin the values of vector embeddings than the transemodel.
we show this analytically.
let (h, r, t) be akb triple and let h, r, and t denote the embeddingsof h, r, and t respectively.
the partial derivative ofscore function of the triple w.r.t.
h is given by.
dr(h, t) = (cid:107)h + r − t(cid:107)2(cid:13)(h + r − t)(cid:13)(cid:13)(cid:107)h + r − t(cid:107)2(cid:13).
(cid:13)(cid:13)(cid:13)(cid:13)2.
∂dr(h, t)∂h.
=.
(cid:13)(cid:13)(cid:13)(cid:13).
(cid:13)(cid:13)(cid:13)(cid:13)2.
= 1.similarly, let (u, v) be an entity-word pair in thetext corpus.
let u and v denote the embeddings ofu and v respectively.
the partial derivative of thescore function for the entity-word pair (u, v) w.r.t..1770model.
transeskip-gram.
projection (balance param.=1e-4)projection (balance param.=1e-3)projection (balance param.=1e-2)projection (balance param.=1e-1)projection (balance param.=1e0)projection (balance param.=1e1).
few-shot link prediction.
analogical reasoning.
mr hits@1 hits@10 mr hits@1 hits@10.
187–.
188186184169134129.
20.3–.
20.420.520.620.722.921.4.
40.4–.
40.440.540.642.047.243.1.
–25.
141210162326.
–50.6.
65.065.961.457.849.642.2.
–78.0.
88.089.087.384.278.975.4.table 2: overall results for projection alignment model for different values of balance parameter..dataset.
no.
of triples.
relation.
transe projection.
same embed..train set (full)train set (support)test set (both in support)test set (missing support).
260.1 m17.1 m110.8 k38.3 k.risk factorsymptomsmedical cond.
cause of death.
31237371314.
26136267246.
15339330299.table 3: few-shot link prediction dataset statistics..table 6: link prediction results for covid-19 casestudy (mean rank)..model.
mean rank.
projection (full)projection (support).
same embed.
align.
(full)same embed.
align.
(support).
transe (full)transe (support).
134208.
102184.
188255.table 4: results for different training set sizes for few-shot link prediction task..model.
mean rank.
projection (full)same embed.
align.
(full)transe (full).
208207213.table 5: results for missing support test set (few-shot link prediction task)..u is given by.
(cid:13)(cid:13)(cid:13)(cid:13)2(cid:13)(cid:13)(cid:13).
d(u, v) = exp(ut v)(cid:13)(ut v)v(cid:13).
= (cid:13).
(cid:13)(cid:13)(cid:13)(cid:13).
∂d(u, v)∂u.
(cid:13)2 = (ut v) (cid:107)v(cid:107)2.
(cid:13)(cid:13)(cid:13)2(cid:13)∂d(u,v)(cid:13)(cid:13)∂u.
(cid:13)(cid:13)(cid:13)2.the value of.
∂dr(h,t)∂h.
equals 1 whereas for.
= (ut v) (cid:107)v(cid:107)2the skip-gram model,which is greater than 1, as seen empirically.
thisshows that the skip-gram embeddings are more sen-sitive to delta changes in values of the parameters.
for them to be reasonably assigned with their kbcounterparts without losing the textual information,thus a lower value of balance parameter is optimal..5.5 case study on covid related triples.
recently, the covid pandemic (fauci et al., 2020)has been responsible for bringing a tremendouschange in the lives of people across the globe.
through this case study, we demonstrate that align-ing embedding representations can help us doknowledge base completion for recent events likecovid-19.
we selected 4 relevant relations (“riskfactor”, “symptoms”, “medical condition” and“cause of death”) with atleast 10 triples in the dif-ference between march 2020 and december 2020snapshots of wikidata.
we use the march 2020wikidata and december 2020 wikipedia to trainthe alignment models and do link prediction onthese triples.
for each of the relations, we keepthe covid-19 entity (entity id: q84263196) un-changed and corrupt the other entity in the triple.
this would correspond to asking questions like“what are the symptoms of covid-19?”, “whodied due to covid-19?” etc.
the results areshown in table 6..we observe that the projection model obtainsa decent improvement over the transe model onthe link prediction task on these triples in termsof mean rank.
similarly, the same embeddingalignment model obtains outperforms the transebaseline for three out of four relations.
this casestudy gives a real-life use-case of how the text in-formation can be injected into the kb embeddingsusing alignment in scenarios when such informa-tion is not yet curated in the kb in structured form..17716 conclusion.
in this work, we presented a systematic study ofdifferent alignment approaches that can be appliedto align entity representations in a knowledge baseand textual corpora.
by evaluating on the few-shotlink prediction task and analogical reasoning task,we found that although all approaches have thedesired outcome, i.e., to incorporate informationfrom the other modality, some approaches performbetter than others on a particular task.
we also ana-lyzed the impact of different factors such as the sizeof the training set, the presence of test set entitiesin the support set, and the balance parameter onthe evaluation task performance.
we believe ourevaluation framework, as well as jointly trained em-beddings can serve as a useful resource for futureresearch and applications..acknowledgements.
we would like to thank the anonymous review-ers for their helpful comments.
this research wassponsored by a gift grant from fujitsu and the ohiosupercomputer center (center, 1987)..references.
s¨oren auer, christian bizer, georgi kobilarov, jenslehmann, richard cyganiak, and zachary ives.
2007. dbpedia: a nucleus for a web of open data.
in the semantic web, pages 722–735.
springer..kurt bollacker, patrick tufts, tomi pierce, and robertcook.
2007. a platform for scalable, collaborative,in intl.
work-structured information integration.
shop on information integration on the web (ii-web’07), pages 22–27..antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems, pages 2787–2795..yixin cao, lifu huang, heng ji, xu chen, and juanzili.
2017. bridge text and knowledge by learningmulti-prototype entity mention embedding.
in pro-ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1623–1633..p. castells, m. fern´andez, and d. vallet.
2007. anadaptation of the vector-space model for ontology-based information retrieval.
ieee transactions onknowledge and data engineering, 19..ohio supercomputer center.
1987. ohio supercom-.
puter center..muhao chen, yingtao tian, mohan yang, and carlozaniolo.
2017. multilingual knowledge graph em-beddings for cross-lingual knowledge alignment.
inproceedings of the 26th international joint con-ference on artiﬁcial intelligence, ijcai’17, page1511–1517.
aaai press..tim dettmers, pasquale minervini, pontus stenetorp,convolutional 2din thirty-second.
and sebastian riedel.
2018.knowledge graph embeddings.
aaai conference on artiﬁcial intelligence..xin dong, evgeniy gabrilovich, geremy heitz, wilkohorn, ni lao, kevin murphy, thomas strohmann,shaohua sun, and wei zhang.
2014. knowledgevault: a web-scale approach to probabilistic knowl-in proceedings of the 20th acmedge fusion.
sigkdd international conference on knowledgediscovery and data mining, pages 601–610..john duchi, elad hazan, and yoram singer.
2011.adaptive subgradient methods for online learningjournal of machineand stochastic optimization.
learning research, 12(7)..manaal faruqui and chris dyer.
2014. improving vec-tor space word representations using multilingualcorrelation.
in proceedings of the 14th conferenceof the european chapter of the association for com-putational linguistics, pages 462–471..anthony s fauci, h clifford lane, and robert r red-ﬁeld.
2020. covid-19—navigating the uncharted..matt gardner, partha talukdar, jayant krishnamurthy,and tom mitchell.
2014. incorporating vector spacesimilarity in random walk inference over knowledgein proceedings of the 2014 conference onbases.
empirical methods in natural language processing(emnlp), pages 397–406..anna gladkova, aleksandr drozd, and satoshi mat-suoka.
2016. analogy-based detection of morpho-logical and semantic relations with word embed-dings: what works and what doesn’t.
in proceedingsof the naacl student research workshop, pages 8–15, san diego, california.
association for computa-tional linguistics..yanchao hao, yuanzhe zhang, shizhu he, kang liu,and jun zhao.
2016. a joint embedding method forentity alignment of knowledge bases.
in china con-ference on knowledge graph and semantic comput-ing, pages 3–14.
springer..guoliang ji, shizhu he, liheng xu, kang liu, andjun zhao.
2015. knowledge graph embedding viain proceedings of thedynamic mapping matrix.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 1: long papers), pages 687–696..denis krompaß, stephan baier, and volker tresp.
2015. type-constrained representation learning inin international semantic webknowledge graphs.
conference, pages 640–655.
springer..1772ni lao and william w cohen.
2010. relational re-trieval using a combination of path-constrained ran-dom walks.
machine learning, 81(1):53–67..ni lao, amarnag subramanya, fernando pereira, andwilliam cohen.
2012. reading the web with learnedsyntactic-semantic inference rules.
in proceedingsof the 2012 joint conference on empirical methodsin natural language processing and computationalnatural language learning, pages 1017–1026..adam lerer, ledell wu,.
jiajun shen, timotheelacroix, luca wehrstedt, abhijit bose, and alexpeysakhovich.
2019. pytorch-biggraph: a large-scale graph embedding system.
in proceedings ofthe 2nd sysml conference, palo alto, ca, usa..yankai lin, zhiyuan liu, maosong sun, yang liu,and xuan zhu.
2015. learning entity and relationembeddings for knowledge graph completion.
intwenty-ninth aaai conference on artiﬁcial intelli-gence..tomas mikolov, quoc v le, and ilya sutskever.
2013a.
exploiting similarities among languages for ma-chine translation.
arxiv preprint arxiv:1309.4168..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013b.
distributed representa-tions of words and phrases and their compositional-in advances in neural information processingity.
systems, pages 3111–3119..deepak nathani, jatin chauhan, charu sharma, andlearning attention-basedmanohar kaul.
2019.embeddings for relation prediction in knowledgein proceedings of the 57th annual meet-graphs.
ing of the association for computational linguis-tics, pages 4710–4723, florence, italy.
associationfor computational linguistics..dai quoc nguyen, tu dinh nguyen, dat quocnguyen, and dinh phung.
2018. a novel embed-ding model for knowledge base completion basedon convolutional neural network.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 327–333, new orleans, louisiana.
as-sociation for computational linguistics..feng niu, benjamin recht, christopher re, andstephen j. wright.
2011. hogwild!
a lock-free ap-proach to parallelizing stochastic gradient descent.
in proceedings of the 24th international confer-ence on neural information processing systems,nips’11, page 693–701, red hook, ny, usa..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, et al.
2019. pytorch: an imperative style,in ad-high-performance deep learning library.
vances in neural information processing systems,pages 8026–8037..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in european semantic web confer-ence, pages 593–607.
springer..chao shang, yun tang, jing huang, jinbo bi, xi-aodong he, and bowen zhou.
2019. end-to-endstructure-aware convolutional networks for knowl-edge base completion.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 33,pages 3060–3067..wei shen, jianyong wang, and jiawei han.
2015. en-tity linking with a knowledge base:issues, tech-niques, and solutions.
ieee transactions on knowl-edge and data engineering, 27:443–460..richard socher, danqi chen, christopher d manning,and andrew ng.
2013. reasoning with neural ten-sor networks for knowledge base completion.
inadvances in neural information processing systems,pages 926–934..fabian m suchanek, gjergji kasneci, and gerhardweikum.
2007. yago: a core of semantic knowledge.
in proceedings of the 16th international conferenceon world wide web, pages 697–706..zhiqing sun, zhi-hong deng, jian-yun nie, and jiantang.
2019. rotate: knowledge graph embeddingby relational rotation in complex space.
in interna-tional conference on learning representations..kristina toutanova, danqi chen, patrick pantel, hoi-fung poon, pallavi choudhury, and michael gamon.
2015. representing text for joint embedding ofin proceedings of thetext and knowledge bases.
2015 conference on empirical methods in naturallanguage processing, pages 1499–1509..denny vrandeˇci´c and markus kr¨otzsch.
2014. wiki-data: a free collaborative knowledgebase.
commu-nications of the acm, 57(10):78–85..zhen wang, jianwen zhang, jianlin feng, and zhengchen.
2014. knowledge graph and text jointly em-bedding.
in proceedings of the 2014 conference onempirical methods in natural language processing(emnlp), pages 1591–1601..zhichun wang, qingsong lv, xiaohan lan, andyu zhang.
2018. cross-lingual knowledge graphalignment via graph convolutional networks.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 349–357..1773ruobing xie, zhiyuan liu, jia jia, huanbo luan, andmaosong sun.
2016. representation learning ofknowledge graphs with entity descriptions.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence..ikuya yamada, akari asai, jin sakuma, hiroyukishindo, hideaki takeda, yoshiyasu takefuji, andyuji matsumoto.
2020. wikipedia2vec: an efﬁ-cient toolkit for learning and visualizing the embed-indings of words and entities from wikipedia.
proceedings of the 2020 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 23–30.
association for com-putational linguistics..ikuya yamada, hiroyuki shindo, hideaki takeda, andyoshiyasu takefuji.
2016. joint learning of the em-bedding of words and entities for named entity dis-ambiguation.
in proceedings of the 20th signllconference on computational natural languagelearning, pages 250–259, berlin, germany.
associ-ation for computational linguistics..xuchen yao and benjamin van durme.
2014..infor-mation extraction over structured data: questionin proceedings of theanswering with freebase.
52nd annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 956–966, baltimore, maryland.
associationfor computational linguistics..mo yu, wenpeng yin, kazi saidul hasan, cicero dossantos, bing xiang, and bowen zhou.
2017.im-proved neural relation detection for knowledge basequestion answering.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 571–581, vancouver, canada.
association for computa-tional linguistics..da zheng, xiang song, chao ma, zeyuan tan, zihaoye, jin dong, hao xiong, zheng zhang, and georgekarypis.
2020a.
dgl-ke: training knowledge graphin proceedings of the 43rdembeddings at scale.
international acm sigir conference on researchand development in information retrieval, sigir’20, page 739–748, new york, ny, usa.
associa-tion for computing machinery..shuangjia zheng, j. rao, y. song, jixian zhang, xian-glu xiao, e. fang, yuedong yang, and zhangmingniu.
2020b.
pharmkg: a dedicated knowledge graphbenchmark for bomedical data mining.
brieﬁngs inbioinformatics..huaping zhong, jianwen zhang, zhen wang, hai wan,and zheng chen.
2015. aligning knowledge andtext embeddings by entity descriptions.
in proceed-ings of the 2015 conference on empirical methodsin natural language processing, pages 267–272..1774