tgea: an error-annotated dataset and benchmark tasks for textgeneration from pretrained language models.
jie he†∗, bo peng§∗, yi liao§, qun liu§ and deyi xiong†† college of intelligence and computing, tianjin university, tianjin, china§ huawei noah’s ark lab, hong kong, china{jieh, dyxiong}@tju.edu.cn,{peng.bo2, liaoyi9, qun.liu}@huawei.com.
abstract.
1.introduction.
in order to deeply understand the capabilityof pretrained language models in text genera-tion and conduct a diagnostic evaluation, wepropose tgea1, an error-annotated datasetwith multiple benchmark tasks for text genera-tion from pretrained language models (plms).
we use carefully selected prompt words toguide gpt-2 to generate candidate sentences,from which we select 47k for error annota-tion.
crowdsourced workers manually checkeach of these sentences and detect 12k erro-neous sentences.
we create an error taxon-omy to cover 24 types of errors occurring inthese erroneous sentences according to the na-ture of errors with respect to linguistics andknowledge (e.g., common sense).
for eacherroneous span in plm-generated sentences,we also detect another span that is closely as-sociated with it.
each error is hence manu-ally labeled with comprehensive annotations,including the span of the error, the associatedspan, minimal correction to the error, the typeof the error, and rationale behind the error.
apart from the fully annotated dataset, we alsopresent a detailed description of the data col-lection procedure, statistics and analysis of thedataset.
this is the ﬁrst dataset with compre-hensive annotations for plm-generated texts,which facilitates the diagnostic evaluation ofplm-based text generation.
furthermore, weuse tgea as a benchmark dataset and proposea series of automatic diagnosis tasks, includ-ing error detection, error type classiﬁcation, as-sociated span detection, error rationale genera-tion, to further promote future study on the au-tomatic error detection and correction on textsgenerated by pretrained language models..∗equal contributions.
1thethe.
dataset.
is.
available.
at.
https://download.mindspore.cn/dataset/tgea/..pretrained language models (devlin et al., 2019;liu et al., 2019; raffel et al., 2020; brown et al.,2020), which are trained on a huge amount of datavia self-supervised learning, have made remarkableprogress on both natural language understanding(nlu) (wang et al., 2018, 2019) and natural lan-guage generation (nlg) (liu and lapata, 2019;weng et al., 2020; cao et al., 2020)..on several nlu datasets, plm-based neuralmodels have gradually achieved human-level per-formance in terms of automatic evaluation met-rics (e.g., accuracy, f1) (he et al., 2020; zhanget al., 2021).
in order to deeply understand andanalyze the capability of plms on nlu, a varietyof more challenging nlu datasets have been pro-posed (warstadt et al., 2020; cui et al., 2020a; jainet al., 2020; talmor et al., 2020).
these datasetscan be used not only to obtain knowledge on howplm-based models work and what they learn, butalso to deﬁne new nlu tasks and to serve as abenchmark for future progress.
for example, evalu-ating and analyzing plm-based models on learningdocument structures with a carefully created bench-mark test suite (chen et al., 2019), helps to developnew methods to enhance the capability of thesemodels on discourse modeling (iter et al., 2020).
knowing the weakness of current plm-based mod-els in commonsense reasoning (zhou et al., 2020)has inspired people to develop various reasoningdatasets (cui et al., 2020a; zhang et al., 2020b)..on the other hand, state-of-the-art plms areable to generate texts that are even not distinguish-able from human-written texts by human evaluators(radford et al., 2019; brown et al., 2020).
thismakes us curious about the capability of plms ontext generation.
are they really reaching human-level performance on text generation?
in contrastto the studies of plms on nlu, research on the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6012–6025august1–6,2021.©2021associationforcomputationallinguistics6012capability of plms on nlg is quite limited, espe-cially in dataset building and diagnostic evaluationof text generation errors..in this paper, in order to recognize the perime-ter of text generation capability of plms, we pro-pose tgea, an error-annotated dataset with multi-ple benchmark tasks for text generation from pre-trained language models.
the original raw data arecollected from texts generated by a chinese gpt-2model.
the entire data collection and annotationprocedure is visualized in figure 1. the goals andcontributions of building tgea are as follows..• tgea, to the best of our knowledge, is theﬁrst dataset built on machine-generated textsfrom state-of-the-art pretrained language mod-els with rich annotations.
the key interest ofthis dataset is detecting and annotating textgeneration errors from plms.
therefore itis different from conventional text genera-tion datasets (e.g., multi-news (fabbri et al.,2019), textcaps (sidorov et al., 2020)) thatare constructed to train models to learn textgeneration (e.g., generating texts from imagesor long documents).
it is also different fromgrammatical error correction (gec) datasets(zhao et al., 2018; flachs et al., 2020) thatare built from human-written texts usually bysecond language learners..• tgea provides rich semantic information fortext generation errors, including error types,associated text spans, error corrections andrationals behind errors, as shown in figure1. marking text spans that are closely relatedto erroneous words allows us to detect long-distance dependencies of errors or reasoningchains related to errors.
rationales behind er-rors directly explain why errors are annotated.
all these error-centered manual annotationsnot only increase the interpretability of ourdataset, but also facilitate a comprehensivediagnostic evaluation of pretrained languagemodels on text generation..• we created an error taxonomy for tgea,which covers 24 error types in a two-levelhierarchy.
with this error taxonomy, we notonly obtain a high agreement on manual er-ror annotation but also recognize the strengthsand weaknesses of gpt-2 on text generationby estimating a distribution over these 24 er-ror types.
comparing our dataset with gecdatasets, we ﬁnd that humans and gpt-2 have.
figure 1: the different stages of the annotation pro-cess for each machine-generated text according to theprompt in tgea.
better viewed in color..a very different error distribution, especiallyon errors related to commonsense reasoning.
• tgea not only exhibits text generation errorsfrom pretrained language models, but also canserve as a dataset to train various models toautomatically detect and correct these errors,like gec datasets for training models to au-tomatically correct human errors.
we deﬁne5 benchmark tasks over our dataset, i.e., er-roneous sentence detection, erroneous spanand associated span detection, error type clas-siﬁcation, error correction and error rationalegeneration.
for all these tasks, we provide ex-perimental results using state-of-the-art mod-els as baselines..2 related work.
our work is related to gec datasets in error annota-tion and correction (machine vs. human errors).
itis also partially related to commonsense reasoningdatasets that have been proposed recently in thatour dataset includes commonsense reasoning errorsand rationales behind these errors.
our dataset isnot related to conventional text generation datasets(vougiouklis et al., 2017; wiseman et al., 2017;parikh et al., 2020) for training text generation mod-els.
a comprehensive comparison to gec datasetsand commonsense reasoning datasets is shown intable 1..6013t(cid:76)(cid:95)(cid:91) p(cid:89)(cid:86)(cid:84)(cid:87)(cid:91): c(cid:86)(cid:89)(cid:89)ec(cid:91)i(cid:85)c(cid:86)(cid:89)(cid:89)ec(cid:91)④ e(cid:89)(cid:89)(cid:86)(cid:89) (cid:91)(cid:96)(cid:87)e c(cid:83)a(cid:90)(cid:90)ifica(cid:91)i(cid:86)(cid:85)② e(cid:89)(cid:89)(cid:86)(cid:85)e(cid:86)(cid:92)(cid:90) a(cid:85)d   a(cid:90)(cid:90)(cid:86)cia(cid:91)ed s(cid:87)a(cid:85)     de(cid:91)ec(cid:91)i(cid:86)(cid:85)③ e(cid:89)(cid:89)(cid:86)(cid:89) c(cid:86)(cid:89)(cid:89)ec(cid:91)i(cid:86)(cid:85) ⑤ ra(cid:91)i(cid:86)(cid:85)a(cid:83) ge(cid:85)e(cid:89)a(cid:91)i(cid:86)(cid:85)c(cid:79)(cid:80)(cid:85)(cid:76)(cid:90)(cid:76) gpt-2(cid:25543)(cid:17157)(cid:9102)(cid:11014)(cid:19119)(cid:19325)别不同。sho(cid:87) p(cid:88)(cid:87) and archer(cid:92) are of differen(cid:87) ca(cid:87)egorie(cid:86).
(cid:13543)(cid:13870)(cid:17798)(cid:11014)(cid:19119)(cid:24274)动员(cid:9770)(cid:10357)(cid:10834)(cid:25543)(cid:17157)(cid:15062)(cid:23645)中(cid:21177)(cid:11929)冠军。japanese archery players won the championship in women's shot put competition.
(cid:13543)(cid:13870)(cid:17798)(cid:11014)(cid:19119)(cid:24274)动员(cid:9770)(cid:10357)(cid:10834)(cid:25543)(cid:17157)(cid:15062)(cid:23645)中(cid:21177)(cid:11929)冠军。japanese archery players won the championship in women's shot put competition.
(cid:13543)(cid:13870)(cid:17798)(cid:25543)(cid:17157)(cid:24274)动员(cid:9770)(cid:10357)(cid:10834)(cid:25543)(cid:17157)(cid:15062)(cid:23645)中(cid:21177)(cid:11929)冠军。japanese shot putters won the championship in women's shot put competition.
(cid:13543)(cid:13870)japan(cid:19325)别不匹(cid:24655)category mismatchte(cid:95)(cid:91) ge(cid:85)e(cid:89)a(cid:91)i(cid:86)(cid:85)① e(cid:89)(cid:89)(cid:86)(cid:85)e(cid:86)(cid:92)(cid:90) te(cid:95)(cid:91)     de(cid:91)ec(cid:91)i(cid:86)(cid:85)task.
rationales.
domain.
#sentences.
language.
dataset.
fceaeswjflegcmegcwebcgecwschellaswagsocial iqacosmosqapiqaabductive nliwinowhytgea (ours).
gecgecgecgecgecgeccoreference resolutionplausible inferencequestion answeringreading comprehensionplausible inferenceplausible inferencereason explanationmultiple tasks.
commonsensereasoning(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:51)(cid:51)(cid:51)(cid:51)(cid:51)(cid:51)(cid:51).
machine-generatedtexts(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54).
(cid:54)(cid:54)(cid:54)(cid:54).
(cid:51).
essayjournal articlestofel examweb doc/essayweb docessayopenwikihow articlessocial situationsnarrativesphysical situationsrocstoriesopenopen.
34k1.2m1,5118k13k0.71m27370k38k35k21k200k2,86547k.
(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:54)(cid:51)(cid:51).
enenenenenzhenenenenenenenzh.
table 1: comparison between our dataset and other datasets..2.1 grammatical error correction datasets.
2.2 commonsense datasets.
fce (yannakoudakis et al., 2011) is an early large-scale english grammatical error correction dataset,where raw texts are produced by english learn-ers taking the first certiﬁcate in english exams.
aesw (daudaravicius et al., 2016) is a gecdataset from a professional editing company.
inaddition to common grammatical errors, aeswcovers style issues as it contains texts mainly fromscholarly papers.
jfleg (napoles et al., 2017) is agec dataset built from tofel exams, which doesnot force annotators to make minimal edits, prefer-ring holistic ﬂuency rewrites.
cmeg (napoleset al., 2019) is different from general grammaticalerror correction datasets with texts from secondlanguage learners.
it uses articles or blogs (e.g.,wiki, yahoo)) written by native english speakersto explore grammatical error phenomena in dif-ferent domains.
cweb (flachs et al., 2020) alsouses website texts in english, such as blogs.
thedifference between cweb and cmeg is that thepercentage of erroneous tokens in the former issmaller than the latter as the purpose of cweb isto study grammatical error correction in low errordensity domains.
cgec (zhao et al., 2018) is alarge-scale chinese grammatical error correctiondataset, derived from wrong sentences written bychinese learners in the process of learning chineseas a second language..in addition to the difference in text sources (i.e.,human-written vs. machine-generated), other sig-niﬁcant differences between our dataset and ex-isting gec datasets are that our dataset containscommonsense reasoning errors and provides associ-ated text span annotations and rationales for errors,as shown in table 1..a variety of commonsense datasets have been pro-posed.
roemmele et al.
(2011) introduce copathat focuses on commonsense causal reasoning.
levesque et al.
(2012) present winograd schemechallenge (wsc), a dataset testing commonsensereasoning in the form of anaphora resolution.
wino-grande, a larger version of wsc, is introduced bysakaguchi et al.
(2020), which contains ∼ 44, 000examples.
winowhy (zhang et al., 2020a) asksannotators to provide reasons for their decisions towsc.
in this aspect, the differences of our datasetfrom winowhy are twofold.
first, we providereasons for errors rather than correct decisions toanaphora.
second, we provide reasons for all textgeneration errors, rather than only errors related tocommonsense reasoning..in addition to copa and wsc-style datasets,many large crowdsourced datasets have been alsoproposed recently.
commonsenseqa (talmoret al., 2019), a commonsense question answeringdataset, has been constructed from conceptnet.
hellaswag (zellers et al., 2019b) and abductivenli (bhagavatula et al., 2020) evaluate common-sense reasoning in the form of natural languageinference.
cosmosqa (huang et al., 2019) is adataset with multi-choice questions that requirecommonsense reading comprehension..beyond datasets for evaluating commonsensereasoning, there are other datasets providing com-monsense knowledge.
piqa (bisk et al., 2020) fo-cuses on physical commonsense knowledge whilesocialiqa (sap et al., 2019) on social common-sense knowledge..commonsense datasets in multiple languages orlanguages other than english have also been cre-ated recently.
xcopa (ponti et al., 2020) is a mul-tilingual dataset for causal commonsense reasoningin 11 typologically different languages.
chinese.
6014level-1 error typeinappropriate combination 医生当即将刘莉的.
example.
missing.
redundancy.
discourse error.
commonsense error.
(cid:58)(cid:58)(cid:58)手术[囊肿]切除，并建议患者住院观察。.
surgery [tumor] and suggested that the patient be hospitalized.
(cid:58)[活动]。.
增效[]增效,使得企业利润增长了10%以上。(cid:58)(cid:58)(cid:58)(cid:58)increased(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58).
the doctor removed liu li’s(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)for observation.
在这里,有众多新闻记者和游客参加here, many journalists and tourists are taking part in(cid:58)(cid:58)[activities].
一些企业减员some enterprises have reduced staff and (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)their proﬁts increase by more than 10%.
他说自己最喜欢安阳的乡间小路，是最美的he said that he likes the country roads in anyang best, and it is the most beautifulmountain [road].
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)在国际市场上，如果信用等级越in the international market, the (cid:58)(cid:58)(cid:58)(cid:58)investors are..(cid:58)(cid:58)高 [低]，投资者在投资时就越不会太放心。higher [lower] the credit rating, the less reassured.
(cid:58)(cid:58)(cid:58)(cid:58)山峦 [路]。.
efﬁciency[] increased efﬁciency, making.
table 2: examples of level-1 error types in tgea.
underwaved words are erroneous words while underlinedwords are associated words.
words in “[]” are corrections to erroneous words..commonsense datasets, such as mandarinograd(bernard and han, 2020) consisting of 154 chinesewinograd scheme examples and cluewsc2020(xu et al., 2020) containing 1838 winograd schemeexamples, have been proposed..in the aspect of commonsense reasoning, ourdataset is different from the mentioned common-sense datasets in that we detect and annotate errorsin machine-generated texts, which violates com-mon sense, rather than creating examples to exam-ine the commonsense reasoning ability of machines..3 dataset creation.
3.1 error taxonomy.
before crowdsourced workers manually annotateerrors in machine-generated texts, we need to cre-ate an error taxonomy for such error coding.
threeprinciples are used to guide the design of the er-ror taxonomy: coverage, exclusiveness and eas-iness.
the coverage rule requires that the errorsystem can cover almost all different types of er-rors in machine-generated texts.
the exclusivenessrequirement indicates that each error type is notoverlapping with other error types in the taxonomy.
the ﬁnal easiness principle means that the errorcoding system is easy to be used by annotators.
with these three principles and aid from a linguist,we created an error taxonomy in a two-level hi-erarchy, which was revised in our pre-annotationstage..the ﬁrst level of the error taxonomy includes 5.error types..• inappropriate combination.
this type of er-rors suggests that two words/phrases are syn-tactically or lexically inappropriately com-.
bined in a sentence.
such errors include notonly lexical collocation errors but also long-distance syntactic constituency combinationerrors (e.g., inappropriate subject-object com-bination).
this error type is similar to “replac-ing” error in some gec datasets (e.g., cweb(flachs et al., 2020)) as one element of aninappropriate combination should be usuallyreplaced with other expressions.
as we wantto ﬁnd text spans associated with erroneouswords/phrases, we term this error type as “in-appropriate combination”.
we further dividethis error type into ﬁve subtypes at the secondlevel..• missing.
grammatical constituencies orwords are missing.
5 subtypes are deﬁnedunder this error type..• redundancy.
words or phrases are unneces-.
sary.
5 subtypes are also deﬁned..• discourse error.
this error type is deﬁnedfor inter-sentential cohesion/coherence errors(e.g., coreference errors, incorrect discourseconnectives)..• commonsense error.
this error code is forerrors related to commonsense reasoning.
wedivide this error type into 8 subtypes accord-ing to the type of commonsense knowledgetype required (e.g., time, spatial, number)..all other errors that cannot be categorized intothe aforementioned error types are grouped into“other”.
table 2 displays examples for the abovedeﬁned error types.
24 error subtypes are displayedin figure 2 and examples of these subtypes areshown in appendix..60153.2 machine-generated text collection.
raw texts in our dataset are collected from a pre-trained chinese gpt-2 (nezha-gen)2, which gen-erates texts according to a system prompt.
nezha-gen has 12 layers and 12 attention heads and istrained on chinese wikipedia and news data (seeappendix for more details on the hyperparametersof nezha-gen).
as it is easy for nezha-gento generate high-quality texts with high-frequencyprompt words, we create a list of prompt wordsaccording to their frequency to guarantee that thereare sufﬁcient erroneous sentences in collected rawtexts.
by doing so, we have found that gpt hasa better chance to generate wrong sentences withsuch prompts.
speciﬁcally, we have randomly sam-pled 2m sentences from the data used to trainnezha-gen. the sampled sentences are thenword-segmented and pos-tagged by baidu lactool3 (jiao et al., 2018).
we then select and sortnouns in a descending order according to their fre-quencies in the sampled corpus.
nouns rankingin the range of top [40%, 60%] are selected asprompts..we further ﬁlter out noisy texts from texts gener-ated with these selected prompts.
noisy texts areeither texts containing no more than 15 charactersor texts where chinese characters account for less70% of all characters..3.3 error annotation.
there are 5 stages in error annotation, as shownin figure 1. we introduce each of them in thissubsection..(1) erroneous text detection.
texts generatedby nezha-gen with prompt words are present toannotators one by one.
the ﬁrst stage of annotationis hence to detect erroneous texts for subsequentannotations.
corresponding tags are annotated fortexts being manually checked..(2) erroneous and associated span detection.
the next task for annotators is to detect erroneousand associated text spans in detected erroneoustexts.
for erroneous span detection, as a text maycontain several spans that can be edited or the textcan be corrected in different ways, which spanshould be regarded as erroneous is closely relatedto the way that we correct the text.
therefore, thebasic principle that guides the annotation of erro-.
2github.com/huawei-noah/pretrained-language-.
model/tree/master/nezha-gen-tensorflow.
3github.com/baidu/lac.
neous spans is also the rule that we use for errorcorrection: making minimal edits, which is alsoused in gec datasets (flachs et al., 2020; napoleset al., 2017).
in addition to the minimal edit prin-ciple, we also provide the following speciﬁc rulesfor annotators:.
• if annotators feel that a text is ambiguous andthat it is difﬁcult to correct the text, the textcan be discarded without any further annota-tions..• if there are several spans that can be edited,the ﬁrst erroneous span is preferred to beedited..• if the number of errors to be corrected in a.text is larger than 4, the text is removed..following these rules, annotators have removed4,291 texts, which account for only 8.36% of alldetected erroneous texts in the ﬁrst stage..in addition to erroneous span annotation, unlikegec datasets (daudaravicius et al., 2016; zhaoet al., 2018), we also detect a text span that isclosely related to the already detected erroneousspan with respect to the error, and term this span as“associated span”.
in table 2, we show exampleswith annotated erroneous and associated text spans.
for an inappropriate combination, the associatedspan is usually a span that should not co-occur withthe erroneous span..(3) error correction.
after detecting erroneousspans in a given text, annotators are required tomake corrections following the minimal edit prin-ciple.
annotators are also required to use commonwords for error correction to make the correctedtext as ﬂuent as possible..(4) error type classiﬁcation.
once annotatorsdetect both erroneous and associated spans as wellas provide corrections, they are becoming quiteaware of these errors.
hence, we now ask themto categorize the annotated errors into error typesdeﬁned in our error taxonomy.
first, they select theprimary type from the level-1 error types.
then,if there are level-2 error subtypes, annotators con-tinue to select a subtype.
we observe that errorsannotated with “other” only account for 5.70%,suggesting that our error taxonomy has good cov-erage..(5) rationale generation.
partially inspired byprevious datasets that provide explanations togetherwith corresponding annotations, e.g., e-snli (cam-buru et al., 2018), winowhy (zhang et al., 2020a).
6016taskerroneous text detectionerroneous and associatedspan detectionerror type classiﬁcation.
iaa (%)87.5.kappa (%)62.1.
51.2.
73.3.
–.
55.7.table 3: inter-annotator agreement results..and r4c (inoue et al., 2020), we ask annotatorsto give a reason for each error to justify their an-notations.
to the best of our knowledge, no gecdatasets provide explanations for error corrections.
we believe that annotated rationales can be usedto improve the interpretability of neural modelstrained on our dataset..3.4 annotation quality control.
in order to ensure the quality of error annotations,we have adopted a very strict quality control proto-col during annotation.
first, we train two reviewerswith 1k machine-generated texts.
the annotationconsistency of the two reviewers on the 1k textsis very high, with an average iaa of 92.3% andcohen’s kappa (mchugh, 2012) of 82.6% acrossthe annotation tasks (1), (2) and (4).
for the textsannotated by the two reviewers, we have conductedan evaluation.
the average accuracy of all tasks is96.3% and 97.4% respectively..second, 200 candidate workers participate ina pre-annotation stage.
the two reviewers willreview annotations from these participants to dis-tinguish whether the annotation is correct or not.
only participants who have reached an accuracy of>90% in every tasks can join in the next stage.
asa result, 20 participants have passed the training inthe pre-annotation stage.
we then divide them intotwo groups and ask them to annotate the same 500texts.
the inter-annotator iaa and cohen’s kappaare shown in table 3, which suggests that the 20annotators are ready for ﬁnal annotation..third, in order to further ensure annotation qual-ity, we have carried out iterative veriﬁcation andamendment.
the two reviewers will review eachannotated text.
if they found the annotation iswrong, the unqualiﬁed data will be returned foramendment until they are qualiﬁed..following this strict quality control protocol, wecomplete the annotation on 47k selected machine-generated texts.
we randomly sample 1k annotatedtexts.
the average accuracy over the three tasks(i.e., (1), (2) and (4)) is 89.6%, 88.5%, 84.3% re-spectively..#textw/ 0 errorw/ 1 errorw/ 2 errorw/ 3 errorw/ 4 errortokensvocabavg.
tokensavg.
t.erravg.
t.assocavg.
d.e-aavg.
t.rationale.
train37,64627,9068,4131,16914117966,76544,59825.682.924.306.998.74.dev4,7063,4881,055141184120,88916,89925.693.094.397.298.72.test4,7063,4881,052149152121,06516,74525.732.953.897.108.75.all47,05834,88210,5201,459174231,208,71948,54725.682.944.277.038.74.table 4: data statistics of tgea.
avg.t.err/avg.t.assoc:the average number of tokens in erroneous/associatedtext spans.
avg.t.rationale: the average number of to-kens in rationales.
avg.d.e-a: the average distance be-tween a erroneous span and its associated span..figure 2: distribution over the level-1 and level-2 errortypes in tgea..4 dataset analysis.
4.1 dataset statistics.
overall statistics.
we reshufﬂe all annotated textsand divide them into the training/dev/test sets witha proportion of 8:1:1. as shown in table 4, thetraining set contains 27,096 correct texts and 9,740erroneous texts.
both the development and test setcontain 4,706 texts, among which 1,218 texts areerroneous.
not surprisingly, most erroneous textscontain only one error..after chinese word segmentation via jieba4,there are 1,208,719 tokens in total.
on average,there are 25.68 tokens in each text.
annotation statistics.
as shown in table 4, eacherroneous text span contains 2.94 tokens while eachassociated span is composed of 4.27 tokens.
theaverage distance from an erroneous text span to itsassociated span is 7.03 tokens, which is about 1/3of the average text length..4github.com/fxsjy/jieba.
6017tgeai(cid:85)a(cid:87)(cid:87)(cid:89)(cid:86)(cid:87)(cid:89)(cid:80)a(cid:91)(cid:76) c(cid:86)(cid:84)(cid:73)(cid:80)(cid:85)a(cid:91)(cid:80)(cid:86)(cid:85)               25.23%       m(cid:80)(cid:90)(cid:90)(cid:80)(cid:85)(cid:78)        8.00%      r(cid:76)(cid:75)(cid:92)(cid:85)(cid:75)a(cid:85)(cid:74)(cid:96)           31.62%d(cid:80)(cid:90)(cid:74)(cid:86)(cid:92)(cid:89)(cid:90)(cid:76) e(cid:89)(cid:89)(cid:86)(cid:89)      10.48%c(cid:86)(cid:84)(cid:84)(cid:86)(cid:85)(cid:90)(cid:76)(cid:85)(cid:90)(cid:76) e(cid:89)(cid:89)(cid:86)(cid:89)            18.96%s(cid:92)(cid:73)-p(cid:89)(cid:76)(cid:75)p(cid:89)(cid:76)(cid:75)-o(cid:73)(cid:81)s(cid:92)(cid:73)-o(cid:73)(cid:81)m(cid:86)(cid:75)(cid:80)(cid:77)(cid:80)(cid:76)(cid:89)f(cid:92)(cid:85)(cid:74)(cid:91)(cid:80)(cid:86)(cid:85) w(cid:86)(cid:89)(cid:75)s(cid:92)(cid:73)p(cid:89)(cid:76)(cid:75)o(cid:73)(cid:81)m(cid:86)(cid:75)(cid:80)(cid:77)(cid:80)(cid:76)(cid:89)f(cid:92)(cid:85)(cid:74)(cid:91)(cid:80)(cid:86)(cid:85) w(cid:86)(cid:89)(cid:75)s(cid:92)(cid:73)p(cid:89)(cid:76)(cid:75)o(cid:73)(cid:81)m(cid:86)(cid:75)(cid:80)(cid:77)(cid:80)(cid:76)(cid:89)f(cid:92)(cid:85)(cid:74)(cid:91)(cid:80)(cid:86)(cid:85) w(cid:86)(cid:89)(cid:75)c(cid:86)(cid:89)(cid:76)(cid:77)(cid:76)(cid:89)(cid:76)(cid:85)(cid:74)(cid:76)t(cid:80)(cid:84)(cid:76)s(cid:87)a(cid:74)(cid:76)n(cid:92)(cid:84)(cid:73)(cid:76)(cid:89)m(cid:86)(cid:91)(cid:80)(cid:93)a(cid:91)(cid:80)(cid:86)(cid:85)e(cid:84)(cid:86)(cid:91)(cid:80)(cid:86)(cid:85)a(cid:83) r(cid:76)a(cid:74)(cid:91)(cid:80)(cid:86)(cid:85)ca(cid:92)(cid:90)a(cid:91)(cid:80)(cid:86)(cid:85)ta(cid:95)(cid:86)(cid:85)(cid:86)(cid:84)(cid:96)b(cid:76)(cid:79)a(cid:93)(cid:80)(cid:86)(cid:89)(cid:90)o(cid:91)(cid:79)(cid:76)(cid:89) e(cid:89)(cid:89)(cid:86)(cid:89)      5.70%o(cid:91)(cid:79)(cid:76)(cid:89) e(cid:89)(cid:89)(cid:86)(cid:89)4.2 error type distribution.
5.2 erroneous span and associated span.
we further show the percentages of both level-1and level-2 error types in figure 2. we observethat only 5.7% cases cannot be categorized intoour deﬁned error types.
the inappropriate combi-nation, missing and redundancy error, which arethe main error types in gec datasets, account for64.85% in our dataset.
in addition to these errors,we see 18.96% commonsense errors and 10.48%discourse errors, which are usually not very com-mon in gec datasets.
however, these two typesof errors with high percentages in our dataset sug-gest that pretrained language models can be furtherimproved on both commonsense reasoning and dis-course modeling..5 tgea as a benchmark.
we use our dataset as a benchmark and propose 5tasks that are deﬁned for errors in texts generatedby plms.
we provide baseline results for thesetasks in this section..we employ three bert-style chinese plms asbaselines in our experiments, namely bert-wwm-ext, roberta-wwm-ext-large developed by cuiet al.
(2020b) 5 and albert-chinese-large6.
fornotational simiplicity, we denote them as bertzh,robertazh and albertzh respectively.
pleaserefer to the appendix for the model hyperparametersettings of each task..5.1 erroneous text detection.
task deﬁnition.
this is a text classiﬁcation task tojudge whether a given text is erroneous.
in order toavoid data imbalance, we use the same number ofcorrect and erroneous texts for training.
model.
the three chinese plms are used withstandard text-classiﬁcation ﬁne-tuning.
results.
all models perform just <14% betterthan chance (random guessing), as shown in ta-ble 5. we also provide human performance onthis task.
the best model robertazh is worsethan human performance by 26 points.
this sug-gests that automatically detecting erroneous textsgenerated by pretrained language models is verychallenging even in the balanced classiﬁcation sce-nario..5github.com/ymcui/chinese-bert-wwm6huggingface.co/voidful/albert chinese large.
detection.
task deﬁnition.
we deﬁne the detection of the twotypes of spans as a joint task as they are closelyrelated to each other.
the joint task is similar tonamed entity recognition (ner) (a sequence label-ing task) and it requires to recognize the erroneousand associated text spans simultaneously.
ner-style word-level tags are hence annotated for eacherroneous text.
model.
the three chinese plms with ner-likeﬁne-tuning are evaluated for this task.
since this isa 3-class token classiﬁcation task, we report class-f1 on erroneous and associated span.
the class-f1 on class x is calculated like a normal f1 fora binary classiﬁcation task, by treating the targetclass x as the positive class and all other classesas negative.
results.
as shown in table 5, all models are verypoor in this task, indicating the difﬁculty of auto-matically detecting erroneous and associated spans.
however, we have found that models can beneﬁtmuch from the joint detection over the detectionof a single type of span (either erroneous or asso-ciated span).
our preliminary experiments on thedetection of only erroneous span show that the bestmodel can only achieve 26.42% erroneous class-f1on the test set, while the joint task achieves 27.66%erroneous class-f1 on the test set..5.3 error type classiﬁcation.
task deﬁnition.
again this is a text classiﬁcationtask.
we only perform classiﬁcation over level-1error types in the form of 5-way classiﬁcation.
model.
we use models similar to the ﬁrst task.
results.
the overall accuracy and macro-f1(shown in table 5) are very low.
however, weﬁnd some error types are easier than others.
theaccuracy on the classiﬁcation of redundancy errorsis 53.91%, the highest among all error types..5.4 error correction.
task deﬁnition.
this task is the same as gec,which transforms an erroneous text into a correctsequence.
model.
we use the state-of-the-art bert-gecmodel (kaneko et al., 2020) as the baseline for thistask, which is an encoder-decoder model using rep-resentations learned by plms as additional inputs.
following wang et al.
(2020)，we feed represen-tations learned by bertzh and robertazh into.
6018task.
model.
devaccuracy (%).
testaccuracy (%).
erroneoustext detection.
erroneous andassociatedspan detection.
error typeclassiﬁcation.
randomalbertzhbertzhrobertazhhuman.
randomalbertzhbertzhrobertazh.
randomalbertzhbertzhrobertazh.
50.0063.5965.1566.6792.35.
50.0063.3064.9466.7993.57.erroneousclass-f1 (%).
01.7127.3627.8528.17accuracy (%).
24.2534.7644.3544.44.associatedclass-f1 (%).
04.2327.4426.9327.08macro-f1 (%).
20.0021.0433.0136.10f0.5 (%).
erroneousclass-f1 (%).
01.7428.1027.6627.75accuracy (%).
24.2534.3841.3144.16.associatedclass-f1 (%).
04.2226.2425.3027.12macro-f1 (%).
20.0020.5631.0537.20f0.5 (%).
p (%).
r (%).
p (%).
r (%).
error correction.
bertzh gecrobertazh gec.
0.620.78bleu rouge-l bert score bleu rouge-l bert score.
0.600.82.
0.760.93.
6.304.15.
6.494.07.
0.740.98.rationale generation nezha-gen.0.06%.
9.17%.
56.58%.
0.06%.
9.02%.
56.17%.
table 5: performance of benchmark models on the development and test set..the bert-gec model.
results.
we report precision, recall and f0.5 scoresusing the ofﬁcial max-match tool (dahlmeierand ng, 2012).
as shown in table 5, the bestrobertazh gec model achieves a very low f0.5of 0.93% and 0.98% on the development and testset respectively.
we speculate that the reasons forthis are twofold.
first, comparing with gec dataon human-written texts, our dataset is relativelysmall.
second, our dataset contains error typesthat are very different from those in previous gecdatasets (zhao et al., 2018; flachs et al., 2020).
punctuation, spelling and other word-character-level errors, which are easy to be corrected, arerare in tgea although they are quite commonin gec datasets.
in contrast, tgea containsmore complicated errors that can only be correctedwith knowledge of common sense, long-distanceor inter-sentential dependencies, etc..5.5 rationale generation.
task deﬁnition.
this is a text generation task thatdirectly generate an explanation with respect to textgeneration errors from an erroneous text.
model.
we use nezha-gen as the base-linean-thisfortask.
in the formnotated textsin our dataset{t, 这句话错误的原因是：, r}({t,ofthe reason behind the errors in this sentence is:,r}), where t is an erroneous sentence, while r.restructure.
we.
is the error rational provided by annotators.
wethen ﬁne-tune nezha-gen on the reformattedtraining set and evaluate the ﬁne-tuned modelon the reformatted development and test set.
wereport bleu (papineni et al., 2002), rouge-l (lin,2004) and bert score (zhang et al., 2020c).
results.
it can be expected that results in thesemetrics will be very low due to the high difﬁ-culty of this task.
we analyze generated textsfrom the baseline and ﬁnd that generated ratio-nales are usually much longer than reference ra-tionales provided by human annotators.
this couldresult in the low bleu score since long hypothe-ses are penalized in bleu computation.
we alsoexperiment zero-shot generation on the test set.
the results are {bleu = 0.04%, rouge-l =6.83%, bert score = 54.27%}, indicating thatﬁne-tuning on the annotated training set can im-prove this task.
we suggest that this generation taskcould be reformulated as a multi-choice questionanswering task by providing alternative rationalesas distractors, similar to vcr (zellers et al., 2019a).
we leave this to our future work..6 discussion.
since we use machine-generated texts for errorannotation, hyperparameters of models (e.g., sam-pling strategies, model size), model types (e.g.,gpt-2, gpt-3 or other plms for text generation),and genres of texts used to train plms, etc., all.
6019have impacts on generated texts and hence on errortypes and error distribution..a straightforward way to mitigate this issue is tocollect raw texts from multiple models with differ-ent hyperparameters, neural architectures and textgenres.
this will lead to an expanded dataset witha much larger number of instances to be manuallyannotated, which is expensive and time-consuming.
yet another issue with this is that it may result in abunch of data due to inconsistency across differentmodels and difﬁculty in setting the proportion ofeach data source..instead, we focus on consistently annotating er-rors for texts generated from a single source.
inorder to make tgea as general and representativeas possible, we use gpt-2 that is not only currentlystate of the art in text generation but also easilyavailable.
we also adopt standard and widely-usedhyperparameters (see appendix for more details)for nezha-gen to generate texts..additionally, we use a random sampling strategywith top k = 30. for setting k, we have analyzed500 examples with different values of k, and foundthat adjusting k has a reasonable impact on thepercentage of redundancy errors.
except for the ex-treme case of k = 1, the types of errors and the dis-tribution of them do not change signiﬁcantly.
takecommonsense errors as an example, which is thebiggest difference from human-written texts.
whenk varies in a range of {5, 10, 20, 30, 50}, the per-centage of commonsense errors is 18.6% ± 5.8%.
redundancy errors account for >95% when k = 1(while commonsense errors account for 0.8%), butsharply drop to 37.4% as k = 5, and the form ofrepetition changes from same-word repetition to amixed repetition of “synonymous/same-word”, sug-gesting that a simple repetition penalty may not besufﬁcient to deal with semantic redundancy.
whenk ∈ {10, 20, 30, 50}, the percentage of redundancyerrors is very close to the result reported in figure2. when k > 30, many generated sentences arecompletely incomprehensible.
a larger k will alsoreduce the generation efﬁciency.
therefore, wechose a sampling strategy of k = 30, which isthe trade-off between text quality and generationefﬁciency..7 conclusions.
in this paper, we have presented tgea, the ﬁrstdataset with a variety of manual annotations onerrors occurring texts generated by pretrained lan-.
guage models.
for each erroneous text generatedby a chinese gpt-2 model, our crowdsourced an-notators detect erroneous text spans with their as-sociated text spans and provide error types deﬁnedin a two-level hierarchical taxonomy as well as ra-tionales behind detected errors.
we elaborate the 5annotation stages for building tgea with a strictannotation quality control protocol.
we also re-port baseline results of the 5 benchmark tasks ontgea.
the low results suggest that our dataset isa challenging testbed for future work on automaticdetection of erroneous spans and types as well asproducing error corrections and rationales for textsgenerated by plms.
tgea is featured with wideerror type coverage, rich semantic annotation andfunctional diversity, which can not only be usedfor deep diagnostic analysis on the text generationcapability of pretrained language models, but alsofacilitate and promote the research of automatic andinterpretable error correction for plm-generatedtexts..acknowledgments.
the present research was supported by huawei.
wewould like to thank the anonymous reviewers fortheir insightful comments.
we also want to thankmindspore7 for the partial suppoort of this work,which is a new deep learning computing frame-work.
the corresponding author is deyi xiong(dyxiong@tju.edu.cn)..references.
timoth´ee bernard and ting han.
2020. mandarino-grad: a chinese collection of winograd schemas.
inproceedings of the 12th language resources andevaluation conference, pages 21–26.
european lan-guage resources association..chandra bhagavatula, ronan le bras, chaitanyamalaviya, keisuke sakaguchi, ari holtzman, han-nah rashkin, doug downey, scott wen-tau yi, andyejin choi.
2020. abductive commonsense reason-international conference on learning repre-ing.
sentations..yonatan bisk, rowan zellers, ronan lebras, jian-feng gao, and yejin choi.
2020. piqa: reasoningabout physical commonsense in natural language.
inaaai, pages 7432–7439..tom brown, benjamin mann, nick ryder, melaniejared d kaplan, prafulla dhariwal,subbiah,arvind neelakantan, pranav shyam, girish sastry,.
72020. mindspore.
https://www.mindspore.cn/.
6020amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish,alec radford, ilya sutskever, and dario amodei.
2020. language models are few-shot learners.
inadvances in neural information processing systems,volume 33, pages 1876–1900.
curran associates,inc..oana-maria camburu, tim rockt¨aschel, thomaslukasiewicz, and phil blunsom.
2018. e-snli: nat-ural language inference with natural language expla-in s. bengio, h. wallach, h. larochelle,nations.
k. grauman, n. cesa-bianchi, and r. garnett, ed-itors, advances in neural information processingsystems 31, pages 9539–9549.
curran associates,inc..yu cao, wei bi, meng fang, and dacheng tao.
2020.pretrained language models for dialogue generationwith multiple input sources.
in findings of the as-sociation for computational linguistics: emnlp2020, pages 909–917, online.
association for com-putational linguistics..mingda chen, zewei chu, and kevin gimpel.
2019.evaluation benchmarks and learning criteria forin pro-discourse-aware sentence representations.
ceedings ofthe 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 649–662, hong kong, china.
association for computa-tional linguistics..leyang cui, yu wu, shujie liu, yue zhang, and mingzhou.
2020a.
mutual: a dataset for multi-turn dia-logue reasoning.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 1406–1416, online.
association forcomputational linguistics..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020b.
revisiting pre-trained models for chinese natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:findings, pages 657–668, online.
association forcomputational linguistics..daniel dahlmeier and hwee tou ng.
2012. betterevaluation for grammatical error correction.
in pro-ceedings of the 2012 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages568–572, montr´eal, canada.
association for com-putational linguistics..vidas daudaravicius, rafael e. banchs, elena volod-ina, and courtney napoles.
2016. a report on the au-tomatic evaluation of scientiﬁc writing shared task.
in proceedings of the 11th workshop on innovative.
use of nlp for building educational applications,pages 53–62, san diego, ca.
association for com-putational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..alexander fabbri, irene li, tianwei she, suyi li, anddragomir radev.
2019. multi-news: a large-scalemulti-document summarization dataset and abstrac-tive hierarchical model.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 1074–1084, florence, italy.
association for computational linguistics..simon flachs, oph´elie lacroix, helen yannakoudakis,marek rei, and anders søgaard.
2020. grammati-cal error correction in low error density domains: anew benchmark and analyses.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 8467–8478,online.
association for computational linguistics..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2020. deberta: decoding-enhancedbert with disentangled attention..lifu huang, ronan le bras, chandra bhagavatula, andyejin choi.
2019. cosmos qa: machine readingcomprehension with contextual commonsense rea-soning.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages2391–2401, hong kong, china.
association forcomputational linguistics..naoya inoue, pontus stenetorp, and kentaro inui.
2020.r4c: a benchmark for evaluating rc systems to getthe right answer for the right reason.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 6740–6750, on-line.
association for computational linguistics..dan iter, kelvin guu, larry lansing, and dan jurafsky.
2020. pretraining with contrastive sentence objec-tives improves discourse performance of languagein proceedings of the 58th annual meet-models.
ing of the association for computational linguistics,pages 4859–4870, online.
association for computa-tional linguistics..sarthak jain, madeleine van zuylen, hannaneh ha-jishirzi, and iz beltagy.
2020. scirex: a chal-lenge dataset for document-level information extrac-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 7506–7516, online.
association for compu-tational linguistics..6021zhenyu jiao, shuqi sun, and ke sun.
2018. chineselexical analysis with deep bi-gru-crf network.
arxivpreprint arxiv:1807.01882..masahiro kaneko, masato mita, shun kiyono, junsuzuki, and kentaro inui.
2020. encoder-decodermodels can beneﬁt from pre-trained masked lan-guage models in grammatical error correction.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4248–4254, online.
association for computational lin-guistics..hector j. levesque, ernest davis, and leora morgen-stern.
2012. the winograd schema challenge.
inproceedings of the thirteenth international confer-ence on principles of knowledge representationand reasoning, kr’12, page 552–561.
aaai press..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..yang liu and mirella lapata.
2019. text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3730–3740, hong kong,china.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..mary mchugh.
2012. interrater reliability: the kappastatistic.
biochemia medica : ˇcasopis hrvatskogadruˇstva medicinskih biokemiˇcara / hdmb, 22:276–82..courtney napoles, maria nadejde, and joel tetreault.
2019. enabling robust grammatical error correctionin new domains: datasets, metrics, and analyses.
transactions of the association for computationallinguistics, 7(0):551–566..courtney napoles, keisuke sakaguchi, and joeltetreault.
2017.jfleg: a ﬂuency corpus andbenchmark for grammatical error correction.
in pro-ceedings of the 15th conference of the europeanchapter of the association for computational lin-guistics: volume 2, short papers, pages 229–234,valencia, spain.
association for computational lin-guistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..ankur parikh, xuezhi wang, sebastian gehrmann,manaal faruqui, bhuwan dhingra, diyi yang, and.
dipanjan das.
2020. totto: a controlled table-to-text generation dataset.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 1173–1186, on-line.
association for computational linguistics..edoardo maria ponti, goran glavaˇs, olga majewska,qianchu liu, ivan vuli´c, and anna korhonen.
2020.xcopa: a multilingual dataset for causal common-in proceedings of the 2020 con-sense reasoning.
ference on empirical methods in natural languageprocessing (emnlp), pages 2362–2376, online.
as-sociation for computational linguistics..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..melissa roemmele, cosmin adrian bejan, and an-drew s gordon.
2011. choice of plausible alterna-tives: an evaluation of commonsense causal reason-ing.
in aaai spring symposium: logical formaliza-tions of commonsense reasoning, pages 90–95..keisuke sakaguchi, ronan le bras, chandra bhagavat-ula, and yejin choi.
2020. winogrande: an adver-sarial winograd schema challenge at scale.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, volume 34, pages 8732–8740.
issue: 05..maarten sap, hannah rashkin, derek chen, ronanle bras, and yejin choi.
2019. social iqa: com-inmonsense reasoning about social interactions.
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4463–4473, hong kong, china.
association for computa-tional linguistics..oleksii sidorov, ronghang hu, marcus rohrbach, andamanpreet singh.
2020. textcaps: a dataset for im-age captioning with reading comprehension.
corr,abs/2003.12462..alon talmor, yanai elazar, yoav goldberg, andjonathan berant.
2020. olmpics-on what languagemodel pre-training captures.
transactions of the as-sociation for computational linguistics, 8:743–758..alon talmor, jonathan herzig, nicholas lourie, andjonathan berant.
2019. commonsenseqa: a ques-tion answering challenge targeting commonsensein proceedings of the 2019 confer-knowledge.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 4149–4158.
association for computa-tional linguistics..6022pavlos vougiouklis, hady elsahar, lucie-aim´eekaffee, christophe gravier, fr´ed´erique laforest,jonathon s. hare, and elena simperl.
2017. neu-ral wikipedian: generating textual summaries fromknowledge base triples.
corr, abs/1711.00155..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel bowman.
2019. superglue: astickier benchmark for general-purpose language un-derstanding systems.
in advances in neural infor-mation processing systems, volume 32, pages 3266–3280. curran associates, inc..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..hongfei wang, michiki kurosawa, satoru katsumata,and mamoru komachi.
2020. chinese grammaticalcorrection using bert-based pre-trained model.
inproceedings of the 1st conference of the asia-paciﬁcchapter of the association for computational lin-guistics and the 10th international joint conferenceon natural language processing, pages 163–168,suzhou, china.
association for computational lin-guistics..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of the as-sociation for computational linguistics, 8:377–392..rongxiang weng, heng yu, shujian huang, shanbocheng, and weihua luo.
2020. acquiring knowl-edge from pre-trained modelto neural machinetranslation.
proceedings of the aaai conference onartiﬁcial intelligence, 34(05):9266–9273..sam wiseman, stuart shieber, and alexander rush.
2017. challenges in data-to-document generation.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2253–2263, copenhagen, denmark.
association forcomputational linguistics..liang xu, hai hu, xuanwei zhang, lu li, chenjiecao, yudong li, yechen xu, kai sun, dian yu,cong yu, yin tian, qianqian dong, weitang liu,bo shi, yiming cui, junyi li, jun zeng, rongzhaowang, weijian xie, yanting li, yina patterson,zuoyu tian, yiwen zhang, he zhou, shaoweihualiu, zhe zhao, qipeng zhao, cong yue, xinruizhang, zhengliang yang, kyle richardson, andzhenzhong lan.
2020. clue: a chinese languagein proceed-understanding evaluation benchmark.
ings of the 28th international conference on com-putational linguistics, pages 4762–4772, barcelona,spain (online).
international committee on compu-tational linguistics..helen yannakoudakis, ted briscoe, and ben medlock.
2011. a new dataset and method for automaticallygrading esol texts.
in proceedings of the 49th an-nual meeting of the association for computationallinguistics: human language technologies, pages180–189, portland, oregon, usa.
association forcomputational linguistics..rowan zellers, yonatan bisk, ali farhadi, and yejinchoi.
2019a.
from recognition to cognition: vi-sual commonsense reasoning.
in proceedings of theieee/cvf conference on computer vision and pat-tern recognition, pages 6720–6731..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019b.
hellaswag: canin pro-a machine really ﬁnish your sentence?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4791–4800, florence, italy.
association for computationallinguistics..hongming zhang, xinran zhao, and yangqiu song.
2020a.
winowhy: a deep diagnosis of essentialcommonsense knowledge for answering winogradin proceedings of the 58th an-schema challenge.
nual meeting of the association for computationallinguistics, pages 5736–5745, online.
associationfor computational linguistics..li zhang, qing lyu, and chris callison-burch.
2020b.
reasoning about goals, steps, and temporal orderingin proceedings of the 2020 con-with wikihow.
ference on empirical methods in natural languageprocessing (emnlp), pages 4630–4639, online.
as-sociation for computational linguistics..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020c.
bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..zhuosheng zhang, junjie yang, and hai zhao.
2021.retrospective reader for machine reading compre-in the thirty-fifth aaai conference onhension.
artiﬁcial intelligence (aaai-21)..yuanyuan zhao, nan jiang, weiwei sun, and xiao-jun wan.
2018. overview of the nlpcc 2018shared task: grammatical error correction: 7thccf international conference, nlpcc 2018, ho-hhot, china, august 26–30, 2018, proceedings, partii, pages 439–445..xuhui zhou, yue zhang, leyang cui, and dandanevaluating commonsense in pre-huang.
2020.in the thirty-fourthtrained language models.
aaai conference on artiﬁcial intelligence, aaai2020, the thirty-second innovative applications ofartiﬁcial intelligence conference, iaai 2020, thetenth aaai symposium on educational advancesin artiﬁcial intelligence, eaai 2020, new york, ny,usa, february 7-12, 2020, pages 9733–9740.
aaaipress..6023a.2 training setting.
table 5: training details for the error correction task..a appendix.
albertzhlarge.
robertazhlarge.
modelmodel sizelearning ratebatch sizeoptimizeradam β1adam β2adam (cid:15)max epochsloss functiondropout.
bertzhbase2 × 10−58adam0.90.981 × 10−850cross-entropy0.1.table 4: training details for the error type classiﬁca-tion task..modelarchitecturelearning ratebatch sizeoptimizeradam β1adam β2adam (cid:15)max epochsloss functiondropout.
bertzh gecbert-wwm-ext.
robertazh gecroberta-wwm-ext-large.
transformer (big)3 × 10−516adam0.90.981 × 10−850label smoothed cross-entropy ((cid:15)ls = 0.1)0.3.modellearning ratebatch sizeoptimizeradam β1adam β2adam (cid:15)max epochsdropout.
nezha-gen5 × 10−54adam0.90.9991 × 10−630.1.table 6: training details for the rationale generationtask..a.3 examples of level-2 error types.
table 7 shows examples of level-2 error types intgea..a.1 nezha-gen hyperparameters.
table 1 show the conﬁguration of the generativemodel (nezha-gen)..modelhidden sizenum hidden layersnum attention headsintermediate sizehidden acthidden dropout probattention probs dropout probmax position embeddingstype vocab sizeinitializer range.
nezha-gen76812123072gelu0.10.1512160.02.table 1: conﬁguration of nezha-gen..table 2, 3, 4, 5, 6 show the training settingsof the baseline models for each task.
in thesetables, albertzh, bertzh, robertazh rep-resent albert-chinese, roberta-wwm-ext androberta-wwm-ext respectively..modelmodel sizelearning ratebatch sizeoptimizeradam β1adam β2adam (cid:15)max epochsloss functiondropout.
large.
large.
albertzh bertzh robertazhbase2 × 10−58adam0.90.981 × 10−850cross-entropy0.1.table 2: training details for the erroneous text detec-tion task..modelmodel sizelearning ratebatch sizeoptimizeradam β1adam β2adam (cid:15)max epochsloss functiondropout.
base.
base.
albertzh bertzh robertazhbase2 × 10−532adam0.90.9991 × 10−65cross-entropy0.1.table 3: training details for the erroneous and associ-ated span detection task..6024level-1 error type.
level-2 error type.
subject-predicate.
inappropriatecombination.
predicate-object.
[of.
(cid:58)[文明]。.
(cid:58)(cid:58)(cid:58)矛盾 [问题]。.
感受 [氛围]。(cid:58)(cid:58)(cid:58).
contradiction [problem] of coal mine safety..关心[]关心孩子成长的人的共同心声。(cid:58)(cid:58).
对 [因为]自身的过错作出了自己应当承担的责任。.
especially (cid:58)(cid:58)(cid:58)foreign(cid:58)(cid:58)(cid:58)banks[], still have many misunderstandings or prej-.
players [task] of women’s football team is a ball, and playing the ball well is their biggest.
(cid:58)[在]上一届奥运会夺得冠军，并且获得当年世界锦标杯赛金牌。.
，尤其是外资银行[]，对我国民营经济的发展还有不少误解或偏见。(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58).
example目前,该市的小说 [话剧]《我是党员、我的团员》、《我是小老头》、《小小老师》、《小(cid:58)(cid:58)(cid:58)小一个农家娃》正在上演。at present, the city’s (cid:58)(cid:58)(cid:58)novels [drama] i am a party member and this is my league member, little oldman like me, little teacher, a little farm boy are on stage.
由我主持，我要带大家去感受一下大赛主题设置的as a host, i will take you to experience the (cid:58)(cid:58)feel [atmosphere] shown from the theme of the competi-tion.
女足的队员 [任务] 就是一个球，能够把球踢好，就是她们最大的资本。(cid:58)(cid:58)the (cid:58)(cid:58)(cid:58)(cid:58)capitals.
另一方面，煤炭企业面临着煤矿安全的on the other hand, coal enterprises are facing the (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)因此，我 (cid:58)therefore,(cid:58)to [because of] my own fault, i took my own responsibility.
当他回到车间时，(cid:58)[车间]已经有了明显的变化。when he returned to the workshop,(cid:58)[the place] had been a marked change这时候我们一开始就有机会扳平比分，但是我们没有(cid:58)[抓住]机会。we had a chance to equalise at the beginning, but we didn’t (cid:58)[caught] chance.
一、坚持解放思想,转变观念,推进社会主义物质文明和精神1. persisting in emancipating the mind, changing ideas and promoting socialist material civilizationand spiritual(cid:58) [civilization].
在国内成立水牛研究中心，有利于增强(cid:58) [水牛对]自然条件和人工环境的适应能力。the establishment of buffalo research center in china is conducive to enhance the adaptabilitybuffalo] to natural conditions and artiﬁcial environment.
他的儿子his son won champion(cid:58) [in] the last olympic games and won the gold medal in the world champi-onship cup that year.
但一些外资银行however, some foreign banks(cid:58), (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)udices about the development of china’s private economy.
这也是所有this is also the common voice of all those who (cid:58)(cid:58)care(cid:58)(cid:58)(cid:58)about[] care about children’s growth同时，学校也开展丰富多彩、有益于学生的社会实践活动活。at the same time, the school also carries out colorful and beneﬁcial social practice activities, (cid:58)(cid:58)(cid:58)socialpractice[] to enrich their after-school life.
(cid:58)(cid:58)(cid:58)(cid:58)它们的皮毛很有光泽,可以用肉眼(cid:58)(cid:58)(cid:58)很难[]看出来。their fur is so shiny that we can see with naked eyes (cid:58)(cid:58)(cid:58)hardly[].
他是被迫进入位于市中心的一个警察局的，威吓。he was forced into a police station in the center of the city, (cid:58)(cid:58)then[] he was taken to the police station,where he was intimidated by handcuffs and police dogs.
在婚姻变得更为不好的时候，对她来说这是痛苦的。但是当(cid:58)(cid:58)她[它]发生变化时，她必须做出调整。it was painful for her when the marriage got worse.
but when (cid:58)(cid:58)she [it] changed, she had to adjust.
他说,中美两国是he said that china and the united states are close (cid:58)(cid:58)(cid:58)(cid:58)cooperation.
国庆 [元旦]假期期间，各大汽车经销商将会以怎么样的姿态迎接新的一年？(cid:58)(cid:58)during the(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)而在4月份，中国石化、招商银行、万科、上海汽车、g长安和g天威成为了最活跃的股票。in april, sinopec, china merchants bank, vanke, saic, g changan and g tianwei became the mostactive (cid:58)5 [6] stocks.
近日，李老的胃疼难忍，为治疗病情已连续就是几天。recently, lao li’s stomach ache is unbearable.
he has been (cid:58)(cid:58)(cid:58)(cid:58)days to treat his illness, and his illness is very serious.
he has been lying down for several days.
对于学校为了保障广大师生员工的安全，采取这些措施，我们深感we are very (cid:58)(cid:58)sorry [pleased] that the school has taken these measures to ensure the safety of students,teachers, and other staff.
低廉[高昂]，子淇在国内是很少有人请得到的大牌艺人之一。据悉，由于身价(cid:58)(cid:58)(cid:58)it is reported that ziqi is one of the few famous artists that are difﬁcult to invite in china because ofhis (cid:58)low [high] value.
(cid:58)酱 [花生] 油是植物油中的一种，食用后可以对皮肤有非常好的润泽效果。(cid:58)(cid:58)soy (cid:58)(cid:58)(cid:58)sauce [peanut oil] is a kind of vegetable oil, which has a very good moisturizing effect on theskin after eating.
一位中国官员表示：我们将在近期和俄罗斯动，以此来缓解人们对恐怖主义威胁的忧虑。in the near future, we will work with russia, (cid:58)(cid:58)(cid:58)china [france] and other countries to further promotethis series of actions to ease people’s concerns about the threat of terrorism, a chinese ofﬁcial said..national (cid:58)(cid:58)day [new year’s day] holiday, how will major auto dealers greet the new year?
(cid:58)5 [6]只.
工作 [休息]两天了，而且病情非常严重，他一躺(cid:58)(cid:58).
随后[]他被带到警察局，并遭到了手铐和警犬的(cid:58)(cid:58).
、中国 [法国] 等国合作进一步推广这一系列行(cid:58)(cid:58)(cid:58)(cid:58).
(cid:58)(cid:58)近邻 [朋友],关系很好,中美合作富有创造性。.
、社会实践[]，丰富他们的课余生(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58).
neighbors [friends] with good relations and creative.
working [resting] for two consecutive.
遗憾[欣慰]。(cid:58)(cid:58).
subject-object.
modiﬁer.
function word.
subject.
predicate.
object.
modiﬁer.
subject.
predicate.
object.
function word.
modiﬁer.
function word.
space.
time.
number.
motivation.
causation.
taxonomy.
behaviors.
misssing.
redundancy.
discourseerror.
coreference.
commonsenseerror.
emotional reactions.
table 7: examples of level-2 error types in tgea.
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)words are associated words.
words in ”[]” are corrections to erroneous words..underwaved (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)words are erroneous words while underlined.
6025