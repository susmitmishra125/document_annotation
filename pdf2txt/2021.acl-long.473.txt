capturing relations between scientiﬁc papers:an abstractive model for related work section generationxiuying chen123, hind alamro34, mingzhe li12,shen gao1, xiangliang zhang3∗, dongyan zhao125∗, rui yan61wangxuan institute of computer technology, peking university, beijing,china2center for data science, peking university, beijing, china3king abdullah university of science and technology, thuwal, saudi arabia4computer science department, umm al-qura university, makkah, saudi arabia5state key laboratory of media convergence production technology and systems6gaoling school of artiﬁcial intelligence, renmin university of chinaxy-chen@pku.edu.cn.
abstract.
given a set of related publications, relatedwork section generation aims to provide re-searchers with an overview of the speciﬁc re-search area by summarizing these works andintroducing them in a logical order.
most ofexisting related work section generation mod-els follow the inﬂexible extractive style, whichdirectly extract sentences from multiple origi-nal papers to form a related work discussion.
hence, in this paper, we propose a relation-aware related work generator (rrg), whichgenerates an abstractive related work sectionfrom multiple scientiﬁc papers in the same re-search area.
concretely, we propose a relation-aware multi-document encoder that relates onedocument to another according to their con-tent dependency in a relation graph.
the rela-tion graph and the document representation in-teract and are reﬁned iteratively, complement-ing each other in the training process.
wealso contribute two public datasets composedof related work sections and their correspond-ing papers1.
extensive experiments on the twodatasets show that the proposed model bringssubstantial improvements over several strongbaselines.
we hope that this work will pro-mote advances in related work section gener-ation task..1.introduction.
the related work section generation task aims toautomatically generate a summary of the most rele-vant works in a speciﬁc research area, which canhelp researchers to familiarize themselves withthe state of the art in the ﬁeld.
several methods(hoang and kan, 2010; hu and wan, 2014; chenand zhuge, 2019) have been proposed to study howto obtain the related work section automatically by.
∗ corresponding author.
1https://github.com/iriscxy/.
relatedworkgeneration.
extractive related work: we ﬁnd that crispr/cas9 canrobustly and speciﬁcally reduce the expression of these mi-crornas up to 96% [1].
we ﬁnd that mirna knockdownphenotypes caused by crispr/cas9 transient editing canbe stably maintained in both in vitro and in vivo modelsfor a long term (up to 30 days) [2].
although genome edit-ing using the crispr-cas system is highly efﬁcient inhuman cell lines, crispr-cas genome editing in primaryhuman cells is more challenging [3]..abstractive related work: recently, [1] showed thatcrisper-cas9 targeted mirna-17, mirna-200c andmirna-141, repressed their activity in human colon can-cer cell lines hct116 and ht-29.
furthermore, in vivotargeting was effective for at least a month [2].
however,off-target mutagenesis and effects of a single mirna onvarious gene targets are the limitations to the use of thismodern technology speciﬁcally in brain disorders likeprion diseases [3]..table 1: comparison of a related work paragraph gen-erated by an extractive method (human-annotated) andan abstractive man-made related work paragraph withthe same multiple original papers..extracting important sentences from multiple orig-inal papers.
however, extractive approaches lackthe sophisticated abilities that are crucial to high-quality summarization such as paraphrasing andgeneralization, and often lead to a related work sec-tion with poor coherence and readability (see et al.,2017; hsu et al., 2018).
for example, as shown intable 1, the extracted sentences share the pattern“we ﬁnd...” as the subject of sentences, which, asa matter of fact, refer to different authors.
on thecontrary, the abstractive related work in table 1reveals that the works are conducted by differentscholars.
it also has conjunction words such as“furthermore” and “however”, which can explainthe logical relationship between the cited works,and thus form an elegant narration.
hence, in thispaper, we target on the abstractive related workgeneration task, which generates a related workincluding novel words and phrases not copied fromthe source text..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6068–6077august1–6,2021.©2021associationforcomputationallinguistics6068there are two main challenges in this task: (1)the related work should summarize the contribu-tion of each paper, and (2) explain the relationshipbetween different papers such as parallel, turning,and progressive relation, so as to introduce themin a logical order.
while existing summarizationmodels can address the ﬁrst problem, they do nottarget at comparing and explaining the relation-ship between these articles.
hence, to tackle theabove challenges, we propose a relation-awarerelated work generator (rrg), which generatesan abstractive related work given multiple scien-tiﬁc papers in the same research area.
firstly, weencode the multiple input articles in a hierarchi-cal manner, obtaining the overall representationfor each document.
then, we propose a relation-aware multi-document encoder that relates multipleinput documents in a relation graph.
in the train-ing process, the relation graph and the documentrepresentation interact and are reﬁned iteratively,complementing each other.
finally, in the decoderpart, we utilize the relation graph information toassist the decoding process, where the model learnsto decide whether to pay attention to the input doc-uments or the relationship between them..to evaluate our model, we introduce two large-scale related work generation datasets, which arecomposed of related work sections and their cor-responding papers.
extensive experimental resultsshow that rrg outperforms several strong base-lines in terms of rouge metrics and human eval-uations on both datasets..in summary, our contributions include:• we address an abstractive related work gener-ation task, which aims to generate an abstractiverelated work with novel words and phrases..• we propose a relation-aware multi-documentencoder that relates one of the multiple input docu-ments to another, and establishes a relation graphstoring the dependency between documents..• we contribute two public large-scale relatedwork generation datasets that are beneﬁcial for thecommunity..2 related work.
we discuss the related work on related work gener-ation and multi-document summarization..related work generation.
most of the previ-ous related work section generation methods areextractive.
for example, hoang and kan (2010)take in a set of keywords arranged in a hierarchical.
fashion to drive the creation of an extractive relatedwork.
later, (hu and wan, 2014) ﬁrst exploitsa probabilistic latent semantic analysis (plsa)model to split the sentence set of multiple referencepapers into different topic-biased parts, and thenapplies regression models to learn the importanceof the sentences.
finally, it employs an optimiza-tion framework to generate the related work section.
chen and zhuge (2019) propose to ﬁrst construct aminimum steiner tree of the keywords.
then thesummary is generated by extracting the sentencesfrom the papers that cite the reference papers of thepaper being written to cover the steiner tree..however, abstractive approaches on related workgeneration have met with limited success.
apartfrom the lack of sufﬁcient training data, neural mod-els also face the challenge of identifying the logicrelationship between multiple input documents..multi-document summarization.
the multi-document summarization task aims to cover thekey shared relevant information among all the docu-ments while avoiding redundancy (goldstein et al.,2000).
existing multi-document summarizationmethods are mostly extractive (christensen et al.,2013; parveen and strube, 2014; ma et al., 2016;chu and liu, 2018).
for example, wang et al.
(2020) present a heterogeneous graph-based neuralnetwork which contains semantic nodes of differentgranularity levels apart from sentences.
recently,a vast majority of the literature is dedicated to ab-stractive multi-document summarization.
lu et al.
(2020) propose a large-scale multi-document sum-marization dataset created from scientiﬁc articles.
jin et al.
(2020) propose a multi-granularity in-teraction network for extractive and abstractive ap-proaches.
li et al.
(2020a) develop a neural abstrac-tive multi-document summarization model whichleverages explicit graph representations of docu-ments to guide the summary generation process..while the multi-document summarization taskaims to extract information shared by multiple doc-uments, related work generation aims to compareand introduce the cited works in logic order..3 related work generation dataset.
since there are no public large-scale related workgeneration datasets, we collect two survey datasetscomposed of related work sections and their corre-sponding papers.
the ﬁrst dataset is collected froms2orc (lo et al., 2020), which consists of papersin multiple domains (physics, math, computer sci-.
6069dataset.
s2orcdelvemulti-newsrwsduc03+04tac 2011.
# pairs(train/valid/test)126,655/5,000/5,00072,927/3,000/3,00044,972/5,622/5,62225320176.
# source(articles)5.023.692.789.471010.
# words(doc)1,0796262,1035,4964,6364,695.
# sents(docs)452682237173188.
# words(summary)14818126336710999.
# sents(summary)6.697.889.9718.282.881.00.vocab size.
377,431190,381666,51515,01919,73424,672.table 2: comparison of our s2orc and delve dataset to other related work and multi-document datasets.
training,validation, and testing size splits are provided when applicable.
statistics for multi-document inputs are calculatedon the concatenation of all input sources..ence, etc.
), and the second is delve (akujuobi andzhang, 2017), which consists of computer sciencepapers.
all the papers in each of these two datasetsform a large connected citation graph, allowingus to make full use of the citation relationshipsbetween papers..dataset preprocessing.
for each case, the gen-eration target is a paragraph with more than twocitations, as a comprehensive related work usu-ally compares multiple works under the same topic.
the abstract of each cited paper is regarded as input,considering that the main idea of a cited paper isdescribed in its abstract.
we then conduct a humanevaluation to examine the dataset quality.
con-cretely, we sample 200 cases from both datasetsand ask three annotators to state how well theyagree with the following statement, on a scale ofone to three (disagree, neutral, agree): the relatedwork can be partly generated based on the givenabstracts of the cited papers.
the evaluation is con-ducted on the amazon mechanical turk, which hasbeen employed in a variety of nlp tasks includingsummarization (liu and lapata, 2019a), questionanswering (gan and ng, 2019), and dialog system(li et al., 2020b).
the result shows that 94.5%cases win 3 scores, while only 3.5% cases obtain1 score.
this demonstrates the good quality of thedatasets..statistics.
table 2 compares delve and s2orcto other public datasets including duc data from2003 and 2004, tac 2011 data, and multi-news,which are typically used in multi-document set-tings.
we also list the statistics of a recent relatedwork generation dataset rws, which is proposedby chen and zhuge (2019).
the total number ofcollected samples for the s2orc and delve isabout 150,000 and 80,000, respectively.
it can beseen that multi-news is most similar to our datasetdue to its large-scale.
however, the average num-ber of documents per case in multi-news is smaller.
than ours..4 problem formulation.
before presenting our approach for related workgeneration, we ﬁrst introduce our problem formu-lation and used notations..1, wi.
2, · · · , wini.
to begin with, for a set of relevant papersd = (d1, d2, · · · , dn ) in a speciﬁc area, wheredi denotes a paper, we assume there is a cor-responding related work y = (y1, y2, · · · , yt ).
n is the number of relevant papers, and di =), where wi(wij is the j-th word ini-th paper, and ni is the number of words in di.
tis the number of words in a related work.
given themultiple papers d, our model generates a relatedwork ˆy = (ˆy1, ˆy2, · · · , ˆy ˆt ).
finally, we use thedifference between generated related work ˆy andground truth related work y as the training signalto optimize the model parameters..5 the proposed rrg model.
5.1 overview.
in this section, we introduce the relation-awarerelated work generator (rrg) in detail.
anoverview of rrg is shown in figure 1, which hasthree main parts:.
• hierarchical encoder reads multiple input doc-uments and learns the multi-level representationsfor words and documents..• relationship modeling relates one paper to.
another and obtains their relationship graph..• related work generator produces the abstrac-tive related work by attending to the hierarchicalrepresentations and the relation graph between doc-uments..5.2 hierarchical encoderto begin with, each input wivector representation ˆei.
j is converted into thej by the learned embeddings..6070figure 1: overview of rrg, which consists of three parts: (1) hierarchical encoder encodes the multiple inputs inhierarchical levels; (2) relationship modeling relates one paper to another and stores their relation graph; and (3)related work generator generates the related work by attending to input documents and the relationship betweenthem..we then assign positional encoding (p e) to indi-cate the position of the word wij where two po-sitions need to be considered, namely documentindex i and word index j. we concatenate the posi-tion embedding p ei, p ej to obtain the ﬁnal posi-tion embedding pij. the deﬁnition of positional en-coding is consistent with the transfomer (vaswaniet al., 2017).
the input word representation eijis obtained by adding embedding ˆeij and positionembedding pij..we then perform multi-head self-attention acrossthe word representations in the same document toobtain the contextual word representation hwi.
:.
hwi.
j.
= mham(ei.
j, ei.
∗),.
j.
(1).
where mham denotes the multi-head attentionmodule (vaswani et al., 2017), and ∗ denotes indexj ∈ (1, ni).
concretely, the ﬁrst input is for queryand the second input is for keys and values.
eachoutput element, hwi, is computed as the weightedjsum of linearly transformed input values:.
= (cid:80)ni.
hwi.
j.l=1 αij,l(cid:16).
exp.
(cid:1) ,.
(cid:0)eilw vw(cid:17).
βij,l(cid:16).
(cid:17) ..(cid:80)ni.
k=1 exp.
βi.
j,k.
αi.
j,l =.
(2).
(3).
here, βition that compares two input elements:.
j,l is computed using a compatibility func-.
βij,l =.
(cid:16).
jw qei.
w.(cid:1)t.lw kw.(cid:17) (cid:0)ei√.
d.,.
(4).
the.
hidden.
where d isandw qw , w vw , w kw are parameter matrices.
from theword-level representation we obtain the overallrepresentation for each document:.
dimension,.
h0di.
= meanpool.
(cid:16)(cid:110).
hwi.
1., · · · , hwini.
(cid:111)(cid:17).
..(5).
5.3 relationship modelingthe document representation h0does not containdicross-document information, thus, it cannot learnricher structural dependencies among textual units.
in this subsection, we introduce a novel graph-based relationship modeling (rm), which not onlyallows sharing information across multiple docu-ments but also models the logic dependency be-tween documents.
note that it is impossible to ex-plicitly list all the relationships between documentsbecause the relationships vary from document pairto pair depending on the document content, andthe content of documents is unlimited.
hence, wemodel the relationships hidden vectors and let themodel capture such diverse relationships by thehidden vectors.
concretely, since the relationshipgraph is constructed based on the representation ofeach document, while a comprehensive documentrepresentation should consider its relationship withother documents.
these two processes complementeach other.
hence, our rm module is an iterativemodule, which has a stack of l identical layers.
in each layer, we iteratively update the relation-ship graph, and then fuse the information from thegraph to the document representation, as shown infigure 2..6071related workgeneratorhierarchicalencoderdocument-level attentionword-level attention.........relationshipmodelingtransformertransformertransformertoken position encodingvocab distribution......weightedreadxl...relation graphupdaterrelation-awareattention moduledoc1doc2doc3relation-aware document...polished relation graphrelation graphmemory state.
this update strategy is conceptu-ally similar to long short-term memory (lstm)(hochreiter and schmidhuber, 1997).
it differs inthat multi-head attention is used and thus multi-ple graph slots are supported instead of a singleone in lstm, which gives it a higher capacity ofmodeling complex relations..next, the updated graph is fused in the relation-aware attention module (ram) to update the doc-ument representation:.
hldi.
= ram(hl−1di.
, hl−1d∗.
, hl.
ri,∗)..(8).
ram is similar to mham, where hl−1is for query,dihl−1is for key and value.
however, there are twod∗changes in equation 2 and equation 4. speciﬁcally,we modify equation 2 to propagate edge informa-tion to the sub-layer output:.
hldi.
= (cid:80)n.j=1 αl−1,r.
i,j.
hl−1dj.
w v.r + hl.
ri,j.
(cid:16).
(cid:17).
..(9).
in this way, the representation of each document ismore comprehensive, consisting of its relation de-pendency information with other documents.
whatis more, when deciding the weight of each edge, i.e.,βl−1,r, we also incorporate relation edge informa-i,jtion, since close relationships such as succession ortransition can have a great impact on edge weight.
concretely, equation 4 is changed to:.
βl−1,ri,j =.
(cid:16).
hl−1di.
w qr.(cid:17) (cid:16).
w k.r + hl.
ri,j.
(cid:17)t...hl−1dj√.
d.(10)we summarize the whole relationship modeling.
process as:.
d , hlhl.
r = rm(h0.
d, h0.
r)..(11).
for brevity, we omit the subscript l in the follow-ing section..5.4 related work generator.
to generate a consistent and informative sum-mary, we propose an rnn-based decoder follow-ing (chen et al., 2019; gao et al., 2019) that incor-porates the outputs of the hierarchical encoder andthe relationship graph as illustrated in figure 1..our decoder is a single-layer unidirectionallstm.
at each step t, the decoder updates thehidden state from st−1 to st:.
st = lstm.
st−1,.
t−1, cdcw.
t−1, e(yt−1).
(cid:16).
(cid:104).
(cid:105)(cid:17).
..(12).
figure 2: framework of the relationship modeling,which consists of a relation graph updater (in the rightcyan part), and a relation-aware attention module (inthe left gray part)..for a start, the relation edge in our graph is ini-.
tialized by the document representation:.
ri,j = mlpa([h0h0di.
; h0dj.
]),.
(6).
where mlp is a multi-layer perceptron, and [; ] isthe concatenation operation..in each iteration, we ﬁrst propose a relationgraph updater (rgu) to renew the graph based onthe polished document representation so far (shownin the right part of figure 2):.
ri,j = rgu(hl−1hl.
ri,j , hl−1d∗.
)..(7).
here, ∗ denotes index i ∈ (1, n ), meaning thatall document representations will be involved inupdating the relation graph.
concretely, rgu ﬁrstaggregates the information from both the previousri,j and the document states hl−1graph hl−1from thed∗last layer, using a multi-head attention (mham in-trodced in §5.2).
the input for query q is hl−1ri,j , andinput for key k and value v is hl−1.
the outputd∗intermediate graph states sl−1i,j are further encodedusing a feed-forward layer and then merged withthe intermediate hidden states hl−1ri,j using a residualconnection and layer norm..we summarize the procedure below:.
sl−1i,j = mham(hl−1cl−1a hl−1i,j = tanh(w l−1zl−1i,j = sigmoid(w l−1ri,j = (1 − zl−1hl.
ri,j , hl−1),d∗ri,j + w l−1c hl−1i,j ) (cid:12) cl−1.
sl−1i,j ),ri,j + w l−1di,j + zl−1.
sl−1i,j ),i,j (cid:12) hl−1ri,j ,.
b.where (cid:12) denotes hadamard product, and cl−1is thei,jinternal cell state.
zl−1is the update gate that con-i,jtrols which information to retain from the previous.
6072lineardocumentmatrix multiplyfeed forwardrelation-awaredocument dot product attrelation graphlinearadd&normxllinearrelation graphupdatermulti-headattentionvkqlinearlinearlinearlinearadd&normadd&normsigmoidtanhpolished graphfollowing previous works (bahdanau et al., 2015),we employ an attention mechanism to compute theattention distribution over the source words in thesequence-to-sequence structure:.
(cid:16).
αw(cid:48),it,j = w gαw,i.
t,j = expt = (cid:80)ncw.
a tanh(cid:16)αw(cid:48),it,j(cid:80)ni.
i=1.
j=1 αw,i.
t,j hwi.
,.
j.w g(cid:17).
b st + w g/ (cid:80)ni.
l=1 exp.
c hwi(cid:16).
j.
(cid:17).
,.
(13).
(cid:17).
αw(cid:48),it,l.
,.
(14).
where cwt denotes word context vector.
similarly,we extend the attention mechanism to documentlevel:.
(cid:16).
(cid:17).
(cid:17).
w g.e st + w gf hdi(cid:16).
/ (cid:80)n.l=1 exp.
αd(cid:48)t,l.
,(cid:17).
,.
αd(cid:48)t,i = w g.αd.
t,i = expt = (cid:80)ncd.
d tanh(cid:16)αd(cid:48)t,ii=1 αd.
t,ihdi..(15).
(16).
(17).
(18).
the encoded relationship information is also im-portant for facilitating the transition introductionin the related work, and the speciﬁc informationin the graph that is needed at each step dependson which document is being introduced.
hence,we employ the document-level attention weights inequation 17 to read the relationship graph:= meanpool (cid:0)(cid:8)hri,1, · · · , hri,nhrmt = (cid:80)ncr..i=1 αd.
t,ihrm.
(cid:9)(cid:1) ,.
(19).
i.i.finally, an output projection layer is applied tot over vocab-.
get the ﬁnal generating distribution p vulary, as shown in equation 20:.
t = softmax(mlpc[st; cwp v.t ; cd.
t ; cr.
t ])..(20).
our objective function is the negative log likeli-.
hood of the target word yt:.
l = − (cid:80)t.t=1 log p v.t (yt)..(21).
in order to handle the out-of-vocabulary (oov)problem, we equip our decoder with a pointer net-work (gu et al., 2016; see et al., 2017).
this pro-cess is the same as the model described in (seeet al., 2017), thus, is omit here due to limited space..6 experimental setup.
6.1 baselines.
to evaluate the performance of our proposed model,we compare it with the following baselines:extractive methods:.
(1) lead: selects the ﬁrst sentence of each doc-ument as the summary as a baseline.
(2) textrank(mihalcea and tarau, 2004): is a multi-document(3) bertsumextgraph-based ranking model.
(liu and lapata, 2019b): is an extractive summa-rization model with bert.
(4) mgsum-ext (jinet al., 2020): is a multi-granularity interaction net-work for extractive multi-document summariza-tion.
abstractive methods:.
(1) ptgen+cov: combines the sequence-to-sequence framework with copy and coverage mech-anism in summarization task (see et al., 2017).
(2) transformerabs: is an abstractive summa-rization model based on the transformer (vaswaniet al., 2017).
(3) bertsumabs (liu and lapata,2019b): is an abstractive summarization networkbuilt on bert.
(4) mgsum-abs (jin et al., 2020):is a multi-granularity interaction network for ab-stractive multi-document summarization.
(5) gs(li et al., 2020a):is a neural abstractive multi-document summarization model that leverageswell-known graphs to produce abstractive sum-maries.
we use the tf-idf graph as the inputgraph..6.2.implementation details.
we implement our model in tensorflow (abadiet al., 2016) on an nvidia gtx 1080 ti gpu.
for all the neural models, we truncate the input ar-ticles to 500 tokens in the following way: for eachexample with s source input documents, we takethe ﬁrst 500/s tokens from each source document.
the maximum document number is set to 5. theminimum decoding step is 50, and the maximumstep is 100. the word embedding dimension isset to 128 and the number of hidden units is 256.we initialize all of the parameters randomly usinga gaussian distribution.
the batch size is set to16, and we limit the vocabulary size to 50k.
weuse adagrad optimizer (duchi et al., 2010) as ouroptimizing algorithm.
we also apply gradient clip-ping (pascanu et al., 2013) with a range of [−2, 2]during training.
for the testing, we employ beamsearch with a beam size of 4 to generate more ﬂuentsummaries..to obtain the extractive oracle, since it is com-putationally expensive to ﬁnd a globally optimalsubset of sentences that maximizes the rougescore, we employ a greedy approach, where weadd one sentence at a time incrementally to the.
6073models.
oracle ext.
sentence extraction methods.
leadtextrank (mihalcea and tarau, 2004)bertsumext (liu and lapata, 2019b)mgsum-ext (jin et al., 2020).
abstractive methods.
ptgen+cov (see et al., 2017)transformerabs (vaswani et al., 2017)bertsumabs (liu and lapata, 2019b)mgsum-abs (jin et al., 2020)gs (li et al., 2020a)rrg.
ablation modelsrrg w/o pprrg w/o rmrrg w/o upd.
s2orc dataset.
delve dataset.
rg-1.
rg-2 rg-l.rg-1.
rg-2 rg-l.38.68.
7.23.
34.31.
38.07.
7.21.
33.27.
20.6022.3624.6224.10.
23.5421.6523.6323.9423.9225.46.
24.8024.3224.58.
2.052.653.623.19.
4.383.644.174.584.514.93.
4.754.504.71.
16.5019.7321.8820.87.
21.1820.4321.6921.5722.0522.97.
22.3021.9522.11.
23.1825.2528.4327.85.
27.5426.8928.0228.1328.2729.10.
28.8928.4028.79.
2.303.043.983.95.
4.093.923.504.124.364.94.
4.644.014.13.
19.0922.1424.7124.28.
24.1223.6424.7424.9525.0826.29.
25.6025.1225.30.table 3: rouge scores comparison between rrg and baselines.
all our rouge scores have a 95% conﬁdenceinterval of at most ±0.22 as reported by the ofﬁcial rouge script..summary, such that the rouge score of the cur-rent set of selected sentences is maximized withrespect to the entire gold summary..7 experimental results.
7.1 automatic evaluation.
following chen et al.
(2018), we evaluate sum-marization quality using rouge f1 (lin, 2004).
we report unigram and bigram overlap (rouge-1and rouge-2) to assess the informativeness andthe longest common subsequence (rouge-l) asa means of the assessing ﬂuency..table 3 summarizes our results.
the ﬁrst blockin the table includes extractive systems, and the sec-ond block includes abstractive baselines.
as canbe seen, abstractive models generally outperformextractive ones, especially in terms of rouge-lscores.
we attribute this result to the observationthat the gold related work of this dataset tends touse novel word combinations to summarize theoriginal input documents, which demonstrates thenecessity of solving the abstractive related workgeneration task.
among abstractive models, sur-prisingly, bertsumabs does not perform as wellas other state-of-the-art baselines.
this is probablybecause bert does not ﬁt well on scholar datathat have technical terms.
finally, our model rrggains an improvement of 1.83 (1.08) points com-pared with bertsumabs, 1.54 (0.83) points com-pared with gs on rouge-1 on s2orc (delve),.
qa(%).
inform.
coh.
succ.
bertsumabsmgsum-absgsrrg.
26.829.932.838.8.
1.862.032.232.37.
1.931.962.062.16.
1.801.902.032.10.table 4: model scores based on questions answered byamt participants and summary quality rating..verifying the effectiveness of our rrg..table 3 also summarizes ablation studies aimingto assess the contribution of individual componentsin our rrg model.
the results conﬁrm that theencoding paragraph position in addition to tokenposition within each paragraph is beneﬁcial (seerow w/o pp), as well as relationship modeling (roww/o rm).
updating the relation graph also helpsthe summarization process, where removing theupdate mechanism causes rouge-l drop by 0.86(0.99) (row w/o upd) on s2orc (delve) dataset..7.2 human evaluation.
we also assessed the generated results by elicit-ing human judgments on 30 randomly selected testinstances from delve dataset.
our ﬁrst evaluationstudy quantiﬁed the degree to which summarizationmodels can retain the key information followinga question-answering paradigm (liu and lapata,2019a).
we created a set of questions based onthe gold-related work and examined whether par-ticipants were able to answer these questions byreading generated related works.
the principle.
6074log.d given a set of annotated images as training data , many methods have been proposed in the literature to ﬁnd mostrepresentative keywords to annotate new images [1] [2] .
however , in most cases , the labeled data are insufﬁcient.
compared to the large size of an image or video data set , the annotated images have a relative small number .
thesemisupervised learning techniques leverage the unlabeled data in addition to labeled data to tackle this difﬁculty [3] [4] ..a are labeled data large enough for image annotation in most cases?
[no]q.what techniques have been proposed to leverage the unlabeled data?
[semisupervised techniques]we introduce a new method to automatically annotate and retrieve images using a vocabulary of image semantics [1] .
we evaluate the innovative sdf-based approach on corel images compared with support vector machine-based approach.
in this paper , we propose a novel scheme that exploits both semi-supervised kernel learning and batch mode activelearning for relevance feedback in cbir [3] .
this paper presents a novel semi-supervised learning method which combinesthe power of learned similarity functions and classiﬁers [4] ..txe-.
m.s active learning methods have been widely used in computer vision tasks , including speech recognition and computerbavision [1] [2] .
in the context of machine learning , there has been substantial research effort in the ﬁeld of active learning-m[3] .
[4] address the problem of active learning to a set of labeled data based on the idea of boosting ..s the problem of image annotation has been studied extensively in recent years [1] [2] .
in contrast , the majority ofwork on image annotation has focused on reducing the amount of the number of required training samples , such asgsemi-supervised learning ( [3] ) , support vector machine ( svm [4] ) ..g there has been a lot of work on image annotation learning [1] [2] .
however , most of the existing methods is notrapplicable when training data size is small .
[3] propose a semi-supervised learning algorithm for active learning such asrlocal svm.
[4] address the problem of active learning to a set of labeled data ..table 5: gold human authored summaries, questions based on them (answers shown in square brackets) and auto-matic summaries produced by mgsum-ext, mgsum-abs, gs, and our rrg.
blue denotes inconsistent sentences,while pink denotes relation conjunction that explains the relationship between different works..for writing a question is that the information tobe answered is about factual description, and isnecessary for a related work section.
two ph.d.students majoring in computer science (also the au-thors) wrote ﬁve questions independently for eachsampled ground truth related work since the delvedataset also consists of computer science papers.
then they together selected the common questionsas the ﬁnal questions that they both consider to beimportant.
finally we obtain 67 questions, wherecorrect answers are marked with 1 and 0 otherwise.
examples of questions and their answers are givenin table 5. our second evaluation study assessedthe overall quality of the related works by askingparticipants to score them by taking into accountthe following criteria: informativeness (does therelated work convey important facts about the topicin question?
), coherence (is the related work coher-ent and grammatical?
), and succinctness (does therelated work avoid repetition?).
the rating scoreranges from 1 to 3, with 3 being the best.
for bothevaluation metrics, a model’s score is the averageof all scores..both evaluations were conducted on the amazonmechanical turk platform with 3 responses per hit.
participants evaluated related works produced bythe bertsumabs, mgsum-abs, gs, and our rrg.
all evaluated models are those who achieved thebest performance in automatic evaluations.
table 4lists the average scores of each model, showing thatrrg outperforms other baseline models among all.
metrics.
we calculate the kappa statistics in termsof informativeness, coherence, and succinctness,and the scores are 0.38, 0.29, 0.34, respectively.
toverify the signiﬁcance of these results, we also con-duct the paired student t-test between our modeland gs (the row with shaded background).
we ob-tain a p-value of 6 × 10−6, 5 × 10−9, and 7 × 10−7for informativeness, coherence, and succinctness.
examples of system output are provided in table 5.we can see that related work generated by rrgcorrectly captures the relationship between papers[1,2] and [3,4], and successfully summarizes thecontributions of corresponding papers.
amongbaselines, mgsum-ext fails to connect the citedpapers in logic.
mgsum-abs and gs fail to cap-ture the transitional relationship between the ﬁrsttwo works and the last two works..7.3 analysis of relation graph.
to fully investigate what is stored by the relationgraph, we draw a heatmap of the graph for the casein table 5. since the edge in relation graph is a vec-tor containing semantic meaning, which cannot bedirectly explained, we use the edge between paper[2] and [3] as a benchmark and compute the cosinesimilarity between the benchmark and other rela-tion edges.
dark color means that the relationshipbetween the corresponding two papers is similarwith edge [2]-[3], and vice versa.
we already knowthat there is a transitional relationship between [2]and [3], so if an edge has a high cosine similarity.
6075summarizing the related works and introducingthem in a logical order.
the positive impact liesin that it can help improve the work efﬁciency ofscholars.
the negative impact may be that in someextreme cases, the system may not be able to givean accurate and faithful related work, which canbe misleading.
hence, in such situation, scholarsshould not directly employ the generated relatedwork as the ﬁnal edition.
instead, they can rely onthis system to give insightful related work sugges-tion..references.
mart´ın abadi, paul barham, jianmin chen, zhifengchen, andy davis, jeffrey dean, matthieu devin,sanjay ghemawat, geoffrey irving, michael isard,et al.
2016. tensorﬂow: a system for large-scalein 12th {usenix} symposiummachine learning.
on operating systems design and implementation({osdi} 16), pages 265–283..uchenna akujuobi and x. zhang.
2017. delve: adataset-driven scholarly search and analysis system.
acm sigkdd explorations newsletter, 19:36–46..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
iclr..jingqiang chen and hai zhuge.
2019. automatic gen-eration of related work through summarizing cita-tions.
concurr.
comput.
pract.
exp., 31..xiuying chen, zhangming chan, shen gao, meng-hsuan yu, dongyan zhao, and rui yan.
2019.learning towards abstractive timeline summariza-tion.
in ijcai..xiuying chen, shen gao, chongyang tao, yan song,dongyan zhao, and rui yan.
2018. iterative docu-ment representation learning towards summarizationwith polishing.
in emnlp..janara christensen, stephen soderland, oren etzioni,et al.
2013. towards coherent multi-document sum-marization.
in proceedings of the 2013 conferenceof the north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 1163–1173..eric chu and peter j. liu.
2018. unsupervised neuralmulti-document abstractive summarization.
arxiv,abs/1810.05739..john c. duchi, elad hazan, and yoram singer.
2010.adaptive subgradient methods for online learningand stochastic optimization.
j. mach.
learn.
res.,12:2121–2159..figure 3: the similarity between each relation edgeand paper [3]-[4] edge.
the darker the color is, thehigher the similarity is..with [2]-[3] pair, then the two papers on this edgealso form a transitional relationship.
as shown infigure 3, relationship vectors between paper [1],[2] with [4] are relatively more similar to [2]-[3]pair.
this is consistent with the fact that paper[1] and [2] are parallel with each other, while theyform a transitional relationship compared with [4].
note that the heatmap is not symmetrical becauseour relation graph is a bipartite graph..8 conclusion.
in this paper, we conceptualized the abstractive re-lated work generation task as a machine learningproblem.
we proposed a new model that is ableto encode multiple input documents hierarchicallyand model the latent relations across them in a re-lation graph.
we also come up with two publiclarge-scale related work generation datasets.
ex-perimental results show that our model producesrelated works that are both ﬂuent and informative,outperforming competitive systems by a wide mar-gin.
in the future, we would like to apply our modelto abstract generation and paper generation tasks..acknowledgments.
we would like to thank the anonymous re-viewers for their constructive comments.
thiswork was supported by the national key re-search and development program of china (no.
2017yfc0804001), the national science founda-tion of china (nsfc no.
61876196 and nsfcno.
61672058).
rui yan is partially supported asa young fellow of beijing institute of artiﬁcialintelligence (baai)..ethics impact.
in this paper, we propose a relation-aware relatedwork generator which aims to provide researcherswith an overview of the speciﬁc research area by.
wee chung gan and h. t. ng.
2019. improving the ro-bustness of question answering systems to questionparaphrasing.
in acl..6076[1][2][3][4][1][2][3][4]0.20.40.60.81.0shen gao, xiuying chen, piji li, zhangming chan,dongyan zhao, and rui yan.
2019. how to writesummaries with patterns?
learning towards abstrac-tive summarization through prototype editing.
inemnlp..jade goldstein, vibhu o mittal, jaime g carbonell,and mark kantrowitz.
2000. multi-document sum-marization by sentence extraction.
in naacl-anlp2000 workshop: automatic summarization..jiatao gu, zhengdong lu, hang li, and victor okincorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640..cong duy vu hoang and min-yen kan. 2010. to-wards automated related work summarization.
incoling 2010: posters, pages 427–435..s. hochreiter and j. schmidhuber.
1997. long short-term memory.
neural computation, 9:1735–1780..wan-ting hsu, chieh-kai lin, ming-ying lee, keruimin, jing tang, and min sun.
2018. a uniﬁedmodel for extractive and abstractive summarizationin proceedings of theusing inconsistency loss.
56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages132–141..yue hu and xiaojun wan.
2014. automatic generationof related work sections in scientiﬁc papers: an opti-mization approach.
in proceedings of the 2014 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1624–1633..hanqi jin, tianming wang, and xiaojun wan.
2020.multi-granularity interaction network for extractiveand abstractive multi-document summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6244–6254..wei li, xinyan xiao, jiachen liu, hua wu, haifengwang, and junping du.
2020a.
leveraging graphto improve abstractive multi-document summariza-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 6232–6243..yu li, kun qian, weiyan shi, and z. yu.
2020b.
end-to-end trainable non-collaborative dialog system.
inaaai..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81..yang liu and mirella lapata.
2019a.
hierarchicaltransformers for multi-document summarization.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5070–5081..yang liu and mirella lapata.
2019b.
text summariza-tion with pretrained encoders.
in proceedings of the2019 conference on empirical methods in naturallanguage processing and the 9th international jointconference on natural language processing, pages3721–3731..kyle lo, lucy lu wang, mark neumann, rodney kin-ney, and daniel s weld.
2020. s2orc: the seman-tic scholar open research corpus.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 4969–4983..yao lu, yue dong, and laurent charlin.
2020. multi-xscience: a large-scale dataset for extreme multi-indocument summarization of scientiﬁc articles.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8068–8074..shulei ma, zhi-hong deng, and yunlun yang.
2016.an unsupervised multi-document summarizationframework based on neural document model.
in pro-ceedings of coling 2016, the 26th internationalconference on computational linguistics: techni-cal papers, pages 1514–1523..rada mihalcea and paul tarau.
2004..textrank:in proceedings of thebringing order into text.
2004 conference on empirical methods in naturallanguage processing, pages 404–411..daraksha parveen and michael strube.
2014. multi-document summarization using bipartite graphs.
inthe workshop onproceedings of textgraphs-9:graph-based methods for natural language pro-cessing, pages 15–24..razvan pascanu, tomas mikolov, and yoshua bengio.
2013. on the difﬁculty of training recurrent neuralin international conference on machinenetworks.
learning, pages 1310–1318.
pmlr..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..danqing wang, pengfei liu, yining zheng, xipengqiu, and xuan-jing huang.
2020. heterogeneousgraph neural networks for extractive document sum-marization.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6209–6219..6077