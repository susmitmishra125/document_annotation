discovering dialogue slots with weak supervision.
vojtˇech hudeˇcek,1 ondˇrej dušek1 and zhou yu21charles university, faculty of mathematics and physics,institute of formal and applied linguistics2 columbia university, department of computer science{hudecek,odusek}@ufal.mff.cuni.cz, zy2461@columbia.edu.
abstract.
task-oriented dialogue systems typically re-quire manual annotation of dialogue slots intraining data, which is costly to obtain.
wepropose a method that eliminates this require-ment: we use weak supervision from existinglinguistic annotation models to identify poten-tial slot candidates, then automatically identifydomain-relevant slots by using clustering algo-rithms.
furthermore, we use the resulting slotannotation to train a neural-network-based tag-ger that is able to perform slot tagging withno human intervention.
this tagger is trainedsolely on the outputs of our method and thusdoes not rely on any labeled data..our model demonstrates state-of-the-art per-formance in slot tagging without labeled train-ing data on four different dialogue domains.
moreover, we ﬁnd that slot annotations dis-covered by our model signiﬁcantly improvethe performance of an end-to-end dialogue re-sponse generation model, compared to usingno slot annotation at all..1.introduction.
task-oriented dialogue systems typically use anno-tation based on slots to represent the meaning ofuser utterances (young et al., 2013).
slots are at-tributes relevant to completing the task (e.g., price,food type, area).
the sets of slots and their val-ues typically need to be designed in advance bydomain experts.
slots and their values are trackedover the course of the dialogue, forming dialoguestate, which allows a dialogue system to plan thenext actions effectively (williams et al., 2013)..getting raw data for dialogue system trainingis not difﬁcult, especially if we restrict the tar-get domain.
a requirement for dialogue state la-bels makes this process much more costly.
how-ever, both traditional pipeline systems (young et al.,2013) and end-to-end task-oriented architectures.
(wen et al., 2017) typically require such annotation.
while some systems use implicit, latent state rep-resentation and do not require annotation (serbanet al., 2016), the behavior of such systems is hard tointerpret or control.
there are several works aimingat keeping interpretability and reducing the anno-tation needs by automating it (chen et al., 2014,2015) or transferring annotation across domains(zhao and eskenazi, 2018; coope et al., 2020), butthey still require signiﬁcant manual effort..in this paper, we present a novel approach todiscovering a set of domain-relevant dialogue slotsand their values given a set of dialogues in thetarget domain (such as transcripts from a call cen-ter).
our approach requires no manual annotationat all in order to tag slots in dialogue data.
thissubstantially simpliﬁes dialogue system design andtraining process, as the developer no longer needsto design a set of slots and annotate their occur-rences in training data.
we discover slots by usingunsupervised clustering on top of annotation ob-tained by domain-independent generic models suchas a semantic frame parser or a named entity rec-ognizer (ner).
to illustrate our approach, let usconsider an example given in figure 1..figure 1: an utterance from the restaurant recommen-dation domain tagged with off-the-shelf frame seman-tic parser.
some tags are domain-relevant (shown inblue), but some are not (shown in gray)..although the annotation is descriptive, it con-tains concepts irrelevant for the domain under con-sideration.
our method selects only relevant slotcandidates (depicted in blue).
slots discovered byour approach can then be used to design or adaptthe database backend for the target domain..our contributions can be summarized as follows:.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2430–2442august1–6,2021.©2021associationforcomputationallinguistics2430find a chinese restaurant that's cheap.originexpensivenesslocalefigure 2: illustration of our pipeline.
first, we analyze an unlabeled in-domain corpus with supplied domain-agnostic linguistic annotation models, such as a frame-semantic parser or ner (section 3.1).
this results inslot candidates.
next, we iteratively merge and select slot candidates to obtain domain-relevant slots (sec-tions 3.2.2, 3.2.1).
finally, we use the resulting slot labels in the corpus to train a neural slot tagger (section 3.3)..1. selecting domain-relevant slots from candi-dates provided by weak supervision fromdomain-generic linguistic annotation tools.
we use framenet-style (fillmore, 1976) se-mantic frames as our main source of weaksupervision.1 we also explore named entityrecognition (ner)..2. training a standalone slot tagger for the se-lected slots.
based on the discovered slots,we train a slot tagger to annotate in-domainutterances.
after it is trained, the slot taggercan be used as a standalone component – itdoes not need the original annotation toolsfor prediction, and is able to improve on theirresults..3. evaluation on multiple domains.
we showthat our approach is domain-independent.
weachieve state-of-the-art results for slot taggingwithout manual supervision in four differentdomains, with a 6-16% absolute f1 score in-crease over the previous benchmark..4. downstream task application.
we evaluateour approach in a full dialogue response gen-eration task.
our slots can be directly usedto perform dialogue state tracking by merg-ing annotations from consecutive turns.
wetrain an end-to-end neural dialogue system us-ing our automatically discovered slots in therestaurant domain and demonstrate that ourapproach improves performance over an unsu-pervised model, ﬁnding the correct venue in5% more cases (35% more when no restaurantontology is provided)..our experimental code is available on github.2.
1see http://framenet.icsi.berkeley.edu/2https://github.com/vojtsek/.
joint-induction.
2 related work.
the idea of using weak supervision to perform ﬁne-grained language understanding based on domain-relevant (slot-like) attributes was proposed by heckand hakkani-tür (2012), who construct a triple-based database of entity relations based on websearch.
they exploit the structure of in-domain webpages to obtain semantic annotations.
there arealso similar works on relation detection (hakkani-tür et al., 2013) or entity extraction (wang et al.,2014).
this approach is, however, limited by re-quiring structured web pages as underlying data..chen et al.
(2014) combine semantic frame pars-ing with word embeddings for weakly supervisedsemantic slot induction.
chen et al.
(2015) alsouse semantic frames, construct lexical knowledgegraphs and perform a random walk to get slot can-didates.
however, both approaches only outputa ranking of potential slot candidates based onframes.
since frame annotation is very ﬁne-grained,this produces a huge number of candidates, requir-ing their manual merging into slots for any practi-cal use.
in contrast, we determine domain-relevantslots automatically.
coope et al.
(2020) focus on afew-shot setting and perform span extraction of slotvalues using pretrained models.
their approach,however, still requires some expert annotation.
an-other direction of research focuses on zero-shotslot ﬁlling.
bapna et al.
(2017)’s recurrent-neural-network-based slot tagger is pretrained on multipledomains and takes a textual description of the targetslot on the input in addition to the user utterance.
this way, adapting to a new domain only involvesproviding new slot descriptions.
further worksextend this idea with more complex architectures(shah et al., 2019; liu et al., 2020)..unsupervised and semi-supervised methodswere also investigated for predicting intents (user.
2431nerframe parser...merging & selectionmergingselectiontagger trainingunlabeledcorpusweak supervisionlabeledcorpusfigure 3: a sample of a dialogue from camrest676 data, with labels from a frame-semantic parser (middle)and our slot tagger (bottom).
although “afghan” food is not in the frame parser output, our tagger was able torecognize it.
the change in value for slot-1 (corresponding to food type) is successfully captured in the secondutterance.
this shows that our model can categorize entities (both “afghan” and “asian” relate to the same slot)..input sentence types).
yang et al.
(2014) use semi-supervised intent clustering, with manual annota-tion to seed and interpret the clusters.
chen et al.
(2016) introduced a model for zero-shot intent em-bedding prediction based on similarity to knownintents.
shi et al.
(2018) proposed a fully unsuper-vised intent detection model with the use of sen-tence clustering based on sentence-level features.
most applications of unsupervised or semi-supervised methods to end-to-end dialogue re-sponse generation avoid explicit dialogue statemodeling (e.g., serban et al., 2016; li et al., 2016;gao et al., 2019).
they aim at a non-task-orientedsetting, where state interpretability or response con-trollability are less of a concern.
other works intask-oriented dialogues use transfer learning foradapting to low-resourced target domains (zhaoand eskenazi, 2018; shalyminov et al., 2019), butalso keep the dialogue state representation latent.
in contrast, jin et al.
(2018) propose to model thedialogue state explicitly, in a semi-supervised way.
they extend the end-to-end encoder-decoder se-quicity model of lei et al.
(2018, cf.
section 4) byintroducing an additional decoder that has accessto posterior information about the system response.
this allows them to train a state representation witha reconstruction loss on unsupervised examples,using the state as a limited memory for essentialconcepts (roughly corresponding to slots).
theirmethod can be applied in fully unsupervised way,but it still requires some amount of in-domain an-notations to achieve good performance.
our workaims at explicit dialogue state modeling withoutthe need for any in-domain supervision..3 method.
our slot discovery method has three main stages:(1) we obtain weak supervision labels from auto-.
matic domain-generic annotation.
(2) we identifydomain-relevant slots based on the annotation la-bels by iteratively (a) merging and (b) ranking andselecting most viable candidates (section 3.2).
(3)we use the discovered slots to train an independentslot tagger (section 3.3)..3.1 acquiring labels.
figure 2 shows the overall data ﬂow of our slotannotation pipeline.
the data are ﬁrst labeledwith domain-generic linguistic annotation models,which we consider weak supervision.
for our ex-periments, we use a frame semantic parser andner, but other models, such as semantic role la-beling (srl; e.g., palmer et al., 2010) or keywordextraction (e.g., hulth, 2003) can be used in gen-eral.
we use a simple union of labels provided byall annotation models.3.
3.2 discovering slots: merging and ranking.
subsequent steps identify domain-relevant slotsbased on candidates provided by the automatic an-notation.
the slot discovery process is iterative –in each iteration, it: (1) merges similar candidates,(2) ranks candidates’ relevance and eliminates ir-relevant ones.
once no more frames are eliminated,the process stops and we obtain slot labels, whichare used to train a slot tagger (see section 3.3)..we refer to the automatically tagged tokens as(slot) ﬁllers, and the tags are considered slot can-didates.
we use generic precomputed word em-beddings as word representation in both steps.
wefurther compute slot embeddings 𝑒(𝑠𝑘) for eachdistinct slot 𝑠𝑘 as word embedding averages over.
3if the same token is labeled multiple times by differentannotation sources, both labels are considered candidates andare very likely to be merged.
if multiple labels remain afterthe merging and ranking process, only the ﬁrst label is kept,the rest are discarded..2432original annotation: original annotation:        user input 1: i would like an expensive restaurant that serves afghan food.
our annotation:        user input 2: how about asian oriental food.
our annotation: expensivenessslot-0localeslot-1originfoodslot-1all respective slot ﬁllers, weighted proportionallyby ﬁller frequency.
the slot embeddings need to bere-computed after each iteration due to the mergingstep.
we will now describe the individual steps..3.2.1 candidate mergingsince automatic annotation may have a very ﬁnegranularity,4 entities/objects of the same type areoften captured by multiple slot candidates.
witha frame parser, for instance, the frames directionand location both relate to the concept of area.
wethus need to merge similar 𝑠1 .
.
.
𝑠𝑛 under a singlecandidate.
we measure similarity of slots 𝑠1, 𝑠2 as:sim(𝑠1, 𝑠2) = sim𝑒 (𝑒(𝑠1), 𝑒(𝑠2)) + simctx(𝑠1, 𝑠2)where sim𝑒 is a cosine similarity and simctx(𝑠1, 𝑠2)is a normalized number of occurrences of 𝑠1 and 𝑠2with the same dependency relation.
if the similarityexceeds a pre-set threshold 𝑇sim, the candidates aremerged into one..3.2.2 candidate ranking and selectionthe main goal of this step is to remove irrelevantslot candidates and select the viable ones only.
wehypothesize that different slots are likely to occurin different contexts (e.g., addresses are requestedmore often than stated by the user).
to preserverelevant slots that only occur in rarer contexts, wecluster the data according to verb-slot pairs.
wethen rank candidates within each cluster (see de-tails below).
we consider candidates with a scorehigher than 𝛼-fraction of a given cluster mean tobe relevant and select them for the next rounds.
ifa slot candidate is selected in at least one of theclusters, it is considered viable overall..clustering the data we process the data with ageneric srl tagger.
each occurrence of a ﬁller isthus associated with a head verb whose semanticargument the corresponding word is, if such ex-ists.
we then compute embeddings of the formedverb-ﬁller pairs as average of the respective tokenembeddings.
the pairs are then clustered usingagglomerative (bottom-up) hierarchical clusteringwith average linkage according to cosine distanceof their embeddings.5 the process stops when apredetermined number of clusters is reached..4this is indeed the case for frame-semantic annotation,which we mostly use in our experiments in section 5. an-notation types that have fewer label types could be furtherdistinguished by e.g.
adding the head verb from syntacticparsing, or using word classes/word clustering over the ﬁllers.
5note that ﬁllers for the same slot candidate may end upin multiple clusters.
this does not mean that the respective.
candidate ranking criteria we use the follow-ing metrics to compute the ranking score:6• frequency frq(𝑠) is used since candidates thatoccur frequently in the data are likely important.
• coherence coh(𝑠) is the average pairwise simi-.
larity of all ﬁllers’ embeddings:.
(cid:213).
𝑑cos(𝑒(𝑣), 𝑒(𝑤)).
coh(𝑠) =.
(𝑣,𝑤) ∈𝐶2𝑠.
|𝐶2𝑠 |.
(1).
where 𝐶2𝑠 is a set of all pairs of ﬁllers for theslot candidate s. we follow chen et al.
(2014)’sassumption that ﬁllers with high coherence, i.e.,focused on one topic, are good slot candidates.
• textrank (mihalcea and tarau, 2004) is a key-word extraction algorithm.
it constructs a graphwhere nodes represent words and edges representtheir co-occurrence.
the dominant eigenvectorof the adjacency matrix of this graph then givesthe individual words’ scores.
we replace ﬁllerswith candidate labels when computing the score,so we obtain results related to slots rather thanto particular values..the ﬁnal score is a simple sum of rankings withrespect to all three scores..3.3 slot tagger model training.
our method described in section 3.2 can give usa good set of dialogue slots.
however, using themerged and ﬁltered slots directly may result in lowrecall since the original annotation models used asweak supervision are not adapted to our speciﬁcdomain.
therefore, we use the obtained labels totrain a new, domain-speciﬁc slot tagger to improveperformance.
the tagger has no access to better la-bels than those derived by our method; however, ithas a simpler task, as the set of target labels is nowmuch smaller and the domain is much narrower..we model the slot tagging task as sequence tag-ging, using a convolutional neural network thattakes word- and character-based embeddings of thetokens as the input and produces a sequence of re-spective tags (lample et al., 2016).7 the outputlayer of the tagger network gives softmax proba-bility distributions over possible tags.
to furtherincrease recall, we add an inference-time rule – if.
slot candidate is split – it is just ranked for relevance multipletimes (with respect to multiple contexts)..6usefulness of the individual metrics is conﬁrmed in an.
ablation study in section 6..7https://github.com/deepmipt/ner.
2433the most probable predicted tag is ‘o’ (i.e., no slot)and the second most probable tag has a probabilityhigher than a preset threshold 𝑇tag, the second tagis chosen as a prediction instead.
as we discussin section 6, this threshold is crucial for achievingsubstantial recall improvement..to improve the robustness of our model, we onlyuse 10% of the original in-domain training set (withlabels from section 3.1) to train the slot taggermodel.
the rest of the training set is used for a gridsearch to determine model hyperparameters (hid-den layer size, dropout rate and 𝑇tag threshold).
wechoose the parameters that yield the best f1 scorewhen compared against the automatic slot discov-ery results (i.e., no manual annotation is neededhere, the aim is at good generalization)..4 application in dialogue response.
generation.
to verify the usefulness of the labels discoveredby our method, we use them to train and evaluatean end-to-end task-oriented dialogue system.
wechoose sequicity (lei et al., 2018) for our exper-iments, an lstm-based encoder-decoder modelthat uses a system of copy nets and two-stage de-coding.
first, it decodes the dialogue state, so thedatabase can be queried externally.
in the subse-quent step, sequicity generates the system responseconditioned on the belief state and database results.
this architecture works with a ﬂat representationof the dialogue state, i.e.
the state is represented asa sequence of tokens – slot values..the default sequicity model uses gold-standarddialogue state annotation.
however, a compatiblestate representation is directly obtainable from ourlabels, simply by concatenating the labels aggre-gated in each turn from user utterances.
whenevera new value for a slot is found in user input by ourtagger, it is either appended to the state represen-tation, or it replaces a previous value of the sameslot.
this artiﬁcial supervision thus allows us toprovide a learning signal to the sequicity modeleven without manually labeled examples..5 experiments.
we evaluate our approach to slot discovery by com-paring the resulting slot labels to gold-standardsupervised slot annotation.
additionally, we eval-uate the structure of clusters created during theselection process (section 3.2.2) by comparing itto gold-standard user intents.
we also test the use-.
fulness of our labels in a full dialogue responsegeneration setup (section 4), where we compare togold-standard dialogue tracking labels..5.1 datasets and experimental setup.
we use the following datasets for our experiments:• camrest676 (cr) (wen et al., 2017) has 676dialogues, 2,744 user utterances, 4 tracked slotsand 2 intents in the restaurant domain..• multiwoz (budzianowski et al., 2018; ericet al., 2020) is a multi-domain corpus; we pickedtwo domains – hotel reservation and attractionrecommendation – to form woz-hotel (wh)with 14,435 utterances, 9 slots, 3 intents andwoz-attr (wa) with 7524 utterances, 8 slotsand 3 intents respectively.8.
• cambridge slu (henderson et al., 2012) (cs)contains 10,569 utterances and tracks 5 slots with5 intents in the restaurant domain..• atis (at) (hemphill et al., 1990) contains4,978 utterances with 79 slots and 17 intents inthe ﬂights domain.9.
as sources of weak supervision providing slot can-didates, we mainly use the frame semantic parserssemafor (das et al., 2010) and open-sesame(swayamdipta et al., 2017) – a union of labels pro-vided by both parsers is used in all our setups.
in ad-dition, to explore combined sources on the named-entity-heavy atis dataset, we include a genericconvolutional ner model provided by spacy.10to provide features for slot candidate merging andselection, we use allennlp (gardner et al., 2017)for srl and fasttext (bojanowski et al., 2017) aspretrained word embeddings..slot merging and selection parameters were setheuristically in an initial trial run on the cam-rest676 data and proved stable across domains.
slot tagger hyperparameters are chosen accordingto grid search on a portion of the training data, asdescribed in section 3.3.11.
5.2 system variants and baselines.
we test multiple ablation variants of our method:• ours-full is the full version of our method (full.
annotation setup and trained slot tagger)..8multiwoz contains more domains such as restaurant,train search, bus search.
however, we decided to not includethese as they are nearly identical to the other domains we use.
fromatis.
version.
9we.
used.
data.
the.
https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk..10https://spacy.io11training details are included in appendix c..2434method ↓ / dataset→tag-supervised∗dict-supervised∗weak supervision →chen et al.
ours-noclours-notagours-nothrours-full.
cr0.7780.778 ± .0040.7780.705 ± .005frames0.535 ± .0020.311 ± .0060.552 ± .0080.586 ± .0240.6650.665 ± .0120.665.cs0.724 ± .0030.7530.753 ± .0050.753frames0.590 ± .0010.393 ± .0110.664 ± .0070.569 ± .0310.6920.692 ± .0080.692.wh0.7420.742 ± .0080.7420.7500.750 ± .0180.750frames0.382 ± .0010.122 ± .0010.388 ± .0020.485 ± .0320.5480.548 ± .0040.548.wa0.7310.731 ± .0020.7310.665 ± .003frames0.375 ± .0010.266 ± .0080.383 ± .0020.435 ± .0020.4390.439 ± .0010.439.at0.8480.848 ± .0030.8480.678 ± .002.frames0.616 ± .0010.631 ± .0020.627 ± .0020.671 ± .0050.678 ± .002.frames & ner–0.677 ± .0020.648 ± .0030.698 ± .0040.7100.710 ± .0020.710.table 1: f1 score values with 95% conﬁdence intervals for slot tagging performance comparison among differentmethods (see section 5.2).
the respective precision and recall values are presented in the appendix (table 7).
the measures are evaluated using a manual slot mapping to the datasets’ annotation, which is not needed for themethods themselves (see section 5.3).
∗note that supervised setups are not directly comparable to our approach..• ours-nothr does not use the recall-increasingsecond-candidate rule in the slot tagger (cf.
sec-tion 3.3)..• ours-notag excludes the slot tagger, directly us-ing the output of our merging and selection step.
• ours-nocl further excludes the clustering step;slot candidate ranking and selection is performedover all candidates together (cf.
section 3.2.2)..we also compare to previous work of chen et al.
(2014),12 which is similar to ours-nocl, but it doesnot merge similar frames and uses different rankingcriteria.
to put our results into perspective, wealso include two supervised models for comparison:tag-supervised is the same model that we use asour slot tagger (see section 3.3), but it is trainedon supervised data.
dict-supervised uses a simpledictionary of labels obtained from the training data..as an intrinsic evaluation of the verb-slot pairclusters formed for slot ranking in section 3.2.2,we compare to gold-standard intent annotation withrespect to the following baselines: (1) a major-ity baseline (assigning the most frequent intentclass to all instances), and (2) a simple methodthat represents the utterances as averages of respec-tive word embeddings and performs sentence-levelintent clustering.
all the slots in a given utteranceare then assumed to have the same intent..the dialogue generation task is evaluated bycomparing to jin et al.
(2018)’s approach intro-duced in section 2. we run their model in a fullyunsupervised way, i.e.
we provide no labeled ex-amples during the training phase, to give a faircomparison against our model.
to provide moreperspective, we also show a supervised variant ofjin et al.
(2018)’s model, where gold-standard slotlabels are provided..12we use our own reimplementation of their approach..5.3 evaluation metrics.
for evaluation, we construct a handcrafted refer-ence mapping between our discovered slots andthe respective ground-truth slots and intents.
themapping is domain-speciﬁc, but it is very easy toconstruct even for an untrained person – the processtakes less than 10 minutes for each of our domains.
it amounts to matching slots from the domain on-tology against slots output by our approach, whichare represented by framenet labels.
most impor-tantly, the mapping is only needed for evaluation,not by our method itself.
we provide an examplemapping in appendix b..we use the following evaluation metrics:.
• slot f1 score: to reﬂect slot tagging perfor-mance, we measure precision, recall, and f1 forevery slot individually.
an average is then com-puted from slot-level scores, weighted by thenumber of slot occurrences in the data.
we mea-sure slot f1 both on standalone user utterances(slot tagging) and in the context of a dialoguesystem (dialogue tracking)..• slot-level average precision (ap).
the slotcandidates picking task is a ranking problem andwe use the average precision metric followingchen et al.
(2014).
considering a ranked list ofdiscovered slots 𝑙 = 𝑠1, .
.
.
, 𝑠𝑘, .
.
.
, 𝑠𝑛 we com-pute ap:.
𝐴𝑃(𝑙) =.
(cid:205)𝑛.𝑃@𝑘 (𝑙)1𝑘𝑘=1# mapped slots.
(2).
where 1𝑘 is an indicator function that equals oneif slot 𝑘 has a reference mapping deﬁned and𝑃@𝑘 (𝑙) is precision at 𝑘 of the ranked list 𝑙.
• slot rand index (ri) is a clustering metric,used to evaluate slot candidate merging.
ri isthe proportion of pairs of slot candidates that arecorrectly assigned into the same or into different.
2435method.
chen et al..ours-nocl.
ours-full.
cr0.315±.0020.5190.5190.519±.0030.5200.5200.520±.004.
cs0.272±.0010.376±.0030.4000.4000.400±.003.
wh0.269±.0010.069±.0740.3170.3170.317±.008.
wa0.393±.0020.176±.0160.4030.4030.403±.006.
at0.2670.2670.267±.0030.069±.0080.208±.018.
table 2: slot candidate ranking average precision forall datasets (see sections 5.2 and 5.3 for details)..method cr0.466rnd0.587ours0.212rnd0.359ours.
cs0.2680.3190.1370.207.wh wa0.1530.1550.1880.1680.1280.0610.1170.101.at0.1780.1710.1710.194.ri.
nmi.
table 3: slot merging evaluation using ri and nmi(cf.
section 5.3) on selected datasets, comparing ourapproach (ours) with a random baseline (rnd)..slots (following the reference mapping).13.
• normalized mutual information (nmi) is themutual information between two clusterings nor-malized into the (0, 1) interval.
thanks to thenormalization, it is suitable for comparing twoclusterings with different numbers of clusters.
• intent accuracy is the percentage of slot oc-currences assigned into the correct intent clusterunder the reference mapping (see section 5.2).
• dialogue joint goal accuracy calculates theproportion of dialogue turns where all user con-straints (i.e., dialogue state summarizing slot val-ues) are captured correctly (mrkši´c et al., 2017).
• dialogue entity match rate calculates the lastturn’s entity in each dialogue.
it veriﬁes if a cor-rect entity would be retrieved from the databaseusing the ﬁnal constraints (wen et al., 2017).
for slot tagging and ranking evaluation, we sam-pled a random data order 50 times and performed5-fold cross-validation for each permutation.
forthe dialogue generation evaluation, we trained themodels 100 times and used averaged results.
allresults are given with 95% conﬁdence intervals..6 results and discussion.
we ﬁrst evaluate the main task of slot tagging andinclude a manual error analysis, then present de-tailed results for subtasks (slot candidate rankingand merging) and additional tasks (intent clusteringand full response generation)..13we compute ri on a union of labels that have a ground-truth slot mapping and all labels selected by our method.
la-bels without ground-truth mapping are assumed to form single-item “pseudo-slots”..slot tagging is evaluated in table 1. ours-full(slot selection + trained tagger) outperforms allother approaches by a large margin, especially interms of recall.
the performance cannot match thesupervised models, but it is not far off in somedomains.14 chen et al.
(2014)’s method has aslightly higher precision, but our recall is muchhigher than theirs (see appendix a.1).
note thatchen et al.
(2014) do not reduce the set of candi-dates, they only rank them so that a manual cut-off can be made.
in contrast, our method reducesthe set of candidates signiﬁcantly.
a compari-son between ours-notag and ours-full shows thatapplying the slot tagger improves both precisionand recall.
tagger without the threshold decisionrule (ours-nothr) mostly performs better than theparser; however, using the threshold is essential toimprove recall.
experiments on atis with ner asan additional source of annotation proved that ourmethod can beneﬁt from it.
as discussed above,the use of the trained tagging model is crucial toimprove the recall of our method.
in figure 4, wecompare the results with and without the tagger.
we change the value of prediction threshold andmeasure the number of cases in which the taggingmodel encounters more true positives, false posi-tives or false negatives, respectively.
as the resultsshow, lowering the threshold increases the num-ber of cases in which the tagger ﬁnds more correctslot values (and therefore improves recall), while itdoes not affect the number of false positives much(and therefore retains precision)..error analysis: we conducted a manual erroranalysis of slot tagging to gain more insight aboutthe output quality and sources of errors.
in general,we found that the tagger can generalize and captureunseen values (cf.
figure 3)..one source of errors is the relatively low recallof the frame-semantic parsers used.
we success-fully address this issue by introducing the slot tag-ger, however, many slot values remain untagged.
this is expected as our method’s performance isinherently limited by the input linguistic annotationquality.
another type of errors is caused by the can-.
14note that our measurements of slot f1 only consider the‘o’ tag as negative (the average is computed over slots only).
this results in lower numbers than those reported in literature(cf.
e.g.
goo et al., 2018), but we believe that this reﬂects theactual performance more accurately..15we present results taken in unsupervised setting, i.e.
whenno ontology is available.
however, since jin et al.
(2018)consider only slot values that are known from the ontology bydefault, we provide the extended results in appendix a.2..2436methodjin et al.
supervisedjin et al.
unsupervisedjin et al.
weak-labelsours-full (unsupervised).
slot f10.967 ± .0010.719 ± .0020.709 ± .0110.7560.756 ± .0040.756.joint goal accuracy0.897 ± .0020.385 ± .0030.335 ± .0080.4650.465 ± .0070.465.entity match rate0.869 ± .0040.019 ± .0020.269 ± .0120.3680.368 ± .0080.368.table 4: evaluation on the downstream task of dialogue generation on camrest676 data.
we evaluate with respectto three state tracking metrics (see section 5.3).
the best results in an unsupervised setting are presented in bold.15.
methodmajorityembeddingours.
cr0.5920.5350.7050.7050.705.cs0.5300.5510.6130.6130.613.wh wa0.6120.8830.5950.8730.6990.8820.6990.6990.8820.882.at0.7270.7270.7270.7050.677.table 5: cluster assignment accuracy of our methodsif we interpret the clustering as user intent detection.
majority is a majority baseline and embedding refersto an average sentence embedding clustering approach..conﬁgurationours-fullours -frqours -cohours -textrank.
f1 score0.663 ± 0.0120.600 ± 0.0080.582 ± 0.0120.514 ± 0.006.table 6: ablation study of slot ranking features oncamrest676.
the full model is compared to variantsleaving out of the scores described in section 3.2.2..didate merging procedure (see also below).
due tofrequent co-occurrence, it might happen that twosemantically unrelated candidates are merged andtherefore some tokens are wrongly included as re-spective slot ﬁllers.
nevertheless, the merging stepis required in order to obtain a reasonable numberof slots for a dialogue domain..our approach does leave some room for improve-ments, especially regarding the consistency of re-sults across different slots, which can be imbal-anced.
for instance, on the woz-hotel data, weobserve a difference of up to 0.5 f1 score amongindividual slots (see appendix a.2)..slot candidate ranking results are given in ta-ble 2. our pipeline signiﬁcantly outperforms chenet al.
(2014)’s approach on 4 out of 5 datasets.
wecan also see that the slot-verb pairs clustering stepis important – in the ablation experiment wherewe do not perform clustering (ours-nocl), perfor-mance falls dramatically on the woz-hotel, woz-attr and atis data.
this is because without theclustering step, a large number of context-irrelevantslot candidates is considered, hurting performance.
in addition, we include a detailed evaluation ofthe contribution of the individual slot candidateranking scores described in section 3.2.2. results.
in table 6 suggest that all of our proposed scoresimprove the performance..slot merging evaluation is shown in table 3. al-though candidates in the camrest676 data aremerged into slots reasonably well, other datasetsshow a relatively low performance.
the low riscores are a result of errors in candidate ranking,which wrongly assigned high ranks to some rare,irrelevant candidates.
these candidates do not ap-pear in the reference mapping and are assumed toform singular “pseudo-slots”.
however, they aretypically joined with similar candidates in the merg-ing process.
this leads to many pairs of candidatesthat are merged into one slot by our approach butappear separately in the reference mapping.
never-theless, this behavior barely inﬂuences slot taggingperformance as the candidates are rare..clustering evaluation: table 5 suggests that ourclustering performs better than simple baselinesand can potentially yield useful results if used forintent detection.
nevertheless, intent detection ismore complex and presumably requires more fea-tures and information about the dialogue context,which we reserve for future work.
the complexityis also suggested by the fact that the naive embed-ding clustering performs worse than the majoritybaseline in 4 out of 5 cases..dialogue response generation: we explore theinﬂuence that our labels have on sequence-to-sequence dialogue response generation in an ex-periment on the camrest676 data (see table 4).
we can see that our method provides helpful slotlabels that improve dialogue state tracking perfor-mance.
compared to jin et al.
(2018)’s systemused in a fully unsupervised setting, our approachshows signiﬁcant improvements in all metrics.
weachieve better results than jin et al.
(2018)’s sys-tem especially with respect to entity match rate,suggesting that our model can provide consistentlabels throughout the whole dialogue.
to makea fair comparison, we further evaluate jin et al.
(2018)’s system in a setting in which it can learn.
2437cant improvement in intrinsic nlu performanceover previous weakly supervised approaches; inparticular, we vastly improve the slot recall.
theusefulness of slots discovered by our method isfurther conﬁrmed in a full dialogue response gener-ation application.
code used for our experimentsis available on github.16.
a drawback of our approach is the reliance onexisting linguistic annotation models.
we showthat the method is able to combine multiple anno-tation sources and create a tagger that functions asa standalone component, generalizing better thanthe original annotation and thus lowering this de-pendency.
nevertheless, the results are still some-what limited by the input annotation structure andquality.
in future, we plan to further improve themodel by unsupervised selection of slot candidatesvia keyword extraction and clustering, as well asby taking context information from preceding di-alogue turns into account.
we also want to focusmore on the intent detection aspect of our work..acknowledgements.
this work was supported by charles universitygrants primus 19/sci/10, gauk 302120, andsvv 260 575. we also want to thank jindˇrich li-bovický and david mareˇcek for helpful commentson the draft, and the anonymous reviewers for theirremarks that helped us further improve the paper..figure 4: the comparison of outputs of our tagger andthe parser.
the plots show a number of cases in whichthe respective approach encounters more tps, fps orfns than the other..references.
from the labels provided directly by weak super-vision (i.e., the frame-semantic parser, not ﬁlteredby our pipeline).
we observe an improvement interms of entity match rate, but it does not matchthe improvement achieved with our ﬁltered labels.
surprisingly, slot f1 and joint goal accuracy evendecrease slightly, which suggests that label qualityis important and the noisy labels obtained directlyfrom weak supervision are not useful enough..7 conclusion.
we present a novel approach for weakly supervisednatural language understanding in dialogue systemsthat discovers domain-relevant slots and tags themin a standalone fashion.
our method removes theneed for annotated training data by using off-the-shelf linguistic annotation models.
experimentson ﬁve datasets in four domains mark a signiﬁ-.
ankur bapna, gokhan tür, dilek hakkani-tür, andlarry heck.
2017. towards zero-shot frame se-mantic parsing for domain scaling.
in proceedingsof interspeech 2017, pages 2476–2480..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withtransactions of the acl,subword information.
5:135–146..paweł budzianowski, tsung-hsien wen, bo-hsiangtseng, inigo casanueva, stefan ultes, osman ra-madan, and milica gaši´c.
2018. multiwoz – alarge-scale multi-domain wizard-of-oz dataset fortask-oriented dialogue modelling.
in proceedings ofemnlp..yun-nung chen, dilek hakkani-tür, and xiaodonghe.
2016. zero-shot learning of intent embeddingsfor expansion by convolutional deep structured se-in proceedings of ieee icassp,mantic models.
pages 6045–6049..16https://github.com/vojtsek/.
joint-induction.
2438yun-nung chen, william yang wang, and alexanderjointly modeling inter-slot rela-rudnicky.
2015.tions by random walk on knowledge graphs for un-supervised spoken language understanding.
in pro-ceedings of naacl, pages 619–629..yun-nung chen, william yang wang, and alexander irudnicky.
2014. leveraging frame semantics anddistributional semantics for unsupervised semanticslot induction in spoken dialogue systems.
in pro-ceedings of ieee slt, pages 584–589..samuel coope, tyler farghly, daniela gerz, ivan vuli´c,and matthew henderson.
2020.span-convert:few-shot span extraction for dialog with pretrainedconversational representations.
in proceedings ofthe 58th annual meeting of the association for com-putational linguistics, pages 107–121, online..dipanjan das, nathan schneider, desai chen, andnoah a. smith.
2010. probabilistic frame-semanticparsing.
in proceedings of naacl-hlt, pages 948–956, los angeles, california..mihail eric, rahul goel, shachi paul, abhishek sethi,sanchit agarwal, shuyang gao, adarsh kumar,anuj goyal, peter ku, and dilek hakkani-tur.
2020.multiwoz 2.1: a consolidated multi-domain di-alogue dataset with state corrections and statetracking baselines.
in proceedings of the 12th lan-guage resources and evaluation conference, pages422–428, marseille, france..charles j fillmore.
1976. frame semantics and the na-ture of language.
annals of the new york academyof sciences, 280(1):20–32..xiang gao, sungjin lee, yizhe zhang, chris brockett,michel galley, jianfeng gao, and bill dolan.
2019.jointly optimizing diversity and relevance in neu-ral response generation.
in proceedings of naacl,minneapolis, mn, usa..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthewpeters, michael schmitz, and luke s. zettlemoyer.
2017. allennlp: a deep semantic natural languageprocessing platform.
in proceedings of acl work-shop for nlp open source software (nlp-oss)..chih-wen goo, guang gao, yun-kai hsu, chih-lihuo, tsung-chieh chen, keng-wei hsu, and yun-nung chen.
2018. slot-gated modeling for jointslot filling and intent prediction.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 753–757, new orleans, louisiana..dilek hakkani-tür, larry heck, and gokhan tur.
2013.using a knowledge graph and query click logs forunsupervised learning of relation detection.
in pro-ceedings of ieee icassp, pages 8327–8331..larry heck and dilek hakkani-tür.
2012. exploitingthe semantic web for unsupervised spoken languageunderstanding.
in proceedings of ieee slt, pages228–233..charles t. hemphill, john j. godfrey, and george r.doddington.
1990. the atis spoken language sys-tems pilot corpus.
in proceedings of the workshopon speech and natural language - hlt ’90, pages96–101, hidden valley, pennsylvania..matthew henderson, milica gaši´c, blaise thomson,pirros tsiakoulis, kai yu, and steve young.
2012.discriminative spoken language understanding us-in proceedings ofing word confusion networks.
ieee slt, pages 176–181..anette hulth.
2003. improved automatic keyword ex-in pro-.
traction given more linguistic knowledge.
ceedings of emnlp, pages 216–223..xisen jin, wenqiang lei, zhaochun ren, hongshenchen, shangsong liang, yihong zhao, and daweiyin.
2018.explicit state tracking with semi-supervision for neural dialogue generation.
in pro-ceedings of acm cikm, pages 1403–1412..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of naacl, pages 260–270..wenqiang lei, xisen jin, min-yen kan, zhaochunren, xiangnan he, and dawei yin.
2018. sequicity:simplifying task-oriented dialogue systems with sin-gle sequence-to-sequence architectures.
in proceed-ings of acl, pages 1437–1447..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting objec-tive function for neural conversation models.
inproceedings of the 15th annual conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, pages 110–119, san diego, ca, usa..zihan liu, genta indra winata, peng xu, and pascalefung.
2020. coach: a coarse-to-fine approach forin proceedings of thecross-domain slot filling.
58th annual meeting of the association for compu-tational linguistics, pages 19–25, online..rada mihalcea and paul tarau.
2004. textrank: bring-ing order into text.
in proceedings of emnlp, pages404–411..nikola mrkši´c, diarmuid o séaghdha, tsung-hsienwen, blaise thomson, and steve young.
2017. neu-ral belief tracker: data-driven dialogue state track-ing.
in proceedings of acl, pages 1777–1788..martha palmer, daniel gildea, and nianwen xue.
2010.semantic role labeling.
synthesis lectures on hu-man language technologies, 3(1):1–103..2439iulian v serban, alessandro sordoni, yoshua bengio,aaron courville, and joelle pineau.
2016. buildingend-to-end dialogue systems using generative hier-archical neural network models.
in proceedings ofaaai, pages 3776–3783..darsh shah, raghav gupta, amir fayazi, and dilekhakkani-tur.
2019.robust zero-shot cross-domain slot filling with example values.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5484–5490, florence, italy..igor shalyminov, sungjin lee, arash eshghi, andoliver lemon.
2019. few-shot dialogue genera-tion without annotated data: a transfer learning ap-in proceedings of sigdial, stockholm,proach.
sweden..chen shi, qi chen, lei sha, sujian li, xu sun,houfeng wang, and lintao zhang.
2018. auto-dialabel: labeling dialogue data with unsupervisedin proceedings of emnlp, pages 684–learning.
689..swabha swayamdipta, sam thomson, chris dyer, andnoah a smith.
2017. frame-semantic parsing withsoftmax-margin segmental rnns and a syntacticscaffold.
arxiv:1706.09528..lu wang, larry heck, and dilek hakkani-tür.
2014.leveraging semantic web search and browse ses-sions for multi-turn spoken dialog systems.
in pro-ceedings of ieee icassp, pages 4082–4086..tsung-hsien wen, david vandyke, nikola mrksi´c,milica gaši´c, lina m rojas-barahona, pei-hao su,stefan ultes, and steve young.
2017. a network-based end-to-end trainable task-oriented dialoguesystem.
in proceedings of eacl, pages 438–449..jason williams, antoine raux, deepak ramachandran,and alan black.
2013. the dialog state trackingchallenge.
in proceedings of sigdial, pages 404–413..xiaohao yang, jia liu, zhenfeng chen, and weilanwu.
2014. semi-supervised learning of dialogueacts using sentence similarity based on word embed-dings.
in proceedings of icalip, pages 882–886..s. young, m. gasic, b. thomson, and j.d.
williams.
pomdp-based statistical spoken dialogthe ieee,.
proceedings of.
2013.systems: a review.
101(5):1160–1179..tiancheng zhao and maxine eskenazi.
2018. zero-shot dialog generation with cross-domain latent ac-tions.
in proceedings of sigdial, pages 1–10..2440a additional results.
a.1 slot tagging precision and recall.
methodptag-supervised∗0.794dict-supervised∗ 0.7930.771chen et al.
0.537ours-nocl0.561ours-notag0.636ours-nothr0.752ours-full.
cr.
cs.
wh.
wa.
at.
r0.8140.7100.4860.3470.5860.5490.643.p0.8230.8310.8130.6160.6900.5850.718.r0.6960.7520.5290.3710.6880.5660.703.p0.8800.8690.3840.1010.3690.4580.494.r0.6830.7100.5790.2180.6070.5750.750.p0.8020.6690.3620.2440.3350.3940.373.r0.7150.8590.4620.3400.5750.5610.606.p0.7720.5460.7010.6340.7150.7010.684.r0.9130.9900.5830.5950.6420.6870.672.at(+ner)rp––––––0.7040.6620.6230.6850.6970.7100.7030.725.table 7: precision (p) and recall (r) values slot tagging performance comparison among different methods (seesection 5.2; frames are used as weak supervision in all setups, the rightmost column on atis additionally usesner).
we can see consistent recall improvement when using our slot tagger.
the measures are evaluated using amanually designed slot mapping to the datasets’ annotation, which is not needed for the methods themselves (seesection 5.3).
∗note that supervised setups are not directly comparable to our approach..a.2.
individual slot performance.
datasetcrcswh.
price0.5430.6290.208.area0.7640.8350.524.request0.7590.4800.107.type–0.8130.125.food0.5900.642–.
day––0.146.people––0.822.stars––0.821.stay––0.341.table 8: per-slot f1 scores of the ours-full method evaluated on selected datasets with slot intersection.
for someslots the performance varies a lot among datasets due to different ranges of values and contexts.
the measures areevaluated using a manually designed slot mapping to the datasets’ annotation, which is not needed for the methodsthemselves (see section 5.3)..method.
jin et al.
supervisedjin et al.
unsupervisedours-full (unsupervised).
slot f1.
onto0.969 ± .0010.8730.873 ± .0030.8730.821 ± .004.no-onto0.967 ± .0010.719 ± .0020.7560.756 ± .0040.756.joint goal accuracyno-ontoonto0.897 ± .0020.911 ± .0020.6320.385 ± .0030.632 ± .0090.6320.4650.465 ± .0070.4650.533 ± .007.entity match rateonto0.892 ± .0040.398 ± .0100.4450.445 ± .0090.445.no-onto0.869 ± .0040.019 ± .0020.3680.368 ± .0080.368.table 9: evaluation on the downstream task of dialogue generation on camrest676 data.
we evaluate with re-spect to three distinct metrics of state tracking performance.
two variations of metrics are included: onto takesonly slot values present in ontology into account, no-onto does not require ontology information and thus ﬁts theunsupervised setting better (cf.
section 5.3).
in bold we present the best results in unsupervised setting..b reference mapping.
cambridgeslu ontology.
ours-full outputexpensivenessorigin + people_by_origindirection + part_orientationalcontacting + artifactlocale_by_use.
↦→ pricerange↦→ food↦→ area↦→ phone↦→ type.
table 10: an example of reference mapping between the output of ours-full represented by framenet labels (left)and ground-truth cambridgeslu ontology (right).
frames merged by our method are shown on a single line,separated by “+”..2441c training details.
here we provide details about training process andmodel sizes:.
• since the models are rather small with regardsto number of parameters, it is sufﬁcient to usea regular desktop pc.
in our experiments, werequire about 4 gb of ram, and we use intelxeon e5-2630 v4 cpus..• our slot candidate selection step takes roughly1 hour.
the tagger model is lightweight, withonly 150k parameters.
its training requires10-30 minutes, depending on the exact conﬁg-uration and data size..• the evaluation scripts are attached and de-.
scribed in the readme ﬁle..• we conduct hyperparameter search using abasic grid search algorithm.
we tested hiddensize values ∈ [50, 200], dropout ∈ [0.5, 0.85]and the threshold 𝑇tag ∈ [0.05, 0.3].
there-fore, we ran 4 × 8 × 6 = 192 search trials..• the best parameters were determined bytagger accuracy on the validation set: hid-den_size = 250, dropout = 0.7, 𝑇tag = 0.3,𝑇sim = 0.9..• links to the data are included in the readmeﬁle, we use train:validation:split ratio equalto 8:1:1..2442