how did this get funded?!.
automatically identifying quirky scientiﬁc achievements.
chen shani∗, nadav borenstein∗, dafna shahaf.
{chenxshani, nadav.borenstein, dshahaf}@cs.huji.ac.il.
the hebrew university of jerusalem.
abstract.
humor is an important social phenomenon,serving complex social and psychologicalfunctions.
however, despite being studied formillennia humor is computationally not wellunderstood, often considered an ai-completeproblem.
in this work, we introduce a novel setting inhumor mining: automatically detecting funnyand unusual scientiﬁc papers.
we are inspiredby the ig nobel prize, a satirical prize awardedannually to celebrate funny scientiﬁc achieve-ments (example past winner: “are cows morelikely to lie down the longer they stand?”).
this challenging task has unique characteris-tics that make it particularly suitable for auto-matic learning.
we construct a dataset containing thousandsof funny papers and use it to learn classiﬁers,combining ﬁndings from psychology and lin-guistics with recent advances in nlp.
we useour models to identify potentially funny papersin a large dataset of over 630,000 articles.
theresults demonstrate the potential of our meth-ods, and more broadly the utility of integrat-ing state-of-the-art nlp methods with insightsfrom more traditional disciplines.
introduction.
1humor is an important aspect of the way we inter-act with each other, serving complex social func-tions (martineau, 1972).
humor can function eitheras a lubricant or as an abrasive: it can be used as akey for improving interpersonal relations and build-ing trust (wanzer et al., 1996; wen et al., 2015), orhelp us work through difﬁcult topics.
it can also aidin breaking taboos and holding power to account.
enhancing the humor capabilities of computers hastremendous potential to better understand interac-tions between people, as well as build more naturalhuman-computer interfaces..nevertheless, computational humor remains along-standing challenge in ai; it requires complexlanguage understanding, manipulation capabilities,creativity, common sense, and empathy.
some evenclaim that computational humor is an ai-completeproblem (stock and strapparava, 2002)..as humor is a broad phenomenon, most workson computational humor focus on speciﬁc humortypes, such as knock-knock jokes or one-liners (mi-halcea and strapparava, 2006; taylor and mazlack,2004).
in this work, we present a novel humorrecognition task: identifying quirky, funny scien-tiﬁc contributions.
we are inspired by the ig nobelprize1, a satiric prize awarded annually to ten sci-entiﬁc achievements that “ﬁrst make people laugh,and then think”.
past ig nobel winners include“chickens prefer beautiful humans” and “beautyis in the eye of the beer holder: people who thinkthey are drunk also think they are attractive”..automatically identifying candidates for the ignobel prize provides a unique perspective on hu-mor.
unlike most humor recognition tasks, thehumor involved is sophisticated, and requires com-mon sense, as well as specialized knowledge andunderstanding of the scientiﬁc culture.
on the otherhand, this task has several characteristics renderingit attractive: the funniness of the paper can oftenbe recognized from its title alone, which is short,with simple syntax and no complex narrative struc-ture (as opposed to longer jokes).
thus, this is arelatively clean setting to explore our methods..we believe humor in science is also particularlyinteresting to explore, as humor is strongly tied tocreativity.
quirky contributions could sometimesindicate fresh perspectives and pioneering attemptsto expand the frontiers of science.
for example,andre geim won an ig nobel in 2000 for levitatinga frog using magnets and a nobel prize in physics.
∗equal contribution.
1improbable.com/ig-about.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages14–28august1–6,2021.©2021associationforcomputationallinguistics14in 2010. the nobel committee explicitly attributedthe win to his playfulness (the royal swedishacademy of science, 2010)..our contributions are:• we formulate a novel humor recognition task.
in the scientiﬁc domain..• we construct a dataset containing thousands.
of funny scientiﬁc papers..• we develop multiple classiﬁers, combiningﬁndings from psychology and linguistics withrecent nlp advances.
we evaluate them bothon our dataset and in a real-world setting, iden-tifying potential ig nobel candidates in a largecorpus of over 0.6m papers..• we devise a rigorous, data-driven way to ag-gregate crowd workers’ annotations for sub-jective questions..• we release data and code2.
beyond the tongue-in-cheek nature of our ap-plication, we more broadly wish to promote com-bining data-driven research with more-traditionalworks in areas such as psychology.
we believe in-sights from such ﬁelds could complement machinelearning models, improving performance as wellas enriching our understanding of the problem..2 related work.
humor in the humanities.
a large body of theo-retical work on humor stems from linguistics andpsychology.
ruch (1992) divided humor into threecategories: incongruity, sexual, and nonsense (andcreated a three-dimensional humor test to accountfor them).
since our task is to detect humor inscientiﬁc contributions, we believe that the thirdcategory can be neglected under the assumptionthat no-nonsense article would (or at least, should)be published (notable exception: the sokal hoax(sokal, 1996))..the ﬁrst category, incongruity, was ﬁrst fullyconceptualized by kant in the eighteenth century(shaw, 2010).
the well-agreed extensions to in-congruity theory are the linguistics incongruity res-olution model and semantic script theory of humor(suls, 1972; raskin, 1985).
both state that if asituation ended in a manner that contradicted ourprediction (in our case, the title contains an unex-pected term) and there exists a different, less likelyrule to explain it – the result is a humorous expe-rience.
simply put, the source of humor lies in.
2github.com/nadavborenstein/iggy.
violation of expectations.
example ig nobel win-ners include: “will humans swim faster or slowerin syrup?” and ”coordination modes in the multi-segmental dynamics of hula hooping”..the second category, sex-related humor is alsocommon among ig nobel winning papers.
exam-ples include: “effect of different types of textileson sexual activity.
experimental study” and “mag-netic resonance imaging of male and female geni-tals during coitus and female sexual arousal”.
humor detection in ai.
most computational hu-mor detection work done in the context of ai relieson supervised or semi-supervised methods and fo-cuses on speciﬁc, narrow, types of jokes or humor.
humor detection is usually formulated as a bi-nary text classiﬁcation problem.
example domainsinclude knock-knock jokes (taylor and mazlack,2004), one-liners (miller et al., 2017; simpsonet al., 2019; liu et al., 2018; mihalcea and strappa-rava, 2005; blinov et al., 2019; mihalcea and strap-parava, 2006), humorous tweets (maronikolakiset al., 2020; donahue et al., 2017; ortega-buenoet al., 2018; zhang and liu, 2014), humorous prod-uct reviews (ziser et al., 2020; reyes and rosso,2012), tv sitcoms (bertero and fung, 2016), shortstories (wilmot and keller, 2020), cartoons cap-tions (shahaf et al., 2015), and even “that’s whatshe said” jokes (hossain et al., 2017; kiddon andbrun, 2011).
related tasks such as irony, sarcasmand satire have also been explored in similarly nar-row domains (davidov et al., 2010; reyes et al.,2012; pt´aˇcek et al., 2014)..3 problem formulation and dataset.
our goal in this paper is to automatically identifycandidates for the ig nobel prize.
more precisely,to automatically detect humor in scientiﬁc papers.
first, we consider the question of input to ouralgorithm.
sagi and yechiam (2008) found a strongcorrelation between funny title and humorous sub-ject in scientiﬁc papers.
motivated by this correla-tion, we manually inspected a subset of ig nobelwinners.
for the vast majority of them, reading thetitle was enough to determine whether it is funny;very rarely did we need to read the abstract, letalone the full paper.
typical past winners’ titles in-clude “why do old men have big ears?” and “if youdrop it, should you eat it?
scientists weigh in on the5-second rule”.
an example of a non-informativetitle is “pouring ﬂows”, a paper calculating theoptimal way to dunk a biscuit in a cup of tea..15based on this observation, we decided to focuson the papers’ titles.
more formally: given a title tof an article, our goal is to learn a binary functionϕ(t) → {0, 1}, reﬂecting whether the paper is hu-morous, or ‘ig nobel-worthy’.
the main challenge,of course, lies in the construction of ϕ..to take a data-driven approach to tackle thisproblem, we crafted a ﬁrst-of-its-kind dataset con-taining titles of funny scientiﬁc papers2.
we startedfrom the 211 ig nobel winners.
next, we manuallycollected humorous papers from online forums andblogs3, resulting in 1,707 papers.
we manuallyveriﬁed all of these papers can be used as positiveexamples.
in section 6 we give more indicationthese papers are indeed useful for our task..for negative examples, we randomly sampled1,707 titles from semantic scholar4 (to obtain abalanced dataset).
we then classify each paper intoone of the following scientiﬁc ﬁelds: neuroscience,medicine, biology, or exact sciences5.
we balancedthe dataset in a per-ﬁeld manner.
while some ofthese randomly sampled papers could, in princi-ple, be funny, the vast majority of scientiﬁc papersare not (we validated this assumption through sam-pling)..4 humor-theory inspired features.
in deep learning, architecture engineering largelytook the place of feature engineering.
one of thegoals of our work is to evaluate the value of featuresinspired by domain experts.
in this section, we de-scribe and formalize 127 features implementinginsights from humor literature.
to validate the pre-dictive power of the features that require training,we divide our data to train and test sets (80%/20%).
we now describe the four major feature families..4.1 unexpected languageresearch suggests that surprise is an importantsource of humor (raskin, 1985; suls, 1972).
in-deed, we notice that titles of ig nobel winners ofteninclude an unexpected term or unusual language,e.g.
: “on the rheology of cats”, “effect of coke onsperm motility” and “pigeons’ discrimination ofpaintings by monet and picasso”.
to quantify un-expectedness, we create several different language-models (lms):.
3e.g., reddit.com/r/sciencehumour,popsci.com/read/funny-science-blog,goodsciencewriting.wordpress.com.
4api.semanticscholar.org/corpus/5using scimagojr.com to map venues to ﬁelds..n-gram based lms.
we train simple n-gramlms with n ∈ {1, 2, 3} on two corpora – 630,000titles from semantic scholar, and 231,600 one-linejokes (moudgil, 2016).
syntax-based lms.
here we test the hypothesisthat humorous text has more surprising grammati-cal structure (oaks, 1994).
we replace each wordin our semantic scholar corpus with its correspond-ing part-of-speech (pos) tag6.
we then trainedn-gram based lms (n ∈ {1, 2, 3}) on this corpus.
transformer-based lms.
we use three differenttransformers based (vaswani et al., 2017) mod-els: 1) bert (devlin et al., 2018) (pre-trained onwikipedia and the bookcorpus), 2) scibert (belt-agy et al., 2019), a variant of bert optimized onscientiﬁc text from semantic scholar, and 3) gpt-2(radford et al., 2019), a large transformer-basedlm, trained on a dataset of 8m web pages.
weﬁne-tuned gpt-2 on our semantic scholar corpora(details in appendix c.1).
using the lms.
for each word in a title, wecompute the word’s perplexity.
for the n-gramlms and gpt-2, we compute the probability to seethe word given the previous words in the sentence(n − 1 previous words in the case of the n-grammodels and all the previous words in the case ofgpt-2).
for the bert-based models, we computethe masked loss of the word given the sentence.
foreach title, we computed the mean, maximum, andvariance of the perplexity across all words in thetitle..4.2 simple languageinspired by previous ﬁndings (ruch, 1992;gultchin et al., 2019), we hypothesize that titles offunny papers tend to be simpler (e.g., the past ig no-bel winners: “chickens prefer beautiful humans”and “walking with coffee: why does it spill?”).
we utilize several simplicity measures:length.
short titles and titles containing manyshort words tend to be simpler.
we compute titlelength and word lengths (mean, maximum, andvariance of word lengths in the title).
readability.
we use the automated readabilityindex (smith and senter, 1967).
age of acquisition (aoa).
a well-establishedmeasure for word’s difﬁculty in psychology (brys-baert and biemiller, 2017), denoting word’s difﬁ-culty by the age a child acquires it.
we computemean, maximum and variance aoa..6obtained using nltk (nltk.org).
16aoa and perplexity.
many basic words canbe found in serious titles (e.g., ‘water’ in a hy-draulics paper).
funny titles, however, containsimple words which are also unexpected.
thus, wecombine aoa with perplexity.
we compute wordperplexity using the semantic scholar n-gram lmsand divide it by aoa.
higher values correspond tosimpler and unexpected words.
we compute themean, maximum, minimum, and variance..4.3 crude languageaccording to relief theory, crude and scatologicalconnotations are often considered humorous (shur-cliff, 1968) (e.g., the ig nobel winners “duration ofurination does not change with body size”, “acutemanagement of the zipper-entrapped penis”)..we trained a naive bayes svm (wang and man-ning, 2012) classiﬁer over a dataset of toxic andrude wikipedia comments (zafar, 2018), and com-pute title probability to be crude.
similar to theaoa feature, we believe that crude words shouldalso be unexpected to be considered funny.
asbefore, we divide perplexity by the word’s proba-bility of being benign.
higher values correspondto crude and unexpected words.
we compute themean, maximum, minimum, and variance..4.4 funny languagesome words (e.g., nincompoop, razzmatazz) are in-herently funnier than others (due to various reasonssurveyed by gultchin et al.
(2019)).
it is reason-able that the funniness of a title is correlated withthe funniness of its words.
we measure funninessusing the model of westbury and hollis (2019),quantifying noun funniness based on humor theo-ries and human ratings.
we measure the funninessof each noun in a title.
we also multiplied perplex-ity and funniness (for funny and unexpected) anduse the mean, maximum, minimum, and variance..4.5 feature importanceas a ﬁrst reality check, we plotted the distributionof our features between funny and not-funny papers(see appendix a.1 for representative examples).
for example, we hypothesized that titles of funnypapers might be linguistically similar to one-liners,and indeed we saw that the one-liner lm assignslower perplexity to funny papers.
similarly, wesaw a difference between the readability scores..to measure the predictive power of our literature-inspired features, we use the wilcoxon signed-rank.
wilcoxon.
feature.
valueunexpected language.
avg.
semantic scholar.
2-gram lm.
avg.
pos 2-gram lm.
avg.
one-liners.
2-gram lm.
4850.
18926.
6919.avg.
gpt-2 lmavg.
bert lm.
742117153simple language89311849316768.readabilitytitle’s length.
avg.
aoa values.
avg.
aoa +2-gram lm.
4882crude language.
p-value.
3.6e-39.
3e-7.
9.2e-33.
2.7e-319e-10.
8.6e-262.4e-62.2e-10.
4.6e-39.
crudeness classiﬁeravg.
crudeness +.
2-gram lm.
4755funny language.
17423.
2.3e-9.
1.8e-39.
avg.
funnynouns model.
avg.
funny nouns +.
2-gram lm.
20101.
1.7e-5.
8886.
3.2e-27.
table 1: wilcoxon and p-values for representative fea-tures using our dataset (tested differentiating ability be-tween funny and serious papers).
combining perplex-ity with other features seems particularly beneﬁcial..test7 (see table 1).
interestingly, all feature familiesinclude useful features.
combining perplexity withother features (e.g., surprising and simple words)was especially prominent.
in the next sections, wedescribe how we use those features to train modelsfor detecting ig nobel worthy papers..5 modelswe can now create models to automatically de-tect scientiﬁc humor.
as mentioned in section 4,one of our goals in this paper is to compare be-tween the nlp sota huge-models approach andthe literature-inspired approach.
thus, we traineda binary multi-layer perceptron (mlp) classiﬁerusing our dataset (described in section 3, see re-producibility details in appendix c.2), receiving asinput the 127 features from section 4. we namedthis classiﬁer ‘iggy’, after the ig nobel prize..7a non-parametric paired difference test used to assess.
whether the mean ranks of two related samples differ..17as baselines representing the contemporary nlpapproach (requiring huge compute and trainingdata), we used bert (devlin et al., 2018) andscibert (beltagy et al., 2019), which is a bertvariant optimized on scientiﬁc corpora, renderingit potentially more relevant for our task.
we ﬁne-tuned scibert and bert for ig nobel classiﬁca-tion using our dataset (see appendix c.3 for imple-mentation details)..we also experimented with two models com-bining bert/scibert with our features (seefigure 6 in appendix c.4), denoted as bertf /scibertf .
in the spirit of the original bert paper,we added two linear layers on top of the models andused a standard cross-entropy loss.
the input tothis ﬁnal mlp is the concatenation of two vectors:our features’ embedding and the last hidden vectorfrom bert/scibert ([cls]).
see appendix c.4for implementation details..for the sake of completeness, we note that wealso conducted exploratory experiments with sim-ple syntactic baselines (title length, maximal wordlength, title containing a question, title containinga colon) as well as bert trained on sarcasm detec-tion8.
none of these baselines was strong enoughon its own.
we note that the colon-baseline tendedto catch smart-aleck titles, but the topic was notnecessarily funny.
the sarcasm baseline achievednear guess-level accuracy (0.482), emphasizing thedistinction between the two humor tasks..6 evaluation on the dataset.
we ﬁrst evaluate the ﬁve models (iggy, scibert,bert, scibertf and bertf ) on our labeleddataset in terms of general accuracy and ig no-bel retrieval ability.
as naive baselines, we addedtwo bag of words (bow) based classiﬁes: randomforest (rf) and logistic regression (lr).
accuracy.
we randomly split the dataset to train,development, and test sets (80%−10%−10%), andused the development set to tune hyper-parameters(e.g., learning rate, number of training epochs).
table 2 summarizes the results.
we note that allﬁve models achieve very high accuracy scores andthat the simple bow models fall behind.
this givessome indication about the inherent difﬁculty of thetask.
both features-based iggy and bert-basedmodels outperform simple baseline.
scibertfoutperforms the other models across all measures..8kaggle.com/raghavkhemka/.
sarcasm-detection-using-bert-92-accuracy.
modeliggyscibertscibertfbertbertfrflr.
accuracy.
0.8970.9100.9220.9040.9000.7610.781.precision recall0.8930.9110.9260.8930.9020.7960.837.
0.9010.9110.9190.9060.8990.7460.754.table 2: accuracy of the different models on ourdataset using cross validation with k=5.
scibertf out-performs..modeliggyscibertscibertfbertbertfrflr.
accuracy.
0.8840.8820.9030.8630.9030.7130.765.precision recall0.8480.8480.8820.8100.8820.7250.787.
0.9130.9090.9210.9050.9210.7080.755.table 3: accuracy of the different models on our ig-nobel retrieval test set.
the combination of sota pre-trained models and our features is superior..ig nobel winners retrieval.
our positive exam-ples consist of 211 ig nobel winners and additional1,496 humorous papers found on the web.
thus,the portion of real ig nobel winning papers in ourdata is relatively small.
we now measure whetherour web-originated papers serve as a good proxyfor ig nobel winners.
thus, we split the datasetdifferently: the test set consists of the 211 ig no-bel winners, plus a random sample of 211 negativetitles (slightly increasing the test set size to 12%).
train set consists of the remaining 2,992 papers.
this experiment follows our initial inspiration ofﬁnding ig nobel-worthy papers, as we test our mod-els’ ability to retrieve only the real winners..table 3 demonstrate that our web-based funnypapers are indeed a good proxy for ig nobel win-ners.
similar to the previous experiment, the com-bination of sota pretrained models with literaturebased features is superior..based on both experiments, we conclude that ourfeatures are indeed informative for our ig nobel-worthy papers detection task..7 evaluation “in the wild”our main motivation in this work is to recommendpapers worthy of an ig nobel prize.
in this section,.
18titlethe kinematics of eating with a spoon: bringing the food to the mouth,or the mouth to the food?
do bonobos say no by shaking their head?
is anakin skywalker suffering from borderline personality disorder?
not eating like a pig: european wild boar wash their foodwhy don’t chimpanzees in gabon crack nuts?
why do people lie online?
“because everyone lies on the internet”which type of alcohol is easier on the gut?
rainbow connection and forbidden subgraphsa scandal of invisibility: making everyone count by counting everyonewhere do we look when we walk on stairs?
gaze behaviour on stairs,transitions, and handrails.
models.
iggy, bertf , scibertf.
iggy, bertf , scibertfiggy, bertf , scibertf.
iggy, bertf.
scibertf , bertf.
bertfbertfbert.
scibertscibert.
table 4: a sample of top rated papers found by our models..we test our models in a more realistic setting; werun them on a large sample of scientiﬁc papers,ranking each paper according to their certainty inthe label (‘humorous’), and identifying promisingcandidates.
we use the same dataset of 630k pa-pers from semantic scholar used for training thelms (section 4).
we compute funniness accordingto our models (excluding random forest and logis-tic regression, which performed poorly).
table 4shows examples of top-rated titles.
we use theamazon mechanical turk (mturk) crowdsourcingplatform to assess models’ performance..in an exploratory study, we asked people to ratethe funniness of titles on a likert scale of 1-5. wenoted that people tended to confuse funny researchtopic and funny title.
for example, titles like “areyou certain about sirt?” or “nash may be trash”received high funniness scores, even though the re-search topic is not even clear from the title.
tomitigate this problem, we redesigned the studyto include two 5-point likert scale questions: 1)whether the title is funny, and 2) whether the re-search topic is funny.
this addition seems to indeedhelp workers understand the task better.
examplepapers rated as serious title, funny topic include“hat-wearing patterns in spectators attending base-ball games: a 10-year retrospective comparison”.
funny title, serious topic include “slicing the psy-choanalytic pie: or, shall we bake a new one?
com-mentary on greenberg”.
unless stated otherwise,the evaluation in the reminder of the paper wasdone on the “funny topic” likert scale..we paid crowd workers $0.04 per title.
as thistask is challenging, we created a qualiﬁcation testwith 4 titles (8 questions), allowing for one mis-.
take.
the code for task and test can be found inthe repository2.
we also required workers to havecompleted at least 1,000 approved hits with atleast 97% success rate..all algorithms classiﬁed and ranked (accordingto certainty) all 630k papers.
however, in any rea-sonable use-case, only the top of the ranked list willever be examined.
there is a large body of work,both in academia and industry, studying how peo-ple interact with ranked lists (in particular, searchresult pages) (kelly and azzopardi, 2015; beus,2020).
many information retrieval algorithms as-sume the likelihood of the user examining a resultto exponentially decrease with rank.
the conven-tional wisdom is that users rarely venture into thesecond page of search results..thus, we posit that in our scenario of ig nobelrecommendations, users will be willing to read onlythe several tens of results.
we choose to evaluatethe top-300 titles for each of our ﬁve models, tostudy (in addition to the performance at the topof the list) how performance decays.
we also in-cluded a baseline of 300 randomly sampled titlesfrom semantic scholar.
altogether we evaluated1375 titles (due to overlap).
each title was rated byﬁve crowd workers.
overall, 13 different workerspassed our test.
seven workers annotated less than300 titles, while four annotated above 1,300 each.
decision rule.
each title was rated by ﬁve differ-ent crowd workers on a 1-5 scale.
there are severalreasonable ways to aggregate these ﬁve continuousscores to a binary decision.
a commonly-used ag-gregation method is the majority vote.
the major-ity vote should return the clear-cut humorous titles.
however, we stress that humor is very subjective.
19decision.
rulemin.
1annotatormin.
2.annotators.
min.
3.annotators.
threshold.
343434.expertcorr.
0.70.490.470.190.150.02.labeled data.
accuracy.
0.840.830.820.730.780.62.table 5: spearman correlation of mturk annotatorswith our expert, along with accuracy of mturk anno-tators on our labeled dataset for the various mappingmethods of the form “minimum (min.)
k annotatorsgave a score at least m (threshold)”..(and in the case of scientiﬁc humor, quite subtle).
indeed, annotators had low agreement on the topicquestion (average pairwise spearman ρ = 0.27)..thus, we explored more aggregation methods9.
our hypothesis class is of the general form “at leastk annotators gave a score at least m” 10. to pickthe best rule, we conducted two exploratory experi-ments: in the ﬁrst one, we recruited an expert scien-tist and thoroughly trained him on the problem.
hethen rated 90 titles and we measured the correlationof different aggregations with his ratings.
resultsare summarized in table 5: the highest-correlationaggregation is when at least one annotator crossedthe 3 threshold (spearman ρ = 0.7)..in the second experiment, we used the exactsame experimental setup as the original task, butwith labeled data.
we used 100 ig nobel winnersas positives and a random sample of 100 papers asnegatives.
the idea was to see how crowd workersrate papers that we know are funny (or not).
table5 shows the accuracy of each aggregation method.
interestingly, the highest accuracy is achieved withthe same rule as in the ﬁrst experiment (at least onecrossing 3).
thus, we chose this aggregation rule.
we believe the method outlined in this sectioncould be more broadly applicable to aggregation ofcrowd sourced annotations for subjective questions.
results.
figure 1 shows precision at k for thetop-rated 300 titles according to each model.
therandom baseline is ∼ 0.03. upon closer inspection,these seem to be false positives of the annotation.
we have argued that in our setting it is reasonablefor users to read the ﬁrst several tens of results..model.
iggyscibertscibertfbertbertf.
precisionat k=50.
0.60.570.530.440.58.precisionat k=300.
0.370.460.410.410.43.table 6: precision at k of our models on the seman-tic scholar corpus for k={50, 300}.
these relativelyhigh scores suggest that our models are able to identifyfunny papers..in this range, iggy slightly outperforms the otherfour models (bert is particularly bad, as it picksup on short, non-informative titles).
for larger kvalues scibert and bertf take the lead.
wenote that even at k = 300, all models still achieveconsiderable (absolute) precision..we obtain similar results using normalized dis-counted cumulative gain (ndcg), a common mea-sure for ranking quality (see table 6 for ndcgscores for the top 50 and the 300 papers).
overall,these relatively high scores suggest that our modelsare able to identify funny papers.
we stress that iggy is a small and simple network(∼ 33k parameters), compared to pretrained 110million parameters bert-based models.
yet de-spite its simplicity, iggy’s performance is roughlycomparable to bert-based methods.
we believethis demonstrates the power of implementing in-sights from domain experts.
we hypothesize thatif the ﬁne-tuning dataset were larger, bertf andscibertf would outperform the other models..8 analysis8.1importance of literature-based featurestaking a closer look at the actual papers in theexperiment of section 7, the overlap between thethree feature-based models is 26 − 56% (for 1 <k < 50) and 39 − 62% (for 1 < k < 300).
berthad very low overlaps with all other models (0% intop 50, 10% in all 300).
scibert had almost nooverlap in top 50 (maximum 2%), 10 − 40% in all300 (see full details in appendix a.3).
we believethis implies that the features were indeed importantand informative for both bertf and scibertf ..9for completeness, see figure 3 in appendix a.2.
10there is a long-running debate about whether it is validto average likert scores.
we believe we cannot treat the ratingsin this study as interval data..interpreting iggy.
8.2we have seen iggy performs surprisingly well,given its relative simplicity.
in this section, we.
20figure 1: precision at k for our chosen decision rule.
iggy outperforms the other models for 0 < k < 50. forlarger k, scibertf and bert achieve better precision..wish to better understand the reasons.
we choseto analyze iggy with shapely additive explanations(shap) (lundberg and lee, 2017).
shap is a fea-ture attribution method to explain the output of anyblack-box model, shown to be superior to more tra-ditional feature importance methods.
importantly,shap provides insights both globally and locally(i.e., for speciﬁc data points)..global interpretability.
we compute feature im-portance globally.
among top contributing featureswe see multiple features corresponding to incon-gruity (both alone and combined with funniness)and to word/sentence simplicity.
interestingly, fea-tures based on the one-liner jokes seem to play animportant role (see figure 4 in appendix a.4)..local interpretability.
to understand how iggyerrs, we examined the shap decision plots forfalse positives and false negatives (see figure 5in appendix a.4).
these show the contribution ofeach feature to the ﬁnal prediction for a given title,and thus can help “debugging” the model..looking at false negatives, it appears that variousperplexity features misled iggy, while funninessand joke lm steered it in the right direction.
wesee a contrary trend in false positives: perplexityhelped, and joke lm confused the classiﬁer..we also observe that the model learned that along title is an indication of a serious paper.
weexpected our rudeness classiﬁer to play a biggerrole in some of the titles (e.g., “adaptive inter-population differences in blue tit life-history traitson corsica”), but the signal was inconclusive, per-haps indicating our rudeness classiﬁer is lacking..8.3 observations.
we now take a more qualitative approach to un-derstand the models.
first, we set out to explorewhether the models confuse funny titles and funnytopics.
using the crowd sourced annotations fromsection 7, we measure the portion of this mistakein the top-rated 300 titles of all ﬁve models.
thatis, we check in how many cases our models clas-sify a title as “ig nobel-worthy” while the work-ers have classiﬁed it as “funny title and non-funnytopic”.
iggy had the highest degree of such con-fusion (0.28).
similarly, bertf and scibertfexhibit more confusion than the versions withoutfeatures (0.24, 0.19 compared to 0.13, 0.08).
ran-dom baseline is 0.02. examples of this kind of errorinclude “a victim of the occam’s razor.”, “whilewaiting to buy a ferrari, do not leave your currentcar in the garage!”, and “reinforcement learning:the good, the bad and the ugly?”.
all were clas-siﬁed as ig nobel-worthy, although their topic isserious (or even unclear from the title)..looking closer at the data, we observe that a highportion of these are editorials with catchy titles.
asour dataset does not differentiate between editorialsand real research contributions, ﬁltering editorialsis not straightforward.
interestingly, the portion ofeditorials is also greater in the lowest annotators’agreement area, hinting that this confusion alsooccurs in humans..in addition to editorials, we notice another cat-egory of papers causing the same type of confu-sion.
there are papers dealing with disturbing orunfortunate topics (violence, death, sexual abuse),whose titles include literary devices used to lighten.
21the mood.
censored (for the readers’ own well-being) examples include “licorice for hepatitis c:yum-yum or just ho-hum?”, “the song of the siren:dealing with masochistic thoughts and behaviors”.
a note on scientiﬁc disciplines.
another observa-tion we make concerns with the portion of ig nobel-worthiness across the different scientiﬁc disciplines.
we notice that most papers classiﬁed by our mod-els as funny belong to social sciences (“dogs candiscriminate human smiling faces from blank ex-pressions”) or medicine (“what, if anything, canmonkeys tell us about human amnesia when theycan’t say anything at all?”), compared to exact sci-ences (“the kinematics of eating with a spoon:bringing the food to the mouth, or the mouth to thefood?”).
we believe this might be the case since,quite often, social sciences and medicine papersstudy topics that are more familiar to the layperson.
we also note that although our models performedabout the same across the different disciplines, theywere slightly better in psychology..9 conclusions & future work.
in this work, we presented a novel task in humorrecognition – detecting funny and unusual scientiﬁcpapers, which represents a subtle and sophisticatedhumor type.
it has important characteristics (short,simple syntax, stand-alone) making it a (relatively)clean setting to explore computational humor..we created a dataset of funny papers and con-structed models, distilling humor literature into fea-tures as well as harnessing sota advances in nlp.
we conducted experiments both on our dataset andin a real-world setting, identifying funny papersin a corpus of over 0.6m papers.
all models wereable to identify funny papers, achieving high ndcgscores.
interestingly, despite the simplicity of theliterature-based iggy, its performance was overallcomparable to complex, bert-based models..our dataset can be further used for various hu-mor related tasks.
for example, it is possible to useit to create an aligned corpus, pairing every funnypaper title with a nearly identical but serious title,using methods similar to west and horvitz (2019).
this would allow us to understand why a paper isfunny at a ﬁner granularity, by identifying the exactwords that make the difference.
this technique willalso allow exploring different types of “funny”..another possible use of our dataset is to col-lect additional meta-data about the papers (e.g.,citations, author information) to explore questions.
about whether funny science achieves dispropor-tionate attention and engagement, who tends toproduce it (and at which career stage), with impli-cations to science of science and science communi-cation..another interesting direction is to expand be-yond paper titles and consider the paper abstract,or even full text.
this could be useful in examplessuch as the ig nobel winner “cure for a headache”,which takes inspiration from woodpeckers to helpcure headaches in humans..finally, we believe multi-task learning is a di-rection worth pursuing towards creating a moreholistic and robust humor classiﬁer.
in multi-tasklearning, the learner is challenged to solve multipleproblems at the same time, often resulting in bet-ter generalization and better performance on eachindividual task (ruder, 2017).
as multi-task learn-ing enables unraveling cross-task similarities, webelieve it might be particularly fruitful to apply totasks highlighting different aspects of humor.
webelieve our dataset, combined with other task spe-ciﬁc humor datasets, could assist in pursuing sucha direction..despite the tongue-in-cheek nature of our task,we believe that computational humor has tremen-dous potential to create personable interactions,and can greatly contribute to a range of nlp ap-plications, from chatbots to educational tutors.
wealso wish to promote complementing data-drivenresearch with insights from more-traditional ﬁelds.
we believe combining such insights could, in addi-tion to improving performance, enrich our under-standing of core aspects of being human.
acknowledgmentswe thank the reviewers for their insightful com-ments.
we thank omri abend, michael doron andmeirav segal, ronen tamari and moran mizrahifor their help, and shuki cohen for preliminarydiscussions.
this work was supported by the euro-pean research council (erc) under the europeanunion’s horizon 2020 research and innovation pro-gramme (grant no.
852686, siam), us nationalscience foundation, us-israel binational sciencefoundation (nsf-bsf) grant no.
2017741, andamazon research awards..referencesiz beltagy, arman cohan, and kyle lo.
2019. scibert:pretrained contextualized embeddings for scientiﬁc.
22text.
arxiv preprint arxiv:1903.10676..dario bertero and pascale fung.
2016. a long short-term memory framework for predicting humor in di-alogues.
in proceedings of the 2016 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 130–135..johannes beus.
2020. why (almost) everything you.
knew about google ctr is no longer valid..vladislav blinov, valeria bolotova-baranova, andpavel braslavski.
2019. large dataset and languagein pro-model fun-tuning for humor recognition.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4027–4032..marc brysbaert and andrew biemiller.
2017. test-based age-of-acquisition norms for 44 thousand en-glish word meanings.
behavior research methods,49(4):1520–1523..dmitry davidov, oren tsur, and ari rappoport.
2010.semi-supervised recognition of sarcastic sentencesin twitter and amazon.
in proceedings of the four-teenth conference on computational naturallan-guage learning, pages 107–116.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..david donahue, alexey romanov,.
and annarumshisky.
2017. humorhawk at semeval-2017task 6: mixing meaning and sound for humor recog-in proceedings of the 11th internationalnition.
workshop on semantic evaluation (semeval-2017),pages 98–102..limor gultchin, genevieve patterson, nancy baym,nathaniel swinger, and adam tauman kalai.
2019.humor in word embeddings: cockamamie gob-arxiv preprintbledegook for nincompoops.
arxiv:1902.02783..nabil hossain, john krumm, lucy vanderwende, erichorvitz, and henry kautz.
2017. filling the blanks(hint: plural noun) for mad libs humor.
in proceed-ings of the 2017 conference on empirical methodsin natural language processing, pages 638–647..diane kelly and leif azzopardi.
2015. how many re-sults per page?
a study of serp size, search behaviorand user experience.
in proceedings of the 38th in-ternational acm sigir conference on research anddevelopment in information retrieval, pages 183–192..chloe kiddon and yuriy brun.
2011. that’s what shesaid: double entendre identiﬁcation.
in proceedingsof the 49th annual meeting of the association for.
computational linguistics: human language tech-nologies: short papers-volume 2, pages 89–94.
as-sociation for computational linguistics..lizhen liu, donghai zhang, and wei song.
2018.modeling sentiment association in discourse for hu-mor recognition.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 586–591..scott m lundberg and su-in lee.
2017. a uniﬁedin ad-approach to interpreting model predictions.
vances in neural information processing systems,pages 4765–4774..antonios maronikolakis, danae s´anchez villegas,daniel preot¸iuc-pietro, and nikolaos aletras.
2020.analyzing political parody in social media.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4373–4384..william h martineau.
1972. a model of the socialfunctions of humor.
the psychology of humor, pages101–125..rada mihalcea and carlo strapparava.
2005. makingcomputers laugh: investigations in automatic humorin proceedings of human languagerecognition.
technology conference and conference on empiri-cal methods in natural language processing, pages531–538..rada mihalcea and carlo strapparava.
2006. learn-ing to laugh (automatically): computational modelsfor humor recognition.
computational intelligence,22(2):126–142..tristan miller, christian f hempelmann, and irynagurevych.
2017. semeval-2017 task 7: detectionand interpretation of english puns.
in proceedings ofthe 11th international workshop on semantic evalu-ation (semeval-2017), pages 58–68..abhinav moudgil.
2016. short-jokes-dataset..dallin d oaks.
1994. creating structural ambiguitiesin humor: getting english grammar to cooperate.
humor, 7(4):377–402..reynier ortega-bueno, carlos e muniz-cuza, jos´ee medina pagola, and paolo rosso.
2018. uo upv:deep linguistic humor detection in spanish socialin proceedings of the third workshop onmedia.
evaluation of human language technologies foriberian languages (ibereval 2018) co-located with34th conference of the spanish society for naturallanguage processing (sepln 2018)..tom´aˇs pt´aˇcek, ivan habernal, and jun hong.
2014.sarcasm detection on czech and english twitter.
in proceedings of coling 2014, the 25th inter-national conference on computational linguistics:technical papers, pages 213–223..23alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..victor raskin.
1985. semantic theory..in semantic.
mechanisms of humor, pages 59–98.
springer..antonio reyes and paolo rosso.
2012. making ob-jective decisions from subjective data: detectingirony in customer reviews.
decision support sys-tems, 53(4):754–760..antonio reyes, paolo rosso, and davide buscaldi.
2012. from humor recognition to irony detection:the ﬁgurative language of social media.
data &knowledge engineering, 74:1–12..willibald ruch.
1992. assessment of appreciation ofhumor: studies with the 3 wd humor test.
advancesin personality assessment, 9:27–75..sebastian ruder.
2017..task learning in deep neural networks.
abs/1706.05098..an overview of multi-arxiv,.
itay sagi and eldad yechiam.
2008. amusing titlesin scientiﬁc journals and article citation.
journal ofinformation science, 34(5):680–687..nobel.
prize committee the royal.
academy of science.
2010.https://www.nobelprize.org/prizes/physics/2010/press-release/.
october 2010]..press.
[online; 5.swedishrelease..dafna shahaf, eric horvitz, and robert mankoff.
2015.inside jokes: identifying humorous cartoon captions.
in proceedings of the 21th acm sigkdd inter-national conference on knowledge discovery anddata mining, pages 1065–1074..joshua shaw.
2010. philosophy of humor.
philosophy.
compass, 5(2):112–126..arthur shurcliff.
1968. judged humor, arousal, and therelief theory.
journal of personality and social psy-chology, 8(4p1):360..edwin simpson, erik-lˆan do dinh, tristan miller, andiryna gurevych.
2019. predicting humorousnessand metaphor novelty with gaussian process prefer-in proceedings of the 57th annualence learning.
meeting of the association for computational lin-guistics, pages 5716–5728..edgar a smith and rj senter.
1967. automated read-ability index.
amrl-tr.
aerospace medical re-search laboratories (us), page 1..alan d sokal.
1996. transgressing the boundaries:toward a transformative hermeneutics of quantumgravity.
social text, (46/47):217–252..oliviero stock and carlo strapparava.
2002. ha-hacronym:for humorousacronyms.
stock, oliviero, carlo strapparava, andanton nijholt.
eds, pages 125–135..humorous.
agents.
jerry m suls.
1972. a two-stage model for the ap-preciation of jokes and cartoons: an information-processing analysis.
the psychology of humor: the-oretical perspectives and empirical issues, 1:81–100..julia m taylor and lawrence j mazlack.
2004. com-putationally recognizing wordplay in jokes.
in pro-ceedings of the annual meeting of the cognitive sci-ence society, volume 26..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..sida i wang and christopher d manning.
2012. base-lines and bigrams: simple, good sentiment and topicin proceedings of the 50th annualclassiﬁcation.
meeting of the association for computational lin-guistics (volume 2: short papers), pages 90–94..melissa bekelja wanzer, melanie booth-butterﬁeld,and steve booth-butterﬁeld.
1996. are funny peo-ple popular?
an examination of humor orientation,loneliness, and social attraction.
communicationquarterly, 44(1):42–52..miaomiao wen, nancy baym, omer tamuz, jaimeteevan, susan t dumais, and adam kalai.
2015.omg ur funny!
computer-aided humor with an ap-plication to chat.
in iccc, pages 86–93..robert west and eric horvitz.
2019..reverse-engineering satire, or” paper on computational hu-mor accepted despite making serious advances”.
arxiv preprint arxiv:1901.03253..chris westbury and geoff hollis.
2019. wriggly,squiffy, lummox, and boobs: what makes somewords funny?
journal of experimental psychology:general, 148(1):97..david wilmot and frank keller.
2020. modelling sus-pense in short stories as uncertainty reduction overneural representation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 1763–1788, online.
associationfor computational linguistics..zafar.
2018. cleaned toxic comments..renxian zhang and naishi liu.
2014. recognizing hu-mor on twitter.
in proceedings of the 23rd acm in-ternational conference on conference on informa-tion and knowledge management, pages 889–898.
acm..yftah ziser, elad kravi, and david carmel.
2020. hu-mor detection in product question answering sys-tems.
in proceedings of the 43rd international acmsigir conference on research and development ininformation retrieval, pages 519–528..24a supplementary ﬁgures.
a.1 dataset analysis.
in section 4 we presented 127 humor literature-based features.
here we present the distribution oftwo example features in funny vs. serious papers inour dataset (described in section 3).
these exam-ples represent the general trend, as many featuresshow predictive power (see figure 2)..a.2.
“in the wild” study results.
for the “in the wild” evaluation executed using se-mantic scholar data, we used crowdsourcing anno-tations (see section 7).
each title was rated by ﬁvedifferent crowd workers on a 1-5 scale, while ourmodels provide binary decision.
there are severalreasonable ways to aggregate these ﬁve continuousscores to a binary decision.
we choose a rule in adata-driven manner (see “decision rule” in section7).
for completeness, here we show the commonly-used aggregation method of majority.
we showhere the precision at k of our ﬁve models usingthe majority vote aggregation rule with a cutoff at3 (see figure 3).
iggy outperforms until k = 30,where scibertf takes the lead afterwards..a.3 models’ overlap.
in section 8.1 we discuss the importance of ourliterature-based features by showing that modelswho received them as input indeed found themuseful.
the overlap was measured on the top 50and top 300 papers retrieved using our ﬁve modelson the semantic scholar data (see section 7 forthe full experimental setup).
the overlap betweenthe 3 features-based models was found to be high(see table 7).
both bert and scibert had verylow overlaps with all other models.
we believe thisimplies that the features were indeed important forour sota based models, bertf and scibertf ..a.4 shap analysis.
in section 8.2 we analysed iggy using shap(lundberg and lee, 2017).
we compute featureimportance globally (figure 4).
to understand howiggy errs, we examined the shap decision plotsfor false positives and false negatives (figure 5).
decision plots show the contribution of each fea-ture to the ﬁnal prediction for a given title.
thus, itcan help “debugging” the model’s mistakes..b reproducibilityb.1 code and data availabilitydataset, code, and data ﬁles can be found in ourgithub repository2..c implementation detailsc.1 fine-tuning gpt-2 lmto ﬁne-tune gpt-2 we used huggingface’s trans-formers package11.
we ﬁne-tuned the model usinglearning rate = 5e−5, one epoch, batch size of 4,weight decay = 0, max gradient norm = 1 andrandom seed = 42. optimization was done usingadam with epsilon = 1e−8.
model conﬁgurationswere set to default..iggy classiﬁer.
c.2we used a simple mlp with a single hidden layerof 256 neurons.
we trained the mlp until conver-gence, using adam optimizer, a learning rate of0.001 and an l2 penalty of 2..c.3 fine-tuning scibert & bertto ﬁne-tune scibert & bert we used hugging-face’s transformers package.
we ﬁne-tuned bothmodels with learning rate = 5e−5 for 3 epochswith batch size of 32, maximal sequence length of128 and random seed = 42. optimization was doneusing adam with warm-up = 0.1 and weight decayof 0.01 model conﬁgurations were set to default..c.4 scibertf & bertf modelsas speciﬁed in section 5, these models were con-structed as follows (see figure 6).
each model hadtwo inputs – the raw text of the title, and a vectorof our 127 features.
the feature vector is fed toan mlp with a single hidden layer of 512 neuronsand an output size of 512 neurons as well.
the rawtext is fed to a frozen scibert /bert model.
wecollect the last hidden vector ([cls]) from bert/scibert.
next, we concatenate this vector to theoutput of the features-mlp network and pass theresult to a second mlp with a single hidden layerof 1,024 neurons.
the output of this mlp, then, isfed to a softmax layer, which represents the ﬁnalprediction of the model..we train the model using a cross-entropy lossand the same parameters that were used to train thevanilla scibert /bert model.
those parametersare described in appendix c.3..11huggingface.co/transformers/.
25figure 2: distribution and gaussian ﬁt of two representative features: one-liners 2-gram lm mean perplexity (left)and automated readability index (right), indicating the predictive power of these features..figure 3: precision at k for majority vote.
iggy outperforms until k = 30 and bertf takes the lead afterwards..figure 4: feature importance shap analysis done on the iggy model.
the top variables according to this plotcontribute more than the bottom ones (have high predictive power).
the analysis reveals that the highest contribu-tion corresponds to short, funny, and simple words (where simplicity was measured using features such as aoaand readability).
we also notice that features which are based on the one-liners lms contributed much to the ﬁnalprediction, meaning that there is indeed some similarity between funny titles and short jokes..26bertfscibertfbertscibert.
iggy.
0.56 (cid:107) 0.620.26 (cid:107) 0.39.
0 (cid:107) 00 (cid:107) 0.3.bertf0.36 (cid:107) 0.480 (cid:107) 0.10.02 (cid:107) 0.4.scibertf.
bert.
0 (cid:107) 0.10.02 (cid:107) 0.2.
0 (cid:107) 0.1.table 7: models’ overlap for the top rated 50 and 300 (left number in a cell corresponds to the overlap in the top50 and right number corresponds to the 300).
the overlap between the 3 features-based models was found to behigh compared with bert and scibert.
we believe this implies that the features were indeed important for oursota based models, bertf and scibertf ..(a) shap decision plot for the 12 false negative of iggy from our test set.
perplexity features misled iggy, while funniness andjoke lm ones provided informative input..(b) shap decision plot for the 11 false positive of iggy from our test set.
perplexity helped shifting the output towards thecorrect label, joke lm features confused the classiﬁer.
figure 5: shap decision plot for iggy’s false negatives and positives from our test set.
decision plots show thecontribution of each feature to the ﬁnal prediction for a given data point.
starting at the bottom of the plot, theprediction line shows how the shap values (i.e., the feature effects) accumulate to arrive at the model’s ﬁnal scoreat the top of the plot.
to get a better intuition, one can think of it in terms of a linear model where the sum ofeffects, plus an intercept, equals the prediction..27figure 6: the ﬂow of scibertf /bertf .
a 2-layers mlp recieves an input the concatenation of two vectors: ourfeatures’ embedding and the last hidden vector ([cls]) from bert /scibert..28