hierarchical context-aware network for dense video event captioning.
lei ji1,2,3 ∗, xianglin guo4∗, haoyang huang3,xilin chen1,21institute of computing technology, cas, beijing, china2university of chinese academy of sciences, beijing, china3microsoft research asia, beijing, china4new york university, new york, usaleiji@microsoft.com,xg893@nyu.eduhaoyang.huang@microsoft.com,xlchen@ict.ac.cn.
abstract.
dense video event captioning aims to gener-ate a sequence of descriptive captions for eachevent in a long untrimmed video.
video-levelcontext provides important information and fa-cilities the model to generate consistent andless redundant captions between events.
inthis paper, we introduce a novel hierarchicalcontext-aware network for dense video eventcaptioning (hcn) to capture context from var-ious aspects.
in detail, the model leverageslocal and global context with different mech-anisms to jointly learn to generate coherentcaptions.
the local context module performsfull interaction between neighbor frames andthe global context module selectively attendsto previous or future events.
according toour extensive experiment on both youcook2and activitynet captioning datasets, the video-level hcn model outperforms the event-levelcontext-agnostic model by a large margin.
thecode is available at https://github.com/kirkguo/hcn..1.introduction.
with the increase of video data uploaded online ev-ery day, the acquisition of knowledge from videosespecially for howto tasks is indispensable for peo-ple’s daily life and work.
however, watching awhole long video is time-consuming.
existing tech-nologies focus on two main research directionsto compact video information: video summariza-tion to trim long videos to short ones and (dense)video captioning to generate a textual descriptionof the key events in the video.
typically for longuntrimmed videos, dense video event captioninggenerates ﬁne-grained captions for all events tofacilitate users quickly skimming the video con-tent and enables various applications e.g.
videochaptering and search inside a video..∗equal contribution.
figure 1: a showcase of dense video event captioning.
given a video and the speech text, the task is to gener-ate event proposals and captions..dense video event captioning (krishna et al.,2017) and multi-modal video event captioning(iashin and rahtu, 2020b) aims to generate a se-quence of captions for all events regarding touni-modality (video) or multi-modality (video +speech) inputs.
figure 1 presents a showcase,which demonstrates the challenges of this task fromboth vision and speech text perspective.
for visionunderstanding, the ﬁne-grained objects are hardto recognize due to ambiguity, occlusion, or statein this case, the object ”dough” is oc-change.
cluded in event 1 and is hard to recognize fromthe video.
however, it can be recognized fromthe previous neighbor video frame with a clear ap-pearance.
from speech text perspective, althoughthe speech text offers semantic concepts (shi et al.,2019; iashin and rahtu, 2020b), it brings anotherchallenge of co-reference and ellipsis in speech textdue to the informal utterance of oral speeches.
inthe case of figure 1, the entity ”dough” in event 3is an ellipsis in the text.
nonetheless, it is capableof generating consistent objects ”dough” in event 3with the contextual information from other eventssuch as event 1 in this example.
to sum up, both.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2004–2013august1–6,2021.©2021associationforcomputationallinguistics2004local neighbor-clip and global inter-event contextsare important for event-level captioning to generatecoherent and less duplication descriptions betweenevents..previous endeavors widely used recurrent neu-ral network(krishna et al., 2017) which suffersfrom capturing long dependency, while recentlyattention-based model(zhou et al., 2018b; sunet al., 2019b,a) is becoming the new paradigmfor dense video event captioning and effective formulti-modal video captioning (shi et al., 2019;iashin and rahtu, 2020b).
however, existingattention-based models generate the captioningonly relying on the video clip inside each event,and ignore video-level local and global context.
motivated by this, we mainly investigate how to ef-fectively and jointly leverage both local and globalcontext for video captioning..in this paper, we propose a novel hierarchicalcontext-aware model for dense video event cap-tioning (hcn) to capture both the local and globalcontext simultaneously.
in detail, we ﬁrst exploit alocal context encoder to embed the visual and lin-guistic features of the source and surrounding clips,then design a global context encoder to capture rel-evant features from other events.
speciﬁcally, weapply different mechanisms: a ﬂat attention mod-ule between the source and local context; a crossattention module for the source to select the globalcontext.
with regards to the neighbor frames (tem-porally close) usually alike, e.g.
with the sameobjects, the ﬂat attention is a full interaction togenerate accurate and coherent captions.
contem-poraneously, the cross attention on global contextcan selectively attend to the relevant events and cap-ture prior temporal dependency between events togenerate coherent and less duplicate captions.
theexperimental results demonstrate the effectivenessof our model.
our contributions can be summa-rized as:.
1) we propose a hierarchical context-awaremodel for dense video event captioning to capturevideo-level context..2) we carefully design different mechanisms tocapture both local and global context: a ﬂat atten-tion model with full interaction between neighborframes and a cross attention model to selectivelycapture inter-event features..agnostic model to a large extent..2 preliminary.
the dense video event captioning task is to producea sequence of events and generate a descriptive sen-tence for each event given a long untrimmed video.
in this work, we focus only on the task to generatecaptions and directly apply the ground-truth eventproposals similar to (hessel et al., 2019; iashin andrahtu, 2020b).
the paradigm for video captioningis an encoder-decoder network, which inputs videofeatures and outputs descriptions for each event.
in this section, we describe the task formulationincluding the context-agnostic model as well as thecontext-aware model in one framework..2.1 overview.
problem deﬁnition we deﬁne a sequence of eventsegment proposals as e = (cid:8)ei|i ∈ [1, m](cid:9), repre-senting the video with m proposals, ei is the featureof the i-th event including both video and text fea-ture, ei = {vi, ti}, where vi is video feature andti is transcript text feature (if available) of the i-thevent.
we take all the video frames and transcripttokens of the event between the start and end time.
the number of video frames is likely to be dif-ferent from the number of text tokens dependingon the actual video clip.
given all events e, thegoal is to predict the target descriptive sentencesy = (cid:8)yi|i ∈ [1, m](cid:9).
each yi is a sequence ofdescriptive words corresponding to each event ei.
the probability of the expected sentences y..p (y |e) = −.
p (yi | ei).
(1).
m(cid:89).
i=1.
which is to predict yi conditioned on the event ei.
the context-aware model considers local contextv(cid:54)=i (the neighboring video clip) and global contexte(cid:54)=i (the clips of past and future events) respectively.
the context-aware probability can be approximatedas.
p (y |e) = −.
p (yi | ei, v(cid:54)=i, e(cid:54)=i).
(2).
m(cid:89).
i=1.
3 methodology.
3.1 context-agnostic model.
3) experimental results on both youcook2 andactivitynet captions dataset demonstrate the effec-tiveness of our models and outperforms the context-.
the context-agnostic model of captioning is togenerate a descriptive sentence given the short-trimmed video clip of each event.
the paradigm.
2005for multi-modal video captioning is an encoder-decoder network as in (hessel et al., 2019).
first,we pre-process each event and extract features sep-arately.
for the event ei, we extract both video fea-ture vi and transcript feature ti if available.
next,both the video features and transcript features areconcatenated together as the input to the trans-former encoder.
this encoder implements self-attention of each modality and cross attention be-tween both modalities in one uniﬁed transformer.
finally, a transformer decoder generates the text to-kens of the description with the enhanced features..3.2 context-aware model.
we propose a context-aware video event captioningmodel with a hierarchical context-aware network(hcn) and the architecture is a general frameworkfor either uni-modal or multi-modal inputs as ex-plained in figure 2..figure 2: hcn provides a general framework with 4modules: local context module (lcm), global contextmodule (gcm), context gate, and decoder.
lcm en-hances the visual feature by local context and option-ally fuse both visual and text features with multi-modalinputs.
gcm employs a cross attention model to en-code the source visual feature with other event features,which employs the sencoder to encode source and con-text separately and adopts the cencoder to selectivelyattend to context..3.2.1 multi-modal feature representationfor visual features, we adopt a pre-trained 3d fea-ture extractor to extract k features as vi = (cid:8)vj|j ∈[1, k](cid:9) of the i-th event.
we further add a pro-jection layer to map the raw feature to the inputdimension through an embedding layer f (vi) =.
{e|e = embedding(vi)}.
for transcript text, wetokenize the text into words and represent eachword with 1-hot representation.
the tokens withineach event are represented as ti = {tj|j ∈ [1, l]},where l is the length of the tokens correspondingto the number of the transcript text in the speechof the event.
moreover, we embed each token tocontinuous representation by an embedding layerf (ti) = {e|e = embedding(ti)}.
similar to thework in (hessel et al., 2019), we build the vocabu-lary using all tokens in the captioning sentence..the input for each event comprises of three typesof embedding: 1) visual feature f (vi) (and speechtext feature f (ti) if available); 2) position embed-ding p(vi) and p(ti) as introduced in the trans-former model(vaswani et al., 2017); 3) type em-bedding s(vi) and s(ti) representing whether thecurrent embedding is from context or source..e(vi) = [f (vi) + p(vi) + s(vi)]e(ti) = [f (ti) + p(ti) + s(ti)].
(3)(4).
where + is the add operator, e(vi) and e(ti) arethe embeddings of video and text respectively.
formulti-modal input, both visual and text features areconcatenated for further processing..we extract two types of contextual information:event-agnostic local context and event-aware globalcontext.
event-agnostic context takes frames tem-porally close to the video event.
video is a continu-ous signal and neighboring video frames are likelyto be semantically related to each other e.g.
sameobjects.
this is especially helpful for recognizingobjects with state change or occluded in the currentevent.
moreover, objects are likely to be explicitlymentioned in the contextual transcript which canbe used to deal with object co-reference and ellip-sis typically for instructional videos.
event-awarecontext utilizes the video frames of both previousand future events, which attempts to model the re-lation between events.
the global context providesoverall features and prior knowledge of temporaldependency.
speciﬁcally for a particular domainlike a recipe, the event “mix the ﬂour and water” isoften followed by “knead the dough”.
this priorknowledge of event dependency learned from aglobal context is effective for understanding longvideos..3.2.2 hierarchical context-aware networkthe overall pipeline includes 4 modules: 1) thehierarchical model starts with a local context mod-ule (lcm) to encode the local context features,.
2006the neighbor video clip temporally close to theevent.
speciﬁcally, the lcm adopts a ﬂat atten-tion model similar to (ma et al., 2020) to enhancethe source video feature by local context.
besides,given multi-modal inputs, lcm is a general modelto fuse both the visual features f (vi) and the textfeatures f (ti) inside the event with one uniﬁedtransformer as in (hessel et al., 2019); 2) we fur-ther employ a global context module (gcm) tomake the source event to interact with other eventfeatures ﬂexibly.
the gcm is a cross attentionmodel, which contains one source encoder sen-coder and one cross encoder cencoder.
sencoderis a self-attention module for encoding event fea-tures, and cencoder is a cross attention module forinteraction between source and context events; 3)the hierarchical context-aware model further com-bines both the neighbor-clip (around the event) orinter-event (other events) context from both previ-ous and future using gating mechanism; 4) ﬁnally,an auto-regressive decoder is used to generate thesentence with a masked transformer model..local context module we ﬁrst introduce the lo-cal context module to encode multi-modal sourcevideo features together with the event-agnostic con-text features (surrounding frames).
the ﬂat trans-former in (ma et al., 2020) is effective for encod-ing contextual information with full interaction be-tween source and context features.
in addition,when the speech text is available for multi-modalvideo captioning, this ﬂat encoder can also performthe fusion of visual and text modalities, which issimilar to (hessel et al., 2019).
to sum up, we em-ploy one uniﬁed ﬂat encoder to accomplish two ac-tions: source-context interaction and multi-modalfusion as explained in figure 3a..e(ei) = [e(vi); e(ti)]h(mi) = ffn(multihead([e(vi±kl ); e(ei)]))h(el.
i) = h(mi)[i1 : in].
(5)(6).
(7).
where [;].
is concatenation operation, ffnmeans the feed-forward network and multiheadis the multi-head attention network in trans-former(vaswani et al., 2017).
we apply residualconnection for all components.
we only performequation 5 for multi-modal video event captioning,and e(ei) is the concatenation of the visual em-bedding and text embedding for the event i. wethen feed the embedding e(ei) together with theembedding of neighbor frames e(vi±kl) into the.
(a) local context module.
(b) global context module.
figure 3: the local context module (lcm) is a ﬂatattention model which adopts a uniﬁed attention modelfor interaction and fusion, and only selects the output ofsource embedding for further processing.
global con-text module (gcm) is a cross attention model, whichadopts a cross attention model to selectively attend tocontext.
finally, a gru gate ⊕ is used to combine thecontext-enhanced feature with the source feature..transformer blocks and get context-aware encodingh(mi), and kl is the local context length.
finally,we only select the output of source encoding in-stead of using all embedding for further processing.
intuitively, the source is more important than thecontext.
in equation 7, h(eli) is the hidden stateof the source input, which requires the model tofocus on the current source event, i1 is the start ofthe event i and in is the end of the event i. lcmoutputs the enhanced event representation by localcontext and multi-modal inputs..global context module we then illustrate theglobal context module to encode the output oflcm together with event-aware context (previ-ous or future events).
gcm is a cross attentionmodel, which selectively attends to previous orfuture events to enhance the source video repre-sentation.
different from lcm, which applies auniﬁed transformer to encode a short context, gcmexploits a cross attention model similar to (marufet al., 2019) to encode long global context efﬁ-ciently.
the uniﬁed transformer model is hard todeal with long input sequences due to complexity.
the cross attention model facilitates the source tointeract with each context event and can easily bescaled out for long videos.
figure 3b illustrates thegcm model structure..we exploit the gcm for each contextual eventand then combine all the encoding through a con-text gating mechanism similar to (maruf et al.,2019).
first, the self-attention module encodeseach source or context event separately.
then, the.
2007cross attention module empowers the source to at-tend to context..4 experiment.
h(ˆei) = ffn(multihead([h(eli)]))h(ej) = ffn(multihead([e(ej)]))h(ec.
j) = ffn(multihead([h(ˆei), h(ej)])).
(8)(9)(10).
where h(ˆei) is the encoding of source event i,h(ej) is encoding of the j-th context event, andh(ecj) is the source attended to the j-th event.
next, we adopt a gated recurrent unit (gru)(cho et al., 2014) to selectively update the sourcefeature with context enhanced feature which isshown to be effective in our ablation study..zj = σ(wzh(ˆei) + uzh(ecj) + bz)rj = σ(wrh(ˆei) + urh(ecj) + br)ˆhj = φ(whh(ˆei) + uh(rj (cid:12) h(echj = (1 − zj) (cid:12) h(ec.
j) + zj ˆhj.
j) + bh).
(11)(12).
(13).
(14).
where σ is a logistic sigmoid operation, φ is theactivation function tanh, w and u are learnableweight matrices, and hj is the encoded representa-tion after the source event i attended to the contextevent j..context gating we adopt the gate in (tu et al.,2018) to regulate the source h(eli) and contextinformation hj.
then we get the context-enhancedsource embedding for further decoding..γ = σ(wjhp + wkhf )hc = γhp + (1 − γ)hfλ = σ(wchc + wsh(eli))h = λhc + (1 − λ)h(eli).
(15)(16).
(17).
(18).
where hc is the integration of all previous contexthp and future context hf .
the wj, wk, wc and wsare learnable parameter matrices, and h is the ﬁnalrepresentation..3.2.3 decoding and loss.
the decoder is an auto-regressive transformermodel to generate tokens one by one.
we adoptthe cross-entropy loss to minimize the negative log-likelihood over ground-truth words and apply thelabel smoothing strategy..l = −.
logp (yi | ei, v(cid:54)=i, e(cid:54)=i).
(19).
m(cid:88).
i=1.
4.1 dataset and evaluation metrics.
we run our experiments on both youcook2 dataset(zhou et al., 2018a) and activitynet captiondataset (krishna et al., 2017).
youcook2 is thetask-oriented instructional video dataset for videoprocedural captioning on the recipe domain.
wefollow the data partition in videobert (sun et al.,2019b) which uses 457 videos in the youcook2validation set as the testing set and the rest fordevelopment.
in all, we use 1,278 videos for train-ing and validation.
we extract the visual featureby s3d model pre-trained on howto100m(miechet al., 2019) dataset through mil-nce(miech et al.,2020) model.
this visual representation is a betterrepresentation of howto videos.
the asr tran-script is automatically extracted from the off-the-shelf recognition tool1..different from the youcook2 dataset, activitynetcaptions are open-domain videos with overlappingproposals, while youcook2 has non-overlappingevent proposals.
we apply the same data parti-tion in (iashin and rahtu, 2020b) with the groundtruth labels.
we directly download the copy ofthe dataset in (iashin and rahtu, 2020b) whichcontains 9,167 (out of 10,009) training and 4,483(out of 4,917) validation videos.
the dataset onlycontains partially available videos (91%) due tono longer available youtube links.
to make a faircomparison, we only list the experimental resultson the same dataset.
this open-source code anddata portal contains the speech content extractedfrom the closed captions (cc) from the youtubeasr system..we employ the metrics bleu3, bleu4 (pa-pineni et al., 2002), meteor (banerjee andlavie, 2005), rouge-l(lin and och, 2004) andcider(vedantam et al., 2015) to evaluate the per-formance.
we follow the work in (iashin and rahtu,2020a) on activitynet caption dataset which re-ported bleu3, bleu4 and meteor.
we directlyapply the open-source tool 2 to evaluate our resultsas in (krishna et al., 2017)..1https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/.
2https://github.com/.
ranjaykrishna/densevid_eval/tree/9d4045aced3d827834a5d2da3c9f0692e3f33c1c.
2008methodsbi-lstm + tempoattn (shou et al., 2016)emt(zhou et al., 2018a)videobert(sun et al., 2019b)videobert (+s3d feature)(sun et al., 2019b)cbt(sun et al., 2019a)dpcat+video(hessel et al., 2019)transformer (w/o context)hcntransformer (w/o context)hcn.
v/tvvvvvvtvtvvvtvt.b-3--6.807.59-7.60-12.7913.7415.0015.72.b-40.874.384.044.335.122.769.016.357.267.109.01.m8.1511.5511.0111.9412.9718.0817.7716.5617.1118.0719.51.r-l-27.4427.5028.8030.44-36.6537.1738.3538.3141.03.cider-38495564-112113121123141.table 1: the dense video event captioning results on the youcook2 dataset, and these results are based on thevalidation set.
the column ”v/t” means whether the results come from uni-modal or multi-modal features.
trans-former(w/o context) is the base method similar to (hessel et al., 2019)..methodswlt (rahman et al., 2019)mdvc (iashin and rahtu, 2020b)bmt (iashin and rahtu, 2020a)transformer (w/o context)hcntransformer (w/o context)hcn.
v/tvvtvtvvvtvt.b-33.044.524.634.445.544.435.82.b-41.461.811.991.832.481.862.62.m7.2310.0910.909.9310.9010.0510.64.table 2: the dense video event captioning results onthe activitynet captions dataset and these results arebased on the ground truth proposals of the validationset.
the column ”v/t” means whether the results comefrom uni-modal or multi-modal features..4.2.implementation details.
we develop our model based on the open-sourcecode 3 of mdvc(iashin and rahtu, 2020b), andwill release our code later.
the embedding sizeof video, hidden size of the multi-head, and feed-forward layer are 1024, 512, and 128 respectively.
the number of the head is 8 and the dropout rateis 0.4. we set the local context length kl as 10,that is, the 10 previous and 10 future frames asa local event-agnostic context, and one previousevent and one next event as a global event-awarecontext for a trade-off between performance andefﬁciency.
we adopt the adam optimizer (kingmaand ba, 2015) with learning rate of 1e-4, and settwo momentum parameters β1= 0.9 and β2= 0.98.for label smoothing, and the smoothing rate is 0.4.we set the batch size to 128. for model complexity,the hcn model introduces only 3% more parame-ters to the base model.
all models are trained on 1tesla p100 gpus for 4 hours for youcook2 and 30hours for activitynet captions..video features we sampled frames at 16 fpsand took the feature activations before the ﬁnal.
3https://github.com/v-iashin/mdvc.
linear classiﬁer of the s3d backbone and applied3d average pooling to obtain a 1024-dimensionfeature vector.
we got 1 feature per second and setk to 80..4.3 compare with state-of-the-art results.
we demonstrate the results of our context-awaremodel on the youcook2 dataset in table 3. thereare several existing baseline models: (1) bi-lstmwith temporal attention (bi-lstm + tempoattn)(shou et al., 2016), which adopts bi-lstm lan-guage encoder; (2) end-to-end masked trans-former (emt) (zhou et al., 2018b), an transformerbased model; (3,4) videobert (sun et al., 2019b)and contrastive bidirectional transformer (cbt)(sun et al., 2019a), the pre-training based meth-ods; (5) at+video (hessel et al., 2019), the multi-modal transformer method.
besides the work(shou et al., 2016) using a recurrent network, otherbaseline methods adopted the transformer model.
our context-aware model achieves the best resultsfor uni-modal video event captioning and outper-forms the context-agnostic base model by a largemargin.
furthermore, our hcn model with multi-modal inputs can achieve comparable results withstate-of-the-art results..we list experimental results on a partial datasetof activitynet captions as (iashin and rahtu,2020b) and ignore others on the full dataset as (kr-ishna et al., 2017) to make a fair comparison.
table2 presents the results of baseline methods and hcn.
there are several baseline methods: (1) wlt (rah-man et al., 2019), a weakly supervised method withmulti-modal input; (2) multi-modal video eventcaptioning (mdvc) (iashin and rahtu, 2020b), atransformer-based model with multi-modal inputs;(3) bmt (iashin and rahtu, 2020a), a better use of.
2009visual-audio information.
among these methods,wlt encoded the context using a recurrent net-work, while others are transformer models.
hcnoutperforms the base context-agnostic methods toa large extent and achieves state-of-the-art results.
from both experimental results, we can see thatour methods with context-aware information canimprove the base context-agnostic model by a largemargin for both unimodal or multi-modal input..4.4 ablation study.
methodshcn- type embedding- future event- past event- gru gate- global context- local contextbase (w/o context).
b-47.266.956.826.436.506.947.176.35.m17.1117.0216.6916.6516.7117.1017.0316.56.r-l38.3538.0237.2337.2537.8637.6837.9337.17.cider121.41122.12118.71116.97119.16121.06119.87113.34.table 3: ablation study on the youcook2 dataset.
’-’means to remove the setting from the full hcn model..we introduce the ablation study of the hcnmodel on the youcook2 dataset.
in our experiment,we use uni-modal input and illustrate the ablationresults in table 3. we remove one component ata time from the full hcn model to compare theperformance.
type embedding: we remove the typeembedding which is used to distinguish whether theinput is source or context event.
from the results,we can observe the performance drop by remov-ing the type embedding.
past/future context: weinvestigate the model with the only past contextor future context and found that both past and fu-ture contexts are effective and complementary witheach other.
the model with the context in bothdirections achieves the best result.
cross attentiongate: the gru gate in the cross attention model ismore effective than the simple combination, whichshows that the gru gate is better for modeling asequential context.
local/global context: from theresults in table 3, we can see that the global con-text is more effective than the local context.
thehcn model with both contexts outperforms all themodels.
context length.
1) with regards to thelocal context, the results of 10 or 20 context framesare similar with cider as 141.1 and 141.3 corre-spondingly, while the performance with 40 framesdrops with cider as 138.
2) for the global context,we have increased the number of previous and nextevents as the global context, but there is no further.
improvement.
we found that irrelevant events evenbring noise or duplicated information to learn..4.5 qualitative analysis.
we analyzed several cases and found two interest-ing videos shown in figure 4 and 5. we depict thevisual thumbnail, ground-truth caption, predictedresults of our baseline and hcn methods..figure 4:in this case, it is hard to distinguish theﬁne-grained object ”chicken” or ”pork” from both vi-sual and the transcript (co-reference ”it”).
the baselinemethod would like to predict ”chicken” with a priorbias for the ambiguous object leading to inconsistentcaptions between events.
modeling event dependencycan make coherent captions.
besides, as shown in event1, our hcn model can leverage local context to learnthe entity ”pork” from previous frames..figure 5: in this case, the baseline context-agnosticmodel would like to generate the same captions givensimilar visual inputs.
with event-aware context, ourhcn model can sequentially generate reasonable sen-tence sequences and reduce redundancy..from the case in figure 4, we can see that thebaseline context-agnostic model generates the cap-tion of each event solely leading to inconsistentcaptions.
the baseline model predicts the ambigu-ous object as ”chicken” for event 1 with prior bias,but output the object as ”pork” for event 2. ourhcn model can tackle this issue and is prone topredict captions with a consistent object in the pro-cedure.
besides, as shown in event 1, the entity”pork” can also be learned from previous frames.
the context-aware model is effective in resolvingentity ambiguity and generating coherent captions.
the case in figure 5 presents another challenge.
since the visual cue of the three events is verysimilar, the base context-agnostic model inevitably.
2010predicts the same caption as ”knead the dough”.
the hcn model can learn the prior dependencybetween events, and hinder generating redundantsentences for similar events in the video.
therefore,the hcn model can generate the correct sentencefor event 3. however, although the model tries topredict different captions for event 1, it is still hardto recognize the ﬁne-grained entity ”salt” from thevideo, and all models predict the object by mistake.
fine-grained entity recognition from a video is stilla challenging problem..to sum up, from these cases we can see that, 1)the neighboring context can provide extra informa-tion to make an accurate and coherent prediction.
2) the hcn model can capture the temporal de-pendency between events as prior knowledge, andgenerate consistent and less duplicate captions be-tween events.
3) ﬁne-grained object recognitionfrom a video is still a challenging problem.
visualcoreference resolution (kottur et al., 2018) can bethe future work to tackle this problem..5 related work.
video captioning the tasks mainly contain threetypes of captioning: single-sentence captioning(xu et al., 2016; wang et al., 2018b; zhang et al.,2018), paragraph-level captioning (yu et al., 2016;lei et al., 2020; ging et al., 2020) and event-levelcaptioning (krishna et al., 2017; li et al., 2018;wang et al., 2018a; mun et al., 2019; chen et al.,2019; zhou et al., 2018b).
the difference betweenthese tasks is whether to generate one or multi-ple sentences for the whole video or each sepa-rate event of the video.
in this paper, we focuson the more challenging dense event-level videocaptioning task to generate descriptions for eachevent.
previous works (krishna et al., 2017; liet al., 2018; wang et al., 2018a) mainly exploitedrecurrent neural models such as long short-termmemory network (lstm) (hochreiter and schmid-huber, 1997) or recurrent unit (gru) (cho et al.,2014) to encode context.
however, the recurrentmodel suffers from modeling long dependency ef-fectively.
zhou et al.
(zhang et al., 2018; sunet al., 2019b,a) introduced a self-attention model(vaswani et al., 2017) which generates the captionbased on the clip of each event solely.
comparedwith these works, we are the ﬁrst to implement anovel video-level hierarchical context-aware net-work for dense video event captioning..multi-modal video captioning video natu-.
rally has multi-modal inputs including visual,speech text, and audio.
previous works explorevisual rgb, motion, optical ﬂow features, audiofeatures (hori et al., 2017; wang et al., 2018b; rah-man et al., 2019) as well as speech text features (shiet al., 2019; hessel et al., 2019; iashin and rahtu,2020b) for captioning.
according to the work in(shi et al., 2019; hessel et al., 2019; iashin andrahtu, 2020b), although the speech text is noisyand informal, it can still capture better semanticfeatures and improve performance especially forinstructional videos.
later on, lashin et al.
(iashinand rahtu, 2020b) proposed to embed all visual,audio, and speech text for dense video event cap-tioning.
however, context-aware models are rarelyinvestigated in multi-modal video event captioning.
therefore, we propose a novel attention model foreffectively encoding the local and global context totackle ambiguous object recognition and transcriptco-reference through jointly modeling multi-modalinputs..context-aware language generation ourwork is inspired by context-aware language genera-tion e.g.
document-level neural machine translation(nmt) (miculicich et al., 2018; maruf et al., 2019;ma et al., 2020).
miculicich et al.
(miculicichet al., 2018) adopted a hierarchical context-awarenetwork in a structured and dynamic manner.
mar-cuf et al.
(maruf et al., 2019) and ma (ma et al.,2020) further explored a scalable and effective at-tention mechanism.
for the local neighbor-clipand global inter-event context, we further design ahierarchical context-aware network with a hybridmechanism of multi-modal video captioning to dy-namically leverage various video-level informationthrough a gating scalar..6 conclusion and discussion.
dense video event captioning is a typical videounderstanding task to learn procedural events ina long untrimmed video.
it is essential to modelholistic video information for event understand-ing.
in this paper, we propose a novel hierarchicalcontext-aware network to encode both the local andglobal context of long videos.
our hcn model iseffective in modeling context and outperforms thecontext-agnostic model by a large margin..in future work, we tend to extend our hierarchi-cal network to further investigate how to effectivelyattend to the long context to ﬁlter ambiguous andirrelevant information..2011references.
satanjeev banerjee and alon lavie.
2005. meteor: anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
in proceedingsof the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65–72..shizhe chen, yuqing song, yida zhao, qin jin,zhaoyang zeng, bei liu, jianlong fu, and alexan-der hauptmann.
2019. activitynet 2019 task 3:exploring contexts for dense captioning events invideos.
arxiv preprint arxiv:1907.05092..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder-decoderfor statistical machine translation.
arxiv preprintarxiv:1406.1078..simon ging, mohammadreza zolfaghari, hamed pir-siavash, and thomas brox.
2020. coot: coopera-tive hierarchical transformer for video-text represen-tation learning.
arxiv preprint arxiv:2011.00597..jack hessel, bo pang, zhenhai zhu, and radu sori-cut.
2019. a case study on combining asr and visualfeatures for generating instructional video captions.
in proceedings of the 23rd conference on computa-tional natural language learning (conll)..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..chiori hori, takaaki hori, teng-yok lee, zimingzhang, bret harsham, john r hershey, tim kmarks, and kazuhiko sumi.
2017. attention-basedmultimodal fusion for video description.
in proceed-ings of the ieee international conference on com-puter vision, pages 4193–4202..vladimir iashin and esa rahtu.
2020a.
a betteruse of audio-visual cues: dense video caption-arxiv preprinting with bi-modaltransformer.
arxiv:2005.08271..vladimir iashin and esa rahtu.
2020b.
multi-modalin proceedings of thedense video captioning.
ieee/cvf conference on computer vision and pat-tern recognition workshops, pages 958–959..diederik p kingma and jimmy ba.
2015. adam: ainternational.
method for stochastic optimization.
conference on learning representations..satwik kottur, jos´e mf moura, devi parikh, dhruv ba-tra, and marcus rohrbach.
2018. visual coreferenceresolution in visual dialog using neural module net-works.
in proceedings of the european conferenceon computer vision (eccv), pages 153–169..ranjay krishna, kenji hata, frederic ren, li fei-fei,and juan carlos niebles.
2017. dense-captioning.
events in videos.
in proceedings of the ieee inter-national conference on computer vision, pages 706–715..jie lei, liwei wang, yelong shen, dong yu, tamaraberg, and mohit bansal.
2020. mart: memory-augmented recurrent transformer for coherent videoin proceedings of the 58thparagraph captioning.
annual meeting of the association for computa-tional linguistics, pages 2603–2614..yehao li, ting yao, yingwei pan, hongyang chao,and tao mei.
2018. jointly localizing and describ-ing events for dense video captioning.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 7492–7500..chin-yew lin and franz josef och.
2004. auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
in proceedings of the 42nd annual meet-ing on association for computational linguistics,page 605. association for computational linguis-tics..shuming ma, dongdong zhang, and ming zhou.
2020.a simple and effective uniﬁed encoder for document-in proceedings of thelevel machine translation.
58th annual meeting of the association for compu-tational linguistics, pages 3505–3511..sameen maruf, andr´e ft martins, and gholamrezahaffari.
2019.selective attention for context-aware neural machine translation.
arxiv preprintarxiv:1903.08788..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neuralmachine translation with hierarchical attention net-works.
arxiv preprint arxiv:1809.01576..antoine miech, jean-baptiste alayrac, lucas smaira,ivan laptev, josef sivic, and andrew zisserman.
2020. end-to-end learning of visual represen-tations from uncurated instructional videos.
incvpr..antoine miech, dimitri zhukov, jean-baptiste alayrac,makarand tapaswi, ivan laptev, and josef sivic.
2019. howto100m: learning a text-video embed-ding by watching hundred million narrated videoclips.
iccv..jonghwan mun, linjie yang, zhou ren, ning xu, andbohyung han.
2019. streamlined dense video cap-in proceedings of the ieee conferencetioning.
on computer vision and pattern recognition, pages6588–6597..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting on association for compu-tational linguistics, pages 311–318.
association forcomputational linguistics..2012tanzila rahman, bicheng xu, and leonid sigal.
2019.watch, listen and tell: multi-modal weakly super-vised dense event captioning.
in proceedings of theieee international conference on computer vision,pages 8908–8917..haonan yu, jiang wang, zhiheng huang, yi yang, andwei xu.
2016. video paragraph captioning usinghierarchical recurrent neural networks.
in proceed-ings of the ieee conference on computer vision andpattern recognition, pages 4584–4593..bowen zhang, hexiang hu, and fei sha.
2018. cross-modal and hierarchical modeling of video and text.
in proceedings of the european conference on com-puter vision (eccv), pages 374–390..luowei zhou, chenliang xu, and jason j corso.
2018a.
towards automatic learning of procedures from webinstructional videos.
in thirty-second aaai confer-ence on artiﬁcial intelligence..luowei zhou, yingbo zhou, jason j corso, richardsocher, and caiming xiong.
2018b.
end-to-enddense video captioning with masked transformer.
inproceedings of the ieee conference on computervision and pattern recognition, pages 8739–8748..botian shi, lei ji, yaobo liang, nan duan, peng chen,zhendong niu, and ming zhou.
2019. dense pro-cedure captioning in narrated instructional videos.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages6382–6391, florence, italy.
association for compu-tational linguistics..zheng shou, dongang wang, and shih-fu chang.
2016. temporal action localization in untrimmedvideos via multi-stage cnns.
in proceedings of theieee conference on computer vision and patternrecognition, pages 1049–1058..chen sun, fabien baradel, kevin murphy, andcordelia schmid.
2019a.
contrastive bidirectionaltransformer for temporal representation learning.
arxiv preprint arxiv:1906.05743..chen sun, austin myers, carl vondrick, kevin mur-phy, and cordelia schmid.
2019b.
videobert: ajoint model for video and language representationlearning.
proceedings of the ieee international con-ference on computer vision..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018. learning to remember translation history witha continuous cache.
transactions of the associationfor computational linguistics, 6:407–420..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recogni-tion, pages 4566–4575..jingwen wang, wenhao jiang, lin ma, wei liu, andyong xu.
2018a.
bidirectional attentive fusion within pro-context gating for dense video captioning.
ceedings of the ieee conference on computer vi-sion and pattern recognition, pages 7190–7198..xin wang, yuan-fang wang, and william yang wang.
2018b.
watch, listen, and describe: globally andlocally aligned cross-modal attentions for video cap-tioning.
arxiv preprint arxiv:1804.05448..jun xu, tao mei, ting yao, and yong rui.
2016. msr-vtt: a large video description dataset for bridgingvideo and language.
in proceedings of the ieee con-ference on computer vision and pattern recognition,pages 5288–5296..2013