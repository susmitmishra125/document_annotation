cross-lingual abstractive summarizationwith limited parallel resources.
yu bai, yang gao, heyan huang∗school of computer science and technology,beijing institute of technology, beijing, chinasoutheast academy of information technology, fujian, chinabeijing engineering research center of high volume language informationprocessing and cloud computing applications, beijing, china{yubai,gyang,hhy63}@bit.edu.cn.
abstract.
parallel cross-lingual summarization data isscarce, requiring models to better use the lim-ited available cross-lingual resources.
exist-ing methods to do so often adopt sequence-to-sequence networks with multi-task frame-works.
such approaches apply multiple de-coders, each of which is utilized for a speciﬁctask.
however, these independent decodersshare no parameters, hence fail to capture therelationships between the discrete phrases ofsummaries in different languages, breaking theconnections in order to transfer the knowl-edge of the high-resource languages to low-resource languages.
to bridge these connec-tions, we propose a novel multi-task frame-work for cross-lingual abstractive summa-rization (mclas) in a low-resource setting.
employing one uniﬁed decoder to generate thesequential concatenation of monolingual andcross-lingual summaries, mclas makes themonolingual summarization task a prerequi-site of the cross-lingual summarization (cls)task.
in this way, the shared decoder learns in-teractions involving alignments and summarypatterns across languages, which encouragesattaining knowledge transfer.
experiments ontwo cls datasets demonstrate that our modelsigniﬁcantly outperforms three baseline mod-els in both low-resource and full-dataset sce-narios.
moreover, in-depth analysis on thegenerated summaries and attention heads ver-iﬁes that interactions are learned well usingmclas, which beneﬁts the cls task underlimited parallel resources..1.introduction.
cross-lingual summarization (cls) helps peopleefﬁciently grasp salient information from articlesin a foreign language.
neural approaches to clsrequire large scale datasets containing millions ofcross-lingual document-summary pairs (zhu et al.,.
∗corresponding author..figure 1: an example of the alignments across sum-maries in different languages.
each color representsphrases with one speciﬁc meaning..2019; cao et al., 2020; zhu et al., 2020).
how-ever, two challenges arise with these approaches:1) most languages are low-resource, thereby lack-ing document-summary paired data; 2) large par-allel datasets across different languages for neural-based cls are rare and expensive, especially underthe current trend of neural networks.
therefore,a low-resource setting is more realistic, and chal-lenging, one for cross-lingual summarization.
toour best knowledge, cross-lingual summarizationunder low-resource settings has not been well in-vestigated and explored.
therefore, in this paper,we will develop a new model for cross-lingual ab-stractive summarization under limited supervision..for low-resource settings, multi-task learninghas been shown to be an effective method since itcan borrow useful knowledge from other relevanttasks to use in the target task (yan et al., 2015;wang et al., 2020; motiian et al., 2017).
cross-lingual summarization can be viewed as the combi-nation of two tasks, i.e., monolingual summariza-tion (ms) and cross-lingual translation (zhu et al.,2019).
a wealth of relationships exist across thetarget summaries of ms and cls tasks, such astranslation alignments and summarization patterns.
illustrated in figure 1, “叙利亚” is mapped to“syria”, and similar maping is done with the other.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6910–6924august1–6,2021.©2021associationforcomputationallinguistics6910aligned phrases.
obviously, leveraging these re-lationships is crucial for the task of transferringsummarization knowledge from high-resource lan-guages to low-resource languages.
unfortunately,existing multi-task frameworks simply utilize inde-pendent decoders to conduct ms and cls task sep-arately (zhu et al., 2019; cao et al., 2020), whichleads to failure in capturing these relationships..to solve this problem, we establish reliant con-nections between ms and cls tasks, makingthe monolingual task a prerequisite for the cross-lingual task.
speciﬁcally, one decoder is sharedby both ms and cls tasks; this is done by settingthe generation target as a sequential concatenationof a monolingual summary and the correspondingcross-lingual summary.
sequentially generatingmonolingual and cross-lingual summaries, the de-coder also conducts the translation task betweenthem, which enhances the interactions between dif-ferent languages.
these interactions implicitlyinvolve translation alignments, similarity in seman-tic units, and summary patterns across differentlingual summaries.
to demonstrate these decoderinteractions, we further visualize them by probingtransformer attention heads in the model.
basedon this process, the new structure with these ad-vanced interactions enhances low-resource scenar-ios which require the model to be capable of trans-ferring summary knowledge from high-resourcelanguages to low-resource language.
we name ourmodel multi-task cross-lingual abstractive sum-marization (mclas) under limited resources..in terms of a training strategy under limited re-sources, we ﬁrst pre-train mclas on large-scalemonolingual document-summary parallel datasetsto well-equip the decoder with general summarycapability.
given a small amount of parallel cross-lingual summary samples, the model is then ﬁne-tuned and is able to transfer the learned summarycapability to the low-resource language, leveragingthe interactions uncovered by the shared decoder..experiments on zh2ensum (zhu et al., 2019)and a newly developed en2desum dataset demon-strate that mclas offers signiﬁcant improvementswhen compared with state-of-the-art cross-lingualsummarization models in both low-resource sce-narios and full-dataset scenario.
at the same time,we also achieved competitive performances in theen2zhsum dataset (zhu et al., 2019).
humanevaluation results show that mclas producesmore ﬂuent, concise and informative summaries.
than baselines models under limited parallel re-sources.
in addition, we analyzed the length ofgenerated summaries and the success of monolin-gual generation to verify advantages offered byidentifying interactions between languages.
wefurther investigate the explainability of the pro-posed multi-task structure by probing the atten-tion heads in the uniﬁed decoder, proving thatmclas learns the alignments and interactions be-tween two languages, and this facilitates translationand summarization in the decoder stage.
our anal-ysis provides a clear explanation of why mclasis capable of supporting cls under limited re-sources.
our implementation and data are availableat https://github.com/woodenwhite/mclas..2 related work.
2.1 cross-lingual summarization.
recently, cross-lingual summarization has receivedattention in research due to the increasing demandto produce cross-lingual information..traditional cls systems are based on a pipelineparadigm (wan et al., 2010; wan, 2011; zhanget al., 2016).
these pipeline systems ﬁrst translatethe document and then summarize it or vice versa.
shen et al.
(2018) propose the use of pseudo sum-maries to train the cross-lingual abstractive summa-rization model.
in contrast, duan et al.
(2019a) andouyang et al.
(2019) generate pseudo sources toconstruct the cross-lingual summarization dataset.
the ﬁrst large-scale cross-lingual summarizationdatasets are acquired by use of a round-trip trans-lation strategy (zhu et al., 2019).
additionly, zhuet al.
(2019) propose a multi-task framework toimprove their cross-lingual summarization system.
following zhu et al.
(2019), more methods havebeen proposed to improve the cls task.
zhu et al.
(2020) use a pointer-generator network to exploitthe translation patterns in cross-lingual summariza-tion.
cao et al.
(2020) utilize two encoders and twodecoders to jointly learn to align and summarize.
in contrast to previous methods, mclas gener-ates the concatenation of monolingual and cross-lingual summaries, thereby modeling relationshipsbetween them..2.2 low-resource natural language.
generation.
language generation (nlg) for low-naturalresource languages or domains has attracted lots ofattention.
gu et al.
(2018) leverage meta-learning.
6911to improve low-resource neural machine transla-tion.
meanwhile, many pretrained nlg modelshave been proposed and adapted to low-resourcescenarios (song et al., 2019; chi et al., 2020; rad-ford et al., 2019; zhang et al., 2019a).
however,these models require large-scale pretraining.
ourwork does not require any large pretrained genera-tion models or translation models, enabling a vitaldecreases in training cost..3 background.
3.1 neural cross-lingual summarization.
1 , ya.
1 , xa.
document da.
a2 , .
.
.
, xa.
sourcem} in language a,.
=given{xaa mono-lingual summarization system converts the sourceinto a summary sa = {yan }, where mand n are the lengths of da and sa, respectively.
a cross-lingual summarization system produces asummary sb = {ybn(cid:48)} consisting oftokens yb in target language b, where n(cid:48) is thelength of sb.
note that the mentioned xa, ya, andyb are all tokens..2 , .
.
.
, yb.
2 , .
.
.
, ya.
1 , yb.
zhu et al.
(2019) propose using the trans-former (vaswani et al., 2017) to conduct cross-lingual summarization tasks.
the transformer iscomposed of stacked encoder and decoder layers.
the encoder layer is comprised of a self-attentionlayer and a feed-forward layer.
the decoder layershares the same architecture as the encoder exceptfor an extra encoder-decoder attention layer, whichperforms multi-head attention over the output ofstacked encoder layers.
the whole transformermodel θ is trained to maximize the conditionalprobability of the target sequence sb as follows:.
lncls =.
logp (yb.
t |yb.
<t, da).
(1).
n(cid:88).
t=1.
3.2.improving ncls with multi-taskframeworks.
considering the relationship between cls and ms,in which they share the same goal to summarizeimportant information in a document, zhu et al.
(2019) proposed employing a one-to-many multi-task framework to enhance the basic transformermodel.
in this framework, one encoder is employedto encode the source document da.
two sepa-rate decoders simultaneously generate a monolin-gual summary sa and a cross-lingual summarysb, leading to a loss as follows:lncls+ms = (cid:80)n.t=1 logp (ya.
t |yat=1 logp (yb.
<t, da)t |yb.
<t, da).
+ (cid:80)n(cid:48).
(2).
figure 2: an overview of our proposed mclas.
a uni-ﬁed decoder produces both monolingual (green) andcross-lingual (red) summaries.
the green and redlines represent the monolingual and cross-lingual sum-maries’ attention, respectively..this multi-task framework shares encoder repre-sentation to enhance cross-lingual summarization.
however, independent decoders in this model areincapable of establishing alignments and connec-tions between cross-lingual summaries..4 mclas with limited parallel.
resources.
to strengthen the connections mentioned, we pro-pose making the monolingual task a prerequisitefor the cross-lingual task through modeling inter-actions.
according to previous work (wan et al.,2010; yao et al., 2015; zhang et al., 2016), interac-tions between cross-lingual summaries (importantphrase alignments, sentence lengths, and summarypatterns, etc) are crucial for the ﬁnal summary’squality.
we leverage these interactions to furthertransfer the rich-resource language knowledge.
de-tailed descriptions of this step are presented in fol-lowing sections..4.1 multi-task learning in mclas.
to model interactions between languages, we needto share the decoder’s parameters.
inspired bydong et al.
(2019), we propose sharing the wholedecoder to carry out both the translation and thesummarization tasks.
speciﬁcally, we substitutethe generation target sa with the sequential con-catenation of sa and sb:.
sab = {[bos], ya[lsep], yb.
1 , ya1 , yb.
2 , .
.
.
, yan ,2 , .
.
.
, ybn(cid:48) , [eos]}.
(3).
where [bos] and [eos] are the beginning and endtoken of the output summaries, respectively.
and.
6912[lsep] is the special token used as the separator ofsa and sb..with the new generation target, the decoderlearns to ﬁrst generate sa, and then generate sbconditioned on sa and da.
the whole generationprocess is illustrated in figure 2..formally, we maximize the joint probability for.
monolingual and cross-lingual summarization:.
lmclas = (cid:80)n.t=1 logp (ya.
t |yat=1 logp (yb.
<t, da)t |yb.
+ (cid:80)n(cid:48).
<t, sa, da).
(4).
the loss function can be divided into two terms.
when generating sa, the decoder conducts the mstask based on da, corresponding to the ﬁrst termin equation (4).
when generating sb, the decoderalready knows the information of correspondingmonolingual summaries.
in this way, it performsthe translation task (for sa) and the cls task (forda), achieved by optimizing the second term inequation (4).
with the modiﬁcation of the target,our model can easily capture interactions betweencross-lingual summaries.
the trained model showseffectiveness in aligning the summaries.
not onlythe output tokens, but also the attention distribu-tions are aligned.
the model we designed leveragesthis phenomenon to enable monolingual knowledgeto be transferred under low-resource scenarios.
de-tailed investigation is presented in section 6..we adopt transformers as our base model.
inaddition, we use multilingual bert (devlin et al.,2019) to initialize the encoder, improving its abilityto produce multilingual representations.
addition-ally, having tried many different position embed-ding and language segmentation embedding meth-ods, we ﬁnd that [lsep] is enough for the modelto distinguish whether it is generating sb.
hencekeeping the original position embedding (vaswaniet al., 2017) and employing no segmentation em-bedding are best for performance and efﬁciency..4.2 learning schemes for mclas under.
limited resources.
since our proposed framework enforces interac-tions between cross multilingual summaries, it hasfurther beneﬁts to the low-resource scenario, asonly a few training summary samples are avail-able in a cross-language.
yet, simply training fromscratch can not make the best of our proposedmodel in low-resource scenarios.
hence we use apre-training and ﬁne-tuning paradigm to transferthe rich-resource language knowledge..scenarios.
zh2ensum.
en2desum.
en2zhsum.
minimummediummaximumfull-dataset.
5,000 (0.3%)25,000 (1.5%)50,000 (3.0%)1,693,713.
2,619 (0.6%)12,925 (3.0%)25,832 (6.0%)429,393.
1,500 (0.4%)7,500 (2.0%)15,000 (4.0%)364,687.table 1: sample sizes of different low-resource scenar-ios.
three low-resource scenarios with various samplesizes are created for each dataset.
minimum, medium,and maximum represent sample sizes in the minimumlow-resource scenario, medium low-resource scenario,and maximum low-resource scenario, respectively..first, we train the model in a monolingual sum-marization dataset.
in this step, the model learnshow to produce a monolingual summary for a givendocument.
then, we jointly learn ms and clswith few training samples, optimizing equation (4).
we adopt similar initialization to existing clsmethods, which is introduced in section 5.3..5 experiments.
5.1 datasets.
we conduct experiments on the en2zhsum,zh2ensum cls datasets1 (zhu et al., 2019)and a newly constructed en2desum dataset.
en2zhsum is an english-to-chinese dataset con-taining 364,687 training samples, 3,000 validation,and 3,000 testing samples.
the dataset is con-verted from the union set of cnn/dm (hermannet al., 2015) and msmo (zhu et al., 2018) usinga round-trip translation strategy.
converted fromthe lcsts dataset, zh2ensum contains 1,693,713chinese-to-english training samples, 3,000 vali-dation, and 3,000 testing samples.
to better ver-ify the cls ability of mclas, we construct anew english-to-german dataset (en2desum), us-ing the same methods proposed by zhu et al.
(2019).
we use wmt’19 english-german winner2 as ourtranslation model to process the english gigaworddataset.3 we set the threshold t1 = 0.6 andt2 = 0.2. the ﬁnal en2desum contains 429,393training samples, 4,305 validation samples, and4,099 testing samples..all the training samples contain a source docu-ment, a monolingual summary, and a cross-lingualsummary.
for the full-dataset scenario, we trainthe model with the whole dataset.
for low-resourcescenarios, we randomly select 3 different amounts.
1www.nlpr.ia.ac.cn/cip/dataset.htm2https://github.com/pytorch/fairseq/.
tree/master/examples/translation.
3ldc2011t07.
6913(minimum, medium, and maximum) of trainingsamples for all datasets to evaluate our model’s per-formance under low-resource scenarios.
detailednumbers are presented in table 1..5.2 training and inference.
we use multilingual bert (mbert) (devlin et al.,2019) to initialize our transformer encoder.
the de-coder is a transformer decoder with 6 layers.
eachattention module has 8 different attention heads.
the hidden size of the decoder’s self-attention is768 and that of the feed-forward network is 2048.the ﬁnal model contains 296,046,231 parameters.
because the encoder is pretrained when the decoderis randomly initialized, we use two separate opti-mizers for the encoder and the decoder (liu andlapata, 2019).
the encoder’s learning rate ηe isset as 0.005, while the decoder’s learning rate ηd is0.2. warmup-steps for the encoder are 10,000 and5,000 for the decoder.
we train the model on twotitan rtx gpus for one day with gradient accu-mulation every 5 steps.
dropout with a probability0.1 is applied before all the linear layers.
we ﬁndthat the target vocabulary type doesn’t have muchinﬂuence on the ﬁnal result.
therefore, we directlyuse mbert’s subwords vocabulary as our targetvocabulary.
nevertheless, in case tokens would beproduced in the wrong language, we constructe atarget token vocabulary for each target language.
in the inference period, we only generate tokensfrom the corresponding vocabulary.
during thedecoding stage, we use beam search (size 5) andtrigram block to avoid repetition.
length penaltyis set between 0.6 and 1. all the hyperparametersare manually tuned using ppl and accuracy metricon the validation set..5.3 baselines.
we compare mclas in low-resource scenarioswith the following baselines:.
ncls cls model proposed by zhu et al.
(2019).
in low-resource scenarios, we initialize our modelwith the pretrained ms model and then use a fewsamples to optimize equation (1)..ncls+ms multi-task framework proposed byzhu et al.
(2019).
we ﬁnd that ncls+ms failsto converge when it is partly initialized by the pre-trained ms model (the cls decoder is randomlyinitialized).
hence, we fully initialize the multi-task model using the pretrained ms model.
speciﬁ-cally, the two separate decoders are both initialized.
by the pretrained monolingual decoder.
then themodel is optimized with equation (2)..tltran transformer-based late translation isa pipeline method.
first, a monolingual summa-rization model summarizes the source document.
a translation model is then applied to translate thesummary.
the summarization model is trained withmonolingual document-summary pairs in threedatasets.
speciﬁcally, we continue using wmt’19english-german winner as the translation modelfor en2desum..some recent proposed models improve the per-formance of cls task.
methods ncls+mt,tetran (zhu et al., 2019), and the system pro-posed by ouyang et al.
(2019) require external longdocument machine translation (mt) corpora.
themethod proposed by cao et al.
(2020) requires notonly parallel summaries but also document pairstranslated by mt systems.
another method pro-posed by zhu et al.
(2020) requires bilingual lex-icons extracted from large parallel mt datasets(2.08m sentence pairs from eight ldc corpora).
we choose not to use these models as baselinessince comparing mclas with them is unfair..5.4 automatic evaluation results.
the overall results under low-resource scenariosand full-dataset scenario are shown in table 2. wereimplement a variety of models and evaluate themusing f1 scores of the standard rouge metric(lin, 2004) (rouge-1, rouge-2, and rouge-l) and bertscore4 (zhang et al., 2019b).
thefollowing analysis is from our observations..in the zh2ensum and en2desum datasets,mclas achieves signiﬁcant improvements overbaselines in all the low-resource scenarios.
it isworth noting that combining ncls+ms in our ex-periments does not bring much improvement tothe ncls model.
we consider that this is becausembert has already provided multilingual encod-ing for our models..however, we ﬁnd that in the en2zhsum dataset,mclas did not perform as well as that in theother two datasets.
we speculate that is due tothe imbalance of english reference and chinesereference.
the average length of sa and sb inen2zhsum is 55.21 and 95.96, respectively (zhuet al., 2019).
this condition largely breaks thealignment between languages, leading to mclas.
4https://github.com/tiiiger/bert_score.
6914zh2ensum.
en2desum.
en2zhsum.
r-l bertscore.
r-1.
r-l bertscore.
r-1.
r-2.
r-l.bertscore.
models.
minimumlow-resourcescenario.
mediumlow-resourcescenario.
maximumlow-resourcescenario.
fulldatasetscenario.
nclsncls+msmclas.
nclsncls+msmclas.
nclsncls+msmclas.
tltrannclsncls+msmclas.
r-1.
20.9320.5021.03.
26.4226.8627.84.
29.0528.6330.73.
33.6435.6034.8435.65.r-2.
5.885.456.03.
8.909.0610.41.
10.8810.6312.26.
15.5816.7816.0516.97.
17.5817.2518.16.
22.0522.4724.12.
24.3224.0026.51.
29.7430.2729.4731.14.r-2.
5.015.275.91.
8.098.3510.09.
9.789.5812.32.
13.3114.2413.8617.21.
17.5917.5219.19.
23.5523.6027.22.
25.8425.5930.31.
28.5731.6131.3336.48.
16.5816.5718.43.
22.1322.1426.00.
24.2523.9628.88.
26.3429.6329.3134.86.
0.50410.50250.5023.
0.53730.53770.5464.
0.54920.54850.5633.
-0.58350.58070.5770.
0.72020.71980.7282.
0.74000.74310.7575.
0.74830.74840.7682.
-0.76800.76750.7897.
34.1433.9632.03.
35.9838.9537.28.
40.1839.8638.35.
30.2044.1642.6842.27.
12.4512.3813.17.
15.8818.0918.10.
19.8619.8719.75.
12.2024.2823.5124.60.
21.2021.0721.17.
23.7925.3925.26.
26.5226.6426.41.
27.0230.2329.2430.09.
0.70960.71020.6529.
0.72980.71720.6839.
0.74350.74450.6921.
-0.74070.73610.7069.table 2: f1 scores of rouge and bertscore in zh2ensum, en2desum and en2zhsum dataset.
r-1, r-2, andr-l represents rouge-1, rouge-2, and rouge-l, respectively..models.
minimum.
medium.
maximum.
if.
cc.
fl.
if.
cc.
fl.
if.
cc.
fl.
0.164 -0.021 0.000 0.236.
0.214-0.264mclas-0.243 -0.386 -0.364 0.036 -0.221 -0.257 -0.129 -0.329 -0.186nclsncls+ms -0.371 -0.407 -0.286 -0.343 -0.536 -0.407 -0.179 -0.364 -0.2140.179gold.
0.300 0.529.
0.257 0.221.
0.879 0.629.
0.164 0.057.
0.500.
0.464.
0.671.table 3: human evaluation results in zh2ensumdataset.
the best results are in bold..scenarios.
fleiss’ kappa overall agreement.
minimummediummaxmium.
0.370.220.20.
60.48%51.35%50.16%.
table 4: fleiss’ kappa and overall agreement percentof our human evaluation results.
a higher value indi-cates higher agreements among participants..the performing slightly weaker.
despite this, re-sults in en2desum and zh2ensum demonstratethat our proposed mclas model is effective forcls under limited resources..finally, our proposed model also has superiorperformance compared to baseline models giventhe full training dataset, achieving the best rougescore in en2desum and zh2ensum datasets..5.5 human evaluation.
in addition to automatic evaluation, we conducta human evaluation to verify our model’s perfor-mance.
we randomly chose 60 examples (20 foreach low-resource scenario) from the zh2ensumtest dataset.
seven graduate students with high lev-els of ﬂuency in english and chinese are askedto assess the generated summaries and gold sum-maries from independent perspectives: informa-tiveness, ﬂuency, and conciseness.
we follow thebest-worst scaling method (kiritchenko and mo-hammad, 2017).
participants are asked to indicate.
scenarios.
models.
en2desum.
zh2ensum.
minimumlow-resourcescenario.
mediumlow-resourcescenario.
maximumlow-resourcescenario.
nclsncls+msmclas.
nclsncls+msmclas.
nclsncls+msmclas.
13.48 (+4.69)12.83 (+4.04)7.80 (−0.90).
13.13 (+4.34)12.90 (+4.11)8.65 (−0.14).
13.37 (+4.58)13.37 (+4.58)8.46 (−0.33).
18.49 (+3.51)18.68 (+3.70)13.16 (−1.82).
18.60 (+3.62)18.57 (+3.59)13.10 (−1.88).
18.44 (+3.46)18.75 (+3.77)12.83 (−2.15).
gold.
8.79.
14.98.table 5: target summary length generated by variousmodels.
the best results are in bold..models.
en2desum.
zh2ensum.
r-1.
r-2.
r-l.r-1.
r-2.
r-l.ms-pretrainncls+msmclas.
39.1636.0645.59.
19.2116.8423.77.
35.4232.7242.51.
41.7139.9841.72.
27.7626.0327.69.
37.9736.1337.92.table 6: monolingual summary results in zh2ensumand en2zhsum datasets.
ms-pretrain refers to the pre-trained model for monolingual summarization..the best and worst items from each perspective.
the result scores are calculated based on the per-centage of times each system is selected as bestminus the times it is selected as worst.
hence, ﬁnalscores range from -1 (worst) to 1 (best).
resultsare shown in table 3..as the data size increases, all the models achievebetter results.
our proposed mclas outperformedncls and ncls+ms in all the metrics.
we noticethat mclas is especially strong in conciseness.
this phenomenon will be analyzed in section 5.7.we show fleiss’ kappa scores of our conductedhuman evaluation in table 4, which demonstratesa good inter-agreement among the participants..6915metrics.
ms-pretrain.
ncls+ms.
mclas.
ground truth.
r-1 recallr-1 precision.
r-2 recallr-2 precision.
r-l recallr-l precision.
58.3730.45.
30.2114.65.
52.9727.51.
52.6228.41.
25.9112.99.
47.8925.71.
46.8846.31.
24.5424.11.
43.7443.15.
--.
--.
--.
avg.
length.
18.64 (+9.53).
17.98 (+8.87).
9.15 (+0.04).
9.11.table 7: analysis on monolingual summary generationability of mclas trained with en2desum dataset..figure 4: an example of generated cross-lingual sum-maries.
important phrases are bold while incorrect in-formation generated by each model is italicized.
non-ﬂuent parts in sentences are bold and italicized..conditioning cross-lingual summaries on relativelyprecise monolingual summaries..5.8 analysis on monolingual summarization.
modeling interactions between languages bringsmany advantages.
speciﬁcally, we ﬁnd thatmclas can preserve more monolingual summa-rization knowledge than the ncls+ms modelduring low-resource ﬁne-tuning, or even promoteits performance.
we generate monolingual sum-maries with models trained in the maximum low-resource scenario.
in table 6, we can clearly seethat mclas retains more monolingual summariza-tion knowledge in the zh2ensum dataset.
in theen2desum dataset, monolingual summarizationperformance is even signiﬁcantly improved.
wespeculate that this is due to mclas’s ability toprovide the interactions between languages..we focus speciﬁcally on digging into results inen2desum, evaluating its detailed rouge andaverage summary length, presented in table 7. weﬁnd that rouge improvement mainly resultedfrom precision while recall barely decrease the per-formances.
this and the avg.
length metric showsthat mclas produces more precise summarieswhile retaining most of the important information,leading to the metric increase in rouge..figure 3: line and column chart of en2desum andzh2ensum results.
lines represent models initializedwith pretrained monolingual summarization model.
columns represent models trained from scratch..5.6 analysis on initialization methods.
we use a monolingual summarization model to ini-tialize our model.
however, whether this initializa-tion method works is still in question.
therefore wecompare our models with non-initialized models,shown in figure 3. among the three datasets, theinitialization methods bring a huge improvementto all of the models..5.7 analysis on summary length.
one of the goals of automatic summarization is toproduce brief text.
yet many neural auto-regressivemodels tend to produce a longer summary to im-prove the recall metric.
results in table 5 showthat interactions enable mclas to generate shortersummaries than other models, which more closelyresembles human summaries.
we can safely con-clude that mclas can keep the summary in afairly appropriate length, leading to concise gener-ated summaries.
we speculate that this is due to itsability to capture interactions between languages,.
6916figure 5: different types of self-attention heads inmclas’s decoder.
the x-axis and y-axis are bothconcatenated source-language summary sa and target-language summary sb tokens.
darker color shows themore highly related associations between tokens.
thehorizontal line in self head represents the [lsep] to-ken.
some attention heads attend to [lsep] to conﬁrmwhether it is generating a cross-lingual summary..figure 6: different types of encoder-decoder attentionheads in mclas’s decoder.
the x-axis representsconcatenated source-language summary sa and target-language summary sb tokens while the y-axis is thedocument da tokens.
in news texts, important infor-mation often gathers in the front part of the document.
we only retain the informative part of the y-axis, omit-ting the blank part that the model do not attend to..5.9 case study.
in figure 4, on the zh2ensum dataset, there is alist comparing the reference summary and outputsof models trained in the maximum low-resourcescenario.
clearly, the ncls model loses the infor-mation “two cars” and generates the wrong infor-mation “no.2 factory”.
the ncls+ms model isnot accurate when describing the number of injuredpeople, dropping important information “morethan”.
additionally, the ncls+ms model alsohas ﬂuency and repetition issues: “in zhengzhou”appears twice in its generated summary.
in contrast,mclas captures all of this information mentionedin both its chinese and english output, and theenglish summary is well aligned with the chinesesummary.
finally, all of the models ignore the in-formation “foxconn printed on the body of the car”.
see appendix a for more examples..6 probing into attention heads.
we have observed a successful alignment betweensa and sb produced by our model in section 5.9.in this section, we dig into this and analyze howthe model learns the relationships.
for a cls taskfrom document da to sb, our hypotheses are:(1) the uniﬁed decoder is implicitly undertakingtranslation from sa to sb; (2) the uniﬁed decoderalso conducts both monolingual and cross-lingualsummarization.
to verify these hypotheses, we vi-sualize attention distributions of the transformerdecoders trained on en2zhsum.
neural models.
can be explicitly explained using probing into theattention heads (michel et al., 2019; voita et al.,2019).
we follow the previous work and visual-ize the function of all attention heads in the de-coder to verify the relationships of the concate-nated cross-lingual summaries (i.e., translation)and cross-lingual document-summary pairs (i.e.,summarization)..6.1 analysis on translation.
we assume that the decoder translates only if thesource summary sa and the target summary sbalign well.
this means that mclas is transferringknowledge from sa to sb.
we visualize and probeall 48 self-attention heads in the uniﬁed decoder.
we ﬁnd 23 (47.9%) translation heads, deﬁned asthe heads attending from ybto the correspondingjwords in language a. these heads undertake atranslation function.
19 (39.6%) heads are localheads, attending to a few words before them andmodeling context information.
12 (25%) headsare self heads, which only attend to themselves toretain the primary information.
some of the headscan be categorized into two types.
note that all ofthe heads behave similarly across different samples.
we ﬁnd that most of the heads are translation heads,indicating that our uniﬁed decoder is translating sainto sb.
we sample some representative heads infigure 5 to show their functionalities..69176.2 analysis on summarization.
to analyze whether the decoder for sb is simplytranslating from sa or that it also summarizes thesource document, we visualize the distribution of48 encoder-decoder attention heads.
we ﬁnd 28(58.3%) summarization heads that attend to thedocument’s important parts when generating boththe monolingual summary and the cross-lingualsummary.
we also ﬁnd 20 (41.7%) translationheads, which focus on the source document whengenerating sa, while focusing on nothing whengenerating sb.
we speculate that summarizationheads are responsible for the summarization func-tion and that translation heads cut down the rela-tion between sb and source document da, leavingspace for translation.
again, all the heads behavesimilarly across different samples.
we select tworepresentative samples in figure 6..the existence of both summarization and trans-lation heads in encoder-decoder attention compo-nents supports our views: the uniﬁed decoder si-multaneously conducts translation and summariza-tion.
therefore, our model enhances the interac-tions between different languages, being able tofacilitate cross-lingual summarization under low-resource scenarios.
see appendix b for detailedvisualization results..7 discussions.
an ideal low-resource experiment should be con-ducted with real low-resource languages.
althoughpossible, it takes much effort to acquire suchdatasets.
hence, it is the second-best choice thatwe simulate our low-resource scenarios by artiﬁ-cially limiting the amount of the available data.
some may question it about the feasibility of ourmethod in real low-resource languages since ma-chine translation systems, which is used to gener-ate document-summary pairs, would be of lowerquality for truly low-resource languages.
for thisconcern, we consider it still possible to acquirethousands of high-quality human translated paral-lel summaries, as duan et al.
(2019b) adopt on theirtest set, to apply our method..8 conclusion.
in this paper, we propose a novel multi-task learn-ing framework mclas to achieve cross-lingualabstractive summarization with limited parallel re-sources.
our model shares a uniﬁed decoder that.
sequentially generates both monolingual and cross-lingual summaries.
experiments on two cross-lingual summarization datasets demonstrate thatour framework outperforms all the baseline modelsin low-resource and full-dataset scenarios..acknowledgements.
this work is supported by the joint funds of the na-tional natural science foundation of china (grantno.
u19b2020), the funds of the integrated ap-plication software project.
we appreciate the help-ful discussions with sanxing chen, jia-ao zhan,xuyang lu, xiao liu, and yuxiang zhou.
we alsothank all the anonymous reviewers for their insight-ful suggestions..references.
yue cao, hui liu, and xiaojun wan.
2020..jointlylearning to align and summarize for neural cross-lingual summarization.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 6220–6231..zewen chi, li dong, furu wei, wenhui wang, xian-ling mao, and heyan huang.
2020. cross-lingualnatural language generation via pre-training.
inaaai, pages 7570–7577..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, volume 32. curran asso-ciates, inc..xiangyu duan, mingming yin, min zhang, boxingchen, and weihua luo.
2019a.
zero-shot cross-lingual abstractive sentence summarization throughteaching generation and attention.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 3162–3172..xiangyu duan, mingming yin, min zhang, boxingchen, and weihua luo.
2019b.
zero-shot cross-lingual abstractive sentence summarization throughteaching generation and attention.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 3162–3172, florence,italy.
association for computational linguistics..6918jiatao gu, yong wang, yun chen, victor ok li,and kyunghyun cho.
2018. meta-learning for low-resource neural machine translation.
in proceedingsof the 2018 conference on empirical methods innatural language processing, pages 3622–3631..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..karl moritz hermann, tomas kocisky, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readand comprehend.
in advances in neural informationprocessing systems, pages 1693–1701..svetlana kiritchenko and saif mohammad.
2017. best-worst scaling more reliable than rating scales: acase study on sentiment intensity annotation.
in pro-ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 465–470..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
text summarizationbranches out..yang liu and mirella lapata.
2019. text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3721–3731..paul michel, omer levy, and graham neubig.
2019.in ad-are sixteen heads really better than one?
vances in neural information processing systems,pages 14014–14024..saeid motiian, quinn jones, seyed mehdi iranmanesh,and gianfranco doretto.
2017. few-shot adversarialdomain adaptation.
in nips..jessica ouyang, boya song, and kathy mckeown.
2019. a robust abstractive system for cross-lingualsummarization.
in proceedings of the 2019 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 2025–2031, minneapolis, minnesota.
association for computational linguistics..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for compu-tational linguistics, pages 5797–5808..xiaojun wan.
2011. using bilingual information forin pro-cross-language document summarization.
ceedings of the 49th annual meeting of the associ-ation for computational linguistics: human lan-guage technologies, pages 1546–1555..xiaojun wan, huiying li, and jianguo xiao.
2010.cross-language document summarization based onmachine translation quality prediction.
in proceed-ings of the 48th annual meeting of the associationfor computational linguistics, pages 917–926..yaqing wang, quanming yao, james t kwok, and li-onel m ni.
2020. generalizing from a few exam-ples: a survey on few-shot learning.
acm comput-ing surveys (csur), 53(3):1–34..wang yan, jordan yap, and greg mori.
2015. multi-task transfer methods to improve one-shot learningfor multimedia event detection.
in bmvc, pages 37–1..jin-ge yao, xiaojun wan, and jianguo xiao.
2015.phrase-based compressive cross-language summa-rization.
in proceedings of the 2015 conference onempirical methods in natural language processing,pages 118–127..jiajun zhang, yu zhou, and chengqing zong.
2016.abstractive cross-language summarization via trans-lation model enhanced predicate argument structurefusing.
ieee/acm transactions on audio, speech,and language processing, 24(10):1842–1853..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..jingqing zhang, yao zhao, mohammad saleh, and pe-ter j liu.
2019a.
pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization.
arxiv preprint arxiv:1912.08777..shi-qi shen, yun chen, cheng yang, zhi-yuan liu,mao-song sun, et al.
2018. zero-shot cross-lingualieee/acm transac-neural headline generation.
tions on audio, speech, and language processing,26(12):2319–2327..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in in-ternational conference on machine learning, pages5926–5936..tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2019b.
bertscore:in interna-evaluating text generation with bert.
tional conference on learning representations..junnan zhu, haoran li, tianshang liu, yu zhou, ji-ajun zhang, and chengqing zong.
2018. msmo:multimodal summarization with multimodal output.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages4154–4164..6919junnan zhu, qian wang, yining wang, yu zhou, jiajunzhang, shaonan wang, and chengqing zong.
2019.ncls: neural cross-lingual summarization.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 3045–3055..junnan zhu, yu zhou, jiajun zhang, and chengqingzong.
2020. attend, translate and summarize: anefﬁcient method for neural cross-lingual summariza-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 1309–1321..6920a samples.
we list some samples from outputs of various mod-els.
samples from en2desum dataset are shownin figure 7. samples from zh2ensum dataset areshown in figure 8. we randomly selected one sam-ple from each low-resource scenario..b attention distributions.
in section “probing into attention heads”, we se-lected some representative attention heads.
we listall of our trained attention heads among 6 trans-former decoder layers in figure 9 and figure 10for reference..figure 7: examples of models trained in en2desumdataset..6921figure 8: examples of models trained in zh2ensum dataset.
6922figure 9: visualization of all the 48 self attention heads.
the x-axis and y-axis are both concatenated source-language summary sa and target-language summary sb tokens.
each row contains all of the attention heads ofcorresponding layer from bottom to the top.
the darker color shows the more highly related associations betweentokens..6923figure 10: visualization of all the 48 encoder-decoder attention heads.
the x-axis is concatenated source-languagesummary sa and target-language summary sb tokens while the y-axis is document da tokens.
each row containsall of the attention heads of corresponding layer from bottom to the top.
the darker color shows the more highlyrelated associations between tokens..6924