towards propagation uncertainty: edge-enhanced bayesian graphconvolutional networks for rumor detection.
lingwei wei1,4, dou hu2, wei zhou1∗, zhaojuan yue3, songlin hu1,4∗1 institute of information engineering, chinese academy of sciences2 national computer system engineering research institute of china3 computer network information center, chinese academy of sciences4 school of cyber security, university of chinese academy of sciences{weilingwei18, hudou18}@mails.ucas.edu.cn{zhouwei, husonglin}@iie.ac.cnyuezhaojuan@cnic.cn.
abstract.
detecting rumors on social media is a verycritical task with signiﬁcant implications tothe economy, public health, etc.
previ-ous works generally capture effective featuresfrom texts and the propagation structure.
how-ever, the uncertainty caused by unreliable re-lations in the propagation structure is com-mon and inevitable due to wily rumor pro-ducers and the limited collection of spreaddata.
most approaches neglect it and mayseriously limit the learning of features.
to-wards this issue, this paper makes the ﬁrstattemptto explore propagation uncertaintyspeciﬁcally, we pro-for rumor detection.
pose a novel edge-enhanced bayesian graphconvolutional network (ebgcn) to capturerobust structural features.
the model adap-tively rethinks the reliability of latent relationsby adopting a bayesian approach.
besides,we design a new edge-wise consistency train-ing framework to optimize the model by en-forcing consistency on relations.
experimentson three public benchmark datasets demon-strate that the proposed model achieves betterperformance than baseline methods on both ru-mor detection and early rumor detection tasks..1.introduction.
with the ever-increasing popularity of social me-dia sites, user-generated messages can quicklyreach a wide audience.
however, social mediacan also enable the spread of false rumor infor-mation (vosoughi et al., 2018).
rumors are nowviewed as one of the greatest threats to democracy,journalism, and freedom of expression.
therefore,detecting rumors on social media is highly desir-able and socially beneﬁcial (ahsan et al., 2019)..* corresponding author..figure 1: an example of uncertain propagation struc-ture.
it includes inaccurate relations, making con-structed graph inconsistent with the real propagation..almost all the previous studies on rumor de-tection leverage text content including the sourcetweet and all user retweets or replies.
as time goeson, rumors form their speciﬁc propagation struc-tures after being retweeted or replied to.
vosoughi(2015); vosoughi et al.
(2018) have conﬁrmed ru-mors spread signiﬁcantly farther, faster, deeper, andmore broadly than the truth.
they provide the possi-bility of detecting rumors through the propagationstructure.
some works (ma et al., 2016; kochkinaet al., 2018) typically learn temporal features alonefrom propagation sequences, ignoring the internaltopology.
recent approaches (ma et al., 2018;khoo et al., 2020) model the propagation struc-ture as trees to capture structural features.
bianet al.
(2020); wei et al.
(2019) construct graphs andaggregate neighbors’ features through edges basedon reply or retweet relations..however, most of them only work well in a nar-row scope since they treat these relations as reliableedges for message-passing.
as shown in figure1, the existence of inaccurate relations brings un-certainty in the propagation structure.
the neglectof unreliable relations would lead to severe erroraccumulation through multi-layer message-passingand limit the learning of effective features..we argue such inherent uncertainty in the prop-agation structure is inevitable for two aspects: i).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3845–3854august1–6,2021.©2021associationforcomputationallinguistics3845tweetrelationsconstructed graph/tree1524realpropagation124536xinaccurate relations in the real world, rumor producers are always wily.
they tend to viciously manipulate others to createfake supporting tweets or remove opposing voicesto evade detection (yang et al., 2020).
in thesecommon scenarios, relations can be manipulated,which provides uncertainty in the propagation struc-ture.
ii) some annotations of spread relations aresubjective and fragmentary (ma et al., 2017; zu-biaga et al., 2016).
the available graph would bea portion of the real propagation structure as wellas contain noisy relations, resulting in uncertainty.
therefore, it is very challenging to handle inherentuncertainty in the propagation structure to obtainrobust detection results..to alleviate this issue, we make the ﬁrst at-tempt to explore the uncertainty in the propagationstructure.
speciﬁcally, we propose a novel edge-enhanced bayesian graph convolutional network(ebgcn) for rumor detection to model the un-certainty issue in the propagation structure from aprobability perspective.
the core idea of ebgcnis to adaptively control the message-passing basedon the prior belief of the observed graph to sur-rogate the ﬁxed edge weights in the propagationgraph.
in each iteration, edge weights are inferredby the posterior distribution of latent relations ac-cording to the prior belief of node features in theobserved graph.
then, we utilize graph convolu-tional layers to aggregate node features by aggre-gating various adjacent information on the reﬁningedges.
through the above network, ebgcn canhandle the uncertainty in the propagation structureand promote the robustness of rumor detection..moreover, due to the unavailable of missingor inaccurate relations for training the proposedmodel, we design a new edge-wise consistencytraining framework.
the framework combines un-supervised consistency training on these unlabeledrelations into the original supervised training onlabeled samples, to promote better learning.
wefurther ensure the consistency between the latentdistribution of edges and the distribution of nodefeatures in the observed graph by computing kl-divergence between two distributions.
ultimately,both the cross-entropy loss of each claim and thebayes by backprop loss of latent relations will beoptimized to train the proposed model..we conduct experiments on three real-worldbenchmark datasets (i.e., twitter15, twitter16, andpheme).
extensive experimental results demon-strate the effectiveness of our model.
ebgcn of-.
fers a superior uncertainty representation strategyand boosts the performance for rumor detection.
the main contributions of this work are summa-rized as follows:.
• we propose novel edge-enhanced bayesiangraph convolutional networks (ebgcn) tohandle the uncertainty in a probability manner.
to the best of our knowledge, this is the ﬁrstattempt to consider the inherent uncertainty inthe propagation structure for rumor detection..• we design a new edge-wise consistency train-ing framework to optimize the model withunlabeled latent relations..• experiments on three real-world benchmarkdatasets demonstrate the effectiveness of ourmodel on both rumor detection and early ru-mor detection tasks1..2 related work.
2.1 rumor detection.
traditional methods on rumor detection adoptedmachine learning classiﬁers based on handcraftedfeatures, such as sentiments (castillo et al., 2011),bag of words (enayet and el-beltagy, 2017) andtime patterns (ma et al., 2015).
based on salientfeatures of rumors spreading, wu et al.
(2015); maet al.
(2017) modeled propagation trees and thenused svm with different kernels to detect rumors.
recent works have been devoted to deep learn-ing methods.
ma et al.
(2016) employed recurrentneural networks (rnn) to sequentially processeach timestep in the rumor propagation sequence.
to improve it, many researchers captured morelong-range dependency via attention mechanisms(chen et al., 2018), convolutional neural networks(yu et al., 2017; chen et al., 2019), and trans-former (khoo et al., 2020).
however, most of themfocused on learning temporal features alone, ignor-ing the internal topology structure..to capture topological-structural features, maet al.
(2018) presented two recursive neural net-work (rvnn) based on bottom-up and top-downpropagation trees.
yuan et al.
(2019); lu and li(2020); nguyen et al.
(2020) formulated the prop-agation structure as graphs.
inspired by graphconvolutional network (gcn) (kipf and welling,2017), bian et al.
(2020) ﬁrst applied two gcns.
1the source code is available at https://github..com/weilingwei96/ebgcn..3846based on the propagation and dispersion graphs.
wei et al.
(2019) jointly modeled the structuralproperty by gcn and the temporal evolution byrnn..however, most of them treat the edge as the re-liable topology connection for message-passing.
ignoring the uncertainty caused by unreliable re-lations could lead to lacking robustness and makeit risky for rumor detection.
inspired by valuableresearch (zhang et al., 2019a) that modeled uncer-tainty caused by ﬁnite available textual contents,this paper makes the ﬁrst attempt to consider theuncertainty caused by unreliable relations in thepropagation structure for rumor detection..2.2 graph neural networks.
graph neural networks (gnns) (kipf and welling,2017; schlichtkrull et al., 2018; velickovic et al.,2018) have demonstrated remarkable performancein modeling structured data in a wide variety ofﬁelds, e.g., text classifcation (yao et al., 2019),recommendation system (wu et al., 2019) and emo-tion recognition (ghosal et al., 2019).
althoughpromising, they have limited capability to handleuncertainty in the graph structure.
while the graphsemployed in real-world applications are themselvesderived from noisy data or modeling assumptions.
to alleviate this issue, some valuable works (luoet al., 2020; zhang et al., 2019b) provide an ap-proach for incorporating uncertain graph informa-tion by exploiting a bayesian framework (maddoxet al., 2019).
inspired by them, this paper exploresthe uncertainty in the propagation structure froma probability perspective, to obtain more robustrumor detection results..3 problem statement.
this paper develops ebgcn which processes textcontents and propagation structure of each claimfor rumor detection.
in general, rumor detectioncommonly can be regarded as a multi-classiﬁcationtask, which aims to learn a classiﬁer from trainingclaims for predicting the label of a test claim..formally, let c = {c1, c2, ..., cm} be the ru-mor detection dataset, where ci is the i-th claimand m is the number of claims.
for each claimci = {ri, xini−1, gi}, gi indicates thepropagation structure, ri is the source tweet, xijrefers to the j-th relevant retweet, and ni representsthe number of tweets in the claim ci.
speciﬁcally,gi is deﬁned as a propagation graph gi = (cid:104)vi, ei(cid:105).
2, ..., xi.
1, xi.
1, xi.
with the root node ri (ma et al., 2018; bian et al.,2020), where vi = {ri, xini−1} refers tothe node set and ei = {eist|s, t = 0, ..., ni − 1}represent a set of directed edges from a tweet to itscorresponding retweets.
denote ai ∈ rni×ni asan adjacency matrix where the initial value is.
2, ..., xi.
αst =.
(cid:26)1,0,.if eist ∈ eiotherwise.
..besides, each claim ci.
is annotated with aground-truth label yi ∈ y, where y represents ﬁne-grained classes.
our goal is to learn a classiﬁerfrom the labeled claimed set, that is f : c → y..4 the proposed model.
in this section, we propose a novel edge-enhancedbayesian graph convolutional network (ebgcn)for rumor detection in section 4.2. for better train-ing, we design an edge-wise consistency trainingframework to optimize ebgcn in section 4.3..4.1 overview.
the overall architecture of ebgcn is shown infigure 2. given the input sample including textcontents and its propagation structure, we ﬁrst for-mulate the propagation structure as directed graphswith two opposite directions, i.e., a top-down prop-agation graph and a bottom-up dispersion graph.
text contents are embedded by the text embed-ding layer.
after that, we iteratively capture richstructural characteristics via two main components,node update module, and edge inference module.
then, we aggregate node embeddings to generategraph embedding and output the label of the claim.
for training, we incorporate unsupervised con-sistency training on the bayes by backprop loss ofunlabeled latent relations.
accordingly, we opti-mize the model by minimizing the weighted sumof the unsupervised loss and supervised loss..4.2 edge-enhanced bayesian graph.
convolutional networks.
4.2.1 graph construction and text.
embedding.
the initial graph construction is similar to the pre-viou work (bian et al., 2020), i.e., build two distinctdirected graphs for the propagation structure ofeach claim ci.
the top-down propagation graph andbottom-up dispersion graph are denoted as gt dand gbu, respectively.
their corresponding initiali = a(cid:62)adjacency matrices are at di ..i = ai and abu.
i.i.
3847figure 2: the architecture of the proposed rumor detection model ebgcn..here, we leave out the superscript i in the followingdescription for better presenting our method..rently observed graph by adopting a soft connec-tion..the initial feature matrix of postings in the claimc can be extracted top-5000 words in terms of tf-idf values, denoted as x = [x0, x1, ..., xn−1] ∈rn×d0, where x0 ∈ rd0 is the vector of the sourcetweet and d0 is the dimensionality of textual fea-tures.
the initial feature matrices of nodes in prop-agation graph and dispersion graph are the same,i.e., xt d = xbu = x..4.2.2 node updategraph convolutional networks (gcns) (kipf andwelling, 2017) are able to extract graph structureinformation and better characterize a node’s neigh-borhood.
they deﬁne multiple graph conventionallayers (gcls) to iteratively aggregate features ofneighbors for each node and can be formulated as asimple differentiable message-passing framework.
motivated by gcns, we employ the gcl to updatenode features in each graph.
formally, node fea-tures at the l-th layer h(l) = [h(l)n−1]can be deﬁned as,.
1 , ..., h(l).
0 , h(l).
h(l) = σ( ˆa.
(l−1).
h(l−1)w(l) + b(l)),.
(1).
(l−1).
where ˆarepresents the normalization of adja-cency matrix a(l−1) (kipf and welling, 2017).
weinitialize node representations by textual features,i.e., h(0) = x..4.2.3 edge inferenceto alleviate the negative effects of unreliable rela-tions, we rethink edge weights based on the cur-.
speciﬁcally, we adjust the weight between twonodes by computing a transformation fe(·; θt)based on node representations at the previous layer.
then, the adjacency matrix will be updated, i.e.,.
g(l)t = fe.
(cid:107)h(l−1)i.
− h(l−1)j.
(cid:107); θt.
(cid:17).
,.
a(l) =.
σ(w(l).
t g(l).
t + b(l).
t ) · a(l−1)..(2).
(cid:16).
t(cid:88).
t=1.
t and w(l).
in practice, fe(·; θt) consists an convolutional layerand an activation function.
t refers to the numberof latent relation types.
σ(·) refers to a sigmoidfunction.
w(l)t are learnable parameters.
we perform share parameters to the edge infer-ence layer in two graphs gt d and gbu .
after thestack of transformations in two layers, the modelcan effectively accumulate a normalized sum offeatures of the neighbors driven by latent relations,denoted as ht d and hbu ..4.2.4 classiﬁcationwe regard the rumor detection task as a graph clas-siﬁcation problem.
to aggregate node representa-tions in the graph, we employ aggregator to formthe graph representations.
given the node represen-tations in the propagation graph ht d and the noderepresentations in the dispersion graph hbu , thegraph representations can be computed as:.
ct d = meanpooling(ht d),cbu = meanpooling(hbu ),.
(3).
3848unsupervised consistency lossnode featuresinput sample breaking: at least 10 dead, …@samuel: the religion of peace strikes again.
@samuel: hi, would you be willing to give…@... please call them terrorists …@edward...@imranali27.kill themtext embedding layer(cid:2207)(cid:1499)the ground-truth label(cid:3549)(cid:2207)the prediction labelfcpoolingpoolingretweet nodesedges(top-down propagation)edges(bottom-up dispersion)source tweet nodenode embeddinggraph embeddingsupervisedcross-entropy losstotal lossedgeinferenceadjust edge weights3561240.210.610.800.420.370.910.120.180.620.4608356124gclgclllayersfc(cid:1858)(cid:3087)(cid:521)(cid:2020)(cid:3047)(cid:2012)(cid:3047)(cid:2870)gaussian samplingthe probability of latent relations(cid:2)(cid:2020)(cid:3047)(cid:481)(cid:2012)(cid:3047)(cid:2870)edgeinference(cid:1869)(cid:2256)(cid:3548)(cid:2200)(cid:2164)(cid:481)(cid:2163)graph(top-down)graph(bottom-up)…(cid:1868)(cid:3548)(cid:2200)(cid:2164)(cid:481)(cid:2163)where meanpooling(·) refers to the mean-poolingaggregating function.
based on the concatenationof two distinct graph representations, label proba-bilities of all classes can be deﬁned by a full con-nection layer and a softmax function, i.e.,.
ˆy = sof tmax (cid:0)wc[ct d; cbu ] + bc.
(cid:1) ,.
(4).
where wc and bc are learnable parameter matrices..4.3 edge-wise consistency training.
framework.
for the supervised learning loss lc, we computethe cross-entropy of the predictions and groundtruth distributions c = {c1, c2, ..., cm}, i.e.,.
lc = −.
yilogˆyi,.
(5).
|y|(cid:88).
i.where yi is a vector representing distribution ofground truth label for the i-th claim sample..for the unsupervised learning loss le, weamortize the posterior distribution of the classiﬁca-tion weight p(ϕ) as q(ϕ) to enable quick predictionat the test stage and learn parameters by minimiz-ing the average expected loss over latent relations,i.e., ϕ∗ = arg minϕ le, where.
(cid:104).
(cid:16).
le = e.dkl.
p(ˆr(l)|h(l−1), g)(cid:107)qϕ(ˆr(l)|h(l−1), g).
(cid:17)(cid:105).
,.
ϕ∗ = arg max.
e[log.
p(ˆr(l)|h(l−1), ϕ)qϕ(ϕ|h(l−1), g)dϕ],.
(cid:90).
ϕ.
(6).
where ˆr is the prediction distribution of latent re-lations.
to ensure likelihood tractably, we modelthe prior distribution of each latent relation rt, t ∈[1, t ] independently.
for each relation, we deﬁne afactorized gaussian distribution for each latent re-lation qϕ(ϕ|h(l−1), g; θ) with means µt and vari-ances δ2t set by the transformation layer,.
qϕ(ϕ|h(l−1), g; θ)) =.
qϕ(ϕt|{g(l).
t }t.t=1).
t(cid:89).
t=1t(cid:89).
distribution of prototype vectors.
the likelihood oflatent relations from the l-th layer based on nodeembeddings can be adaptively computed by,.
p(ˆr(l)|h(l−1), ϕ) =.
p(ˆr(l)t.|h(l−1), ϕt),.
t(cid:89).
t=1.
p(ˆr(l)t.|h(l−1), ϕt) =.
(cid:16).
exp.
(cid:17).
wtg(l)(cid:16).
t + btwtg(l).
t + bt.
(cid:17) ..(cid:80)t.t=1 exp.
(8).
in this way, the weight of edges can be adaptivelyadjusted based on the observed graph, which canthus be used to effectively pass messages and learnmore discriminative features for rumor detection.
to sum up, in training, we optimize our modelebgcn by minimizing the cross-entropy loss oflabeled claims lc and bayes by backprop loss ofunlabeled latent relations le, i.e.,.
θ∗ = arg minθ.γlc + (1 − γ)le,.
(9).
where γ is the trade-off coefﬁcient..5 experimental setup.
5.1 datasets.
we evaluate the model on three real-world bench-mark datasets: twitter15 (ma et al., 2017), twit-ter16 (ma et al., 2017), and pheme (zubiagaet al., 2016).
the statistics are shown in table 1.twitter15 and twitter162 contain 1,490 and 818claims, respectively.
each claim is labeled as non-rumor (nr), false rumor (f), true rumor (t), orunveriﬁed rumor (u).
following (ma et al., 2018;bian et al., 2020), we randomly split the datasetinto ﬁve parts and conduct 5-fold cross-validationto obtain robust results.
pheme dataset3 provides2,402 claims covering nine events and containsthree labels, false rumor (f), true rumor (t),and unveriﬁed rumor (u).
following the previ-ous work (wei et al., 2019), we conduct leave-one-event-out cross-validation, i.e., in each fold, oneevent’s samples are used for testing, and all the restare used for training..=.
n (µt, δ2.
t ),.
(7).
µt = fµ({g(l).
t }t.t=1t=1; θµ), δ2.
tt = fδ({g(l)t=1; θδ),t }.
5.2 baselines.
where fµ(·; θµ) and fδ(·; θµ) refer to compute themean and variance of input vectors, parameterizedby θµ and θδ, respectively.
such that amounts toset the weight of each latent relation..besides, we also consider the likelihood of la-tent relations when parameterizing the posterior.
for twitter15 and twitter16, we compare our pro-posed model with the following methods.
dtc.
2https://www.dropbox.com/s/.
7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0.
3https://figshare.com/articles/dataset/pheme_dataset_for_rumour_detection_and_veracity_classification/6392078.
3849dataset# of claims# of false rumors# of true rumors# of unveriﬁed rumors# of non-rumors# of postings.
twitter15 twitter16 pheme818205205203205204,820.
2,4026381,067697-105,354.
1,490370374374372331,612.table 1: statistics of the datasets..(castillo et al., 2011) adopted a decision tree clas-siﬁer based on information credibility.
svm-ts(ma et al., 2015) leveraged time series to modelthe chronological variation of social context fea-tures via a linear svm classiﬁer.
svm-tk (maet al., 2017) applied an svm classiﬁer with a prop-agation tree kernel to model the propagation struc-ture of rumors.
gru-rnn (ma et al., 2016) em-ployed rnns to model the sequential structuralfeatures.
rvnn (ma et al., 2018) adopted two re-cursive neural models based on a bottom-up anda top-down propagation tree.
sta-plan (khooet al., 2020) employed transformer networks to in-corporate long-distance interactions among tweetswith propagation tree structure.
bigcn (bianet al., 2020) utilized bi-directional gcns to modelbottom-up propagation and top-down dispersion..for pheme, we compare with several repre-sentative state-of-the-art baselines.
niletmrg(enayet and el-beltagy, 2017) used linear sup-port vector classiﬁcation based on bag of words.
branchlstm (kochkina et al., 2018) decom-posed the propagation tree into multiple branchesand adopted a shared lstm to capture structuralfeatures.
rvnn (ma et al., 2018) consisted oftwo recursive neural networks to model propaga-tion trees.
hierarchical gcn-rnn (wei et al.,2019) modeled structural property based on gcnand rnn.
bigcn (bian et al., 2020) consisted ofpropagation and dispersion gcns to learn struc-tural features from propagation graph..5.3 evaluation metrics.
for twitter15 and twitter16, we follow (ma et al.,2018; bian et al., 2020; khoo et al., 2020) and eval-uate the accuracy (acc.)
over four categories andf1 score (f1) on each class.
for pheme, follow-ing (enayet and el-beltagy, 2017; kochkina et al.,2018; wei et al., 2019), we apply the accuracy(acc.
), macro-averaged f1 (mf1) as evaluationmetrics.
also, we report the weighted-averagedf1 (wf1) because of the imbalanced class problem..5.4 parameter settings.
following comparison baselines, the dimensionof hidden vectors in the gcl is set to 64. thenumber of latent relations t and the coefﬁcientweight γ are set to [1, 5] and [0.0, 1.0], respec-tively.
we train the model via backpropagation anda wildly used stochastic gradient descent namedadam (kingma and ba, 2015).
the learning rate isset to {0.0002, 0.0005, 0.02} for twitter15, twit-ter16, and pheme, respectively.
the training pro-cess is iterated upon 200 epochs and early stopping(yuan et al., 2007) is applied when the validationloss stops decreasing by 10 epochs.
the optimalset of hyperparameters are determined by testingthe performance on the fold-0 set of twitter15 andtwitter16, and the class-balanced charlie hebdoevent set of pheme..besides, on pheme, following (wei et al.,2019), we replace tf-idf features with word em-beddings by skip-gram with negative sampling(mikolov et al., 2013) and set the dimension oftextual features to 200. we implement this variantof bigcn and ebgcn, denoted as bigcn(skp)and ebgcn(skp), respectively..for results of baselines, we implement bigcnaccording to their public project4 under the sameenvironment.
other results of baselines are refer-enced from original papers (khoo et al., 2020; weiet al., 2019; ma et al., 2018)..6 results and analysis.
6.1 performance comparison with baselines.
table 2 shows results of rumor detection on twit-ter15, twitter16, and pheme datasets.
our pro-posed model ebgcn obtains the best perfor-mance among baselines.
speciﬁcally, for twitter15,ebgcn outperforms state-of-the-art models 2.4%accuracy and 3.6% f1 score of false rumor.
fortwitter16, our model obtains 3.4% and 6.0% im-provements on accuracy and f1 score of non-rumor,respectively.
for pheme, ebgcn signiﬁcantlyoutperforms previous work by 40.2% accuracy,34.7% mf1 , and 18.0% wf1..deep learning-based (rvnn, sta-plan,bigcn and ebgcn) outperform conventionalmethods using hand-crafted features (dtc, svm-ts), which reveals the superiority of learninghigh-level representations for detecting rumors..4https://github.com/tianbian95/bigcn.
3850method.
acc..45.5dtcsvm-ts54.4gru-rnn 64.166.7svm-tkrvnn72.3sta-plan 85.287.1bigcn89.2ebgcn.
method.
acc..46.5dtcsvm-ts54.4gru-rnn 63.666.7svm-tkrvnn72.3sta-plan 85.288.5bigcn91.5ebgcn.
twitter15nrf173.379.668.461.968.284.086.086.9twitter16nrf164.379.661.761.968.284.082.987.9.ff135.547.263.466.975.884.686.789.7.ff139.347.271.566.975.884.689.990.6.pheme.
tf131.740.468.877.282.188.491.493.4.tf141.940.457.777.282.188.493.294.7.uf141.548.357.164.565.483.785.486.7.uf140.348.352.764.565.483.788.291.0.method.
acc.
mf1 wf129.736.0niletmrg25.931.4branchlstm26.434.1rvnn31.7hierarchical gcn-rnn 35.646.749.2bigcn48.356.9bigcn(skp)62.969.0ebgcn57.571.5ebgcn(skp).
----63.266.874.679.1.table 2: results (%) of rumor detection..moreover, compared with sequence-based mod-els gru-rnn, and sta-plan, ebgcn outper-form them.
it can attribute that they capture tem-poral features alone but ignore internal topologystructures, which limit the learning of structuralfeatures.
ebgcn can aggregate neighbor featuresin the graph to learn rich structural features..furthermore, compared with state-of-the-artgraph-based bigcn, ebgcn also obtains betterperformance.
we discuss the fact for two main rea-sons.
first, bigcn treats relations among tweetnodes as reliable edges, which may introduce in-accurate or irrelevant features.
thereby their per-formance lacks robustness.
ebgcn considers theinherent uncertainty in the propagation structure.
in the model, the unreliable relations can be reﬁned.
(a) the effect of edge inference.
(b) the effect of unsupervised relation learning loss.
figure 3: results of model analysis on three datasets..in a probability manner, which boosts the bias ofexpress uncertainty.
accordingly, the robustnessof detection is enhanced.
second, the edge-wiseconsistency training framework ensures the con-sistency between uncertain edges and the currentnodes, which is also beneﬁcial to learn more effec-tive structural features for rumor detection..besides, ebgcn(skp) and bigcn(skp) out-performs ebgcn and bigcn that use tf-idffeatures in terms of acc.
and wf1.
it shows thesuperiority of word embedding to capture textualfeatures.
our model consistently obtains better per-formance in different text embedding.
it revealsthe stability of ebgcn..6.2 model analysis.
in this part, we further evaluate the effects of keycomponents in the proposed model..the effect of edge inference.
the number oflatent relation types t is a critical parameter in theedge inference module.
figure 3(a) shows the ac-curacy score against t .
the best performance isobtained when t is 2, 3, and 4 on twitter15, twit-ter16, and pheme, respectively.
besides, thesebest settings are different.
an idea explanation isthat complex relations among tweets are various indifferent periods and gradually tend to be more so-phisticated in the real world with the development.
3851tion deadline or the less the tweet count, the lesspropagation information can be available..figure 4 shows the performance of early rumordetection.
first, all models climb as the detectiondeadline elapses or tweet count increases.
partic-ularly, at each deadline or tweet count, our modelebgcn reaches a relatively high accuracy scorethan other comparable models..second, compared with rvnn that capturestemporal features alone and stm-tk based onhandcrafted features, the superior performance ofebgcn and bigcn that explored rich structuralfeatures reveals that structural features are morebeneﬁcial to the early detection of rumors..third, ebgcn obtains better early detection re-sults than bigcn.
it demonstrates that ebgcncan learn more conducive structural features toidentify rumors by modeling uncertainty and en-hance the robustness for early rumor detection..overall, our model not only performs better long-term rumor detection but also boosts the perfor-mance of detecting rumors at an early stage..6.4 the case study.
in this part, we perform the case study to show theexistence of uncertainty in the propagation struc-ture and explain why ebgcn performs well.
werandomly sample a false rumor from pheme, asdepicted in figure 5. the tweets are formulatedas nodes and relations are modeled as edges in thegraph, where node 1 refers to the source tweet andnode 2-8 refer to the following retweets..as shown in the left of figure 5, we observe thattweet 5 is irrelevant with tweet 1 although replying,which reveals the ubiquity of unreliable relationsamong tweets in the propagation structure and it isreasonable to consider the uncertainty caused bythese unreliable relations..right of figure 5 indicates constructed graphswhere the color shade indicates the value of edgeweights.
the darker the color, the greater the edgeweight.
the existing graph-based models alwaysgenerate the representation of node 1 by aggregat-ing the information of its all neighbors (node 2, 5,and 6) according to seemingly reliable edges.
how-ever, edge between node 1 and 5 would bring noisefeatures and limit the learning of useful features forrumor detection.
our model ebgcn successfullyweakens the negative effect of this edge by both theedge inference layer under the ingenious edge-wiseconsistency training framework.
accordingly, the.
figure 4: performance of early rumor detection..of social media.
the edge inference module canadaptively reﬁne the reliability of these complexrelations by the posterior distribution of latent re-lations.
it enhances the bias of uncertain relationsand promotes the robustness of rumor detection..the effect of unsupervised relation learningloss.
the trade-off parameter γ controls the ef-fect of the proposed edge-wise consistency trainingframework.
γ = 0.0 means this framework is omit-ted.
the right in figure 3 shows the accuracy scoreagainst γ. when this framework is removed, themodel gains the worst performance.
the optimalγ is 0.4, 0.3, and 0.3 on twitter15, twitter16, andpheme, respectively.
the results proves the ef-fectiveness of this framework.
due to wily rumorproducers and limited annotations of spread infor-mation, it is common and inevitable that datasetscontains unreliable relations.
this framework canensure the consistency between edges and the corre-sponding node pairs to avoid the negative features..6.3 early rumor detection.
rumor early detection is to detect a rumor at itsearly stage before it wide-spreads on social mediaso that one can take appropriate actions earlier.
itis especially critical for a real-time rumor detec-tion system.
to evaluate the performance on rumorearly detection, we follow (ma et al., 2018) andcontrol the detection deadline or tweet count sincethe source tweet was posted.
the earlier the detec-.
3852figure 5: the case study.
left shows a false rumor sampled from pheme.
the gray-highlighted tweet is theirrelevant one towards this rumor propagation but included in.
right is the constructed directed graphs in top-down and bottom-up directions based on the propagation structure.
our model iteratively adjusts the weights ofedges in each graph to strength the effect of reliable edges and weaken the effect of unreliable edges..model is capable of learning more conducive char-acteristics and enhances the robustness of results..in pakdd (workshops), volume 11154 of lecturenotes in computer science, pages 40–52.
springer..7 conclusion.
in this paper, we have studied the uncertainty inthe propagation structure from a probability per-spective for rumor detection.
speciﬁcally, wepropose edge-enhanced bayesian graph convo-lutional networks (ebgcn) to handle uncertaintywith a bayesian method by adaptively adjustingweights of unreliable relations.
besides, we de-sign an edge-wise consistency training frameworkincorporating unsupervised relation learning to en-force the consistency on latent relations.
exten-sive experiments on three commonly benchmarkdatasets have proved the effectiveness of modelinguncertainty in the propagation structure.
ebgcnsigniﬁcantly outperforms baselines on both rumordetection and early rumor detection tasks..references.
mohammad ahsan, madhu kumari, and t. p. sharma.
2019. rumors detection, veriﬁcation and control-ling mechanisms in online social networks: a sur-vey.
online soc.
networks media, 14..tian bian, xi xiao, tingyang xu, peilin zhao, wen-bing huang, yu rong, and junzhou huang.
2020.rumor detection on social media with bi-directionalgraph convolutional networks.
in aaai, pages 549–556. aaai press..carlos castillo, marcelo mendoza, and barbarapoblete.
2011. information credibility on twitter.
inwww, pages 675–684.
acm..tong chen, xue li, hongzhi yin, and jun zhang.
2018.call attention to rumors: deep attention based re-current neural networks for early rumor detection..yixuan chen, jie sui, liang hu, and wei gong.
2019.attention-residual network with cnn for rumor de-tection.
in cikm, pages 1121–1130.
acm..omar enayet and samhaa r. el-beltagy.
2017.niletmrg at semeval-2017 task 8: determining ru-mour and veracity support for rumours on twitter.
pages 470–474.
association for computational lin-guistics..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander f. gelbukh.
2019.dialoguegcn: a graph convolutional neural net-work for emotion recognition in conversation.
inemnlp/ijcnlp (1), pages 154–164.
associationfor computational linguistics..ling min serena khoo, hai leong chieu, zhong qian,and jing jiang.
2020. interpretable rumor detectionin microblogs by attending to user interactions.
inaaai, pages 8783–8790.
aaai press..diederik p. kingma and jimmy ba.
2015. adam:in iclr.
a method for stochastic optimization.
(poster)..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in iclr (poster).
openreview.net..elena kochkina, maria liakata, and arkaitz zubiaga.
2018. all-in-one: multi-task learning for rumourveriﬁcation.
in coling, pages 3402–3413.
associ-ation for computational linguistics..yi-ju lu and cheng-te li.
2020. gcan: graph-awareco-attention networks for explainable fake news de-tection on social media.
in acl, pages 505–514.
as-sociation for computational linguistics..yadan luo, zi huang, zheng zhang, ziwei wang,mahsa baktashmotlagh, and yang yang.
2020.learning from the past: continual meta-learningin aaai,with bayesian graph neural networks.
pages 5021–5028.
aaai press..3853(cid:884)(cid:883)5(cid:888)(cid:889)(cid:890)(cid:885)(cid:886)hi henry would you be willing to give itv news a phone interview for our lunchtime bulletin in 2 hours?the religion of peace strikes again.if only people didn't hand out gunsexplain.tickets go on sale this weekkill them wherever you find them, and turn them out from where they have turned you out.idiot strikes again with his stupid tweet.breaking: at least 10 dead, 5 injured after to gunman open fire in offices of charlie  hebdo, satirical mag that published mohammed cartoonsxedge inferenceinitial propagation structurerefined propagationstructure(cid:884)(cid:883)5(cid:888)(cid:889)(cid:890)(cid:885)(cid:886)(cid:884)(cid:883)5(cid:888)(cid:889)(cid:890)(cid:885)(cid:886)(cid:884)(cid:883)5(cid:888)(cid:889)(cid:890)(cid:885)(cid:886)(cid:883)(cid:884)(cid:885)(cid:886)5(cid:888)(cid:889)(cid:890)0.640.500.890.190.420.570.780.660.900.050.490.620.740.351.01.01.01.01.01.01.01.01.01.01.01.01.01.0jing ma, wei gao, prasenjit mitra, sejeong kwon,bernard j. jansen, kam-fai wong, and meeyoungcha.
2016. detecting rumors from microblogs within ijcai, pages 3818–recurrent neural networks.
3824. ijcai/aaai press..jing ma, wei gao, zhongyu wei, yueming lu, andkam-fai wong.
2015. detect rumors using time se-ries of social context information on microbloggingwebsites.
in cikm, pages 1751–1754.
acm..jing ma, wei gao, and kam-fai wong.
2017. detectrumors in microblog posts using propagation struc-ture via kernel learning.
in acl (1), pages 708–717.
association for computational linguistics..jing ma, wei gao, and kam-fai wong.
2018. ru-mor detection on twitter with tree-structured recur-sive neural networks.
in acl (1), pages 1980–1989.
association for computational linguistics..wesley j. maddox, pavel izmailov, timur garipov,dmitry p. vetrov, and andrew gordon wilson.
2019.a simple baseline for bayesian uncertainty in deeplearning.
in neurips, pages 13132–13143..shu wu, yuyuan tang, yanqiao zhu, liang wang,xing xie, and tieniu tan.
2019. session-based rec-ommendation with graph neural networks.
in aaai,pages 346–353.
aaai press..xiaoyu yang, yuefei lyu, tian tian, yifei liu, yudongliu, and xi zhang.
2020. rumor detection on socialmedia with graph structured adversarial learning.
inijcai, pages 1417–1423.
ijcai.org..liang yao, chengsheng mao, and yuan luo.
2019.graph convolutional networks for text classiﬁcation.
in aaai, pages 7370–7377.
aaai press..feng yu, qiang liu, shu wu, liang wang, and tieniutan.
2017. a convolutional approach for misinfor-mation identiﬁcation.
in ijcai, pages 3901–3907..chunyuan yuan, qianwen ma, wei zhou, jizhong han,and songlin hu.
2019. jointly embedding the localand global relations of heterogeneous graph for ru-mor detection.
in icdm, pages 796–805.
ieee..yao yuan, lorenzo rosasco, and andrea caponnetto.
2007. on early stopping in gradient descent learn-ing.
constructive approximation, 26(2):289 – 315..tom´as mikolov, ilya sutskever, kai chen, gregory s.corrado, and jeffrey dean.
2013. distributed repre-sentations of words and phrases and their composi-tionality.
in nips, pages 3111–3119..qiang zhang, aldo lipani, shangsong liang, and em-ine yilmaz.
2019a.
reply-aided detection of mis-in www,information via bayesian deep learning.
pages 2333–2343.
acm..van-hoang nguyen, kazunari sugiyama, preslavnakov, and min-yen kan. 2020. fang: leveragingsocial context for fake news detection using graphrepresentation.
in cikm, pages 1165–1174.
acm..yingxue zhang, soumyasundar pal, mark coates, anddeniz ¨ustebay.
2019b.
bayesian graph convolu-tional neural networks for semi-supervised classiﬁ-cation.
in aaai, pages 5829–5836.
aaai press..arkaitz zubiaga, geraldine wong sak hoi, marialiakata, rob procter, and peter tolmie.
2016.analysing how people orient to and spread rumoursin social media by looking at conversational threads.
plos one, 11(3):e0150989..michael sejr schlichtkrull, thomas n. kipf, peterbloem, rianne van den berg, ivan titov, and maxwelling.
2018. modeling relational data with graphconvolutional networks.
in eswc, volume 10843 oflecture notes in computer science, pages 593–607.
springer..petar velickovic, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
2018. graph attention networks.
in iclr (poster).
openreview.net..soroush vosoughi.
2015. automatic detection and ver-.
iﬁcation of rumors on twitter..soroush vosoughi, deb roy, and sinan aral.
2018.the spread of true and false news online.
science,359(6380):1146–1151..penghui wei, nan xu, and wenji mao.
2019. mod-eling conversation structure and temporal dynamicsfor jointly predicting rumor stance and veracity.
inemnlp/ijcnlp (1), pages 4786–4797.
associationfor computational linguistics..ke wu, song yang, and kenny q. zhu.
2015. false ru-mors detection on sina weibo by propagation struc-tures.
in icde, pages 651–662..3854