a closer look at few-shot crosslingual transfer:the choice of shots matters.
mengjie zhao1* yi zhu2* ehsan shareghi3, 2.ivan vuli´c2.
roi reichart4 anna korhonen2 hinrich sch ¨utze11cis, lmu munich 2ltl, university of cambridge3department of data science & ai, monash university4faculty of industrial engineering and management, technion, iitmzhao@cis.lmu.de, {yz568,iv250,alk23}@cam.ac.uk,ehsan.shareghi@monash.edu, roiri@technion.ac.il.
abstract.
few-shot crosslingual transfer has been shownto outperform its zero-shot counterpart withpretrained encoders like multilingual bert.
despite its growing popularity, little to no at-tention has been paid to standardizing and an-alyzing the design of few-shot experiments.
in this work, we highlight a fundamental riskposed by this shortcoming, illustrating that themodel exhibits a high degree of sensitivity tothe selection of few shots.
we conduct a large-scale experimental study on 40 sets of sampledfew shots for six diverse nlp tasks across upto 40 languages.
we provide an analysis ofsuccess and failure cases of few-shot transfer,which highlights the role of lexical features.
additionally, we show that a straightforwardfull model ﬁnetuning approach is quite effec-tive for few-shot transfer, outperforming sev-eral state-of-the-art few-shot approaches.
asa step towards standardizing few-shot crosslin-gual experimental designs, we make our sam-pled few shots publicly available.1.
1.introduction.
multilingual pretrained encoders like multilingualbert (mbert; devlin et al.
(2019)) and xlm-r (conneau et al., 2020) are the top performersin crosslingual tasks such as natural language in-ference (conneau et al., 2018), document clas-siﬁcation (schwenk and li, 2018; artetxe andschwenk, 2019), and argument mining (toledo-ronen et al., 2020).
they enable transfer learn-ing through language-agnostic representations incrosslingual setups (hu et al., 2020)..a widely explored transfer scenario is zero-shotcrosslingual transfer (pires et al., 2019; conneauand lample, 2019; artetxe and schwenk, 2019),.
* equal contribution..1code and resources are available at https://github..com/fsxlt.
where a pretrained encoder is ﬁnetuned on abun-dant task data in the source language (e.g., english)and then directly evaluated on target-language testdata, achieving surprisingly good performance (wuand dredze, 2019; hu et al., 2020).
however, thereis evidence that zero-shot performance reported inthe literature has large variance and is often not re-producible (keung et al., 2020a; rios et al., 2020);the results in languages distant from english fall farshort of those similar to english (hu et al., 2020;liang et al., 2020)..lauscher et al.
(2020) stress the importance offew-shot crosslingual transfer instead, where theencoder is ﬁrst ﬁnetuned on a source languageand then further ﬁnetuned with a small amount(10–100) of examples (few shots) of the target lan-guage.
the few shots substantially improve modelperformance of the target language with negligi-ble annotation costs (garrette and baldridge, 2013;hedderich et al., 2020)..in this work, however, we demonstrate that thegains from few-shot transfer exhibit a high degreeof sensitivity to the selection of few shots.
forexample, different choices for the few shots canyield a performance variance of over 10% accuracyin a standard document classiﬁcation task.
mo-tivated by this, we propose to ﬁx the few shotsfor fair comparisons between different crosslingualtransfer methods, and provide a benchmark resem-bling the standard “n -way k-shot” few-shot learn-ing conﬁguration (fei-fei et al., 2006; koch et al.,2015).
we also evaluate and compare several state-of-the-art (sota) few-shot ﬁnetuning techniques,in order to understand their performance and sus-ceptibility to the variance related to few shots..we also demonstrate that the effectiveness offew-shot crosslingual transfer depends on the typeof downstream task.
for syntactic tasks such asnamed-entity recognition, the few shots can im-prove results by up to ≈20 f1 points.
for chal-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5751–5767august1–6,2021.©2021associationforcomputationallinguistics5751lenging tasks like adversarial paraphrase identiﬁca-tion, the few shots do not help and even sometimeslead to worse performance than zero-shot transfer.
to understand these phenomena, we conduct addi-tional in-depth analyses, and ﬁnd that the modelstend to utilize shallow lexical hints (geirhos et al.,2020) in the target language, rather than leverag-ing abstract crosslingual semantic features learnedfrom the source language..our contributions: 1) we show that few-shotcrosslingual transfer is prone to large variations intask performance; this property hinders unbiasedassessments of the effectiveness of different few-shot methods.
2) to remedy this issue, we publishﬁxed and standardized few shots to support faircomparisons and reproducibility.
3) we empiri-cally verify that few-shot crosslingual transfer hasdifferent performance impact on structurally differ-ent tasks; we provide in-depth analyses concerningthe source of performance gains.
4) we analyzeseveral sota few-shot learning methods, and showthat they underperform simple full model ﬁnetun-ing.
we hope that our work will shed new lighton the potential and current difﬁculties of few-shotlearning in crosslingual setups..2 background and related work.
zero-/few-shot crosslingual transfer.
multi-lingual pretrained encoders show strong zero-shotcrosslingual transfer (zs-xlt) ability in variousnlp tasks (pires et al., 2019; hsu et al., 2019;artetxe and schwenk, 2019).
in order to guideand measure the progress, standardized bench-marks like xtreme (hu et al., 2020) and xglue(liang et al., 2020) have been developed..recently, lauscher et al.
(2020) and hedderichet al.
(2020) extended the focus on few-shotcrosslingual transfer (fs-xlt): they assume theavailability of a handful of labeled examples in atarget language,2 which are used to further ﬁnetunea source-trained model.
the extra few shots bringlarge performance gains at low annotation cost.
inthis work, we systematically analyze this recentfs-xlt scenario..fs-xlt resembles the intermediate-task trans-fer (stilt) approach (phang et al., 2018; pruk-sachatkun et al., 2020).
in stilt, a pretrainedencoder is ﬁnetuned on a resource-rich intermedi-.
2according to garrette and baldridge (2013), it is possibleto collect ≈100 pos-annotated sentences in two hours evenfor low-resource languages such as malagasy..ate task, and then ﬁnetuned on a (resource-lean)target task.
likewise, fs-xlt focuses on transfer-ring knowledge and general linguistic intelligence(yogatama et al., 2019), although such transfer isbetween languages in the same task instead of be-tween different tasks..few-shot learning was ﬁrst explored in com-puter vision (miller et al., 2000; fei-fei et al., 2006;koch et al., 2015); the aim there is to learn newconcepts with only few images.
methods like pro-totypical networks (snell et al., 2017) and model-agnostic meta-learning (maml; finn et al.
(2017))have also been applied to many monolingual (typi-cally english) nlp tasks such as relation classiﬁ-cation (han et al., 2018; gao et al., 2019), named-entity recognition (hou et al., 2020a), word sensedisambiguation (holla et al., 2020), and text clas-siﬁcation (yu et al., 2018; yin, 2020; yin et al.,2020; bansal et al., 2020; gupta et al., 2020).
how-ever, recent few-shot learning methods in computervision consisting of two simple ﬁnetuning stages,ﬁrst on base-class images and then on new-classfew shots, have been shown to outperform mamland achieve sota scores (wang et al., 2020; chenet al., 2020; tian et al., 2020; dhillon et al., 2020).
inspired by this work, we compare various few-shot ﬁnetuning methods from computer vision inthe context of fs-xlt..task performance variance.
deep neural net-works’ performance on nlp tasks is bound to ex-hibit large variance.
reimers and gurevych (2017)and dror et al.
(2019) stress the importance of re-porting score distributions instead of a single scorefor fair(er) comparisons.
dodge et al.
(2020), mos-bach et al.
(2021), and zhang et al.
(2021) showthat ﬁnetuning pretrained encoders with differentrandom seeds yields performance with large vari-ance.
in this work, we examine a speciﬁc sourceof variance: we show that the choice of the fewshots in crosslingual transfer learning also intro-duces large variance in performance; consequently,we offer standardized few shots for more controlledand fair comparisons..3 method.
following lauscher et al.
(2020) and hedderichet al.
(2020), our fs-xlt method comprises twostages.
first, we conduct source-training: thepretrained mbert is ﬁnetuned with abundant an-notated data in the source language.
similar tohu et al.
(2020), liang et al.
(2020) and due to.
5752name metricacc.
xnliacc.
pawsxacc.
mldocacc.
marcf1posf1ner.
tasknatural language inferenceparaphrase identiﬁcationnews article classiﬁcationamazon reviewspart-of-speech taggingnamed-entity recognition.
|t |3245177.tsnonoyesyesyesyes.
# of lang.
157862940.table 1: evaluation datasets.
|t |: number of classes(classiﬁcation tasks) and label set size (pos and ner).
ts: availability of a training split in the target language..the abundant labeled data for many nlp tasks, wechoose english as the source in our experiments.
directly evaluating the source-trained model af-ter this stage corresponds to the widely studiedzs-xlt scenario.
the second stage is target-adapting: the source-trained model from previ-ous stage is adapted to a target language using fewshots.
we discuss details of sampling the few shotsin §4.
the development set of the target languageis used for model selection in this stage..4 experimental setup.
we consider three types of tasks requiring vary-ing degrees of semantic and syntactic knowledgetransfer: sequence classiﬁcation (cls), named-entity recognition (ner), and part-of-speech tag-ging (pos) in up to 40 typologically diverse lan-guages (cf., appendix §b)..4.1 datasets and selection of few shots.
for the cls tasks, we sample few shots fromfour multilingual datasets: news article classiﬁ-cation (mldoc; schwenk and li (2018)); ama-zon review classiﬁcation (marc; keung et al.
(2020b)); natural language inference (xnli; con-neau et al.
(2018); williams et al.
(2018)); andcrosslingual paraphrase adversaries from wordscrambling (pawsx; zhang et al.
(2019); yanget al.
(2019)).
we use treebanks in universaldependencies (nivre et al., 2020) for pos, andwikiann dataset (pan et al., 2017; rahimi et al.,2019) for ner.
table 1 reports key informationabout the datasets..we adopt the conventional few-shot samplingstrategy (fei-fei et al., 2006; koch et al., 2015;snell et al., 2017), and conduct “n -way k-shot”sampling from the datasets; n is the number ofclasses and k refers to the number of shots perclass.
a group of n -way k-shot data is referredto as a bucket.
we set n equal to the number oflabels |t |.
following wang et al.
(2020), we sam-ple 40 buckets for each target (i.e., non-english).
language of a task to get a reliable estimation ofmodel performance..cls tasks.
for mldoc and marc, each lan-guage has a train/dev/test split.
we sample thebuckets without replacement from the training setof each target language, so that buckets are dis-joint from each other.
target languages in xnliand pawsx only have dev/test splits.
we sam-ple the buckets from the dev set; the remainingdata serves as a single new dev set for model selec-tion during target-adapting.
for all tasks, we usek ∈ {1, 2, 4, 8}..pos and ner.
for the two structured predic-tion tasks, “n -way k-shot” is not well-deﬁned be-cause each sentence contains one or more labeledtokens.
we use a similar sampling principle as withcls, where n is the size of the label set for eachlanguage and task, but k is set to the minimumnumber of occurrences for each label.
in particu-lar, we utilize the minimum-including algorithm(hou et al., 2020b,a) to satisfy the following criteriawhen sampling a bucket: 1) each label appears atleast k times, and 2) at least one label will appearless than k times if any sentence is removed fromthe bucket.
appendix §c gives sampling details.
in contrast to sampling for cls, we do not enforcesamples from different buckets to be disjoint dueto the small amount of data in some low-resourcelanguages.
we only use k ∈ {1, 2, 4} and ex-clude k = 8, as 8-shot buckets already have lotsof labeled tokens, and thus (arguably) might not beconsidered few-shot..4.2 training setup.
we use the pretrained cased mbert model (devlinet al., 2019), and rely on the pytorch-based (paszkeet al., 2019) huggingface transformers repository(wolf et al., 2019) in all experiments..for source-training, we ﬁnetune the pretrainedencoder for 10 epochs with batch size 32. fortarget-adapting to every target language, the few-shot data is a sampled bucket in this language,and we ﬁnetune on the bucket for 50 epochs withearly-stopping of 10 epochs.
the batch size isset to the number of shots in the bucket.
eachtarget-adapting experiment is repeated 40 times us-ing the 40 buckets.
we use the adam optimizer(kingma and ba, 2015) with default parametersin both stages with learning rates searched over{1e − 5, 3e − 5, 5e − 5, 7e − 5}.
for cls tasks, weuse mbert’s [cls] token as the ﬁnal represen-.
5753in {0 .
.
.
39}.
figure 1 presents the dev set perfor-mance distribution of the 40 runs with 40 randomseeds (top) and 40 1-shot buckets (bottom)..with exactly the same training data, using differ-ent random seeds yields a 1–2 accuracy differenceof fs-xlt (figure 1 top).
a similar phenomenonhas been observed in ﬁnetuning monolingual en-coders (dodge et al., 2020) and multilingual en-coders with zs-xlt (keung et al., 2020a; wu anddredze, 2020b; xia et al., 2020); we show this ob-servation also holds for fs-xlt.
the key takeawayis that varying the buckets is a more severe problem.
it causes much larger variance (figure 1 bottom):the maximum accuracy difference is ≈6 for demarc and ≈10 for es mldoc.
this can be dueto the fact that difﬁculty of individual examplesvaries in a dataset (swayamdipta et al., 2020), re-sulting in different amounts of information encodedin buckets..this large variance could be an issue when com-paring different few-shot learning algorithms.
thebucket choice is a strong confounding factor thatmay obscure the strength of a promising few-shottechnique.
therefore, for fair comparison, it is nec-essary to work with a ﬁxed set of few shots.
wepropose to ﬁx the sampled buckets for unbiasedcomparison of different fs-xlt methods.
we pub-lish the sampled buckets from the six multilingualdatasets as a ﬁxed and standardized few-shot evalu-ation benchmark..in what follows, each fs-xlt experiment is re-peated 40 times using 40 different buckets withthe same ﬁxed random seed; we report mean andstandard deviation.
as noted, the variance due torandom seeds is smaller (cf., figure 1) and hasbeen well studied before (reimers and gurevych,2017; dodge et al., 2020).
in this work, we thus fo-cus our attention and limited computing resourceson understanding the impact of buckets, the newlydetected source of variance.
however, we encour-age practitioners to report results with both factorsconsidered in the future..different numbers of shots.
a comparisonconcerning the number of shots (k), based on thefew-shot results in table 2 and figure 2, reveals thatthe buckets largely improve model performance ona majority of tasks (mldoc, marc, pos, ner)over zero-shot results.
this is in line with priorwork (lauscher et al., 2020; hedderich et al., 2020)and follows the success of work on using boot-strapped data (chaudhary et al., 2019; sherborne.
figure 1: histograms of dev set accuracies.
top: 40runs with different random seeds.
bottom: 40 runs withdifferent 1-shot buckets.
left: de marc.
right: esmldoc.
the variance due to buckets is larger..tation.
for ner and pos, following devlin et al.
(2019), we use a linear classiﬁer layer on top of therepresentation of each tokenized word, which is itslast wordpiece (he and choi, 2020)..we set the maximum sequence length to 128after wordpiece tokenization (wu et al., 2016), inall experiments.
further implementation detailsare shown in our reproducibility checklist in ap-pendix §a..5 results and discussion.
5.1 source-training results.
the zs-xlt performance from english (en) totarget languages of the four cls tasks are shown inthe k = 0 column in table 2. for ner and pos,the results are shown in figure 2..for xtreme tasks (xnli, pawsx, ner,pos), our implementation delivers results compa-rable to hu et al.
(2020).
for mldoc, our resultsare comparable to (dong and de melo, 2019; wuand dredze, 2019; eisenschlos et al., 2019).
it isworth noting that reproducing the exact results ischallenging, as suggested by keung et al.
(2020a).
for marc, our zero-shot results are worse thankeung et al.
(2020b)’s who use the dev set of eachtarget language for model selection while we useen dev, following the common true zs-xlt setup..5.2 target-adapting results.
variance of few-shot transfer.
we hypothesizethat fs-xlt suffers from large variance (dodgeet al., 2020) due to the large model complexityand small amount of data in a bucket.
to test thisempirically, we ﬁrst conduct two experiments onmldoc and marc.
first, for a ﬁxed random seed,we repeat 1-shot target-adapting 40 times using dif-ferent 1-shot buckets in german (de) and spanish(es).
second, for a ﬁxed 1-shot bucket, we repeatthe same experiment 40 times using random seeds.
5754484950515253validation accuracy (%)05countperf.
distribution of german in marc78808284868890validation accuracy (%)010countperf.
distribution of spanish in mldoc4648505254validation accuracy (%)010countperf.
distribution of german in marc78808284868890validation accuracy (%)010countperf.
distribution of spanish in mldock=096.8888.3083.0581.9074.1372.3384.3874.58.
64.5249.6247.3048.4440.4038.84.en.
de.
fr.
es.
it.
ru.
zh.
ja.
en.
de.
fr.
es.
zh.
ja.
es.
fr.
zh.
en.
de.
ru.
ar.
82.6770.3273.5773.7168.7069.3264.9767.5865.6756.57hisw 48.0846.17th60.4057.0569.82.bg.
ur.
tr.
el.
vi.
en.
de.
fr.
es.
zh.
ja.
ko.
93.9083.8086.9088.2577.7573.3072.05.codlm.cram.ilnx.xswap.k=1-90.36 ± 1.4888.94 ± 2.4683.99 ± 2.3574.97 ± 2.0477.40 ± 4.2787.18 ± 1.4576.23 ± 1.59.
-51.50 ± 1.5849.32 ± 1.3449.72 ± 1.2443.19 ± 1.7641.95 ± 2.09.
-70.58 ± 0.3673.41 ± 0.4873.84 ± 0.4068.81 ± 0.5269.73 ± 0.9464.75 ± 0.3668.15 ± 0.6965.64 ± 0.4056.94 ± 0.8250.33 ± 1.0849.43 ± 2.6061.02 ± 0.6857.56 ± 0.8570.04 ± 0.59.
-84.14 ± 0.4087.07 ± 0.2787.90 ± 0.5477.71 ± 0.3773.78 ± 0.7573.75 ± 1.30.k=2-90.77 ± 0.8789.71 ± 1.6885.65 ± 1.6075.29 ± 1.5780.57 ± 1.3787.31 ± 1.5376.71 ± 2.12.
-52.76 ± 0.8749.70 ± 1.4349.96 ± 1.1244.45 ± 1.3643.63 ± 1.30.
-70.60 ± 0.3473.74 ± 0.4673.87 ± 0.4468.76 ± 0.5469.75 ± 0.9464.82 ± 0.2368.19 ± 0.7565.73 ± 0.3657.07 ± 0.8250.28 ± 1.2450.08 ± 2.4261.20 ± 0.6157.83 ± 0.9170.14 ± 0.75.
-84.08 ± 0.4287.06 ± 0.3787.80 ± 0.5677.63 ± 0.4773.71 ± 1.0473.11 ± 1.05.k=4-91.85 ± 0.8390.80 ± 0.8886.30 ± 1.8576.43 ± 1.4181.33 ± 1.3388.33 ± 1.1178.60 ± 2.43.
-52.78 ± 1.0050.64 ± 0.9450.45 ± 1.2245.40 ± 1.2643.98 ± 0.89.
-70.61 ± 0.3973.57 ± 0.4973.74 ± 0.4868.87 ± 0.5570.56 ± 0.7664.82 ± 0.2368.55 ± 0.6765.80 ± 0.4157.21 ± 1.1451.08 ± 0.6251.32 ± 2.0761.35 ± 0.4958.20 ± 0.9370.23 ± 0.63.
-84.04 ± 0.4787.03 ± 0.3187.84 ± 0.5377.68 ± 0.5173.48 ± 0.6973.79 ± 0.92.k=8-91.98 ± 0.8291.01 ± 0.9488.46 ± 1.9078.12 ± 1.2581.91 ± 1.2188.72 ± 1.0581.17 ± 1.72.
-53.32 ± 0.5951.23 ± 0.7651.25 ± 0.9346.40 ± 0.9344.44 ± 0.69.
-70.70 ± 0.5073.77 ± 0.4473.87 ± 0.4668.81 ± 0.7770.62 ± 0.8664.94 ± 0.3768.32 ± 0.7066.00 ± 0.5357.82 ± 1.1851.01 ± 0.7952.16 ± 2.4361.31 ± 0.5658.67 ± 1.0370.41 ± 0.70.
-84.23 ± 0.6686.94 ± 0.4187.85 ± 0.7577.82 ± 0.6473.79 ± 1.2873.31 ± 0.61.table 2: zero-shot (column k = 0) and few-shot(columns k > 0) results (acc.
in %) on the test setfor cls tasks.
green [red]: few-shot transfer outper-forms [underperforms] zero-shot transfer..et al., 2020)..in general, we observe that: 1) 1-shot bucketsbring the largest relative performance improvementover zs-xlt; 2) the gains follow the increase of k,but with diminishing returns; 3) the performancevariance across the 40 buckets decreases as k in-creases.
these observations are more pronouncedfor pos and ner; e.g., 1-shot en to urdu (ur)pos transfer shows gains of ≈22 f1 points (52.40with zero-shot, 74.95 with 1-shot)..for individual runs, we observe that models infs-xlt tend to overﬁt the buckets quickly at smallk values.
for example, in around 32% of ner 1-shot buckets, the model achieves the best dev scoreright after the ﬁrst epoch; continuing the trainingonly degrades performance.
similar observationshold for semantic tasks like marc, where in 10out of 40 de 1-shot buckets, the dev set perfor-mance peaks at epoch 1 (cf.
learning curve in ap-pendix §d figure 6).
this suggests the necessity ofrunning the target-adapting experiments on multi-ple buckets if reliable conclusions are to be drawn.
different downstream tasks.
the models fordifferent tasks present various levels of sensitiv-.
ity to fs-xlt.
among the cls tasks that requiresemantic reasoning, fs-xlt beneﬁts mldoc themost.
this is not surprising given the fact that key-word matching can largely solve mldoc (artetxeet al., 2020a,b): a few examples related to targetlanguage keywords are expected to signiﬁcantlyimprove performance.
fs-xlt also yields promi-nent gains on the amazon review classiﬁcationdataset marc.
similar to mldoc, we hypothe-size that just matching a few important opinion andsentiment words (liu, 2012) in the target languagebrings large gains already.
we provide further qual-itative analyses in §5.4..xnli and pawsx behave differently frommldoc and marc.
xnli requires higher levelsemantic reasoning on pairs of sentences.
fs-xlt performance improves modestly (xnli) oreven decreases (pawsx-es) compared to zs-xlt, even with large k. pawsx requires amodel to distinguish adversarially designed non-paraphrase sentence pairs with large lexical over-lap like “flights from new york to florida” and“flights from florida to new york” (zhang et al.,2019).
this poses a challenge for fs-xlt, giventhe small amount of target language informationin the buckets.
therefore, when buckets are small(e.g., k = 1) and for challenging semantic taskslike pawsx, the buckets do not substantially help.
annotating more shots in the target language is anintuitive solution.
designing task-speciﬁc pretrain-ing/ﬁnetuning objectives could also be promising(klein and nabi, 2020; ram et al., 2021)..unlike cls tasks, pos and ner beneﬁt fromfs-xlt substantially.
we speculate that there aretwo reasons: 1) both tasks often require little to nohigh-level semantic understanding or reasoning; 2)due to i.i.d.
sampling, train/dev/test splits are likelyto have overlapping vocabulary, and the labels inthe buckets can easily propagate to dev and test.
we delve deeper into these conjectures in §5.4..different languages.
for languages that aremore distant from en, e.g., with different scripts,small lexical overlap, or fewer common typologicalfeatures (pires et al., 2019; wu and dredze, 2020a),fs-xlt introduces crucial lexical and structuralinformation to guide the update of embedding andtransformer layers in mbert..we present several ﬁndings based on the nerand pos results for a typologically diverse lan-guage sample.
figure 2 shows that for languageswith non-latin scripts (different from en), despite.
5755figure 2: improvement in f1 (mean and standard deviation) of fs-xlt over zs-xlt (numbers shown on x-axis beneath each language) for ner (top) and pos (bottom) for three different bucket sizes.
see appendix §d(tables 12 and 13) for absolute numerical values..task.
ner.
pos.
factorlexical overlap# of common linguistic featureslexical overlap# of common linguistic features.
s-0.34-0.37-0.63-0.57.p-0.35-0.10-0.50-0.54.table 3: correlations between fs-xlt f1 score gainsand the two factors (lexical overlap and the number ofcommon linguistic features with en) when consideredindependently for pos and ner: s/r denotes spear-man’s/pearson’s ρ. see footnotes 3, 4 for informationon the two factors..their small to non-existent lexical overlap3 and di-verging typological features (see appendix §d ta-bles 9 and 14), the performance boosts are gen-erally larger than those in the same-script targetlanguages: 6.2 vs. 3.0 average gain in ner and11.4 vs. 5.4 in pos for k = 1. this clearly man-ifests the large information discrepancy betweentarget-language buckets and source-language data.
en data is less relevant to these languages, sothey obtain very limited gain from source-training,reﬂected by their low zs-xlt scores.
with asmall amount of target-language knowledge in thebuckets, the performance is improved dramatically,highlighting the effectiveness of fs-xlt..table 3 shows that, besides script form, lexicaloverlap and the number of linguistic features com-.
3we deﬁne lexical overlap as |v |l∩|v |en.
where v denotesvocabulary.
|v |l is computed with the 40 buckets of a targetlanguage l..|v |en.
mon with en4 also contribute directly to fs-xltperformance difference among languages: there isa moderate negative correlation between f1 scoregains vs. the two factors when considered indepen-dently for both syntactic tasks: the fewer over-laps/features a target language shares with en, thelarger the gain fs-xlt achieves..this again stresses the importance of buckets –they contain target-language-speciﬁc knowledgeabout a task that cannot be obtained by zs-xlt,which solely relies on language similarity.
interest-ingly, pearson’s ρ indicates that common linguisticfeatures are much less linearly correlated with fs-xlt gains in ner than in pos..5.3.importance of source-training.
table 4 reports the performance drop when directlycarrying out target-adapting, without any priorsource-training of mbert.
we show the scoresfor mldoc and pawsx as a simple and a chal-lenging cls task, respectively.
for ner and pos,we select two high- (russian (ru), es), mid- (viet-namese (vi), turkish (tr)), and low-resource lan-guages (tamil (ta), marathi (mr)) each.5.
the results clearly indicate that omitting the.
4following pires et al.
(2019), we use six wals features:81a (order of subject, object and verb), 85a (order of ad-position and noun), 86a (order of genitive and noun), 87a(order of adjective and noun), 88a (order of demonstrativeand noun), and 89a (order of numeral and noun)..5the categorization based on resource availability is ac-.
cording to wikisize (wu and dredze, 2020a)..5756nl82.8fr80.4de79.0it80.3pt79.3af78.4et71.9hu71.3es77.2ms68.6sw68.4fi68.4tl69.2tr65.8vi64.7eu55.4jv61.2id60.10510152025303540f1 score improvementssame script = yesel75.2bg78.6ka61.3ko46.5ml46.8mr54.7my42.5hi65.8ta46.1he56.4ru65.2bn64.2te50.0th1.5kk40.3ja7.2ar39.9ur40.8yo35.5fa40.7zh13.9same script = nof1 score improvements1-shot2-shot4-shotnl88.3pt86.5id70.8et79.2it86.0de86.4es86.6fi74.5af86.6tr57.6fr82.5hu75.1vi55.0eu49.505101520253035f1 score improvementsru86.4he76.8te67.5bg87.0el81.9ar66.5ta53.5zh63.0mr58.7hi64.3fa65.7ko42.3ur52.4ja47.6mldoc.
pawsx.
posk=1 k=4.
ner.
es.
k=1.
k=8.
k=8-7.67.k=1de -37.73fr -38.14 -13.21 -33.02 -32.34.k=1k=4-31.11 -30.82 ru -15.89 -3.20 -48.19 -35.77-0.93 -63.98 -41.53-9.51es-0.36 -54.41 -41.45-7.82-33.69 -14.38 -33.76 -33.97 vitr -15.05 -8.08 -54.35 -34.52-33.63 -12.62itru -30.66 -11.08ta -13.72 -4.40 -34.70 -24.81zh -37.31 -12.57 -23.74 -23.65 mr -11.34 -3.63 -40.10 -25.68ja -29.82 -14.32 -20.97 -20.82-19.83 -19.68.
--.
--.
--.
--.
--.
--.
--.
ko.
-.
-.
table 4: performance drop when conducting target-adapting without source-training..figure 4: improvement of word-label predictions intro-duced by a bucket (x-axis) in fa (top), ur (mid), andhi (bottom), in relation to the words’ presence in thebucket (true or false)..figure 3: normalized (with softmax) jaccard index(%) of a bucket (row) and the improved predictionsachieved with 10 buckets (column)..source-training stage yields large performancedrops.
even larger variance is also observed inthis scenario (cf.
appendix §d table 11).
there-fore, the model indeed learns, when trained on thesource language, some transferable crosslingualfeatures that are beneﬁcial to target languages, bothfor semantic and syntactic tasks..5.4.importance of lexical features.
we now investigate the sources of gains brought byfs-xlt over zs-xlt..for syntactic tasks, we take persian (fa) pos asan example.
figure 3 visualizes the lexical overlap,measured by the jaccard index, of 10 1-shot buck-ets (rows) and the improved word-label predictionsintroduced by target-adapting on each of the buck-ets (columns).
in more detail, for column c, wecollect the set (denoted as cc) of all test set wordswhose label is incorrectly predicted by the zero-shot model, but correctly predicted by the modeltrained on the c-th bucket.
for row i, we denotewith bi the set of words occurring in bucket i. theﬁgure shows in cell (i, k) the jaccard index of biand ck.
the bright color (i.e., higher lexical over-lap) on the diagonal reﬂects that the improvements.
figure 5: marc (5 classes) test set prediction confu-sion matrices.
top: de.
bottom: zh.
left: zero-shotmodels.
right: 1-shot models.
colorbar numbers rep-resent the number of instances in that cell..introduced by a bucket are mainly6 those word-label predictions that are lexically more similar tothe bucket than to other buckets..we also investigate the question: how manyword-label predictions that are improved after fs-xlt occur in the bucket, i.e., in the training data?
figure 4 plots this for the 40 1-shot buckets in fa,ur, and hindi (hi).
we see that many test wordsdo occur in the bucket (shown in orange), in linewith recent ﬁndings (lewis et al., 2021; elangovanet al., 2021).
these analyses shed light on whythe buckets beneﬁt ner/pos – which heavily relyon lexical information – more than higher levelsemantic tasks..for the cls task marc, which requires un-.
6note that the sampled buckets for pos are not completely.
disjoint (cf.
sampling strategy in §4)..5757507251341322232507251341322232jaccard index of buckets and improved predictions (fa)9.9810.0010.0210.0410.0610.080k2kcountintersectedfalsetrue0k2kcountintersectedfalsetruebucket index0k10kcountintersectedfalsetrue12345123450-shot predictions de123451-shot predictions de10020030040050060010020030040050060012345123450-shot predictions zh123451-shot predictions zh100200300400500100200300400500token∆attn.
[sep]+4.13.
.
+2.91.
nicht+1.84.
!
-1.75.die-0.92.sehr-0.81.table 5: tokens with the highest attention change from[cls], comparing zero-shot with a 1-shot de bucket..derstanding product reviews, figure 5 visualizesthe confusion matrices of test set predictions forde and chinese (zh) zero- and 1-shot models;axis ticks are review scores in {1, 2, 3, 4, 5}.
thesquares on the diagonals in the two left heatmapsshow that parameter initialization on en is a goodbasis for well-performing zs-xlt: this is particu-larly true for de, which is linguistically closer toen.
two extreme review scores – 1 (for de) and5 (for zh) – have the largest confusions.
the tworight heatmaps show that improvements broughtby the 1-shot buckets are mainly achieved by cor-rectly predicting more cases of the two extremereview scores: 2 → 1 (de) and 4 → 5 (zh).
butthe more challenging cases (reviews with scores 2,3, 4), which require non-trivial reasoning, are notsigniﬁcantly improved, or even become worse..we inspect examples that are incorrectly pre-dicted by the few-shot model (predicting 1), but arecorrectly predicted by the zero-shot model (predict-ing 2).
speciﬁcally, we compute the difference ofwhere [cls] attends to, before and after adaptingthe model on a 1-shot de bucket.
we extract andaverage attentions computed by the 12 heads fromthe topmost transformer layer..table 5 shows that “nicht” (“not”) draws highattention change from [cls].
“nicht” (i.e., nega-tion) by itself is not a reliable indicator of senti-ment, so giving the lowest score to reviews solelybecause they contain “nicht” is not a good strategy.
the following review is classiﬁed as 1 by the 1-shotmodel, but 2 is the gold label (as the review is notentirely negative):.
“die uhr ging nicht einmal eine minute ... op-tisch allerdings sehr sch¨on.” (“the clock didn’t evenwork one minute ... visually, however, very nice.”).
pretrained multilingual encoders are shown tolearn and store “language-agnostic” features (pireset al., 2019; zhao et al., 2020); §5.3 shows thatsource-training mbert on en substantially ben-eﬁts other languages, even for difﬁcult semantictasks like pawsx.
conditioning on such language-agnostic features, we expect that the buckets shouldlead to good understanding and reasoning capabili-ties for a target language.
however, plain few-shotﬁnetuning still relies heavily on unintended shallow.
lexical cues and shortcuts (niven and kao, 2019;geirhos et al., 2020) that generalize poorly.
otheropen research questions for future work arise: howdo we overcome this excessive reliance on lexicalfeatures?
how can we leverage language-agnosticfeatures with few shots?
our standardized buckets,baseline results, and analyses are the initial step to-wards researching and answering these questions..5.5 target-adapting methods.
sota few-shot learning methods (chen et al., 2019;wang et al., 2020; tian et al., 2020; dhillon et al.,2020) from computer vision consist of two stages:1) training on base-class images, and 2) few-shotﬁnetuning using new-class images.
source-trainingand target-adapting stages of fs-xlt, albeit amonglanguages, follow an approach very similar to thesemethods.
therefore, we test their effectivenessfor crosslingual transfer.
these methods are builtupon cosine similarity that imparts inductive biasabout distance and is more effective than a fully-connected classiﬁer layer (fc) with small k (wanget al., 2020).
following (chen et al., 2019; wanget al., 2020; tian et al., 2020), we freeze the em-bedding and transformer layers of mbert, andexplore four variants of the target-adapting stageusing marc..cos+pooler.
we randomly initialize a train-able weight matrix w ∈ rh×c where h is the hid-den dimension size and c is the number of classes.
rewriting w as [w1, .
.
.
, wi, .
.
.
, wc], we com-pute the logits of an input sentence representationx ∈ rh (from mbert) belonging to class i as.
α ·.
x(cid:124)wi(cid:107)x(cid:107)2 · (cid:107)wi(cid:107)2.,.
where α is a scaling hyperparameter, set to 10 inall experiments.
during training, w and mbert’spooler layer containing a linear layer and a tanhnon-linearity are updated..fc+pooler.
during training, we update the lin-.
ear classiﬁer layer and mbert’s pooler layer..fc only.
during training, we only update thelinear classiﬁer layer.
this variant largely reducesmodel complexity and exhibit lower variance whenk is small..fc(reset)+pooler.
similar to fc+pooler, butthe source-trained linear classiﬁer layer is randomlyre-initialized before training..table 6 shows the performance of these methodsalong with full model ﬁnetuning (without freez-ing).
fc+pooler performs the best among the.
5758k=049.6247.3048.4440.4038.84.de.
fr.
es.
zh.
ja.
full-model finetuningk=8k=153.32 ± 0.5951.50 ± 1.5851.23 ± 0.7649.32 ± 1.3451.25 ± 0.9349.72 ± 1.2446.40 ± 0.9343.19 ± 1.7644.44 ± 0.6941.95 ± 2.09.fc only.
fc + pooler.
cos + pooler.
k=150.82 ± 1.1748.19 ± 0.7849.03 ± 0.7341.90 ± 1.1540.76 ± 1.76.k=852.58 ± 0.6349.05 ± 0.9349.69 ± 0.5743.34 ± 0.8843.14 ± 0.76.k=151.18 ± 1.1348.60 ± 1.0249.28 ± 0.8542.30 ± 1.3741.40 ± 1.74.k=853.17 ± 0.5849.97 ± 0.7750.21 ± 0.6344.42 ± 0.6543.81 ± 0.56.k=137.98 ± 5.5339.93 ± 3.5040.01 ± 4.3333.10 ± 5.4834.36 ± 4.19.k=845.85 ± 2.1444.41 ± 1.9545.35 ± 2.3738.31 ± 1.8738.95 ± 1.80.fc (reset) + poolerk=8k=149.46 ± 2.2138.52 ± 6.6447.77 ± 2.0040.12 ± 5.0447.73 ± 2.3340.89 ± 4.9642.07 ± 2.1931.83 ± 7.0041.18 ± 1.6832.80 ± 5.17.table 6: accuracy (%) on marc when varying classiﬁer head conﬁgurations.
full-model finetuning updatesall parameters during training; the other four methods only update a subset as described in §5.5.
the best results(excluding full-model finetuning) are in bold..four for both k = 1 and k = 8 in all lan-guages.
however, it underperforms the full modelﬁnetuning, especially when k = 8. fc only issub-optimal; yet the decrease in comparison tofc+pooler is small, highlighting that en-trainedmbert is a strong feature extractor.
cos+poolerand fc(reset)+pooler perform considerably worsethan the other two methods and zero-shot transfer –presumably because their new parameters need tobe trained from scratch with few shots..we leave further exploration of other possibil-ities of exploiting crosslingual features throughcollapse-preventing regularization (aghajanyanet al., 2021) or contrastive learning (gunel et al.,integrating prompting2021) to future work.
(brown et al., 2020; schick and sch¨utze, 2020;gao et al., 2020; liu et al., 2021) – a strong per-forming few-shot learning methodology for nlp– into the crosslingual transfer learning pipeline isalso a promising direction..6 conclusion and future work.
we have presented an extensive study of few-shotcrosslingual transfer.
the focus of the study hasbeen on an empirically detected performance vari-ance in few-shot scenarios: the models exhibit ahigh level of sensitivity to the choice of few shots.
we analyzed and discussed the major causes ofthis variance across six diverse tasks for up to 40languages.
our results show that large languagemodels tend to overﬁt to few shots quickly andmostly rely on shallow lexical features presentin the few shots, though they have been trainedwith abundant data in english.
moreover, we haveempirically validated that state-of-the-art few-shotlearning methods in computer vision do not outper-form a conceptually simple alternative: full modelﬁnetuning..our study calls for more rigor and accurate re-porting of the results of few-shot crosslingual trans-fer experiments.
they should include score distri-butions over standardized and ﬁxed few shots.
to.
aid this goal, we have created and provided suchﬁxed few shots as a standardized benchmark for sixmultilingual datasets..few-shot learning is promising for crosslingualtransfer, because it mirrors how people acquire newlanguages, and that the few-shot data annotation isfeasible.
in future work, we will investigate moresophisticated techniques and extend the work tomore nlp tasks..acknowledgments.
this work was funded by the european researchcouncil: erc nonsequetor (#740516) and erclexical (#648909).
we thank the anonymousreviewers and fei mi for their helpful suggestions..references.
armen aghajanyan, akshat shrivastava, anchit gupta,naman goyal, luke zettlemoyer, and sonal gupta.
2021. better ﬁne-tuning by reducing representa-in international conference ontional collapse.
learning representations..mikel artetxe, sebastian ruder, and dani yogatama.
2020a.
on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..mikel artetxe, sebastian ruder, dani yogatama,gorka labaka, and eneko agirre.
2020b.
a callfor more rigor in unsupervised cross-lingual learn-in proceedings of the 58th annual meetinging.
of the association for computational linguistics,pages 7375–7388, online.
association for compu-tational linguistics..mikel artetxe and holger schwenk.
2019. mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transac-tions of the association for computational linguis-tics, 7:597–610..trapit bansal, rishikesh jha, and andrew mccallum.
2020. learning to few-shot learn across diverse.
5759in proceed-natural language classiﬁcation tasks.
ings of the 28th international conference on com-putational linguistics, pages 5108–5123, barcelona,spain (online).
international committee on compu-tational linguistics..tom brown, benjamin mann, nick ryder, melaniesubbiah,jared d kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish,alec radford, ilya sutskever, and dario amodei.
2020. language models are few-shot learners.
inadvances in neural information processing systems,volume 33, pages 1877–1901.
curran associates,inc..aditi chaudhary, jiateng xie, zaid sheikh, grahamneubig, and jaime carbonell.
2019. a little anno-tation does a lot of good: a study in bootstrappinglow-resource named entity recognizers.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5164–5174, hongkong, china.
association for computational lin-guistics..wei-yu chen, yen-cheng liu, zsolt kira, yu-chiang frank wang, and jia-bin huang.
2019. ain interna-closer look at few-shot classiﬁcation.
tional conference on learning representations..yinbo chen, xiaolong wang, zhuang liu, huijuana new meta-arxiv preprint.
xu, and trevor darrell.
2020.baseline for few-shotlearning.
arxiv:2003.04390..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau and guillaume lample.
2019. cross-in advanceslingual language model pretraining.
in neural information processing systems, pages7059–7069..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel r. bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluating cross-lingual sentence representations.
in proceedings ofthe 2018 conference on empirical methods in natu-ral language processing.
association for computa-tional linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..guneet singh dhillon, pratik chaudhari, avinashravichandran, and stefano soatto.
2020. a baselinein internationalfor few-shot image classiﬁcation.
conference on learning representations..jesse dodge, gabriel ilharco, roy schwartz, alifarhadi, hannaneh hajishirzi, and noah smith.
2020.fine-tuning pretrained language models:weight initializations, data orders, and early stop-ping..xin dong and gerard de melo.
2019. a robust self-learning framework for cross-lingual text classiﬁca-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages6306–6310, hong kong, china.
association forcomputational linguistics..rotem dror, segev shlomov, and roi reichart.
2019.deep dominance - how to properly compare deepneural models.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2773–2785, florence, italy.
associa-tion for computational linguistics..julian eisenschlos, sebastian ruder, piotr czapla,and jeremymarcin kadras, sylvain gugger,howard.
2019. multifit: efﬁcient multi-lingual lan-in proceedings of theguage model ﬁne-tuning.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5702–5707, hong kong,china.
association for computational linguistics..aparna elangovan, jiayuan he, and karin verspoor.
2021. memorization vs. generalization : quantify-ing data leakage in nlp performance evaluation.
inproceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 1325–1335, online.
association for computational linguistics..l. fei-fei, r. fergus, and p. perona.
2006. one-ieee transac-shot learning of object categories.
tions on pattern analysis and machine intelligence,28(4):594–611..chelsea finn, pieter abbeel, and sergey levine.
2017.model-agnostic meta-learning for fast adaptation ofin proceedings of the 34th in-deep networks.
ternational conference on machine learning, vol-ume 70 of proceedings of machine learning re-.
5760search, pages 1126–1135, international conventioncentre, sydney, australia.
pmlr..processing (emnlp), pages 2580–2591, online.
as-sociation for computational linguistics..tianyu gao, adam fisch, and danqi chen.
2020.making pre-trained language models better few-shotlearners.
arxiv preprint arxiv:2012.15723..tianyu gao, xu han, hao zhu, zhiyuan liu, pengli, maosong sun, and jie zhou.
2019. fewrel 2.0:towards more challenging few-shot relation classiﬁ-in proceedings of the 2019 conference oncation.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages6251–6256, hong kong, china.
association forcomputational linguistics..dan garrette and jason baldridge.
2013. learning apart-of-speech tagger from two hours of annotation.
in proceedings of the 2013 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 138–147, atlanta, georgia.
association forcomputational linguistics..robert geirhos,.
j¨orn-henrik jacobsen, claudiomichaelis, richard zemel, wieland brendel,matthias bethge, and felix a. wichmann.
2020.shortcut learning in deep neural networks.
naturemachine intelligence, 2(11):665–673..beliz gunel, jingfei du, alexis conneau, and veselinstoyanov.
2021. supervised contrastive learning forpre-trained language model ﬁne-tuning.
in interna-tional conference on learning representations..aakriti gupta, kapil thadani, and neil o’hare.
2020.effective few-shot classiﬁcation with transfer learn-ing.
in proceedings of the 28th international con-ference on computational linguistics, pages 1061–1066, barcelona, spain (online).
international com-mittee on computational linguistics..xu han, hao zhu, pengfei yu, ziyun wang, yuanyao, zhiyuan liu, and maosong sun.
2018. fewrel:a large-scale supervised few-shot relation classiﬁca-tion dataset with state-of-the-art evaluation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4803–4809, brussels, belgium.
association for computa-tional linguistics..nithin holla, pushkar mishra, helen yannakoudakis,and ekaterina shutova.
2020. learning to learnto disambiguate: meta-learning for few-shot wordin findings of the associa-sense disambiguation.
tion for computational linguistics: emnlp 2020,pages 4517–4533, online.
association for compu-tational linguistics..yutai hou, wanxiang che, yongkui lai, zhihan zhou,yijia liu, han liu, and ting liu.
2020a.
few-shotslot tagging with collapsed dependency transfer andlabel-enhanced task-adaptive projection network.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 1381–1393.
associa-tion for computational linguistics..yutai hou, jiafeng mao, yongkui lai, cheng chen,wanxiang che, zhigang chen, and ting liu.
2020b.
fewjoint: a few-shot learning benchmark for jointlanguage understanding.
corr, abs/2009.08138..transfer learning with multi-lingual.
tsung-yuan hsu, chi-liang liu, and hung-yi lee.
2019. zero-shot reading comprehension by cross-linguallan-guage representation model.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5933–5940, hong kong,china.
association for computational linguistics..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in proceedings of the 37th internationalconference on machine learning, volume 119 ofproceedings of machine learning research, pages4411–4421, virtual.
pmlr..phillip keung, yichao lu, julian salazar, and vikasbhardwaj.
2020a.
don’t use english dev: on thezero-shot cross-lingual evaluation of contextual em-beddings.
in proceedings of the 2020 conference onempirical methods in natural language processing(emnlp), pages 549–554, online.
association forcomputational linguistics..han he and jinho d. choi.
2020. establishing strongbaselines for the new decade: sequence tagging,syntactic and semantic parsing with bert.
inproceedings of the 33rd international florida ar-tiﬁcial intelligence research society conference,flairs’20.
best paper candidate..phillip keung, yichao lu, gy¨orgy szarvas, andnoah a. smith.
2020b.
the multilingual amazonin proceedings of the 2020 con-reviews corpus.
ference on empirical methods in natural languageprocessing (emnlp), pages 4563–4568, online.
as-sociation for computational linguistics..michael a. hedderich, david adelani, dawei zhu, je-sujoba alabi, udia markus, and dietrich klakow.
2020. transfer learning and distant supervisionfor multilingual transformer models: a study onafrican languages.
in proceedings of the 2020 con-ference on empirical methods in natural language.
diederik p. kingma and jimmy ba.
2015. adam:in iclr.
a method for stochastic optimization.
(poster)..tassilo klein and moin nabi.
2020. contrastive self-supervised learning for commonsense reasoning.
in.
5761proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7517–7523, online.
association for computational lin-guistics..gregory koch, richard zemel, and ruslan salakhut-dinov.
2015. siamese neural networks for one-shotin icml 2015 deep learningimage recognition.
workshop..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020. from zero to hero: on thelimitations of zero-shot language transfer with mul-tilingual transformers.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 4483–4499, on-line.
association for computational linguistics..patrick lewis, pontus stenetorp, and sebastian riedel.
2021. question and answer test-train overlap inin pro-open-domain question answering datasets.
ceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 1000–1008, online.
association for computational linguistics..yaobo liang, nan duan, yeyun gong, ning wu, fen-fei guo, weizhen qi, ming gong, linjun shou,daxin jiang, guihong cao, xiaodong fan, ruofeizhang, rahul agrawal, edward cui, sining wei,taroon bharti, ying qiao, jiun-hung chen, winniewu, shuguang liu, fan yang, daniel campos, ran-gan majumder, and ming zhou.
2020. xglue: anew benchmark datasetfor cross-lingual pre-training,understanding and generation.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 6008–6018,online.
association for computational linguistics..bing liu.
2012. sentiment analysis and opinion min-ing.
synthesis lectures on human language technolo-gies, 5(1):1–167..xiao liu, yanan zheng, zhengxiao du, ming ding,yujie qian, zhilin yang, and jie tang.
2021. gptunderstands, too.
arxiv preprint arxiv:2103.10385..erik g miller, nicholas e matsakis, and paul a viola.
2000. learning from one example through shareddensities on transforms.
in proceedings ieee con-ference on computer vision and pattern recogni-tion.
cvpr 2000 (cat.
no.
pr00662), volume 1,pages 464–471.
ieee..marius mosbach, maksym andriushchenko, and diet-rich klakow.
2021. on the stability of ﬁne-tuning{bert}: misconceptions, explanations, and strongbaselines.
in international conference on learningrepresentations..joakim nivre, marie-catherine de marneffe, filip gin-ter, jan hajiˇc, christopher d. manning, sampopyysalo, sebastian schuster, francis tyers, anddaniel zeman.
2020. universal dependencies v2:an evergrowing multilingualtreebank collection.
in proceedings of the 12th language resourcesand evaluation conference, pages 4034–4043, mar-seille, france.
european language resources asso-ciation..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019. pytorch:an imperative style, high-performance deep learn-ing library.
in advances in neural information pro-cessing systems, volume 32, pages 8026–8037.
cur-ran associates, inc..f. pedregosa, g. varoquaux, a. gramfort, v. michel,b. thirion, o. grisel, m. blondel, p. prettenhofer,r. weiss, v. dubourg, j. vanderplas, a. passos,d. cournapeau, m. brucher, m. perrot, and e. duch-scikit-learn: machine learning inesnay.
2011.journal of machine learning research,python.
12:2825–2830..jason phang, thibault f´evry, and samuel r bowman.
2018. sentence encoders on stilts: supplementarytraining on intermediate labeled-data tasks.
arxivpreprint arxiv:1811.01088..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..yada pruksachatkun,.
jason phang, haokun liu,phu mon htut, xiaoyi zhang, richard yuanzhepang, clara vania, katharina kann, and samuel r.bowman.
2020. intermediate-task transfer learningwith pretrained language models: when and whyin proceedings of the 58th annualdoes it work?
meeting of the association for computational lin-guistics, pages 5231–5247, online.
association forcomputational linguistics..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language ar-guments.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 4658–4664, florence, italy.
associationfor computational linguistics..afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..5762ori ram, yuval kirstain, jonathan berant, amirgloberson, and omer levy.
2021. few-shot ques-tion answering by pretraining span selection.
arxivpreprint arxiv:2101.00438..nils reimers and iryna gurevych.
2017. reportingscore distributions makes a difference: performancestudy of lstm-networks for sequence tagging.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages338–348, copenhagen, denmark.
association forcomputational linguistics..annette rios, mathias m¨uller, and rico sennrich.
2020. subword segmentation and a single bridgelanguage affect zero-shot neural machine translation.
in proceedings of the fifth conference on machinetranslation, pages 526–535, online.
association forcomputational linguistics..timo schick and hinrich sch¨utze.
2020..it’snot just size that matters: small language mod-arxiv preprintels are also few-shotarxiv:2009.07118..learners..holger schwenk and xian li.
2018. a corpus for mul-tilingual document classiﬁcation in eight languages.
in proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), paris, france.
european language resourcesassociation (elra)..tom sherborne, yumo xu, and mirella lapata.
2020.bootstrapping a crosslingual semantic parser.
infindings of the association for computational lin-guistics: emnlp 2020, pages 499–517, online.
as-sociation for computational linguistics..jake snell, kevin swersky, and richard zemel.
2017.prototypical networks for few-shot learning.
in ad-vances in neural information processing systems,volume 30, pages 4077–4087.
curran associates,inc..swabha swayamdipta, roy schwartz, nicholas lourie,yizhong wang, hannaneh hajishirzi, noah a.smith, and yejin choi.
2020. dataset cartography:mapping and diagnosing datasets with training dy-namics.
in proceedings of the 2020 conference onempirical methods in natural language process-ing (emnlp), pages 9275–9293, online.
associa-tion for computational linguistics..yonglong tian, yue wang, dilip krishnan, joshua b.tenenbaum, and phillip isola.
2020. rethinkingfew-shot image classiﬁcation: a good embedding isin computer vision – eccv 2020,all you need?
pages 266–282, cham.
springer international pub-lishing..xin wang, thomas huang, joseph gonzalez, trevordarrell, and fisher yu.
2020. frustratingly simplein proceedings of thefew-shot object detection.
37th international conference on machine learning,volume 119 of proceedings of machine learning re-search, pages 9919–9928, virtual.
pmlr..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r’emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
arxiv, abs/1910.03771..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..shijie wu and mark dredze.
2020a.
are all languagescreated equal in multilingual bert?
in proceedingsof the 5th workshop on representation learning fornlp, repl4nlp@acl 2020, online, july 9, 2020,pages 120–130.
association for computational lin-guistics..shijie wu and mark dredze.
2020b.
do explicit align-ments robustly improve multilingual encoders?
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4471–4482, online.
association for computa-tional linguistics..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, łukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation..orith toledo-ronen, matan orbach, yonatan bilu,artem spector, and noam slonim.
2020. multilin-ingual argument mining: datasets and analysis.
findings of the association for computational lin-guistics: emnlp 2020, pages 303–317, online.
as-sociation for computational linguistics..patrick xia, shijie wu, and benjamin van durme.
2020. which *bert?
a survey organizing contex-tualized encoders.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 7516–7533, online.
as-sociation for computational linguistics..5763yinfei yang, yuan zhang, chris tar, and jasonpaws-x: a cross-lingual ad-baldridge.
2019.versarial dataset for paraphrase identiﬁcation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3687–3692, hong kong, china.
association for computa-tional linguistics..wenpeng yin.
2020. meta-learning for few-shot natu-.
ral language processing: a survey..wenpeng yin, nazneen fatema rajani, dragomirradev, richard socher, and caiming xiong.
2020.universal natural language processing with limitedannotations: try few-shot textual entailment as ain proceedings of the 2020 conference onstart.
empirical methods in natural language process-ing (emnlp), pages 8229–8239, online.
associa-tion for computational linguistics..dani yogatama, cyprien de masson d’autume, jeromeconnor, tomas kocisky, mike chrzanowski, ling-peng kong, angeliki lazaridou, wang ling, lei yu,chris dyer, and phil blunsom.
2019. learning andevaluating general linguistic intelligence..mo yu, xiaoxiao guo, jinfeng yi, shiyu chang, salonipotdar, yu cheng, gerald tesauro, haoyu wang,and bowen zhou.
2018. diverse few-shot text clas-siﬁcation with multiple metrics.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 1206–1215, new orleans, louisiana.
association for computational linguistics..tianyi zhang, felix wu, arzoo katiyar, kilian qweinberger, and yoav artzi.
2021. revisiting few-sample {bert} ﬁne-tuning.
in international confer-ence on learning representations..yuan zhang, jason baldridge, and luheng he.
2019.paws: paraphrase adversaries from word scram-in proceedings of the 2019 conference ofbling.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages1298–1308, minneapolis, minnesota.
associationfor computational linguistics..wei zhao, steffen eger, johannes bjerva, and is-inducing language-arxiv.
abelle augenstein.
2020.agnostic multilingualpreprint arxiv:2008.09112..representations..5764a reproducibility checklist.
a.1 mbert architecture and number of.
parameters.
we use the “bert-base-multilingual-cased” model7.
it contains 12 transformer blocks with 768 hiddendimensions.
each block has 12 self attention heads.
the model is pretrained on the concatenation of thewikipedia dump of 104 languages..there are about 179 million parameters inmbert.
for all the tasks, we use a linear outputlayer.
denoting the output dimension of a task asm, e.g., m = 2 for pawsx.
then we have in total179 million + 768×m + m parameters for the task..a.2 computing infrastructure.
all experiments are conducted on geforce gtx1080ti.
in the source-training stage, we use 4gpus with per-gpu batch size 32. in the target-adapting stage, we use a single gpu and the batchsize is equal to the number of examples in a bucket..a.3 evaluation metrics and validation.
performance.
we follow the standard evaluation metricsused in xtreme (hu et al., 2020) andthey are shown in table 1; evaluation func-tions in scikit-learn (pedregosa et al.,2011) and seqeval (https://github.com/chakki-works/seqeval) are used.
link to code:code/utils/eval meters.py..the validation performance of the english-trained models are shown in the ﬁrst row of table 7;the optimal learning rate for each task is shown inthe second row..algorithm 1: minimum-including.
require: # of shot k, language data d, label set ld1: initialize a bucket s = {}, count(cid:96)j = 0 (∀(cid:96)j ∈ ld)2: for (cid:96) in ld do.
while count(cid:96) < k do.
from d, randomly sample a.
(x(i), y(i)) pair that y(i) includes (cid:96).
add (x(i), y(i)) to supdate all count(cid:96)j (∀(cid:96)j ∈ ld).
3: for each (x(i), y(i)) in s do.
remove (x(i), y(i)) from supdate all count(cid:96)j (∀(cid:96)j ∈ ld)if any count(cid:96)j < k then.
put (x(i), y(i)) back to supdate all count(cid:96)j (∀(cid:96)j ∈ ld).
4: return s.the sensitivity of our results to hyperparameter se-lection..a.5 datasets and preprocessing.
for tasks (xnli, pawsx, pos, ner) coveredin xtreme (hu et al., 2020), we utilize theprovided preprocessed datasets.
our mldocdataset is obtained from https://github.com/facebookresearch/mldoc.
we retrieve marcfrom docs.opendata.aws/amazon-reviews-ml/readme.html.
table 8 shows example entries ofthe datasets.
it is worth noting that marc is asingle sentence review classiﬁcation task, however,we put the “review title” and “product category” inthe “text b” ﬁeld, following keung et al.
(2020b).
we utilize the tokenizer in the huggingfacetransformers package (wolf et al., 2019) topreprocess all the texts.
in all experiments, we use128 maximum sequence length and truncate fromthe end of a sentence if its length exceeds the limit..mldoc marc.
98.11e-5.
65.11e-5.
xnli83.53e-5.
pawsx94.51e-5.
pos95.61e-5.
ner84.31e-5.
b languages.
table 7: source-training validation performance (%)and the optimal learning rate..for all the fs-xlt experiments, we enclosed thevalidation scores in https://github.com/fsxlt/running-logs..a.4 hyperparameter search.
for both source-training and target-adapting, theonly hyperparameter we search is learning rate(from {1e − 5, 3e − 5, 5e − 5, 7e − 5}) to reduce.
7https://github.com/google-research/.
bert/blob/master/multilingual.md.
we work on 40 languages in total.
they are shownin table 9, together with their iso 639-1 codes,writing script, and language features from wals(https://wals.info/) used in our experiments..c minimum-including algorithm.
we utilize the minimum-including algorithm fromhou et al.
(2020a,b) for sampling the buckets ofpos and ner which have several labels in a sen-tence.
denoting as x a sentence that consists of anarray of words (x1, .
.
.
, xn), and the array y thatconsists of a series of labels (y1, .
.
.
, yn).
we sam-ple the buckets by using algorithm 1. note that we.
5765marc.
xnli.
pawsx.
posner.
text a tr`es mignons et de bonne qualit´e.
la ﬁgurine est assez imposante mais conforme `a la taille indiqu´ee dans le descriptif.
text b jolis d´etails .
hometext a ich musste anfagen seminare zu belegen .
text b ich brauchte keine vorbereitung .
text a lo entren´o john vel´azquez y en sus carreras m´as importantes lo mont´o el jinete dale romans.
text b lo entren´o john vel´azquez, y el jinete dale romans lo mont´o en las carreras m´as importantes.
text a (lo,pron), (sanno,verb), (oramai,adv), (quasi,adv), (tutti,pron), (che,sconj), (un,det), (respiro,noun), (affannoso,adj) ...text a (sempat,o), (pindah,o), (ke,o), (hjk,b-org), (dan,o), (1899,b-org), (hoffenheim,i-org), (yang,o), (meminjamkannya,o), (ke,o) ....table 8: example entries of the datasets.
we convert the raw text to the mbert format “text a” and “text b”(devlin et al., 2019).
for pos and ner, we list (word, tag) pairs in the sentence.
following schwenk and li(2018), we provide document indices of mldoc for retrieving the documents from rcv1 and rcv2..81a.
85a.
86a.
87a.
88a.
89a.
order of subject, object and verb order of adposition and noun order of genitive and noun order of adjective and noun order of demonstrative and noun order of numeral and noun.
language.
writing script.
english (en)afrikaans (af)arabic (ar)bulgarian (bg)bengali (bn)german (de)greek (el)spanish (es)estonian (et)basque (eu)persian (fa)finnish (fi)french (fr)hebrew (he)hindi (hi)hungarian (hu)indonesian (id)italian (it)japanese (ja)javanese (jv)georgian (ka)kazakh (kk)korean (ko)malayalam (ml)marathi (mr)malay (ms)burmese (my)dutch (nl)portuguese (pt)russian (ru)swahili (sw)tamil (ta)telugu (te)thai (th)tagalog (tl)turkish (tr)urdu (ur)vietnamese (vi)yoruba (yo)chinese (zh) chinese ideograms.
latinlatinarabiccyrillicbrahmiclatingreeklatinlatinlatinperso-arabiclatinlatinhebrewdevanagarilatinlatinlatinideogramslatingeorgiancyrillichangulbrahmicdevanagarilatinbrahmiclatinlatincyrilliclatinbrahmicbrahmicbrahmiclatinlatinperso-arabiclatinlatin.
svo-vsosvosovno dominant orderno dominant ordersvosvosovsovsvosvosvosovno dominant ordersvosvosov-sov-sovsovsov-sovno dominant ordersvosvosvosovsovsvovsosovsovsvosvosvo.
prepositions-prepositionsprepositions-prepositionsprepositionsprepositionspostpositionspostpositionsprepositionspostpositionsprepositionsprepositionspostpositionspostpositionsprepositionsprepositionspostpositions-postpositions-postpositions-postpositions-postpositionsprepositionsprepositionsprepositionsprepositionspostpositionspostpositionsprepositions-postpositionspostpositionsprepositionsprepositionsno dominant order.
no dominant order-noun-genetiveno dominant order-noun-genetivenoun-genetivenoun-genetivegenetive-noungenetive-nounnoun-genetivegenetive-nounnoun-genetivenoun-genetivegenetive-noungenetive-nounnoun-genetivenoun-genetivegenetive-noun-genetive-noun-genetive-noungenetive-noungenetive-noun-genetive-nounnoun-genetivenoun-genetivenoun-genetivenoun-genetivegenetive-noungenetive-nounnoun-genetivenoun-genetivegenetive-noungenetive-nounnoun-genetivenoun-genetivegenetive-noun.
adjective-noun-noun-adjectiveadjective-noun-adjective-nounadjective-nounnoun-adjectiveadjective-nounnoun-adjectivenoun-adjectiveadjective-nounnoun-adjectivenoun-adjectiveadjective-nounadjective-nounnoun-adjectivenoun-adjectiveadjective-noun-adjective-noun-adjective-nounadjective-nounadjective-noun-noun-adjectiveadjective-nounnoun-adjectiveadjective-nounnoun-adjectiveadjective-nounadjective-nounnoun-adjectiveno dominant orderadjective-nounadjective-nounnoun-adjectivenoun-adjectiveadjective-noun.
demonstrative-noun-demonstrative-noundemonstrative-noun-demonstrative-noundemonstrative-noundemonstrative-noundemonstrative-nounnoun-demonstrativedemonstrative-noundemonstrative-noundemonstrative-nounnoun-demonstrativedemonstrative-noundemonstrative-nounnoun-demonstrativedemonstrative-noundemonstrative-noun-demonstrative-noun-demonstrative-noundemonstrative-noundemonstrative-noun-demonstrative-noundemonstrative-noundemonstrative-noundemonstrative-nounnoun-demonstrativedemonstrative-noundemonstrative-nounnoun-demonstrativemixeddemonstrative-noundemonstrative-nounnoun-demonstrativenoun-demonstrativedemonstrative-noun.
numeral-noun-numeral-nounnumeral-noun-numeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-nounnumeral-noun-numeral-noun-numeral-nounnumeral-nounnumeral-noun-noun-numeralnumeral-noun-numeral-nounnoun-numeralnumeral-nounnumeral-nounnoun-numeralnumeral-nounnumeral-nounnumeral-nounnumeral-nounnoun-numeralnumeral-noun.
table 9: all languages for the experiments along with their iso 639-1 codes, writing script, and linguistic features.
“-” denotes lacking feature information from wals..sample with replacement for pos and ner..d additional results.
d.1 learning curve.
figure 6 visualizes the averaged learning curveof 10 out of 40 german 1-shot marc bucketsfor which the best dev performance is obtained atepoch 1..d.2 numerical values.
the numerical values of the pos and ner fs-xltresults are shown in table 13 and table 12. theabsolute performances of few-shot transfer withoutenglish source-training are shown in table 11. thelexical overlap of target languages with en forner and pos is shown in table 14..2924524465992551366038.
584630259692531654340126283.
782504972377536112266283127.
27641965253573370174322462.
19112416553716202373290.
5261766522165702691436332.
361554298872726241628416339.
43162369176424512621919059.
318021847124556125270395314.
402850244670676584189555.table 10: numerical value of the confusion matricesin figure 5. for 1-shot confusion matrices (right), weaverage results of 5 buckets and then round to integers..5766mldoc.
pawsx.
pos.
ner.
defresitruzhjako.
k=152.63 ± 8.9850.80 ± 8.5050.30 ± 8.3041.34 ± 6.8246.74 ± 9.4849.87 ± 10.4446.41 ± 6.59-.
k=884.31 ± 3.6077.80 ± 4.4474.08 ± 6.4865.50 ± 4.2170.83 ± 5.6376.15 ± 5.1066.85 ± 6.54-.
k=153.03 ± 1.6754.05 ± 1.3354.14 ± 1.53--53.97 ± 1.7952.81 ± 0.9653.92 ± 0.78.k=853.41 ± 1.4754.60 ± 0.9753.88 ± 1.72--54.17 ± 1.3852.97 ± 1.1553.63 ± 0.99.ruesvitrtamr--.
k=173.18 ± 4.4280.54 ± 4.1756.97 ± 5.1648.96 ± 3.1549.12 ± 4.6760.26 ± 5.72--.
k=486.65 ± 1.3290.26 ± 0.9972.00 ± 1.9959.65 ± 1.8364.96 ± 2.1673.58 ± 2.39--.
k=119.11 ± 6.9415.21 ± 5.9814.36 ± 4.2815.02 ± 5.5813.11 ± 4.5515.68 ± 7.09--.
k=435.57 ± 6.2339.37 ± 5.3329.63 ± 5.5537.81 ± 5.6327.42 ± 4.8233.50 ± 6.02--.
table 11: target-adapting results without source-training.
numbers are mean and standard deviation of 40 runs..table 12: zero- (column k=0) and few- (columnsk>0) shot cross-lingual transfer results (%) on postest set..enafarbgdeeleseteufafifrhehihuiditjakomrnlptrutatetrurvizh.
enafarbgbndeeleseteufafifrhehihuiditjajvkakkkomlmrmsmynlptruswtatethtltrurviyozh.
k=095.3986.6066.5587.0286.3881.8986.6479.1749.5165.7374.4982.5476.7964.2975.1070.8085.9747.6042.2958.7088.3586.4586.3653.5167.4857.5852.4054.9663.01.k=083.6578.3639.9178.5964.1779.0075.2077.1671.8855.3540.7368.4380.3856.3665.8471.2860.1080.307.1661.1861.2640.2946.5046.7754.7068.6142.4582.7779.2865.2068.3646.1250.021.5369.2365.7840.7764.6735.4813.95.k=1-91.10 ± 1.1175.64 ± 1.0991.01 ± 0.9789.38 ± 0.9089.69 ± 1.0590.05 ± 1.0181.69 ± 1.0968.44 ± 2.4780.82 ± 2.1478.25 ± 1.2289.55 ± 1.0880.40 ± 1.4278.87 ± 1.2684.44 ± 1.4072.68 ± 1.0888.77 ± 0.8775.84 ± 1.6857.43 ± 1.3671.60 ± 2.5288.97 ± 0.7388.18 ± 0.7089.07 ± 0.7662.84 ± 2.6971.46 ± 2.5864.01 ± 1.5374.95 ± 2.1564.79 ± 2.3374.15 ± 1.96.k=2-92.12 ± 1.1577.01 ± 0.8491.97 ± 0.9090.21 ± 0.5090.53 ± 0.8991.19 ± 0.7483.05 ± 0.9871.94 ± 1.7882.81 ± 1.7979.65 ± 0.8590.84 ± 0.6482.42 ± 1.0680.80 ± 0.8086.31 ± 0.9073.64 ± 0.7889.93 ± 0.5078.46 ± 1.3159.92 ± 1.1874.89 ± 1.9589.55 ± 0.7988.98 ± 0.6689.85 ± 0.5766.30 ± 1.5675.72 ± 1.9466.02 ± 1.2878.53 ± 1.3869.39 ± 1.7376.62 ± 1.39.k=4-93.50 ± 0.5678.52 ± 0.6793.18 ± 0.5691.32 ± 0.4391.58 ± 0.7292.31 ± 0.5284.39 ± 0.5675.89 ± 1.2084.95 ± 1.1681.32 ± 0.8291.66 ± 0.6083.98 ± 0.8381.97 ± 0.9288.61 ± 0.6774.34 ± 0.7590.77 ± 0.5980.42 ± 0.9862.37 ± 1.2277.21 ± 1.7790.83 ± 0.5489.78 ± 0.3891.13 ± 0.5169.36 ± 1.1378.84 ± 1.4467.73 ± 0.8279.57 ± 1.2472.36 ± 1.5179.42 ± 0.83.k=1-79.07 ± 1.4754.44 ± 6.7478.65 ± 0.3866.37 ± 1.6979.33 ± 0.7174.93 ± 0.7979.19 ± 1.9772.58 ± 1.1759.60 ± 3.3259.20 ± 5.3471.43 ± 2.6180.54 ± 0.9358.24 ± 2.2567.16 ± 1.6172.23 ± 1.3377.87 ± 6.3180.68 ± 0.7920.71 ± 7.0767.80 ± 4.7261.62 ± 1.0950.42 ± 5.4947.25 ± 1.3647.83 ± 2.3055.78 ± 2.5471.04 ± 3.0743.55 ± 3.8882.73 ± 0.4379.89 ± 0.9967.30 ± 2.3871.07 ± 4.2847.81 ± 1.8152.57 ± 1.914.56 ± 4.8772.34 ± 2.2569.37 ± 2.2458.48 ± 6.5168.77 ± 3.5453.55 ± 6.1932.84 ± 7.10.k=2-79.69 ± 1.4060.51 ± 4.3078.70 ± 0.3966.66 ± 1.5779.61 ± 0.7675.18 ± 0.9580.28 ± 1.7173.60 ± 1.6561.59 ± 3.8468.55 ± 4.0473.92 ± 2.4481.08 ± 0.8559.43 ± 2.2967.56 ± 2.1873.03 ± 1.4478.57 ± 4.1481.00 ± 0.9228.23 ± 5.3269.79 ± 3.3762.25 ± 1.5654.97 ± 6.8148.69 ± 1.8249.51 ± 3.0157.22 ± 2.4374.51 ± 4.2846.03 ± 4.4882.83 ± 0.5480.39 ± 0.9868.78 ± 2.7370.08 ± 3.1549.86 ± 2.9954.02 ± 2.656.08 ± 4.8872.63 ± 2.4369.53 ± 2.0763.38 ± 4.8869.64 ± 3.6358.22 ± 5.4740.34 ± 5.32.k=4-80.24 ± 1.1663.61 ± 2.6578.87 ± 0.4865.98 ± 2.1179.74 ± 0.7375.40 ± 0.9380.90 ± 1.9474.60 ± 1.5964.68 ± 2.9671.13 ± 3.4575.81 ± 2.1581.22 ± 0.9360.27 ± 2.4368.29 ± 1.7674.14 ± 1.6181.07 ± 1.5080.90 ± 1.1232.93 ± 6.0372.12 ± 3.3463.68 ± 1.6662.94 ± 4.5551.76 ± 2.3051.41 ± 3.3159.18 ± 3.1376.25 ± 3.0447.81 ± 4.2882.82 ± 0.4680.49 ± 0.9571.34 ± 2.8274.33 ± 5.2552.23 ± 2.6355.75 ± 2.725.87 ± 4.1473.55 ± 2.2572.33 ± 2.8566.49 ± 4.6471.08 ± 3.2865.46 ± 7.1048.49 ± 4.30.figure 6: early stopped 1-shot transfer (en → de)learning curve.
the english-trained model overﬁts the1-shot bucket quickly, showing decreasing dev perfor-mance during training..k=14.54af0.65ar0.98bg0.39bn8.75de1.45el6.29es4.80et3.77eu0.27fa5.61fi6.26fr0.86he0.95hi5.07hu5.34id7.89it1.75ja2.49jv1.99ka0.89kk1.48ko0.36ml0.53mr4.86ms0.21my7.18nl6.29ptru1.60sw 5.900.65ta0.77te1.63th4.83tl4.89tr0.30ur4.33vi1.90yo1.81zh.
ner.
k=28.750.952.190.7713.201.8410.595.965.550.449.0510.831.901.169.199.8210.942.023.054.001.221.541.040.567.440.3610.6511.002.348.101.540.801.878.968.480.278.392.581.99.k=413.441.573.230.8020.613.5919.6611.2412.311.0115.6619.013.231.9914.3516.9421.272.143.445.782.113.321.300.7113.700.4220.1419.133.7712.372.081.192.0814.9816.430.6813.412.882.14.k=14.973.51--9.361.9610.005.812.600.374.5915.601.220.443.189.3911.992.60---2.33-0.24--7.948.884.15-1.320.20--2.090.741.62-3.04.pos.
k=26.114.49--15.332.8717.539.223.450.377.0325.231.930.273.9213.7816.153.68---3.85-0.24--11.4213.386.11-1.280.20--2.261.352.16-4.86.k=47.905.30--21.483.0422.6313.174.690.418.7837.392.260.514.1521.7521.355.00---5.67-0.24--16.7920.139.32-1.620.20--3.012.162.90-7.33.table 13: zero- (column k=0) and few- (columnsk>0) shot cross-lingual transfer results (%) on nertest set..table 14: lexical overlap (per-mille) of target lan-guages with en for ner and pos using different k-shot buckets..5767246810epoch index404244464850validation accuracy (%)learning curve of 1-shot german marc1-shot0-shot