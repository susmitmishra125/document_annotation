can generative pre-trained language models serve asknowledge bases for closed-book qa?.
cunxiang wang♠♣∗, pai liu♣∗and yue zhang♣♥†♠zhejiang university, china♣school of engineering, westlake university, china♥institute of advanced technology, westlake institute for advanced study, china{wangcunxiang, zhangyue, liupai}@westlake.edu.cn.
abstract.
recent work has investigated the interestingquestion using pre-trained language models(plms) as knowledge bases for answeringopen questions.
however, existing work islimited in using small benchmarks with hightest-train overlaps.
we construct a new datasetof closed-book qa using squad, and in-vestigate the performance of bart.
experi-ments show that it is challenging for bartto remember training facts in high precision,and also challenging to answer closed-bookquestions even if relevant knowledge is re-tained.
some promising directions are found,including decoupling the knowledge memoriz-ing process and the qa ﬁnetune process, forc-ing the model to recall relevant knowledgewhen question answering..1.introduction.
large-scare pre-trained language models (plms)such as bert (devlin et al., 2019), gpt (radfordet al., 2018) have signiﬁcantly improved the perfor-mance of nlp tasks (radford et al., 2019).
thereis increasing evidence showing that plms containworld knowledge (petroni et al., 2019; zhou et al.,2020; talmor et al., 2020).
as a result, recent re-search considers generative plms such as t5 (raf-fel et al., 2020) and bart (lewis et al., 2020a) forclosed-book qa, which has only question-answerpairs without external knowledge source.
for ex-ample, after being ﬁnetuned on a few qa pairs, agenerative lm can directly output “florence” af-ter being given the question “where was danteborn?”.
roberts et al.
(2020) ﬁnd that generativeplms can store and use knowledge as they canachieve relatively high performance in closed-bookqa task on three datasets.
however, lewis et al.
(2020b) ﬁnd that the excellent results are mainly.
∗equal contribution† the corresponding author.
figure 1: process of generative plms for closed-bookqa.
(1) bart performs poorly on closed-book qaafter qa ﬁnetuning; (2) we lm-ﬁnetune bart withrelated passages to feed knowledge and use a recit-ing task to evaluate how much knowledge the lm-ﬁnetuned model memorizes; (3) though memorizingmost needed knowledge, bart still faces challenge onclosed-book qa after qa ﬁnetuning..due to high question/answer overlap rates betweentraining and testing data..existing research leaves many open questionson the potential of generative pre-trained lms onclosed-book qa.
for example, the used datasetsconsist of question-answer pairs only, and there isno mechanism to control what factual knowledgeis already used to train a generative plm beforetaking the closed-book questions.
in addition, thehigh overlapping rates between training and testingquestions and answers make it difﬁcult to under-stand whether the answer that a model gives comesfrom its inherent knowledge or superﬁcial cues intraining data.
to address these issues, we makea new benchmark of question-answer pairs fromsquad (rajpurkar et al., 2018), where each ques-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3241–3251august1–6,2021.©2021associationforcomputationallinguistics3241tion has a corresponding wikipedia passage as atraceable knowledge source for pre-training.
weﬁnd that despite giving around 25% accuracy on ex-isting test sets (i.e., webquestions and triviaqa),bart gives only 1.5% accuracy on the squaddataset..this result shows that there is still much chal-lenge in using bart for closed-book qa directly.
we further investigate the reason by separatelyexamining whether bart can remember factualknowledge accurately, and whether it can makeuse of remembered knowledge to answer questions.
the general process of investigating these two is-sues is presented in figure 1..for the ﬁrst issue, we use related passages insquad to further extra pre-train bart, whichwe call as lm-ﬁnetuning, and test the ratio of re-tained factual knowledge using a language mod-eling task, which we call as reciting.
resultsshow that as the number of training passages grows,bart demonstrates severe issues of forgetting, los-ing track of exact facts in the lm task.
for example,when the number of passage is around 500, bartcan memorize 66% needed knowledge.
but whenthe number of passage increases to about 5000, theratio becomes 4%..for the second issue, we use versions of lm-ﬁnetuned bart that can retain the majority offactual knowledge for further qa ﬁnetuning, byconstraining the number of passages.
althoughall the training and testing questions concern thepassages in lm-ﬁnetuning, bart still fails to an-swer the majority of questions.
this demonstratesdifﬁculties in making use of internal knowledgefor qa.
in addition, further experiments show thatqa ﬁnetuning can negatively inﬂuence the retainedfactual knowledge as measured using the originallm task..while reporting such challenges, we also ﬁndsome promising directions by using simple dataaugmentation tricks.
for example, simply addingrelated passages to test outputs can help bartretrieve relevant factual knowledge and give thecorrect answer.
in addition, rather than treatingqa ﬁnetuning in the same way as lm pre-training(roberts et al., 2020), decoupling the lm pre-training task and the qa ﬁnetuning tasks can alsoallow a model to better retain factual knowledgethrough the qa-ﬁnetuning task.
1.
1we.
have.
released.
the.
code.
and.
dataset.
at.
https://github.com/wangcunxiang/can_plm_server_as_kb for future study..webquestionstriviaqanaturalquestions.
train set3778961091107369.dev set10164975900.test set10164976900.
(a) the qa pairs of three datasets..squad.
train set.
test set86396(19035) 2968(602) 2930(602).
dev set.
(b) the qa pairs and passages statistics of squad.
the numbers in () are the passage amounts..table 1: details of each dataset after our processing..models \ datasetoriginal bart-large→ qa-ﬁnetuneoriginal bart-large→ pre-trained withall passages→ qa-ﬁnetune.
squad wb.
tq.
nq.
1.5% 30.0% 24.9% 23.0%.
1.8%.
-.
-.
-.
table 2: closed-book qa performance of bart onfour datasets.
for squad, only qa pairs are used inthis experiments.
wb, tq and nq means webques-tions, triviaqa and naturalquestions, respectively..2 using squad for closed-book qa.
in the closed-book qa task (roberts et al., 2020), amodel needs to answer questions without externalresources.
formally, the input is a question q, andthe output is a sequence of tokens o. for evaluation,the correct golden answer g will be compared witho. previous work (roberts et al., 2020) uses theexact match (em) metric to score o against g..we conduct closed-book qa by using thebart model (lewis et al., 2020a) on fourdatasets-webquestions (berant et al., 2013), triv-iaqa (joshi et al., 2017), naturalquestions(kwiatkowski et al., 2019) and squad2 (ra-jpurkar et al., 2018).
bart is a transformer-based(vaswani et al., 2017) sequence-to-sequence gen-erative plm, which we choose because it hasachieved several state-of-the-art results on genera-tive tasks.
we use the publicly released checkpointbart-large in this work.2.
to use a generative plm on each dataset,the model is ﬁrst ﬁnetuned using the trainingquestion-answer pairs.
we call this process as qa-ﬁnetuning.
while the other three datasets are usedby following previous work (roberts et al., 2020),we make a novel adaptation of the squad datasetfor closed-book qa.
squad (rajpurkar et al.,2018) is a wildly-adopted qa dataset typically forextractive qa, where the input is a question to-.
2https://huggingface.co/facebook/.
bart-large/tree/main.
3242dataset\overlap type answer overlap question overlap.
naturalquestionstriviaqawebquestionssquad.
61.5%78.7%59.3%24.0%.
32.5%33.6%27.5%1.0%.
table 3: question and answer overlaps on fourdatasets.
question overlaps data of naturalquestions,triviaqa and webquestions are from lewis et al.
(2020b); answer overlaps on the three datasets are abit different from lewis et al.
(2020b) because of ourdataset pre-processing..dataset \ overlap type.
webquestionssquad.
otest overlapwith gtrain88.5%39.8%.
gtest overlapwith gtrain59.3%24.0%.
table 4: overlap analysis between test outputs/goldenanswers and training answers.
we select the top-performing results to analyze..gether with a passage containing the answer fact,and the answer is a span from the passage.
how-ever, no previous work has used squad for closed-book qa yet.
compared to other qa datasets,squad is the most suitable for our setting, con-taining corresponding passages, lower test-trainoverlap, and receiving more research attention.
toapply squad on closed-book qa, we only useqa pairs for input and output when qa-ﬁnetuning.
for triviaqa and webquestions, many questionshave multiple answers.
in order to align with theother two data sets, we split one question with sev-eral answers into several same questions with oneanswer when training, and take one test output ascorrect if it appears in the answer list when testing.
as the test sets of squad, naturalquestions andtriviaqa are not fully publicly released yet andwebquestions does not have a development set,we split the development set of the three datasetsand the test set of webquestions into two subsetsto serve as a new development set and a new testset.
we report performance on the new test sets intable 2 while analyzing the overlaps on the twosubsets together in table 4 and table 5. the detailsof four datasets after our pre-processing are shownin table 1..previous work shows that t5 and bart canachieve promising results (roberts et al., 2020;lewis et al., 2020b) on webquestions, triviaqaand naturalquestions.
however, recently, lewiset al.
(2020b) ﬁnd that the high performance ismainly because the three datasets have severe test-train overlap problems.
in particular, we use an-.
overlap29.8% (604).
non-overlap0.2% (5).
correctincorrect 58.7% (1189) 11.3% (228)(a) on webquestions.
overlap1.3% (77).
non-overlap0.1% (6).
correctincorrect 38.5% (2272) 60.1% (3530)(a) on squad.
table 5: overlap analysis of test outputs on webques-tions and squad by bart.
in the result cells, wepresent both percentages and case numbers.
we selectthe top performing result to analyze..swer overlap to denote the situation where theanswer a in a test (q, a) pair exists in training an-swers, and the term question overlap to denote thefact that a training question with similar meaningcan be found for q. to analyze whether squadhas the same problem, we also compute the overlapof it.
answer overlap can be easily calculated.
forquestion overlap, following lewis et al.
(2020b),we ﬁrst randomly sample 1,000 (q, a) pairs fromthe squad test set.
then for each test question,we automatically select squad training questionswhose answer is a sub-sequence of the test answer.
then we ask three human experts to ﬁnd whetherthe test q overlaps with any training question..the breakdown statistics are given in table 3.squad has much fewer test-train overlapped casesthan the other three datasets.
for example, onlyaround 1% of squad test questions overlap withtraining questions while the number is around 30%in the other three datasets..2.1 results.
the overall qa results on the four datasets areshown in the ﬁrst row of table 2.bartachieves relatively high results on the three datasetswebquestions, triviaqa, and naturalquestions.
however, it performs poorly on squad in closed-book qa, with only 1.5% accuracy.
we also usesquad passages to further pre-train bart andthen conduct qa-ﬁnetuning.
the result is shownin the second row of table 2, the performance is1.8% a bit better than 1.5% but still extremely low.
according to lewis et al.
(2020b), the resultsare inﬂuenced by test-train overlap rates.
for sim-plicity, we deﬁne the set of gold standard answersin the train set as gtrain, the set of gold standardanswers in the test set as gtest.
we deﬁne the set ofoutput answers of bart on the test set as otest, theset of output answers which are correct as ocorrect.
to further investigate how overlap inﬂuences.
3243bart’s outputs, we choose webquestions as thehigh-overlap dataset representative to compare withthe low-overlap dataset squad.
results are shownin table 4. the otest of bart on webquestionshave an 88.5% overlap with gtrain, which is adecisive proportion.
however, the gtest have only59.3% overlap with gtrain.
for bart on squad,the ratios are 39.8% to 24.9%, which is relativelyless severe.
this indicates that if testing questionshave a large overlap with training questions, themodel tends to generate the targets and words inthe train set..we further measure the relationship betweenhow correct/incorrect outputs and overlap/non-overlap with gtrain.
the results are shown intable 5, 604 of ocorrect of bart on webques-tions overlap with gtrain, and only 5 instancesof ocorrect do not exist in gtrain.
however, allthe ﬁve non-overlapping otest on webquestionsare combinations of words of gtrain and questionwords, which can be viewed as a mild type of over-lap.
the situation is similar but sightly better onsquad.
these results indicate that it is much eas-ier for bart to answer correctly by superﬁcialcues than by using its internal knowledge..3 task design.
the original purpose of previous research (petroniet al., 2019; roberts et al., 2020) is to use pre-trained language models (plms) as knowledgebases (kbs) and answer questions according tointernal knowledge the model contains.
however,if the model tends to match test questions withtraining questions for retrieving answers, then thesource of knowledge is restricted to training ques-tions.
this deviates from the ultimate goal..we are interested in quantitatively measuringthe capability of pre-trained model in closed-bookqa using its own internal knowledge from pre-training.
this capability can be broken down intotwo components.
first, the capability of a memoriz-ing knowledge from pre-training.
second, the abil-ity of retrieving memorized knowledge for questionanswering.
we show investigations and report theresults in the two sections below..figure 2: the main task design.
the lower right boldcontext of each process are names of this process.
thebold context in the upper middle of each process is thecorresponding process in the classroom teaching.
themiddle context is the purpose of this process.
the lefticon represent the state of the model..models \ dataset.
random-initialized bartoriginal bartbart → lm-ﬁnetuning.
all squad(20279)0.0%2.2%2.7%.
table 6: the reciting performance on all squad pas-sages.
we use the bart-large checkpoint.
lm-ﬁnetuning and reciting are both conducted on the same20279 passages..to test how well they know the book.
next, theteacher gives the student some exercise questionsfor practice.
finally, the teacher gives a differentset of exam questions to test the student.
note thatthe whole book is taught and recited, rather thana split of the book, and the exercise questions andexam questions are all related to the book..section 4 (knowledge memory) correspondsthe teaching and reciting processes in the class-room teaching.
section 5 (question answering)corresponds the practice and exam processes..3.1 procedure.
4 knowledge memory.
as shown in figure 2, our design is motivated byclassroom teaching.
a teacher ﬁrst teaches thecontent of a textbook and then asks the studentto recite the important points of the book in order.
to investigate whether bart can acquire and storeknowledge from raw corpus, we use passages fromsquad to ﬁnetune the bart model, which wecall lm-ﬁnetuning.
this period can be seen as.
324420.models \ datasetoriginal bart.
60201.5% 5.2% 3.6% 3.2% 2.9% 2.2%bart → lm-ﬁnetuning 87.3% 72.6% 66.3% 34.3% 14.0% 3.9%bart → lm-ﬁnetuning(added preﬁx/sufﬁx).
85.5% 79.6% 59.5% 40.4% 15.8% 4.0%.
1094.
1641.
160.
547.table 7: performance of reciting.
we use the bart-large checkpoint.
for the header of each column, the numbersstand for passage amounts of the subset.
note that lm-ﬁnetuning and reciting are both conducted on the samepassages.
the last row of this table will be discussed in section 5.3.inspired by petroniknowledge the model has.
et al.
(2019) and talmor et al.
(2020), who askdiscriminative plms to ﬁll masks of given maskedpassages/sentences, our reciting task is to give agenerative plm several masked passages and askit to recover them.
for each passage, we mask thetoken spans which are answers of related questions.
an example is shown in the last row of figure 3.in this way, we can assume that if the bart canrecover the speciﬁc-masked passages, it must havethe knowledge needed for further qa.
note thatdoing training for lm-ﬁnetuning, the masked to-kens are randomly chosen, following bart (lewiset al., 2020a).
besides, because the answer spansare mostly entities or independent knowledge seg-ments, it is relatively less likely for models to re-cover them by heuristics or superﬁcial cues.
itis natural to do reciting to probe the model’s in-ternal knowledge since it is most related to themasked language model process (lm-ﬁnetuningand bart’s pre-training task)..evaluation metrics.
we use the accuracy ofmasked spans recovery to measure how muchknowledge the model memorizes.
because manyanswer spans appear several times in passages, wecannot simply treat the presence of the span as cor-rect.
in addition, even when the masked token isgenerated correctly, if its contextual words change,the meaning of the sentence may be different.
con-sidering these, we choose a more strict evaluationmetric for the reciting accuracy.
we treat a span ascorrectly predicted only if subsequent words afterthe current mask and before the next mask (or thesubsequent 10 tokens if the span between maskedtokens is more than 10) are also correctly predicted..4.1 results.
we ﬁrst conduct reciting experiments on allsquad passages using the original bart, arandom-initialized bart and a lm-ﬁnetunedbart.
the results are shown in table 6. therandom-initialized bart gives zero accuracy,demonstrating that the task is difﬁcult and there.
figure 3: examples of two types of mask policies intraining and testing periods of lm-ﬁnetuning.
the pas-sage masked randomly is for training and the passagemasked with answer spans is for testing (reciting)..feeding knowledge into bart.
then we test themodel to examine how much knowledge bartcan memorize.
we also call this testing process asreciting..training of lm-ﬁnetuning.
we follow theoriginal training objective of bart for the mlm-ﬁnetune step, which is a denoising auto-encodingprocess.
the original bart training objective in-volves ﬁve operations, namely token masking, sen-tence permutation, document rotation, token dele-tion and text inﬁlling (lewis et al., 2020a).
weonly adopt token inﬁlling in this work becauseit shows beneﬁts on all downstream tasks (lewiset al., 2020a).
in addition, the sentence permutationtask is shown harmful for tasks despite only beinguseful for text summarization (lewis et al., 2020a).
for each input passage, we randomly mask 30%tokens following lewis et al.
(2020a).
an exampleis shown in the third row of figure 3. we ask themodel to recover the passage as the output, anduse the output and the original passage to computeloss..testing of lm-ﬁnetuning (reciting).
in test-ing period of lm-ﬁnetuning, we develop a taskcalled ‘reciting’ to probe how much (speciﬁc).
3245is no possibility of guessing.
the original bartscores 2.2%, showing that it contains certain butlimited knowledge.
the lm-ﬁnetuned bartgives 2.7% accuracy.
this result shows that lm-ﬁnetuning is useful to a certain extent.
however, de-spite that 100% knowledge is given, lm-ﬁnetuningonly increases the result by 0.5%, demonstratingthat bart faces signiﬁcant challenges in memoriz-ing important knowledge contained in pre-trainingsquad texts..given above observations, we try to reduce thechallenge by producing smaller datasets by ex-tracting subsets from squad.
the subsets include20, 160, 547, 1094, 1641, 6020 passages, respec-tively, where the three numbers indicate the passageamounts.
for these reciting experiments, we con-sider only the original and lm-ﬁnetuned bart..the results are shown in the ﬁrst two rows oftable 7. we can ﬁnd that (1) using lm-ﬁnetuning,bart can memorize some knowledge.
for ex-ample, when passage subset is 547, the originalbart can only recover 3.6% masked spans cor-rectly while the lm-ﬁnetuned bart can recover66.3% masked spans; (2) the memorization abilityquickly decreases when the passage amount in-creases.
for example, when passage subset are 20,bart can recover 87.3% masks correctly; whenit is 1094, the accuracy falls to 34.3%; when it is6020, the accuracy is only 3.9%..we conclude that bart has a certain abilityto store (factual) knowledge, but the capacity israther weak.
if we control the number of passagesfor lm-ﬁnetuning, we can make sure that bartcan memorize most needed knowledge.
the lm-ﬁnetuned model trained on smaller subsets givesa more useful setting for testing qa abilities ofbart when we are conﬁdent that relevant knowl-edge is retained..5 question answering.
we employ the settings in the ﬁrst three columnsin table 7, where models can memorize at least50% of needed knowledge, for further analyzingthe relationship between memory and qa ability.
for these experiments, all qa pairs come frompassages that bart has been lm-ﬁnetuned on..5.1 overall results.
besides exact match (em) which is commonlyused in previous closed-book qa work (robertset al., 2020; lewis et al., 2020b), we also consider.
figure 4: an intuitive approach to qa-bridge-tuning.
to make the model more dependent on the inter-nal knowledge to answer the question, the model isrequired to generate not only answer but also thecorresponding passage.
the outputs should be ‘p<answer> a’, where ‘p’ stands for the correspond-ing passage, <answer> is a special marker and thea stands for the answer..human evaluation (he) and f1 for two reasons.
first, we observe that em cannot fully indicate cor-rectness.
for example, a question is “what centurydid ... ?” and the golden answer is “10th century”.
the model outputs “10th” which is actually cor-rect in but taken incorrect by em.
second, f1 canhelp indicate the similarity between the outputs andgolden answers..the overall results are presented in the ﬁrsttwo rows of table 8. according to the resultof ‘original bart-large→lm-ﬁnetuning→qa-ﬁnetuning’, compared to reciting accuracy (ra)of each model, the qa accuracy is much lower(87.3% vs 30%, 72.6% vs 6.5%, 66.3% vs 6.7% inhe).
this result shows that bart’s ability to useits internal knowledge to answer questions is weak.
in addition, comparison between the ﬁrst row andthe second row shows that memorized knowledgehelps the models better answer questions, thoughthe help is not much (30% vs 0.0%, 6.5% vs 4.3%,6.9% vs 4.9% in he)..for the reciting-qa-accuracy gap, we proposetwo possible explanations, the ﬁrst is that the modelcannot activate related memory for question an-swering; the second is that the memorized knowl-edge is somehow corrupted during qa-ﬁnetuning..5.2 strengthening memory retrieval.
qualitative cases show that, even the model con-tains needed knowledge, the model does not neces-sarily refer to the most relevant memory for ques-tion answering after qa-ﬁnetuning.
we list severalthis kind of examples in the ‘qa-ﬁnetune’ col-umn of table 9. for example, in the ﬁrst row oftable 9, for the question “what is southern cali-fornia often abbreviated as?”, despite of the model.
3246models \ dataset.
bart → qa-ﬁnetuningbart → lm-ﬁnetuning→ qa-ﬁnetuningbart → lm-ﬁnetuning→ qa-ﬁnetuning(added preﬁx/sufﬁx)bart → lm-ﬁnetuning→ qa-bridge-tuningbart → lm-ﬁnetuning→ qa-bridge-tuning(added preﬁx/sufﬁx).
20 (16/2/2;125/8/10).
160 (128/16/16;653/107/93) 547 (442/53/52;2334/314/306)ra(%) em(%) he(%) f1(%) ra(%) em(%) he(%) f1(%) ra(%) em(%) he(%) f1(%).
1.5.
0.0.
0.0.
87.3.
10.0.
30.0.
11.0.
15.4.
5.2.
72.6.
2.2.
3.2.
4.3.
6.5.
6.4.
9.0.
3.6.
66.3.
1.9.
2.3.
4.9.
6.9.
7.0.
6.7.
85.5.
10.0.
30.0.
21.0.
79.6.
3.2.
10.8.
10.1.
59.5.
2.9.
7.8.
8.2.
87.3.
20.0.
40.0.
27.8.
72.6.
9.7.
20.4.
15.3.
66.3.
4.6.
11.8.
9.3.
85.5.
20.0.
40.0.
31.7.
79.6.
11.8.
22.6.
16.3.
59.5.
5.6.
12.7.
10.3.table 8: qa performance on three subsets of squad.
the numbers in headers are the passage and qa pairamounts, for example, ‘160 (128/16/16;653/107/93)’ indicates this subset has overall 160 passages and 128/16/16passages, 653/107/93 qa pairs in train/dev/test set, respectively.
the number in ra column stands for recitingaccuracy, which is the same with table 7. the ras in the table can show how much knowledge bart memo-rizes before qa-ﬁnetuning, of which values the model should achieve in qa accuracy if it can fully use internalknowledge to answer questions.
the cells with bold text are our methods.
em, he indicate exact match, humanevaluation, respectively.
‘bart’ denotes the ‘bart-large’ checkpoint..question&answer.
model output.
qa-bridge-tune.
qa-ﬁnetune.
q: what is southerncalifornia oftenabbreviated as?
a: socal.
q: what centurydid the normansﬁrst gain theirseparate identity?
a:10th century.
q: what is thelargest stadiumin australia?
a: melbournecricket groundq: when did the1973 oil crisis begin?
a: october 1973.southerncalifornia.
southern california, oftenabbreviated socal, is...<answer> socal.
20thcentury.
... distinct cultural andethnic identity of thenormans emerged initiallyin the ﬁrst half of the10th century ...<answer> 10th.
australiastadium.
... <answer>melbourne cricketground.
1973.
... <answer> october1973.table 9: four real output examples on qa-ﬁnetuningand qa-bridge-tuning by bart..is trained with “southern californi, often abbrevi-ated socal”, it still answers ‘southern california’,which indicates that the model cannot retrieve re-lated memory for answering questions..we propose a simple way to strength knowledgeretrieval, namely qa-bridge-tune, which is a ex-tended qa-ﬁnetuning process.
the process is il-lustrated in figure 4, for each question input, theoutput concatenates the related passage with theanswer.
thus, the model can explicitly recall thememorized passages when answering questions, bywhich qa-bridge-tune builds a bridge between qaand memorized knowledge so that the model can.
a > b a = b a < brelevance 30.2% 53.3% 16.6%.
table 10: human-evaluated relevance between the re-sults using and not using qa-bridge-tune with correctanswers.
a > b means that a’s outputs are more re-lated to correct answers than b’s, etc.
a = qa-bridge-tune, b = qa-ﬁnetune in this table..models \ dataset.
16/2/2 128/16/16 442/53/52.
85.5% 79.6%.
bart → lm-ﬁnetuning 87.3% 72.6%bart → lm-ﬁnetuning(added preﬁx/sufﬁx)bart → lm-ﬁnetuning→ qa-ﬁnetuningbart → lm-ﬁnetuning→ qa-ﬁnetuning(added preﬁx/sufﬁx).
5.7% 51.4%.
2.8% 10.9%.
66.3%.
59.5%.
2.4%.
16.2%.
table 11: performance of reciting after qa.
the num-bers in the header is the passage amount of this subset.
‘bart’ denotes the ‘bart-large’ checkpoint..answer questions with learned knowledge.
in addi-tion, this method can help improve interpretability.
the results are shown in table 8. we can seethat qa-bridge-tune can help the model wake upthe related memorize knowledge when qa, thusimproving em accuracy and by two or three timeson baselines.
in addition to answer correctness,we also consider the relevance between model out-puts and golden answers regardless whether theanswer is correct.
for example, the question is“the amazon rainforest makes up what amount ofearth’s rainforests?” and the golden answer is“over half”, and two generated answers are “60%”and “the amazon rainforest”.
they are both incor-.
3247rect but the former is more relevant and thereforea better answer.
we ask human experts to man-ually compare the results between using and notusing qa-bridge-tuning, selecting results by us-ing ‘bart→lm-ﬁnetuning→qa-ﬁnetuning’ and‘obj→lm-ﬁnetuning→qa-bridge-tuning’ strate-gies on the ‘128/16/16’ subset.
the results areshown as table 10. according to human experts,in 30.2% cases, the outputs of qa-bridge-tuningare more relevant to the golden answer than thoseof qa-ﬁnetuning while only in 16.6% cases, qa-ﬁnetuning is more relevant.
this result showsthat qa-bridge-tuning can help bart ﬁnd morerelevant knowledge.
we also list several exam-ples showing in figure 9. as the example inthe ﬁrst paragraph of this subsection, for ques-tion “what is southern california often abbrevi-ated as?”, bart can output the correspondingpassage along with the correct answer “socal” af-ter qa-bridge-tuning.
these results suggests thatqa-bridge-tuning can effectively help the modelrecall the remembered knowledge..5.3.inﬂuence of qa on memory.
to explore whether qa-ﬁnetune interferes withthe memory of lm-ﬁnetuned models, we use qa-ﬁnetuned models for the reciting task.
the resultsare given in table 11. after qa-ﬁnetuning, themodels’ reciting accuracy declines.
we have twopossible explanations for this phenomenon.
first,qa-ﬁnetune process disrupts the models’ internalmemory with regard to representation; second,the tasks are different, so model output space isdisturbed, but the model still retains knowledge.
though we cannot qualitatively understand the in-ﬂuence of each reason above, isolating the qa func-tionality from pre-trained denoising auto-encodingcan potentially address interference issues..we experiment with a simple intuitive solution tothis issue, namely to decouple the qa-ﬁnetune pro-cess and the lm-ﬁnetune process, so that the twotask input/output spaces are differentiated to someextent.
this is done simply in the input and out-put level.
we add <passage>/<question>preﬁx tokens and </passage>/</question>sufﬁx tokens to each input passage/question whenlm-ﬁnetuning and reciting/qa-ﬁnetuning, respec-tively, and also add </passage>/</answer>sufﬁx tokens to each output passage/answer..the results are shown in the rows with (addedpreﬁx/sufﬁx) in table 11. the reciting accuracy.
models \ dataset 16/2/2 128/16/16 442/53/52.
original gpt-2→ lm-ﬁnetuning→ qa-ﬁnetuning.
0%.
1.1%.
1.0%.
table 12: performance of gpt2 in the same setting asthe second row of 8. the numbers in the header is thepassage amount of this subset.
the score is evaluatedwith exact match (em)..with preﬁx/sufﬁx after lm-ﬁnetuning is not muchdifferent compared without preﬁx/sufﬁx.
how-ever, the qa accuracy signiﬁcantly improves whenadding preﬁx/sufﬁx (2.8% to 5.7%, 10.9% to51.4%, 2.4% to 16.2% in he).
the results showthat our decoupled methods can help the modeldistinguish the input type to ﬁnd the appropriatesemantic space, thus alleviating this problem.
be-sides, according to the comparison between thesecond row and the third row in table 8, addingpreﬁx/sufﬁx can help models better answer ques-tions.
we suppose it is also because this methodcan help models distinguish the input/output space..5.4 gpt-3.
gpt3 has also been shown to have certain capabil-ities to answer factual closed-book questions.
asshown in table 3.3 of brown et al.
(2020), it canachieve relatively high performance on triviaqain closed-book task even in zero-shot learning set-ting.
however, it underperforms t5 (roberts et al.,2020) in the other two datasets webquestions andnaturalquestions, which indicates that super largescale pre-training is not the ultimate solution to theissue we discussed.
there is also a possibility thatgpt-3 has seen most test qa pairs of triviaqa inthe pre-training stage as it crawls extremely largedocuments from the internet..we also apply gpt-2 to lm-ﬁnetuning and qa-ﬁnetuning, which has similar architecture, pre-training and ﬁnetune process with gpt-3.
thus webelieve that they can have the same fundamentalproblem.
the results are shown in table 12. lm-ﬁnetuned gpt-2 has worse performance comparedto lm-ﬁnetuned bart.
this conﬁrms that the ar-chitecture and the training process of gpt3/gpt-2do not solve the problems we ﬁnd using bart..6 related work.
there are two types of pre-trained language mod-els (plms), discriminative plms such as bert(devlin et al., 2019), elmo (peters et al., 2018)and generative plms such as gpt (radford et al.,.
32482018), bart (lewis et al., 2020a).
the key dif-ference is that generative plms are of encoder-decoder architectures so they can generate textsequences of any length or token.
an increas-ing number of works have shown that plms con-tains world knowledge.
petroni et al.
(2019) ﬁrstsolves that discriminative plms such as bert(devlin et al., 2019) can be used for cloze-styleqa using a mask language modeling task with-out external resources, such as “dante was bornin [mask].” → “florence”.
their results showthat plms have certain factual knowledge.
tal-mor et al.
(2020) set eight types of cloze-styleqa, such as ‘always-never’ and ‘age com-parison’, to test different types of knowledgein several discriminative plms, including bertand roberta (liu et al., 2019).
they also usethe mask language modeling task to do qa with-out ﬁnetuning, and results show that the evaluatedplms indeed contain those kinds of knowledge.
wang et al.
(2019); zhou et al.
(2020) adopt somediscriminative plms on commonsense reasoningqa tasks such as comve (wang et al., 2020) andswag (zellers et al., 2018) without ﬁnetuning, in-dicating the plms have commonsense knowledge.
bosselut et al.
(2019) show that pretrained trans-former models can be used to help construct com-monsense knowledge graphs, such as conceptnet(speer and havasi, 2012).
however, poerner et al.
(2019) argue that bert uses some superﬁcial cuessuch as stereotypical characters to solve factualquestions.
gpt-3 (brown et al., 2020) seems tohave ability to answer factual questions in zero-shot setting, but there exists some evidence thatgpt-3 is limited in storing and using knowledge(bergdahl, 2020)..roberts et al.
(2020) ﬁrstly use closed-book qato detect how much knowledge is in pre-trainedlanguage models’ parameters.
they perform ex-periments on three datasets webquestions (berantet al., 2013), triviaqa (joshi et al., 2017) and nat-uralquestions (kwiatkowski et al., 2019) by t5model (raffel et al., 2020).
the results are rela-tively pleasant.
however, lewis et al.
(2020b) ﬁndthat the high performance of roberts et al.
(2020) ismainly due to the high test-train overlap of the threedatasets rather than the model’s internal knowledge.
our ﬁndings conﬁrm the conclusions of lewis et al.
(2020b), and we further experiment with a morecontrolled squad dataset, and discussed the weak-ness of bart in both memorization and knowledge.
retrieval.
because t5 (raffel et al., 2020) is moreresource demanding, considering the balance of ef-fectiveness and experimental feasibility, we choosebart rather than the t5 model..different from closed-book qa, where no addi-tional resource is available when answering ques-tions, open-domain qa requires models to generatea sequence of tokens as the answer to each ques-tion by looking up related text from unstructureddocuments (chen et al., 2017).
chen et al.
(2017)ﬁrst try to retrieve related passages from wikipediafor each question and encode both the question andpassages into the model, then output the answer.
guu et al.
(2020) integrate the retrieval processinto pre-training process, helping the plms bet-ter retrieve information from external knowledgesource when needed, and ﬁnding beneﬁts on open-domain qa task.
retriever-based models have theadvantage of relieving the burden of pre-trainedlanguage models to remember every factual detail.
the retrieval qa setting is slightly reminiscent toour data augmentation setting in figure 4, but withthe related passage being the input, rather than theoutput.
in contrast, the settings we consider fullyrely on a neural model for all knowledge..squad (rajpurkar et al., 2016, 2018) is awidely-used dataset for machine reading compre-hension, which is also a type of qa task.
it asksmodels to use a text span from a given referentialpassage to answer questions.
it is also used in othertype of qa task, for example, chen et al.
(2017)adopt it in the open-domain qa task.
we ﬁrst ap-ply it on closed-book qa and analyze why it issuperior than other three commonly used datasets..7 conclusion.
we investigated by using squad, ﬁnding thatclosed-book qa is still challenging for generativepre-trained language models such as bart.
thechallenge lies both in remembering the knowledgedetails and in answering the questions after remem-bering the knowledge.
potential solutions includeexplicitly asking models to recall relevant knowl-edge when answering questions and decouplinglm-ﬁnetuning process and qa-ﬁnetuning process..8.
*acknowledgement.
the work was supported by nsfc 61976180. wethank yongjing yin, chuang fan, yuchen niu, saragong, tony ou, libo qin and all reviewers for theirgenerous help and advice during this research..3249references.
jonathan berant, andrew chou, roy frostig, and percyliang.
2013. semantic parsing on freebase fromquestion-answer pairs.
in proceedings of the 2013conference on empirical methods in natural lan-guage processing, pages 1533–1544, seattle, wash-ington, usa.
association for computational lin-guistics..jacob bergdahl.
2020. no, gpt-3 is not superintelligent.
it’s not tricking humans, and it‘s not pretending to bestupid.
medium website..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, a. c¸ elikyilmaz, and yejin choi.
2019. comet: commonsense transformers for au-tomatic knowledge graph construction.
in acl..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in association for computa-domain questions.
tional linguistics (acl)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..kelvin guu, kenton lee, z. tung, panupong pasu-pat, and ming-wei chang.
2020. realm: retrieval-arxiv,augmented language model pre-training.
abs/2002.08909..mandar joshi, eunsol choi, daniel weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1601–1611, van-couver, canada.
association for computational lin-guistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, matthew kelcey,jacob devlin, kenton lee, kristina n. toutanova,llion jones, ming-wei chang, andrew dai, jakob.
uszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
transactions of the association of compu-tational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020a.
bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..patrick lewis, pontus stenetorp, and sebastian riedel.
2020b.
question and answer test-train overlap inopen-domain question answering datasets..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proc.
of naacl..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..nina poerner, ulli waltinger, and hinrich sch¨utze.
2019. bert is not a knowledge base (yet): fac-tual knowledge vs. name-based reasoning in unsu-pervised qa.
arxiv, abs/1911.03681..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 784–789.
association for computational linguistics..3250a *ethics / impact statement.
our used data is from open source datasets, includ-ing naturalquestions3, triviaqa4, webquestion5and squad26.
we split the development set of thenaturalquestions, triviaqa and squad2 and thetest set of webquestions into two subsets to serveas a new development set and a new test set.
andwe extract several subsets from squad2 to serveas our new datasets.
there is no additional datacollection process..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in nat-ural language processing, pages 2383–2392.
asso-ciation for computational linguistics..adam roberts, colin raffel, and noam shazeer.
2020.how much knowledge can you pack into the param-in proceedings of theeters of a language model?
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 5418–5426,online.
association for computational linguistics..robyn speer and catherine havasi.
2012. repre-senting general relational knowledge in concept-in proceedings of the eighth internationalnet 5.conference on language resources and evaluation(lrec’12), pages 3679–3686, istanbul, turkey.
eu-ropean language resources association (elra)..alon talmor, yanai elazar, yoav goldberg, andjonathan berant.
2020. olmpics-on what languagemodel pre-training captures.
transactions of the as-sociation for computational linguistics, 8:743–758..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..cunxiang wang, shuailong liang, y. jin, yilong wang,x. zhu, and y. zhang.
2020. semeval-2020 task 4:in se-commonsense validation and explanation.
meval..cunxiang wang, shuailong liang, yue zhang, xiao-nan li, and tian gao.
2019. does it make sense?
and why?
a pilot study for sense making and ex-planation.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 4020–4026, florence, italy.
association forcomputational linguistics..rowan zellers, yonatan bisk, roy schwartz, and yejinchoi.
2018. swag: a large-scale adversarial datasetfor grounded commonsense inference.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing (emnlp)..xuhui zhou, yue zhang, leyang cui, and dandanevaluating commonsense in pre-huang.
2020.in proceedings of thetrained language models.
aaai conference on artiﬁcial intelligence, vol-ume 34, pages 9733–9740..triviaqa/.
sempre/.
3https://ai.google.com/research/.
naturalquestions.
4http://nlp.cs.washington.edu/.
5https://nlp.stanford.edu/software/.
6https://rajpurkar.github.io/.
squad-explorer/.
3251