hiddencut: simple data augmentation for natural languageunderstanding with better generalization.
jiaao chen, dinghan shen1, weizhu chen1, diyi yanggeorgia institute of technology, 1microsoft dynamics 365 ai{jchen896,dyang888}@gatech.edu{dishen,wzchen}@microsoft.com.
abstract.
fine-tuning large pre-trained models with task-speciﬁc data has achieved great success innlp.
however, it has been demonstrated thatthe majority of information within the self-attention networks is redundant and not uti-lized effectively during the ﬁne-tuning stage.
this leads to inferior results when general-izing the obtained models to out-of-domaindistributions.
to this end, we propose asimple yet effective data augmentation tech-to better regularize thenique, hiddencut,model and encourage it to learn more gen-eralizable features.
speciﬁcally, contiguousspans within the hidden space are dynamicallyand strategically dropped during training.
ex-periments show that our hiddencut methodoutperforms the state-of-the-art augmentationmethods on the glue benchmark, and con-sistently exhibits superior generalization per-formances on out-of-distribution and challeng-ing counterexamples.
we have publicly re-leased our code at https://github.com/gt-salt/hiddencut..1.introduction.
fine-tuning large-scale pre-trained language mod-els (plms) has become a dominant paradigm in thenatural language processing community, achievingstate-of-the-art performances in a wide range ofnatural language processing tasks (devlin et al.,2019; liu et al., 2019; yang et al., 2019a; joshiet al., 2019; sun et al., 2019; clark et al., 2019;lewis et al., 2020; bao et al., 2020; he et al., 2020;raffel et al., 2020).
despite the great success, dueto the huge gap between the number of model pa-rameters and that of task-speciﬁc data available, themajority of the information within the multi-layerself-attention networks is typically redundant andineffectively utilized for downstream tasks (guoet al., 2020; gordon et al., 2020; dalvi et al., 2020)..as a result, after task-speciﬁc ﬁne-tuning, mod-els are very likely to overﬁt and make predictionsbased on spurious patterns (tu et al., 2020; kaushiket al., 2020), making them less generalizable to out-of-domain distributions (zhu et al., 2019; jianget al., 2019; aghajanyan et al., 2020)..in order to improve the generalization abilitiesof over-parameterized models with limited amountof task-speciﬁc data, various regularization ap-proaches have been proposed, such as adversarialtraining that injects label-preserving perturbationsin the input space (zhu et al., 2019; liu et al., 2020;jiang et al., 2019), generating augmented data viacarefully-designed rules (mccoy et al., 2019; xieet al., 2020; andreas, 2020; shen et al., 2020), andannotating counterfactual examples (goyal et al.,2019; kaushik et al., 2020).
despite substantialimprovements, these methods often require signif-icant computational and memory overhead (zhuet al., 2019; liu et al., 2020; jiang et al., 2019; xieet al., 2020) or human annotations (goyal et al.,2019; kaushik et al., 2020)..in this work, to alleviate the above issues, werethink the simple and commonly-used regulariza-tion technique—dropout (srivastava et al., 2014)—in pre-trained transformer models (vaswani et al.,2017).
with multiple self-attention heads in trans-formers, dropout converts some hidden units to ze-ros in a random and independent manner.
althoughplms have already been equipped with the dropoutregularization, they still suffer from inferior perfor-mances when it comes to out-of-distribution cases(tu et al., 2020; kaushik et al., 2020).
the un-derlying reasons are two-fold: (1) the linguisticrelations among words in a sentence is ignoredwhile dropping the hidden units randomly.
in real-ity, these masked features could be easily inferredfrom surrounding unmasked hidden units with theself-attention networks.
therefore, redundant in-formation still exists and gets passed to the upper.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4380–4390august1–6,2021.©2021associationforcomputationallinguistics4380layers.
(2) the standard dropout assumes that ev-ery hidden unit is equally important with the ran-dom sampling procedure, failing to characterizethe different roles these features play in distincttasks.
as a result, the learned representations arenot generalized enough while applied to other dataand tasks.
to drop the information more effec-tively, shen et al.
(2020) recently introduce cutoffto remove tokens/features/spans in the input space.
even though models will not see the removed infor-mation during training, examples with large noisemay be generated when key clues for predictionsare completely removed from the input..to overcome these limitations, we propose a sim-ple yet effective data augmentation method, hid-dencut, to regularize plms during the ﬁne-tuningstage.
speciﬁcally, the approach is based on the lin-guistic intuition that hidden representations of ad-jacent words are more likely to contain similar andredundant information.
hiddencut drops hiddenunits more structurally by masking the whole hid-den information of contiguous spans of tokens afterevery encoding layer.
this would encourage mod-els to fully utilize all the task-related information,instead of learning spurious patterns during train-ing.
to make the dropping process more efﬁcient,we dynamically and strategically select the infor-mative spans to drop by introducing an attention-based mechanism.
by performing hiddencut in thehidden space, the impact of dropped informationis only mitigated rather than completely removed,avoiding injecting too much noise to the input.
wefurther apply a jensen-shannon divergence con-sistency regularization between the original andthese augmented examples to model the consistentrelations between them..to demonstrate the effectiveness of our methods,we conduct experiments to compare our hiddencutwith previous state-of-the-art data augmentationmethod on 8 natural language understanding tasksfrom the glue (wang et al., 2018) benchmarkfor in-distribution evaluations, and 5 challengingdatasets that cover single-sentence tasks, similarityand paraphrase tasks and inference tasks for out-of-distribution evaluations.
we further perform abla-tion studies to investigate the impact of differentselecting strategies on hiddencut’s effectiveness.
results show that our method consistently outper-forms baselines, especially on out-of-distributionand challenging counterexamples.
to sum up, ourcontributions are:.
• we propose a simple data augmentationmethod, hiddencut, to regularize plms dur-ing ﬁne-tuning by cutting contiguous spans ofrepresentations in the hidden space..• we explore and design different strategic sam-pling techniques to dynamically and adap-tively construct the set of spans to be cut..• we demonstrate the effectiveness of hidden-cut through extensive experiments on both in-distribution and out-of-distribution datasets..2 related work.
2.1 adversarial training.
adversarial training methods usually regularizemodels through applying perturbations to the inputor hidden space (szegedy et al., 2013; goodfel-low et al., 2014; madry et al., 2017) with addi-tional forward-backward passes, which inﬂuencethe model’s predictions and conﬁdence withoutchanging human judgements.
adversarial-basedapproaches have been actively applied to variousnlp tasks in order to improve models’ robustnessand generalization abilities, such as sentence clas-siﬁcation (miyato et al., 2017), machine readingcomprehension (mrc) (wang and bansal, 2018)and natural language inference (nli) tasks (nieet al., 2020).
despite its success, adversarial train-ing often requires extensive computation overheadto calculate the perturbation directions (shafahiet al., 2019; zhang et al., 2019a).
in contrast, ourhiddencut adds perturbations in the hidden spacein a more efﬁcient way that does not require extracomputations as the designed perturbations can bedirectly derived from self-attentions..2.2 data augmentation.
another line of work to improve the model ro-bustness is to directly design data augmentationmethods to enrich the original training set such ascreating syntactically-rich examples (mccoy et al.,2019; min et al., 2020) with speciﬁc rules, crowd-sourcing counterfactual augmentation to avoidlearning spurious features (goyal et al., 2019;kaushik et al., 2020), or combining examples in thedataset to increase compositional generalizabilities(jia and liang, 2016; andreas, 2020; chen et al.,2020b,a).
however, they either require careful de-sign (mccoy et al., 2019; andreas, 2020) to inferlabels for generated data or extensive human anno-tations (goyal et al., 2019; kaushik et al., 2020),.
4381which makes them hard to generalize to differenttasks/datasets.
recently shen et al.
(2020) intro-duce a set of cutoff augmentation which directlycreates partial views to augment the training in amore task-agnostic way.
inspired by these priorwork, our hiddencut aims at improving models’generalization abilities to out-of-distribution vialinguistic-informed strategically dropping spans ofhidden information in transformers..2.3 dropout-based regularization.
variations of dropout (srivastava et al., 2014) havebeen proposed to regularize neural models by in-jecting noise through dropping certain informationso that models do not overﬁt training data.
how-ever, the major efforts have been put to convo-lutional neural networks and trimmed for struc-tures in images recently such as droppath (lars-son et al., 2017), dropblock (ghiasi et al., 2018),dropcluster (chen et al., 2020c) and autodropout(pham and le, 2021).
in contrast, our work takes acloser look at transformer-based models and intro-duces hiddencut for natural language understand-ing tasks.
hiddencut is closely related to drop-block (ghiasi et al., 2018), which drops contigu-ous regions from a feature map.
however, differentfrom images, hidden dimensions in plms that con-tain syntactic/semantic information for nlp tasksare more closely related (e.g., ner and pos in-formation), and simply dropping spans of featuresin certain hidden dimensions might still lead toinformation redundancy..3 hiddencut approach.
to regularize transformer models in a more struc-tural and efﬁcient manner, in this section, we intro-duce a simple yet effective data augmentation tech-nique, hiddencut, that reforms dropout to cuttingcontiguous spans of hidden representations aftereach transformer layer (section 3.1).
intuitively,the proposed approach encourages the models tofully utilize all the hidden information within theself-attention networks.
furthermore, we proposean attention-based mechanism to strategically andjudiciously determine the speciﬁc spans to cut (sec-tion 3.2).
the schematic diagram of hiddencut,applied to the transformer architecture (and its com-parison to dropout) are shown in figure 1..3.1 hiddencut.
for an input sequence s = {w0, w1, ..., wl} withl tokens associated with a label y, we employ apre-trained transformer model f1:m (·) with m lay-ers like roberta (liu et al., 2019) to encode thetext into hidden representations.
thereafter, aninference network g(·) is learned on top of the pre-trained models to predict the corresponding labels.
in the hidden space, after layer m, every word wiin the input sequence is encoded into a d dimen-sional vector hmi ∈ rd and the whole sequencecould be viewed as a hidden matrix hm ∈ rl×d.
with multiple self-attention heads in the trans-former layers, it is found that there is extensiveredundant information across hmi ∈ h that are lin-guistically related (dalvi et al., 2020) (e.g., wordsthat share similar semantic meanings).
as a result,the removed information from the standard dropoutoperation may be easily inferred from the remain-ing unmasked hidden units.
the resulting modelmight easily overﬁt to certain high-frequency fea-tures without utilizing all the important task-relatedinformation in the hidden space (especially whentask-related data is limited).
moreover, the modelalso suffers from poor generalization ability whilebeing applied to out-of-distribution cases..inspired by ghiasi et al.
(2018); shen et al.
(2020), we propose to improve the dropout reg-ularization in transformer models by creating aug-mented training examples through hiddencut,which drops a contiguous span of hidden informa-tion encoded in every layer, as shown in figure 1(c).
mathematically, in every layer m, a span ofhidden vectors, s ∈ rl×d, with length l = αl inthe hidden matrix hm ∈ rl×d are converted to 0,and the corresponding attention masks are adjustedto 0, where α is a pre-deﬁned hyper-parameter in-dicating the dropping extent of hiddencut.
afterbeing encoded and hiddencut through all the hiddenlayers in pre-trained encoders, augmented trainingdata f hiddencut(s) is created for learning the infer-ence network g(·) to predict task labels..3.2 strategic sampling.
different tasks rely on learning distinct sets of in-formation from the input to predict the correspond-ing task labels.
performing hiddencut randomlymight be inefﬁcient especially when most of thedropping happens at task-unrelated spans, whichfails to effectively regularize model to take advan-tage of all the task-related features.
to this end, we.
4382figure 1: illustration of the differences between dropout (a) and hiddencut (b), and the position of hiddencutin transformer layers (c).
a sentence in the hidden space can be viewed as a l × d matrix where l is the lengthof the sentence and d is the number of hidden dimensions.
the cells in blue represent that they are masked.
dropout masks random independent units in the matrix while our hiddencut selects and masks a whole span ofhidden representations based on attention weights received in the current layer.
in our experiments, we performhiddencut after the feed-forward network in every transformer layer..propose to select the spans to be cut dynamicallyand strategically in every layer.
in other words, wemask the most informative span of hidden repre-sentations in one layer to force models to discoverother useful clues to make predictions instead ofrelying on a small set of spurious patterns..attention-based sampling strategy the mostdirect way is to deﬁne the set of tokens to be cutby utilizing attention weights assigned to tokensin the self-attention layers (kovaleva et al., 2019).
intuitively, we can drop the spans of hidden rep-resentations that are assigned high attentions bythe transformer layers.
as a result, the informationredundancy is alleviated and models would be en-courage to attend to other important information.
speciﬁcally, we ﬁrst derive the average attention foreach token, ai, from the attention weights matrixa ∈ rp ×l×l after self-attention layers, wherep is the number of attention heads and l is thesequence length:.
ai =.
(cid:80)p.j ((cid:80)lk a[j][k][i])p...we then sample the start token hi for hiddencutfrom the set that contains top βl tokens with higheraverage attention weights (β is a pre-deﬁned param-eter).
then hiddencut is performed to mask thehidden representations between hi and hi+l.
notethat the salient sets are different across differentlayers and updated throughout the training..other sampling strategies we also exploreother widely used word importance discovery meth-.
ods to ﬁnd a set of tokens to be strategically cut byhiddencut, including:.
• random: all spans of tokens are viewed asequally important, thus are randomly cut..• lime (ribeiro et al., 2016) deﬁnes the impor-tance of tokens by examining the locally faith-fulness where weights of tokens are assignedby classiﬁers trained with sentences whosewords are randomly removed.
we utilizedlime on top of a svm classiﬁer to pre-deﬁnea ﬁxed set of tokens to be cut..• gem (yang et al., 2019b) utilizes orthogo-nal basis to calculate the novelty scores thatmeasure the new semantic meaning in tokens,signiﬁcance scores that estimate the alignmentbetween the semantic meaning of tokens andthe sentence-level meaning, and the unique-ness scores that examine the uniqueness of thesemantic meaning of tokens.
we compute thegem scores using the hidden representationsat every layer to generate the set of tokens tobe cut, which are updated during training..• gradient (baehrens et al., 2010): we deﬁnethe set of tokens to be cut based on the rank-ings of the absolute values of gradients they re-ceived at every layer in the backward-passing.
this set would be updated during training..43833.3 objectives.
1.
(s), ..., f hiddencutn.during training, for an input text sequence s witha label y, we generate n augmented examples{f hiddencut(s)} through perform-ing hiddencut in pre-trained encoder f (·).
thewhole model g(f (·)) is then trained though sev-eral objectives including general classiﬁcation loss(lori and laug) on data-label pairs and consistencyregularization (ljs) (miyato et al., 2017, 2018;clark et al., 2018; xie et al., 2019; shen et al.,2020) across different augmentations:.
lori = ce(g(f (s)), y).
laug =.
ce(g(f hiddencuti.
(s)), y).
ljs =.
kl[p(y|g(f hiddencut.
i.
(s))||pavg].
(cid:88).
n(cid:88).
n.where ce and kl represent the cross-entropy lossand kl-divergence respectively.
pavg stands for theaverage predictions across the original text and allthe augmented examples..combining these three losses, our overall objec-.
tive function is:.
l = lori + γlaug + ηljs.
where γ and η are the weights used to balance thecontributions of learning from the original data andaugmented data..4 experiments.
4.1 datasets.
we conducted experiments on both in-distributiondatasets and out-of-distribution datasets to demon-strate the effectiveness of our proposed hiddencut..in-distribution datasets we mainly trained andevaluated our methods on the widely-used gluebenchmark (wang et al., 2018) which covers awide range of natural language understanding tasks:single-sentence tasks including: (i) stanford senti-ment treebank (sst-2) which predict the sentimentof movie reviews to be positive or negative, and (ii)corpus of linguistic acceptability (cola) whichpredict whether a sentence is linguistically accept-able or not; similarity and paraphrase tasks includ-ing (i) quora question pairs (qqp) which predictwhether two question are paraphrases, (ii) semantictextual similarity benchmark (sts-b) which pre-dict the similarity ratings between two sentences,and (iii) microsoft research paraphrase corpus.
(mrpc) which predict whether two given sen-tences are semantically equivalent; inference tasksincluding (i) multi-genre natural language in-ference (mnli) which classiﬁed the relationshipsbetween two sentences into entailment, contradic-tion, or neutral, (ii) question natural languageinference (qnli) which predict whether a givensentence is the correct answer to a given question,and (iii) recognizing textual entailment (rte)which predict whether the entailment relation holdsbetween two sentences.
accuracy was used as theevaluation metric for most of the datasets exceptthat matthews correlation was used for cola andspearman correlation was utilized for sts-b..out-of-distribution datasets to demonstratethe generalization abilities of our proposed meth-ods, we directly evaluated on 5 different out-of-distribution challenging sets, using the models thatare ﬁne-tuned on glue benchmark datasets:.
• single sentence tasks: models ﬁne-tunedfrom sst-2 are directly evaluated on tworecent challenging sentiment classiﬁcation(gardnerimdb contrast setdatasets:et al., 2020) including 588 examples andimdb counterfactually augmented dataset(kaushik et al., 2020) including 733 examples.
both of them were constructed by asking nlpresearchers (gardner et al., 2020) or amazonmechanical turkers (kaushik et al., 2020) tomake minor edits to examples in the originalimdb dataset (maas et al., 2011) so that thesentiment labels change while the major con-tents keep the same..• similarity and paraphrase tasks: mod-els ﬁne-tuned from qqp are directly evalu-ated on the recently introduced challengingparaphrase dataset paws-qqp (zhang et al.,2019b) that has 669 test cases.
paws-qqpcontains sentence pairs with high word over-lap but different semantic meanings createdvia word-swapping and back-translation fromthe original qqp dataset..• inference tasks: models ﬁne-tuned frommnli are directly evaluated on two challeng-ing nli sets: hans (mccoy et al., 2019)with 30,000 test cases and adversarial nli(a1 dev sets) (nie et al., 2020) including1,000 test cases.
the former one was con-structed by using syntactic rules (lexical over-lap, subsequence and constituent) to generate.
4384method mnli qnli qqp rte sst-2 mrpc cola sts-b avg.
roberta-base.
87.6alum 88.188.288.288.488.2.token cutofffeature cutoffspan cutoffhiddencut †.
92.893.193.193.393.493.7.
91.992.091.992.092.092.0.
78.780.281.281.682.383.4.
94.895.395.195.395.495.8.
89.590.991.190.791.192.0.
63.663.664.163.664.766.2.
91.291.191.291.291.291.3.
86.386.887.087.087.387.8.table 1: in-distribution evaluation results on the dev sets of the glue benchmark.
† means our proposed method..method.
roberta-basespan cutoffhiddencut †.
single-sentence.
imdb-cont..imdb-cad.
similarity&paraphrasepaws-qqp.
inferencehans advnli (a1).
84.685.587.8.
88.489.290.4.
38.438.841.5.
67.868.471.2.
31.231.132.8.table 2: out-of-distribution evaluation results on 5 different challenging sets.
† means our proposed method.
forall the datasets, we did not use their training sets to further ﬁne-tune the derived models from glue..non-entailment examples with high premise-hypothesis overlap from mnli.
the latter onewas created by adversarial human-and-model-in-the-loop framework (nie et al., 2020) to cre-ate hard examples based on bert-large mod-els(devlin et al., 2019) pre-trained on snli(bowman et al., 2015) and mnli..4.2 baselines.
we compare our methods with several baselines:.
• roberta (liu et al., 2019) is used as ourbase model.
note that roberta is regular-ized with dropout during ﬁne-tuning..• alum (liu et al., 2020) is the state-of-the-art adversarial training method for neural lan-guage models, which regularizes ﬁne-tuningvia perturbations in the embedding space..• cutoff (shen et al., 2020) is a recent data aug-mentation for natural language understandingtasks by removing information in the inputspace, including three variations: token cutoff,feature cutoff, and span cutoff..4.3.implementation details.
we used the roberta-base model (liu et al.,2019) to initialize all the methods.
note that hid-dencut is agnostic to different types of pre-trainedmodels.
we followed liu et al.
(2019) to set thelinear decay scheduler with a warmup ratio of 0.06for training.
the maximum learning rate was se-lected from {5e − 6, 8e − 6, 1e − 5, 2e − 5} and the.
max number of training epochs was set to be either5 or 10. all these hyper-parameters are shared forall the models.
the hiddencut ratio α was set 0.1after a grid search from {0.05, 0.1, 0.2, 0.3, 0.4}.
the selecting ratio β in the important sets sam-pling process was set 0.4 after a grid search from{0.1, 0.2, 0.4, 0.6}.
the weights γ and η in our ob-jective function were both 1. all the experimentswere performed using a geforce rtx 2080ti..4.4 results on in-distribution datasets.
based on table 1, we observed that, compared toroberta-base with only dropout regularization,alum with perturbations in the embedding spacethrough adversarial training has better results onmost of these glue tasks.
however, the extraadditional backward passes to determine the per-turbation directions in alum can bring in signiﬁ-cantly more computational and memory overhead.
by masking different types of input during train-ing, cutoff increased the performances while beingmore computationally efﬁcient..in contrast to span cutoff, hiddencut not onlyintroduced zero additional computation cost, butalso demonstrated stronger performances on 7 outof 8 glue tasks, especially when the size of train-ing set is small (e.g., an increase of 1.1 on rteand 1.5 on cola).
moreover, hiddencut achievedthe best average result compared to previous state-of-the-art baselines.
these in-distribution improve-ments indicated that, by strategically dropping con-tiguous spans in the hidden space, hiddencut not.
4385only helps pre-trained models utilize hidden infor-mation in a more effective way, but also injects lessnoise during the augmentation process compared tocutoff, e.g., span cutoff might bring in additionalnoises for cola (which aims to judge whetherinput sentences being linguistically acceptable ornot) when one span in the input is removed, sinceit might change the labels..4.5 results on out-of-distribution datasets.
to validate the better generalizability of hidden-cut, we tested our models trained on sst-2, qqpand mnli directly on 5 out-of-distribution/out-of-domain challenging sets in zero-shot settings.
as mentioned earlier, these out-of-distribution setswere either constructed with in-domain/out-of-domain data and further edited by human to makethem harder, or generated by rules that exploitedspurious correlations such as lexical overlap, whichmade them challenging to most existing models..as shown in table 2, span cutoff slightly im-proved the performances compared to robertaby adding extra regularizations through creatingrestricted input.
hiddencut signiﬁcantly outper-formed both roberta and span cutoff.
for exam-ple, it outperformed span cutoff.
by 2.3%(87.8%85.5%) on imdb-conts, 2.7%(41.5% vs.vs.38.8%) on paws-qqp, and 2.8%(71.2% vs 68.4%)on hans consistently.
these superior resultsdemonstrated that, by dynamically and strategi-cally dropping contiguous span of hidden represen-tations, hiddencut was able to better utilize all theimportant task-related information which improvedthe model generalization to out-of-distribution andchallenging adversary examples..4.6 ablation studies.
this section presents our ablation studies on differ-ent sampling strategies and the effect of importanthyper-parameters in hiddencut..4.6.1 sampling strategies in hiddencut.
we compared different ways to cut hidden repre-sentations (dropblock (ghiasi et al., 2018) whichrandomly dropped spans in certain random hid-den dimensions instead of the whole hidden space)and different sampling strategies for hiddencut de-scribed in section 3.2 (including random, lime(ribeiro et al., 2016), gem (yang et al., 2019b),gradient (yeh et al., 2019), attention) based onthe performances on sst-2 and qnli.
for thesestrategies, we also experimented with a reverse set.
strategy.
sst-2 qnli.
limelime-r.robertadropblock.
94.895.4random 95.495.295.3gem 95.595.195.695.195.894.6.gem-rgradientgradient-rattentionattention-r.92.893.293.593.193.293.493.293.693.493.793.4.table 3: the performances on sst-2 and qnli withdifferent strategies when dropping information in thehidden space.
different sampling strategies combinedwith hiddencut are presented.
“-r” means samplingoutside the set to be cut given by these strategies..denoted by “-r” where we sampled outside theimportant set given by above strategies..from table 3, we observed that (i) samplingfrom important sets resulted in better performancesthan random sampling.
sampling outside the de-ﬁned importance sets usually led to inferior per-formances.
these highlights the importance ofstrategically selecting spans to drop.
(ii) samplingfrom dynamic sets sampled by their probabilitiesoften outperformed sampling from predeﬁned ﬁxedsets (lime), indicating the effectiveness of dynam-ically adjusting the sampling sets during training.
(iii) the attention-based strategy outperformed allother sampling strategies, demonstrating the effec-tiveness of our proposed sampling strategies forhiddencut.
(iv) completely dropping out the spansof hidden representations generated better resultsthan only removing certain dimensions in the hid-den space, which further validated the beneﬁt ofhiddencut over dropblock in natural language un-derstanding tasks..4.6.2 the effect of hiddencut ratios.
the length of spans that are dropped by hidden-cut is an important hyper-parameter, which is con-trolled by the hiddencut ratio α and the lengthof input sentences.
α could also be interpretedas the extent of perturbations added to the hid-den space.
we presented the results of hidden-cut on mnli with a set of different α including{0.05, 0.1, 0.2, 0.3, 0.4} in table 5. hiddencutachieved the best performance with α = 0.1, and.
4386methodrobertahiddencutrobertahiddencutrobertahiddencutrobertahiddencut.
<s><s><s><s><s><s><s><s>.
ii.wouldwouldthe moviethe moviewouldwouldthe moviethe movie.
ii.original and counterfactual sentencesrateratebecamebecamerateratebecamebecame.
starsstarsandandstarsstarsslightlyslightly.
outoutmoremoreoutoutmoremore.
88moremore88onlyonly.
ofofintriguingintriguingofofintriguingintriguing.
</s></s>.
</s></s>.
1010</s></s>2020</s></s>.
predictionpositivepositivepositivepositivepositivenegativepositivenegative.
table 4: visualization of the attention weights at the last layer in models.
the sentences in the ﬁrst section arefrom imdb with positive labels and the sentences in the second section is constructed by changing ratings ordiminishing via qualiﬁers (kaushik et al., 2020) to ﬂip their corresponding labels.
deeper blue represents thatthose tokens receive higher attention weights..α.
0.05.mnli.
88.07.
0.188.23.
0.2.
0.3.
0.4.β.
0.1.
0.2.
88.13.
88.07.
87.64.sst-2.
95.18.
95.30.
0.495.76.
0.6.
95.46.table 5: performances on mnli with different hidden-cut ratio α, which controls the length of span to cut inthe hidden space..table 6: performances on sst-2 with different sam-pling ratio β, which controls the size of important tokenset from which hiddencut would sample..the performance gradually decreased with higher αsince larger noise might be introduced when drop-ping more hidden information.
this suggested theimportance of balancing the trade-off between ap-plying proper perturbations to regularize modelsand injecting potential noises..4.6.3 the effect of sampling ratios.
the number of words that are considered importantand selected by hiddencut is also an inﬂuentialhyper-parameter controlled by the sampling ratio βand the length of input sentences.
as shown in ta-ble 6, we compared the performances on sst-2 byadopting different β including {0.1, 0.2, 0.4, 0.6}.
when β is too small, the number of words in the im-portant sets is limited, which might lead hiddencutto consistently drop certain hidden spans during theentire training process.
the low diversities reducethe improvements over baselines.
when β is toolarge, the important sets might cover all the wordsexcept stop words in sentences.
as a result, theattention-based strategy actually became randomsampling, which led to lower gains over baselines.
the best performance was achieved when β = 0.4,indicating a reasonable trade-off between diversi-ties and efﬁciencies..terfactual examples in table 4. we observed thatroberta only assigned higher attention weightson certain tokens such as “8 stars”, “intriguing”and especially the end special token “</s>”, whilelargely ignored other context tokens that were alsoimportant to make the correct predictions such asscale descriptions (e.g., “out of 10”) and qualiﬁerwords (e.g., “more and more”).
this was probablybecause words like “8 stars” and “intriguing” werehighly correlated with positive label and robertamight overﬁt such patterns without probable reg-ularization.
as a result, when the scale of ratings(e.g., from “10” to “20”) or the qualiﬁer wordschanged (e.g., from “more and more” to “onlyslightly more”), roberta still predicted the labelas positive even when the groundtruth is negative.
with hiddencut, models mitigated the impact oftokens with higher attention weights and were en-couraged to utilize all the related information.
sothe attention weights in hiddencut were more uni-formly distributed, which helped models make thecorrect predictions for out-of-distribution counter-factual examples.
taken together, hiddencut helpsimprove model’s generalizability by facilitating itto learn from more task-related information..4.7 visualization of attentions.
5 conclusion.
to further demonstrate the effectiveness of hid-dencut, we visualize the attention weights that thespecial start token (“<s>”) assigns to other tokens atthe last layer, via several examples and their coun-.
in this work, we introduced a simple yet effec-tive data augmentation technique, hiddencut, toimprove model robustness on a wide range ofnatural language understanding tasks by drop-.
4387ping contiguous spans of hidden representationsin the hidden space directed by strategic attention-based sampling strategies.
through hiddencut,transformer models are encouraged to make useof all the task-related information during train-ing rather than only relying on certain spuriousclues.
through extensive experiments on in-distribution datasets (glue benchmarks) and out-of-distribution datasets (challenging counterexam-ples), hiddencut consistently and signiﬁcantly out-performed state-of-the-art baselines, and demon-strated superior generalization performances..acknowledgment.
we would like to thank the anonymous reviewers,and the members of georgia tech salt group fortheir feedback.
this work is supported in part bygrants from amazon and salesforce..references.
armen aghajanyan, akshat shrivastava, anchit gupta,naman goyal, luke zettlemoyer, and sonal gupta.
2020. better ﬁne-tuning by reducing representa-tional collapse.
arxiv preprint arxiv:2008.03156..jacob andreas.
2020. good-enough compositionaldata augmentation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7556–7566, online.
associationfor computational linguistics..david baehrens, timon schroeter, stefan harmel-ing, motoaki kawanabe, katja hansen, and klaus-robert müller.
2010. how to explain individual clas-siﬁcation decisions.
journal of machine learningresearch, 11(61):1803–1831..hangbo bao, li dong, furu wei, wenhui wang,nan yang, xiaodong liu, yu wang, songhaopiao,jianfeng gao, ming zhou, et al.
2020.unilmv2: pseudo-masked language models for uni-ﬁed language model pre-training.
arxiv preprintarxiv:2002.12804..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..jiaao chen, zichao yang, and diyi yang.
2020b.
mix-text: linguistically-informed interpolation of hid-den space for semi-supervised text classiﬁcation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2147–2157, online.
association for computational lin-guistics..liyan chen, p. gautier, and sergül aydöre.
2020c.
dropcluster: a structured dropout for convolutionalnetworks.
arxiv, abs/2002.02997..kevin clark, minh-thang luong, quoc v le, andchristopher d manning.
2019. electra: pre-trainingtext encoders as discriminators rather than genera-tors.
in international conference on learning rep-resentations..kevin clark, minh-thang luong, christopher d. man-semi-supervisedin.
ning, and quoc v. le.
2018.sequence modeling with cross-view training.
emnlp..fahim dalvi, hassan sajjad, nadir durrani, andyonatan belinkov.
2020. analyzing redundancy inin proceedings ofpretrained transformer models.
the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 4908–4926, online.
association for computational lin-guistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt..matt gardner, yoav artzi, victoria basmov, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hannaneh hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f. liu, phoebe mulcaire, qiang ning, sameersingh, noah a. smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating models’ local decision boundariesin findings of the associationvia contrast sets.
for computational linguistics: emnlp 2020, pages1307–1323, online.
association for computationallinguistics..g. ghiasi, tsung-yi lin, and quoc v. le.
2018. drop-block: a regularization method for convolutionalnetworks.
in neurips..ian j goodfellow, jonathon shlens, and christianszegedy.
2014. explaining and harnessing adversar-ial examples.
arxiv preprint arxiv:1412.6572..jiaao chen, zhenghui wang, ran tian, zichao yang,and diyi yang.
2020a.
local additivity based dataaugmentation for semi-supervised ner.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1241–1251..mitchell gordon, kevin duh, and nicholas andrews.
2020. compressing bert: studying the effects ofweight pruning on transfer learning.
in proceedingsof the 5th workshop on representation learning fornlp, pages 143–155, online.
association for com-putational linguistics..4388yash goyal, ziyan wu, jan ernst, dhruv batra, deviparikh, and stefan lee.
2019. counterfactual visualexplanations.
in icml, pages 2376–2384..demi guo, alexander m. rush, and yoon kim.
2020.parameter-efﬁcient transfer learning with diff prun-ing..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2020. deberta: decoding-enhancedarxiv preprintbert with disentangled attention.
arxiv:2006.03654..robin jia and percy liang.
2016. data recombinationfor neural semantic parsing.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages12–22, berlin, germany.
association for computa-tional linguistics..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2019.smart: robust and efﬁcient ﬁne-tuning for pre-language models through princi-trained naturalarxiv preprintpled regularized optimization.
arxiv:1911.03437..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2019.improving pre-training by representingspanbert:and predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..aleksander madry, aleksandar makelov, ludwigschmidt, dimitris tsipras, and adrian vladu.
2017.towards deep learning models resistant to adversar-ial attacks.
arxiv preprint arxiv:1706.06083..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticheuristics in natural language inference.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3428–3448,florence, italy.
association for computational lin-guistics..junghyun min, r. thomas mccoy, dipanjan das,emily pitler, and tal linzen.
2020.syntacticdata augmentation increases robustness to inferenceheuristics.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 2339–2352, online.
association for computa-tional linguistics..divyansh kaushik, eduard hovy, and zachary lipton.
2020. learning the difference that makes a differ-ence with counterfactually-augmented data.
in inter-national conference on learning representations..takeru miyato, andrew m. dai, and ian j. good-fellow.
2017. adversarial training methods forsemi-supervised text classiﬁcation.
arxiv: machinelearning..olga kovaleva, alexey romanov, anna rogers, andanna rumshisky.
2019. revealing the dark secretsof bert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4365–4374, hong kong, china.
association forcomputational linguistics..gustav.
larsson, m. maire,.
and gregoryshakhnarovich.
2017. fractalnet: ultra-deep neuralnetworks without residuals.
arxiv, abs/1605.07648..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2020.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
scl..xiaodong liu, hao cheng, pengcheng he, weizhuchen, yu wang, hoifung poon, and jianfeng gao.
2020. adversarial training for large neural languagemodels.
arxiv preprint arxiv:2004.08994..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019..takeru miyato, shin-ichi maeda, masanori koyama,and shin ishii.
2018. virtual adversarial training:a regularization method for supervised and semi-ieee transactions on pat-supervised learning.
tern analysis and machine intelligence, 41(8):1979–1993..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..hieu pham and quoc v. le.
2021. autodropout: learn-ing dropout patterns to regularize deep networks..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
"why should i trust you?
": explain-ing the predictions of any classiﬁer.
in proceedingsof the 22nd acm sigkdd international conference.
4389zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019a.
xlnet: generalized autoregressive pretrain-in advances ining for language understanding.
neural information processing systems, pages 5754–5764..ziyi yang, chenguang zhu, and weizhu chen.
2019b.
parameter-free sentence embedding via orthogonalin proceedings of the 2019 conference onbasis.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages638–648, hong kong, china.
association for com-putational linguistics..chih-kuan yeh, cheng-yu hsieh, arun sai suggala,david i. inouye, and pradeep ravikumar.
2019. onthe (in)ﬁdelity and sensitivity of explanations.
inneurips..dinghuai zhang, tianyuan zhang, yiping lu, zhanx-ing zhu, and bin dong.
2019a.
you only propagateonce: painless adversarial training using maximalprinciple.
arxiv preprint arxiv:1905.00877, 2(3)..yuan zhang, jason baldridge, and luheng he.
2019b.
paws: paraphrase adversaries from word scram-in proceedings of the 2019 conference ofbling.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages1298–1308, minneapolis, minnesota.
associationfor computational linguistics..chen zhu, yu cheng, zhe gan, siqi sun, tom gold-stein, and jingjing liu.
2019. freelb: enhanced ad-versarial training for natural language understanding.
in international conference on learning represen-tations..on knowledge discovery and data mining, kdd’16, page 1135–1144, new york, ny, usa.
asso-ciation for computing machinery..ali shafahi, mahyar najibi, mohammad amin ghi-asi, zheng xu, john dickerson, christoph studer,larry s davis, gavin taylor, and tom goldstein.
in advances2019. adversarial training for free!
in neural information processing systems, pages3358–3369..dinghan shen, m. zheng, y. shen, yanru qu, andw. chen.
2020. a simple but tough-to-beat data aug-mentation approach for natural language understand-ing and generation.
arxiv, abs/2009.13818..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
journal of machine learning re-search, 15(56):1929–1958..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019. ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..christian szegedy, wojciech zaremba, ilya sutskever,joan bruna, dumitru erhan, ian goodfellow, andintriguing properties of neuralrob fergus.
2013.networks.
arxiv preprint arxiv:1312.6199..lifu tu, garima lalwani, spandana gella, and he he.
2020. an empirical study on robustness to spuri-ous correlations using pre-trained language models.
transactions of the association for computationallinguistics, 8:621–633..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, l. kaiser,and illia polosukhin.
2017. attention is all you need.
arxiv, abs/1706.03762..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2018.glue: a multi-task benchmark and analysis plat-form for natural language understanding.
in black-boxnlp@emnlp..yicheng wang and mohit bansal.
2018. robust ma-chine comprehension models via adversarial train-ing.
in proceedings of the 2018 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 2 (short papers), pages 575–581, neworleans, louisiana.
association for computationallinguistics..qizhe xie, zihang dai, eduard hovy, minh-thang lu-ong, and quoc v le.
2019. unsupervised data aug-mentation for consistency training.
arxiv preprintarxiv:1904.12848..qizhe xie, zihang dai, eduard hovy, minh-thang lu-ong, and quoc v. le.
2020. unsupervised data aug-mentation for consistency training..4390