improving encoder by auxiliary supervision tasks for table-to-textgeneration.
liang li1,2, can ma1∗, yinliang yue1∗ and dayong hu31institute of information engineering, chinese academy of sciences, beijing, china2school of cyber security, university of chinese academy of sciences, beijing, china3heilongjiang network space research center, harbin 150010, china{liliang, macan, yueyinliang}@iie.ac.cnsuperhudayong@163.com.
abstract.
table-to-text generation aims at automaticallygenerating natural text to help people conve-niently obtain salient information in tables.
al-though neural models for table-to-text haveachieved remarkable progress, some problemsare still overlooked.
previous methods can-not deduce the factual results from the entity’s(player or team) performance and the relationsbetween entities.
to solve this issue, we ﬁrstbuild an entity graph from the input tables andintroduce a reasoning module to perform rea-soning on the graph.
moreover, there are dif-ferent relations (e.g., the numeric size relationand the importance relation) between recordsin different dimensions.
and these relationsmay contribute to the data-to-text generation.
however, it is hard for a vanilla encoder tocapture these.
consequently, we propose toutilize two auxiliary tasks, number ranking(nr) and importance ranking (ir), to super-vise the encoder to capture the different rela-tions.
experimental results on rotowireand rw-fg show that our method not only hasa good generalization but also outperforms pre-vious methods on several metrics: bleu, con-tent selection, content ordering..1.introduction.
table-to-text generation is an essential task for textgeneration from structured data.
it aims at auto-matically producing descriptive natural languagetext to help people obtain the salient informationfrom the tables.
over the past several years, neu-ral text generation methods have made signiﬁcantprogress on this task.
lebret et al.
(2016); wisemanet al.
(2017); bao et al.
(2018) view the input tableas a record sequence and model it as a machinetranslation task.
to generate text containing moresalient and well-organized facts, sha et al.
(2018);moryossef et al.
(2019); trisedya et al.
(2020);.
∗∗corresponding authors: can ma, yinliang yue.
figure 1: (a) are tables in rotowire.
(b) is a human-written summary related to (a).
factual results thatneed be reasoned are in red.
(c) is the entity graph con-structing from the input tables..bai et al.
(2020) explicitly model content selectionand planning.
to better represent tables, liu et al.
(2018); nema et al.
(2018); gong et al.
(2019) ex-plicitly model the structure of a table from multiplelevels or different dimensions..figure 1 (a) contains basketball game statisticaltables from rotowire (wiseman et al., 2017), abenchmark of nba basketball games.
as can beseen, each entity (player or team) takes one rowin the corresponding table.
moreover, each rowcomprises several records of different types, whichdescribe the entity’s performance in different as-pects.
in terms of generating a summary from thesetables, it is necessary to make reasoning to obtainsome factual results from the entities’ performanceand the relationships between entities.
for instance,when humans describe the tables in figure 1 (a),they usually give some factual results, such as “theboston celtics dominated the visiting new yorkknicks” or “isaiah thomas was huge for boston...”.
these results need to be reasoned from the entities’performance and the relationships between entities.
therefore, it is necessary to give the model the rea-soning ability.
however, previous methods do notexplicitly model this ability..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5979–5989august1–6,2021.©2021associationforcomputationallinguistics5979the boston celtics dominated the visiting new york knicks, 115 -87, on friday night at td garden … isaiah thomas was huge for boston ( 4 -4 ) as he led the way offensively with 29 points on 9-of-17 shooting, in only 28 minutes... avery bradley and marcus smart both filled the stat sheet.
smart finished with 12 points, 10 assists, six rebounds and three steals, while bradley notched 15 points, 10 rebounds, two assists and four steals… kristaps porzingis was the high-point manwith 14 points, along with six rebounds and two blocks , in 23 minutes.
derrick rose added 11 points , six assists and four rebounds , while the only other player to tally double-digits for the knicks was justin holiday……teamplayer…vsplayforplaywithcelticsknicksi.
thomasm.
smarta.
johnsonc.
anthonyk.
porzingisk.
o'quinnname   minptsastreb…marcus smart3412106…amir johnson21213…kelly olynyk301937…………………isaiah thomas282943…name   ptsastrebtov…celtics115235315…knicks87195725…(a)(b)(c)name   minptsastreb…carmelo anthony121212…kristaps porzingis231416…joakim noah229210…………………kyle o'quinn3203…numerical tables mean most records in thesetables are numerical and are very common.
for in-stance, 86.82% of the records and almost 86.49%of the column types are numeric in rotowire.
we observe that there are different relations be-tween records in different dimensions.
for exam-ple, there are two kinds of relations in numericaltables.
the ﬁrst one is numerical size relation inthe column dimension, i.e., in the same type col-umn.
the other is the relative importance relationin the row dimension.
it refers to the relative impor-tance of different types of records, which are in thesame row, to the entity that they belong to.
on theone hand, these relations may contribute to table-to-text generation.
let us take figure 1 (a) as anexample.
i.thomas’s score is 29, which is higherthan other records in the column pts.
and he hasthree rebounds, which is lower than most otherrecords in the column reb.
therefore, humansare more likely to describe his scores rather thanhis rebounds when summarizing his performance.
on the other hand, a vanilla encoder may not ef-fectively capture the relations existing in differentdimensions without any auxiliary supervision..we employ a hierarchical encoder, which com-prises a record encoder and a reasoning module,to encode the input tables from record level androw level.
speciﬁcally, inspired by gong et al.
(2019), the record encoder utilizes two cascadedself-attention modules to encode the table fromthe column and the row dimension, respectively.
moreover, to endow the model with the reason-ing ability, we ﬁrst build an entity graph on the rowlevel according to the relations between players andteams.
and then, we introduce a reasoning moduleto perform reasoning on the graph.
furthermore,we utilize different auxiliary tasks to help the en-coder capture the different relations among records.
more speciﬁcally, two auxiliary tasks named num-ber ranking (nr) and importance ranking (ir) areproposed to supervise the learning of the differentparts of the record encoder, respectively..we conducted experiments on rotowire andrw-fg(wang, 2019) to verify the effectiveness ofthe proposed approach.
the experimental resultsdemonstrate that it is necessary to enable the modelthe reasoning ability.
moreover, the proposed twoauxiliary tasks can improve the data-to-text model’sperformance without introducing extra parameters.
furthermore, the results also show our method notonly has a good generalization but also outperforms.
previous methods on bleu, content selection,and content ordering metrics..2 related work.
recently, neural models have been the mainstreamfor table-to-text generation and obtained impres-sive results.
early works on table-to-text gener-ation regard it as a distinct machine translationtask and view a structured table as a record se-quence (lebret et al., 2016; wiseman et al., 2017;bao et al., 2018).
most recent works are inspiredby the traditional methods for data-to-text gener-ation and introduce explicit content selection andplanning to improve the results (sha et al., 2018;puduppully et al., 2019b; moryossef et al., 2019;trisedya et al., 2020; bai et al., 2020), and theyobtain training labels by aligning the input tableswith related summaries.
however, this alignmentmay introduce additional errors.
some works at-tempt to use additional knowledge to improve thequality of the generated text.
nie et al.
(2018) uti-lize pre-executed symbolic operations on the inputtable in a sequence-to-sequence model to improvethe ﬁdelity of neural table-to-text generation.
chenet al.
(2019) introduce the background knowledgeof the entity in the table to improve results..in addition to introducing external knowledge,some works learn better representation for the tableby explicitly modeling the table’s structure.
liuet al.
(2018) propose a structure-aware seq2seq ar-chitecture, which incorporates the ﬁled informationas the additional inputs to the table encoder.
someworks (bao et al., 2018; nema et al., 2018; jainet al., 2018) model the table’s representation fromthe row and column levels, and utilize the dual at-tention decoder to generate text.
gong et al.
(2019)introduce the historical data for each table and uti-lize a self-attention-based hierarchical encoder onthree dimensions (row, column, and time) to enrichthe table’s representation.
furthermore, liu et al.
(2019) propose three auxiliary supervision tasks(sequence labeling, text auto-encoding, and multi-label classiﬁcation) to help the encoder capture amore accurate semantic representation of the tables..gong et al.
(2020) also explicitly model the rela-tions between the numeric records.
they pretrain amulti-layer transformer encoder to obtain records’contextual numerical value representations.
more-over, when training the data-to-text model, theyreplace the record’s token embedding with its con-.
5980figure 2: an overview of our method.
rel and rfg denote record embedding layer and record fusion gate,respectively..textual representation from the pre-trained model.
differently, our number ranking task is trainedwith the data-to-text model and can supervise themodel actively to capture the numeric size relationwithout introducing extra parameters..3 approach.
3.1 record encoder.
each input instance consists of three different ta-bles t 1, t 2, t 3, containing records about players’performance in the home team, players’ perfor-mance in the visiting team, and the team’s overallperformance.
each cell in the table is regarded as arecord.
inspired by gong et al.
(2019), we utilizetwo self-attention modules to model each record’scontexts from the column and the row dimension,respectively.
after that, we obtain the fusion repre-sentation for records by the record fusion gate..record embedding following previous work(wiseman et al., 2017), we utilize four tuples torepresent each record r. the four tuples include:entity r.e (the name of team or player, such ascarmelo anthony), type r.t (e.g., pts) and valuer.v as well as feature r.f (e.g., home or visiting)which indicates whether a player or a team com-pete in home court or not.
and we utilize 1-layermlp to encode the embeddings of each record’sfour types of information into a dense vector remb,i,j = relu(w e[ri,j.e; ri,j.t; ri,j.v; ri,j.f ] + be),remb.
i,j.
where i, j denote a record in the table of i-th rowand j-th column, [; ] denotes the vector concatena-tion, w e and be are trainable parameters..column-wise encoder to capture the numericsize relation between records, we adopt a self-attention module to model record in the contextof other records in the same column and obtain thecolumn dimension representation vector rcol.
i,j as:.
i,j,i(cid:48) ∝ exp(w colαcol2.tanh(w col.1 [rembi,j.
; remb.
i(cid:48),j ]) (1).
r(cid:88).
˜rcoli,j =.
i,j,i(cid:48)rembαcoli(cid:48),j.i(cid:48)=1,i(cid:48)(cid:54)=i3 [˜rcol.
i,j = w colrcol.
i,j ; rembi,j.
].
(2).
(3).
and w colwhere w colare trainable parame-3ters, r represents the number of rows in the table..1 , w col.2.row-wise encoder considering the size relationcaptured by the column-wise encoder (ce) mayhelp the learning of importance relation on rowlevel, we have the column-wise encoder and therow-wise encoder (re) in series (as shown in fig-ure 2).
in other words, the input of re is rcoli,j ratherthan remb.
we use another self-attention module,similar to the ce, to obtain the row dimension rep-resentation rrowi,j.
for records..i,j.
record fusion gate the record representationsfrom different dimensions contribute differently in.
5981number rankingimportance rankingrelcererfgenirow-wise encodercolumn-wiseencoder……nameminptsastrebstlmarcus smart34121063amir johnson222131kelly olynyk3019372avery bradley32152103isaiah thomas2829431i.
thomasimportancescoresisaiah thomas41255number rankingpts122191529self attentionselfattentionα!,#$α!,#$α!,#$α!,#$α!,#$122191529122191529191529<s>2829431282943129428<s>2829431gatedgat…thebos<s>decoderreasoning moduleentity node initialization……×"gatedgatdual attentioncelticsrecord encoderreﬂecting the record’s information.
therefore, weutilize a fusion gate to combine the two dimensionrepresentations adaptively(gong et al., 2019).
first,we concatenate the two dimension representationsof a record and utilize an mlp to obtain a generalrepresentation for it as rgeni,j .
then, we comparethe column dimension representation with rgentoi,jobtain its important score:.
i,j ∝ exp(w fscol.
2 tanh(w f.1 [rgen.
i.; rcol.
i,j ])).
(4).
1 and w f.where w f2 are trainable parameters.
equally, we obtain the important score srowfori,jthe row dimension representation rrowi,j .
finally,we obtain the fused record representation rfi,j byweighted sum scoli,j + srowi,j rrowi,j .
the fusedrecord representations {rfi,j}r,ci=1,j=1 will be usedas the input of the text decoder..i,j rcol.
3.2 reasoning module.
as mentioned in section 1, we observe some fac-tual results in text that require reasoning from theentities’ performance and the relationships betweenthem.
therefore, it is necessary to enable modelthe reasoning ability.
to achieve this, we primar-ily build an entity graph according to the entities’relationships in input tables, as shown in figure1 (c).
and then, we leverage graph neural net-works (gnn) to perform reasoning.
following, wedescribe the details of the reasoning process..primarily, we obtain the initialized representa-tion for each entity in tables by the entity nodeinitialization module (eni).
considering that dif-ferent records in the same row may not contributethe same, we combine them dynamically by atten-tion mechanism.
we ﬁrst compute a general rep-resentation vector egenfor the entity ei, which isgiven by mean-pooling over the same row recordsrfi,1, rfi,c.
then we compare each recordin the i-th row with egenand obtain the initializedientity representation e0.
i,2, ..., rf.
i by weighted sum:.
i.
2 tanh(w r.1 [egen.
i.; rf.
i,j])).
i,j ∝ exp(w rαrj=c(cid:88).
e0i =.
i,jrfαr.
i,j.
j=1.
(5).
(6).
after obtaining the initial representations of en-tities, we adopt graph neural networks to propagateentity node information to their neighbors.
inspiredby gat(velickovic et al., 2018), we use multi-head.
attention to measure the relatedness between targetentity node ei and its neighbor nodes at layer l:.
i,j = m ultiheadattention(el−1αl.
i., el−1j.
).
(7).
where j ∈ ni and ni means the neighbor nodesset of target entity ei..the neighbor entities include information that isnot relevant to the target entity.
therefore, we mod-ify the way the information ﬂow in gat.
explicitly,we incorporate gate mechanisms into informationaggregation to ﬁlter out noises from neighbor nodesand extract useful information, which we namegatedgat.
the representation eli of ei at layer l iscalculated as follows:.
i = gatelel˜eli = elu (.
i ∗ el−1i + (1 − gatel(cid:88)i,jel−1αlj.
).
i) ∗ ˜eli.j∈ni.
gatel.
i = sigmoid(w l[el−1.
; ˜el.
i]).
i.
(8).
(9).
(10).
where w l is a learnable parameter.
the entities’i }rrepresentations {eli=1 at the last layer l are em-ployed in text decoder..3.3 decoder with dual attention.
to make use of record-level and row-level seman-tics information, we adopt the dual attention mech-anism.
speciﬁcally, at decoding step t, the inputof the lstm unit is the embedding of the pre-viously predicted word yt−1.
and given the de-coder state dt, we ﬁrst calculate the row-level atten-tion βt,i, which is based on the similarity betweenthe decoder state dt and the entities’ representa-tions {eli=1.
then we compute the record-levelattention αt,i over all the record representations{rfi,j which are normalized among records inthe same row.
finally, we fuse these two-levelattention and obtain the context representation as:.
i,j}r,c.
i }r.(cid:48).
α.t,i,j = αt,iβt,i,jr(cid:88).
c(cid:88).
cdt =.
i=1.
j=1.
(cid:48).
α.t,i,jri,j.
(11).
(12).
given a reference output {yi}t.i=1, we use thecross-entropy loss as the objective function of table-to-text generation:.
llm = −.
pθ(yt|y1:t−1; cdt ).
(13).
t(cid:88).
i=1.
59823.4 auxiliary supervision task.
liu et al.
(2019) have shown that a single encoderwithout any auxiliary assistant may not be effec-tive to capture the accurate semantic representation.
inspired by this, we propose two auxiliary tasks,number ranking (nr) and importance ranking(ir), to help the column-wise encoder and therow-wise encoder capture the size relation and therelative importance relation among records respec-tively..number rankingin practice, many tablesmainly comprise numeric records.
different fromtext-type content, the numerical content containsless semantic information but the size relation.
thesize relation means the value of a record is largeror smaller than others, and it plays an essential rolein records selection.
for example, humans tend tofocus on the highest scores or the fewest faults ina basketball game table.
therefore, it is necessaryto incorporate size relation into record represen-tation.
to achieve this, we propose an auxiliarysupervision task named number ranking (nr) tosupervise the learning of the column-wise encoder.
as shown in figure 2 top, we take a list of recordsin column pts to illustrate how it works.
speciﬁ-cally, we regard the pts column of the table as anout-of-order set of records c = r1, r2, ..., rr, andthe goal is to generate a sequence of record pointersin descending order according to their value.
weadopt the pointer networks (vinyals et al., 2015)to solve this problem and the output of column-wise encoder rcol(we omitted the indices on thecolumn dimension) as its input.
let z = z1, ..., zrdenote the sequence of the ranked records’ indices.
each zk points to an input record and is between 1and r. as shown in figure 2, we use an lstm asthe decoder.
the m eanp ooling({ri}ri=1) is usedas the initialization of the ﬁrst hidden state of thedecoder.
at each decoding step t, we calculate adistribution over the input records:.
i.ht = lst m (ht−1, rcolzt−1)t,i ∝ exp(wnr[ht; rcolpn]).
i.
(14).
(15).
where wnr is a trainable parameter, and pnt,i de-notes the probability that the output points to therecord ri at step t. we take the cross-entropy lossfor this task:.
lnr = −.
log pn.
i,zi.
(16).
c(cid:88).
r(cid:88).
j=1.
i=1.
importance ranking when people describe aplayer’s performance in a basketball game, theytend to focus on his relatively important record anddescribe these ﬁrstly.
consequently, we introducethe importance ranking task (ir) to supervise therow-wise encoder to capture the relative impor-tance relations between records in the same row.
this task’s input is a sequence record in the samerow, and the output is a sequence of records indescending order of the records’ importance.
weemploy a pointer network similar to the one usedin the number ranking task to model this task.
however, different from the records in the samecolumn, these in the same row cannot be directlycompared as they represent different meanings.
toaddress this issue, we take the rank of each recordin the column as an importance indicator.
figure2 left bottom shows an example of calculating theimportance scores for records in the last row of thetable..j.the input of the decoder is the output of therow-wise encoder {rrow}rj=1.
and the output isthe ascending order of the input, according to therecords’ importance scores.
let pst,j denote theprobability of pointing to record rj at decodingstep t, the loss function for this task is:.
lir = −.
log ps.
j,zj.
(17).
r(cid:88).
c(cid:88).
i=1.
j=1.
3.5 loss function and training.
these two tasks are trained together with the table-to-text task, and the overall objective function con-sists of three parts:.
l = llm + λ1lnr + λ2lir.
(18).
where λ1 and λ2 are tunable hyper-parameters..4 experiment.
4.1 dataset and evaluation metrics.
we conduct experiments on both rotowire andrw-fg datasets.
they all comprise pairs of nbabasketball game statistics and summaries.
thereare two main differences between rotowire andrw-fg.
the ﬁrst is the team statistic table in latercontaining more numeric records.
the other isrw-fg removes the unsupported sentences by theinput tables.
we use the ofﬁcial training, develop-ment, and test splits for both datasets, which are3,398/727/728 and 5,232/1,125/1,119, respectively..5983model.
goldtempcc (wiseman et al., 2017)ncp (puduppully et al., 2019a)ncp (our implementation)ent (puduppully et al., 2019b)hetd (gong et al., 2019)du (gong et al., 2020)duv (gong et al., 2020)ours.
templateentncpncp + tr (wang, 2019)ours.
rg.
rotowirecs.
co.bleu.
#23.3154.2323.7234.2831.9530.1131.4729.4226.9432.73.
51.8035.6935.9937.4938.08.p% p% r% f1% dld%94.7999.9474.8087.4786.9692.9691.4688.0587.4593.14.
10026.9929.4934.1833.1338.6736.0938.1940.7340.80.
10014.9215.4218.5817.4720.1720.8622.1423.3225.30.
100-31.5240.9939.0643.0941.2143.1844.3947.16.
98.8993.7294.2195.794.75.
23.9839.0443.3142.9042.72.
31.0343.5748.5248.9249.04.
10.2517.523.4624.4725.23.
10058.1636.1851.2247.5948.5148.0149.6648.7855.88rw-fg43.9649.2955.1556.9157.56.
1008.4614.1916.5015.2616.1216.8516.1215.9217.96.
12.0921.2323.8624.4124.52.table 1: automatic evaluation results on the test set.
on rotowire, our results are obtained with puduppullyet al.
(2019a)’s updated models.
the others are from corresponding papers.
on rw-fg, the baselines’ results aretaken from wang (2019), and we evaluate directly using the code released by wang (2019)..following previous works, we use bleu and threeextractive evaluation metrics, relation generation(rg), content selection (cs), and content order-ing (co) (wiseman et al., 2017) to evaluate thetable-to-text results.
more speciﬁcally, rg mea-sures the content ﬁdelity of generated text, cs mea-sures how well the generated text matches the ref-erence in selecting which records to generate, andco measures the ability on context planning.
werefer the readers to wiseman et al.
(2017)’s paperfor more detailed information on these extractivemetrics..we apply accuracy (acc) and normalized dam-erau levenshtein distance (dld) (brill and moore,2000) to evaluate the two auxiliary supervisiontasks.
accuracy measures the percentage of recordsequences for which their absolute positions arecorrectly predicted (logeswaran et al., 2018)..4.2.implementation details.
to make a fair comparison, we follow the conﬁg-urations in (puduppully et al., 2019a; gong et al.,2019).
for the table-to-text model, we set wordembedding and lstm decoder hidden size as 600.we set gatedgat’s layer as 2 and the numbers ofheads as 2. we employ a two-layer lstm de-coder with input feeding during text generation..we apply dropout at a rate 0.3. for text decod-ing, we use bptt and set the truncate size to 100.we set the beam size to 5 during inference.
forthe two auxiliary tasks, we employ two one-layerlstm as the decoder and set the lstm decoderhidden size as 600, respectively.
we adjust λ1 be-tween 0.8 and 1.0, λ2 between 0.2-0.4. finally, weset them to 0.9 and 0.25 on rotowire, 1.0 and0.4 on rw-fg.
for inferring, we use the greedysearch algorithm.
all experiments are conductedon an nvidia tesla v100.
code of our modelcan be found at https://github.com/liang8qi/data2textwithauxiliarysupervision..4.3 baselines.
we compare our method with several strong base-lines, including:.
• temp (wiseman et al., 2017) is a template-based method.
we refer the readers to thispaper for more detailed information on tem-plates..• cc (wiseman et al., 2017) is a standardencoder-decoder system with conditional copymechanism..• ncp (puduppully et al., 2019a) and ncp +tr (wang, 2019) are two conditional copymodels with the explicit content planning..5984development.
nr task.
ir task.
acc% dld% acc% dld%27.2946.4391.4389.3687.7486.56.
7.7287.8184.07.
66.1592.6390.44test.
nr task.
ir task.
acc% dld% acc% dld%26.9346.5491.2689.1587.6886.54.
66.0292.4790.40.
7.7187.6083.98.model.
originalseparateours.
model.
originalseparateours.
table 2: automatic evaluation of the number rank-ing(nr) task and the importance ranking (ir) task onrotowire development and test datasets..the latter improves ncp by introducing a ta-ble restructure loss..• ent (puduppully et al., 2019b) is a methodthat creates entity-speciﬁc representations andgenerates text using hierarchical attention overthe input table and entity memory..• hetd (gong et al., 2019) is a method mod-eling table from three different dimensions(row, column and, time)..• du & duv (gong et al., 2020):.
the dubrings the sense of value comparison into con-tent planning.
furthermore, duv introducescontent plan veriﬁcation into du..4.4 main results.
automatic evaluation our results on the twotest datasets are summarized in table 1. for ro-towire, compared with previous neural models,our method achieves state-of-the-art results on con-tent selection (cs), content ordering (co), andbleu.
more speciﬁcally, compared with the pre-vious best neural models, we obtain more than 4improvement on cs-p and achieve the best resultson cs-r. this implies our method can generatetext that contains more salient records.
comparedwith ncp, du, and duv, our method scores thehighest on co, even without explicitly modelingcontent selection and planning.
this indicates thatour model can better organize the records whengenerating a summary for the input tables.
weconsider there are two main reasons.
the ﬁrst isthat our reasoning module can learn a better en-tity representation on row level.
the other is thatour proposed two auxiliary tasks can supervise therecord encoder to learn a number-aware and rela-tive importance-aware record representation.
as aresult, the data-to-text model can make good con-.
model.
our model- series- rm+ ne+ ne & ie+ nr+ ir+ nr & ir.
rg.
cs.
co.#.
p% f1% dld%.
bleu.
34.37 90.03 44.34 23.64 17.3132.74 91.56 41.42 21.52 17.1933.91 89.58 43.71 23.04 16.9838.41 92.28 44.22 23.16 16.2332.85 92.68 45.33 24.49 16.8132.47 93.76 45.93 24.29 18.5635.30 92.65 43.34 22.04 17.4733.93 92.40 46.13 25.28 17.68.table 3: ablation results for evaluating each compo-nent’s contribution on rotowire development set..tent planning by considering the entity’s perfor-mance and the relative importance of the record..as shown in table 1, the results on rw-fg fol-low a pattern similar to rotowire.
we noticethat all models perform better on rw-fg than onrotowire.
we consider that the improvementcomes from the puriﬁcation of data in rw-fg.
wang (2019) removes the sentences that are notsupported by the input tables, which reduces thenoise in the text and improves the dataset’s quality.
due to this, we can obtain more accurate contentplanning labels from the dataset to train the mod-els (ncp, ncp+tr) that explicitly model contentplanning and lead to better performance.
therefore,ncp outperforms ent on rw-fg.
however, thepuriﬁcation may make the task easier because somesentences that do not be supported by the tables di-rectly but can be obtained by reasoning may also beremoved.
this may weaken the reasoning moduleof our model.
nevertheless, we still outperform thecompared baselines..table 2 shows our model’s performance, whichis trained together with the two auxiliary tasks onthe two auxiliary tasks.
we compare it with twobaselines.
the ﬁrst is original, which denotes amethod that takes the input record sequence as theoutputs.
moreover, we separately train our modelon the two auxiliary tasks, denoted as separate.
as a result, our model achieves comparable perfor-mance to separate and is much better than orig-inal, even only using the greedy search at testing.
the results indicate that the two auxiliary tasks canhelp the record encoder capture the size relationand relative importance relation among records..ablation study first, we examine the effect ofchanges in the model structure on the results.
fromtable 3, our model means our data-to-text modelwithout two auxiliary tasks.
we change the con-nection mode between the column-wise encoder.
5985model.
ncpncp+henc+ nr+ nr&ir.
rg.
co.csf1% dld%.
bleu.
#.
p%86.67 31.46 40.02 18.73 15.6187.22 27.36 43.55 22.42 15.8389.41 28.54 44.56 23.50 16.1790.96 27.71 46.29 24.23 16.29.table 4: generalization study on rotowire develop-ment set.
henc denotes our hierarchical encoder withreasoning module..gold -11.33 -14.00 14.89 12.88ncpenthetd 0.225.78ours.
sup contra gram cohere concise15.33-8.00 -20.898.67-7.11-5.11-1.333.562.00.
11.339.78 -10.44-6.00 -1.11 -3.333.56 -5.334.221.78.table 5: human evaluation results..(ce) and the row-wise encoder (re) to parallelfrom series (- series).
moreover, we replace thereasoning module with a row-level encoder withthe content selection gate (- rm), which is pro-posed by puduppully et al.
(2019a).
according tothe results, the serial connection and the reasoningmodule contribute to the overall performance be-cause bleu, cs, and co drop signiﬁcantly aftersubtracting them from the full model..furthermore, we investigate the impact of thetwo auxiliary tasks on table-to-text generation.
ta-ble 3 shows that both number ranking (nr) andimportance ranking (ir) tasks can improve ourbasic model.
this indicates that it is necessary toexplicitly model the size relation and relative im-portance relation between records.
we notice thatthe model’s performance is degraded on cs-f1 andco when only the ir task is introduced.
on the onehand, we believe this is because the modeling ofrelative importance relation in the row dimensionbetween records depends heavily on its size rela-tion in the column dimension.
on the other hand,the ce cannot accurately capture the size relationbetween records without direct supervision..finally, we compare the method that introducesadditional feature vectors of the ranking of numberand relative importance to record embedding withthe two auxiliary tasks.
speciﬁcally, we ﬁrst intro-duce the embedding of ranking of the number (+ne) and further add the embedding of the relativeimportance of records (+ ie).
as shown in the thirdsection in table 3, the ne only improves the modelon rg.
moreover when the ie is incorporated, themodel achieves better performance on almost allmetrics.
however, the improvement is not as sig-niﬁcant as the auxiliary tasks.
we believe it maybe a better way to effectively capture the accuratesemantic representation by introducing auxiliarysupervision tasks than adding feature vectors di-rectly..generalization study our method can be ap-plied to the existing works, especially those thatexplicitly model content selection and planning(ncp, duv), to improve their performance.
toexam our method’s generalization, we combine ourmethod with ncp and conduct experiments on therotowire development set.
the results are sum-marized in table 4. first, we use the released codeto retrain the ncp model.
and then, we replacethe ncp’s content selection encoder with our hier-archical encoder.
as can be seen, our hierarchicalencoder with the reasoning module improves thencp model on almost all evaluation metrics.
more-over, we train the model with the proposed twoauxiliary supervision tasks.
the performance ofthe model is further improved.
this indicates thatour method has a good generalization, as it can beeasily adapted to other methods and improve theirperformance..human evaluation to examine whether humanjudgments corroborate improvements in automaticevaluation metrics, we conducted a human eval-uation.
three graduate students with basketballbackground knowledge and good english readingability were invited to conduct the evaluation.
wecompared our best performing model against gold,ncp, ent, and hetd.
speciﬁcally, we randomlyselected 30 games from the test set, and each gameis rated by three workers.
for each game, we ar-ranged every 5-tuple of summaries into ten pairs.
given each pair, the participants were asked tochoose which one is better according to ﬁve crite-ria: supporting (does the summary contain moresupported facts?
), contradicting (does the summarycontain more contradicting facts?
), grammatical-ity (is the summary ﬂuent and grammatical?
), co-herence (do the sentences, in summary, follow acoherent discourse?
), and conciseness (does thesummary contain less redundant information andrepetitions?).
following previous work (pudup-pully et al., 2019a), we calculated a model’s scorefor each criterion as the difference between the per-.
5986centage of times when the model is chosen as thebest and the percentage of times when the model ischosen as the worst..the results are summarized in table 5. as canbe seen, the gold texts have signiﬁcant advantagesin contradicting, grammaticality, coherence, andconciseness.
compared with other neural methods,our method receives the highest scores in coherenceand grammaticality.
this implies that our methodcan generate texts that contain well-organized facts.
though the ent model outperforms our model incontradicting and conciseness, our method can beeasily applied to it, which we leave for future work..5 conclusion.
in this work, we mainly make two contributions.
the ﬁrst one is we introduce a reasoning moduleinto a hierarchical table encoder, which enables themodel reasoning ability.
moreover, we present toutilize the different auxiliary supervision tasks tohelp the encoder capture the different relations be-tween records.
in detail, the number ranking (nr)task is proposed to supervise the column-wise en-coder to model the numeric size relation betweenrecords in the same column.
and the importanceranking (ir) task helps the row-wise encodercapture the relative importance between recordsin the same row.
experimental results conductedon rotowire and rw-fg datasets demonstratethe effectiveness of our method.
furthermore, wemigrate our method to the ncp model and signiﬁ-cantly improve its performance on rowtowire.
this indicates that our proposed method has a goodgeneralization..references.
yang bai, ziran li, ning ding, ying shen, and hai-tao zheng.
2020.infobox-to-text generation withtree-like planning based attention network.
in pro-ceedings of the twenty-ninth international jointconference on artiﬁcial intelligence, ijcai 2020,pages 3773–3779.
ijcai.org..eric brill and robert c. moore.
2000. an improved er-ror model for noisy channel spelling correction.
inproceedings of the 38th annual meeting of the as-sociation for computational linguistics, pages 286–293, hong kong.
association for computationallinguistics..shuang chen, jinpeng wang, xiaocheng feng, fengjiang, bing qin, and chin-yew lin.
2019. enhanc-ing neural data-to-text generation models with ex-in proceedings ofternal background knowledge.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3022–3032, hong kong,china.
association for computational linguistics..heng gong, wei bi, xiaocheng feng, bing qin, xi-aojiang liu, and ting liu.
2020. enhancing contentplanning for table-to-text generation with data under-standing and veriﬁcation.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 2905–2914, online.
association for computa-tional linguistics..heng gong, xiaocheng feng, bing qin, and ting liu.
2019. table-to-text generation with effective hier-archical encoder on three dimensions (row, columnand time).
in proceedings of the 2019 conferenceon empirical methods in natural language process-ing and the 9th international joint conference onnatural language processing (emnlp-ijcnlp),pages 3143–3152, hong kong, china.
associationfor computational linguistics..parag jain, anirban laha, karthik sankaranarayanan,preksha nema, mitesh m. khapra, and shreyasshetty.
2018. a mixed hierarchical attention basedencoder-decoder approach for standard table summa-rization.
in proceedings of the 2018 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 622–627,new orleans, louisiana.
association for computa-tional linguistics..r´emi lebret, david grangier, and michael auli.
2016.neural text generation from structured data within proceed-application to the biography domain.
ings of the 2016 conference on empirical methodsin natural language processing, pages 1203–1213,austin, texas.
association for computational lin-guistics..junwei bao, duyu tang, nan duan, zhao yan, yuan-hua lv, ming zhou, and tiejun zhao.
2018. table-to-text: describing table region with natural lan-in proceedings of the thirty-second aaaiguage.
conference on artiﬁcial intelligence,(aaai-18),the 30th innovative applications of artiﬁcial intel-ligence (iaai-18), and the 8th aaai symposiumon educational advances in artiﬁcial intelligence(eaai-18), new orleans, louisiana, usa, february2-7, 2018, pages 5020–5027.
aaai press..tianyu liu, fuli luo, qiaolin xia, shuming ma,baobao chang, and zhifang sui.
2019. hierarchi-cal encoder with auxiliary supervision for neuraltable-to-text generation: learning better represen-in the thirty-third aaai con-tation for tables.
ference on artiﬁcial intelligence, aaai 2019, thethirty-first innovative applications of artiﬁcial in-telligence conference, iaai 2019, the ninth aaaisymposium on educational advances in artiﬁcialintelligence, eaai 2019, honolulu, hawaii, usa,.
5987january 27 - february 1, 2019, pages 6786–6793.
aaai press..tianyu liu, kexiang wang, lei sha, baobao chang,and zhifang sui.
2018. table-to-text generationin proceed-by structure-aware seq2seq learning.
ings of the thirty-second aaai conference on ar-tiﬁcial intelligence, (aaai-18), the 30th innovativeapplications of artiﬁcial intelligence (iaai-18), andthe 8th aaai symposium on educational advancesin artiﬁcial intelligence (eaai-18), new orleans,louisiana, usa, february 2-7, 2018, pages 4881–4888. aaai press..lajanugen logeswaran, honglak lee, and dragomir r.radev.
2018. sentence ordering and coherence mod-eling using recurrent neural networks.
in proceed-ings of the thirty-second aaai conference on ar-tiﬁcial intelligence, (aaai-18), the 30th innovativeapplications of artiﬁcial intelligence (iaai-18), andthe 8th aaai symposium on educational advancesin artiﬁcial intelligence (eaai-18), new orleans,louisiana, usa, february 2-7, 2018, pages 5285–5292. aaai press..amit moryossef, yoav goldberg, and ido dagan.
2019.step-by-step: separating planning from realizationin neural data-to-text generation.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 2267–2277, minneapolis,minnesota.
association for computational linguis-tics..preksha nema, shreyas shetty, parag jain, anirbanlaha, karthik sankaranarayanan, and mitesh m.khapra.
2018. generating descriptions from struc-tured data using a bifocal attention mechanism andgated orthogonalization.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 1539–1550, new orleans, louisiana.
associ-ation for computational linguistics..feng nie, jinpeng wang, jin-ge yao, rong pan,and chin-yew lin.
2018. operation-guided neu-ral networks for high ﬁdelity data-to-text genera-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 3879–3889, brussels, belgium.
associationfor computational linguistics..ratish puduppully, li dong, and mirella lapata.
2019a.
data-to-text generation with content selec-in the thirty-third aaai con-tion and planning.
ference on artiﬁcial intelligence, aaai 2019, thethirty-first innovative applications of artiﬁcial in-telligence conference, iaai 2019, the ninth aaaisymposium on educational advances in artiﬁcialintelligence, eaai 2019, honolulu, hawaii, usa,january 27 - february 1, 2019, pages 6908–6915.
aaai press..ratish puduppully, li dong, and mirella lapata.
2019b.
data-to-text generation with entity model-in proceedings of the 57th annual meetinging.
of the association for computational linguistics,pages 2023–2035, florence, italy.
association forcomputational linguistics..lei sha, lili mou, tianyu liu, pascal poupart, su-jian li, baobao chang, and zhifang sui.
2018.order-planning neural text generation from struc-in proceedings of the thirty-secondtured data.
aaai conference on artiﬁcial intelligence, (aaai-18), the 30th innovative applications of artiﬁcial in-telligence (iaai-18), and the 8th aaai symposiumon educational advances in artiﬁcial intelligence(eaai-18), new orleans, louisiana, usa, february2-7, 2018, pages 5414–5421.
aaai press..bayu distiawan trisedya, jianzhong qi, and ruizhang.
2020. sentence generation for entity descrip-in aaai, pagestion with content-plan attention.
9057–9064..petar velickovic, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in 6th inter-2018. graph attention networks.
national conference on learning representations,iclr 2018, vancouver, bc, canada, april 30 - may3, 2018, conference track proceedings.
openre-view.net..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems 28: annual con-ference on neural information processing systems2015, december 7-12, 2015, montreal, quebec,canada, pages 2692–2700..hongmin wang.
2019. revisiting challenges in data-to-text generation with fact grounding.
in proceed-ings of the 12th international conference on nat-ural language generation, pages 311–322, tokyo,japan.
association for computational linguistics..sam wiseman, stuart shieber, and alexander rush.
2017. challenges in data-to-document generation.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2253–2263, copenhagen, denmark.
association forcomputational linguistics..a development performance.
we present the performance of the compared base-lines and our model on rotowire1 and rw-fg2development sets in table 6. as can be seen, thetest datasets’ results in table 1 follow a patternsimilar to the development sets..1https://github.com/harvardnlp/boxscore-data2https://github.com/wanghm92/rw fg.
5988model.
goldtempccncpenthetdduduvours.
templateentncpncp + trours.
rg.
rotowirecs.
co.bleu.
#23.3454.2923.9533.8830.3932.1128.8126.1133.93.
51.8135.5636.2837.0438.50.p% p% r% f1% dld%94.7999.9275.1087.5191.9891.8487.2387.3592.40.
10014.4215.3318.5719.6620.7022.9724.8625.28.
10036.6931.5240.5241.6241.0944.4645.9146.13.
10026.6128.1133.5236.6235.3939.0342.0038.65.
99.0993.394.2795.6594.35.
23.7839.0443.3143.0942.88.
30.8150.1748.9149.1749.52.
10.0617.8124.0824.7525.30.
10059.1635.8651.2148.1848.9851.6450.6357.2rw-fg43.7540.1955.9657.2458.16.
1008.5114.5716.1915.9716.2416.6416.2917.68.
11.9621.6724.4924.8024.62.table 6: automatic evaluation results on development sets..model.
henc.
+ a-nr+ a-nr & d-ir+ a-nr & a-ir+ d-nr+ d-nr & a-ir+ d-nr & d-ir.
rg.
cs.
co.bleu.
#34.3735.2036.7335.0033.3833.9332.47.p% p% r% f1% dld%90.0392.0390.9692.3893.7692.4091.05.
44.3445.3445.7245.7645.9346.1346.08.
23.6424.1724.7724.8224.2925.2824.97.
55.8757.358.6757.1755.7457.255.83.
36.7537.5137.4638.1539.0538.6539.22.
17.3117.6417.2717.3818.5617.6818.01.table 7: impact of different settings of number ranking (no) and importance ranking (so).
henc denotesour data-to-text model, which incorporates a reasoning module.
the preﬁxes a and d denote ascending anddescending operations, respectively..b impact of different ranking directions.
we also explore the impact of different settings fornumber ranking and importance ranking on thedata-to-text model.
the results are summarizedin table 7. we observe that compared with thebasic model, almost all the settings can improvethe data-to-text model on content selection(cs),content ordering(co), and bleu.
this indicatesthe proposed two tasks are effective and robust..5989