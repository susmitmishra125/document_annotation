enhancing content preservation in text style transferusing reverse attention and conditional layer normalization.
dongkyu lee.
zhiliang tian.
lanqing xue.
nevin l. zhang.
department of computer science and engineering,the hong kong university of science and technology{dleear, ztianac, lxueaa, lzhang}@cse.ust.hk.
abstract.
text style transfer aims to alter the style (e.g.,sentiment) of a sentence while preserving itscontent.
a common approach is to map a givensentence to content representation that is freeof style, and the content representation is fedto a decoder with a target style.
previous meth-ods in ﬁltering style completely remove tokenswith style at the token level, which incurs theloss of content information.
in this paper, wepropose to enhance content preservation by im-plicitly removing the style information of eachtoken with reverse attention, and thereby re-tain the content.
furthermore, we fuse contentinformation when building the target style rep-resentation, making it dynamic with respect tothe content.
our method creates not only style-independent content representation, but alsocontent-dependent style representation in trans-ferring style.
empirical results show that ourmethod outperforms the state-of-the-art base-lines by a large margin in terms of contentpreservation.
in addition, it is also competitivein terms of style transfer accuracy and ﬂuency..1.introduction.
style transfer is a popular task in computer visionand natural language processing.
it aims to convertan input with a certain style (e.g., sentiment, for-mality) into a different style while preserving theoriginal content..one mainstream approach is to separate stylefrom content, and to generate a transferred sentenceconditioned on the content information and a targetstyle.
recently, several models (li et al., 2018;xu et al., 2018; wu et al., 2019) have proposedremoving style information at the token level byﬁltering out tokens with style information, whichare identiﬁed using either attention-based methods(bahdanau et al., 2015) or frequency-ratio basedmethods (wu et al., 2019).
this line of work isbuilt upon the assumption that style is localized to.
figure 1: illustration of difference between our method andﬁltering method in handling ﬂat attention distribution.
eachbar indicates attention score of the corresponding word..certain tokens in a sentence, and a token has eithercontent or style information, but not both.
thusby utilizing a style marking module, the modelsﬁlter out the style tokens entirely when construct-ing a style-independent content representation ofthe input sentence.
the drawback with the ﬁlter-ing method is that one needs to manually set athreshold to decide whether a token is stylistic orcontent-related.
previous studies address this issueby using the average attention score as a threshold(li et al., 2018; xu et al., 2018; wu et al., 2019).
amajor shortcoming of this approach is the incapa-bility of handling ﬂat attention distribution.
whenthe distribution is ﬂat, in which similar attentionscores are assigned to tokens, the style markingmodule would remove/mask out more tokens thannecessary.
this incurs information loss in contentas depicted in figure 1..in this paper, we propose a novel method for textstyle transfer.
a key idea is to exploit the fact that atoken often posses both style and content informa-tion.
for example, the word “delicious” is a tokenwith strong style information, but it also implies thesubject is food.
such words play a pivotal role inrepresenting style (e.g., positive sentiment) as wellas presenting a hint at the subject matter/content(e.g., food).
the complete removal of such tokensleads to the loss of content information..for the sake of enhancing content preservation,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages93–102august1–6,2021.©2021associationforcomputationallinguistics93toourknowledge,thisisthebestdealinphoenix.filteringoursto our knowledge, this is the <mask> <mask> <mask> <mask>.to our knowledge, this is the best deal in phoenix .average attention scorewe propose a method to implicitly remove style atthe token level using reverse attention.
we utilizeknowledge attained from attention networks (bah-danau et al., 2015) to estimate style informationof a token, and suppress such signal to take outstyle.
attention mechanism is known to attend tointerdependent representations given a query.
instyle classiﬁcation task, an attention score couldbe interpreted as to what extent a token has styleattribute.
if we can identify which tokens revealstylistic property and to what extent, it is then pos-sible to take the negation and to approximate theamount of content attribute within a token.
in thispaper, we call it reverse attention.
we utilize suchscore to suppress the stylistic attribute of tokens,fully capturing content property..this paper further enhances content preservationby fusing content information in creating targetstyle representation.
despite of extensive effortsin creating content representation, the previouswork has overlooked building content-dependentstyle representations.
the common approach is toproject the target style onto an embedding space,and share the style embedding among the samestyle as an input to the decoder.
however, our worksheds light on building content-related style by uti-lizing conditional layer normalization (cln).
thismodule of ours takes in content representations,and creates content-dependent style representationby shaping the content variable to ﬁt in the distri-bution of target style.
this way, our style represen-tation varies according to the content of the inputsequence even with the same target style..our method is based on two techniques, reverseattention and conditional layer normalization,thus we call it racoln.
in empirical evaluation,racoln achieves the state-of-the-art performancein terms of content preservation, outperformingthe previous state-of-the-art by a large margin, andshows competency in style transfer accuracy andﬂuency.
the contributions are as follows:.
• we introduce reverse attention as a way tosuppress style information while preservingcontent information when building a contentrepresentation of an input..• aside from building style-independent con-tent representation, our approach utilizesconditional layer normalization to constructcontent-dependent style representation..• our model achieves state-of-the-art perfor-.
mance in terms of content preservation, out-performing current state-of-the-art by morethan 4 bleu score on yelp dataset, and showscompetency in other metrics as well..2 related work.
in recent years, text style transfer in unsupervisedlearning environment has been studied and ex-plored extensively.
text style transfer task views asentence as being comprised of content and style.
thus, there have been attempts to disentangle thecomponents (shen et al., 2017; li et al., 2018; xuet al., 2018; wu et al., 2019).
shen et al.
(2017)map a sentence to a shared content space amongstyles to create style-independent content variable.
some studies view style as localized feature of sen-tences.
xu et al.
(2018) propose to identify style to-kens with attention mechanism, and ﬁlter out suchtokens.
frequency-based is proposed to enhancethe ﬁltering process (wu et al., 2019).
this streamof work is similar to our work in that the objectiveis to take out style at the token level, but differentsince ours does not remove tokens completely..instead of disentangling content and style, otherpapers focus on revising an entangled representa-tion of an input.
a few previous studies utilize apre-trained classiﬁer and edit entangled latent vari-able until it contains target style using the gradient-based optimization (wang et al., 2019; liu et al.,2020).
he et al.
(2020) view each domain of dataas a partially observable variable, and transfer sen-tence using amortized variational inference.
daiet al.
(2019) use the transformer architecture andrewrite style in the entangled representation at thedecoder.
we consider this model as the strongestbaseline model in terms of content preservation..in the domain of computer vision, it is a preva-lent practice to exploit variants of normalization totransfer style (dumoulin et al., 2017; ulyanov et al.,2016).
dumoulin et al.
(2017) proposed condi-tional instance normalization (cin) in which eachstyle is assigned with separate instance normal-ization parameter, in other words, a model learnsseparate gain and bias parameters of instance nor-malization for each style..our work differs in several ways.
style trans-fer in image views style transfer as changing the“texture” of an image.
therefore, dumoulin et al.
(2017) place cin module following every convo-lution layer, “painting” with style-speciﬁc parame-ters on the content representation.
therefore, the.
94figure 2: input x ﬁrst passes style marker module for computing reverse attention.
the reverse attention score is then appliedto token embeddings, implicitly removing style.
the content representation from the encoder is fed to stylizer, in which stylerepresentation is made from the content.
the decoder generates transferred output by conditioning on the two representations..network passes on entangled representation of animage.
our work is different in that we disentanglecontent and style, thus we do not overwrite con-tent with style-speciﬁc parameters.
in addition, weapply cln only once before passing it to decoder..3 approach.
3.1 task deﬁnitionlet d = {(xi, si)ni=1} be a training corpus, whereeach xi is a sentence, and si is its style label.
ourexperiments were carried on a sentiment analysistask, where there are two style labels, namely “pos-itive” and “negative.”.
the task is to learn from d a model ˆxˆs =fθ(x, ˆs), with parameters θ, that takes an inputsentence x and a target style ˆs as inputs, and out-puts a new sentence ˆxˆs that is in the target styleand retains the content information of x..3.2 model overview.
we conduct this task in an unsupervised environ-ment in which ground truth sentence xˆs is not pro-vided.
to achieve our goal, we employ a styleclassiﬁer s = c(x) that takes a sentence x as inputand returns its style label.
we pre-train such modelon d and keep it frozen in the process of learningfθ..given the style classiﬁer c(x), our task be-comes to learn a model ˆxˆs = fθ(x, ˆs) such that.
c(ˆxˆs) = ˆs.
as such, the task is conceptually sim-ilar to adversarial attack: the input x is from thestyle class s, and we want to modify it so that itwill be classiﬁed into the target style class ˆs..the architecture of our model fθ is shown infigure 2, which will some times referred to as thegenerator network.
it consists of an encoder, a styl-izer and a decoder.
the encoder maps an inputsequence x into a style-independent representationzx.
particularly, the encoder has a style markermodule that computes attention scores of input to-kens, and it “reverses” them to estimate the contentinformation.
the reversed attention scores are ap-plied to the token embedding e(x) and the resultse(cid:48)(x) are fed to bidirectional gru to produce zx.
the stylizer takes a target style ˆs and the con-tent representation zx as inputs, and produces acontent-related style representation zˆs.
finally, thedecoder takes the content representation zx andstyle representation zˆs as inputs, and generates anew sequence ˆxˆs..3.3 encoder.
3.3.1 style marker module.
let x = [x1, x2, .
.
.
, xt ] be a length t sequenceof input with a style s. the style marker module ispre-trained in order to calculate the amount of styleinformation in each token in a given input.
weuse one layer of bidirectional gru with attention.
95thefoodisdeliciouspre-trained  grugrugrustyle-independent content representationcontent-dependent style representationclnthefoodisdeliciousthefoodisbland1342embeddingattentionreverse attentioninitial hidden statestyle marker module1removing style2stylizer34decoderinput xe(x)zxẑszxẑŝx̂sstop gradiente′ (x)output (yang et al., 2016).
speciﬁcally,.
vt = tanh(wwht + bw).
(1).
(2).
(cid:80)t.αt =.
t u/τ ).
t u/τ ).
exp(vtt=1 exp(vtwhere ht is the hidden representation from the bidi-rectional gru at time step t. u is learnable parame-ters initialized with random weights, and τ denotesthe temperature in softmax.
when pre-trainingthe style marker module, we construct a sentencerepresentation by taking the weighted sum of thetoken representations with the weights being theattention scores, and feed the context vector to afully-connected layer..o =.
αtht.
t(cid:88).
t=1.
(3).
(4).
p = sof tmax(wco + bc).
the cross-entropy loss is used to learn the param-eters of the style marker module.
the attentionscores in the style marker indicate what tokens areimportant to style classiﬁcation, and to what ex-tent.
those scores will be “reversed” in the nextsection to reveal the content information.
the fully-connected layer of the style marker module is nolonger needed once the style marker module istrained.
it is hence removed..3.3.2 reverse attention.
using attention score from the pre-trained stylemarker module, we propose to implicitly removethe style information in each token.
we negate theextent of style information in each token to estimatethe extent of content information, namely reverseattention..˜αt = 1 − αt,.
αt = 1.
(5).
t(cid:88).
t=1.
where αt is an attention value from style markermodule, and ˜αt is the corresponding reverse atten-tion score.
we multiply the reverse attention scoresto the embedding vectors of tokens..˜et = ˜αtet,.
et = e(xt).
(6).
intuitively, this can be viewed as implicitly remov-ing the stylistic attribute of tokens, suppressing the.
norm of a token embedding respect to correspond-ing reverse attention score.
the representationsﬁnally ﬂow into a bidirectional gru.
zx = bidirectionalgru (˜e).
(7).
to produce a content representation zx, which isthe last hidden state of the bidirectional gru.
byutilizing reverse attention, we map a sentence tostyle-independent content representation..3.4 stylizer.
the goal of the stylizer is to create a content-relatedstyle representation.
we do this by applying condi-tional layer normalization on the content represen-tation zx from encoder as input to this module..layer normalization requires the number of gainand bias parameters to match the size of input rep-resentation.
therefore, mainly for the purpose ofshrinking the size, we perform afﬁne transforma-tion on the content variable..˜zx = wzzx + bz.
(8).
the representation is then fed to conditional layernormalization so that the representation falls intotarget style distribution in style space.
speciﬁcally,.
zˆs = cln (˜zx; ˆs) = γ ˆs (cid:12) n (˜zx) + β ˆs.
(9).
n (˜zx) =.
˜zx − µσ.
(10).
where µ and σ are mean and standard deviationof input vector respectively, and ˆs is target style.
our model learns separate γs (gain) and βs (bias)parameters for different styles..normalization method is commonly used tochange feature values in common scale, but knownto implicitly keep the features.
therefore, we ar-gue that the normalized content feature values re-tain content information of the content variable.
by passing through conditional layer normaliza-tion module, the content latent vector is scaled andshifted with style-speciﬁc gain and bias parameter,falling into target style distribution.
thus, unlikeprevious attempts in text style transfer, the stylerepresentation is dynamic respect to the content,being content-dependent embedding..in order to block backpropagation signal relatedto style ﬂowing into zx, we apply stop gradient onzx before feeding it to stylizer..963.5 decoder.
the decoder generates a sentence with the targetstyle conditioned on content-related style represen-tation and content representation.
we construct ourdecoder using one single layer of gru..ˆxˆs ∼ decθ(zx, zˆs) = pd(ˆxˆs|zx, zˆs).
(11).
as brieﬂy discussed in section 3.2, the outputsfrom our generator are further passed on for differ-ent loss functions.
however, sampling process orgreedy decoding does not allow gradient to ﬂow,because the methods are not differentiable.
there-fore, we use soft sampling to keep the gradientﬂow.
speciﬁcally, when the gradient ﬂow is re-quired through the outputs, we take the product ofprobability distribution of each time step and theweight of embedding layer to project the outputsonto word embedding space.
we empirically foundthat soft sampling is more suitable in our environ-ment than gumbel-softmax (jang et al., 2017)..3.6 pre-trained style classiﬁer.
due to the lack of parallel corpus, we cannot traingenerator network with maximum likelihood es-timation on style transfer ability.
therefore, thispaper employs a pre-trained classiﬁer c(x) to trainour generator on transferring style.
our classiﬁernetwork has the same structure as style marker mod-ule with fully-connected layer appended, nonethe-less, it is a separate model obtained from a differentset of initial model parameters.
we use the cross-entropy loss for training:.
lpre = −e(x,s)∼d[log pc(s|xs)].
(12).
we freeze the weights of this network after it has.
been fully trained..3.7 the loss function.
as shown in figure 3, our loss function consists offour parts: a self reconstruction loss lself , a cyclereconstruction loss lcycle, a content loss lcontent,and a style transfer loss lstyle..3.7.1 self reconstruction loss.
let (x, s) ∈ d be a training example.
if we askour model to fθ(x, ˆs) to “transfer” the input intoits original style, i.e., ˆs = s, we would expect it toreconstruct the input..lself = −e(x,s)∼d[log pd(x|zx, zs)].
(13).
figure 3: illustration of loss functions in training phase.
encθ, styθ, and decθ denote the encoder, the stylizer, andthe decoder respectively.
the circle ﬁgure denotes a generatedsentence with soft sampling.
as illustrated, lcycle, lstyle andlcontent require soft sampling to keep the gradient ﬂow..where zx is the content representation of the inputx, zs is the representation of the style s, and pd isthe conditional distribution over sequences deﬁnedby the decoder..3.7.2 cycle reconstruction losssuppose we ﬁrst transfer a sequence x into anotherstyle ˆs to get ˆxˆs using soft sampling, and thentransfer ˆxˆs back to the original style s. we wouldexpect to reconstruct the input x. hence we havethe following cycle construction loss:.
lcycle = −e(x,s)∼d[log pd(x|zˆxˆs, zs)].
(14).
where zˆxˆs is the content representation of the trans-ferred sequence ˆxˆs.1.
3.7.3 content lossin the aforementioned cycle reconstruction process,we obtain a content representation zx of the input xand a content representation zˆxˆs of the transferredsequence ˆxˆs.
as the two transfer steps presumablyinvolve only style but not content, the two contentrepresentations should be similar.
hence we havethe following content loss:.
lcontent = e(x,s)∼d||zx − zˆxˆs||22.
(15)1strictly speaking, the quantity is not well-deﬁned becausethere is no description of how the target style ˆs is picked.
in our experiments, we use data with two styles.
so, thetarget style just means the other style.
to apply the method toproblems with multiple styles, random sampling of differentstyle should be added.
this remark applies also to the twoloss terms to be introduced below..97decθxencθzx̂ŝxsẑx̂ŝx̂sc(̂x̂s)lselflcyclelstylelcontentsdecθzxx̂xsencθencθdecθstyθstyθstyθ3.7.4 style transfer losswe would like the transferred sequence ˆxˆs to be ofstyle ˆs.
hence we have the following style transferloss:.
lstyle = −e(x,s)∼d[log pc(ˆs|ˆxˆs)].
(16).
where pc is the conditional distribution over stylesdeﬁned by the style classiﬁer c(x).
as mentionedin section 3.5, ˆxˆs was generated with soft sam-pling..3.7.5 total lossin summary, we balance the four loss functions totrain our model..l = λ1lself + λ2lcycle + λ3lcontent + λ4lstyle(17).
where λi is balancing parameter..4 experiment.
4.1 datasets.
following prior work on text style transfer, we usetwo common datasets: yelp and imdb review..4.1.1 yelp reviewour study uses yelp review dataset (li et al., 2018)which contains 266k positive and 177k negativereviews.
test set contains a total of 1000 sen-tences, 500 positive and 500 negative, and human-annotated sentences are provided which are usedin measuring content preservation..imdb movie review.
4.1.2another dataset we test is imdb movie reviewdataset (dai et al., 2019).
this dataset is comprisedof 17.9k positive and 18.8k negative reviews fortraining corpus, and 2k sentences are used for test-ing..4.2 automatic evaluation.
4.2.1 style transfer accuracystyle transfer accuracy (s-acc) measures whetherthe generated sentences reveal target style property.
we have mentioned a style classiﬁer before: c(x)which is used in the loss function.
to evaluatetransfer accuracy, we train another style classiﬁerceval(x).
it has the identical architecture as be-fore and trained on the same data, except from adifferent set of initial model parameters.
we uti-lize such structure due to its superior performancecompared to that of commonly used cnn-based.
classiﬁer (kim, 2014).
our evaluation classiﬁerachieves accuracy of 97.8% on yelp and 98.9% onimdb, which are higher than that of cnn-based..4.2.2 content preservationa well-transferred sentence must maintain its con-tent.
in this paper, content preservation was evalu-ated with two bleu scores (papineni et al., 2002),one between generated sentence and input sen-tence (self-bleu), and the other with human-generated sentence (ref-bleu).
with this metric,one can evaluate how a sentence maintains its con-tent throughout inference..4.2.3 fluencya natural language generation task aims to out-put a sentence, which is not only task-speciﬁc,but also ﬂuent.
this study measures perplexity(ppl) of generated sentences in order to measureﬂuency.
following (dai et al., 2019), we use 5-gram kenlm (heaﬁeld, 2011) trained on the twotraining datasets.
a lower ppl score indicates atransferred sentence is more ﬂuent..4.2.4 bert scorezhang et al.
(2020) proposed bert score whichcomputes contextual similarity of two sentences.
previous methods, such as bleu score, compute n-gram matching score, while bert score evaluatesthe contextual embedding of the tokens obtainedfrom pre-trained bert (devlin et al., 2019).
thisevaluation metric has been shown to correlate withhuman judgement, thus our paper includes bertscore between model generated output and the hu-man reference sentences.
we report precision, re-call, and f1 score..4.3 human evaluation.
in addition to automatic evaluation, we validate thegenerated outputs with human evaluation.
witheach model, we randomly sample 150 outputs fromeach of the two datasets, total of 300 outputs permodel.
given the target style and the originalsentence, the annotators are asked to evaluate themodel generated sentence with a score range from1 (very bad) to 5 (very good) on content preserva-tion, style transfer accuracy, and ﬂuency.
we reportthe average scores from the 4 hired annotators intable 3..4.4.implementation details.
in this paper, we set the embedding size to 128dimension and hidden representation dimension of.
98table 1: automatic evaluation result on yelp dataset.
bold numbers indicate best performance.
g-score denotes geometricmean of self-bleu and s-acc, and bert-p, bert-r, and bert-f1 are bert score precision, recall and f1 respectively.
allthe baseline model outputs and codes were used from their ofﬁcial repositories if provided to the public..yelps-acc ref-bleu self-bleu ppl g-score bert-p bert-r bert-f1.
cross-alignment (shen et al., 2017)2controlledgen (hu et al., 2017)3style transformer (dai et al., 2019)4deep latent (he et al., 2020)5racoln (ours).
74.283.787.385.291.3.
4.216.119.815.120.0.
13.250.555.240.759.4.
53.1146.373.836.760.1.
32.065.069.458.973.6.
87.890.691.689.891.8.
86.289.089.988.690.3.
87.089.890.789.291.0.table 2: automatic evaluation result on imdb dataset.
boldnumbers indicate best performance.
as for imdb dataset, inthe absence of human reference, bert score and referencebleu are not reported..imdbs-acc self-bleu ppl g-score.
cross-alignmentcontrolledgenstyle transformerdeep latentracoln (ours).
63.981.274.059.383.1.
1.163.870.464.070.9.
29.9119.771.241.145.3.
8.471.272.261.676.8.table 3: human evaluation result.
each score indicates theaverage score from the hired annotators.
the inter-annotatoragreement, krippendorff’s alpha, is 0.729..yelp.
imdb.
style content fluency style content fluency.
cross-alignmentcontrolledgenstyle transformerdeep latentracoln (ours).
2.63.33.73.54.0.
2.44.04.33.64.5.
3.33.74.04.34.2.
2.23.33.32.73.6.
2.13.84.03.74.1.
2.33.63.84.24.1.encoder to 500. the size of bias and gain parame-ters of conditional layer norm is 200, and the sizeof hidden representation for decoder is set to 700 tocondition on both content and style representation.
adam optimizer (kingma and ba, 2015) was usedto update parameter with learning rate set to 0.0005.for balancing parameters of total loss function, weset to 0.5 for λ1 and λ2, and 1 for the rest..4.5 experimental result & analysis.
we compare our model with the baseline models,and the automatic evaluation result is presentedin table 1. our model outperforms the baseline.
2https://github.com/shentianxiao/.
language-style-transfer.
3https://github.com/asyml/texar/tree/.
master/examples/text_style_transfer.
4https://github.com/fastnlp/.
style-transformer.
5https://github.com/cindyxinyiwang/.
deep-latent-sequence-model.
models in terms of content preservation on bothof the datasets.
especially, on yelp dataset, ourmodel achieves 59.4 self-bleu score, surpassingthe previous state-of-the-art model by more than4 points.
furthermore, our model also achievesthe state-of-the-art result in content preservationon imdb dataset, which is comprised of longersequences than those of yelp..in terms of style transfer accuracy and ﬂuency,our model is highly competitive.
our modelachieves the highest score in style transfer accu-racy on both of the datasets (91.3 on yelp and 83.1on imdb).
additionally, our model shows the abil-ity to produce ﬂuent sentences as shown in theperplexity score.
in terms of the bert scores, theproposed model performs the best, having the high-est contextual similarity with the human referenceamong the style transfer models..with the automatic evaluation result, we seea trend of trade-off.
most of the baseline mod-els are good at particular metric, but show roomfor improvement on other metrics.
for example,deep latent and cross-alignment constantly per-form well in terms of perplexity, but their abilityto transfer style and preserving content needs im-provement.
style transformer achieves compara-ble performance across all evaluation metrics, butour model outperforms the model on every metricon both of the datasets.
therefore, the result showsthat our model is well-balanced but also strong inevery aspect in text style transfer task..as for the human evaluation, we observe that theresult mainly conform with the automatic evalua-tion.
our model received the highest score on thestyle and content evaluation metric on both of thedatasets by a large margin compared to the otherbaselines.
moreover, the ﬂuency score is compa-rable with that of deep latent model, showing itscompetency in creating a ﬂuent output.
both auto-matic and human evaluation depict the strength of.
99table 4: sample outputs generated by the baseline modelsand our approach on yelp and imdb dataset.
bold wordsindicate successful transfer in style without grammatical error..yelp.
original input.
cross-alignment.
everyone is always super friendly and helpful .
everyone is always super friendlyand helpful and inattentive .
tonight selection of meats and cheeses .
now i ’m not sure how to be ..controlledgendeep latentstyle transformer which is n’t super friendly .
racoln (ours).
everyone is always super rude and unprofessional ..original inputcross-alignmentcontrolledgendeep latentstyle transformerracoln (ours).
i love this place , the service is always great !
i know this place , the food is just a horrible !
i avoid this place , the service is nasty depressing vomiti do n’t know why the service is always great !
i do n’t recommend this place , the service is n’t !
i avoid this place , the service is always horrible !.
imdb.
original input.
cross-alignment.
controlledgen.
deep latent.
style transformer.
racoln (ours).
i actually disliked the leading characters so muchthat their antics were never funny but pathetic .
i have never get a good movie , i have never haveseen in this movie .
i actually anticipated the leading characters so muchthat their antics were never funny but timeless .
i actually disliked the leading characters so muchthat their antics were never funny but incredible .
i actually disliked the leading characters so muchthat their antics were never funny but vhs .
i actually liked the leading characters so muchthat their antics were never corny but appropriate ..original inputcross-alignmentcontrolledgendeep latentstyle transformer the plot is joys and has ﬂynn in it .
racoln (ours).
the plot is clumsy and has holes in it .
the worst ﬁlm is one of the worst movies i ’ve ever seen .
the plot is top-notch and has one-liners in it .
the plot is tight and has found it in a very well done ..the plot is incredible and has twists in it ..the proposed model not only in preserving content,but also on other metrics..4.5.1 style and content space.
we visualize the test dataset of yelp projected oncontent and style space using t-sne in figure 4. itis clearly observed that the content representations(zx) are spread across content space, showing thatthe representations are independent of style.
afterthe content representations go through the stylizermodule, there is a clear distinction between differ-ent styles representations (zˆs) in style space.
this isin sharp contrast to the corresponding distributionsof the style-independent content representationsshown on the right of the ﬁgure.
the ﬁgure clearlydepicts how style-speciﬁc parameters in the stylizermodule shape the content representations to fall inthe target style distribution.
this ﬁgure illustrateshow our model successfully removes style at theencoder, and constructs content-related style at thestylizer module..style space.
content space.
figure 4: visualization of yelp test dataset on content andstyle space using t-sne.
gray dots denote sentences withnegative style transferred to positive sentiment, while reddots are sentences with positive style transferred to negativesentiment..table 5: ablation study on the proposed model.
(-) indicatesremoving the corresponding component from the proposedmodel..s-acc ref-bleu self-bleu ppl.
input copy.
proposed model.
(-) reverse attention(-) stylizer(-) lcontent.
2.2.
91.3.
84.091.887.2.
22.7.
20.0.
16.619.119.5.
100.0.
59.4.
47.253.054.8.
41.2.
60.1.
60.559.062.sented in table 5. we observe a signiﬁcant dropacross all aspects without the reverse attention mod-ule.
in other case, where we remove the stylizermodule and use style embedding as in the previouspapers, the model loses the ability to retain content,drop of around 6 score on self-bleu.
we ﬁnd thatthe two core components are interdependent in suc-cessfully transferring style in text.
lastly, as forthe loss functions, incorporating lcontent brings ameaningful increase in content preservation.6.
5 conclusion.
in this paper, we introduce a way to implicitly re-move style at the token level using reverse attention,and fuse content information to style representationusing conditional layer normalization.
with the twocore components, our model is able to enhance con-tent preservation while keeping the outputs ﬂuentwith target style.
both automatic and human evalu-ation shows that our model has the best ability inpreserving content and is strong in other metrics aswell.
in the future, we plan to study problems withmore than two styles and apply multiple attribute.
4.5.2 ablation study.
in order to validate the proposed modules, we con-duct ablation study on yelp dataset which is pre-.
6other loss functions were not included, since the lossfunctions have been extensively tested and explored in previ-ous papers (prabhumoye et al., 2018; dai et al., 2019)..100style transfer, where the target style is comprisedof multiple styles..acknowledgement.
(hk$), which is higher than hong kong’s statu-tory minimum wage.
the annotators evaluated1,500 sentences in total (750 sentences per dataset),thus each annotator was compensated with the totalamount of hk$1,500..research on this paper was supported by hongkong research grants council under grant16204920 and tencent ai lab rhino-bird focusedresearch program (no.
gf202035)..references.
ethical considerations.
a text style transfer model is a conditional genera-tive model, in which the condition is the target style.
this makes a wide range of applications possible,since a style can be deﬁned as any common featurein a corpus, such as formality, tense, sentiment, etc.
however, at the same time, due to its inherentfunctionality, a text style transfer model can posepotential harm when used with a malicious inten-tion.
it can lead to a situation where one deliber-ately distorts a sentence for his or her own beneﬁt.
to give an example in a political context, politi-cal stance can be viewed a style in political slantdataset (voigt et al., 2018) as in (prabhumoye et al.,2018).
if one intentionally changes the style (polit-ical stance) of a person with the proposed modelstructure, the generated output can be exploited tocreate fake news or misinformation.
one possibleremedy for such potentially problematic situationis to employ fact checking system as a safety mea-sure (nadeem et al., 2019).
we are fully aware thatfact checking is not the fundamental solution tothe potential harm that text style transfer modelspossess.
nevertheless, one can ﬁlter out misleadinginformation using the system in certain domains(i.e., politics), lowering the level of the danger thatcan be otherwise posed by style transfer.
in con-clusion, such problem is shared among conditionalgenerative models in general, and future studies onhow to mitigate this problem are in crucial need..our work validates the proposed model and thebaseline models on human evaluation, in whichmanual work was involved.
thus, we disclosethe compensation level given to the hired anno-tators.
the average lengths of the two corporatested are 10.3 words for yelp and 15.5 wordsfor imdb.
in addition, the annotation was per-formed on sentence-level, in which the annotatorswere asked to score a model generated sentence.
considering the length and the difﬁculty, the ex-pected annotations per hour was 100 sentences.
the hourly pay was set to 100 hong kong dollars.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in 3rd internationalconference on learning representations, iclr 2015,san diego, ca, usa, may 7-9, 2015, conferencetrack proceedings..ning dai, jianze liang, xipeng qiu, and xuanjinghuang.
2019. style transformer: unpaired text styletransfer without disentangled latent representation.
in proceedings of the 57th annual meeting of the as-sociation for computational linguistics, pages 5997–6007, florence, italy.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186, minneapolis, minnesota.
association forcomputational linguistics..vincent dumoulin, jonathon shlens, and manjunathkudlur.
2017. a learned representation for artisticstyle.
in 5th international conference on learningrepresentations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings..junxian he, xinyi wang, graham neubig, and tay-lor berg-kirkpatrick.
2020. a probabilistic formu-in 8thlation of unsupervised text style transfer.
international conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020..kenneth heaﬁeld.
2011. kenlm: faster and smallerlanguage model queries.
in proceedings of the sixthworkshop on statistical machine translation, pages187–197, edinburgh, scotland.
association for com-putational linguistics..zhiting hu, zichao yang, xiaodan liang, ruslansalakhutdinov, and eric p. xing.
2017. toward con-trolled generation of text.
in proceedings of the 34thinternational conference on machine learning, vol-ume 70 of proceedings of machine learning re-search, pages 1587–1596, international conventioncentre, sydney, australia.
pmlr..eric jang, shixiang gu, and ben poole.
2017. categori-cal reparameterization with gumbel-softmax.
in 5th.
101international conference on learning representa-tions, iclr 2017, toulon, france, april 24-26, 2017,conference track proceedings..and r. garnett, editors, advances in neural infor-mation processing systems 30, pages 6830–6841.
curran associates, inc..dmitry ulyanov, andrea vedaldi, and victor s. lempit-sky.
2016. instance normalization: the missing in-gredient for fast stylization.
corr, abs/1607.08022..rob voigt, david jurgens, vinodkumar prabhakaran,dan jurafsky, and yulia tsvetkov.
2018. rtgender:a corpus for studying differential responses to gen-der.
in proceedings of the eleventh internationalconference on language resources and evaluation(lrec 2018), miyazaki, japan.
european languageresources association (elra)..ke wang, hang hua, and xiaojun wan.
2019. con-trollable unsupervised text attribute transfer via edit-ing entangled latent representation.
in advances inneural information processing systems 32, pages11036–11046.
curran associates, inc..xing wu, tao zhang, liangjun zang, jizhong han, andsonglin hu.
2019. mask and inﬁll: applying maskedlanguage model for sentiment transfer.
in proceed-ings of the twenty-eighth international joint con-ference on artiﬁcial intelligence, ijcai-19, pages5271–5277.
international joint conferences on arti-ﬁcial intelligence organization..jingjing xu, xu sun, qi zeng, xiaodong zhang, xu-ancheng ren, houfeng wang, and wenjie li.
2018.unpaired sentiment-to-sentiment translation: a cy-cled reinforcement learning approach.
in proceed-ings of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 979–988, melbourne, australia.
associationfor computational linguistics..zichao yang, diyi yang, chris dyer, xiaodong he,alex smola, and eduard hovy.
2016. hierarchicalattention networks for document classiﬁcation.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1480–1489, san diego, california.
associa-tion for computational linguistics..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..yoon kim.
2014. convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational linguis-tics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..juncen li, robin jia, he he, and percy liang.
2018.delete, retrieve, generate: a simple approach to senti-ment and style transfer.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 1865–1874, new orleans, louisiana.
associa-tion for computational linguistics..dayiheng liu, jie fu, yidan zhang, chris pal, andjiancheng lv.
2020. revision in continuous space:unsupervised text style transfer without adversariallearning.
in the thirty-fourth aaai conference onartiﬁcial intelligence, aaai 2020, the thirty-secondinnovative applications of artiﬁcial intelligence con-ference, iaai 2020, the tenth aaai symposium oneducational advances in artiﬁcial intelligence, eaai2020, new york, ny, usa, february 7-12, 2020,pages 8376–8383.
aaai press..moin nadeem, wei fang, brian xu, mitra mohtarami,and james glass.
2019. fakta: an automaticin proceedingsend-to-end fact checking system.
of the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 78–83, minneapolis, min-nesota.
association for computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic evalu-ation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..shrimai prabhumoye, yulia tsvetkov, ruslan salakhut-dinov, and alan w black.
2018. style transferthrough back-translation.
in proceedings of the 56thannual meeting of the association for computationallinguistics (volume 1: long papers), pages 866–876,melbourne, australia.
association for computationallinguistics..tianxiao shen, tao lei, regina barzilay, and tommijaakkola.
2017. style transfer from non-parallel textby cross-alignment.
in i. guyon, u. v. luxburg,s. bengio, h. wallach, r. fergus, s. vishwanathan,.
102