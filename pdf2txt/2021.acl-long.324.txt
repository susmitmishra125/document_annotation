end-to-end amr coreference resolution.
qiankun fu1,2,3, linfeng song4, wenyu du2,3, yue zhang2,31. zhejiang university2. school of engineering, westlake university3. institute of advanced technology, westlake institute for advanced study4. tencent ai lab, bellevue, wa, usafqiankun@gmail.com and lfsong@tencent.com{duwenyu, zhangyue}@westlake.edu.cn.
abstract.
although parsing to abstract meaning rep-resentation (amr) has become very popu-lar and amr has been shown effective onmany sentence-level tasks, little work has stud-ied how to generate amrs that can repre-sent multi-sentence information.
we introducethe ﬁrst end-to-end amr coreference resolu-tion model in order to build multi-sentenceamrs.
compared with the previous pipelineand rule-based approaches, our model allevi-ates error propagation and it is more robust forboth in-domain and out-domain situations.
be-sides, the document-level amrs obtained byour model can signiﬁcantly improve over theamrs generated by a rule-based method (liuet al., 2015) on text summarization..figure 1: multi-sentence amr example, where nodeswith the same non-black color are coreferential and thedotted ellipse represents an implicit role coreference..1.introduction.
abstract meaning representation (amr) (ba-narescu et al., 2013) is a semantic formalism fornatural language understanding.
it represents asentence as a rooted, directed and acyclic graph,where nodes (e.g., “bill” in figure 1) representsconcepts and edges (e.g., “:arg0”) are the seman-tic relations.
encompassing knowledge of namedentities, semantic roles and coreference structures,amr has been proven effective for downstreamtasks, including information extraction (rao et al.,2017), text summarization (liu et al., 2015; hardyand vlachos, 2018; liao et al., 2018), paraphrasedetection (issa alaa aldine et al., 2018), event de-tection (li et al., 2015), machine translation (songet al., 2019b) and dialogue understanding (bonialet al., 2020)..existing work on amr mainly focuses on in-dividual sentences (lyu and titov, 2018; naseemet al., 2019; ge et al., 2019; zhang et al., 2019;cai and lam, 2020a; zhou et al., 2020).
on theother hand, with the advance of neural networksin nlp, tasks involving multiple sentences with.
cross-sentence reasoning (e.g., text summarization,reading comprehension and dialogue response gen-eration) have received increasing research atten-tion.
given the effectiveness of amr on sentence-level tasks (pan et al., 2015; rao et al., 2017; issaalaa aldine et al., 2018; song et al., 2019b), it isimportant to extend sentence-level amrs into themulti-sentence level.
to this end, a prerequisitestep is amr coreference resolution, which aimsto ﬁnd the amr components referring to the sameentity.
figure 1 shows the amr graphs of twoconsecutive sentences in a document.
an amrcoreference resolution model need to identify twocoreference cases: “he” refers to “bill” in the ﬁrstgraph, and “arrive-01” omits an argument “:arg3”that refers to “paris”..relatively little research has been done on amrcoreference resolution.
initial attempts (liu et al.,2015) merge the nodes that have the same surfacestring.
to minimize noise, only named entitiesand date entities are considered, and they do notconsider merging non-identical nodes (e.g., “bill”and “he” in figure 1) that are also frequent in real-life situation.
subsequent work considers more.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4204–4214august1–6,2021.©2021associationforcomputationallinguistics4204leave-11personnamenamecitybillparissentence1:bill left for paris.
sentence2: he arrived at noon.arrive-01parishedate-entitynoon:name:name:arg0:arg3:arg1:arg2:dayperiod:op1:op1co-reference cases by either manually annotatingamr coreference information (o’gorman et al.,2018) or taking a pipeline system (anikina et al.,2020) consisting of a textual coreference resolu-tion model (lee et al., 2018) and an amr-to-textaligner (flanigan et al., 2014).
yet there is littleresearch on automatically resolving coreferenceambiguities directly on amr, making use of amrgraph-structural features..in this work, we formulate amr coreference res-olution as a missing-link prediction problem overamr graphs, where the input consists of multiplesentence-level amrs, and the goal is to recoverthe missing coreference links connecting the amrnodes that represent to the same entity.
there aretwo types of links.
the ﬁrst type corresponds tothe standard situation, where the edge connectstwo entity nodes (e.g., “bill” and “he” in figure1) that refer to the same entity.
the second typeis the implicit role coreference, where one node(e.g., “paris” in figure 1) is a dropped argument(“:arg3”) of other predicate node (“arrive-01”)..we propose an amr coreference resolutionmodel by extending an end-to-end text-based coref-erence resolution model (lee et al., 2017).
in par-ticular, we use a graph neural network to representinput amrs for inducing expressive features.
toenable cross-sentence information exchange, wemake connections between sentence-level amrsby linking their root nodes.
besides, we intro-duce a concept identiﬁcation module to distinguishfunctional graph nodes (non-concept nodes, e.g.,“person” in figure 1), entity nodes (e.g., “bill”),verbal nodes with implicit role (e.g., “arrive-01”)and other regular nodes (e.g., “leave-11”) to helpimprove the performance.
the ﬁnal antecedent pre-diction is conducted between the selected nodesand all their possible antecedent candidates, follow-ing previous work on textual coreference resolution(lee et al., 2017)..experiments on the ms-amr benchmark1(o’gorman et al., 2018) show that our model out-performs competitive baselines by a large margin.
to verify the effectiveness and generalization ofour proposed model, we annotate an out-of-domaintest set over the gold amr little prince 3.0 datafollowing the guidelines of o’gorman et al.
(2018),and the corresponding results show that our modelis consistently more robust than the baselines indomain-transfer scenarios.
finally, results on docu-.
1it consists gold coreference links on gold amrs..ment abstractive summarization show that our doc-ument amrs lead to much better summary qual-ity compared to the document amrs by liu et al.
(2015).
this further veriﬁes the practical value ofour approach.
our code and data is available athttps://github.com/sean-blank/amrcoref.
2 model.
formally, an input instance of amr corefer-ence resolution consists of multiple sentence-levelamrs g1, g2, ..., gn, where each gi can be writ-ten as gi = (cid:104)vi, ei(cid:105) with vi and ei represent-ing the corresponding nodes and edges for gi.
we consider a document-level amr graph ˆg =[g1, g2, ..., gn; ˆe1, ˆe2, ..., ˆem], where each ˆei is acoreference link connecting two nodes from dif-ferent sentence-level amrs.
the task of amrcoreference resolution aims to recover ˆe1, ..., ˆem,which are missing from the inputs.
figure 2 showsthe architecture of our model, which consists of agraph encoder (§ 2.1), a concept identiﬁer (§ 2.2),and an antecedent prediction module (§ 2.3)..2.1 representing input amrs using grn.
given sentence-level amrs g1, ..., gn as the in-put, randomly initialized word embeddings areadopted to represent each node vk as a dense vectorek.
to alleviate data sparsity and to obtain betternode representation, character embeddings echarare computed by using a character-level cnn.
weconcatenate both ek and echarembeddings for eachkconcept before using a linear projection to form theinitial representation:.
k.xk = w node([ek; echar.])
+ bnode,.
k.(1).
where w node and bnode are model parameters..to enable global information exchange acrossdifferent sentence-level amrs, we construct a draftdocument-level graph by connecting the root nodesof each amr subgraph as shown in figure 2. thisis important because amr coreference resolutioninvolves cross-sentence reasoning.
we then adoptgraph recurrent network (grn, song et al., 2018;zhang et al., 2018; beck et al., 2018) to obtainrich document-level node representations.
grn isone type of graph neural network that iterativelyupdates its node representations with the messagepassing framework (scarselli et al., 2009).
com-pared with alternatives such as graph convolu-tional network (gcn, kipf and welling 2017;.
4205figure 2: model framework for end-to-end amr coreference resolution..bastings et al.
2017) and graph attention net-work (gat, veliˇckovi´c et al.
2018), grn has beenshown to give competitive results..message passing in the message passing frame-work, a node vk receives information from its di-rectly connected neighbor nodes at each layer l.we use a hidden state vector hlk to represent eachnode, and the initial state h0k is deﬁned as a vectorof zeros..in the ﬁrst step at each message passing layer,the concept representation of each neighbor of vk iscombined with the corresponding edge representa-tion to make a message xk,j.
this is because edgescontain semantic information that are important forlearning global representation and subsequent rea-soning.
formally, a neighbor vj of node vk can berepresented as.
xk,j = w node([ej; echar.
j.; elabelk,j.])
+ bnode,.
(2).
where elabeledge from node vk and to vj..k,j denotes the label embedding of the.
next, representations of neighboring nodes fromthe incoming and outgoing directions are aggre-gated:.
xin.
k =.
(cid:88).
xl.
i,k.
i∈nin(k)(cid:88).
xout.
k =.
xl.
k,j.
j∈nout(k)k , xoutk ],.
xlk = [xin.
similarly, the hidden states from incoming and out-going neighbors are also summed up:.
min.
k =.
(cid:88).
hl−1i.mout.
k =.
i∈nin(k)(cid:88).
j∈nout(k).
hl−1j.ml.
k = [min.
k , moutk ],.
(4).
(5).
j.where hl−1denotes the hidden state vector for nodevj at the previous (l−1) layer.
finally, the messagepassing from layer l − 1 to l is conducted follow-ing the gated operations of lstm (hochreiter andschmidhuber, 1997):.
k + w xk + w xk + w xk + w x.k = σ(w milk = σ(w molk = σ(w mf lk = σ(w mulclk = f lk = olhl.
i mlo mlf mlu mlk (cid:12) cl−1k + ilk (cid:12) tanh(clk),.
i xlo xlf xlu xlk (cid:12) ulk.k + bi)k + bo)k + bf )k + bu).
(3).
k, ol.
k and f l.where ilk are a set of input, output andforget gates to control information ﬂow from differ-k represents the input messages, clent sources, ulkis the cell vector to record memory, and c0k is alsoinitialized as a vector of zeros.
w mz , w xz and bz(z ∈ {i, o, f, u}) are model parameters.
we adoptl grn layers in total, where l is determined by adevelopment experiment.
the output hlk at layerl is adopted as the representation of each node vkfor subsequent procedures..where nin(k) and nout(k) denote the set of in-coming and outgoing neighbors of vk, respectively..4206ffnn  &  softmaxleave-11personnamebill…arrive-01hedate-entity…𝑣𝑣1𝑠𝑠1𝑣𝑣2𝑠𝑠1𝑣𝑣3𝑠𝑠1𝑣𝑣4𝑠𝑠1𝑣𝑣1𝑠𝑠2𝑣𝑣2𝑠𝑠2𝑣𝑣3𝑠𝑠2……:name:op1:arg1:arg0:time:connectinput representation       grn encoderconcept identification                antecedent prediction                                                                  ℒ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡+ℒ𝑎𝑎𝑎𝑎𝑡𝑡𝑡𝑡𝑎𝑎𝑡𝑡𝑎𝑎𝑡𝑡𝑎𝑎𝑡𝑡=𝓛𝓛hesoftmax𝑠𝑠(leave-11, he) 𝑠𝑠(bill, he)𝑠𝑠(dummy 𝜖𝜖, he)𝑠𝑠(arrive-01, he)predicted link:  (bill, he):𝒆𝒆𝑎𝑎𝑛𝑛𝑎𝑎𝑡𝑡:𝒆𝒆𝑎𝑎𝑐𝑎𝑎𝑐𝑐:𝒉𝒉𝑙𝑙:𝒆𝒆𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡:𝒉𝒉𝐿𝐿:𝒉𝒉𝑚𝑚: 𝑠𝑠𝑚𝑚:𝑠𝑠𝑎𝑎𝑎𝑎:𝑠𝑠∶dropped2.2 concept identiﬁcation.
concept identiﬁcation aims to distinguish the amrnodes in regard to its concept type.
we consider 6concept types t = {func, ent, ver0, ver1, ver2, reg},which denotes the functional nodes, entity concepts,verbal concepts verx with implicit arguments (i.e.,“:argx” x ∈ {0, 1, 2}2) and other regular nodes(e.g., “leave-11”), respectively.
this module iscomparable to the mention detection procedure intextual coreference resolution (lee et al., 2017)..formally, a concept representation hl.
k from thetop grn layer is concatenated with a learnabletype embedding etype(t) of type t for each conceptvk, and the corresponding type score sktype(t) iscomputed using a feed-forward network:.
k.type(t) = ffnntype(w type[hlsk.
k ; etype.
k.(t)]), (6).
where w type is a mapping matrix.
etype(t) repre-sents a concept-type embedding and is randomlyinitialized.
a probability distribution p (t|vk) overall concept types t for each concept vk is calcu-lated as follows using a softmax layer:.
k.p (t|vk) =.
esktype(t)t(cid:48) ∈t eskfinally, we predicate the type t∗.
(cid:80).
..type(t(cid:48) ).
k for each concept.
(7).
k = argmaxt∈t skt∗.
type(t),.
(8).
and use it to ﬁlter the input nodes.
in particular,functional concepts are dropped directly and theother concepts (i.e., ent, ver0, ver1, ver2, reg) areselected as candidate nodes for antecedent predic-tion..2.3 antecedent prediction.
given a selected node vk by the concept identiﬁer,the goal is to predict its antecedent yk from allpossible candidate nodes yk = {(cid:15), yπ, ..., yk−1},where a dummy antecedent (cid:15) is adopted for thenodes that are not coreferent with any previous con-cepts.
π = min(1, k − ψ), where ψ represents themaximum antecedents considered as candidates.
as mentioned by previous work on textual coref-erence resolution (lee et al., 2017), consideringtoo many candidates can hurt the ﬁnal performance.
we conduct development experiments to decidethe best ψ. the ﬁnally predicted coreference linksimplicitly determine the coreference clusters..2we do not model other :argx to avoid long tail issue..type information in § 2.2 can help to guide theantecedent prediction and ensure global type con-sistency.
we combine the node hidden vector andits type representation as the ﬁnal concept state:.
k = [hlhm.
k ; etype.
k.(t∗)],.
(9).
where etypethe concept type of node vk..k.(t∗) denotes the learned embedding of.
similar with lee et al.
(2017), the goal of theantecedent prediction module is to learn a distribu-tion q(yk) over the antecedents for each node vk:.
q(yk) =.
es(k,yk)y(cid:48)∈y(k) es(k,y(cid:48)).
(cid:80).
(10).
where s(k, a) computes a coreference link scorefor each concept pair (vk, va):.
s(k, a) = sm(k) + sm(a) + san(k, a)..(11).
here a < k, and sm(k) means whether conceptvk is a mention involved in a coreference link.
it iscalculated by using a feed-forward network:.
sm(k) = ffnnm(hm.
k )..(12).
san(k, a) indicates whether mention va is an an-tecedent of vk and measures the semantic similaritybetween vk and va, computed with rich features us-ing a feed-forward network:.
a , hm.
k , hm.
k ◦hm.
san(k, a) = ffnnan([hm.
a , φ(k, a)])(13)where ◦ denotes element-wise multiplication ofeach mention pair (vk, va), and a feature vectorφ(k, a) represents the normalized distance betweentwo mentions and the speaker information if avail-able.
following lee et al.
(2017), we also nor-malize the distance values by grouping them intothe following buckets [1, 2, 3, 4, 5-7, 8-15, 16-31,32-63, 64+].
all features (speaker, distance, con-cept type) are randomly initialized 32-dimensionalembeddings jointly learned with the model..2.4 training.
our objective function takes two parts: ltype(θ)the concept-type identiﬁcation loss), and(i.e.,lantecedent (i.e., the antecedent prediction loss).
l(θ) = ltype(θ) + λlantecedent(θ),.
(14).
where λ is the weight coefﬁcient (we empiricallyset λ = 0.1 in this paper)..4207data (portion)ms-amr (train)ms-amr (dev)ms-amr (test)lp (test).
#doc273996.
#amr #links120037705216121404201463282.
#nodes86704159927452333.resolution information over the development andtest data of the little prince (lp) amr corpus4and use it as an out-of-domain test set.
for thisdataset, we consider each chapter as a document.
the data statistics are shown in table 1..table 1: statistics of ms-amr (ﬁrst group) and ourannotated out-of-domain test data based on lp corpus..3.1 setup.
ltype measuresconcept identiﬁcation loss.
whether our model can accurately identify mean-ingful concepts and learn the correct type rep-resentations.
speciﬁcally, given the concept setv = {v1, ...vn }, the concept identiﬁer is trainedto minimize an average cross-entropy loss:.
ltype(θ) = −.
logp (t∗.
k|vk),.
(15).
1n.n(cid:88).
k=1.
where θ are the set of model parameters, p (t∗k|vk)denotes the output probability of predicted type t∗kfor each node vk as in eq.
7.antecedent prediction loss.
given a trainingamr document with gold coreference clustersgold(k)|nk=1 and antecedent candidates yk ={(cid:15), yπ, ..., yk−1} for mention vk, lantecedent mea-sures whether mentions are linked to their correctantecedent.
since the antecedents are latent, theantecedent loss is a marginal log-likelihood of allcorrect antecedents implied by gold clustering:.
lantecedent(θ) =.
logq(y).
n(cid:89).
(cid:88).
k=1.
y∈yk∩gold(k).
evaluation metrics we use the standard evalua-tion metrics for coreference resolution evaluation,computed using the ofﬁcial conll-2012 evalua-tion toolkit.
three measures include: muc (vilainet al., 1995), b3 (bagga and baldwin, 1998) andceafφ4 (luo, 2005).
following previous studies(lee et al., 2018), the primary metric avg-f is theunweighted average of the above three f-scores..baselines to study the effectiveness of end-to-end amr coreference resolution, we compare ourmodel with the following baselines:.
• rule-based (liu et al., 2015): a heuristic methodthat builds a large document-level amr graphby linking identical entities..• pipeline (anikina et al., 2020): it uses an off-the-shelf coreference system (lee et al., 2018) withspanbert (joshi et al., 2020) embeddings, andan amr-to-text aligner (flanigan et al., 2014).
the former generates coreference from text, andthe later projects this information from text toamrs..(16).
models we study two versions of our model withor without bert features..where gold(k) = (cid:15) if mention vk does not be-long to any gold cluster.
q(y) is calculated usingeq.
10..3 experiments.
we conduct experiments on the ms-amr dataset3(o’gorman et al., 2018), which is annotated over aprevious gold amr corpus (ldc2017t10).
it has293 annotated documents in total with an averageof 27.4 amrs per document, covering roughly10% of the total amr corpus.
we split a dev datawith the same size as the test set from the trainingset..following the annotation guidelines of ms-amr, we manually annotate the amr coreference.
3the ms-amr dataset considers 3 types of coreferencelinks: regular, implicit and part-whole.
we ignore the lasttype, which has been challenging and ignored since textualcoreference resolution..• amrcoref-base: it corresponds to our modeldescribed in § 2 only with word embeddings..• amrcoref-bert: it denotes our model in § 2 ex-cept that the word embeddings (ek in eq.
1) areconcatenated with bert outputs.
speciﬁcally,we use a cased bert-base model with ﬁxed pa-rameters to encode a sentence, taking an amr-to-text aligner (flanigan et al., 2014) to projectbert outputs to the corresponding amr nodes..hyperparameters we set the dimension of con-cept embeddings to 256. characters in the charactercnn (§ 2.1) are represented as learned embeddingswith 32 units and the convolution window sizes in-clude 2, 3, and 4 characters, each consisting of 100ﬁlters.
we use adam (kingma and ba, 2015) witha learning rate of 0.005 for optimization..4https://amr.isi.edu/download/amr-bank-struct-v3.0.txt..4208model.
rule-basedpipelineamrcoref-baseamrcoref-bert.
muc50.858.066.172.5.in-domain test setb3ceafφ422.441.125.043.038.149.750.664.1.average f1 muc53.355.264.469.9.
38.142.051.362.4.out-domain test setb3ceafφ425.941.726.742.331.445.848.561.9.average f140.341.447.260.1.table 2: main results on the ms-amr data and lp test sets..#links cover(%).
distances.
≤ 50≤ 100≤ 150≤ 200≤ 250≤ 300> 300.
184206212214215216216.
85.295.498.199.199.5100.0100.0.f142.945.245.447.252.149.748.3.table 3: devset statistics on mention-gold-antecedentdistance and the performances of amrcoref-base usingthe distance as the search space..figure 3: development results of amrcoref-base onthe number of grn layers..3.3 main results.
3.2 development experiments.
we ﬁrst conduct development experiment to choosethe values for the crucial hyperparameters.
grn encoder layers the number of recurrentlayers l in grn deﬁnes the amount of messageinteractions.
large message passing layers maylead to over-smoothing problems, while small lay-ers may result in weak graph representation (qinet al., 2020; zhang et al., 2018).
figure 3 showsdevelopment experiments of the amrcoref-basemodel in this aspect.
we observe large improve-ments when increasing the layers from 1 to 3, butfurther increase from 3 to 7 does not lead to furtherimprovements.
therefore, we choose 3 layers forour ﬁnal model.
antecedent candidates how many antecedentsare considered as candidates (denoted as ψ in sec-tion 2.3) for making each coreference decision isanother important hyperparameter in a coreferenceresolution model (lee et al., 2017).
intuitively,allowing more antecedents gives a higher upperbound, but that also introduces a larger searchspace.
table 3 shows the statistics of the distancebetween each mention and its gold antecedent andthe devset performance of amrcoref-base modelthat uses this distance as the search space.
theperformance of amrcoref-base improves when in-creasing the search space, and the best performancewas observed when 250 antecedents are consideredas the search space.
we choose ψ =250 in subse-quent experiments..table 2 shows the ﬁnal in-domain results on thems-amr test set and out-domain results on the an-notated little prince (lp) data, where we compareour model (amrcoref-base and amrcoref-bert)with the rule-based and pipeline baselines.
in-domain results the rule-based method per-forms the worst, because it only links the identicalentity and suffers from low recall.
the pipelinemodel performs better than the rule-based modeldue to better coverage, but it can suffer from errorpropagation in both textual coreference and inac-curate amr aligner.
in addition, it does not makeuse of amr structure features, which is less sparsecompared to text cues.
our proposed amrcoref-base model outperforms the two baselines by ahuge margin, gaining at least 9.3% and 13.2% av-erage f1 scores, respectively.
this veriﬁes theeffectiveness of the end-to-end framework..out-domain results on the cross-domain lpdata, our model largely outperforms both rule-based method and the pipeline model.
comparedwith the in-domain setting, there is minor dropon the out-of-domain dataset (4.1% and 2.3% f1score for amrcoref-base and amrcoref-bert re-spectively).
neither the performances of rule-based nor pipeline change much on this dataset,which is because these systems are not trained ona certain domain.
this also reﬂects the qualityof our lp annotations, because of the consistentperformance changes of both amrcoref-base andamrcoref-bert when switching from ms-amr tolp..4209number of grn layersmodelamrcoref-base- concept identiﬁcation+ gold mention+ bert concatenate+ bert graph- distance feature- speaker feature- character cnn- graph connections.
average f151.331.470.462.462.049.249.450.149.0.
∆--19.9+19.1+11.1+10.7-2.1-1.9-1.2-2.3.table 4: ablation study on the test set of ms-amr..3.4 analysis.
we analyze the effects of mention type, textual em-bedding and various extra features in this section.
concept identiﬁcation as shown in the ﬁrstgroup of table 4, we conduct an ablation studyon the concept identiﬁcation module, which hasbeen shown crucial on the textual coreference res-olution (lee et al., 2017).
removing the conceptidentiﬁer from the amrcoref-base model results ina large performance degradation of up to 19.9%, in-dicating that concept type information of the amrnode can positively guide the prediction of coref-erence links.
on the other hand, when the conceptidentiﬁer outputs are replaced with gold mentions,the results can be further improved by 19.1%.
thisindicates that better performances can be expectedif concept identiﬁcation can be further improved..injecting bert knowledge as shown in thesecond group of table 4, we study the inﬂuenceof rich features from bert in our model, whichhas been proven effective on text-based corefer-ence resolution.
two alternatives of using bertare studied, concatenate (i.e.
amrcoref-bert) de-notes concatenating the amr node embeddingswith the corresponding textual bert embedding,and graph means that we construct an amr-tokengraph that connects amr nodes and the corre-sponding tokens.
we ﬁnd that the amrcoref-basemodel can be improved by a similar margin usingboth approaches.
this is consistent with existingobservations from other structured prediction tasks,such as constituent parsing (kitaev et al., 2019)and dependency parsing (li et al., 2019).
due tothe limited scale of our training data, we expect thegain to be less with more training data..figure 4: testing results of amrcoref-base regardingdifferent ratios of training data used.
the f1 score ofpipeline is 42.0% (table 2)..the distance between a pair of amr concepts isan important feature.
the ﬁnal model performancedrops by 2.1% when removing the distance feature(eq.
13).
second, the speaker indicator features(eq.
13) contribute to our model by a 1.9% im-provement.
intuitively, speaker information is help-ful for pronoun coreference resolution in dialogues.
for example, “my package” in one sentence mayrepresent identical entity with “your package” inthe next utterance.
third, the character cnn pro-vides morphological information and a way to backoff for out-of-vocabulary tokens.
for amr noderepresentations, we see a modest contribution of1.2% f1 score.
finally, we exploit the necessityof cross-sentence amr connections.
comparedto encoding each amr graph individually, globalinformation exchange across sentences can help toachieve a signiﬁcant performance improvement..data hunger similar to other results, it is im-portant to study how much data is necessary toobtain a strong performance (at least be better thanthe baseline).
figure 4 shows the performanceswhen training the amrcoref-base model on differ-ent portions of data.
as the number of trainingsamples increases, the performance of our modelcontinuously improves.
this shows that our modelhas room for further improvement with more train-ing data.
moreover, our model even outperformsthe pipeline baseline when trained on only 20%data.
this conﬁrms the robustness of our end-to-end framework..features ablation as shown by the last group intable 4, we investigate the impacts of each compo-nent in our proposed model on the development setof ms-amr.
we have the following observations.
first, consistent with ﬁndings of lee et al.
(2017),.
effect of document length figure 5 showsthe performance on different ms-amr documentlengths (i.e., the number of amr graphs in thedocument).
we can see that both our model andthe pipeline model show performance decrease.
4210modelliu et al.
(2015)dohare et al.
(2017)d2s-rule-basedd2s-pipelined2s-amrcoref-based2s-amrcoref-bert.
r-144.344.847.647.948.449.1.r-2–17.320.119.520.420.5.r-l–30.632.532.633.233.6.table 5: test summarization results on ldc2015e86.
r-1/2/l is short for rouge-1/2/l..the overall performance of the d2s models out-perform the previous approaches, indicating thatour experiments are conducted on a stronger base-line.
though pipeline is better than rule-based onamr coreference resolution, d2s-pipeline is com-parable with d2s-rule-based on the downstreamsummerization task.
this shows that the error prop-agation issue of pipeline can introduce further neg-ative effects to a downstream application.
on theother hand, both d2s-amrcoref-base and d2s-amrcoref-bert show much better results than thebaselines across all rouge metrics.
this demon-strates that the improvements made by our end-to-end model is solid and can transfer to a downstreamapplication.
d2s-amrcoref-bert achieves the bestperformance, which is consistent with the aboveexperiments..4 related work.
multi-sentence amr although some previouswork (szubert et al., 2020; van noord and bos,2017) explore the coreference phenomena of amr,they mainly focus on the situation within a sentence.
on the other hand, previous work on multi-sentenceamr primarily focuses on data annotation.
songet al.
(2019a) annotate dropped pronouns over chi-nese amr but only deals with implicit roles inspeciﬁc constructions.
gerber and chai (2012) pro-vide implicit role annotations, but the resourceswere limited to a small inventory of 5-10 predicatetypes rather than all implicit arguments.
o’gormanet al.
(2018) annotated the ms-amr dataset bysimultaneously considering coreference, implicitrole coreference and bridging relations.
we con-sider coreference resolution as the prerequisite forcreating multi-sentence amrs, proposing the ﬁrstend-to-end model for this task..coreference resolution coreference resolutionis a fundamental problem in natural languageprocessing.
neural network models have shownpromising results over the years.
recent work (leeet al., 2017, 2018; kantor and globerson, 2019).
figure 5: testing results regarding document length..when increasing input document length.
this islikely because a longer document usually involvesmore complex coreference situations and bringsmore challenge for the encoder.
insufﬁcient infor-mation interaction for distant nodes further leadsto weaker inference performance.
as expected,the rule-based approach (liu et al., 2015) is notsigniﬁcantly affected, but its result is still prettylow.
when the document contains more than30 sentences, the amrcoref-base model slightlyunder-performs both the rule-based method andthe pipeline baseline.
one reason is that only a fewtraining instances have a long document length, sowe expect that the performance of our model canbe further improved given more long documents..3.5 application on summarization.
table 5 compares the summarization performancesusing the document-level amrs generated by var-ious methods on the ldc2015e86 benchmark(knight et al., 2014).
following liu et al.
(2015),rouge scores (r-1/2/l lin 2004) are used as themetrics.
to consume each document amr andthe corresponding text, we take a popular dual-to-sequence model (d2s, song et al.
2019b), whichextends the standard sequence-to-sequence frame-work with an additional graph encoder and a dualattention mechanism for extracting both text andgraph contexts during decoding..for previous work, summarization using amrwas ﬁrst explored by liu et al.
(2015).
they ﬁrstuse a rule-based method to build document amrsand then take a statistic model to generate sum-maries.
dohare et al.
(2017) improves this ap-proach by selecting important sentences beforebuilding a document amr.
the d2s-rule-basedcan be considered as a fair comparison with liuet al.
(2015) on the same summerization platform..421138.537.837.538.645.642.840.938.759.657.949.538.2102030405060700-1010-2020-3030upaveragef1documentlengthperformanceondocumentlengthrule-basedpipelineamrcoref-basetackled the problem end-to-end by jointly detect-ing mentions and predicting coreference.
lee et al.
(2018) build a complete end-to-end system withthe span-ranking architecture and higher-order in-ference technique.
while previous work considersonly text-level coreference, we investigate amrco-reference resolution..amr representation using gnn to encodeamr graphs, many variants of gnns such asgrns (song et al., 2018; beck et al., 2018), gcns(zhou et al., 2020; zhang et al., 2020) and gats(damonte and cohen, 2019; cai and lam, 2020b;wang et al., 2020) have been introduced.
wechoose a classic grn model following song et al.
(2018) to represent our document-level amr graphand leave the exploiting on a more efﬁcient gnnstructure for future work..5 conclusion.
we investigated a novel end-to-end multi-sentenceamr coreference resolution model using a graphneural network.
compared with previous rule-based and pipeline methods, our model better cap-tures multi-sentence semantic information.
resultson ms-amr (in-domain) and lp (out-of-domain)datasets show the superiority and robustness of ourmodel.
in addition, experiments on the downstreamtext summarization task further demonstrate the ef-fectiveness of the document-level amrs producedby our model..in future work, we plan to resolve both the cross-amr coreference links and the sentence-level onestogether with our model..acknowledgments.
linfeng song is the corresponding author.
wewould like to thank the anonymous reviewers fortheir insightful comments.
we gratefully acknowl-edge funding from the national natural sciencefoundation of china (nsfc no.61976180).
it alsoreceives supported by tencent ai lab rhino-birdfocused research program..references.
tatiana anikina, alexander koller, and michael roth.
predicting coreference in abstract mean-2020.in proceedings of the thirding representations.
workshop on computational models of reference,anaphora and coreference, pages 33–38, barcelona,spain (online).
association for computational lin-guistics..amit bagga and breck baldwin.
1998. algorithmsfor scoring coreference chains.
in the ﬁrst interna-tional conference on language resources and evalua-tion workshop on linguistics coreference, volume 1,pages 563–566..laura banarescu, claire bonial, shu cai, madalinageorgescu, kira grifﬁtt, ulf hermjakob, kevinknight, philipp koehn, martha palmer, and nathanschneider.
2013. abstract meaning representationfor sembanking.
in proceedings of the 7th linguis-tic annotation workshop and interoperability withdiscourse, pages 178–186, soﬁa, bulgaria.
associa-tion for computational linguistics..jasmijn bastings,.
ivan titov, wilker aziz, diegomarcheggiani, and khalil sima’an.
2017. graphconvolutional encoders for syntax-aware neural ma-chine translation.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 1957–1967, copenhagen, den-mark.
association for computational linguistics..daniel beck, gholamreza haffari, and trevor cohn.
graph-to-sequence learning using gated2018.thegraph neural networks.
56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 273–283, melbourne, australia.
associationfor computational linguistics..in proceedings of.
claire bonial, lucia donatelli, mitchell abrams,stephanie m. lukin, stephen tratz, matthew marge,ron artstein, david traum, and clare voss.
2020.dialogue-amr: abstract meaning representationin proceedings of the 12th lan-for dialogue.
guage resources and evaluation conference, pages684–695, marseille, france.
european language re-sources association..deng cai and wai lam.
2020a.
amr parsing viagraph-sequence iterative inference.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1290–1301, on-line.
association for computational linguistics..deng cai and wai lam.
2020b.
graph transformer forgraph-to-sequence learning.
in aaai, pages 7464–7471..marco damonte and shay b. cohen.
2019. structuralneural encoders for amr-to-text generation.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 3649–3658,minneapolis, minnesota.
association for computa-tional linguistics..shibhansh dohare, harish karnick, and vivek gupta.
2017. text summarization using abstract meaningrepresentation.
arxiv preprint arxiv:1706.01678..jeffrey flanigan, sam thomson, jaime carbonell,chris dyer, and noah a. smith.
2014. a discrim-.
4212inative graph-based parser for the abstract mean-ing representation.
in proceedings of the 52nd an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1426–1436, baltimore, maryland.
association for compu-tational linguistics..donglai ge, junhui li, muhua zhu, and shoushan li.
2019. modeling source syntax and semantics forin proceedings of the twenty-neural amr parsing.
eighth international joint conference on artiﬁcialintelligence, ijcai-19, pages 4975–4981.
interna-tional joint conferences on artiﬁcial intelligenceorganization..matthew gerber and joyce y. chai.
2012. semanticrole labeling of implicit arguments for nominal pred-icates.
computational linguistics, 38(4):755–798..hardy hardy and andreas vlachos.
2018. guided neu-ral language generation for abstractive summariza-tion using abstract meaning representation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 768–773, brussels, belgium.
association for computa-tional linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..ahmad issa alaa aldine, mounira harzallah, giuseppeberio, nicolas b´echet, and ahmad faour.
2018.expr at semeval-2018 task 9: a combined ap-proach for hypernym discovery.
in proceedings ofthe 12th international workshop on semantic eval-uation, pages 919–923, new orleans, louisiana.
as-sociation for computational linguistics..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..ben kantor and amir globerson.
2019. coreferencein proceed-resolution with entity equalization.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 673–677, flo-rence, italy.
association for computational linguis-tics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in international conference on learningrepresentations (iclr)..nikita kitaev, steven cao, and dan klein.
2019. multi-lingual constituency parsing with self-attention andin proceedings of the 57th annualpre-training..meeting of the association for computational lin-guistics, pages 3499–3505, florence, italy.
associa-tion for computational linguistics..kevin knight, laura baranescu, claire bonial,madalina georgescu, kira grifﬁtt, ulf hermjakob,daniel marcu, martha palmer, and nathan schnei-der.
2014.deft phase 2 amr annotation r1ldc2015e86.
philadelphia: linguistic data consor-tium.
abstract meaning representation (amr) an-notation release, 1..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017. end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing,pages 188–197, copenhagen, denmark.
associationfor computational linguistics..kenton lee, luheng he, and luke zettlemoyer.
2018.higher-order coreference resolution with coarse-to-ﬁne inference.
in proceedings of the 2018 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 2 (short papers), pages687–692, new orleans, louisiana.
association forcomputational linguistics..xiang li, thien huu nguyen, kai cao, and ralph gr-ishman.
2015. improving event detection with ab-in proceedings ofstract meaning representation.
the first workshop on computing news storylines,pages 11–15, beijing, china.
association for com-putational linguistics..ying li, zhenghua li, min zhang, rui wang, shengli, and luo si.
2019. self-attentive biafﬁne depen-dency parsing.
in ijcai, pages 5067–5073..kexin liao, logan lebanoff, and fei liu.
2018. ab-stract meaning representation for multi-documentin proceedings of the 27th inter-summarization.
national conference on computational linguistics,pages 1178–1190, santa fe, new mexico, usa.
as-sociation for computational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..fei liu, jeffrey flanigan, sam thomson, normansadeh, and noah a. smith.
2015. toward abstrac-tive summarization using semantic representations.
in proceedings of the 2015 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1077–1086, denver, colorado.
associationfor computational linguistics..xiaoqiang luo.
2005. on coreference resolution per-in proceedings of human lan-formance metrics.
guage technology conference and conference onempirical methods in natural language processing,pages 25–32, vancouver, british columbia, canada.
association for computational linguistics..4213chunchuan lyu and ivan titov.
2018. amr parsing asgraph prediction with latent alignment.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 397–407, melbourne, australia.
asso-ciation for computational linguistics..tahira naseem, abhishek shah, hui wan, radu flo-rian, salim roukos, and miguel ballesteros.
2019.rewarding smatch: transition-based amr parsingwith reinforcement learning.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 4586–4592, florence,italy.
association for computational linguistics..tim o’gorman, michael regan, kira grifﬁtt, ulf her-mjakob, kevin knight, and martha palmer.
2018.amr beyond the sentence: the multi-sentence amrin proceedings of the 27th internationalcorpus.
conference on computational linguistics, pages3693–3702, santa fe, new mexico, usa.
associ-ation for computational linguistics..xiaoman pan, taylor cassidy, ulf hermjakob, heng ji,and kevin knight.
2015. unsupervised entity link-in pro-ing with abstract meaning representation.
ceedings of the 2015 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages1130–1139..libo qin, xiao xu, wanxiang che, and ting liu.
2020.agif: an adaptive graph-interactive framework forinjoint multiple intent detection and slot ﬁlling.
findings of the association for computational lin-guistics: emnlp 2020, pages 1807–1816, online.
association for computational linguistics..sudha rao, daniel marcu, kevin knight, and haldaum´e iii.
2017. biomedical event extraction us-in bionlping abstract meaning representation.
2017, pages 126–135, vancouver, canada,.
associa-tion for computational linguistics..f. scarselli, m. gori, a. c. tsoi, m. hagenbuchner,and g. monfardini.
2009. the graph neural net-ieee transactions on neural net-work model.
works, 20(1):61–80..li song, yuan wen, sijia ge, bin li, and weiguang qu.
2019a.
an easier and efﬁcient framework to anno-tate semantic roles: evidence from the chinese amrcorpus.
in workshop on chinese lexical semantics,pages 474–485.
springer..linfeng song, daniel gildea, yue zhang, zhiguowang, and jinsong su.
2019b.
semantic neural ma-chine translation using amr.
transactions of theassociation for computational linguistics, 7:19–31..linfeng song, yue zhang, zhiguo wang, and danielgildea.
2018. a graph-to-sequence model for amr-in proceedings of the 56th an-to-text generation.
nual meeting of the association for computational.
linguistics (volume 1: long papers), pages 1616–1626, melbourne, australia.
association for compu-tational linguistics..ida szubert, marco damonte, shay b. cohen, andmark steedman.
2020. the role of reentrancies inabstract meaning representation parsing.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 2198–2207, online.
as-sociation for computational linguistics..rik van noord and johan bos.
2017. dealing withco-reference in neural semantic parsing.
in proceed-ings of the 2nd workshop on semantic deep learn-ing (semdeep-2), pages 41–49..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..marc vilain, john burger, john aberdeen, dennis con-nolly, and lynette hirschman.
1995. a model-theoretic coreference scoring scheme.
in sixth mes-sage understanding conference (muc-6): proceed-ings of a conference held in columbia, maryland,november 6-8, 1995..tianming wang, xiaojun wan, and hanqi jin.
2020.amr-to-text generation with graph transformer.
transactions of the association for computationallinguistics, 8:19–33..sheng zhang, xutai ma, kevin duh, and benjaminvan durme.
2019. amr parsing as sequence-to-graph transduction.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 80–94, florence, italy.
associa-tion for computational linguistics..yan zhang, zhijiang guo, zhiyang teng, wei lu,shay b. cohen, zuozhu liu, and lidong bing.
2020.lightweight, dynamic graph convolutional networksfor amr-to-text generation.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 2162–2172,online.
association for computational linguistics..yue zhang, qi liu, and linfeng song.
2018. sentence-state lstm for text representation.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 317–327, melbourne, australia.
associationfor computational linguistics..qiji zhou, yue zhang, donghong ji, and hao tang.
2020. amr parsing with latent structural infor-in proceedings of the 58th annual meet-mation.
ing of the association for computational linguistics,pages 4306–4319, online.
association for computa-tional linguistics..4214