cil: contrastive instance learning framework for distantly supervisedrelation extraction.
tao chen1,2, haizhou shi1, siliang tang1,2∗, zhigang chen3,fei wu1 & yueting zhuang11zhejiang university2alibaba-zhejiang university joint research institute of frontier technologies3state key laboratory of cognitive intelligence, hefei, china{ttc, shihaizhou, siliang, yzhuang}@zju.edu.cnzgchen@iflytek.com, wufei@cs.zju.edu.cn.
abstract.
the journey of reducing noise from distantsupervision (ds) generated training data hasbeen started since the ds was ﬁrst introducedinto the relation extraction (re) task.
forthe past decade, researchers apply the multi-instance learning (mil) framework to ﬁnd themost reliable feature from a bag of sentences.
although the pattern of mil bags can greatlyreduce ds noise, it fails to represent manyother useful sentence features in the datasets.
in many cases,these sentence features canonly be acquired by extra sentence-level hu-man annotation with heavy costs.
therefore,the performance of distantly supervised remodels is bounded.
in this paper, we gobeyond typical mil framework and proposea novel contrastive instance learning (cil)framework.
speciﬁcally, we regard the ini-tial mil as the relational triple encoder andconstraint positive pairs against negative pairsfor each instance.
experiments demonstratethe effectiveness of our proposed framework,with signiﬁcant improvements over the previ-ous methods on nyt10, gds and kbp..1.introduction.
relation extraction (re) aims at predicting the re-lation between entities based on their context.
sev-eral studies have been carried out to handle thiscrucial and complicated task over decades as theextracted information can serve as a signiﬁcant rolefor many downstream tasks.
since the amount oftraining data generally limits traditional supervisedre systems, current re systems usually resort todistant supervision (ds) to fetch abundant train-ing data by aligning knowledge bases (kbs) andtexts.
however, such a heuristic way inevitably in-troduces some noise to the generated data.
traininga robust and unbiased re system under ds data.
∗ corresponding author.
figure 1: classical mil framework for dsre.
(left)a set of instances (x1, x2, .
.
.
, xm) with the same kbfact [e1, e2, r] form a bag b; (right) the mil frame-work trains the dsre model at bag level( (cid:101)b : (cid:80) αihi)..noise becomes the biggest challenge for distantlysupervised relation extraction (dsre)..with awareness of the existing ds noise, zenget al.
(2015) introduces the multi-instance learning(mil) framework to dsre by dividing traininginstances into several bags and using bags as newdata units.
regarding the strategy for selecting in-stances inside the bag, the soft attention mechanismproposed by lin et al.
(2016) is widely used for itsbetter performance than the hard selection method.
the ability to form accurate representations fromnoisy data makes the mil framework soon becomea paradigm of following-up works..however, we argue that the mil framework iseffective to alleviate data noise for dsre, but isnot data-efﬁcient indeed: as figure 1 shows: theattention mechanism in the mil can help select rel-atively informative instances (e.g.h1, h2) inside thebag, but may ignore the potential information ofother abundant instances (e.g.hm).
in other words,no matter how many instances a bag contains, onlythe formed bag-level representation can be used forfurther training in the mil, which is quite inefﬁ-cient.
thus, our focus is on how to make the initialmil framework efﬁcient enough to leverage allinstances while maintaining the ability to obtainan accurate model under ds data noise?.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6191–6200august1–6,2021.©2021associationforcomputationallinguistics6191(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:3560)(cid:1828)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1870)(cid:1870)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)sentenceencoder(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:2869)(cid:2869)(cid:2869)(cid:2869)(cid:2869)(cid:1710)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:1876)(cid:2870)(cid:2870)(cid:2870)(cid:2870)(cid:2870)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:3560)(cid:1828)(cid:1710)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1870)kb fact [(cid:1857)(cid:2869),(cid:1857)(cid:2870),(cid:1870)](cid:1828)here, we propose a contrastive-based method tohelp the mil framework learn efﬁciently.
in detail,we regard the initial mil framework as the bag en-coder, which provides relatively accurate represen-tations for different relational triples.
then we de-velop contrastive instance learning (cil) to utilizeeach instance in an unsupervised manner: in short,the goal of our cil is that the instances sharing thesame relational triples (i.e.positive pairs) ought tobe close in the semantic space, while the represen-tations of instances with different relational triples(i.e.negative pairs) should be far away..experiments on three public dsre benchmarks— nyt10 (riedel et al., 2010; hoffmann et al.,2011), gds (jat et al., 2018) and kbp (ling andweld, 2012) demonstrate the effectiveness of ourproposed framework cil, with consistent improve-ments over several baseline models and far exceedthe state-of-the-art (sota) systems.
furthermore,the ablation study shows the rationality of our pro-posed positive/negative pair construction strategy.
accordingly, the major contributions of this pa-.
per are summarized as follows:.
• we discuss the long-standing mil frameworkand point out that it can not effectively utilizeabundant instances inside mil bags..• we propose a novel contrastive instance learn-ing method to boost the dsre model perfor-mances under the mil framework..• evaluation on held-out and human-annotatedsets shows that cil leads to signiﬁcant im-provements over the previous sota models..2 methodology.
in this paper, we argue that the mil frameworkis effective to denoise but is not efﬁcient enough,as the initial mil framework only leverages theformed bag-level representations to train modelsand sacriﬁces the potential information of numer-ous instances inside bags.
here, we go beyond thetypical mil framework and develop a novel con-trastive instance learning framework to solve theabove issue, which can prompt dsre models toutilize each instance.
a formal description of ourproposed cil framework is illustrated as follows..2.1.input embeddings.
token embedding for input sentence/instancex, we utilize bert tokenizer to split it into several.
tokens: (t1, t2, .
.
.
e1 .
.
.
e2 .
.
.
tl), where e1, e2 arethe tokens corresponding to the two entities, and lis the max length of all input sequences.
followingstandard practices (devlin et al., 2019), we add twospecial tokens to mark the beginning ([cls]) andthe end ([sep]) of sentences..in bert, token [cls] typically acts as a poolingtoken representing the whole sequence for down-stream tasks.
however, this pooling representa-tion considers entity tokens e1 and e2 as equivalentto other common word tokens ti, which has beenproven (baldini soares et al., 2019) to be unsuit-able for re tasks.
to encode the sentence in anentity-aware manner, we add four extra special to-kens ([h-cls], [h-sep]) and ([t-cls], [t-sep])to mark the beginning and the end of two entities..position embeddingin the transformer atten-tion mechanism (vaswani et al., 2017), positionalencodings are injected to make use of the orderof the sequence.
precisely, the learned positionembedding has the same dimension as the tokenembedding so that the two can be summed..figure 2: bert encoder: n × transformer blocks..2.2 sentence encoder.
bert encoder (transformer blocks, see figure 2)transforms the above embedding inputs (token em-bedding & position embedding) into hidden featurevectors: (h1, h2, .
.
.
he1 .
.
.
he2 .
.
.
hl), where he1and he2 are the feature vectors corresponding tothe entities e1 and e2.
by concatenating the two en-tity hidden vectors, we can obtain the entity-awaresentence representation h = [he1; he2] for the inputsequence x. we denote the sentence encoder h as:.
h(x) = [he1; he2] = h.6192multi-headattentionadd & normtokenembeddingfeedforwardadd & norminputs (cid:1876)position embeddingn (cid:28772)outputs (cid:1860)2.3 bag encoder.
2.4 contrastive instance learning.
as illustrated in section 1, the goal of our frame-work cil is that the instances containing the samerelational triples (i.e.positive pairs) should be asclose (i.e.∼) as possible in the hidden semanticspace, and the instances containing different rela-tional triples (i.e.negative pairs) should be as far(i.e.
(cid:28)) away as possible in the space.
a formaldescription is as follows..assume there is a batch bag input (with a batchsize g): (b1, b2, .
.
.
, bg), the relational triples ofall bags are different from each other.
each bag bin the batch is constructed by a certain relationaltriple [e1, e2, r], and all instances x inside the bagsatisfy this triple.
the representation of the triplecan be obtained by bag encoder as (cid:101)b..we pick any two bags bs and bt:t(cid:54)=s in the batchto further illustrate the process of contrastive in-stance learning.
bs is deﬁned as the source bag con-structed with relational triple [es1, es2, rs] while btis the target bag constructed with triple [et1, et2, rt].
and we discuss the positive pair instance and nega-tive pair instances for any instance xs in bag bs..it is worth noting that all bags are constructedautomatically by the distantly supervised method,which extracts relational triples from instances ina heuristic manner and may introduce true/falsepositive label noise to the generated data.
in otherwords, though the instance x is included in the bagwith relational triple [e1, e2, r], it may be noisy andfail to express the relation r..2.4.1 positive pair constructioninstance xs ∼ random instance xs(cid:48) one intu-itive choice of selecting positive pair instance for in-stance xs is just picking another instance xs(cid:48) (cid:54)= xsfrom the bag b randomly.
however, both of theinstances xs and xs(cid:48) may suffer from data noise,and they are hard to express the same relationaltriple simultaneously.
thus, taking instance xs andrandomly selected instance xs(cid:48) as a positive pair isnot an optimal option..under the mil framework, a couple of instances xwith the same relational triple [e1, e2, r] form a bagb. we aim to design a bag encoder f to obtainrepresentation (cid:101)b for bag b, and the obtained bagrepresentation is also a representative of the currentrelational triple [e1, e2, r], which is deﬁned as:.
f(b) = f([e1, e2, r]) = (cid:101)b.with the help of the sentence encoder describedin section 2.2, each instance xi in bag b can beﬁrst encoded to its entity-aware sentence represen-tation hi = h(xi).
then the bag representation (cid:101)bcan be regarded as an aggregation of all instances’representations, which is further deﬁned as:.
f([e1, e2, r]) = (cid:101)b =.
αihi.
k(cid:88).
i=1.
where k is the bag size.
as for the choice of weightαi, we follow the soft attention mechanism usedin (lin et al., 2016), where αi is the normalized at-tention score calculated by a query-based functionfi that measures how well the sentence representa-tion hi and the predict relation r matches:.
αi =.
efij efj.
(cid:80).
where fi = hiaqr, a is a weighted diagonal ma-trix and qr is the query vector which indicates therepresentation of relation r (randomly initialized).
then, to train such a bag encoder parameterizedby θ, a simple fully-connected layer with activationfunction softmax is added to map the hidden featurevector (cid:101)b to a conditional probability distributionp(r| (cid:101)b, θ), and this can be deﬁned as:.
p(r| (cid:101)b, θ) =.
eor(cid:80)nri=1 eoi.
where o = m (cid:101)b + b is the score associated to allrelation types, nr is the total number of relations,m is a projection matrix, and b is the bias term..and we deﬁne the objective of bag encoder using.
cross-entropy function as follows:.
figure 3: instance xs ∼ random instance xs(cid:48).
lb(θ) = −.
log p(ri| (cid:101)bi, θ).
(cid:88).
i=1.
instance xs ∼ relational triple (cid:101)bs anotherpositive pair instance candidate for instance xs is.
6193(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:4593)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:1499)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3047)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1710)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3046)(cid:1499)(cid:1710)(cid:1710)(cid:1870)(cid:3046)(cid:1870)(cid:3047)(cid:1876)(cid:3046)(cid:1876)(cid:3047)(cid:1876)(cid:3047)(cid:1499)the relational triple representation (cid:101)bs of currentbag b. though (cid:101)bs can be regarded as a de-noisedrepresentation, xs may be still noisy and expressother relation r (cid:54)= rs.
besides, the quality of con-structed positive pairs heavily relies on the modelperformance of the bag encoder..negative pair instance.
under this strategy, xs is faraway from the average representation (cid:80)ki=1 αihiof the bag bt, where all αi = 1k approximately.
and the randomly selected instance xt may be toonoisy to represent the relational triple of bag bt, sothat the model performance may be inﬂuenced..figure 4: instance xs ∼ relational triple (cid:101)bs.
figure 7: instance xs (cid:28) random instance xt.
instance xs ∼ augmented instance x∗s fromthe above analysis, we can see that the general pos-itive pair construction methods often encounter thechallenge of ds noise.
here, we propose a noise-free positive pair construction method based ontf-idf data augmentation: if we only make smalland controllable data augmentation to the originalinstance xs, the augmented instance x∗s should sat-isfy the same relational triple with instance xs..instance xs (cid:28) relational triple (cid:101)bt comparedto the random selection strategy, using relationaltriple representation (cid:101)bt as the negative pair in-stance for xs is a better choice to reduce the im-pact of data noise.
as the instance xi can be seenas be far away from a weighted representation(cid:80)ki=1 αihi of the bag bt, where all αi are learn-able.
though the instance xs may still be noisy, xsand (cid:101)bt can not belong to the same relational triple..figure 5: instance xs ∼ augmented instance x∗s.figure 8: instance xs (cid:28) relational triple (cid:101)bt.
in detail: (1) we ﬁrst view each instance as adocument and view each word in the instances asa term, then we train a tf-idf model on the totaltraining corpus.
(2) based on the trained tf-idfmodel, we insert/substitute some unimportant (lowtf-idf score, see figure 6) words to/in instance xswith a speciﬁc ratio, and can obtain its augmentedinstance x∗s. particularly, special masks are addedto entity words to avoid them being substituted..2.5 training objective.
as discussed above, for any instance xs in thesource bag bs: (1) the instance x∗s after control-lable data augmentation based on xs is its positivepair instance.
(2) the relational triple represen-tations (cid:101)bt of other different (t (cid:54)= s) bags in thebatch are its negative pair instances.
the overallschematic diagram of cil is shown in figure 9..figure 6: an example of word substitution: the low-scoring word of is replaced with word goehr, and entitywords stephen king and maine are protected..2.4.2 negative pair constructioninstance xs (cid:28) random instance xt similarly,for instance xs in bag bs, we can randomly selectan instance xt from another different bag bt as its.
figure 9: contrastive instance learning.
and we deﬁne the objective for instance xs in.
6194(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:4593)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:1499)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3047)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1710)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3046)(cid:1499)(cid:1710)(cid:1710)(cid:1870)(cid:3046)(cid:1870)(cid:3047)(cid:1876)(cid:3046)(cid:1876)(cid:3047)(cid:1876)(cid:3047)(cid:1499)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:4593)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:1499)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3047)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1710)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3046)(cid:1499)(cid:1710)(cid:1710)(cid:1870)(cid:3046)(cid:1870)(cid:3047)(cid:1876)(cid:3046)(cid:1876)(cid:3047)(cid:1876)(cid:3047)(cid:1499)[stephenking]’s[maine]chronicleshaveunderscoredofpoint.[stephenking]’s[maine]chronicleshaveunderscoredgoehrpoint.
(cid:1876)(cid:3047)(cid:1499)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3047)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:4593)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:1499)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1710)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3046)(cid:1499)(cid:1710)(cid:1710)(cid:1870)(cid:3046)(cid:1870)(cid:3047)(cid:1876)(cid:3046)(cid:1876)(cid:3047)(cid:1876)(cid:3047)(cid:1499)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3047)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:4593)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:1499)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1710)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3046)(cid:1499)(cid:1710)(cid:1710)(cid:1870)(cid:3046)(cid:1870)(cid:3047)(cid:1876)(cid:3046)(cid:1876)(cid:3047)(cid:1876)(cid:3047)(cid:1499)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3047)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:4593)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1876)(cid:3046)(cid:1876)(cid:3046)(cid:1499)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:2009)(cid:2869)(cid:2009)(cid:2870)(cid:2009)(cid:3014)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1710)(cid:1710)(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:3014)(cid:1860)(cid:2869)(cid:1860)(cid:3014)(cid:1860)(cid:2870)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:1710)(cid:1876)(cid:3046)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1710)(cid:3560)(cid:1828)(cid:3046)(cid:1710)(cid:3560)(cid:1828)(cid:3047)(cid:1876)(cid:3046)(cid:1499)(cid:1710)(cid:1710)(cid:1870)(cid:3046)(cid:1870)(cid:3047)(cid:1876)(cid:3046)(cid:1876)(cid:3047)bag bs using infonce (oord et al., 2018) loss:.
lc(xs; θ) = − log.
esim(hs,h∗s ).
esim(hs,h∗.
s ) + (cid:80).
t:t(cid:54)=s esim(hs, (cid:101)bt).
where sim(a, b) is the function to measure the sim-ilarity between two representation vectors a, b, andhs = h(xs), h∗s) are the sentence repre-sentations of instances xs, x∗s..s = h(x∗.
besides, to inherit the ability of language under-standing from bert and avoid catastrophic forget-ting (mccloskey and cohen, 1989), we also add themasked language modeling (mlm) objective to ourframework.
pre-text task mlm randomly maskssome tokens in the inputs and allows the modelto predict the masked tokens, which prompts themodel to capture rich semantic information in thecontexts.
and we denote this objective as lm(θ).
accordingly, the total training objective of our.
contrastive instance learning framework is:.
l(θ) =.
lc(x; θ)+lb(θ)+λmlm(θ).
λ(t)n.(cid:88).
(cid:88).
b.x∈b.
where n = kg is the total number of instancesin the batch, λm is the weight of language modelobjective lm, and λ(t) ⊂ [0, 1] is an increasingfunction related to the relative training steps t:.
λ(t) =.
2.
1 + e−t − 1.at the beginning of our training, the value of λ(t)is relatively small, and our framework cil focuseson obtaining an accurate bag encoder (lb).
thevalue of λ(t) gradually increases to 1 as the relativetraining steps t increases, and more attention is paidto the contrastive instance learning (lc)..3 experiments.
our experiments are designed to verify the effec-tiveness of our proposed framework cil..3.1 benchmarks.
datasetnyt10-dnyt10-hgdskbp.
# rel.
532557.
# ins.
694,491362,69118,824148,666.
# test ins.
172,4483,7775,6631,940.
# test setdsmapartly mama.
table 1: statistics of various used datasets.
rel.
: rela-tion, ins.
: instance and ma: manually annotated..gds(jat et al., 2018) is created by extendingthe google re corpus with additional instances foreach entity pair, and this dataset assures that theat-least-one assumption of mil always holds..kbp(ling and weld, 2012) uses wikipedia ar-ticles annotated with freebase entries as the train-ing set, and employs manually-annotated sentencesfrom 2013 kbp slot ﬁlling assessment results (elliset al., 2012) as the extra test set..3.2 evaluation metrics.
following previous literature (lin et al., 2016;vashishth et al., 2018; alt et al., 2019), we ﬁrst con-duct a held-out evaluation to measure model per-formances approximately on nyt10-d and gds.
besides, we also conduct an evaluation on twohuman-annotated datasets (nyt10-h & kbp) tofurther support our claims.
speciﬁcally, precision-recall curves (pr-curve) are drawn to show thetrade-off between model precision and recall, thearea under curve (auc) metric is used to evaluateoverall model performances, and the precision atn (p@n) metric is also reported to consider theaccuracy value for different cut-offs..3.3 baseline models.
we choose six recent methods as baseline models..mintzregression re model under ds setting..(mintz et al., 2009) a multi-class logistic.
pcnn-att(lin et al., 2016) a piece-wisecnn model with selective attention over instances..we evaluate our method on three popular dsrebenchmarks — nyt10, gds and kbp, and thedataset statistics are listed in table 1..mtb-mil (baldini soares et al., 2019) a rela-tion learning method based on distributional simi-larity, achieves amazing results for supervised re1..nyt10(riedel et al., 2010) aligns freebase en-tity relations with new york times corpus, and ithas two test set versions: (1) nyt10-d employsheld-out kb facts as the test set and is still underdistantly supervised.
(2) nyt10-h is constructedmanually by (hoffmann et al., 2011), which con-tains 395 sentences with human annotations..reside (vashishth et al., 2018) a nn modelthat makes use of relevant side information (entitytypes and relational phrases) and employs graphcnn to capture syntactic information of instances..1for mtb-mil, we ﬁrstly conduct mtb pre-training tolearn relation representations on the entire training corpus andcontinually ﬁne-tune the model by the mil framework..6195nyt10-dp@200.p@100.p@300.p@m auc.
p@500.p@2000.p@m.method.
mintz†pcnn-att‡mtb-milreside‡redsandt‡distre†cil∗.
auc.
10.734.140.841.542.442.250.8.
52.373.076.281.878.068.090.1.
50.268.071.175.475.067.086.1.
45.067.369.474.373.065.381.8.
49.269.472.277.275.366.886.0.
-79.988.589.186.189.991.6.
-90.694.894.895.697.098.4.gdsp@1000.
-87.692.291.192.693.895.3.
-75.287.082.784.687.688.7.
-84.591.389.591.092.894.1.table 2: model performances on nyt10-d and gds.
(†)/(‡) marks the results on (nyt10-d column)/(bothcolumns) are reported in the previous papers.
bold and underline indicate the best and the second best scores, and∗ indicates that our model shows signiﬁcant gains (p > 0.05) over the second-best model based on student’s t-test..redsandt (christou and tsoumakas, 2021) atransformer-based dsre method that manages tocapture highly informative instance and label em-beddings by exploiting bert pre-trained model..distre (alt et al., 2019) a transformer-basedmodel, gpt ﬁne-tuned for dsre under the mil..3.4 evaluation on distantly supervised set.
we summarize the model performances of ourmethod and above-mentioned baseline models intable 2. from the results, we can observe that: (1)on both two datasets, our proposed framework cilachieves the best performance in all metrics.
(2)on nyt10-d, compared with the previous sotamodel distre, cil improves the metric auc(42.2→50.8) by 20.4% and the metric p@mean(66.8→86.0) by 28.7%.
(3) on gds, though themetric of previous models is already high (≈ 90.0),our model still improves it by nearly 2 percentagepoints.
(89.9→91.6 & 92.8→94.1)..in figure 10. from the curve, we can observe that:(1) compared to pr-curves of other baseline mod-els, our method shifts up the curve a lot.
(2) pre-vious sota model distre performs worse thanmodel reside at the beginning of the curve andyields a better performance after a recall-level ofapproximately 0.25, and our method cil surpassesprevious two sota models in all ranges along thecurve, and it is more balanced between precisionand recall.
(3) furthermore, as a sota scheme ofrelation learning, mtb fails to achieve competitiveresults for dsre.
this is because mtb relies onlabel information for pre-training, and noisy labelsin dsre may inﬂuence its model performance..3.5 evaluation on manually annotated set.
the automated held-out evaluation may not reﬂectthe actual performance of dsre models, as it givesfalse positive/negative labels and incomplete kb in-formation.
thus, to further support our claims, wealso evaluate our method on two human-annotateddatasets, and the results2 are listed in table 3..method.
aucpcnn-a 38.937.8distre46.0cil.
nyt10-hf147.050.955.5.p@m auc15.458.622.154.130.163.0.kbpf131.537.544.0.p@m32.846.448.2.table 3: model performances on nty10-h and kbp.
pcnn-a denotes pcnn-att.
f1 refers to micro-f1..from the above result table, we can see that: (1)our proposed framework cil can still perform wellunder accurate human evaluation, with averagely21.7% auc improvement on nyt10-h and 36.2%on kbp, which means our method can generalize.
figure 10: pr-curve on nyt10-d..the overall pr-curve on nyt10-d is visualized.
2manual evaluation is performed for each test sentence..61960.00.10.20.30.40.50.60.7recall0.30.40.50.60.70.80.91.0precisionmintz | auc-10.7pcnn-att | auc-34.1mtb-mil | auc-40.8reside | auc-41.5redsandt | auc-42.4distre | auc-42.2cil | auc-50.8to real scenarios well.
(2) on nyt10-h, distrefails to surpass pcnn-att in metric p@mean.
this indicates that distre gives a high recall buta low precision, but our method cil can boost themodel precision (54.1→63.0) while continuouslyimproving the model recall (37.8→46.0).
and thehuman evaluation results further conﬁrm the obser-vations in the held-out evaluation described above..figure 11: pr-curve on kbp..we also present the pr-curve on kbp in fig-ure 11. under accurate sentence-level evaluationon kbp, the advantage of our model is more obvi-ous with averagely 36.2% improvement on auc,17.3% on f1 and 3.9% on p@mean, respectively..3.6 ablation study.
to further understand our proposed framework cil,we also conduct ablation studies..we ﬁrstly conduct an ablation experiment to ver-ify that cil has utilized abundant instances insidebags: (1) by removing our proposed contrastiveinstance learning, the framework degenerates intovanilla mil framework, and we train the mil onregular bags (milbag).
(2) to prove the mil cannot make full use of sentences, we also train themil on sentence bags (milsent), which repeatseach sentence in the training corpus to form a bag3..methodcilmilbagmilsent.
auc50.840.3(-10.5)36.0(-14.8).
f152.247.1(-5.1)43.5(-8.7).
p@m86.070.0(-16.0)63.3(-22.7).
from table 4 we can see that: (1) milbag onlyresorts to the accurate bag-level representations totrain the model and fails to play the role of eachinstance inside bags; thus, it performs worse thanour method cil (50.8→40.3).
(2) though milsentcan access all training sentences, it loses the advan-tages of noise reduction in milbag (40.3→30.6).
the noisy label supervision may wrongly guidemodel training, and its model performance heavilysuffers from ds data noise (86.0→63.3).
(3) ourframework cil succeeds in leveraging abundantinstances while retaining the ability to denoise..to validate the rationality of our proposed pos-itive/negative pair construction strategy, we alsoconduct an ablation study on three variants of ourframework cil.
we denote these variants as:cilrandpos: randomly select an instance xs(cid:48) alsofrom bag bs as the positive pair instance for xs.
cilbagpos: just take the relational triple represen-tation (cid:101)bs as the positive pair instance for xs.
cilrandneg: randomly select an instance xt fromanother bag bt as the negative pair instance for xs.
and we summarize the model performances of.
our cil and other three variants in table 5..methodcilcilrandposcilbagposcilrandneg.
auc50.849.2(-1.6)47.8(-3.0)48.4(-2.4).
f152.250.9(-1.3)50.5(-1.7)50.6(-1.6).
p@m86.083.8(-2.2)79.2(-6.8)78.2(-7.8).
table 5: model performances of our proposed frame-work cil and its three variants..as the previous analysis in section 2.4, the threevariants of our cil framework may suffer from dsnoise: (1) both variants cilrandpos and cilbagposmay construct noisy positive pairs; therefore, theirmodel performances have a little drop (50.8→49.2,50.8→47.8).
besides, the variant cilbagpos alsorelies on the bag encoder, for which it performsworse than the variant cilrandpos (49.2→47.8).
(2)though the constructed negative pairs need not beas accurate as positive pairs, the variant cilrandnegtreats all instances equally, which gives up the ad-vantage of formed accurate representations.
thus,its model performance also declines (50.8→48.4)..table 4: model performances of three training patterns..3all the results in table 4 are obtained under the same test.
setting that uses mil bags (i.e.bert+att) as test units..we select a typical bag (see table 6) from the train-ing set to better illustrate the difference betweenmilsent, milbag and our framework cil..3.7 case study.
61970.00.10.20.30.40.50.60.7recall0.00.20.40.60.81.0precisionpcnn-att | auc-15.4distre | auc-22.1cil | auc-30.1sentencejohn mcgahern, the eldestof seven children, was bornon nov.12, 1934, in dublin..john mcgahern, whose stark..., died yesterday in dublin..predicted relations: /place borned (cid:51)b: /place borned (cid:51)c: /place borned (cid:51)s: /place borned (cid:55)b: /place borned (cid:55)c: /place deaded (cid:51).
table 6: a typical bag selected from the training set:the bag is constructed with relational triple (john mc-gahern, /place borned, dubin), and the ﬁrst sentence(s1) is clean to express relation /place borned whilethe second instance (s2) are noisy with true relation/place deaded.
s: milsent, b: milbag and c: cil..under milsent pattern, both s1, s2 are usedfor model training, and the noisy sentence s2 mayconfuse the model.
as for milbag pattern, s1 isassigned with a high attention score while s2 has alow attention score.
however, milbag only relieson the bag-level representations, and sentences likes2 can not be used efﬁciently.
our framework cilmakes full use of all instances (s1, s2) and avoidsthe negative effect of ds data noise from s2..4 related work.
our work is related to dsre, pre-trained languagemodels, and recent contrastive learning methods..dsre traditional supervised re systems heav-ily rely on the large-scale human-annotated dataset,which is quite expensive and time-consuming.
dis-tant supervision is then introduced to the re ﬁeld,and it aligns training corpus with kb facts to gen-erate data automatically.
however, such a heuristicprocess results in data noise and causes classicalsupervised re models hard to train.
to solve thisissue, lin et al.
(2016) applied the multi-instancelearning framework with selective attention mech-anism over all instances, and it helps re modelslearn under ds data noise.
following the milframework, recent works improve dsre modelsfrom many different aspects: (1) yuan et al.
(2019)adopted relation-aware attention and constructedsuper bags to alleviate the problem of bag labelerror.
(2) ye et al.
(2019) analyzed the label distri-bution of dataset and found the shifted label prob-lem that signiﬁcantly inﬂuences the performanceof dsre models.
(3) vashishth et al.
(2018) em-ployed graph convolution networks (defferrardet al., 2016) to encode syntactic information fromthe text and improves dsre models with addi-tional side information from kbs.
(4) alt et al..(2019) extended the gpt to the dsre, and ﬁne-tuned it to achieve sota model performance..pre-trained lm recently pre-trained languagemodels achieved great success in the nlp ﬁeld.
vaswani et al.
(2017) proposed a self-attentionbased architecture — transformer, and it soon be-comes the backbone of many following lms.
bypre-training on a large-scale corpus, bert (devlinet al., 2019) obtains the ability to capture a notableamount of “common-sense” knowledge and gainssigniﬁcant improvements on many tasks followingthe ﬁne-tune scheme.
at the same time, gpt (rad-ford et al., 2018), xl-net (yang et al., 2019) andgpt2 (radford et al., 2019) are also well-knownpre-trained representatives with excellent transferlearning ability.
moreover, some works (radfordet al., 2019) found that considerably increasing thesize of lm results in even better generalization todownstream tasks..contrastive learning as a popular unsuper-vised method, contrastive learning aims to learnrepresentations by contrasting positive pairs againstnegative pairs (hadsell et al., 2006; oord et al.,2018; chen et al., 2020; he et al., 2020).
wuet al.
(2018) proposed to use the non-parametricinstance-level discrimination to leverage more in-formation in the data samples.
our approach, how-ever, achieves the goal of data-efﬁciency in a morecomplicated mil setting: instead of contrasting theinstance-level information during training, we ﬁndthat instance-bag negative pair is the most effec-tive method, which constitutes one of our maincontributions.
in the nlp ﬁeld, dai and lin(2017) proposed to use contrastive learning forimage caption, and clark et al.
(2020) trained adiscriminative model for language representationlearning.
recent literature (peng et al., 2020) hasalso attempted to relate the contrastive pre-trainingscheme to classical supervised re task.
differentfrom our work, peng et al.
(2020) aims to utilizeabundant ds data and help classical supervisedre models learn a better relation representation,while our cil focuses on learning an effective andefﬁcient dsre model under ds data noise..5 conclusion.
in this work, we discuss the long-standing dsreframework (i.e.mil) and argue the mil is not efﬁ-cient enough, as it aims to form accurate bag-levelrepresentations but sacriﬁces the potential informa-.
6198tion of abundant instances inside mil bags.
thus,we propose a contrastive instance learning methodcil to boost the mil model performances.
experi-ments have shown the effectiveness of our cil withstable and signiﬁcant improvements over severalbaseline models, including current sota systems..acknowledgments.
this work has been supported in part by na-tional key research and development programof china (2018aaa0101900), zhejiang nsf(lr21f020004), key technologies and systems ofhumanoid intelligence based on big data (phaseii) (2018yfb1005100), zhejiang university ifly-tek joint research center, funds from city cloudtechnology (china) co. ltd., zhejiang university-tongdun technology joint laboratory of artiﬁcialintelligence, chinese knowledge center of engi-neering science and technology (ckcest)..references.
christoph alt, marc h¨ubner, and leonhard hennig.
2019. fine-tuning pre-trained transformer languagemodels to distantly supervised relation extraction.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages1388–1398, florence, italy.
association for compu-tational linguistics..livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-in proceedings of the 57th annual meetinging.
of the association for computational linguistics,pages 2895–2905, florence, italy.
association forcomputational linguistics..ting chen, simon kornblith, mohammad norouzi,and geoffrey e. hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproceedings of the 37th international conference onmachine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 1597–1607.
pmlr..despina christou and grigorios tsoumakas.
2021.improving distantly-supervised relation extractionthrough bert-based label & instance embeddings.
corr, abs/2102.01156..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..bo dai and dahua lin.
2017. contrastive learning forimage captioning.
in advances in neural informa-tion processing systems 30: annual conference onneural information processing systems 2017, de-cember 4-9, 2017, long beach, ca, usa, pages898–907..micha¨el defferrard, xavier bresson, and pierre van-dergheynst.
2016. convolutional neural networkson graphs with fast localized spectral ﬁltering.
inadvances in neural information processing sys-tems 29: annual conference on neural informa-tion processing systems 2016, december 5-10, 2016,barcelona, spain, pages 3837–3845..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..joe ellis, xuansong li, kira grifﬁtt, stephanie mstrassel, and jonathan wright.
2012. linguistic re-sources for 2013 knowledge base population evalua-tions.
in tac..raia hadsell, sumit chopra, and yann lecun.
2006.dimensionality reduction by learning an invariantmapping.
in 2006 ieee computer society confer-ence on computer vision and pattern recognition(cvpr’06), volume 2, pages 1735–1742.
ieee..kaiming he, haoqi fan, yuxin wu, saining xie, andross b. girshick.
2020. momentum contrast for un-in 2020supervised visual representation learning.
ieee/cvf conference on computer vision and pat-tern recognition, cvpr 2020, seattle, wa, usa,june 13-19, 2020, pages 9726–9735.
ieee..raphael hoffmann, congle zhang, xiao ling, lukezettlemoyer, and daniel s. weld.
2011. knowledge-based weak supervision for information extractionof overlapping relations.
in proceedings of the 49thannual meeting of the association for computa-tional linguistics: human language technologies,pages 541–550, portland, oregon, usa.
associa-tion for computational linguistics..sharmistha jat, siddhesh khandelwal, and parthatalukdar.
2018. improving distantly supervised rela-tion extraction using word and entity based attention.
arxiv preprint arxiv:1804.06987..yankai lin, shiqi shen, zhiyuan liu, huanbo luan,and maosong sun.
2016. neural relation extractionwith selective attention over instances.
in proceed-ings of the 54th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 2124–2133, berlin, germany.
associa-tion for computational linguistics..6199zhirong wu, yuanjun xiong, stella x. yu, and dahualin.
2018. unsupervised feature learning via non-in 2018 ieeeparametric instance discrimination.
conference on computer vision and pattern recog-nition, cvpr 2018, salt lake city, ut, usa, june18-22, 2018, pages 3733–3742.
ieee computer so-ciety..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 5754–5764..qinyuan ye, liyuan liu, maosen zhang, and xiangren.
2019. looking beyond label noise: shiftedlabel distribution matters in distantly supervised re-lation extraction.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3841–3850, hong kong, china.
as-sociation for computational linguistics..yujin yuan, liyuan liu, siliang tang, zhongfei zhang,yueting zhuang, shiliang pu, fei wu, and xiangren.
2019. cross-relation cross-bag attention fordistantly-supervised relation extraction.
in proceed-ings of the aaai conference on artiﬁcial intelli-gence, volume 33, pages 419–426..daojian zeng, kang liu, yubo chen, and jun zhao.
2015. distant supervision for relation extraction viain pro-piecewise convolutional neural networks.
ceedings of the 2015 conference on empirical meth-ods in natural language processing, pages 1753–1762, lisbon, portugal.
association for computa-tional linguistics..xiao ling and daniel s. weld.
2012. fine-grained en-tity recognition.
in proceedings of the twenty-sixthaaai conference on artiﬁcial intelligence, july 22-26, 2012, toronto, ontario, canada.
aaai press..michael mccloskey and neal j cohen.
1989. catas-trophic interference in connectionist networks: thesequential learning problem.
in psychology of learn-ing and motivation, volume 24, pages 109–165.
el-sevier..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation ex-in proceedings oftraction without labeled data.
the joint conference of the 47th annual meeting ofthe acl and the 4th international joint conferenceon natural language processing of the afnlp,pages 1003–1011, suntec, singapore.
associationfor computational linguistics..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
arxiv preprint arxiv:1807.03748..hao peng, tianyu gao, xu han, yankai lin, pengli, zhiyuan liu, maosong sun, and jie zhou.
2020.learning from context or names?
an empiricalin proceed-study on neural relation extraction.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages3661–3672, online.
association for computationallinguistics..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..sebastian riedel, limin yao, and andrew mccallum.
2010. modeling relations and their mentions with-in joint european conferenceout labeled text.
on machine learning and knowledge discovery indatabases, pages 148–163.
springer..shikhar vashishth, rishabh joshi, sai suman prayaga,chiranjib bhattacharyya, and partha talukdar.
2018.reside: improving distantly-supervised neural re-lation extraction using side information.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 1257–1266,brussels, belgium.
association for computationallinguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..6200