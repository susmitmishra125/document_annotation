piglet:language grounding through neuro-symbolic interaction in a 3d world.
rowan zellers ari holtzman matthew peters~roozbeh mottaghi~ aniruddha kembhavi~ ali farhadi yejin choi~paul g. allen school of computer science & engineering, university of washington~allen institute for artiﬁcial intelligencehttps://rowanzellers.com/piglet.
abstract.
we propose piglet: a model that learns phys-ical commonsense knowledge through interac-tion, and then uses this knowledge to groundlanguage.
we factorize piglet into a physi-cal dynamics model, and a separate languagemodel.
our dynamics model learns not justwhat objects are but also what they do: glasscups break when thrown, plastic ones don’t.
we then use it as the interface to our languagemodel, giving us a uniﬁed model of linguis-tic form and grounded meaning.
piglet canread a sentence, simulate neurally what mighthappen next, and then communicate that re-sult through a literal symbolic representation,or natural language..experimental results show that our model ef-fectively learns world dynamics, along withhow to communicate them.
it is able to cor-rectly forecast “what happens next” given anenglish sentence over 80% of the time, outper-forming a 100x larger, text-to-text approach byover 10%.
likewise, its natural language sum-maries of physical interactions are also judgedby humans as more accurate than lm alter-natives.
we present comprehensive analysisshowing room for future work..1.introduction.
as humans, our use of language is linked to thephysical world.
to process a sentence like “therobot turns on the stove, with a pan on it” (figure 1)we might imagine a physical pan object.
thismeaning representation in our heads can be seenas a part of our commonsense world knowledge,about what a pan is and does.
we might reasonablypredict that the pan will become hot – and ifthere’s an egg on it, it would become cooked ..as humans, we learn such a commonsense worldmodel through interaction.
young children learnto reason physically about basic objects by manip-ulating them: observing the properties they have,.
t.t+1.
name: egg.
name: egg.
temperature: roomtemp.
temperature: hot.
iscooked: false.
isbroken: true.
....<heatup, pan>.
iscooked: true.
isbroken: true.
....physical dynamics model.
piglet.
language model.
the robot turns on the stove, with a pan on it..the pan is now hot and the egg becomes cooked..figure 1: piglet.
through physical interaction in a 3dworld, we learn a model for what actions do to objects.
we use our physical model as an interface for a lan-guage model, jointly modeling elements of languageform and meaning.
given an action expressed symbol-ically or in english, piglet can simulate what mighthappen next, expressing it symbolically or in english..and how they change if an action is applied onthem (smith and gasser, 2005).
this process ishypothesized to be crucial to how children learnlanguage: the names of these elementary objectsbecome their ﬁrst “real words” upon which otherlanguage is scaffolded (yu and smith, 2012)..in contrast, the dominant paradigm today is totrain large language or vision models on staticdata, such as language and photos from the web.
yet such a setting is fundamentally limiting, assuggested empirically by psychologists’ failed at-tempts to get kittens to learn passively (held andhein, 1963).
more recently, though large trans-formers have made initial progress on benchmarks,they also have frequently revealed biases in thosesame datasets, suggesting they might not be solv-ing underlying tasks (zellers et al., 2019b).
thishas been argued philosophically by a ﬂurry of re-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2040–2050august1–6,2021.©2021associationforcomputationallinguistics2040:.
.
y tax tceboworht. .
j.name: vase.
name: laptop.
size: medium.
size: medium.
isbroken: false.
isbroken: false.
ispickedup: true.
ispickedup: false...isturnedon: true.
isturnedon: false.
the robot is holding a vase, and there is a laptop on the coffee table that is on..<throwheldobjectat, laptop>.
the robot throws the vase onto the coffee table..name: vase.
name: laptop.
size: medium.
size: medium.
isbroken: true.
isbroken: false.
ispickedup: false.
ispickedup: false...isturnedon: false.
isturnedon: false.
the laptop and the vase both break, with the vase shattering into smaller pieces, and the laptop powers off..figure 2: pigpen, a setting for few-shot language-world grounding.
we collect data for 280k physical interactionsin thor, a 3d simulator with 20 actions and 125 object types, each with 42 attributes (e.g.
isbroken).
we annotate2k interactions with english sentences describing the initial world state, the action, and the action result..cent work arguing that no amount of language formcould ever specify language meaning (mcclellandet al., 2019; bender and koller, 2020; bisk et al.,2020); connecting back to the symbol groundingproblem of harnad (1990)..in this paper, we investigate an alternate strategyfor learning physical commonsense through inter-action, and then transferring that into language.
we introduce a model named piglet, short forphysical interaction as grounding for languagetransformers.
we factorize an embodied agent intoan explicit model of world dynamics, and a modelof language form.
we learn the dynamics modelthrough interaction.
given an action heatup ap-plied to the pan in figure 1, the model learns thatthe egg on the pan becomes hot and cooked , andthat other attributes do not change..we integrate our dynamics model with a pre-trained language model, giving us a joint modelof linguistic form and meaning.
the combinedpiglet can then reason about the physical dynam-ics implied by english sentences describing actions,predicting literally what might happen next.
it canthen communicate that result either symbolicallyor through natural language, generating a sentencelike ‘the egg becomes hot and cooked.” our sep-aration between physical dynamics and languageallows the model to learn about physical common-sense from the physical world itself, while alsoavoiding recurring problems of artifacts and biasesthat arise when we try to model physical worldunderstanding solely through language..we study this through a new environment andevaluation setup called pigpen, short for physicalinteraction grounding paired with natural lan-guage.
in pigpen, a model is given unlimited ac-cess to an environment for pretraining, but only 500examples with paired english annotations.
modelsin our setup must additionally generalize to novel‘unseen’ objects for which we intentionally do notprovide paired language-environment supervision.
we build this on top of the thor environment.
(kolve et al., 2017), a physics engine that enablesagents to perform contextual interactions (fig 2)on everyday objects..experiments conﬁrm that piglet performs wellat grounding language with meaning.
given a sen-tence describing an action, our model predicts theresulting object states correctly over 80% of thetime, outperforming even a 100x larger model (t5-11b) by over 10%.
likewise, its generated naturallanguage is rated by humans as being more correctthan equivalently-sized language models.
last, itcan generalize in a ‘zero-shot’ way to objects thatit has never read about before in language..in summary, we make three key contributions.
first, we introduce piglet, a model decouplingphysical and linguistic reasoning.
second, we in-troduce pigpen, to learn and evaluate the transferof physical knowledge to the world of language.
third, we perform experiments and analysis sug-gesting promising avenues for future work..2 pigpen: a resource for.
neuro-symbolic language grounding.
we introduce pigpen as a setting for learning andevaluating physically grounded language under-standing.
an overview is shown in figure 2. theidea is that an agent gets access to an interactive3d environment, where it can learn about the worldthrough interaction – for example, that objects suchas a vase can become broken if thrown.
the goalfor a model is to learn natural language meaninggrounded in these interactions..task deﬁnition.
through interaction, an agent(rep-.
observes the interplay between objects o2oresented by their attributes) and actions athrough the following transition:.
2a.
o1, .
.
.
, on }{~o, state pre-action.
a.
⇥.
!{.
o01, .
.
.
, o0n }~o0, state post-action.
..(1).
|.
{z.actions change the state of a subset of objects:turning on a faucet affects a nearby sink , but itwill not change a mirror on the wall..{z.
|.
}.
}.
2041to encourage learning from interaction, and notjust language, an agent is given a small numberof natural language annotations of transitions.
wedenote these sentences as s~o, describing the statepre-action, sa the action, and s~o0 the state post-action respectively.
during evaluation, an agentwill sometimes encounter new objects o that werenot part of the paired training data..we evaluate the model’s transfer in two ways:a. pigpen-nlu.
a model is given object states~o, and an english sentence sa describing an ac-tion.
it must predict the grounded object states~o0 that result after the action is taken..b. pigpen-nlg.
a model is given object statesit must generate a.
~o and a literal action a.sentence s~o0 describing the state post-action.
we next describe our environment, feature rep-.
resentation, and language annotation process..2.1 environment: thor.
we use ai2-thor as an environment for this task(kolve et al., 2017).
in thor, a robotic agentcan navigate around and perform rich contextualinteractions with objects in a house.
for instance,it can grab an apple , slice it, put it in a fridge ,drop it, and so on.
the state of the apple , such aswhether it is sliced or cold, changes accordingly;this is not possible in many other environments..in this work, we use the underlying thor sim-ulator as a proxy for grounded meaning.
withinthor, it can be seen as a ‘complete’ meaning rep-resentation (artzi et al., 2013), as it fully speciﬁesthe kind of grounding a model can expect in itsperception within thor..objects.
the underlying thor representationof each object o is in terms of 42 attributes; we pro-vide a list in appendix b. we treat these attributesas words speciﬁc to an attribute-level dictionary;for example, the temperature hot is one of threepossible values for an object’s temperature; theothers being cold and roomtemp ..actions.
an action a in thor is a function thattakes up to two objects as arguments.
actions arehighly contextual, affecting not only the argumentsbut potentially other objects in the scene (figure 2).
we also treat action names as words in a dictionary.
filtering out background objects.
most ac-tions change the state of only a few objects, yetthere can be many objects in a scene.
we keep an-notation and computation tractable by having mod-els predict (and humans annotate) possible changes.
of at most two key objects in the scene.
as knowingwhen an object doesn’t change is also important,we include non-changing objects if fewer than twochange..exploration.
any way of exploring the environ-ment is valid for our task, however, we found thatexploring intentionally was needed to yield goodcoverage of interesting states.
similar to prior workfor instruction following (shridhar et al., 2020), wedesigned an oracle to collect diverse and interest-ing trajectories.
our oracle randomlyselects one of ten high level tasks, see appendix bfor the list.
these in turn require randomly choos-ing objects in the scene; e.g.
a vase and a laptopin figure 2. we randomize the manner in whichthe oracle performs the task to discover diversesituations..~o, a, ~o0}{.
in total, we sampled 20k trajectories.
from thesewe extracted 280k transitions (eqn 1’s) where atleast one object changes state, for training..2.2 annotating interactions with language.
2.2.1 data selection for annotationwe select 2k action state-changes from trajectoriesheld out from the training set.
we select them whilealso balancing the distribution of action types toensure broad coverage in the ﬁnal dataset.
we arealso interested in a model’s ability to generalize tonew object categories – beyond what it has readabout, or observed in a training set.
we thus se-lect 30 objects to be “unseen,” and exclude thesefrom paired environment-language training data.
we sample 500 state transitions, containing only“seen” objects to be the training set; we use 500 forvalidation and 1000 for testing..2.2.2 natural language annotationworkers on mechanical turk were shown an envi-ronment in thor before and after a given actiona. each view contains the thor attributes of thetwo key objects.
workers then wrote three en-glish sentences, corresponding to s~o, sa, and s~o0respectively.
workers were instructed to write at aparticular level of detail: enough so that a readercould infer “what happens next” from s~o and sa,yet without mentioning redundant attributes.weprovide more details in appendix c..3 modeling piglet.
in this section, we describe our piglet model.
first, we learn a neural physical dynamics model.
2042ho1.
<latexit sha1_base64="cqr6vsvzguzlpuxklqpmzy9btz4=">aaadoxicfvjlj9mwepagbzbw6siriuvusihdlqashffagcukrak7kzvvnhenqvu/itublvzo/bqucosx7jeb4sofwglzic3lsnz8/mbseeyvz8bg8flocgn38pwre9fc6zdu3ro92l9zbfstky6p4kqf5mcqm4ljyyzh00ojijzjsb541dpppqa2tmn3dlnhveapwceowe9lg/upadvpczdvmpfmis/munjlvjmlttyyxqn4jde2sdowjj0czfvbbjpttbyolevgzcsjkzt1oc2jhjswrq1wqbdq4srdcqln1k3qakkhnplfhdl+sbut2l9fobcmtc97tlmbtvtl/ss2qw3xyuqyrgqlkq4dftwprirapkqzppfavvqaqgy+14joqqo1vnw9kklmlmn1svejo8bpnyk1vhngz/qsrm7yp34blvhsk+thjms+m4v+vdz84zolctterttcqm4udpwa/bq0hvrova1qg1x6sutblwlogtfp/7kxuxbzogxdvzfj5pzsg+mno+tpkh73bhjwstugpxkppccpsekekwpyhhyrmahkm/lcvpjvwffgr/az+lv2dxa6n3djt4lffwaihrpu</latexit>.
....ho2.
<latexit sha1_base64="qdwil9dc1n1euq5jgckmt4efrp4=">aaadoxicfvjlj9mwepagbzbw6siriuvusihdlewiwxefhlggfonurtru0csdpfb9igwhtlg58wu4wpffwpeb4sofwglzic3lsnz8/mbseeyvz8bg8fed4nlu5stx966f12/cvhv7sh/nxkhauxxtxzu+y8egzxlhllmoz5vgednh03zxorwfvkdtmjlv7llcqybssojrsj7kbvdtaxaef27ezc7nfz+zpfdkqsy7allbmb7fk4m2qdkbienkonspdtozorvaaskhyyzjxnmpa20z5dieaw2warqaeiceshbopm5vrxm99mwskpt2r9poxf79woewbxres83abnpa8l+2sw2lz1phzfvblhqdqkh5zfxuniwamy3u8quhqdxzuuz0dhqo9a3rrre1t0yrd71khavo+0ypozozet5nnxldpvbbcmgxwlk/iln22vz077xmg58pjdshcquwfnjzyecx6kel8bxv3jsknvilh7sudcngvhgd/p8bk2s3r8mw9hutbg7jnjg5gcwho/jtk+hr826d9sg98oa8igl5so7ik3jmxosst+qz+uk+bt+ch8hp4nfandjp3twlpql+/welprpv</latexit>.
action application.
mlpapply.
<latexit sha1_base64="zpfxfrp2awq1em5tglqotqo0h4o=">aaadoxicfvjnj9mwepwghv3cvxeoccmiqkicqmrbyo8r4mcbfuwiuyu1vtvxp6lvo7bscbreoffrumkrx8krg+lkh8btg0ralpesp795m2opjzvsoirjbzvbpd3lv/b2r4bxrt+4eat1cpvu6cjy7hettt1pwaeuofzikmrzyxfukvesnt1f+s/eoxvc529pyxcoimvfrhagt41a9waec7kqphnvrublnxmyixdvnwq14068smgbjdvos9q6o4ngdzdwvfcye5fgxd+jdq1lscs4xcocfa4n8blk2pcwb4vuwk7euuuppdoojtr6lvo0yv+okee5t1cpvyqgqdv0lcl/+fofty6gpchnqzjzdafjisps0bip0vhy5cqxhgc3wt814lowwmm3rlfffzke1e8blyk5sn5kmgtmkvi8yvqutnxotugclfat/6i8a7kpap4lkzesayvbjvktzwspu7dwc/s/zfhed+61qquk7anyadztmk/kev+ftorrmd/dmprzk2xoyty4pewkjzvxmyft42f1bo2zu+w+e8gs9pqds5esy3qms4/se/vmvgrfg+/bj+dnwhrs1df3wmocx78byruugw==</latexit>.
<latexit sha1_base64="0abhhpkgf29puvvxn/yje6akeyy=">aaadqxicfvlnjtmwepagbzbw14ujl4hqbejqjqsshffagqtikejusk0vtdxjatu/ke3afitpwnnwhsnpwsnwq1y54lq5kjzljgs+fzp2/oyvz8bg8fed4nlu5stx966f12/cvhv7sh/nxkhauxxtxzu+y8egzxlhllmoz5vgednh03zxorwfvkdtmjlv7llcqybssojrsj7kbgdrogfrugf2nhdu3jszs3pfz2ypvhiqs5ohttyyxqn4jde2sdowjj0cz/vbbjpttbyolevgzcsjkzt1oc2jhjswrq1wqbdq4srdcqln1k3qaaidz8yiqml/pi1w7n8vhajt5uc927znpq0l/2wb1lz4nnvmvrvfsdebipphvkvtc6iz00gtx3oavdofa0tnoifa38jefffzy7t60kveuec0z5qaqjmj531wizfsy78nf3yplfwjkmwfzux/xmu+8znsub0iv2phitcxbn6jfloax/vovalqg1x6kutblwlog9fp/7kxuxbzogxdvzfj5pzsg5pdufj4fl99mjx63m3qhrlh7pohjcfpyrf5ry7jmfdyixwmx8jx4fvwi/gz/fq7bjvdm7ukj8hvp1cnfvw=</latexit>.
ˆho 1....ˆho 2.
<latexit sha1_base64="lcctrn5jui+ppitihmg7pacll3k=">aaadqxicfvlnjtmwepagbzbw14ujl4hqbejqpqsshffagqtikejusk0vtdxjytu/ke3afitpwnnwhsnpwsnwq1y54lq5kjzljgs+fzp2/gyvz8bg8fed4nlu5stx966f12/cvhv7sh/nxkhau5xqxzu+y8agzxinllmoz5vgebnh02zxorwfvkdtmjlv7llcmybcspxrsj5kbwdruoj1iqbbzrkrmyz1sab43cyfv06lh82djh0m41g8kmgbjdswjj0cp/vbbjjxtbyolevgzhqcv3bmqftgotzhuhusgc6gwkmhegsamvvv00qhnplhudl+sbut2l9fobcmzc97tnmbtvtl/ss2rw3+boayrgqlkq4d5twprira5krzppfavvqaqgy+14iwoifa38jefffzy7t60kveuec0zxqaqplr8z6rkrv2sd+gc77uyvpryalpzqj/rzxf+exp3a6rkbwwkjkla79epy2nr33n3lsowsr9ycwgcwhnjev0/9yyxlt5hyah35vx5pzsg5pd0fjxkh77zhj0vnugpxkp3ccpyzg8jufkftkme0ljj/kzfcffg2/bj+bn8gvtgux0b+6sngs//wbz4rb9</latexit>.
objectdecodertdec.
<latexit sha1_base64="2gp2+xykdi+qahqkmatfm5chgxa=">aaadnxicfvlnjtmwepaghv3cxxdoietehyq4vomcxb5xwielypg2uys1vtvxj6lvo47scbryeu/dfy48cwduicuvgnvmqfqwksx//uz/pgkphau4/r4txnm9em1v/3p44+at23c6b3fprk4mxwhxupulfcxkuecabem8ka2csiwep7oxs/35ezrw6okufiwofosfyaqh8ts4cz9rqnm0c6f12cweczlktzdx9bjtjxvxsqjt0g9alzvymj4idpoj5pxcgrgea4f9uksra0ocs6zdpljyap9bjkmpc1bor27vqx098swkyrtxp6boxf7t4ubzu1cpt1xwbdd1s/jfumff2dhiiakscau+tprvmiidlqcstyrbtnlhaxajfk0rn4ibtn5srsyqkism/tdqxhgqvm3kbsqp4pm2a1ba8be9hktcgk3+e4q8zaaq/a6m3aimdw6nslweeat20ssv0p+wwtd+cm9lnedaphejmfzbvhbn/t8zuazn/b2god+b/uawbiozw17/as9+96x7/klzoh32gd1kj1mfpwfh7du7yqpg2sf2mx1hx4nvwy/gz/brbrrsnd73weuc338ap8usmg==</latexit>.
<latexit sha1_base64="dsjfxbpnrwowtpn8zynxn7n+bky=">aaadmxicfvjlb9naen6aasw8uhanlhyrangi7iiexwo4ceeuibsv4igabybjkvuwztelyeufwxwo/jreeff+bjveb5xqrlrnt988d3akugrr0vrij7qye/xa9b0b8c1bt+/c7ezfo7amio59bqsh0wissqgx74stefosgioknhtzn0v7yrmsfuz/cosshwqmwkwebxeouedbfobc54wry7tqqxlt109gnw7as1esbiosav3wynfop9rnx4zxcrxjeqwdzgnphh7ics6xjvpkygl8dlmcbkhbor36vf918jgw42rikbztkhx7d4qhzzfdbu8fbmy3buvyx7zb5savhl7osnko+brqpjkjm8lygmlyehinfweajxf6tfgmclgli2tvuzv0gszn1ks8b8nbzjsgnal+3myjprvf2mo4jcuzf75gt9tsodr3iurgmko4xaiwzu6gsjcwfovhtwjfh8l9kjhagxrmc6cpgvpan/p/bkkv3yko4zjstba5jdvg+kcxpe+lh190d183g7thhrjh7cnl2et2yn6xi9znnhn2lx1j36mf0ux0m/q1do12mpj7rcxr7z9kcrao</latexit>.
~o0name: vasesize: mediumisbroken: trueispickedup: false.
isturnedon: false.
~o<latexit sha1_base64="tk3hxnfvirkvryije/qednlvrco=">aaadmhicfvjlb9naen66bvrzsoebf4sicxgi7iiexwo4ceeuibsv4igabybokvuwztelqex/0isc+tvwqr3yk9gkpucemtjqvv3mubotfvjyf8c/t4ltnrs3b+3uhbfv3l13v7p/4niakjj2uzggtjowkixgvhno4mlbccqtejln3i7sj2divhj92c0lhcritzgids5to86j9ax5lwzgju1cevwzuh51unevxkq0czigdfkjr6p9yccdg14q1i5lshaqxiubvkbocil1mjywc+azyhhgoqafdlgt26+jp54zrxnd/mgxldm/iypqdtgc91tgpnbdtid/zruubvj6waldla41xxwaldjyjlrmiholqu7k3apgjhyvez8caxd+yq0qqprokpnseknfqfi2kxmuu8hp2yyhtokipyzrupjx/md03myz1b6xjnesgclnepkxmwezvbbwo/s/rfjbt+5jgqto0pmqbcovnndvo//njvtkzeswdp3ejotbsgmod3rji1786wx38e2zqbvsmxvcnrgevwkh7d07yn3g2qw7zf/zt+b78cp4fvytxiotjuyha0nw+w/lmq/3</latexit>.
name: vasesize: mediumisbroken: falseispickedup: true.
isturnedon: false.
action: throwheldobjectat.
target: floor.
a<latexit sha1_base64="eli5feejppem9uunrf3iqjsevxu=">aaadkhicfvjlb9naen6aqot5txdkyhehiq6rtzhgwaehlogikbzseqrxzukssg9rdlwarpwprndk13bdvfjl2cq+4iqy0mq+/ea5s5oxwnlo08ut6nr29rs7uzfjw7fv3l23t3//2lukjpak045oc/colcuek9z4whkcytwe5npxc/vjozjxzn7kwylda4vvyywba/vpkds98jmtva3zs71o2k2xkmycraed0cjr2x60prg5wrm0ldv438/skoc1ecupcr4pko8lycku2a/qgke/rjdtz5phgrkly0fhwe6w7n8rnri/6c14gucjx7ctyh/z+hwpxw5rzcuk0cpvoxgle3bjygbjsbfk1rmaqjikvszyagssw6ravuylwzh73hpjluhlnlmqlbmll9osofbqs3smv6qkx+fhbnfmc9o+v6txkjnczrk5c1og3f9z+a2g3yj8fyb3vkqcdvs0hgavbi7mdap/56bsyi3ooi7d3mtrw7ijjp91s4nu+uf55/bvs0g74qf4jj6itlwqh+ktobi9iqwjr+kb+b79ih5gv6lllwu01cq8ec2jfv8b/q8m6g==</latexit>.
sa<latexit sha1_base64="ler1jafp96te2mtqrxbvv0e7xeo=">aaado3icfvi7bxnben4caclxcqck4isfhcisu4aezqqunigg4sssbvlz6/f55x2cducgznulv4ywsn4inr2ipwdtx5gzcsot5ttvznyeo3kpham0/betxdq9foxq3rx4+o2bt2539u8co1nzjn1uplgnotiuqmofbek8ls2cyiwe5poxs/vjb7rogp2efiwofbratauhcts4c3+ygzlxcxwud/xyn79dxy873bsxritzblkduqyro/f+tducgf4p1mqlodfi0pjghiwjlrgoh5xdevgcchweqeghg/lvj3xymdctzgpsojqsfxs+wonyy+kcpwkauu3bkvyxbvdr9pnic11whjqve00rmzbjlmnjjsiij7kialgvodaez8acpzc8vhzvsrlwfgx14jli3mykc+vm8lm2a1e68ak9hguetibcj+mizeaqfa+s3hjmwnxokrszj8jdhylfyfgti2/c5n6waigmfeyhyasfz7vv9p/chf67br3hcdibbhnltshxqs970kvfpe0evmg2ai/dyw/yi5axz+yqvwzhrm84+8y+sk/sw/q9+hn9in6vxaodjuyua0n05y+verua</latexit>.
objectencodertenc.
<latexit sha1_base64="faldccjyxbbe5frg+r9it6vukma=">aaadnxicfvjlb9naen6aasw8ujghlhyreuiqoqujjhvw4iioutnwiqnovrk7q+zdmh1dgmxxa7jckd/cgrviyl9gnfqae8piq/n2m5mdnudakokojr/vbjd2l1+5unctvh7j5q3bvf07j86wkgakrlj4lnihshoyksqfzwuc16mc03txsrgfvgd00ppjwhuw0tw3mpock6emvxuj5jrps+q4nlyjwzjqv2bexu97/xgqryxabsmw9fkrr9p9ydezwvfqmcqud248jauavbxjcgv1mjqoci4wpiexh4zrcjnqxumdpftmlmos+mmowrn/r1rco7fsqfdsfuw2bq35l9u4poz5pjkmkkmpa50ok1venmoaes0kgic18oallp6vkzhz5ij82zpzdkliov3qqaqsxikukymv5lisuyyccvjjtw0xpimw/hhm3mvt3b2xqdyeswjbkvjrf8rtd2hiv+cnhfdgd+5tacjj4umq4zhrvqyrvv/ptzpzn6/dmpr7m9zckm1wcjayphne7572d1+0g7th7rmh7bebsmfskl1mr2zebpveprmv7gvwlfgr/ax+nbsgo23mxdar4pcfwwospa==</latexit>.
actionencoder.
the robot is holding a glass vase..the robot throws the vase..language model.
tlm<latexit sha1_base64="bwxv2o8nnh4h6p1viaq01k2mz6e=">aaadk3icfvjlb9naen6aasu8mpyjf4sicxgibeccywu9ckcilzq2uhjf483ywwuf1u4ygiz/eq5w5ndwanhlf3st+iatykir+fab585owkjhki5/bgu3tm/eur1zj7x77/6d3c7e/pkzpexy50yae5gcqyk09kmqxivciqhu4nk6e7uwn39e64trpzqvckqg1yithmht487uuafn06w6rcfv+6n63ongvxgp0szigtbljryp94lt4ctwuqemlsg5qrixnkrakuas63byoiyazydhgycaflprtey8jp54zhjlxvqjkvqyf0duojybq9r7lvp067yf+s/bokts9agsuigjnv8vykozkykwy4gmwiinofcaubw+14hpwqinp6xwfvvketz8ar2k4ib5m8ktffpbl9usrene5/yyrklpdflp0xmbtvx7xlq5lsxy3cyrgjmjsn21hq/r/5bfiz+5dwvaigofvuowuyllumr0/9yexrl5hyah35tkfus2wdnzxvkif5+87b68atzohz1ij9ltlrbx7ic9y8eszzgr2rf2lx0lvgc/gl/b75vrsnxepgqtcf5carg6dum=</latexit>.
ha<latexit sha1_base64="iptzzwz9ijk0oggcismvc68hbq0=">aaadn3icfvjlb9naen66byp5pexyi0wehdhenidbsai9ceeuibsv4igab8bokvuwdtdtw8ohfg1xopjtohfdxpkhrbmfceizatxffjo788xkzoyn4+9bwfbojzu3dm+hd+7eu/+gt7d/alslkq6p4kqfz2cqm4ldyyzh81ijiizjwty/auxnf6gnu/kdxzq4flbiljmk1lot3keqwm6y3m3qiuszxadmibxyunetxj8exeujnkhsgj5p5wsyf+yku0urgdjsdsamkri0ywfamsqxdtpkyal0dgwopjqg0izdsoo6euyzazqr7y+00zl9+4udyzrkvgets1m3nes/bkpk5q/gjsmysijpklbe8ciqqgljnguaqeuld4bq5non6aw0uosb14kikm6zvpedshwftrtmoagcmxrvztvywz5223dnl1pzpybzdnlmdo+v5mufky2bitkl5hyyc23gy/tt0vjwd+5dirqs0k9dcroqcfw7vv/pjcmvm9dhgpq9sda3zbocphskzwfx+xf9w9ftbu2sa/kipcejeukoyrtyqoaekk/km/lcvgbfgh/bz+dxyjxyat88jb0jfv8b94etig==</latexit>.
action summarizermlp .
<latexit sha1_base64="j5yl7sr91yru8/lffsopltfavle=">aaadmxicfvjlb9naen6aasw8uhanlhyreuiq2yaexwp64ebfkehbky6i8wbirlipa3cmczz/dfc48mt6q1z5e2wsh3bcgwk1337z3nnjcikcxfhfxnbl/+q16wc3wpu3bt+52zm8d+pmatkoujhgnmfguaqnaxik8bywccqtejbn36zsz5/qomh0r1owofkqazevhmht486dlhbbvlun7/r1ueqpurlu40437svrixzb0oaua6q/pgz204nhpujnxijzwyquafsbjcel1mfaoiyazyhhoycaflprte6/jh57zhjnjfvhu7rm/46oqdm3vjn3veazt21bkf+yduuavhpvqhcloeabqtnsrmsi1tciibdiss49ag6f7zxim7dayy+svuwvkoq1n1svqthi3mzyc8vm8ewbtsid+niewyuprsh/ntpvs5lq30srt5izi7slmmpmbjm7tpax+t+yeoin975ac2ts0yofmyty1fwj/+cm9mbn6zam/d4k21uyc06f9zlnvfjdi+7r62addthd9og9yql7yy7yw9zna8zzxb6yb+x78co4ch4gvzauwv4tc5+1jpj9b9kod/m=</latexit>.
language model.
tlm<latexit sha1_base64="bwxv2o8nnh4h6p1viaq01k2mz6e=">aaadk3icfvjlb9naen6aasu8mpyjf4sicxgibeccywu9ckcilzq2uhjf483ywwuf1u4ygiz/eq5w5ndwanhlf3st+iatykir+fab585owkjhki5/bgu3tm/eur1zj7x77/6d3c7e/pkzpexy50yae5gcqyk09kmqxivciqhu4nk6e7uwn39e64trpzqvckqg1yithmht487uuafn06w6rcfv+6n63ongvxgp0szigtbljryp94lt4ctwuqemlsg5qrixnkrakuas63byoiyazydhgycaflprtey8jp54zhjlxvqjkvqyf0duojybq9r7lvp067yf+s/bokts9agsuigjnv8vykozkykwy4gmwiinofcaubw+14hpwqinp6xwfvvketz8ar2k4ib5m8ktffpbl9usrene5/yyrklpdflp0xmbtvx7xlq5lsxy3cyrgjmjsn21hq/r/5bfiz+5dwvaigofvuowuyllumr0/9yexrl5hyah35tkfus2wdnzxvkif5+87b68atzohz1ij9ltlrbx7ic9y8eszzgr2rf2lx0lvgc/gl/b75vrsnxepgqtcf5carg6dum=</latexit>.
s o .
<latexit sha1_base64="99uqbasrvay02f4vvinx5hjslv0=">aaadqnicfvlnbhmxehaxamx5s+hizuxejzheuwwppvbagquiskstli2iwweysek1v/zsabd2dxgarndkjxgfbogrb5xkd92empi1n7/59xiyugplcfxjk7iyffxa9z0b4c1bt+/c7ezeo7a6mhz7xetttjowkixcpgmsefoahcktejlnxi3sj2dorndqa81lhbaqkzerhmhto87jnnnyboefv87wi5eeixcxsv3xt+prpxv34qvemybpqjc1cjtadbbtsezvgyq4bgshsvzs0iehwsxwyvpzlihpimebhwokteo3ffadpflmojpo44+iamlejhbq2ev73rmamtp124l8l21q0erg6iqqk0lfv4umlyxir4vprgnhkjocewdccn9rxkdggjofyatkuuksrn9svcrxklzn5abkqednbdagtojtewyxpdsa/f+pvm1mrftegbmwtbvcljfppspi7kwfx6p/lynv/etelwiathnmujb5aee1a/t/3irauxkdhqhfm2r9szbb8v4ved6l37/ohr5snmihpwap2vowsh12yn6wi9znnh1mx9hx9i34hvwmfgw/v67bvhnzn7uk+pmxrayyjw==</latexit>.
the vase breaks and is no longer being held..figure 3: piglet architecture.
we pretrain a model of physical world dynamics by learning to transform objects ~oand actions a into new updated objects ~o0.
our underlying world dynamics model – the encoder, the decoder, andthe action application module, can augment a language model with grounded commonsense knowledge..from interactions, and second, integrate with a pre-trained model of language form..3.1 modeling physical dynamics.
we take a neural, auto-encoder style approach tomodel world dynamics.
an object o gets encodedrdo.
the model likewise encodesas a vector ho 2rda, using it to ma-an action a as a vector ha 2nipulate the hidden states of all objects.
the modelcan then decode any object hidden representationback into a symbolic form..3.1.1 object encoder and decoder.
we use a transformer (vaswani et al., 2017) tordo, and thenencode objects into vectors oanother to decode from this representation..2.encoder.
objects o are provided to the encoderas a set of attributes, with categories c1,..., cn.
eachattribute c has its own vocabulary and embeddingec.
for each object o, we ﬁrst embed all the at-tributes separately and feed the result into a trans-former encoder tenc.
this gives us (with positionembeddings omitted for clarity):.
ho = tenc.
e1(o1), .
.
.
, ecn(ocn).
(2).
⌘.
⇣.
decoder.
we can then convert back into the origi-nal symbolic representation through a left-to-righttransformer decoder, which predicts attributes one-by-one from c1 to cn.
this captures the inherentcorrelation between attributes, while making no in-dependence assumptions, we discuss our orderingin appendix a.2.
the probability of predicting thenext attribute oci+1 is then given by:p(oci+1|.
ho,e1(o1),..., eci(oci)ho, o:ci)=tdec⌘⇣.
(3).
3.1.2 modeling actions as functions.
we treat actions a as functions that transform thestate of all objects in the scene.
actions in ourenvironment take at most two arguments, so weembed the action a and the names of its arguments,concatenate them, and pass the result through amultilayer perceptron; yielding a vector representa-tion ha..applying actions.
we use the encoded actionha to transform all objects in the scene, obtainingupdated representations ˆho0 for each one.
we takea global approach, jointly transforming all objects.
this takes into account that interactions are contex-tual: turning on a faucet might ﬁll up a cup ifand only if there is one beneath it..letting the observed objects in the interactionbe o1 and o2, with encodings ho1 and ho2 respec-tively, we model the transformation via the follow-ing multilayer perceptron:.
, ˆho02.]
= mlpapply.
[ˆho01⇣⇥the result can be decoded into symbolic formusing the object decoder (equation 3)..ha, ho1, ho2.
⇤⌘.
(4).
..3.1.3 loss function and training.
we train our dynamics model on (~o,a,~o0) transi-tions.
the model primarily learns by running ~o,athrough the model, predicting the updated outputstate ˆho0, and minimizing the cross-entropy of gen-erating attributes of the real changed object ~o0.
wealso regularize the model by encoding objects ~o, ~o0and having the model learn to reconstruct them.
weweight all these cross-entropy losses equally.
wediscuss our architecture in appendix a.1; it uses3-layer transformers, totalling 17m parameters..20433.2 language grounding.
after pretraining our physical dynamics model, weintegrate it with a transformer language model(lm).
in our framework, the role of the lm willbe to both encode natural language sentences ofactions into a hidden state approximating ha, aswell as summarizing the result of an interaction(~o,a,~o0) in natural language..choice of lm.
our framework is compatiblewith any language model.
however, to explore theimpact of pretraining data on grounding later inthis paper, we pretrain our own with an identicalarchitecture to the smallest gpt2 (radford et al.
(2019); 117m).
to handle both classiﬁcation andgeneration well, we mask only part of the attentionweights out, allowing the model to encode a “preﬁx”bidirectionally; it generates subsequent tokens left-to-right (dong et al., 2019).
we pretrain the modelon wikipedia and books; details in appendix d..we next discuss architectural details of perform-ing the language transfer, along with optimization..3.2.1 transfer architectureenglish actions to vector form.
given a natu-ral language description sa of an action a, like“the robot throws the vase,” for pigpen-nlu, ourmodel will learn to parse this sentence into a neuralrepresentation ha, so the dynamics model can sim-ulate the result.
we do this by encoding sa throughour language model, tlm , with a learned lineartransformation over the resulting (bidirectional) en-coding.
the resulting vector hsa can then be usedby equation 4..summarizing the result of an action.
forpigpen-nlg, our model simulates the result of anaction a neurally, resulting in a predicted hiddenstate ˆho for each object in the scene o. to writean english summary describing “what changed,”we ﬁrst learn a lightweight fused representationof the transition, aggregating the initial and ﬁnalstates, along with the action, through a multilayerperceptron.
for each object oi we have:.
h oi = mlp ([hoi, ˆho0i.
, ha])..(5).
we then use the sequence [h o1, h o2] as bidi-rectional context for our our lm to decode from.
additionally, since our test set includes novel ob-jects not seen in training, we provide the names ofthe objects as additional context for the lm genera-tor (e.g.
‘vase, laptop’); this allows a lm to copythose names over rather than hallucinate wrong.
ones.
importantly we only provide the surface-form names, not underlying information aboutthese objects or their usage as with few-shot scenar-ios in the recent gpt-3 experiments (brown et al.,2020) – necessitating that piglet learns what thesenames mean through interaction..3.2.2 loss functions and training.
modeling text generation allows us to incorporatea new loss function, that of minimizing the log-likelihood of generating each s~o0 given previouswords and the result of equation 5:.
p(sposti+1|.
s ~o0,1:i) = tlm(h o1, h o2, s ~o0,1:i).
(6).
we do the same for the object states s~o pre-action,using hoi as the corresponding hidden states..for pigpen-nlu, where no generation isneeded, optimizing equation 5 is not strictly nec-essary.
however, as we will show later, it helpsprovide additional signal to the model, improvingoverall accuracy by several percentage points..4 experiments.
we test our model’s ability to encode language intoa grounded form (pigpen-nlu), and decode thatgrounded form into language (pigpen-nlg)..4.1 pigpen-nlu results..we ﬁrst evaluate models by their performance onpigpen-nlu: given objects ~o, and a sentence sadescribing an action, a model must predict the re-sulting state of objects ~o0.
we primarily evaluatemodels by accuracy; scoring how many objects forwhich they got all attributes correct.
we comparewith the following strong baselines:a. no change: this baseline copies the initial state.
of all objects ~o as the ﬁnal state ~o0..b. gpt3-175b (brown et al., 2020), a very largelanguage model for ‘few-shot’ learning usinga prompt.
for gpt3, and other text-to-textmodels, we encode and decode the symbolicobject states in a json-style dictionary format,discussed in appendix a.4..c. t5 (raffel et al., 2019).
with this model, weuse the same ‘text-to-text’ format, howeverhere we train it on the paired data from pig-pen.
we consider varying sizes of t5, fromt5-small – the closest in size to piglet, upuntil t5-11b, roughly 100x the size.
(alberti et al., 2019)-style.
this paper origi-nally proposed a model for vcr (zellers et al.,.
d..204494.8.
94.798.198.495.896.593.5.
97.595.197.495.8.
99.0.
10.4.
72.0.
78.581.8.
89.3.accuracy (%).
test.
attribute-level accuracy (test-overall,%).
size.
distance.
mass.
temperature.
isbroken.
overall.
seen unseen.
8-way.
8-way.
8-way.
3-way.
boolean.
model.
no change.
gpt3-175b (brown et al., 2020)t5-11b (raffel et al., 2019)t5-3bt5-larget5-baset5-small.
e alberti et al.2019, pretrained dynamics.
alberti et al.2019g&d2019, pretrained dynamicsg&d2019.
txet-ot-txet.lytstreb.val.
27.4.
23.868.566.656.556.039.9.
61.39.743.815.1.
25.5.
22.464.263.354.153.936.2.
53.96.835.311.3.piglet.
81.8.
81.1.
29.9.
22.479.577.169.269.257.0.
71.416.260.923.1.
83.8.
24.0.
21.459.158.749.148.838.0.
48.13.726.97.3.
80.2.
83.2.
73.783.981.681.881.182.2.
87.753.483.068.6.
92.3.
84.1.
77.088.990.084.687.584.9.
87.643.686.947.3.
91.9.
96.3.
89.594.394.094.393.693.8.
97.584.094.082.2.
99.2.
86.0.
84.295.495.696.396.189.6.
93.488.193.788.3.
99.8.table 1: overall results.
left: we show the model accuracies at predicting all attributes of an object correctly.
wecompare piglet with ‘text-to-text’ approaches that represent the object states as a string, along with bert-styleapproaches with additional machinery to encode inputs or decode outputs.
piglet outperforms a t5 model 100xits size (11b params) and shows gains over the bert-style models that also model action dynamics through alanguage transformer.
right: we show several attribute-level accuracies, along with the number of categories perattribute; piglet outperforms baselines by over 4 points for some attributes such as size and distance..2019a), where grounded visual information isfed into a bert model as tokens; the trans-former performs the grounded reasoning.
weadapt it for our task by using our base lmand feeding in object representations from ourpretrained object encoder, also as tokens.
ourobject decoder predicts the object, given thelm’s pooled hidden state.
this is “pretraineddynamics,” we also consider a version withouta randomly initialized dynamics model.
(gupta and durrett, 2019)-style.
thiso paperproposes using transformers to model physicalstate, for tasks like entity tracking in recipes.
here, the authors propose decoding a physicalstate attribute (like iscooked ) by feeding themodel a label-speciﬁc [cls] token, and thenmapping the result through a hidden layer.
wedo this and use a similar object encoder as our(alberti et al., 2019)-style baseline..e..we discuss hyperparameters in appendix a.3..results.
from the results (table 1), we can drawseveral patterns.
our model, piglet performs bestat getting all attributes correct; doing so over 80%on both validation and test sets, even for novelobjects not seen during training.
the next clos-est model is t5-11b, which scores 68% on vali-dation.
though when evaluated on objects ‘seen’during training it gets 77%, that number drops byover 18% for unseen objects.
on the other hand,piglet has a modest gap of 3%.
this suggests thatour approach is particularly effective at connectingunpaired language and world representations.
at.
model.
accuracy (val;%).
piglet, no pretraining.
piglet, non-global mlpapply.
piglet, global mlpapplypiglet, global mlpapply, gen. loss (6).
piglet, symbols only (upper bound).
table 2: ablation study on pigpen-nlu’s validationset.
our model improves 6% by modeling global dy-namics of all objects in the scene, versus applying ac-tions to single objects in isolation.
we improve another3% by adding an auxiliary generation loss..the other extreme, gpt3 does poorly in its ‘few-shot’ setting, suggesting that size is no replacementfor grounded supervision..piglet also outperforms ‘bert style’ ap-proaches that control for the same language modelarchitecture, but perform the physical reasoninginside the language transformer rather than as aseparate model.
performance drops when the phys-ical decoder must be learned from few paired exam-ples (as in gupta and durrett (2019)); it drops evenfurther when neither model is given access to ourpretrained dynamics model, with both baselinesthen underperforming ‘no change.’ this suggeststhat our approach of having a physical reasoningmodel outside of an lm is a good inductive bias..4.1.1 ablation studyin table 2 we present an ablation study of piglet’scomponents.
of note, by using a global represen-tation of objects in the world (equation 4), we get.
2045over 6% improvement over a local representationwhere objects are manipulated independently.
weget another 3% boost by adding a generation loss,suggesting that learning to generate summarieshelps the model better connect the world to lan-guage.
last, we benchmark how much headroomthere is on pigpen-nlu by evaluating model per-formance on a ‘symbols only’ version of the task,where the symbolic action a is given explicitly toour dynamics model.
this upper bound is roughly7% higher than piglet, suggesting space for futurework..model.
bleuval test.
bertscore human (test; [91, 1])val testfaithfulnessfluency.
t546.6 43.4lm baseline 44.6 39.7piglet49.0 43.9.
82.2 81.081.6 78.883.6 81.3.human.
44.5 45.6.
82.6 83.3.
0.820.910.92.
0.94.
0.15-0.130.22.
0.71.table 3: text generation results on pigpen-nlg,showing models of roughly equivalent size (up to117m parameters).
our piglet outperforms the lmbaseline (using the same architecture but omitting thephysical reasoning component) by 4 bleu points, 2bertscore f1 points, and 0.35 points in a human eval-uation of language faithfulness to the actual scene..4.2 pigpen-nlg results.
next, it leads to more faithful generation as well..next, we turn to pigpen-nlg: given objects ~o,and the literal next action a, a model must generatea sentence s~o0 describing what will change in thescene.
we compare with the following baselines:a. t5.
we use a t5 model that is given a json-style dictionary representation of both ~o and a,it is ﬁnetuned to generate summaries s~o0.
b. lm baseline.
we feed our lm hidden statesho from our pretrained encoder, along withits representation of a. the key difference be-tween it and piglet is that we do not allow itto simulate neurally what might happen next –mlpapply is never used here..size matters.
arguably the most important factorcontrolling the ﬂuency of a language generator isits size (kaplan et al., 2020).
since our lm couldalso be scaled up to arbitrary size, we control forsize in our experiments and only consider modelsthe size of gpt2-base (117m) or smaller; we thuscompare against t5-small as t5-base has 220mparameters.
we discuss optimization and samplinghyperparameters in appendix a.3..evaluation metrics.
we evaluate models overthe validation and test sets.
we consider threemain evaluation metrics: bleu (papineni et al.,2002) with two references, the recently proposedbertscore (zhang et al., 2020), and conduct ahuman evaluation.
humans rate both the ﬂuency ofpost-action text, as well as its faithfulness to trueaction result, on a scale from.
1 to 1..results.
we show our results in table 3. of note,piglet is competitive with t5 and signiﬁcantlyoutperforms the pure lm baseline, which uses apretrained encoder for object states, yet has thephysical simulation piece mlpapply removed.
thissuggests that simulating world dynamics not onlyallows the model to predict what might happen.
 .
5 analysis.
5.1 qualitative examples..we show two qualitative examples in figure 4, cov-ering both pigpen-nlu as well as pigpen-nlg.
in the ﬁrst row, the robot empties a held mug that isﬁlled with water.
piglet gets the state, and gener-ates a faithful sentence summarizing that the mugbecomes empty.
t5 struggles somewhat, emptyingthe water from both the mug and the (irrelevant)sink .
it also generates text saying that the sinkbecomes empty, instead of the mug..in the second row, piglet correctly predicts thenext object states, but its generated text is incom-plete – it should also write that the mug becomesﬁlled wtih coffee.
t5 makes the same mistakein generation, and it also underpredicts the statechanges, omitting all changes to the mug ..we suspect that t5 struggles here in part becausemug is an unseen object.
t5 only experiences itthrough language-only pretraining, but this mightnot be enough for a fully grounded representation..5.2 representing novel words.
the language models that perform best today aretrained on massive datasets of text.
however, thishas unintended consequences (bender et al., 2021)and it is unlike how children learn language, withchildren learning novel words from experience(carey and bartlett, 1978).
the large scale of ourpretraining datasets might allow models to learnto perform physical-commonsense like tasks forwrong reasons, overﬁtting to surface patterns ratherthan learning meaningful grounding..we investigate the extent of this by traininga ‘zero-shot’ version of our backbone lm onwikipedia and books – the only difference is that.
2046state pre-action.
predicted post-action states.
piglet.
t5.
ground truth post-action states.
t.name: sink.
name: sink.
name: sink.
name: sink.
isfilledwithliquid:true.
isfilledwithliquid:true.
isfilledwithliquid:false.
isfilledwithliquid:true.
name: mug.
isfilledwithliquid:true.
ispickedup: true.
name: mug.
name: mug.
name: mug.
isfilledwithliquid:false.
isfilledwithliquid:false.
isfilledwithliquid:false.
ispickedup: true.
ispickedup: true.
ispickedup: true.
the robot empties the mug..<emptyliquid,mug>.
the mug is now empty..the sink is now empty..the mug is no longer ﬁlled with water..t.name: mug.
name: mug.
name: mug.
name: mug.
isfilledwithliquid:false.
temperature: roomtemp.
name: coffeemachine.
isturnedon: false.
containsobject: mug.
the robot turns on the coffee maker..<toggleobject,coffeemaker>.
isfilledwithliquid:true.
isfilledwithliquid:false.
isfilledwithliquid:true.
temperature: hot.
temperature: roomtemp.
temperature: hot.
name: coffeemachine.
name: coffeemachine.
name: coffeemachine.
isturnedon: true.
isturnedon: true.
isturnedon: true.
containsobject: mug.
containsobject: mug.
containsobject: mug.
the coffee machine is turned on..the coffee machine becomes on..the coffee maker is now on and the mug is hot and ﬁlled with coffee..figure 4: qualitative examples.
our model piglet reliably predicts what might happen next (like the mug be-coming empty in row 1), in a structured and explicit way.
however, it often struggles at generating sentences forunseen objects like mug that are excluded from the training set.
t5 struggles to predict these changes, for example,it seems to suggest that emptying the mug causes all containers in the scene to become empty..sense reasoning at the representation and conceptlevel.
the aim is to train models that learn to ac-quire concepts more like humans, rather than per-forming well on a downstream task that (for hu-mans) requires commonsense reasoning.
thus, thiswork is somewhat different versus other 3d em-bodied tasks like qa (gordon et al., 2018; daset al., 2018), along with past work for measur-ing such grounded commonsense reasoning, likeswag, hellaswag, and vcr (zellers et al., 2018,2019b,a).
the knowledge covered is different, as itis self-contained within thor.
while vcr, for in-stance, includes lots of visual situations about whatpeople are doing, this paper focuses on learning thephysical properties of objects..zero-shot generalization.
there has been a lotof past work involved with learning ‘zero-shot’:often learning about the grounded world in lan-guage, and transferring that knowledge to vision.
techniques for this include looking at word embed-dings (frome et al., 2013) and dictionary deﬁni-tions (zellers and choi, 2017).
in this work, wepropose the inverse.
this approach was used tolearn better word embeddings (gupta et al., 2019)or semantic tuples (yatskar et al., 2016), but weconsider learning a component to be plugged intoa deep transformer language model..past work evaluating these types of zero-shotgeneralization have also looked into how wellmodels can compose concepts in language to-gether (lake and baroni, 2018; ruis et al., 2020).
our work considers elements of compositional-ity through grounded transfer.
for example, in.
figure 5: pigpen-nlu performance of a zero-shotpiglet, that was pretrained on books and wikipediawithout reading any words of our ‘unseen’ objects like‘mug.’ it outperforms a much bigger t5-11b overall,though is in turn beaten by piglet on unseen objectslike ‘sink’ and ‘microwave.’.
we explicitly exclude all mentioned sentences con-taining one of our “unseen” object categories.
inthis setting, not only must piglet learn to groundwords like ‘mug,’ it must do so without having seenthe word ‘mug’ during pretraining.
this is signiﬁ-cant because we count over 20k instances of ‘mug’words (including morphology) in our dataset..we show results in figure 5. a version ofpiglet with the zero-shot lm does surprisinglywell – achieving 80% accuracy at predicting thestate changes for “mug” – despite never havingbeen pretrained on one before.
this even out-performs t5 at the overall task.
nevertheless,piglet outperforms it by roughly 7% at unseenobjects, with notable gains of over 10% on highlydynamic objects like toaster s and sink s..6 related work.
grounded commonsense reasoning.
in thiswork, we study language grounding and common-.
2047pigpen-nlg, models must generate sentencesabout the equivalent of dropping a ‘dax’, despitenever having seen one before.
however, our workis also contextual, in that the outcome of ‘droppinga dax’ might depend on external attributes (likehow high we’re dropping it from)..structured models for attributes and ob-jects.
the idea of modeling actions as functionsthat transform objects has been explored in thecomputer vision space (wang et al., 2016).
pastwork has also built formal structured models forconnecting vision and language (matuszek et al.,2012; krishnamurthy and kollar, 2013), we take aneural approach and connect today’s best modelsof language form to similarly neural models of asimulated environment..7 conclusion.
in this paper, we presented an approach piglet forjointly modeling language form and meaning.
wepresented a testbed pigpen for evaluating ourmodel, which performs well at grounding languageto the (simulated) world..acknowledgments.
we thank the reviewers for their helpful feedback,and the mechanical turk workers for doing a greatjob in annotating our data.
thanks also to zakstone and the google cloud tpu team for helpwith the computing infrastructure.
this work wassupported by the darpa cwc program througharo (w911nf-15-1-0543), the darpa mcs pro-gram through niwc paciﬁc (n66001-19-2-4031),and the allen institute for ai..references.
chris alberti, jeffrey ling, michael collins, and davidreitter.
2019. fusion of detected objects in textin proceedings offor visual question answering.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2131–2140..yoav artzi, nicholas fitzgerald, and luke s zettle-moyer.
2013. semantic parsing with combinatorycategorial grammars.
acl (tutorial abstracts), 3..emily m bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language modelsbe too big.
proceedings of facct..emily m. bender and alexander koller.
2020. climb-ing towards nlu: on meaning, form, and under-in proceedings of thestanding in the age of data.
58th annual meeting of the association for compu-tational linguistics, pages 5185–5198, online.
as-sociation for computational linguistics..yonatan bisk, ari holtzman, jesse thomason, jacobandreas, yoshua bengio, joyce chai, mirella lap-ata, angeliki lazaridou, jonathan may, aleksandrnisnevich, et al.
2020. experience grounds lan-guage.
arxiv preprint arxiv:2004.10151..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..s. carey and e. bartlett.
1978. acquiring a single new.
word..abhishek das, samyak datta, georgia gkioxari, ste-fan lee, devi parikh, and dhruv batra.
2018. em-in proceedings of thebodied question answering.
ieee conference on computer vision and patternrecognition, pages 1–10..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..li dong, nan yang, wenhui wang, furu wei,xiaodong liu, yu wang, jianfeng gao, mingzhou, and hsiao-wuen hon.
2019.uniﬁedlanguage model pre-training for natural languagearxiv preprintunderstanding and generation.
arxiv:1905.03197..andrea frome, greg corrado, jonathon shlens, samybengio, jeffrey dean, marc’aurelio ranzato, andtomas mikolov.
2013. devise: a deep visual-semantic embedding model..daniel gordon, aniruddha kembhavi, mohammadrastegari, joseph redmon, dieter fox, and alifarhadi.
2018. iqa: visual question answering in in-teractive environments.
in proceedings of the ieeeconference on computer vision and pattern recog-nition (cvpr)..aditya gupta and greg durrett.
2019. effective useof transformer networks for entity tracking.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 759–769..2048tanmay gupta, alexander schwing, and derek hoiem.
2019. vico: word embeddings from visual co-in proceedings of the ieee interna-occurrences.
tional conference on computer vision, pages 7425–7434..stevan harnad.
1990. the symbol grounding prob-physica d: nonlinear phenomena, 42(1-.lem.
3):335–346..richard held and alan hein.
1963. movement-produced stimulation in the development of visuallyguided behavior.
journal of comparative and physi-ological psychology, 56(5):872..mandar joshi, danqi chen, yinhan liu, daniel s weld,luke zettlemoyer, and omer levy.
2020. spanbert:improving pre-training by representing and predict-ing spans.
transactions of the association for com-putational linguistics, 8:64–77..jared kaplan, sam mccandlish, tom henighan,tom b brown, benjamin chess, rewon child, scottgray, alec radford, jeffrey wu, and dario amodei.
2020. scaling laws for neural language models.
arxiv preprint arxiv:2001.08361..diederik p. kingma and jimmy ba.
2014. adam:corr,.
a method for stochastic optimization.
abs/1412.6980..eric kolve, roozbeh mottaghi, winson han, eli van-derbilt, luca weihs, alvaro herrasti, daniel gor-don, yuke zhu, abhinav gupta, and ali farhadi.
2017. ai2-thor: an interactive 3d environment forvisual ai.
arxiv preprint arxiv:1712.05474..jayant krishnamurthy and thomas kollar.
2013.jointly learning to parse and perceive: connectingnatural language to the physical world.
transac-tions of the association for computational linguis-tics, 1:193–206..brenden lake and marco baroni.
2018. generalizationwithout systematicity: on the compositional skillsof sequence-to-sequence recurrent networks.
in in-ternational conference on machine learning, pages2873–2882.
pmlr..cynthia matuszek, nicholas fitzgerald, luke zettle-moyer, liefeng bo, and dieter fox.
2012. a jointmodel of language and perception for grounded at-in proceedings of the 29th inter-tribute learning.
national coference on international conference onmachine learning, pages 1435–1442..james l mcclelland, felix hill, maja rudolph, ja-son baldridge, and hinrich sch¨utze.
2019. ex-tending machine language models toward human-arxiv preprintlevelarxiv:1912.05877..language understanding..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
techni-cal report, openai..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv e-prints..laura ruis, jacob andreas, marco baroni, dianebouchacourt, and brenden m lake.
2020. a bench-mark for systematic generalization in grounded lan-guage understanding.
advances in neural informa-tion processing systems, 33..noam shazeer and mitchell stern.
2018. adafactor:adaptive learning rates with sublinear memory cost.
in international conference on machine learning,pages 4603–4611..mohit shridhar, jesse thomason, daniel gordon,yonatan bisk, winson han, roozbeh mottaghi,luke zettlemoyer, and dieter fox.
2020. alfred:a benchmark for interpreting grounded instructionsfor everyday tasks.
in proceedings of the ieee/cvfconference on computer vision and pattern recog-nition, pages 10740–10749..linda smith and michael gasser.
2005. the develop-ment of embodied cognition: six lessons from ba-bies.
artiﬁcial life, 11(1-2):13–29..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st internationalconference on neural information processing sys-tems, pages 6000–6010.
curran associates inc..xiaolong wang, ali farhadi, and abhinav gupta.
2016..actions ˜ transformations.
in cvpr..mark yatskar, vicente ordonez, and ali farhadi.
2016.stating the obvious: extracting visual commonsense knowledge.
in proceedings of the 2016 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 193–198..chen yu and linda b smith.
2012. embodied at-tention and word learning by toddlers.
cognition,125(2):244–262..rowan zellers, yonatan bisk, ali farhadi, and yejinchoi.
2019a.
from recognition to cognition: vi-sual commonsense reasoning.
in the ieee confer-ence on computer vision and pattern recognition(cvpr)..rowan zellers, yonatan bisk, roy schwartz, andyejin choi.
2018. swag: a large-scale adversar-ial dataset for grounded commonsense inference.
inproceedings of the 2018 conference on empirical.
2049methods in natural language processing, pages 93–104, brussels, belgium.
association for computa-tional linguistics..rowan zellers and yejin choi.
2017. zero-shot activ-ity recognition with verb attribute induction.
in pro-ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 946–958..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019b.
hellaswag: canin pro-a machine really ﬁnish your sentence?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4791–4800, florence, italy.
association for computationallinguistics..tianyi zhang, v. kishore, felix wu, kilian q. wein-berger, and yoav artzi.
2020. bertscore: evaluatingtext generation with bert.
arxiv, abs/1904.09675..2050