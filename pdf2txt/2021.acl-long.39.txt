explaining contextualization in language models using visual analytics.
rita sevastjanova∗ and aikaterini-lida kalouli† and christin beck† andhanna sch¨afer∗ and mennatallah el-assady∗university of konstanzfirstname.lastname@uni-konstanz.de.
abstract.
despite the success of contextualized languagemodels on various nlp tasks, it is still unclearwhat these models really learn.
in this paper,we contribute to the current efforts of explain-ing such models by exploring the continuumbetween function and content words with re-spect to contextualization in bert, based onlinguistically-informed insights.
in particular,we utilize scoring and visual analytics tech-niques: we use an existing similarity-basedscore to measure contextualization and inte-into a novel visual analytics tech-grate itnique, presenting the model’s layers simulta-neously and highlighting intra-layer propertiesand inter-layer differences.
we show that con-textualization is neither driven by polysemynor by pure context variation.
we also provideinsights on why bert fails to model words inthe middle of the functionality continuum..1.introduction.
the rise of contextualized language models (lm),i.e., contextualized word and sentence represen-tations, such as elmo (peters et al., 2018) andbert (devlin et al., 2019), has brought many well-known nlp tasks to a tremendous breakthrough.
contextualized embeddings have replaced earlierstatic embeddings (mikolov et al., 2013; penning-ton et al., 2014; conneau et al., 2017), creatingnew standards for the state-of-the-art.
lms havelearned highly transferable and task-agnostic prop-erties of language (e.g., belinkov, 2018; conneauet al., 2018; peters et al., 2018), even to a degreeof imitating the classical nlp pipeline (tenneyet al., 2019a).
despite these research efforts, itremains yet unclear as to what extent lms likebert capture complex linguistic phenomena andwhether different linguistic properties are learned.
∗ contribution to the visualization part.
† equal contribution to the computational linguistics part..across the different layers of the model’s archi-tecture: the existing evidence is conﬂicting andin some cases even contradictory (rogers et al.,2020).
one recent line of work (ethayarajh, 2019)explores the actual contextualization captured inthese models, i.e., the degree to which a word ismodeled as context-speciﬁc.
this sheds light on thecontext-speciﬁcity of individual words and the de-gree of contextualization of different word groups.
this paper contributes to this line of work by ex-amining the degree of contextualization of functionvs. content words.
we treat functionality as a con-tinuum, comparing and contrasting bert’s (de-vlin et al., 2019) modeling of categories of wordswithin this continuum with the expected modelingaccording to the theoretical linguistic literature.
ithas been repeatedly shown that lms fail to gener-alize and capture the compositionality of languagebecause they struggle with words of high function-ality, e.g., quantiﬁers, prepositions, modals, con-junctions (dasgupta et al., 2018; naik et al., 2018;mccoy et al., 2019, to name only a few).
thus, ourlinguistically-informed analysis sheds light on thepeculiarities of these phenomena and contributesto our better understanding of bert..this paper utilizes the self-similarity contextual-ization score of ethayarajh (2019) for better compa-rability.
the exploration of the scores and phenom-ena is enabled by lmexplorer, a visual analytics(va) technique for the layer-wise explanation ofcontextualized word embeddings.
lmexplorer con-tributes a new perspective on the learned patternsof the model, and shows clusters and score devel-opments in the model’s layers simultaneously..overall, the contribution of this paper is two-fold: (1) we generate insights as to how bert cap-tures function vs. content words (sections 4 and 5),and (2) present a novel visual analytics techniquethat facilitates such insights by explaining lmsthrough contextualization scoring (section 3)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages464–476august1–6,2021.©2021associationforcomputationallinguistics4642.interpretability of language models.
research on the interpretability of lms has beenpursued in two main directions, mainly focusingon bert.
for one, probing tasks are used to in-vestigate the linguistic properties learned by thelm by training a linear model on the basis of thecorresponding contextualized embeddings for theprediction of speciﬁc linguistic properties.
for an-other, the interpretability of lms has been exploredvia adversarial datasets to assess the performanceof an lm with respect to challenging linguistic phe-nomena.
to further explore the interpretablity oflms, we see work coming from the ﬁeld of va aspromising.
va techniques have been used exten-sively for exploring and interpreting different deeplearning models (hohman et al., 2019), incl.
lms.
probing – probing experiments have shown thatbert’s transformer architecture encodes seman-tic information such as word senses and seman-tic roles (reif et al., 2019; tenney et al., 2019b;ettinger, 2020; zhao et al., 2020), syntactic in-formation in the form of constituents and hier-archical structure (goldberg, 2019; hewitt andmanning, 2019; warstadt and bowman, 2020; chiet al., 2020), morphosyntactic and morphologicalfeatures (edmiston, 2020; tenney et al., 2019b),and discourse-related information necessary fortasks such as coreference resolution (tenney et al.,2019b).
moreover, the traditional nlp pipelinesequence of pos tagging, syntactic parsing, namedentity recognition, semantic role labeling and coref-erence resolution can be mapped onto bert’stransformer layers from lower to higher (tenneyet al., 2019a).
accordingly, several probing studieshave shown that bert captures a hierarchy of lin-guistic information (e.g., jawahar et al., 2019; linet al., 2019; edmiston, 2020): surface features arerepresented best in the lower layers, while syntacticfeatures are captured best in the middle layers.
themiddle to higher layers represent morphologicalfeatures best, and semantic information is capturedbest in the higher layers..adversarial testing – adversarial testing hasshown that lms struggle in making generalizationson basic lexical relations (glockner et al., 2018),identifying ungrammaticality (marvin and linzen,2018), efﬁciently capturing challenging linguis-tic phenomena, such as negation (dasgupta et al.,2018; richardson et al., 2020), modals, quantiﬁersand monotonicity (richardson et al., 2020), pas-sives (zhu et al., 2018), conditionals (richardson.
et al., 2020), conjunctions (mccoy et al., 2019),implicatives and factives (mccoy et al., 2019), andmodeling human reasoning patterns, such as nu-merical or common-sense reasoning (naik et al.,2018).
overall, the evidence from adversarial test-ing contradicts the results of the probing studies:if the lm indeed is able to acquire ‘deep’ linguis-tic knowledge (e.g., about syntactic hierarchies), itshould be able to deal with the phenomena presentin the adversarial test sets..contextualization – despite the conﬂicting ev-idence about the linguistic capacities of lms likebert, it is widely acknowledged that the wordembeddings generated by such models are contex-tualized, i.e., there is no ﬁnite number of wordsense representations and a word has different vec-tor representations across different contexts.
par-ticularly, by assessing a word’s contextualizationon the basis of self-similarity scores, ethayarajh(2019) shows that the embeddings become morecontextualized, i.e., more context-speciﬁc, in theupper layers of bert.
moreover, it has been shownthat contextualized embeddings generally clusterwith one another with respect to word senses (reifet al., 2019; wiedemann et al., 2019)..visual lm explanations – approaches for vi-sual lm explanations can be grouped into twomain categories.
one strand of research focuseson transformer-based lms and explains how theylearn through visualizing attentions (e.g., nl-ize (liu et al., 2018), seq2seq-vis (strobelt et al.,2018), bertviz (vig, 2019), exbert (hoover et al.,2020), sanvis (park et al., 2019), and attentionflows (derose et al., 2021)).
another strand ofresearch explains what the model learns by visual-izing word embeddings.
although most existingwork on embedding explanation is based on prob-ing tasks, visualization of embedding characteris-tics has emerged as an active research topic.
theﬁrst tools were related to the exploration of staticembeddings, e.g., by liu et al.
(2017), who visual-ize word2vec and glove embeddings, focusing onanalogy exploration.
heimerl and gleicher (2018)explain the same models and present visualizationsthat support analysis of multiple tasks, among oth-ers, the analysis of local word neighborhoods.
also,boggust et al.
(2019) explain static embeddingsof word2vec, glove, and fasttext.
their expla-nations focus on local neighborhoods visualizedusing small multiples by applying a dimensionalityreduction.
berger (2020) has recently presented a.
465figure 1: the main visualization of our technique uses layer-wise interlinked-projections that show embeddingsfrom the layers of an lm in a 2d space; here, bert’s 12 layers.
the words in each layer are depicted as pointsand connected to their corresponding position throughout layers.
by selectively mapping the colors of the links tocomputed scores or identiﬁed clusters, this visualization provides a global overview of the analyzed corpus..visual approach for exploring correlations betweenembedding clusters in bert for a single model’slayer at a time.
the novelty of our approach is theexplanation of contextualized word embeddingsthrough contextualization scores that are visualizedfor all of the model’s layers simultaneously..3 lmexplorer: visual analytics technique.
to support the analysis of word contextualizationwithin the functionality continuum, we have devel-oped a va technique called lmexplorer.
this tech-nique discloses layer-wise spatial and score-basedpatterns in the learned embedding representations.
using interlinked embedding projections, we showthe spatial relations of the high-dimensional em-bedding space.
to provide further insights into theword contextualization, the technique utilizes scor-ing functions (i.e., word self-similarity) as a con-textualization explanation.
the scores are used toexplore and navigate the embedding space, whichis facilitated by supporting views and interactions.
the technique is integrated into the lingvis.io frame-work (el-assady et al., 2019a)..task analysis – the technique is designed tosupport model analysts in gaining insights intothe word contextualization.
the proposed designis informed by a set of tasks that were obtainedthrough investigating the analysts in their typicalanalysis workﬂow.
these are: (t1) analyze spa-tial structure of the embedding space; (t2) gain aglobal overview of the corpus; (t3) conduct interac-tive pattern analysis; (t4) create user-deﬁned wordgroupings for detailed inspection; and (t5) conducta focused analysis of contextualization..3.1 layer-wise interlinked projections.
the main visual components of our technique arelayer-wise interlinked projections (figure 1) – anovel visualization displaying layers of the lm si-multaneously for effective spatial pattern analysis.
motivation – the design of this visualizationwas informed by t1 and t2, i.e., corpus level ex-ploration of embedding spatial patterns in differ-ent layers of the lm.
projection-based visualiza-tions are the most common methods to visualizeword embeddings (e.g., smilkov et al., 2016; liuet al., 2017; van aken et al., 2019; aken et al.,2020) and although some approaches have enabledthe exploration of embeddings in different layers(e.g., smilkov et al., 2016; van aken et al., 2019;aken et al., 2020), they typically visualize onlyone layer of the lm at a time.
however, changesin embedding positions and their neighborhoodsacross layers can be an indicator of the model cap-turing new context information.
to support suchanalyses, our technique displays the embeddingsfor all layers of the lm simultaneously and visuallyhighlights changes in their neighborhoods..design rationale – to implement the explo-ration of such spatial patterns, we use a dimension-ality reduction technique on the computed embed-ding vectors from each layer of the lm.
in partic-ular, we reduce the 768-dimensional embeddingvectors to two dimensions, used as x and y coordi-nates to visualize words in one layer.
using thistechnique, words with similar embeddings are rep-resented by similar coordinates in the 2d space.
intotal, 12 projections are created, each representingone layer of the bert-base model.
the projections.
466are ordered vertically underneath each other, start-ing from layer one at the very top and ending withthe last layer at the bottom.
the words in the pro-jection are visualized as shapes.
by default, theyare displayed as circles and colored according tothe word’s position in the 2d space, cf., el-assadyet al.
(2019b).
after displaying the projections, weadd connecting lines between layers to support theanalysis of word position changes in the visualizedspace.
to reduce the number of crossing edges, weadditionally apply an edge-bundling technique thatcombines neighboring edges in a more coherentrepresentation.
an example of the visualizationis shown in figure 1. in our approach, both con-textualized word embeddings and aggregated wordembeddings (i.e., average or median embedding ofall contexts of a word) can be visualized..the words in each projection (i.e., layer) are rep-resented by different embedding vectors.
hence,although we visualize the same words, the consec-utive projections differ and may even get rotatedor ﬂipped due to artefacts that are common formost of the dimensionality reduction techniques(e.g., umap (mcinnes et al., 2018), t-sne (van dermaaten and hinton, 2008)).
even if words main-tained their neighborhoods, the rotation of theprojections would prevent the users from easilycomprehending on embedding positional changes.
thus, to prevent such artifacts, we apply an exten-sion of umap called alignedumap.
it reducesthe rotation artifacts by using the already projecteddata as an anchoring.
hence, we project the em-beddings from layer 2 by specifying relations to theprojection of embeddings from layer 1, and iteratethis alignment process up to the last layer..this spatialization concept enables an effectivelayer comparison as well as the detection of wordgroups with similar spatial patterns (t1, t2).
theinterlinked projections beneﬁt the analysis of wordfunctionality across layers, especially in the ex-ploratory phase of the analysis.
the user can brushneighboring words in the projection to gain anoverview of word groups that are relevant to ob-serve in detail.
to support hypothesis generationand testing, we provide multiple interaction tech-niques that help explore the analyzed corpus.
whenhovering over a word in the projection, the wordand its path through the different layers gets high-lighted (t3) and its contexts are displayed for close-reading.
to ease the analysis of words with com-mon spatial patterns, the user can brush a group of.
neighboring words in the projection and drag themaside.
this reduces the displayed information andsupports a more detailed pattern analysis (t4)..3.2 explaining contextualizationwe employ common approaches in explaining con-textualization and compute multiple word-levelcontextualization scores.
these are integrated intothe interlinked-projection view as an overlay (t5).
scoring functions – to explain the contextu-alization of a word’s representation, ethayarajh(2019) introduces three metrics: self-similarity,maximum explainable variance, and intra-sentencesimilarity.
in this paper, we focus on the word self-similarity, which ethayarajh describes as “the aver-age cosine similarity of a word with itself across allthe contexts in which it appears, where representa-tions of the word are drawn from the same layer ofa given model.” although the analysis in this paperis solely based on the self-similarity score, the tech-nique can be effortlessly extended to further expla-nation scores.
for instance, we have explored theword’s contextualization also by deﬁning a base-line embedding and obtaining its similarity to thecontextualized one.
it is possible to create multiplebaselines by either reducing the context size (e.g.,extracting embedding from a word without a sur-rounding context) or selecting a speciﬁc layer of thelm for reference.
ethayarajh (2019) describes the0th layer as an appropriate baseline.
however, forspeciﬁc hypothesis testing, one could even selectone of the upper layers as a reference layer..score overlay – the scores are mapped to thewords in the interlinked-projection view to providefurther insights into the embedding contextualiza-tion.
in particular, we use three visual design ele-ments: (a) color, (b) shape, and (c) size.
first, weuse a diverging color scale that maps the scoresfrom brown (min value) to green (max value)colors.
second, we highlight words having ex-treme values (i.e., one standard deviation above themin value and below the max value of the score’sdistribution in the particular layer) by displayingthem as rectangles instead of the default circles.
third, we map the score’s range across all layersof the model to the shape’s size, supporting layercomparison (shown in figure 4)..3.3 supporting visualizations & interactions.
to support the exploration of words with commoncharacteristics (e.g., spatial patterns), we providesupporting visualizations and interactions..467(a) the range of contextualization scores for all words in different layers aredisplayed in distribution plots, supporting layer comparison..(b) words can be ﬁltered and highlighted inthe projection by specifying a score’s range..figure 2: the distribution plots show that the average self-similarity of words decreases and, hence, word contex-tualization increases with increasing layers of bert, which replicates the ﬁndings by ethayarajh (2019)..the distribution plots provide an overview ofthe embedding contextualization scores (i.e., self-similarity) and are placed next to the correspondinglayer projection.
they enable the analysis of scorechanges through the model’s layers.
as shownin figure 2a, the self-similarity score decreases inupper layers, and the standard deviation increasesaccordingly.
the distribution plots can be furtherused for ﬁltering words by specifying a range inthe contextualization score (shown in figure 2b).
words that ﬁt within the range are highlighted inthe interlinked-projection view..for tailored score-pattern analysis, we displaythe score changes in an additional, more compactmatrix plot visualization (shown in figure 3).
thecolumns of the matrix represent words in the cor-pus, and rows show the layer-wise contextualiza-tion scores.
the user can deﬁne a query by select-ing a word in the matrix plot and the words withsimilar patterns (i.e., the response of the query)are highlighted in the interlinked-projection view.
to obtain similar patterns, we ﬁrst represent eachword by a vector of 12 score values correspondingto each layer for bert-base.
we then compute thecosine similarity on these vectors to retrieve wordswith similar score patterns..figure 3: the matrix plot gives an overview of the self-similarity score changing over layers.
by clicking on acolumn, the matrix is queried for similar score-patterns..4 exploring contextualization in bert.
while ethayarajh (2019) initially found that theincrease in contextualization across the differentbert layers (i.e., the decreasing self-similarity)seems to be driven by polysemy, ‘stopwords’ suchas and, of, the and to seem to contradict this con-clusion.
stopwords, which in essence are functionwords, also become increasingly contextualized inthe upper layers.
thus, contextualization seems notto be entirely driven by polysemy, but rather thevariety of contexts a word appears in (ethayarajh,2019).
however, function words are not a homoge-neous class, and some function words indeed havesemantic content in addition to having a grammati-cal function.
thus, we decided to investigate func-tion and content words in more detail, using thelmexplorer to explore contextualization in bertwith respect to the functionality continuum..4.1 functional and content words.
in theoretical linguistics, there is a traditional dis-tinction between function and content words.
sev-eral criteria have been proposed to distinguish be-tween the two groups, e.g., semantic content, mem-bership openness, ﬂexibility of syntactic attach-ment, separability from complements (corver andvan riemsdijk, 2001).
while content words com-prise a speciﬁc semantic content and contributeto the principal meaning of a sentence, functionwords are rather ‘non-conceptual’ and mainly ful-ﬁll some grammatical function (e.g., expressingmodality or deﬁniteness), gluing content wordstogether.
furthermore, content words are open-class because new members can freely be added.
in contrast, function words are closed-class, i.e.,they are members of a ﬁxed set.
additionally, con-tent words are ﬂexible with respect to the syntacticphrase they attach to, e.g., the verb think can becomplemented by an np or a clause, while functionwords typically only combine with a speciﬁc syn-tactic phrase, e.g., a determiner with an np.
also,.
468figure 4: exploring bert’s layer 10 allows us to draw insights about function and content words (section 5)..in contrast to content words, function words aregenerally inseparable from their content word com-plements, i.e., they cannot be detached from theirlexical heads, e.g., in in the house, the functional incannot be separated from the content word house.
despite these ‘hard’ criteria, the two categories arenot rigid.
function and content words form a quasi-continuum (‘squishiness’), a gradience between thetwo categories (ross, 1972; emonds, 1985).
thiscontinuum is based on the fact that some wordsshare properties of both categories.
such wordscan be placed on a sliding scale of functionality.
for example, prepositions are less functional thanarticles, e.g., some prepositions are associated witha locative or directional meaning, but they are alsomore functional than nouns or verbs, e.g., becausethey are inseparable from their content words..within computational linguistics and especiallynlp, this functionality continuum has not receivedmuch attention.
prototypically functional wordsare mostly treated as stopwords and often re-moved from the analysis.
nevertheless, a morelinguistically-motivated look in this continuum cancontribute to the explainability of lms like bert..4.2 visual analysis.
utilizing the lmexplorer, we visualize a randomsubset of 800 unique sentences of the rte-1 (da-gan et al., 2005), rte-2 (bar-haim et al., 2006)and rte-3 (giampiccolo et al., 2007) corpora.
these corpora contain sentence pairs originally in-tended for natural language inference.
they stemfrom the news domain and thus contain variablecontent.
the pairs are split into single sentencesand mapped to their pos tags based on the stanfordpos tagger (toutanova et al., 2003).
we visualizethe bert-base embeddings and self-similarity of.
496 unique words with a frequency greater than 5and lower than 50, following ethayarajh (2019)..the distribution plots show at-a-glance that eachof the distributions roughly follows that of a nor-mal distribution and that the mean self-similaritydecreases across layers while the standard devia-tion increases, see figure 2a.
this observation is inline with the ﬁnding by ethayarajh (2019) that con-textualized word representations are more context-speciﬁc in higher layers, i.e., the self-similarity de-creases overall.
moreover, we ﬁnd speciﬁc spatialpatterns in the interlinked-projection view, see fig-ure 1, i.e., speciﬁc groups of content words, e.g.,named entities, and speciﬁc groups of functionwords, e.g., prepositions, seem to cluster togetheracross the layers.
by ﬁltering for different scoreranges based on self-similarity via the distributionplots, we ﬁrst investigate the three groups min, maxand mid (one standard deviation around the meanstandard deviation; grey area) in more detail.
inaddition, we explore the self-similarity patterns inthese areas in the matrix plots..score areas – across the layers, mostly namedentities, e.g., place names (israel, korea, haiti),monosemous words (rabies), and polysemouswords1, whose senses are closely related (e.g., re-search, currency, marijuana), occupy the max areaacross all layers, see, e.g., layer 10 in figure 4. inthe min area, highly polysemous words, e.g.
ﬁeld,.
1the distinction between polysemy and homonymy is con-troversial.
we take polysemous words to have multiple senseswhich exhibit some kind of semantic relation, e.g., home asa building/location vs. as a social institution.
homonymouswords comprise unrelated senses, e.g., bank as ﬁnancial in-stitution vs. as natural object (utt and pad´o, 2011) – of-ten of different syntactic categories, e.g., present as a gift(noun) and as the verb to present.
we base our decisions onhomonymy/polysemy on wordnet 3.1 (fellbaum, 1998)..469higher end of mid and moving towards the middle.
monosemous words like attorney, river, tsunamiare mostly found in the max range, with a decreas-ing tendency across layers, but remain in the upperends of the max area.
polysemous words whosesenses are very closely related, e.g., universe, state-ment, are also mostly found in the max area, whilehighly polysemous words whose senses are looselyrelated, e.g., ﬁeld, are located in the min area inthe lower layers and although their self-similarityincreases, they remain in the min-mid area acrosslayers.
finally, homonymous words, e.g., set, arein the min area across layers.
these observationslead to new insights into how bert captures con-textualization, see section 5..5.insights: the functionality continuum.
during our exploration, we came across patternsthat ﬁt to the theory of the functionality continuumand others that were contrary to our expectations.
above all, we observed that contextualization isneither triggered merely by polysemy nor by vari-ation in context.
to explain the observed patterns,a) we positioned the deﬁned categories within thefunctionality continuum2 based on the inherent lin-guistic properties of the words and on insights fromlexical semantics, and b) we identiﬁed three criteriaas potential triggers of contextualization, as shownin table 1. the ﬁrst criterion refers to the sense vari-ation (sense var.
), i.e., whether a word has multiplesenses (high variation), or only one or multiple butvery closely related senses (low variation).
the sec-ond criterion captures syntactic context variation(synctx.
var.
), i.e., whether a word needs to be partof a speciﬁc syntactic structure (low) or is ﬂexiblein terms of attachment and can be found in differ-ent kinds of syntactic structures (high).
anotherpotential trigger we identiﬁed is that of variationof semantic context (semctx.
var.).
this captureswhether the contexts in which a word can occurare semantically similar (low) or different (high)to one another.
based on these triggers and pre-vious ﬁndings on contextualization by ethayarajh(2019), we derive the expected contextualization(exp.
contextual.)
of each of the predeﬁned cate-gories.
we can then compare this to bert’s actualbehavior (bert) and shed light on bert’s abili-ties to capture the functionality continuum.
notethat here the expected contextualization coincideswith the semctx.var.
for the categories investi-.
2see also semantic proximity continuum by blank (1997)..figure 5: layer-wise self-similarity scores for wordsamples/groups across the functionality continuum..home, and homonymous words, e.g., set, occupythe space in the upper layers (e.g., layer 10, see fig-ure 4), and can also be found across the preced-ing layers.
prepositions (e.g., of, for) occur in themin range from the middle layers onwards.
more-over, the determiner the occurs in the min range atlayer 11 and generally shows a low self-similarity(see figure 3).
in the mid range, we ﬁnd temporaladverbials, e.g., today and now, modal verbs (must,should) as well as polysemous and monosemouswords; see figure 4. to shed light on these contex-tualization patterns, we explore the functionalitycontinuum in more detail by looking at differentgroups of words across the layers..word-based selection – we discern the follow-ing groups of words for our further explorations:1) articles, 2) prepositions, 3) quantiﬁers, 4) modalverbs, 5) temporal adverbials, 6) monosemouswords, 7) polysemous words and 8) homonymouswords.
each group demonstrates a different patternof self-similarity across layers, as shown in fig-ure 5. first, we observe that, before (almost) end-ing up in the min range, the determiners the and astart off in the mid range of the distribution with adecreasing self-similarity across the layers.
prepo-sitions such as of, in, on, for, at are found in themid-min area until layer 6 but from then on, theyare grouped under min.
quantiﬁers like some, all,every remain in the mid range across all layers.
modal verbs such as must, should, may follow aninconsistent pattern: while must and should startoff in the upper ends of the mid area (max-mid)and end up in the mid range from layer 9 on, mayis at ﬁrst in the min area and after layer 5 in themid range.
temporal adverbials such as yesterday,never, now are also inconsistent.
some of them(e.g., yesterday) belong to the max group in thelower layers, but slowly move towards the midarea as the layers increase – without ever enter-ing the exact mid area.
others (e.g., now, never)are constantly within the mid range, starting at the.
470functionality continuum sense var..semctx.
var.
exp.
contextual..homonymouspolysemousmonosemoustemp.
adverbialsmodalsquantiﬁersprepositionsarticles.
highlow/highlowlowhighhighhighnone.
synctx.
var.
highhighhighlowlowlowlowlow.
highlow/highlowhighhighhighhighhigh.
highlow/highlowhighhighhighhighhigh.
bert.
high.
(cid:51)low/high (cid:51)(cid:51)(cid:55)(cid:55)(cid:55)(cid:51)(cid:51).
lowlowlowlowhighhigh.
table 1: expected contextualization (exp.
contextual.)
and contextualization in bert (bert) on the basis ofsense variation (sense var.
), syntactic context variation (synctx.
var.)
and semantic context variation (semctx.
var.
), ordered based on the functionality continuum, from content (blue, top) to function words (yellow, bottom)..gated, but might deviate for others.
additionally,differences between the expected contextualizationand the semctx.var.
might currently be absorbedby our binary encoding (low/high).
we envisiona more ﬁne-grained exp.
contextual.
measure,accounting in detail for the relative positioning ofwords in the middle of the continuum..homonymy – homonymous words, being onthe ‘more content-like’ end of the continuum, havea high sense variation due to their multiple (unre-lated) senses, a high syntactic variation (ﬂexibleattachment as content words) and a high semanticcontext variation as, due to their multiple senses,they can occur in semantically very different con-texts.
this means that we expect a high contex-tualization, i.e., the embeddings of homonymouswords are highly context-speciﬁc.
this is indeedconﬁrmed with our ﬁndings since these words gen-erally occur in the min area..polysemy – polysemous words, mostly with‘content-like’ properties, exhibit a low/high sensevariation, depending on whether they are highlypolysemous, i.e., have loosely related senses, ornot, i.e., have semantically related senses.
as it istypical of content words, polysemous words showhigh syntactic variation.
concerning their semanticcontext variation, they are again in a ‘grey’ areadepending on the degree of polysemy: highly pol-ysemous words mostly appear in semantically dif-ferent contexts, while plain polysemy is mostlyfound in semantically similar contexts since thesenses are closely related.
with this, the expectedcontextualization is respective to the degree of thepolysemy.
indeed, bert meets these expectations:highly polysemous words like ﬁeld, home are inthe min area across layers (high contextualization),while plain polysemous words are rather found inthe max area (low contextualization)..monosemy – monosemous words also seem tobe correctly captured by bert.
such words havelow sense variation, high syntactic variation (as.
content words) and low semantic context variation(due to their low sense variation).
according tothis, they are also expected to have low contex-tualization.
we ﬁnd this low contextualization inbert as well, where monosemous words have maxself-similarity across layers..temporal adverbials – at the middle of thefunctionality continuum, temporal adverbials havea low sense variation, e.g., yesterday has only onemeaning,3 as well as low syntactic variation.
onthe other hand, their semantic context variation ishigh because they can occur in semantically verydifferent contexts.
thus, the expected contextual-ization is high, i.e., their embeddings should becontext-speciﬁc to match the semantically differentcontexts they can appear in.
bert fails to learnthis: temporal adverbials are either found withinthe mid area across all layers or end up in this rangein the upper layers, contrary to the expected min.
modals & quantiﬁers – bert also struggles incapturing the functionality continuum with modalsand quantiﬁers.
these are comparable to wordswith high ‘sense’ variation: modals can not onlyhave a deontic or an epistemic ﬂavor, but also ex-press variation through their variable quantiﬁca-tional force; similarly, quantiﬁers exhibit variationvia their variable scope interpretation (wide or nar-row).
both modals and quantiﬁers have low syn-tactic variation; they can only attach with speciﬁcsyntactic phrases.
the contexts they appear in canbe semantically very different and thus they have ahigh semantic context variation.
based on this half-functional-half-content nature, modals and quanti-ﬁers are expected to have high contextualization,i.e., have context-speciﬁc embeddings based on themodal ﬂavor they express, the quantiﬁcational forcethey capture, the scope resolution, etc.
however,we can see that bert fails to meet this expectation..3it should be noted that such adverbials have one meaning,even if their extension is always a different one due to differentreference points..471modals and quantiﬁers mostly occur in the midrange – instead of the expected min..prepositions – at the functional end of the con-tinuum, we ﬁnd prepositions and articles.
preposi-tions are comparable to words with a high ‘sense’variation, capturing the fact that the same prepo-sition can, for example, be locative or temporal,depending on the context.
prepositions have lowsyntactic variation, as most functional words.
still,their semantic context variation matches their mul-tiple ‘senses.’ therefore, we expect the prepositionembeddings to be highly context-speciﬁc: this isindeed the case in bert, where prepositions aremostly found in the min area..articles – last, we investigate articles and par-ticularly the determiners the and a. we take themto have no sense,4 low syntactic variation and highsemantic context variation – the contexts they ap-pear in do not have any semantic similarity in mostcases.
thus, we expect them to demonstrate highcontextualization with highly context-speciﬁc em-beddings.
bert is able to model this through lowself-similarity, which is more prominent for thethan for a, nonetheless consistent for both..discussion – summing up, we see that bertstruggles to efﬁciently capture the functionalitycontinuum.
while bert manages to model theends of the continuum, i.e., the mostly contentand mostly functional words, it fails to create ex-pressive embeddings for categories with contentas well as functional properties.
this ﬁnding isin line with previous literature that has shown thatcurrent lms cannot efﬁciently capture hard linguis-tic phenomena (e.g., dasgupta et al.
(2018); mc-coy et al.
(2019); richardson et al.
(2020)), withmodals, quantiﬁers and temporal reasoning belong-ing to these phenomena.
our work suggests that thebert embeddings are not speciﬁc enough to cap-ture the inherent functionality of certain word types,i.e., bert does not learn the relevant generaliza-tions.
additionally, we show that contextualiza-tion is neither entirely driven by polysemy nor con-text variation.
rather, contextualization can be ex-plained via the harmonical combination of function-ality, sense variation, syntactic variation and seman-tic context variation: bert can efﬁciently modelpolysemy, homonymy and mononymy, i.e., it canefﬁciently capture words that appear in semanticcontexts of high variation and low variation and.
4we treat determiners as deﬁniteness markers, rather thanas quantiﬁers or discourse markers, to be in-line with theirtreatment in popular nlp tasks such as nli..independently of their polysemy.
what it cannotmodel are words that have a semi-functional/semi-content nature (models, quantiﬁers, temporal ad-verbials), see table 1. concerning models andquantiﬁers, bert cannot learn the inherent func-tionality from the context alone and thus treats thewords as simple monosemous words.
concern-ing temporal adverbials, bert cannot deal withthe combination of low sense variation and highsemantic context variation – a rather unusual com-bination – and is unable to conclude a single wordmeaning.
although prepositions have the same trig-gers as modals and quantiﬁers, bert follows ourexpectations with respect to contextualization.
thiscould be due to their higher syntactic ﬂexibility ortheir close semantic relatedness with their contentcomplements, but this needs to be explored as partof future work.
overall, bert seems to followﬁndings of psycholinguistics and language acqui-sition: children learn content words easier and ear-lier than function words (bates et al., 1994; caselliet al., 1995).
drawing from language acquisition re-search, we see an opportunity for explainable meth-ods to inspect bert’s inner-workings and improveits linguistic understanding, raising lms from theirinfantile state to a more linguistically-mature one..6 conclusion and future work.
this paper presented new insights on the contex-tualization of the functionality continuum, show-ing that bert fails to capture the nature of semi-functional-semi-content words.
these insightswere generated through a novel visual analyticstechnique for contextualized word embedding ex-ploration and analysis.
for a deeper understandingof the weaknesses of bert, our technique can beextended with scores that model common linguis-tic properties of words and their nearest neighbors,e.g., wordnet semantic similarity or pos similarityscores.
hence, they could serve as means of expla-nation and bring added value to the explainableartiﬁcial intelligence (xai) research ﬁeld.
moreinformation about the project can be found under:https://embeddings-explained.lingvis.io..acknowledgments.
we thank the deutsche forschungsgemeinschaft(dfg, german research foundation) for fundingwithin project bu 1806/10-2 “questions visual-ized” of the for2111 and project d02 “evalua-tion metrics for visual analytics in linguistics”(project id: 251654672 – trr 161)..472broader impact statement.
in the following, we describe the two main pointswith respect to the broader impact statement..impact.
with regard to the broader impact of our work, weare going beyond just measuring scores by reveal-ing and explaining the inner-workings of languagemodels.
we put the measured scores in contextthrough visual analytics, in combination with prob-ing and adversarial testing methods, for the explo-ration, explanation, and analysis.
with our work,we aim to open new perspectives on measuring andobtaining the model performance, which go beyondtypically used performance metrics..reproducibility.
with regard to reproducibility concerns, we wouldlike to note that the contextualization scores calcu-lated in this paper rely on the word frequencies and,thus, may differ depending on the analyzed corpus.
future work should investigate the exact effect ofword frequency and account for its impact..references.
betty van aken, benjamin winter, alexander l¨oser,and felix a. gers.
2019. how does bert answerquestions?
a layer-wise analysis of transformerrepresentations.
in proceedings of the 28th acm in-ternational conference on information and knowl-edge management, cikm ’19, page 1823–1832,new york, ny, usa.
association for computingmachinery..betty van aken, benjamin winter, alexander l¨oser,and felix a. gers.
2020. visbert: hidden-statevisualizations for transformers.
in companion pro-ceedings of the web conference 2020, www ’20,page 207–211, new york, ny, usa.
association forcomputing machinery..roy bar-haim, ido dagan, bill dolan, lisa ferro,danilo giampiccolo, bernardo magnini, and idanszpektor.
2006. the second pascal recognis-in proceedingsing textual entailment challenge.
of the second pascal recognising textual entail-ment challenge workshop, pages 1–9, venice, italy..elizabeth bates, virginia marchman, donna thal,larry fenson, philip dale, j. steven reznick, judyreilly, and jeff hartung.
1994. developmental andstylistic variation in the composition of early vocab-ulary.
journal of child language, 21(1):85–123..yonatan belinkov.
2018. on internal language repre-sentations in deep learning: an analysis of machine.
translation and speech recognition.
ph.d. thesis,massachusetts insitute of technology..m. berger.
2020. visually analyzing contextualizedin 2020 ieee visualization confer-embeddings.
ence (vis), pages 276–280, los alamitos, ca, usa.
ieee computer society..andreas blank.
1997. prinzipien des lexikalischenbedeutungswandels am beispiel der romanischensprachen.
niemeyer, t¨ubingen..angie boggust, brandon carter, and arvind satya-narayan.
2019. embedding comparator: visualiz-ing differences in global structure and local neigh-borhoods via small multiples.
arxiv e-prints, pagearxiv:1912.04853..maria cristina caselli, elizabeth bates, paola casadio,judi fenson, larry fenson, lisa sanderl, and judyweir.
1995. a cross-linguistic study of early lexicaldevelopment.
cognitive development, 10(2):159 –199..ethan a. chi, john hewitt, and christopher d. man-ning.
2020. finding universal grammatical rela-tions in multilingual bert.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5564–5577, online.
as-sociation for computational linguistics..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representationsfrom natural language inference data.
in proceed-ings of the 2017 conference on empirical methodsin natural language processing, pages 670–680,copenhagen, denmark.
association for computa-tional linguistics..alexis conneau, german kruszewski, guillaume lam-ple, lo¨ıc barrault, and marco baroni.
2018. whatyou can cram into a single $&!#* vector: probingsentence embeddings for linguistic properties.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2126–2136, melbourne, aus-tralia.
association for computational linguistics..norbert corver and henk van riemsdijk.
2001. semi-lexical categories: the function of content wordsand the content of function words.
de gruytermouton, berlin, new york..ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in proceedings of the machine learningchallenges workshop, pages 177–190, southamp-ton, uk.
springer..ishita dasgupta, demi guo, andreas stuhlm¨uller,samuel j. gershman, and noah d. goodman.
2018.evaluating compositionality in sentence embed-dings.
corr, abs/1802.04302..473joseph f derose, jiayao wang, and m. berger.
2021.attention flows: analyzing and comparing atten-tion mechanisms in language models.
ieee trans-actions on visualization and computer graphics,27:1160–1170..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..daniel edmiston.
2020. a systematic analysis of mor-phological content in bert models for multiplelanguages.
arxiv preprint arxiv:2004.03032..mennatallah el-assady, wolfgang jentner, fabiansperrle, rita sevastjanova, annette hautli, miriambutt, and daniel keim.
2019a.
lingvis.io – a lin-guistic visual analytics framework.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics: system demonstrations,pages 13–18..mennatallah el-assady, rebecca kehlbeck, christo-pher collins, daniel keim, and oliver deussen.
2019b.
semantic concept spaces: guided topicmodel reﬁnement using word-embedding projec-tions.
ieee transactions on visualization and com-puter graphics, 26(1):1001–1011..joseph e. emonds.
1985. a uniﬁed theory of syntacticcategories.
de gruyter mouton, berlin, boston..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the ge-ometry of bert, elmo, and gpt-2 embeddings.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 55–65,hong kong, china.
association for computationallinguistics..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnosticsfor language models.
transactions of the associa-tion for computational linguistics, 8:34–48..christiane fellbaum.
1998. wordnet: an electroniclexical database (language, speech, and commu-nication).
the mit press, cambridge, ma, usa..max glockner, vered shwartz, and yoav goldberg.
2018. breaking nli systems with sentences thatin proceed-require simple lexical inferences.
ings of the 56th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 650–655.
association for computa-tional linguistics..yoav goldberg.
2019. assessing bert’s syntactic.
abilities.
arxiv preprint arxiv:1901.05287..florian heimerl and michael gleicher.
2018. interac-tive analysis of word vector embeddings.
in com-puter graphics forum, volume 37, pages 253–265.
wiley online library..john hewitt and christopher d. manning.
2019. astructural probe for finding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..f. hohman, m. kahng, r. pienta, and d. h. chau.
2019.visual analytics in deep learning: an interrogativesurvey for the next frontiers.
ieee transactions onvisualization and computer graphics, 25(8):2674–2693..benjamin hoover, hendrik strobelt, and sebastiangehrmann.
2020. exbert: a visual analysis toolto explore learned representations in transformersin proceedings of the 58th annual meet-models.
ing of the association for computational linguis-tics, system demonstrations.
association for com-putational linguistics..ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structurein proceedings of the 57th annualof language?
meeting of the association for computational lin-guistics, pages 3651–3657, florence, italy.
associa-tion for computational linguistics..yongjie lin, yi chern tan, and robert frank.
2019.open sesame: getting inside bert’s linguisticknowledge.
in proceedings of the 2019 acl work-shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 241–253, florence,italy.
association for computational linguistics..shusen liu, peer-timo bremer, jayaraman j thiagara-jan, vivek srikumar, bei wang, yarden livnat, andvalerio pascucci.
2017. visual exploration of se-mantic relationships in neural word embeddings.
ieee transactions on visualization and computergraphics, 24(1):553–562..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recog-in proceed-nizing textual entailment challenge.
ings of the workshop on textual entailment andparaphrasing, prague, czech republic.
associationfor computational linguistic..shusen liu, zhimin li, tao li, vivek srikumar, vale-rio pascucci, and peer-timo bremer.
2018. nlize:a perturbation-driven visual interrogation tool foranalyzing and interpreting natural language infer-ieee transactions on visualizationence models.
and computer graphics, 25(1):651–660..474laurens van der maaten and geoffrey hinton.
2008.visualizing data using t-sne.
journal of machinelearning research, 9(11):2579–2605..rebecca marvin and tal linzen.
2018. targeted syn-tactic evaluation of language models.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 1192–1202,brussels, belgium.
association for computationallinguistics..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntac-intic heuristics in natural language inference.
proceedings ofthethe 57th annual meeting ofassociation for computational linguistics, pages3428–3448, florence, italy.
association for compu-tational linguistics..leland mcinnes, john healy, nathaniel saul, andlukas grossberger.
2018. umap: uniform mani-fold approximation and projection.
the journal ofopen source software, 3(29):861..tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space.
proceedings of workshop aticlr..aakanksha naik, abhilasha ravichander, normansadeh, carolyn rose, and graham neubig.
2018.stress test evaluation for natural language infer-ence.
in proceedings of the 27th international con-ference on computational linguistics, pages 2340–2353, santa fe, new mexico, usa.
association forcomputational linguistics..cheonbok park, inyoup na, yongjang jo, sungbokshin, jaehyo yoo, bum chul kwon, jian zhao,hyungjong noh, yeonsoo lee, and jaegul choo.
2019. sanvis: visual analytics for understandingin 2019 ieee visualiza-self-attention networks.
tion conference (vis), pages 146–150.
ieee..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordin proceedings of the 2014 con-representation.
ference on empirical methods in natural languageprocessing (emnlp), pages 1532–1543..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..emily reif, ann yuan, martin wattenberg, fernanda bviegas, andy coenen, adam pearce, and been kim.
2019. visualizing and measuring the geometry ofbert.
in h. wallach, h. larochelle, a. beygelz-imer, f. d’alch´e buc, e. fox, and r. garnett, editors,.
advances in neural information processing systems32, pages 8594–8603.
curran associates, inc..kyle richardson, hai hu, lawrence s. moss, andashish sabharwal.
2020. probing natural languageinference models through semantic fragments.
inassociation for the advancement of artiﬁcial intelli-gence (aaai), pages 8713–8721.
aaai press..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we knowabout how bert works.
transactions of the asso-ciation for computational linguistics, 8:842–866..john r ross.
1972. the category squish: endstationhauptwort.
in chicago linguistic society, volume 8,pages 316–328..daniel smilkov, nikhil thorat, charles nicholson,emily reif, fernanda b. vi´egas, and martin watten-berg.
2016. embedding projector: interactive visu-alization and interpretation of embeddings.
arxive-prints, page arxiv:1611.05469..hendrik strobelt, sebastian gehrmann, michaelbehrisch, adam perer, hanspeter pﬁster, andalexander m rush.
2018. seq2seq-vis: a visualdebugging tool for sequence-to-sequence models.
ieee transactions on visualization and computergraphics, 25(1):353–363..ian tenney, dipanjan das, and ellie pavlick.
2019a.
bert rediscovers the classical nlp pipeline.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r thomas mccoy, najoung kim,benjamin van durme, sam bowman, dipanjan das,and ellie pavlick.
2019b.
what do you learn fromcontext?
probing for sentence structure in contextu-in international con-alized word representations.
ference on learning representations..kristina toutanova, dan klein, christopher d. man-ning, and yoram singer.
2003. feature-rich part-of-speech tagging with a cyclic dependency net-work.
in proceedings of the 2003 human languagetechnology conference of the north american chap-ter of the association for computational linguistics,pages 252–259..jason utt and sebastian pad´o.
2011. ontology-baseddistinction between polysemy and homonymy.
inproceedings of the ninth international conferenceon computational semantics (iwcs 2011)..jesse vig.
2019. a multiscale visualization of at-in proceedingstention in the transformer model.
of the 57th annual meeting of the association forcomputational linguistics: system demonstrations,pages 37–42, florence, italy.
association for com-putational linguistics..475alex warstadt and samuel r. bowman.
2020. canneural networks acquire a structural bias from rawlinguistic data?
in proceedings of the 42nd annualvirtual meeting of the cognitive science society, on-line..gregor wiedemann, steffen remus, avi chawla, andchris biemann.
2019. does bert make anysense?
interpretable word sense disambiguationin proceedingswith contextualized embeddings.
of konvens 2019, erlangen, germany..mengjie.
zhao,.
dufter,.
philipp.
yadollahyaghoobzadeh, and hinrich sch¨utze.
2020. quanti-fying the contextualization of word representationsin findings of thewith semantic class probing.
association for computational linguistics: emnlp2020, pages 1219–1234, online.
association forcomputational linguistics..xunjie zhu, tingfeng li, and gerard de melo.
2018.exploring semantic properties of sentence embed-dings.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 2: short papers), pages 632–637, melbourne,australia.
association for computational linguis-tics..476