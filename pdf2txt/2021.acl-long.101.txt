improving zero-shot translation by disentanglingpositional information.
danni liu1, jan niehues1, james cross2, francisco guzm´an2, xian li21department of data science and knowledge engineering, maastricht university2facebook ai{danni.liu,jan.niehues}@maastrichtuniversity.nl{jcross,fguzman,xianl}@fb.com.
abstract.
multilingual neural machine translation hasshown the capability of directly translating be-tween language pairs unseen in training, i.e.
zero-shot translation.
despite being conceptu-ally attractive, it often suffers from low outputquality.
the difﬁculty of generalizing to newtranslation directions suggests the model repre-sentations are highly speciﬁc to those languagepairs seen in training.
we demonstrate that amain factor causing the language-speciﬁc rep-resentations is the positional correspondenceto input tokens.
we show that this can beeasily alleviated by removing residual connec-tions in an encoder layer.
with this mod-iﬁcation, we gain up to 18.5 bleu pointson zero-shot translation while retaining qual-ity on supervised directions.
the improve-ments are particularly prominent between re-lated languages, where our proposed modeloutperforms pivot-based translation.
more-over, our approach allows easy integration ofnew languages, which substantially expandstranslation coverage.
by thorough inspectionsof the hidden layer outputs, we show thatour approach indeed leads to more language-independent representations.1.
1.introduction.
multilingual neural machine translation (nmt) sys-tem encapsulates several translation directions ina single model (firat et al., 2017; johnson et al.,2017).
these multilingual models have been shownto be capable of directly translating between lan-guage pairs unseen in training (johnson et al., 2017;ha et al., 2016).
zero-shot translation as such isattractive both practically and theoretically.
com-pared to pivoting via an intermediate language, thedirect translation halves inference-time computa-.
1code and scripts available in: https://github..com/nlp-dke/nmtgminor/tree/master/recipes/zero-shot.
figure 1: an example of language-speciﬁc encoderoutputs as a results of the strong positional correspon-dence to input tokens (even assuming the word embed-dings are cross-lingually mapped)..tion and circumvents error propagation.
consider-ing data collection, zero-shot translation does notrequire parallel data for a potentially quadratic num-ber of language pairs, which is sometimes imprac-tical to acquire especially between low-resourcelanguages.
using less supervised data in turn re-duces training time.
from a modeling perspective,zero-shot translation calls for language-agnosticrepresentations, which are likely more robust andcan beneﬁt low-resource translation directions..despite the potential beneﬁts, achieving high-quality zero-shot translation is a challenging task.
prior works (arivazhagan et al., 2019; zhang et al.,2020a; rios et al., 2020) have shown that standardsystems tend to generate poor outputs, sometimesin an incorrect target language.
it has been furthershown that the encoder-decoder model capturesspurious correlations between language pairs withsupervised data (gu et al., 2019).
during training,the model only learns to encode the inputs in a formthat facilitates translating the supervised directions.
the decoder, when prompted for zero-shot trans-lation to a different target language, has to handleinputs distributed differently from what was seenin training, which inevitably degrades performance.
ideally, the decoder could translate into any tar-get language it was trained on given an encodedrepresentation independent of input languages.
inpractice, however, achieving a language-agnosticencoder is not straightforward..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1259–1273august1–6,2021.©2021associationforcomputationallinguistics1259encodera  big  catencoderun  gato  grandeh1h2h3h1h3h2in a typical transformer encoder (vaswani et al.,2017), the output has a strong positional corre-spondence to input tokens.
for example in theenglish sentence in figure 1, encoder outputsh1,2,3 correspond to “a”, “big”, “cat” respectively.
while this property is essential for tasks such as se-quence tagging, it hinders the creation of language-independent representations.
even assuming thatthe input embeddings were fully mapped on a lex-ical level (e.g.
“cat” and “gato” have the sameembedding vector), the resulting encoder outputsare still language-speciﬁc due to the word orderdifferences.
in this light, we propose to relax thisstructural constraint and offer the model some free-dom of word reordering in the encoder already.
ourcontributions are as follow:.
• we show that the positional correspondenceto input tokens hinders zero-shot translation.
we achieve considerable gains on zero-shottranslation quality by only removing residualconnections once in a middle encoder layer..• our proposed model allows easy integrationof new languages, which enables zero-shottranslation between the new language and allother languages previously trained on..• based on a detailed analysis of the model’s in-termediate outputs, we show that our approachcreates more language-independent represen-tations both on the token and sentence level..2 disentangling positional information.
zero-shot inference relies on a model’s general-izability to conditions unseen in training.
in thecontext of zero-shot translation, the input shouldideally be encoded into an language-agnostic repre-sentation, based on which the decoder can translateinto any target language required, similar to thenotion of an interlingua.
nevertheless, the ideal of“any input language, same representation” cannotbe easily fulﬁlled with a standard encoder, as wehave shown in the motivating example in figure 1.we observe that the encoder output has a posi-tional correspondence to input tokens.
formally,given input token embeddings (x1, .
.
.
, xn), in theencoder output (h1, .
.
.
, hn), the i-th hidden statehi mostly contains information about xi.
whilethis structure is prevalent and is indeed necessaryin many tasks such as contextual embedding and se-quence tagging, it is less suitable when consideringlanguage-agnostic representations.
as a sentence.
illustrations of our proposed modiﬁcationsfigure 2:to an original encoder layer: dropping residual connec-tions once (§2.1); making attention query based on po-sition encoding (§2.2).
before each self-attention (sa)and feed forward (ff) layer we apply layer normaliza-tion, which is not visualized here for brevity..in different languages are likely of varying lengthsand word orders, the same semantic meaning willget encoded into different hidden state sequences.
there are two potential causes of this positionalcorrespondence: residual connections and encoderself-attention alignment.
we further hypothesizethat, by modifying these two components accord-ingly, we can alleviate the positional correspon-dence.
speciﬁcally, we set one encoder layer freefrom these constraints, so that it could create itsown output ordering instead of always following aone-to-one mapping with its input..2.1 modifying residual connections.
in the original transformer architecture fromvaswani et al.
(2017), residual connections (heet al., 2016) are applied in every layer, for boththe multihead attention and the feed-forward layer.
by adding the input embeddings to the layer out-puts, the residual connections are devised to facili-tate gradient ﬂow to bottom layers of the network.
however, since the residual connections are presentthroughout all layers, they strictly impose a one-to-one alignment between the inputs and outputs.
forthe encoder, this causes the outputs to be position-ally corresponding to the input tokens..we propose to relax this condition, such that theencoder outputs becomes less position- and hencelanguage-speciﬁc.
meanwhile, to minimize the im-pact on the model architecture and ensure gradientﬂow, we limit this change to only one encoder layer,and only its multihead attention layer.
figure 2(b).
1260saqkvffresidual...saqkvffresidual...inputinamiddleencoderlayer:(a)original(b)dropresidual(c)position-basedqueryinputsaqkvff...inputgives a visualization of this change in comparisonto the original encoder in figure 2(a)..by training on 2n directions, we still explore thisscenario..2.2 position-based self-attention query.
3.1 datasets.
besides the residual connections, another poten-tial reason for the positional correspondence isthe encoder self-attention alignment.
via the self-attention transform, each position is a weightedsum from all input positions.
while the weightstheoretically can distribute over all input positions,they are often concentrated locally, particularlywith output position i focusing on input position i.previous works on various sequence tasks (yanget al., 2020; zhang et al., 2020b) have shownheavy weights on the diagonal of the encoder self-attention matrices..in this light, the motivation of our method startswith the formation of the self-attention weight ma-trix: score(q, k) = qkt , where q and k andthe query and key matrices.
this n × n matrix en-capsulates dot product at each position against alln positions.
since the dot product is used as a sim-ilarity measure, we hypothesize that when q andk are similar, the matrix will have heavy weightson the diagonal, thereby causing the positional cor-respondence.
indeed, q and k are likely similarsince they are projections from the same input.
wetherefore propose to reduce this similarity by re-placing the projection base of the self-attentionquery by a set of sinusoidal positional encodings.
moreover, to avoid possible interaction with posi-tional information retained in k, we use a wavelength for this set of sinusoidal encodings that isdifferent from what is added onto encoder input em-beddings.
figure 2(c) contrasts our position-basedattention query with the original model in figure2(a), where the key, query, value are all projectedfrom the input to the self-attention layer..3 experimental setup.
our experiments cover high- and low-resource lan-guages and different data conditions.
we choosean english-centered setup, where we train on x ↔en parallel data, and test the zero-shot translationbetween all non-english languages.
this scenariois particularly difﬁcult for zero-shot translation, ashalf of the target-side training data is in english.
indeed, recent works (fan et al., 2020; rios et al.,2020) have outlined downsides of the english-centered conﬁguration.
nevertheless, intrigued bythe potential of covering n 2 translation directions.
our datasets originate from three sources: iwslt2017 (cettolo et al., 2017), europarl v7 (koehn,2005), and pmindia (haddow and kirefu, 2020).
the iwslt and europarl data are taken from themmcr4nlp corpus (dabre and kurohashi, 2017).
an overview of the datasets is in table 1..to investigate the role of training data diversity,we construct two conditions for europarl, whereone is fully multiway aligned, and the other has nomultiway alignment at all.
both are subsets of thefull dataset with 1m parallel sentences per direc-tion.
moreover, we study the challenging case ofpmindia with little training data, distinct writingsystems, and a large number of agglutinate lan-guages that are specially difﬁcult to translate into.
table 2 outlines the languages in our experiments..dataset.
iwslt.
pmindia.
europarl:w/o overlapmultiwayfull.
x ↔ en.
{it, nl, ro}.
{bn, gu, hi,ml, mr, kn,or, te, pa}.
{da, de,es, ﬁ, fr,it, nl, pt}.
# zero-shotdirections.
# sent.
perdirection.
6.
72.
56.
145k.
26-53k.
119k119k1m.
table 1: overview of the datasets..code.
name.
family.
script.
itnlrodadeesﬁfrpt.
bnguhiknmlmrorpate.
italiandutchromanianitaliangermanspanishfinnishfrenchportugese.
bengaligujaratihindikannadamalayalammarathiodiapunjabitelugu.
romancegermanicromanceromancegermanicromanceuralicromanceromance.
indo-aryanindo-aryanindo-aryandravidiandravidianindo-aryanindo-aryanindo-aryandravidian.
latin.
bengaligujaratidevanagarikannadamalayalamdevanagariodiagurmukhitelugu.
table 2: overview of the languages in our experiments..12613.2 model details and baselines.
training details by default we use transformer(vaswani et al., 2017) with 5 encoder and decoderlayers.
for the europarl datasets with more train-ing data, we enlarge the model to 8 encoder anddecoder layers.
to control the output language,we use a target-language-speciﬁc begin-token aswell as language embeddings concatenated withdecoder word emebeddings2, similar to pham et al.
(2019).
we use 8 attention heads, embedding sizeof 512, inner size of 2048, dropout rate of 0.2, la-bel smoothing rate of 0.1. we use the learning rateschedule from vaswani et al.
(2017) with 8,000warmup steps.
the source and target word embed-dings are shared.
furthermore, in the decoder, theparameters of the projection from hidden states tothe vocabulary are tied with the transposition of theword lookup table..moreover, we include variational dropout (galand ghahramani, 2016) as a comparison since itwas used in a previous work on zero-shot trans-lation (pham et al., 2019) instead of the standardelement-wise dropout.
with variational dropout,all timesteps in a layer output share the same mask.
this differs from the standard dropout, where eachelement in each timestep is dropped according tothe same dropout rate.
we hypothesize that thistechnique helps reduce the positional correspon-dence with input tokens by preventing the modelfrom relying on speciﬁc word orders..we train for 64 epochs and average the weightsof the 5 best checkpoints ordered by dev loss.
bydefault, we only include the supervised translationdirections in the dev set.
the only exception isthe europarl-full case, where we also include thezero-shot directions in dev set for early stopping.
when analyzing model hidden representationsthrough classiﬁcation performance (subsection 5.1and 5.2), we freeze the trained encoder-decoderweights and train the classiﬁer for 5 epochs.
theclassiﬁer is a linear projection from the encoder hid-den dimension to the number of classes, followedby softmax activation.
as the classiﬁcation task islightweight and convergence is fast, we reduce thewarmup steps to 400 while keeping the learningrate schedule unchanged..proposed models as motivated in section 2,we modify the residual connections and the self-.
2the concatenation of language embedding and decoderword embedding is then projected down to the embeddingdimension to form the input embedding to the decoder..attention layer in a middle encoder layer.
specif-ically, we choose the 3-rd and 5-th layer of the 5-and 8-layer models respectively.
we use “resid-ual” to indicate residual removal and “query” theposition-based attention query.
for the projectionbasis of the attention query, we use positional en-coding with wave length 100..zero-shot vs. pivoting we compare the zero-shot translation performance with pivoting, i.e.
di-rectly translating the unseen direction x → y vs.using english as an intermediate step, as in x →english → y. the pivoting is done by the base-line multilingual model, which we expect to havesimilar performance to separately trained bilingualmodels.
for a fair comparison, in the europarl-full case, pivoting is done by a baseline modeltrained till convergence with only supervised devdata rather than the early-stopped one..3.3 preprocessing and evaluation.
for the languages with latin script, we ﬁrst ap-ply the moses tokenizer and truecaser, and thenlearn byte pair encoding (bpe) using subword-nmt(sennrich et al., 2016).
for the indian languages,we use the indicnlp library3 and sentencepiece(kudo and richardson, 2018) for tokenization andbpe respectively.
we choose 40k merge opera-tions and only use tokens with minimum frequencyof 50 in the training set.
for iwslt, we use the of-ﬁcial tst2017 set.
for pmindia, as the corpus doesnot come with dev and test sets, we partition thedataset ourselves by taking a multiway subset of alllanguages, resulting in 1,695 sentences in the devand test set each.
for europarl, we use the test setsin the mmcr4nlp corpus (dabre and kurohashi,2017).
the outputs are evaluated by sacrebleu4(post, 2018)..3.4 adaptation procedure.
to simulate the case of later adding a new language,we learn a new bpe model for the new languageand keep the previous model unchanged.
due to theincreased number of unique tokens, the vocabulary.
3https://github.com/anoopkunchukuttan/.
indic_nlp_library.
4we use bleu+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.12 by default.
onpmindia, we use the spm tokenizer (tok.spm instead oftok.13a) for better tokenization of the indic languages.
atthe time of publication, the argument tok.spm is only avail-able as a pull request to sacrebleu: https://github.
com/mjpost/sacrebleu/pull/118.
we applied thepull request locally to use the spm tokenizer..1262dataset.
supervised directions.
zero-shot directions.
baseline.
residual.
+query.
pivot.
baseline.
residual.
+query.
(1)(2)(3)(4)(5).
iwslteuroparl multiwayeuroparl w/o overlapeuroparl fullpmindia.
29.834.235.635.430.4.
29.433.935.436.429.9.
29.433.134.935.929.2.
19.125.927.128.422.1.
10.811.38.217.50.8.
17.7 (+6.9)26.1 (+14.8)26.7 (+18.5)27.5 (+10.0)2.3 (+1.5).
17.825.125.826.51.1.table 3: bleu5scores on supervised and zero-shot directions.
on iwslt and europarl (row (1)-(4)), removingresidual connections once substantially improves zero-shot translation while retaining performance on superviseddirections.
on pmindia (row 5), our approach can be improved further by additional regularization (table 5)..dataset.
family.
baseline pivot.
residual.
europarlmultiway.
germanicromance.
europarlw/o overlap.
germanicromance.
6.910.2.
11.813.5.
25.932.8.
24.831.0.
26.2 (+0.3)33.1 (+0.3).
25.5 (+0.7)32.3 (+1.3).
table 4: zero-shot bleu scores between languagesof the same families on europarl multiway and non-overlap (row (2) and (3) from table 3).
our approachoutperforms pivoting via english..of the previously-trained model is expanded.
inthis case, for the model weights related to the wordlookup table size, we initialize them as the averageof existing embedding perturbed by random noise..4 results.
our approach substantially improves zero-shottranslation quality, as summarized in table 3. theﬁrst observation is that modiﬁcation in residualconnections is essential for zero-shot performance6.
we gain 6.9 and up to 18.5 bleu points over thebaseline on iwslt and europarl (row 1 to 4) re-spectively.
when inspecting the model outputs, wesee that the baseline often generates off-target trans-lation in english, in line with observations fromprior works (arivazhagan et al., 2019; zhang et al.,2020a).
our proposed models are not only consis-tent in generating the required target languages inzero-shot conditions, but also show competitive per-formance to pivoting via english.
the effects areparticularly prominent between related languages.
as shown in table 4, on europarl, zero-shot out-performs the pivoting when translating between.
5due to the large number of languages, we report thebleu scores averaged over all directions here, and refer thereaders to the appendix for detailed results..6we also experimented with: 1) removing the residual inmore layers, but observed large negative impact on conver-gence; 2) replacing the residual connections by meanpooledsentence embeddings, but the gains on zero-shot directionswere less than removing the residual connections..languages from the same families.
this is an at-tractive property especially when the computationresource is limited at inference time..in the very challenging case of pmindia (row5), while removing residual does improve the zero-shot performance, the score of 2.3 indicates that theoutputs are still far from being useful.
nonetheless,we are able to remedy this by further regularizationas we will present in subsection 4.1..contrary to the large gains by removing residualconnections, the attention query modiﬁcation is noteffective when combined with residual removal.
this suggests that the primary source of position-speciﬁc representation is the residual connections.
moreover, by contrasting row 2 and 3 of table3, we show the effect of training data diversity.
inreal-life, the parallel data from different languagepairs are often to some degree multiway.
multiwaydata could provide an implicit bridging that facili-tates zero-shot translation.
with non-overlappingdata, gains can come from training with a largervariety of sentences.
given these two opposinghypotheses, our results suggest that the diversetraining data is more important for both supervisedand zero-shot performance.
with non-overlappingdata, we ﬁrst obverse improved supervised transla-tion performance by around 1.5 points for all threemodel conﬁgurations (baseline, residual, resid-ual+query).
meanwhile, the zero-shot score alsoincreases from 26.1 to 26.7 points with our model(residual).
the baseline, on the contrary, losesfrom 11.3 to 8.2 points.
this suggests that ourmodel can better utilize the diverse training datathan the baseline under zero-shot conditions..4.1 effect of additional regularization.
in subsection 3.2, we hypothesized that variationaldropout helps reduce position-speciﬁc representa-tion.
table 5 shows the outcome of replacing thestandard dropout by this technique.
first, vari-.
1263ational dropout also improves zero-shot perfor-mance over the baseline, yet not as strongly asresidual removal.
on iwslt and europarl, there isno additive gain by combining both techniques.
onpmindia, however, combining our model and varia-tional dropout is essential for achieving reasonablezero-shot performance, as shown by the increasefrom 2.4 to 14.3 points.
why is the picture differ-ent on pmindia?
we identify two potential reasons:1) the low lexical overlap7 among the languages(8 different scripts in the 9 indian languages); 2)the extreme low-resource condition (30k sentencesper translation direction on average)..to understand this phenomenon, we create anartiﬁcial setup based on iwslt with 1) no lexi-cal overlap by appending a language tag beforeeach token; 2) extremely low resource by taking asubset of 30k sentences per translation direction.
the scores in table 6 show the increasing beneﬁtof variational dropout given very low amount oftraining data and shared lexicon.
we interpret thisthrough the lens of generalizable representations:with low data amount or lexical overlap, the modeltends to represent its input in a highly language-speciﬁc way, hence hurting zero-shot performance..zero-shot directions.
dataset.
iwslteuroparlpmindia.
baseline +vardrop residual +vardrop.
10.88.20.8.
14.925.12.3.
17.726.72.4.
17.726.414.3.zero-shot bleu scores by variationaltable 5:(“+vardrop”) on iwslt, europarl non-dropoutoverlap, and pmindia.
on the ﬁrst two datasets, com-bining residual removal and variational dropout has nosynergy.
on pmindia with little data and low lexicaloverlap, the combination of the two is essential..condition.
residual + vardrop.
(1) normal(2)(3).
(1)+little data(2)+no lexical overlap.
17.711.99.7.
17.7 (+0.0)12.9 (+1.0)12.2 (+2.5).
table 6: zero-shot bleu scores of on a subset ofiwslt artiﬁcially constructed with little training dataand no shared lexicon.
the beneﬁt of regularizing byvariational dropout becomes prominent as the amountof training data and shared lexicon decreases..7we also tried mapping the 9 indian languages into thedevanagari script, but got worse zero-shot performance com-pared to the current setup..4.2 adaptation to unseen language.
so far our model has shown promising zero-shotperformance.
here we extend the challenge ofzero-shot translation by integrating a new language.
speciﬁcally, we ﬁnetune a trained english-centeredmany-to-many system with a new language using asmall amount of xnew ↔ english parallel data.
attest time, we perform zero-shot translation betweenxnew and all non-english languages previously in-volved in training.
this practically simulates thescenario of later acquiring parallel data between alow-resource language and the central bridging lan-guage in an existing system.
after ﬁnetuning withthe new data, we can potentially increase transla-tion coverage by 2n directions, with n being thenumber of languages originally in training.
weﬁnetune a trained system on iwslt (row 1 in ta-ble 3) using a minimal amount of de ↔ en datawith 14k sentences.
when ﬁnetuning we includethe original xold ↔ en training data, as otherwisethe model would heavily overﬁt.
this procedure isrelatively lightweight, since the model has alreadyconverged on the original training data..in table 7, our model outperforms the baselineon zero-shot translation, especially when translat-ing from the new language (xnew →).
when in-specting the outputs, we see the baseline almostalways translate into the wrong language (english),causing the low score of 1.8. we hypothesize thatthe baseline overﬁts more on the supervised di-rection (xnew → en), where it achieves the higherscore of 18.5. in contrast, our model is less suscep-tible to this issue and consistently stronger underzero-shot conditions..supervised.
zero-shot.
baseline residual.
baseline residual.
xnew →→ xnew.
18.513.6.
17.313.4.
1.88.3.
6.710.7.table 7: effects of adaptation to new language (de ↔en on iwslt.
zero-shot translation directions are de↔ {it, nl, ro}.
our model has signiﬁcantly strongerzero-shot performance..5 discussions and analyses.
to see beyond bleu scores, we ﬁrst analyze howmuch position- and language-speciﬁc informationis retained in the encoder hidden representationsbefore and after applying our approaches.
we thenstudy circumstances where zero-shot translation.
1264we also try to recover the position id’s based onthe outputs from each layer.
as shown in figure3, the accuracy drops sharply at the third layer,where the residual connection is removed.
thisshows that the devised transition point at a middleencoder layer is effective..5.2.inspecting language independence.
to test whether our model leads to more language-independent representations, we assess the similar-ity of encoder outputs on the sentence and tokenlevel using the two following methods:.
svcca the singular vector canonical correla-tion analysis (svcca; raghu et al., 2017) mea-sures similarity of neural network outputs, and hasbeen used to assess representational similarity innmt (kudugunta et al., 2019).
as svcca oper-ates on ﬁxed-size inputs, we meanpool the encoderoutputs and measure similarity on a sentence level..language classiﬁcation accuracy since moresimilar representations are more difﬁcult to distin-guish, poor performance of a language classiﬁerindicates high similarity.
based on a trained model,we learn a token-level linear projection from the en-coder outputs to the number of classes (languages)..findings as shown in table 9, our model con-sistently achieves higher svcca scores and lowerclassiﬁcation accuracy than the baseline, indicatingmore language-independent representations.
whenzooming into the difﬁculty of classifying the lan-guages, we further notice much higher confusion(therefore similarity) between related languages.
for instance, figure 4 shows the confusion ma-trix when classifying the 8 source languages ineuroparl.
after residual removal, the similarity ismuch higher within the germanic and romancefamily.
this also corresponds to cases where ourmodel outperforms pivoting (table 4)..tends to outperform its pivoting-based counterpart.
lastly, we discuss the robustness of our approachto the impact of different implementation choices..5.1.inspecting positional correspondence.
to validate whether the improvements in zero-shotperformance indeed stem from less positional corre-spondence to input tokens, we assess the difﬁcultyof recovering input positional information beforeand after applying our proposed method.
speciﬁ-cally, we train a classiﬁer to predict the input tokenid’s (which word it is) or position id’s (the word’sabsolute position in a sentence) based on encoderoutputs.
such prediction tasks have been used toanalyze linguistic properties of encoded represen-tation (adi et al., 2017).
our classiﬁer operateson each timestep and uses a linear projection fromthe embedding dimension to the number of classes,i.e.
number of unique tokens in the vocabulary ornumber of maximum timesteps..table 8 compares the classiﬁcation accuracy ofthe baseline and our model.
first, the baseline en-coder output has an exact one-to-one correspon-dence to the input tokens, as evidenced by thenearly perfect accuracy when recovering token id’s.
this task becomes much more difﬁcult under ourmodel.
we see a similar picture when recoveringthe position id’s..dataset.
iwslt.
europarlnon-overlap.
pmindia.
model.
token id.
position id.
baselineresidual.
baselineresidual.
baselineresidual.
99.9%48.5%.
99.5%71.6%.
99.6%63.3%.
93.3%51.4%.
85.1%22.5%.
90.1%26.9%.
table 8: accuracy of classiﬁers trained to recover in-put positional information (token id or position id)based on encoder outputs.
lower values indicate higherdifﬁculty of recovering the information, and thereforeless positional correspondence to the input tokens..baseline.
residual.
ycarucca.
).
%.
(.
1007550250.layer 1 layer 2 layer 3 layer 4 layer 5.figure 3: accuracy of recovering position id’s aftereach encoder layer on iwslt.
when we remove theresidual connection in the 3rd encoder layer, classiﬁca-tion is much more difﬁcult..figure 4: confusion matrices when classifying lan-guages in europarl non-overlap (x: true, y: predicted).
encoder outputs of related languages (romance / ger-manic) are more similar after residual removal..1265dadeesfifritnlptdadeesfifritnlptbaselinedadeesfifritnlptdadeesfifritnlptresidual0.00.20.40.60.81.0dataset↑.
iwslt.
europarlnon-overlap.
pmindia.
model↑.
svcca score↑.
accuracy↓.
baselineresidual.
baselineresidual.
baselineresidual.
0.6820.703.
0.6520.680.
0.6210.650.
95.9%87.6%.
87.0%69.9%.
74.1%62.0%.
go together with the noun “beispiel (example)”.
the zero-shot outputs, on the other hand, directlytranslates “geven (give; dutch)” to “geben (give;german)”, resulting in a more natural pairing with“beispiel (example)”.
with this example, we intendto showcase the potential of bypassing the pivotingstep and better exploiting language similarity..table 9: average pairwise similarity of encoder out-puts for between all languages in each dataset.
highersvcca scores and lower classiﬁcation accuracy indi-cate higher similarity.
we note that the svcca scorebetween random vectors is around 0.57..moreover, we compare the svcca scores aftereach encoder layer, as shown in figure 5. conﬁrm-ing our hypotheses, the model outputs are muchmore similar after the transition layer, as shownby the sharp increase at layer 3. this contrasts thebaseline, where similarity increases nearly linearly..baseline.
residual.
accvs∆.
serocs.0.150.100.050.00.layer 1 layer 2 layer 3 layer 4 layer 5.figure 5: average svcca scores after each encoderlayer between all language pairs in iwslt ({it, nl, ro,en}).
scores are reported additive to scores betweenrandom vectors.
similarity signiﬁcantly increases afterthe 3rd layer where we apply residual removal..given these ﬁndings and previous analyses insubsection 5.1, we conclude that our devisedchanges in a middle encoder layer allows highercross-lingual generalizability in top layers whileretaining the language-speciﬁc bottom layers..5.3 understanding gains of zero-shot.
translation between related languages.
in subsection 4 we have shown that between re-lated languages zero-shot translation surpasses piv-oting performance.
here we manually inspect somepivoting translation outputs (nl→en→de) and com-pare them to zero-shot outputs (de→en).
in general,we observe that the translations without pivotingare much more similar to the original sentences.
for instance in table 4, when pivoting, the dutchsentence “geven het voorbeeld (give the example)”is ﬁrst translated to “set the example”, then to “set-zen das beispiel (set the example)” in german,which is incorrect as the verb “setzen (set)” cannot.
input(nl).
pivot-in(nl→en).
pivot-out(en→de).
zero-shot(nl→de).
... geven in dit verband het verkeerdevoorbeeld, maar anderen helaas ook..... are setting the wrong example here,but others are unfortunately also..... setzen hier das falsche beispiel ein,andere sind leider auch..... geben in diesem zusammenhang dasfalsche beispiel, aber leider auch andere..table 10: an example of pivoting (nl→en→de) vszero-shot (nl→de).
pivoting via english leads to theincorrect verb-noun pairing of “setzen das beispiel (setthe example)” in german, while zero-shot output uti-lizes language similarity to get higher output quality..5.4 where to remove residual connections.
in our main experiments, all proposed modiﬁca-tions take place in a middle encoder layer.
aftercomparing the effects of residual removal in each ofthe encoder layers, our ﬁrst observation is that thebottom encoder layer should remain fully position-aware.
removing the residual connections in theﬁrst encoder layer degrades zero-shot performanceby 2.8 bleu on average on iwslt.
secondly, leav-ing out residual connections in top encoder layers(fourth or ﬁfth layer of the ﬁve layers) slows downconvergence.
when keeping the number of trainingepochs unchanged from our main experiments, itcomes with a loss of 0.4 bleu on the superviseddirections.
this is likely due to the weaker gradientﬂow to the bottom layers.
the two observationstogether support our choice of using the middleencoder layer as a transition point..5.5 learned or fixed positional embedding.
while we use ﬁxed trigonometric positional en-codings in our main experiments, we also validateour ﬁndings with learned positional embeddingson the iwslt dataset.
first, the baseline still suf-fers from off-target zero-shot translation (averagebleu scores on supervised directions: 29.6; zero-shot: 4.8).
second, removing the residual connec-tion in a middle layer is also effective in this case(supervised: 29.1; zero-shot: 17.1).
these ﬁndingssuggest that our approach is robust to the form of.
1266positional embedding.
although learned positionalembeddings are likely more language-agnostic byseeing more languages, as we still present sourcesentences as a sequence of tokens, the residual con-nections, when present in all layers, would stillenforce a one-to-one mapping to the input tokens.
this condition allows our motivation and approachto remain applicable..6 related work.
initial works on multilingual translation systemsalready showed some zero-shot capability (johnsonet al., 2017; ha et al., 2016).
since then, severalworks improved zero-shot translation performanceby controlling or learning the level of parametersharing between languages (lu et al., 2018; platan-ios et al., 2018)..recently, models with full parameter sharinghave gained popularity, with massively multilin-gual systems showing encouraging results (aharoniet al., 2019; zhang et al., 2020a; fan et al., 2020).
besides advantages such as compactness and easeof deployment, the tightly-coupled model compo-nents also open up new questions.
one question ishow to form language-agnostic representations at asuitable abstraction level.
in this context, one ap-proach is to introduce auxiliary training objectivesto encourage similarity between the representationsof different languages (arivazhagan et al., 2019;pham et al., 2019).
in this work we took a differentperspective: instead of introducing additional ob-jectives, we relax some of the pre-deﬁned structureto facilitate language-independent representations.
another line of work on improving zero-shottranslation utilizes monolingual pretraining (guet al., 2019; ji et al., 2020) or synthetic data for thezero-shot directions by generated by backtransla-tion (gu et al., 2019; zhang et al., 2020a).
withboth approaches, the zero-shot directions must beknown upfront in order to train on the correspond-ing languages.
in comparison, our adaptation pro-cedure offers more ﬂexibility, as the ﬁrst trainingstep remains unchanged regardless of which newlanguage is later ﬁnetuned on.
this could be suit-able to the practical scenario of later acquiring datafor the new language.
our work is also relatedto adaptation to new languages.
while the exist-ing literature mostly focused on adapting to one ormultiple supervised training directions (zoph et al.,2016; neubig and hu, 2018; zhou et al., 2019;murthy et al., 2019; bapna and firat, 2019), ourfocus in this work is to rapidly expand translation.
coverage via zero-shot translation..while our work concentrates on an english-centered data scenario, another promising directionto combat zero-shot conditions is to enrich avail-able training data by mining parallel data betweennon-english languages (fan et al., 2020; freitagand firat, 2020).
on a broader scope of sequence-to-sequence tasks, dalmia et al.
(2019) enforcedencoder-decoder modularity for speech recognition.
the goal of modular encoders and decoders is anal-ogous to our motivation for zero-shot translation..7 conclusion.
in this work, we show that the positional correspon-dence to input tokens hinders zero-shot translation.
speciﬁcally, we demonstrate that: 1) the encoderoutputs retain word orders of source languages; 2)this positional information reduces cross-lingualgeneralizability and therefore zero-shot translationquality; 3) the problems above can be easily allevi-ated by removing the residual connections in onemiddle encoder layer.
with this simple modiﬁca-tion, we achieve improvements up to 18.5 bleupoints on zero-shot translation.
the gain is espe-cially prominent in related languages, where ourproposed model outperforms pivot-based transla-tion.
our approach also enables integration ofnew languages with little parallel data.
similarto interlingua-based models, by adding two trans-lation directions, we can increase the translationcoverage by 2n language pairs, where n is theoriginal number of languages.
in terms of modelrepresentation, we show that the encoder outputsunder our proposed model are more language-independent both on a sentence and token level..acknowledgment.
this work is supported by a facebook sponsoredresearch agreement.
we thank yuqing tang forhelpful comments, and ngoc quan pham for shar-ing the training details of (pham et al., 2019)..broader impact.
we proposed approaches to improve zero-shottranslation, which is especially suitable to low-resource scenarios with no training data availablebetween some languages.
we also validated our ap-proaches on actual low-resource languages.
how-ever, as the models are trained on single domains,when facing out-of-domain test sentences, theycould suffer from hallucination, i.e.
produce trans-lations unrelated to the input sentences..1267references.
yossi adi, einat kermany, yonatan belinkov, oferlavi, and yoav goldberg.
2017. fine-grained anal-ysis of sentence embeddings using auxiliary pre-in 5th international conferencediction tasks.
on learning representations, iclr 2017, toulon,france, april 24-26, 2017, conference track pro-ceedings.
openreview.net..roee aharoni, melvin johnson, and orhan firat.
2019.massively multilingual neural machine translation.
in proceedings of the 2019 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 1 (long and short papers), pages3874–3884, minneapolis, minnesota.
associationfor computational linguistics..naveen arivazhagan, ankur bapna, orhan firat, roeeaharoni, melvin johnson, and wolfgang macherey.
2019. the missing ingredient in zero-shot neuralmachine translation.
corr, abs/1903.07091..ankur bapna and orhan firat.
2019. simple, scal-inable adaptation for neural machine translation.
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1538–1548, hong kong, china.
association for computa-tional linguistics..mauro cettolo, marcello federico, luisa bentivogli,niehues jan, st¨uker sebastian, sudoh katsuitho,yoshino koichiro, and federmann christian.
2017.overview of the iwslt 2017 evaluation campaign.
in international workshop on spoken languagetranslation, pages 2–14..raj dabre and sadao kurohashi.
2017. mmcr4nlp:multilingual multiway corpora repository for naturallanguage processing.
corr, abs/1710.01025..siddharth dalmia, abdelrahman mohamed, mikelewis, florian metze, and luke zettlemoyer.
2019.enforcing encoder-decoder modularity in sequence-to-sequence models.
corr, abs/1911.03782..angela fan, shruti bhosale, holger schwenk, zhiyima, ahmed el-kishky, siddharth goyal, man-deep baines, onur celebi, guillaume wenzek,vishrav chaudhary, naman goyal, tom birch, vi-taliy liptchinsky, sergey edunov, edouard grave,michael auli, and armand joulin.
2020.be-yond english-centric multilingual machine transla-tion.
corr, abs/2010.11125..orhan firat, kyunghyun cho, baskaran sankaran,fatos t. yarman-vural, and yoshua bengio.
2017.multi-way, multilingual neural machine translation.
comput.
speech lang., 45:236–252..markus freitag and orhan firat.
2020. complete mul-in proceed-tilingual neural machine translation.
ings of the fifth conference on machine translation,.
pages 550–560, online.
association for computa-tional linguistics..yarin gal and zoubin ghahramani.
2016. a theo-retically grounded application of dropout in recur-rent neural networks.
in d. d. lee, m. sugiyama,u. v. luxburg, i. guyon, and r. garnett, editors,advances in neural information processing systems29, pages 1019–1027.
curran associates, inc..jiatao gu, yong wang, kyunghyun cho, and vic-tor o.k.
li.
2019.improved zero-shot neural ma-chine translation via ignoring spurious correlations.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages1258–1268, florence, italy.
association for compu-tational linguistics..thanh-le ha, jan niehues, and alexander waibel.
2016. toward multilingual neural machine transla-tion with universal encoder and decoder.
in proceed-ings of iwslt..barry haddow and faheem kirefu.
2020. pmindia - acollection of parallel corpora of languages of india.
corr, abs/2001.09907..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in 2016 ieee conference on computer vi-sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 770–778.
ieee computer society..baijun ji, zhirui zhang, xiangyu duan, min zhang,boxing chen, and weihua luo.
2020. cross-lingualpre-training based transfer for zero-shot neural ma-chine translation.
in the thirty-fourth aaai con-ference on artiﬁcial intelligence, aaai 2020, thethirty-second innovative applications of artiﬁcialintelligence conference, iaai 2020, the tenth aaaisymposium on educational advances in artiﬁcial in-telligence, eaai 2020, new york, ny, usa, febru-ary 7-12, 2020, pages 115–122.
aaai press..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..philipp koehn.
2005. europarl: a parallel corpus forstatistical machine translation.
in mt summit, vol-ume 5, pages 79–86.
citeseer..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..1268annette rios, mathias m¨uller, and rico sennrich.
2020. subword segmentation and a single bridgelanguage affect zero-shot neural machine translation.
in proceedings of the fifth conference on machinetranslation, pages 528–537, online.
association forcomputational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in i. guyon, u. v. luxburg, s. bengio,h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..shan yang, heng lu, shiyin kang, liumeng xue,jinba xiao, dan su, lei xie, and dong yu.
2020.on the localness modeling for the self-attentionbased end-to-end speech synthesis.
neural net-works, 125:121–130..biao zhang, philip williams, ivan titov, and rico sen-nrich.
2020a.
improving massively multilingual neu-ral machine translation and zero-shot translation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1628–1639, online.
association for computational lin-guistics..shucong zhang, erfan loweimi, peter bell, and steverenals.
2020b.
when can self-attention be replacedby feed forward layers?
corr, abs/2005.13895..chunting zhou, xuezhe ma, junjie hu, and grahamneubig.
2019. handling syntactic divergence inlow-resource machine translation.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1388–1394, hong kong,china.
association for computational linguistics..barret zoph, deniz yuret, jonathan may, and kevinknight.
2016. transfer learning for low-resourcein proceedings of theneural machine translation.
2016 conference on empirical methods in natu-ral language processing, pages 1568–1575, austin,texas.
association for computational linguistics..sneha kudugunta, ankur bapna, isaac caswell, andorhan firat.
2019. investigating multilingual nmtin proceedings of therepresentations at scale.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1565–1575, hong kong,china.
association for computational linguistics..yichao lu, phillip keung, faisal ladhak, vikas bhard-waj, shaonan zhang, and jason sun.
2018. a neu-ral interlingua for multilingual machine translation.
in proceedings of the third conference on machinetranslation: research papers, pages 84–92, brus-sels, belgium.
association for computational lin-guistics..rudra murthy, anoop kunchukuttan, and pushpakbhattacharyya.
2019. addressing word-order di-vergence in multilingual neural machine translationfor extremely low resource languages.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3868–3873, min-neapolis, minnesota.
association for computationallinguistics..graham neubig and junjie hu.
2018. rapid adapta-tion of neural machine translation to new languages.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages875–880, brussels, belgium.
association for com-putational linguistics..ngoc-quan pham, jan niehues, thanh-le ha, andalexander waibel.
2019. improving zero-shot trans-lation with language-independent constraints.
inproceedings of the fourth conference on machinetranslation (volume 1: research papers), pages 13–23, florence, italy.
association for computationallinguistics..emmanouil antonios platanios, mrinmaya sachan,graham neubig, and tom mitchell.
2018. contex-tual parameter generation for universal neural ma-chine translation.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, pages 425–435, brussels, belgium.
as-sociation for computational linguistics..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..maithra raghu, justin gilmer, jason yosinski, andjascha sohl-dickstein.
2017. svcca: singular vec-tor canonical correlation analysis for deep learningdynamics and interpretability.
in i. guyon, u. v.luxburg, s. bengio, h. wallach, r. fergus, s. vish-wanathan, and r. garnett, editors, advances in neu-ral information processing systems 30, pages 6076–6085. curran associates, inc..1269a appendix: bleu scores per translation direction.
a.1 multiway iwslt 2017.direction.
pivot (x→en→y).
baseline +vardrop.
residual +vardrop.
residual+query +vardrop.
a.2 europarl.
a.2.1 non-overlapping data.
direction.
pivot (x→en→y).
baseline +vardrop.
residual +vardrop.
residual+query +vardrop.
desivrepus.tohs-orez.desivrepus.tohs-orez.en-iten-nlen-roit-ennl-enro-enaverage.
it-nlit-ronl-itnl-roro-itro-nlaverage.
da-ende-enes-enﬁ-enfr-enit-ennl-enpt-enen-daen-deen-esen-ﬁen-fren-iten-nlen-ptaverage.
da-deda-esda-ﬁda-frda-itda-nlda-ptde-dade-esde-ﬁde-frde-itde-nlde-ptes-daes-dees-ﬁes-fres-ites-nles-ptﬁ-daﬁ-deﬁ-esﬁ-frﬁ-itﬁ-nlﬁ-ptfr-dafr-defr-esfr-ﬁfr-itfr-nlfr-ptit-dait-deit-esit-ﬁit-frit-nlit-ptnl-danl-denl-es.
19.818.320.116.421.118.519.1.
------.
----------------.
24.233.118.130.626.126.329.929.232.117.929.925.625.929.231.424.819.434.829.626.835.126.320.829.227.223.222.826.529.423.635.917.928.426.332.827.622.333.717.031.424.930.827.723.230.7.
30.427.923.235.831.030.629.8.
11.511.810.09.212.210.310.8.
38.336.143.032.639.437.334.441.136.528.142.522.537.732.729.738.335.6.
5.215.85.09.911.56.514.49.511.64.27.38.85.510.810.24.34.59.19.85.913.69.84.011.97.68.45.310.88.63.411.83.98.05.011.58.33.510.93.46.95.010.610.14.412.6.
30.127.722.935.731.030.429.6.
15.015.815.214.115.813.414.9.
38.536.043.232.839.536.934.241.036.728.042.622.238.032.729.938.435.7.
21.932.416.628.124.324.328.626.931.216.727.823.423.728.028.121.417.333.428.724.335.323.617.827.824.320.920.225.025.619.335.515.727.322.831.824.318.133.913.430.221.630.425.520.030.0.
29.827.422.935.530.430.129.4.
18.517.817.915.519.616.817.7.
38.135.943.032.439.136.734.040.736.227.742.222.237.832.429.638.035.4.
25.032.818.129.424.825.829.329.232.117.528.924.626.128.931.024.318.634.829.726.535.925.420.228.425.421.822.025.328.623.136.417.228.725.933.326.821.334.415.931.624.031.327.723.530.8.
29.827.622.435.030.529.429.1.
18.617.718.115.519.816.617.7.
38.235.542.931.939.036.433.840.636.127.542.221.637.532.229.438.035.2.
24.532.517.129.325.026.029.129.131.616.729.224.526.128.430.523.817.534.929.626.136.024.919.427.625.421.421.624.828.322.736.216.028.325.433.126.721.134.413.831.423.731.127.823.430.6.
30.027.423.335.230.529.829.4.
18.718.217.915.619.516.717.8.
37.735.242.531.938.836.433.340.235.827.541.721.337.432.029.537.934.9.
23.932.317.228.924.825.528.827.531.016.328.423.725.228.229.523.417.734.229.125.335.524.118.827.324.720.920.724.026.821.635.115.827.724.932.625.220.233.514.630.823.430.926.622.530.1.
29.726.922.233.829.929.328.6.
18.017.318.015.519.616.517.5.
37.334.942.131.037.535.232.939.535.726.741.620.837.031.829.137.634.4.
23.731.616.628.424.325.528.428.330.916.128.623.625.427.929.623.117.034.128.925.535.524.318.926.624.320.120.923.926.421.334.715.226.824.731.925.420.033.013.030.723.330.727.022.729.8.
1270direction.
pivot (x→en→y).
baseline +vardrop.
residual +vardrop.
residual+query +vardrop.
nl-ﬁnl-frnl-itnl-ptpt-dapt-dept-espt-ﬁpt-frpt-itpt-nlaverage.
16.528.824.528.230.524.337.718.434.129.326.427.1.
4.78.39.212.29.53.514.34.17.78.75.48.2.
15.527.122.527.027.120.637.716.832.627.923.725.1.
16.028.123.927.529.523.638.317.933.929.225.726.7.
15.828.323.827.329.223.238.217.134.129.025.626.4.
15.627.623.127.228.022.437.517.233.428.724.825.8.
15.427.823.327.128.422.537.416.633.328.325.225.6.a.2.2 multiway data.
direction.
pivot (x→en→y).
baseline.
residual.
residual+query.
desivrepus.tohs-orez.da-ende-enes-enﬁ-enfr-enit-ennl-enpt-enen-daen-deen-esen-ﬁen-fren-iten-nlen-ptaverage.
da-deda-esda-ﬁda-frda-itda-nlda-ptde-dade-esde-ﬁde-frde-itde-nlde-ptes-daes-dees-ﬁes-fres-ites-nles-ptﬁ-daﬁ-deﬁ-esﬁ-frﬁ-itﬁ-nlﬁ-ptfr-dafr-defr-esfr-ﬁfr-itfr-nlfr-ptit-dait-deit-esit-ﬁit-frit-nlit-ptnl-danl-denl-esnl-ﬁnl-frnl-itnl-ptpt-dapt-dept-espt-ﬁpt-frpt-itpt-nl.
----------------.
23.331.617.329.525.125.228.627.530.516.228.724.324.627.730.323.818.433.328.425.933.224.919.627.826.021.821.925.128.322.633.917.026.925.131.026.721.532.215.929.924.129.526.422.029.315.827.723.326.729.323.435.817.632.627.925.5.
36.333.640.530.337.034.732.338.335.728.042.021.337.532.229.337.934.2.
10.011.86.214.414.311.212.914.810.15.211.612.49.810.516.49.46.315.617.310.716.113.47.910.311.511.89.011.014.28.312.15.315.39.612.813.87.711.74.712.29.212.215.49.310.85.612.513.412.114.38.314.25.313.816.09.3.
36.133.440.130.136.834.432.237.935.727.141.420.837.231.829.237.633.9.
23.932.217.029.024.825.428.928.431.416.228.524.125.428.330.024.017.834.029.426.235.424.619.427.725.221.221.424.827.522.135.616.428.125.032.526.321.033.915.031.223.630.727.122.830.215.527.723.627.128.823.038.017.033.428.625.4.
35.032.339.129.235.233.431.237.334.826.541.220.536.731.428.737.233.1.
22.831.416.427.824.224.128.326.930.715.127.223.124.227.028.922.316.932.828.524.734.624.018.327.223.920.520.424.026.120.534.415.326.923.931.325.219.433.414.829.822.429.826.121.629.314.726.722.626.227.721.637.016.332.128.124.3.
1271direction.
pivot (x→en→y).
baseline.
residual.
residual+query.
average.
25.9.
11.3.
26.1.
25.1.a.2.3 full data.
direction.
pivot (x→en→y).
baseline.
residual.
residual+query.
desivrepus.tohs-orez.da-ende-enes-enﬁ-enfr-enit-ennl-enpt-enen-daen-deen-esen-ﬁen-fren-iten-nlen-ptaverage.
da-deda-esda-ﬁda-frda-itda-nlda-ptde-dade-esde-ﬁde-frde-itde-nlde-ptes-daes-dees-ﬁes-fres-ites-nles-ptﬁ-daﬁ-deﬁ-esﬁ-frﬁ-itﬁ-nlﬁ-ptfr-dafr-defr-esfr-ﬁfr-itfr-nlfr-ptit-dait-deit-esit-ﬁit-frit-nlit-ptnl-danl-denl-esnl-ﬁnl-frnl-itnl-ptpt-dapt-dept-espt-ﬁpt-frpt-itpt-nlaverage.
----------------.
26.134.520.031.827.527.531.030.533.619.131.326.727.530.532.926.521.036.231.128.436.427.722.330.728.824.424.228.030.525.036.819.629.627.533.729.123.934.818.332.526.231.928.724.331.517.929.925.529.031.725.738.920.035.430.427.928.4.
38.235.743.332.439.837.433.941.535.727.941.922.037.732.129.138.035.4.
13.627.515.921.716.810.424.816.122.814.017.314.08.521.818.513.915.824.118.711.030.215.411.222.417.413.58.620.916.812.427.414.616.59.826.815.711.326.113.319.68.924.916.512.423.613.718.615.422.417.213.030.814.823.117.710.417.5.
39.236.844.133.540.538.434.942.336.829.043.022.938.333.130.439.136.4.
25.533.718.930.126.326.230.629.732.618.230.125.526.129.831.224.719.634.931.026.736.825.920.529.326.122.922.226.729.423.837.418.529.826.634.628.222.035.416.530.325.032.527.923.231.317.229.124.428.530.224.439.319.234.830.326.227.5.
38.736.443.633.040.138.134.341.336.328.442.422.337.832.729.938.835.9.
24.833.017.829.325.926.129.328.731.616.029.424.525.427.930.324.518.434.330.226.335.924.919.928.224.822.221.625.028.523.036.516.929.125.733.026.721.634.515.029.324.031.527.123.530.415.428.223.926.929.023.738.317.533.929.525.526.5.a.3 pmindia.
1272direction.
pivot (x→en→y).
baseline +vardrop.
residual +vardrop.
residual+query.
desivrepus.tohs-orez.te-enkn-enml-enbn-engu-enhi-enmr-enor-enpa-enen-teen-knen-mlen-bnen-guen-hien-mren-oren-paaverage.
te-knte-mlte-bnte-gute-hite-mrte-orte-pakn-tekn-mlkn-bnkn-gukn-hikn-mrkn-orkn-paml-teml-knml-bnml-guml-himl-mrml-orml-pabn-tebn-knbn-mlbn-gubn-hibn-mrbn-orbn-pagu-tegu-kngu-mlgu-bngu-higu-mrgu-orgu-pahi-tehi-knhi-mlhi-bnhi-guhi-mrhi-orhi-pamr-temr-knmr-mlmr-bnmr-gumr-himr-ormr-paor-teor-knor-mlor-bnor-guor-hior-mror-papa-tepa-knpa-mlpa-bnpa-gupa-hipa-mrpa-oraverage.
------------------.
24.915.818.028.921.619.525.428.212.816.017.929.322.219.725.929.011.924.017.527.320.518.924.827.010.721.614.025.318.917.423.825.113.226.616.919.225.621.428.132.014.028.017.720.034.922.630.635.311.924.115.617.428.521.625.327.611.423.114.917.528.221.718.727.712.826.416.618.932.426.821.328.022.1.
31.231.428.925.635.337.829.029.035.916.333.420.422.639.332.525.733.738.830.4.
0.61.00.71.20.41.20.70.50.60.90.91.10.41.20.60.50.60.50.91.10.41.30.60.50.50.71.01.10.51.60.90.60.40.50.70.70.41.40.60.50.40.50.60.51.01.00.50.40.50.60.90.91.10.50.70.60.60.70.90.91.30.41.50.60.50.60.80.81.40.51.40.70.8.
30.231.127.925.434.737.128.728.935.216.733.420.822.539.632.625.733.738.930.2.
3.23.82.72.40.53.82.32.12.03.62.62.60.53.62.42.22.33.32.82.40.53.92.32.02.42.93.92.70.74.23.02.51.62.33.01.80.53.22.02.31.11.31.91.21.62.61.41.51.92.83.52.42.50.72.22.22.02.93.53.03.00.84.22.71.62.33.22.02.60.63.42.22.4.
30.731.328.225.034.737.028.828.935.116.132.620.321.938.931.525.433.038.029.9.
2.53.22.02.31.02.62.62.52.03.12.32.41.02.92.62.71.92.82.32.31.13.02.52.41.92.83.22.11.13.03.12.51.62.72.61.91.32.82.92.91.41.92.21.41.92.42.02.31.82.52.92.22.21.42.62.81.83.03.22.62.71.53.13.11.82.62.71.92.61.53.02.72.3.
30.231.127.925.434.737.128.728.935.216.132.620.321.938.931.525.433.038.029.8.
16.310.812.419.312.812.617.217.98.410.912.119.912.912.817.418.48.216.012.017.911.912.116.316.77.414.29.317.811.911.716.716.98.416.610.712.515.714.518.120.68.515.610.412.921.114.518.121.78.015.710.412.119.513.417.017.08.015.510.312.719.613.912.318.78.516.210.612.622.017.213.818.614.3.
29.730.327.925.033.535.828.128.334.515.732.220.121.838.030.824.832.637.129.2.
1.31.52.00.91.91.71.10.60.71.82.20.80.41.71.40.70.71.22.00.90.41.81.30.60.71.31.71.00.51.81.30.60.51.21.51.80.41.71.30.70.51.01.11.20.81.61.00.60.71.41.52.10.90.61.40.70.61.51.62.31.00.51.70.70.51.11.51.91.00.41.71.21.1.
1273