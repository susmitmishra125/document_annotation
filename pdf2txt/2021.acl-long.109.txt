clip: a dataset for extracting action items for physicians fromhospital discharge notes.
james mullenbach1∗ yada pruksachatkun2∗† sean adler1jennifer seale3†jordan swartz t. greg mckelvey4† hui dai yi yang1 david sontag1,51 asapp2 amazon alexa ai3 cuny graduate center4 united states digital service5 mit.
abstract.
continuity of care is crucial to ensuring pos-itive health outcomes for patients dischargedfrom an inpatient hospital setting, and im-proved information sharing can help.
to shareinformation, caregivers write discharge notescontaining action items to share with patientsand their future caregivers, but these actionitems are easily lost due to the lengthiness ofthe documents.
in this work, we describe ourcreation of a dataset of clinical action items an-notated over mimic-iii, the largest publiclyavailable dataset of real clinical notes.
thisdataset, which we call clip, is annotated byphysicians and covers 718 documents repre-senting 100k sentences.
we describe the taskof extracting the action items from these doc-uments as multi-aspect extractive summariza-tion, with each aspect representing a type of ac-tion to be taken.
we evaluate several machinelearning models on this task, and show thatthe best models exploit in-domain languagemodel pre-training on 59k unannotated docu-ments, and incorporate context from neighbor-ing sentences.
we also propose an approachto pre-training data selection that allows us toexplore the trade-off between size and domain-speciﬁcity of pre-training datasets for this task..1.introduction.
transitioning patient care from hospitals to primarycare providers (pcps) can frequently result in med-ical errors (kripalani et al., 2007).
when patientsare discharged, they often require further actionsto be taken by their pcp, who manages their long-term health, such as reviewing results for lab testsonce they are available (moore et al., 2007).
yetpcps often have many patients and little time toreview new clinical documents related to a recent.
∗ equal contribution† work done while at asapp..hospital stay (baron, 2010), so making this reviewfast, actionable, and accurate will be beneﬁcial..discharge notes are typically lengthy (weis andlevy, 2014) and written as free text, so pcpsmay fail to identify important pending actions,which inadvertently leads patients to poor out-comes.
spencer et al.
(2019) found that pcps con-sidered the lack of a standardized follow-up sectionto be a key driver in missing follow-up action items.
while discharge notes may include follow-up sec-tions, they are typically aimed at the patient and notcurated for pcp use.
jackson et al.
(2015) foundthat following up on pending clinical actions is criti-cal for minimizing risk of medical error during caretransitions, especially for patients with complextreatment plans.
automatic extraction of actionitems can make physicians more efﬁcient by re-ducing the high cognitive load and time-consumingburden of using electronic health records (tai-sealeet al., 2017; sinsky et al., 2016; singh et al., 2013;farri et al., 2013).
to our knowledge, there hasbeen little previous work using machine learningto address this important clinical problem..potential impact successful automatic extrac-tion of action items can have several direct beneﬁts.
first, it can improve patient safety by fosteringmore comprehensive and complete care by pcps.
second, it might make physicians more efﬁcient atperforming a comprehensive review of action items,which is critical as physicians spend an increasingamount of time interacting with electronic healthrecord (ehr) systems (tai-seale et al., 2017; sin-sky et al., 2016).
further, reviewing and synthesiz-ing lengthy or complicated patient histories placesa signiﬁcant cognitive load on physicians, whichhas been associated with increased medical error(singh et al., 2013; farri et al., 2013), so reducingthis cognitive load is an area of opportunity.
finally,a working system might integrate with ehrs to au-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1365–1378august1–6,2021.©2021associationforcomputationallinguistics1365actiontype.
description.
example.
appointment appointments to be made by the pcp, or monitoredto ensure the patient attends them..the patient requires a neurology consult at xyz forevaluation..laboratory tests that either have results pending orneed to be ordered by the pcp..we ask that the patients’ family physician repeatthese tests in 2 weeks to ensure resolution..lab.
procedure.
procedures that the pcp needs to either order, en-sure another caregiver orders, or ensure the patientundergoes..please follow-up for egd with gi..medication medications that the pcp either needs to ensurethat the patient is taking correctly, e.g.
time-limitedmedications or new medications that may need doseadjustment..imaging.
imaging studies that either have results pending orneed to be ordered by the pcp..patientstructions.
in-.
post-discharge instructions that are directed to thepatient, so the pcp can ensure the patient under-stands and performs them..other.
other actionable information that is important torelay to the pcp but does not fall under existing as-pects (e.g.
the need to closely observe the patient’sdiet, or fax results to another provider)..the patient was instructed to hold asa and refrainfrom nsaids for 2 weeks..superior segment of the left lower lobe: roundeddensity which could have been related to infection,but follow-up for resolution recommended to ex-clude possible malignancy.
no driving until post-op visit and you are no longertaking pain medications..since the patient has been struggling to gain weightthis past year, we will monitor his nutritional statusand trend weights closely..table 1: description and examples of action items.
we created all examples speciﬁcally for the purpose of clariﬁ-cation, and they do not stem from any real patient source..tomatically address certain action items such asscheduling appointments, thereby improving ehrusability and further reducing medical error..contributions we introduce a new clinical nat-ural language processing task that accomplishesfocused information extraction from intensive careunit (icu) discharge notes by selecting sentencesthat contain action items for pcps or patients.
anaction item is a statement in a discharge note thatexplicitly or implicitly directs the reader to an ac-tion that should be taken as a result of the hospitalstay described in the document.
given a dischargenote, the task is to extract all action items in thenote.
we cast this task as a special case of multi-aspect document summarization, with each aspectrepresenting an area of patient care to monitor oron which to take action (see examples in table 1).
we create the ﬁrst annotated dataset for this newtask, clip, a dataset of 718 annotated notes frommimic-iii (johnson et al., 2016), comprising over100k annotated sentences.
this will be, to ourknowledge, one of the largest annotated datasetsfor clinical nlp, which tend to be smaller due tothe expense of expert annotators..we evaluate machine learning methods to tacklethis task.
similar to prior work on multi-aspect.
extractive summarization, we employ sentence-level multi-label classiﬁcation techniques (hayashiet al., 2020).
our proposed architecture consists ofpassing a sentence, and its neighboring sentenceson its left and right, through a pre-trained bertmodel (devlin et al., 2019) with minor modiﬁca-tions.
since there is limited annotated data buta wealth of unlabeled in-domain clinical notes,we also explore the impact of unsupervised learn-ing on this task.
we develop a method for task-targeted pre-training data selection, in which amodel trained on the downstream task selects unla-beled document segments for ﬁne-tuning a bertmodel.
we ﬁnd that this focused pre-training ismuch faster than pre-training on all available dataand achieves competitive results.
our results showthat unsupervised pre-training of any form is criti-cal to improving results..our code is available as open-source software 1,and our annotations are available via physionet 2,to fully enable reproduction of our results and toprovide a benchmark for evaluating future advancesin clinical nlp in the context of this clinically.
1https://github.com/asappresearch/clip2as they are built on top of mimic-iii, which physionetmaintains, access to our annotations requires the completionof an ethics course and a data use agreement..1366important problem (mullenbach et al., 2021)..2 related work and datasets.
clinical information extraction there has beena wealth of previous work on extracting informa-tion from clinical notes, much of which also fol-lows an extractive summarization approach.
forexample, were et al.
(2010) extracts items such aspatient smoking status and obesity comorbiditiesfrom discharge notes.
liang et al.
(2019) createda hybrid system of regex-based heuristics, neuralnetwork models trained on pre-existing datasets,and models such as support vector machines fordisease-speciﬁc extractive summarization..liu et al.
(2018b) developed a pseudo-labelling,semi-supervised approach, using intrinsic correla-tion between notes, to train extractive summariza-tion models for disease-speciﬁc summaries.
wediffer from these efforts in that we do not aim togenerate general-purpose or disease-speciﬁc sum-maries, rather we focus on extracting speciﬁc ac-tion items from discharge notes to facilitate caretransfer..clinical datasets datasets and challenges on theextraction of medication, tests, and procedure men-tions in clinical text (uzuner et al., 2010, 2011;jagannatha et al., 2019) have been released, butwithout the focus on providing actionable insightto pcps.
additionally, multiple datasets (uzuneret al., 2012; sun et al., 2013) have been introducedfor detecting temporal and co-reference relationsbetween parts of a note.
while it may be useful fora model to have a good grasp of co-reference andtemporal dependencies to understand what consti-tutes actionable information for a pcp, we chooseto optimize directly for the end task, noting recentwork demonstrating that modern pre-trained neuralnetworks will identify and exploit such informa-tion as needed (tenney et al., 2019).
although ondifferent tasks, we note that our dataset of 718 an-notated documents is larger than recently releaseddatasets, such as those from the n2c2 shared tasks.
for comparison, 500 documents were annotated foradverse drug event extraction (henry et al., 2020a),150 documents for family history extraction (liuet al., 2018a), and 100 documents for clinical con-cept normalization (henry et al., 2020b).
one ofthe largest annotated clinical datasets, emrqa, isbuilt on 2,425 clinical notes (pampari et al., 2018)..summarization priorsummarization work,which we build on, uses pre-trained transformermodels to construct sentence representations thatare contextualized with the entire document (liuand lapata, 2019; hayashi et al., 2020).
liuand lapata (2019) evaluate on three benchmarksummarization datasetsconsisting of newsarticles.
they are shorter, with average documentlengths from 400-800 words, whereas mimic-iiidischarge notes average over 1,400 words.
liuand lapata (2019) evaluate with rouge scoresstandard in summarization, whereas we takeadvantage of having ground truth extractedsentences and evaluate with classiﬁcation metrics,providing a substantially different task..liang et al.
(2019) develop a disease-speciﬁcsummary dataset, but it is not public and theirmethods involve combining a mix of outputs frommodels performing auxiliary tasks such as con-cept recognition, adverse drug event extraction, andmedication change detection, each of which haveto be individually developed and maintained..3 clip dataset.
in this section, we describe the process of creatingour clip dataset, short for clinicalfollow-up, and report statistics on the dataset..3.1 data collection.
clip is created on top of the popular clinicaldataset mimic-iii (johnson et al., 2016).
themimic-iii dataset contains 59,652 critical care dis-charge notes from the beth israel deaconess medi-cal center over the period of 2001 to 2012, amongmillions of other notes and structured data.
weannotated 718 randomly sampled discharge notesfrom the set of patients that were discharged fromthe icu (i.e., survived) and thus brought back to thecare of their primary care physician or relevant spe-cialists.
though this dataset is orders of magnitudesmaller than general summarization datasets suchas nallapati et al.
(2016), we note the relativelylarge expense associated with clinical annotationdue to both the length of documents (∼160 sen-tences on average) and the requirement of domainexperts.
this dataset is also the ﬁrst of its kind inthe clinical space.
the total number of sentences is107,494, of which 12,079 have at least one label..the sampled mimic-iii data is further split ran-domly into training, validation, and test sets, suchthat all sentences from a document go to the same.
1367patient instructionsappointmentsmedicationslab testsproceduresimagingother.
6.55%4.59%1.88%0.69%0.28%0.18%0.05%.
table 2: prevalence of each label type in clip trainingset..set, with 518, 100, and 100 notes respectively..our dataset was annotated by four physiciansand one resident over the course of three months.
we underwent several rounds of initial annotationswith calibration processes and instruction reﬁne-ment in between.
additional annotation details areprovided in the appendix and the full guidelines areavailable on our public repository.
we estimatedinter-rater reliability by having two physician anno-tators independently annotate a set of 13 documentscomprising 2600 sentences.
comparing predictionson a binary reduction of the task, in which a matchindicates that both annotators labeled a sentence(regardless of chosen label types), we measured acohen’s kappa statistic of 0.925..the seven action item aspects that we labeledin the dataset, along with example discharge notesnippets for each one, are presented in table 1..to emphasize the subtlety and complexity ofthis task, we highlight here some example rulesthat state what should not be annotated.
for theappointment label, we should exclude sentencesthat refer to “as needed” appointments, e.g.
“seeyour endocrinologist as needed.”; this describes nodeviation from status quo behavior and thus doesnot warrant follow-up action.
for the medicationlabel, we speciﬁcally exclude sentences describingsimple additions to the medication list, e.g.
“dis-charged on glargine 10u at bedtime,” as these typ-ically do not require further action.
however weinclude instructions to hold and restart medications,new medications with an end date (e.g.
antibiotics),and medications requiring dosage adjustment (e.g.
“...the plan is to keep patient off diuretics with mon-itoring of his labs and reinstitution once the kidneyfunction improves”), as these are likely to requirepcp action..3.2 training set statistics.
due to the large amount of discharge note text thathas information not directly actionable for follow-up, most sentences remain without a label after theannotation process; 11.2% of training set sentenceshave a label.
of the sentences with labels, 28.6%have multiple labels.
table 2 shows the frequencyof each label type at the sentence level in the train-ing set..3.3 dataset comparison and phenomena.
analysis.
to distinguish the contribution of our dataset in thecontext of existing text summarization datasets, weperformed a manual quantitative comparison be-tween clip and the summarization datasets cnn(hermann et al., 2015) and wikiasp (hayashiet al., 2020).
for wikiasp, we chose sentencesfrom the “event” genre of summary, as our datasetdescribes hospital stays which could be consideredevents.
inspired by suhr et al.
(2017), we identi-ﬁed ﬁve phenomena to compare across datasets -quantiﬁcation (in the numerical sense, as in “300mg” or “twenty-three people”), temporal expres-sions, conditional expressions, imperative moodor second-person statements, and out of vocabu-lary (oov) terms 3. we gathered 100 sentencesfrom the summaries of each dataset and countedthe occurrences of each phenomena..we see that clip has a relative wealth of im-perative and second-person statements, which isnot surprising due to the prevalence of patient-directed language in “patient instructions”-labeledsentences.
clip and wikiasp both have moretemporal expressions than cnn, which are con-tained in around half of the sample sentences ofeach.
despite the prevalence of clinical jargon inclip, wikiasp actually contained the most oovwords, perhaps due to the diversity of sources ofthat dataset.
conditional language, such as “if youmiss any doses of this medication, your stents couldclot off again...”, were uncommon in all datasetsbut occurred most in clip..4 learning to extract action items.
with a discharge note as input, the task is to out-put the clinically actionable follow-up items found.
3by oov, we mean any token that must be split into mul-tiple wordpiece tokens given the vanilla bert vocabulary.
for example, the common abbreviation for “patient”, “pt”,becomes “p”, “##t”..1368(a).
(b).
figure 1: (a) illustration of our bert-based architecture.
we input the sentence (red; top) to classify along with2 sentences of context on either side, joined with [sep] tokens and accompanied with segment (green; middle)and position (purple; bottom) embeddings, to integrate neighboring intra-document context into the token repre-sentations of the focus sentence.
we then apply a linear classiﬁcation layer over the [sep] token representationat the end of the focus sentence.
(b) our pre-training method.
first, we train a supervised model with the labeleddata (blue).
then, we apply it to unlabeled data (gray) to surface a fraction of the data to pre-train the model (a)with (red).
after pre-training, we ﬁne-tune on the labeled data, which leads to similar results as pre-training withall unlabeled data..cnn wikiasp clip.
quantiﬁcationtemporalconditionalimperative / 2nd person# oov.
2734030.91.
2656342.15.
274810412.10.table 3: comparing sentences in existing summariza-tion datasets with ours.
we randomly sampled 100 sen-tences from extractive summaries in each dataset andcounted each phenomenon.
# oov is reported as theaverage number of oov terms in a sentence.
for cnnand wikiasp, we adopted the greedy approach of nal-lapati et al.
(2016) to create extractive summaries..within the note.
there are many summarizationmethods that could appropriately handle this prob-lem.
the length of these documents and the highrelative risk of missing information in a clinicalsetting discourages the option of truncating doc-uments to ﬁt into modern neural network modelswhich may have maximum length requirements, sowe develop methods that approach the task as multi-label sentence classiﬁcation.
summarization of afull document can then be accomplished with theresulting model by feeding each sentence into themodel in sequence and aggregating the sentencesthat the model labels.
we will evaluate our experi-.
ments on this multilabel classiﬁcation formulation,as well as on a binary reduction of the problemin which the objective is to simply identify whichsentences have any type of label.
this binary fram-ing is still useful, as surfacing the sentences for areader is the primary objective that will save timeand effort, with classiﬁcation of the sentence beinga secondary beneﬁt..4.1 model architecture.
the bert architecture (devlin et al., 2019) hasbeen widely used within clinical nlp in the pastyear with successful results (lee et al., 2020;alsentzer et al., 2019; mulyar et al., 2019; johnsonet al., 2020; mcdermott et al., 2020; zhang et al.,2020).
in particular, si et al.
(2020) has shown theeffectiveness of bert for use on small annotatedclinical datasets, such as the one we develop.
weuse bert as the basis for our proposed model..bert-based baselines to demonstrate baselinebert performance, we ﬁne-tune pre-trained bertmodels on our task.
as the simplest approach,we feed a sentence into bert, take the hiddenstate of the [cls] token as the sentence-level rep-resentation, and train a linear layer over that rep-resentation.
to adapt bert to our domain, wealso experiment with a previously released version.
1369focus    right context left contexts1s2bertew0ew1ew2ew3ew4e0e1e2e3e4ebebebebebŷwordpiece embeddingsposition embeddingssegment embeddingsebeb[cls][sep][sep]w1w2sbsbp1p2[sep]w4w5sbsbp4p5[sep]w7w8sasap7p8sbp0sbp3sbp6sap9[sep]w10w11sbsbp10p11sbp12w13w14sbsbp13p14xclsxsepx1x2xsepx4x5xsepx7x8xsepx10x11x13x14bertberttrainunlabeled datatask-targeted datalabeled datamodeltrained modelbert+contextpre-trainedbert+contextfinalmodelapplypre-trainfine-tuneof bert which has been further pre-trained onmimic-iii discharge notes (alsentzer et al., 2019),and ﬁne-tune it on our task in the same way.
werefer to this variant as mimic-dnote-bert.
alsentzer et al.
(2019) also release a version pre-trained on all mimic-iii notes, which we referto as mimic-full-bert.
both mimic-full-bert and mimic-dnote-bert are initializedwith biobert (lee et al., 2020), which is pre-trained on a corpus of biomedical research articles..incorporating neighboring context surround-ing contexts are critical for the task, for two rea-sons: 1) an individual sentence may not have thefull picture on the type of the action; 2) neighboringsentences tend to share the same label (occurs for27% of sentences).
so, we incorporate context be-yond an individual sentence into our bert-basedsentence representations, by concatenating the twosentences each that immediately precede and fol-low the sentence to the input.
to do this, we followthe encoder architecture of liu and lapata (2019),which concatenates sentences with special tokensand applies alternating segment embeddings to al-ternating sentences.
we make the following mod-iﬁcations: we exclude the additional transformerlayers on top of the bert output, use only sep to-kens to separate sentences, and apply the segmentembedding sa to the tokens in the focus sentenceand sb to all other tokens, as pictured in figure 1.we initialize models of this architecture with vari-ous pre-trained bert parameters in experiments..4.2 task-targeted pre-training.
given the limited amount of annotated data, we aremotivated to pursue semi-supervised approaches.
we seek to explore the trade-off between general-ized and domain- or task-speciﬁc data for languagemodel pre-training, by introducing a technique fortargeted pre-training which we call task-targetedpre-training (ttp).
ttp requires less data and com-putation, yet attains comparable performance topre-training on large in-domain datasets that priorwork studied (alsentzer et al., 2019).
the goalof this approach is to surface unlabeled sentencesthat may be positive examples, in the vein of self-supervision techniques such as snorkel (ratneret al., 2017).
in contrast to snorkel, which usesmodel predictions to generate pseudolabels to trainwith, ttp uses model predictions to select sen-tences for pre-training, using auxiliary tasks..to create a task-targeted dataset, we ﬁrst ﬁne-.
tune a vanilla bert model on our task, and then weuse the learned model to classify all unlabeled sen-tences.
we select all sentences that the model pre-dicts as having action items, using a ﬁxed threshold.
due to the multi-label nature of our task, we applythe threshold across all labels and select sentencesin which at least 1 label score is above the thresh-old.
the threshold used to select the task-targetedsentences can be tweaked to create datasets forpre-training that are smaller and more task-focused(for higher thresholds), or larger and more general(for lower thresholds), which we experiment with.
this approach is inspired by and similar to task-adaptive pre-training (tapt) introduced by gu-rurangan et al.
(2020).
in that work, a pre-trainedbag-of-words language model encodes sentences inlabeled and unlabeled datasets, and for each labeledsentence selects its nearest neighbor unlabeled sen-tences according to the model.
in this paper, weselect data points using the full prediction model(rather than just an encoder), and use threshold-ing which provides maximal control over the sizeof the selected dataset.
further, directly applyingtapt to our case may not work well as it doesnot distinguish positive and negative samples in thein-domain dataset, so the surfaced sentences fromtapt may be less relevant.
our approach beneﬁtsfrom using an encoding method that is trained onthe task we are targeting..after selecting data, we pre-train a bert-context model on the targeted dataset, pulling inneighboring sentences of the targeted sentences.
as auxiliary tasks, we used masked language mod-eling (mlm) and a sentence switching task (wanget al., 2019).
for mlm, we mask tokens in the con-text sentence only, independently with probability0.15. for sentence switching, with probability 0.25we swap the focus sentence with another randomlychosen sentence from the same document, and pre-dict whether the focus sentence was swapped usingthe context sentences.
cross entropy losses for bothtasks are computed and summed to compute the to-tal loss for an instance.
these tasks encourage themodel to learn how to incorporate information fromthe context sentences into its representation.
fig-ure 1 depicts the entire process.
this process canbe repeated, by using the ﬁnal resulting model tothen select a new set of sentences for pre-training,however we did not experiment with this as one it-eration was enough to produce competitive results..1370model.
bag-of-words+tfidfcnnbertmimic-full-bertmimic-dnote-bert.
bert+contextmimic-full-bert+contextmimic-dnote-bert+contextttp-bert+context (2m)ttp-bert+context (1m)ttp-bert+context (500k)ttp-bert+context (250k).
micro.
auc.
macro.
auc.
f1.
0.7090.723 (0.010)0.758 (0.008)0.765 (0.005)0.767 (0.004).
0.791 (0.007)0.794 (0.008)0.796 (0.012)0.809 (0.006)0.802 (0.004)0.803 (0.005)0.807 (0.009).
0.9580.964 (0.003)0.962 (0.006)0.971 (0.002)0.972 (0.006).
0.947 (0.011)0.954 (0.010)0.958 (0.015)0.957 (0.008)0.959 (0.010)0.953 (0.016)0.962 (0.010).
f1.
0.5120.540 (0.013)0.593 (0.028)0.624 (0.016)0.631 (0.018).
0.635 (0.013)0.641 (0.031)0.661 (0.025)0.660 (0.013)0.654 (0.012)0.671 (0.017)0.668 (0.028).
0.9370.962 (0.002)0.963 (0.003)0.966 (0.003)0.967 (0.002).
0.971 (0.003)0.972 (0.003)0.977 (0.003)0.973 (0.004)0.974 (0.003)0.976 (0.004)0.975 (0.002).
binary.
f1.
0.7830.810 (0.008)0.827 (0.006)0.832 (0.004)0.834 (0.004).
0.856 (0.007)0.857 (0.003)0.856 (0.008)0.865 (0.003)0.857 (0.006)0.862 (0.005)0.866 (0.007).
table 4: experiment results on the clip test set.
we report results as an average of at least 10 runs with varyingrandom seeds, with standard deviations in parentheses.
models using context sentences are listed in decreasingorder of amount of pre-training data used..5 evaluation.
5.1 data preparation and model training.
we ﬁrst generate synthetic surrogates for entitiesredacted during de-identiﬁcation, apply a customsentence tokenizer adapted from open-source soft-ware 4 5 to tokenize the document into sentences,and lower case every sentence.
discharge notes inmimic often have semi-structured sections, withheaders denoting them, e.g.
brief hospitalcourse:, which the tokenizer is built to identify.
using ttp, we select pre-training datasets ofsizes ∼250k, ∼500k, ∼1m, and ∼2m sentencesfrom the set of mimic-iii discharge notes..as baselines, we train a tf-idf-weighted bag-of-words logistic regression model with l1 reg-ularization and a max-pooling 1-d convolutionalneural network (cnn).
the cnn is initialized withbiowordvec vectors (zhang et al., 2019; chenet al., 2019), which are trained on pubmed andmimic-iii notes, and the cnn is trained with thebinary cross-entropy (bce) loss..all bert-based models are loaded, pre-trainedas appropriate, and ﬁne-tuned using the transform-ers library (wolf et al., 2019), using bce loss,and backpropagating and applying gradient updatesthrough all of bert’s parameters.
we used librarydefault parameters, except for the batch size whichwe adjusted to 32 based on validation set perfor-mance and training stability.
all neural models aretrained with early stopping on the macro-averaged.
auroc metric.
early stopping is also applied tothe pre-training step, using the loss on an unlabeledheld-out set as the criterion..5.2 reported metrics.
we report results on the test set using micro- andmacro-averaged metrics common in multilabel clas-siﬁcation, and f1 for the binary reduction of thetask.
micro-averaged metrics treat each (sentence,label) pair as an individual binary prediction, andmacro-averaged metrics compute the metric per-label and then average these results across labels.
for binary f1, we transform the label and modelpredictions into binary variables indicating whetherany type of label was predicted for the sentence,and then calculate metrics, ignoring whether thetypes of the predicted labels were accurate..5.3 choosing prediction thresholds.
to ensure the fairest comparison between mod-els and eliminate some arbitrariness in results thatmay arise when training on imbalanced data andevaluating with a ﬁxed 0.5 threshold, we also tunethresholds for each label such that its f1 score onthe validation set is maximized.
for micro metrics,we choose the threshold that provides the highestmicro f1 score.
we then apply these thresholdswhen evaluating on the test set..6 results.
4https://github.com/fnl/syntok5https://github.com/wboag/mimic-tokenize.
the main set of results are reported in table 4.models pre-trained with ttp have the size of their.
1371model.
patient appt medication lab.
procedure.
imaging other.
bag-of-wordscnnbertmimic-dnote-bert.
mimic-dnote-bert+contextttp-bert+context (250k).
0.7410.7590.7800.783.
0.8300.841.
0.7920.8240.8550.854.
0.8820.887.
0.5460.5950.6350.656.
0.6590.668.
0.6250.6290.7190.741.
0.7440.745.
0.3020.3150.4150.524.
0.5970.548.
0.3430.4310.4740.567.
0.5670.566.
0.2360.2280.2750.294.
0.3490.365.table 5: average balanced f1 scores on the test set for each label across 10 runs..pre-training dataset denoted in parentheses.
bertand both mimic-bert models outperform thelogistic regression and cnn baselines.
the re-sults using mimic-dnote-bert demonstrate theimportance of domain-speciﬁc pre-training; it im-proves in all metrics over bert.
using neighboringsentences, as we do in “-context” models, also pro-vides a performance boost across all metrics savefor macro auc, comparing mimic-dnote-bertto mimic-dnote-bert+context.
to comparewith human performance, our inter-annotator agree-ment on the binary task, measured in terms of f-1,was 0.930, and the highest mean binary-f1 fromthe model evaluations approaches 0.86..when using just 250,000 sentences from themimic discharge notes for pre-training (ttp-bert-context 250k), task results are competitivewith and in some cases exceed mimic-dnote-bert+context, which is pre-trained on all mimicdischarge notes, which contain 9m sentences.
ourttp approach is able to complete domain-speciﬁcpre-training within ∼12 hours, while alsentzeret al.
(2019) report a pre-training time of 17-18days for mimic-full-bert..we next investigate results on each label (seetable 5), for a subset of models.
the in-domainpre-training for mimic-dnote-bert models pro-vides gains for nearly all label types, and includingcontext also gives a boost to the f1 score of mostlabels.
all models perform poorly predicting the“other” label, which encompasses a long tail ofmany different types of follow-ups which we didnot further categorize, making modeling difﬁcult.
imaging and procedure label performance lags oth-ers, likely due to their lower prevalence (table 2)..6.1 error analysis.
we examine errors made by ttp-bert-context(1m), focusing on false negatives, the most costlytype of error in this use case.
inspection of the testset with physician input yields two high-level phe-.
nomena of the data that occur repeatedly in errorcases: clinical jargon / knowledge, and temporalexpressions / conditional language..jargon perhaps.
clinicalthe most obviousdrawback of applying general-purpose languagemodels to clinical language data is that clinicallanguage is heavily laden with clinical jargon,abbreviations, and misspellings.
although thewordpiece tokenization used by bert-basedmodels can tokenize any input, the more separationof clinical terms happens, the more model capacityis reduced, as lower layers in bert have to learnhow to combine the meaning of the wordpiecesinto word-level representations.
we observedseveral cases in which even common clinicaljargon may have interfered with the model’sperformancein an otherwise unambiguoussentence.
bolded words are oov’s: <-pleasetake medications as directed-follow up with pcp mark carterusing>,xray pa/lat and lordotic view toreevaluate when returns 12-18 forwound check>..<plan for repeat chest.
3 ..0.5 tablet po.
many cases of this type of error also suggest thata lack of explicit clinical knowledge could be a bar-rier, in addition to the technical issue of wordpiecetokenization.
in this example promethazineis a drug that can be prescribed for a shortpromethazine 25deﬁned period:mg tablet sig :q6h ( every 6 hours ) as neededfor nausea .
in the following example, theprocedures described are required but do notneed an appointment, and the model erroneouslylabel: however ,applied the appointmentthe patient will need aggressivepulmonary toilet including goodoral suctioning care and chest ptas pt is at risk for aspiration ..1372temporal expressions the model may alsostruggle with temporal expressions, which arecommon especially in the “medication” labeltype.
this label is intended to surface cases ofmedications that need to be tweaked, started,or stopped after a speciﬁed time period.
exam-ple: ...you should go back to yourregular home dosing of 20 unitsin the morning and 24 units atdinner time after completing yourprednisone .
while many training examplesgave explicit durations (e.g.
“for 14 days”), manyof the false negative examples described dependen-cies between future patient actions, including withconditional “if” statements.
example: if heneeds further management he maydo well with clonidine ..7 discussion.
our results show that the common regime of ﬁne-tuning a large pre-trained model is a useful methodfor our task of extracting clinical action items.
ad-ditionally, we investigated the trade-off betweentask-speciﬁcity and pre-training data size, andfound our task-targeted pre-training method en-ables one to navigate this trade-off, producing mod-els with comparable performance on the end taskthat require less data for pre-training.
while tradingoff these concerns may not be needed if effectivepublic models exist for a given task, we believethis technique is useful in scenarios in which usershave large, domain-speciﬁc, private datasets andspeciﬁc tasks in mind.
this is often the case forhealthcare institutions and developers of clinicalmachine learning software, as privacy concernstend to preclude data sharing between institutions.
from a modeling perspective, there are manypossible avenues for future work.
taking a struc-tured prediction lens and leveraging sentence-levellabel dependencies or applying structured predic-tion models could be helpful, although cohan et al.
(2019) note that crf layers did not improve theirperformance for a sequential sentence classiﬁca-tion task.
we acknowledge that our sentence clas-siﬁcation approach is a simpliﬁcation of the moregeneral span detection problem, and this approachcould bring improved precision by focusing onwhich parts of sentences matter, which may beimportant as we found that sentence tokenizationwas non-trivial for clinical notes..finally, the question of whether such an ap-.
proach to follow-up workﬂow augmentation is suc-cessful in increasing patient safety, clinician efﬁ-ciency, or ehr usability is an empirical one.
wehope to evaluate in the future whether a highlightednote such as one these models could provide willreduce the time a physician takes to, for exam-ple, answer certain questions about a patient’s hos-pital stay.
in alignment with recent calls for in-creased rigor in the evaluation of machine learning-derived clinical decision support systems (kellyet al., 2019), future work should include furtherprospective, controlled evaluation of the general-izability, stability, interpretability, unbiasedness,usability, and efﬁcacy of this approach.
we hopethat our dataset and initial model development canlay the groundwork for future investigation..8 conclusion.
we introduce the task of detecting clinical ac-tion items from discharge notes to help primarycare physicians more quickly and comprehensivelyidentify actionable information, and present theclip dataset, which we will release to the com-munity.
given perfect performance, this wouldreduce the number of sentences a pcp may needto read by 88%.
the best model’s binary f1 isnear 0.9, compared to the human benchmark of0.93. these models could additionally be used forclinical research.
for example, a calibrated modelcould derive statistics for how often each type ofaction item is seen for different patient populations,which can provide insight into typical patient orpcp burden after hospital discharge..we evaluated bert-based models that incor-porate multi-sentence context, and introduced anovel task-targeted pre-training approach that canreduce pre-training time while maintaining similarperformance to models pre-trained on much largerin-domain datasets.
the models have promisingresults, however we anticipate there is still roomfor improvement, particularly for the rare labels..we encourage the clinical nlp community tofurther investigate the problem of detecting actionitems from hospital discharge notes, which canhelp improve reliably safe transitions of care forthe most vulnerable patients..acknowledgements.
we thank our team of physician annotators for theirfruitful collaboration and the reviewers for theircomments which improved this paper..1373references.
emily alsentzer, john murphy, william boag, wei-hung weng, di jindi, tristan naumann, andmatthew mcdermott.
2019. publicly available clini-in proceedings of the 2ndcal bert embeddings.
clinical natural language processing workshop,pages 72–78, minneapolis, minnesota, usa.
asso-ciation for computational linguistics..richard j. baron.
2010. what’s keeping us so busy inprimary care?
a snapshot from one practice.
thenew england journal of medicine, 362 17:1632–6..q. chen, y. peng, and z. lu.
2019. biosentvec: cre-ating sentence embeddings for biomedical texts.
inproceedings of the 2019 ieee international confer-ence on healthcare informatics (ichi), pages 1–5..arman cohan, iz beltagy, daniel king, bhavana dalvi,and daniel s weld.
2019. pretrained language mod-in pro-els for sequential sentence classiﬁcation.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 3684–3690..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-in proceedings of the 2019 north americaning.
chapter of the association for computational lin-guistics: human language technologies (naacl-hlt), pages 4171–4186..oladimeji farri, karen a. monsen, serguei v. s.pakhomov, david s. pieczkiewicz, stuart m.speedie, and genevieve b. melton.
2013. effectsof time constraints on clinician-computer interac-tion: a study on information synthesis from ehr clin-ical notes.
journal of biomedical informatics, 466:1136–44..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages8342–8360..hiroaki hayashi, prashant budania, peng wang, chrisackerson, raj neervannan, and graham neubig.
2020. wikiasp: a dataset for multi-domain aspect-based summarization.
transactions of the associa-tion for computational linguistics (tacl)..sam henry, kevin buchan, michele filannino, am-ber stubbs, and ozlem uzuner.
2020a.
2018 n2c2shared task on adverse drug events and medica-tion extraction in electronic health records.
jour-nal of the american medical informatics associa-tion, 27(1):3–12..sam henry, yanshan wang, feichen shen, and ozlemuzuner.
2020b.
the 2019 national natural language.
processing (nlp) clinical challenges (n2c2)/openhealth nlp (ohnlp) shared task on clinical con-journalcept normalization for clinical records.
of the american medical informatics association,27(10):1529–1537..km hermann, t koˇcisk`y, e grefenstette, l espeholt,w kay, m suleyman, and p blunsom.
2015. teach-ing machines to read and comprehend.
advances inneural information processing systems, 28..carlos t. jackson, mohammad shahsahebi, tiffanywedlake, and c annette dubard.
2015. timeli-ness of outpatient follow-up: an evidence-based ap-proach for planning after hospital discharge.
annalsof family medicine, 13 2:115–22..abhyuday jagannatha, feifan liu, weisong liu, andhong yu.
2019. overview of the ﬁrst natural lan-guage processing challenge for extracting medica-tion, indication, and adverse drug events from elec-tronic health record notes (made 1.0).
drug safety,42:99–111..alistair e. w. johnson, tom j. pollard, lu shen, li weih. lehman, mengling feng, mohammad m. ghas-semi, benjamin moody, peter szolovits, leo an-thony celi, and roger g. mark.
2016. mimic-iii, afreely accessible critical care database.
in scientiﬁcdata..alistair ew johnson, lucas bulgarelli, and tom j pol-lard.
2020. deidentiﬁcation of free-text medicalrecords using pre-trained bidirectional transformers.
in proceedings of 2020 the association for comput-ing machinery (acm) conference on health, infer-ence, and learning (chil), pages 214–221..christopher j. kelly, alan karthikesalingam, mustafasuleyman, greg corrado, and dominic king.
2019.key challenges for delivering clinical impact withartiﬁcial intelligence.
in bmc medicine..sunil kripalani, frank lefevre, christopher o.phillips, mark v. williams, preetha basaviah, anddavid w. baker.
2007. deﬁcits in communicationand information transfer between hospital-basedand primary care physicians implications for pa-tient safety and continuity of care.
journal of theamerican medical informatics association (jamia),297(8):831–841..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2020. biobert: a pre-trainedbiomedicalforbiomedical text mining.
bioinformatics (oxford,england), 36(4)..language representation model.
jennifer liang, ching-huei tsou, and ananya poddar.
2019. a novel system for extractive clinical notesummarization using ehr data.
in proceedings ofthe 2nd clinical natural language processing work-shop, pages 46–54, minneapolis, minnesota, usa.
association for computational linguistics..1374sijia liu, majid rastegar mojarad, yanshan wang, li-wei wang, feichen shen, sunyang fu, and hong-fang liu.
2018a.
overview of the biocreative/ohnlp2018 family history extraction task.
in proceedingsof the biocreative 2018 workshop, page 2018..carin.
2020. students need more attention: bert-based attention model for small data with applica-tion to automatic patient message triage.
in proceed-ings of machine learning research, volume 126.pmlr..xiangan liu, keyang xu, pengtao xie, and eric p.xing.
2018b.
unsupervised pseudo-labeling for ex-tractive summarization on electronic health records.
corr, abs/1811.08040..yang liu and mirella lapata.
2019. text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3721–3731..matthew mcdermott, tzu ming harry hsu, wei-hungweng, marzyeh ghassemi, and peter szolovits.
2020. chexpert++: approximating the chexpert la-beler for speed, differentiability, and probabilisticin proceedings of machine learning re-output.
search, volume 126. pmlr..carlton moore, thomas mcginn, and ethan a. halm.
2007. tying up loose ends: discharging patientswith unresolved medical issues.
archives of internalmedicine, 167 12:1305–11..james mullenbach, yada pruksachatkun, sean adler,jennifer seale, jordan swartz, t. greg mckelvey,yi yang, and david sontag.
2021. clip: a datasetfor extracting action items for physicians from hos-pital discharge notes (version 1.0.0).
physionet..andriy mulyar,.
schumacher, masoudelliotrouhizadeh, and mark dredze.
2019.pheno-typing of clinical notes with improved documentclassiﬁcation models using contextualized neurallanguage models.
arxiv, abs/1910.13664..ramesh nallapati, bowen zhou, cicero dos santos,c¸ a˘glar gulc¸ehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, pages 280–290..anusri pampari, preethi raghavan, jennifer liang, andjian peng.
2018. emrqa: a large corpus for questionin pro-answering on electronic medical records.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 2357–2368..alexander ratner, stephen h bach, henry ehrenberg,jason fries, sen wu, and christopher r´e.
2017.snorkel: rapid training data creation with weak su-pervision.
in proceedings of the vldb endowment.
international conference on very large data bases,volume 11, page 269. nih public access..shijing si, rui wang, jedrek wosik, hao zhang, daviddov, guoyin wang, ricardo henao, and lawrence.
hardeep singh, christiane spitzmueller, nancy j. pe-tersen, mona k. sawhney, and dean f. sittig.
2013.information overload and missed test results in elec-tronic health record-based settings.
jama internalmedicine, 173 8:702–4..christine sinsky, lacey colligan, ling li, mirelaprgomet, sam reynolds, lindsey goeders, johannawestbrook, michael tutty, and george blike.
2016.allocation of physician time in ambulatory prac-tice: a time and motion study in 4 specialties.
an-nals of internal medicine, 165(11):753–760..rachel a spencer,.
2019..sarah rodgers, ndeshisalema, stephen m campbell, and anthony javery.
sum-maries in general practice:a qualitative inter-view study with gps and practice managers.
british journal of general practice(bjgp),https://doi.org/10.3399/bjgpopen18x101625..processing.
discharge.
alane suhr, mike lewis, james yeh, and yoav artzi.
2017. a corpus of natural language for visual rea-in proceedings of the 55th annual meet-soning.
ing of the association for computational linguistics(volume 2: short papers), pages 217–223..weiyi sun, anna rumshisky, and ¨ozlem uzuner.
2013.evaluating temporal relations in clinical text: 2012i2b2 challenge.
journal of the american medicalinformatics association (jamia), 20 5:806–13..m tai-seale, cw olson, li j, chan as, morikawa c,durbin m, wang w, and luft hs.
2017. electronichealth record logs indicate that physicians splittime evenly between seeing patients and desktopmedicine.
health aff (millwood), 1(35):655–662..ian tenney, dipanjan das, and ellie pavlick.
2019.in pro-bert rediscovers the classical nlp pipeline.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601..¨ozlem uzuner, andreea bodnari, shuying shen, tylerforbush, john pestian, and brett r. south.
2012.evaluating the state of the art in coreference reso-lution for electronic medical records.
journal of theamerican medical informatics association (jamia),19 5:786–91..¨ozlem uzuner, imre solti, and eithon cadag.
2010.extracting medication information from clinical text.
journal of the american medical informatics asso-ciation (jamia), 17 5:514–8..¨ozlem uzuner, brett r. south, shuying shen, andscott l. duvall.
2011.
2010 i2b2/va challenge onconcepts, assertions, and relations in clinical text.
journal of the american medical informatics asso-ciation (jamia), 18 5:552–6..1375hong wang, xin wang, wenhan xiong, mo yu, xi-aoxiao guo, shiyu chang, and william yang wang.
2019. self-supervised learning for contextualizedin proceedings of theextractive summarization.
57th annual meeting of the association for compu-tational linguistics, pages 2221–2227..justin m weis and paul c levy.
2014. copy, paste,and cloned notes in electronic health records.
chest,145(3):632–638..martin c. were, sergey gorbachev, jason cadwallader,joe kesterson, xiaochun li, j. marc overhage, andjeff friedlin.
2010. natural language processing toextract follow-up provider information from hospi-tal discharge summaries.
amia annual symposiumproceedings, 2010:872–6..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2019.huggingface’s transformers: state-of-the-art naturallanguage processing.
arxiv, abs/1910.03771..haoran zhang, amy x lu, mohamed abdalla,matthew mcdermott, and marzyeh ghassemi.
2020.hurtful words: quantifying biases in clinical con-in proceedings of thetextual word embeddings.
acm conference on health, inference, and learn-ing, pages 110–120..yijia zhang, qingyu chen, zhihao yang, hongfei lin,and zhiyong lu.
2019. biowordvec, improvingbiomedical word embeddings with subword infor-mation and mesh.
in scientiﬁc data..1376were commonly in the “other” category, which en-compasses a long tail of information - we capturethese in the guidelines with a non-comprehensiveset of examples demonstrating both labeled andunlabeled cases..the patient instructions label originallyinstructed annotators to choose only those instruc-tions that are unique to that patient, and excludegeneral guidelines such as “call your doctor ifyou experience a fever.” however, we observedthis was too ambiguous in practice, so we choseto automatically label any sentence in documentsections “followup instructions” and “dischargeinstructions” as the patient instructions label,using regular expressions to identify common sec-tion headers in the mimic-iii discharge notes.
ourannotators provided additional manual annotations,so not all examples of this type come from theserule-derived annotations; any annotations with the“patient instructions” label which appear outsideof the “followup instructions” and “discharge in-structions” sections were manually annotated..finally, two of the original annotators revised allexisting annotations, to catch mistakes and adjustto the reﬁned guidelines..c clip dataset phenomena details.
in table 6 we provide a breakdown of high-levelphenomena in the clip dataset by label type.
wesampled sentences randomly, ensuring each labeltype had at least 20 examples..a additional data processing details.
when a focus sentence is near the start or endof a document, we use special <doc start>and <doc end> tokens in place of sentences,as needed when the limits of the document arereached.
because bert takes a maximum lengthof 512 tokens, and due to occasional long sentencesin mimic, when including context we may haveto truncate our input, which occurs for just un-der 1% of sentences.
to do this, we ﬁrst removethe shorter of the two outermost context sentences,then remove other context sentences as needed, al-ternating sides and moving inward.
finally, thetokenized input along with position and segmentembeddings is fed into the transformer layers to ob-tain contextualized representations for each token..b additional annotation details.
we built an internal annotation tool, which allowedannotators to select and label arbitrary character-level spans of text within the document.
thesecharacter-level spans were later converted into thesentence annotations..since mimic-iii is an anonymized dataset, enti-ties such as names, dates, phone numbers, hospitalnames, and others censored and replaced with atemplated substitute.
we apply a surrogate gener-ation process to ﬁll in synthetic entities in placeof these templates, to make reading and annotat-ing notes easier.
these surrogates are also usedat prediction time.
due to space constraints, thefull guidelines are provided on our public githubrepository 6..b.1 annotation reﬁnement.
after collecting initial annotations, we met withthe annotators in multiple sessions to reconcile dif-ferences in their annotations.
we adjusted the an-notation guidelines slightly to reduce ambiguityand improve labeling consistency.
using an initialset of examples that were annotated by multipleexperts, we identiﬁed examples where labels dis-agreed across annotators.
in the reconciliation pro-cess, we discussed those disagreements as a groupto determine whether (a) one annotator misappliedor forgot to apply an annotation guideline, or (b)the proper annotation was ambiguous given theguidelines at that time.
in the case of ambiguousguidelines, we would then add a new rule or exam-ple to the guidelines.
the sources of disagreement.
6https://github.com/asappresearch/clip.
1377label.
n.# oov quantities temporal conditional.
imperative.
imagingappointmentmedicationprocedurelabpatientotherall.
20472524237821160.
1.702.022.721.711.871.632.332.00.
0.400.190.640.210.260.180.000.25.
0.700.620.520.500.440.500.290.48.
0.050.110.040.170.090.140.100.11.
0.450.320.160.330.390.580.240.38.table 6: observed phenomena for a random selection of each label type.
# oov is an average across sentences,while the other measures are fractions..1378