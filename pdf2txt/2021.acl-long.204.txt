stacked acoustic-and-textual encoding: integrating the pre-trainedmodels into speech translation encoders.
chen xu1, bojie hu2, yanyang li3, yuhao zhang1,shen huang2, qi ju2∗ , tong xiao1,4∗, jingbo zhu1,41nlp lab, school of computer science and engineering,northeastern university, shenyang, china2tencent minority-mandarin translation, beijing, china3the chinese university of hong kong, hong kong, china4niutrans research, shenyang, china{xuchenneu,blamedrlee,yoohaozhang}@outlook.com,{bojiehu,springhuang,damonju}@tencent.com,{xiaotong,zhujingbo}@mail.neu.edu.cn.
abstract.
encoder pre-training is promising in end-to-end speech translation (st), given the factthat speech-to-translation data is scarce.
butst encoders are not simple instances of au-tomatic speech recognition (asr) or ma-chine translation (mt) encoders.
for exam-ple, we ﬁnd that asr encoders lack the globalcontext representation, which is necessary fortranslation, whereas mt encoders are not de-signed to deal with long but locally attentiveacoustic sequences.
in this work, we pro-pose a stacked acoustic-and-textual encoding(sate) method for speech translation.
our en-coder begins with processing the acoustic se-quence as usual, but later behaves more likean mt encoder for a global representation ofthe input sequence.
in this way, it is straight-forward to incorporate the pre-trained modelsinto the system.
also, we develop an adaptormodule to alleviate the representation incon-sistency between the pre-trained asr encoderand mt encoder, and develop a multi-teacherknowledge distillation method to preserve thepre-training knowledge.
experimental resultson the librispeech en-fr and must-c en-de st tasks show that our method achievesstate-of-the-art bleu scores of 18.3 and 25.2.to our knowledge, we are the ﬁrst to developan end-to-end st system that achieves compa-rable or even better bleu performance thanthe cascaded st counterpart when large-scaleasr and mt data is available1..1.introduction.
end-to-end speech translation (e2e st) has be-come popular recently for its ability to free design-ers from cascading different systems and shorten.
∗corresponding author1the source code is available at https://github.com/xuchen.
neu/sate.
setting.
model.
bleu.
restricted.
unrestricted.
cascadede2e+pre-training.
cascadede2e+pre-training.
23.323.1.
28.125.6.table 1: bleu scores [%] of a cascaded st model andan end-to-end st model with pre-training on the must-c en-de corpus.
restricted = training is restricted tothe st data, and unrestricted = additional training datais allowed for asr and mt..the pipeline of translation (duong et al., 2016; be-rard et al., 2016; weiss et al., 2017).
promisingresults on small-scale tasks are generally favor-able.
however, speech-to-translation paired data isscarce.
researchers typically use pre-trained au-tomatic speech recognition (asr) and machinetranslation (mt) models to boost st systems (be-rard et al., 2018).
for example, one can initializethe st encoder using a large-scale asr model(bansal et al., 2019).
but we note that, despitesigniﬁcant development effort, our end-to-end stsystem with pre-trained models was not able tooutperform the cascaded st counterpart when theasr and mt data size was orders of magnitudelarger than that of st (see table 1)..in this paper, we explore reasons why pre-training has been challenging in st, and how pre-trained asr and mt models might be used to-gether to improve st. we ﬁnd that the st encoderplays both roles of acoustic encoding and textualencoding.
this makes it problematic to view anst encoder as either an individual asr encoder oran individual mt encoder.
more speciﬁcally, thereare two problems..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2619–2630august1–6,2021.©2021associationforcomputationallinguistics2619• modeling deﬁciency: the mt encoder tries tocapture long-distance dependency structuresof language, but the asr encoder focusesmore on local dependencies in the input se-quence.
since the st encoder is initializedby the pre-trained asr encoder (berard et al.,2018), it fails to model large contexts in theutterance.
but a large scope of representa-tion learning is necessary for translation (yanget al., 2018)..• representation inconsistency: on the decoderside of st, the mt decoder is in general usedto initialize the model.
the assumption hereis that the upstream component is an mt-likeencoder, whereas the st encoder actually be-haves more like an asr encoder..we address these problems by marrying theworld of asr encoding with the world of mtencoding.
we propose a stacked acoustic-and-textual encoding (sate) method to cascade theasr encoder and the mt encoder.
it ﬁrst readsand processes the sequence of acoustic featuresas a usual asr encoder.
then an adaptor mod-ule passes the acoustic encoding output to an mtencoder with two principles: informative and adap-tive.
in this way, pre-trained asr and mt encoderscan work for what we would originally design them,and the incorporation of pre-trained models into stis more straightforward.
in addition, we developa multi-teacher knowledge distillation method torobustly train the st encoder and preserve the pre-trained knowledge during ﬁne-tuning (yang et al.,2020)..we test our method in a transformer-based end-to-end st system.
experimental results on the lib-rispeech en-fr and must-c en-de speech transla-tion benchmarks show that it achieves the state-of-the-art performance of 18.3 and 25.2 bleupoints.
under a more challenging setup, where thelarge-scale asr and mt data is available, sateachieves comparable or even better performancethan the cascaded st counterpart.
we believe thatwe are the ﬁrst to present an end-to-end system thatcan beat the strong cascaded system in unrestrictedspeech translation tasks..2 related work.
speech translation aims at learning models that canpredict, given some speech in the source language,the translation into the target language.
the earliest.
of these models were cascaded: they treated stas a pipeline of running an asr system and anmt system sequentially (ney, 1999; mathias andbyrne, 2006; schultz et al., 2004).
this allowsthe use of off-the-shelf models, and was (and is)popular in practical st systems.
however, thesesystems were sensitive to the errors introduced bydifferent component systems and the high latencyof the long pipeline..as another stream in the st area, end-to-endmethods have been promising recently (berardet al., 2016; weiss et al., 2017; berard et al., 2018).
the rise of end-to-end st can be traced back to thesuccess of deep neural models (duong et al., 2016).
but, unlike other well-deﬁned tasks in deep learn-ing, annotated speech-to-translation data is scarce,which prevents well-trained st models.
a simplesolution to this issue is data augmentation (pinoet al., 2019, 2020).
this method is model-freebut generating large-scale synthetic data is timeconsuming.
as an alternative, researchers usedmulti-task learning (mtl) to robustly train the stmodel so that it could beneﬁt from additional guidesignals (weiss et al., 2017; anastasopoulos andchiang, 2018; berard et al., 2018; sperber et al.,2019; dong et al., 2021).
generally, mtl requiresa careful design of the loss functions and morecomplicated architectures..in a similar way, more recent work pre-trainsdifferent components of the st system, and consol-idates them into one.
for example, one can initial-ize the encoder with an asr model, and initializethe decoder with the target-language side of an mtmodel (berard et al., 2018; bansal et al., 2019;stoian et al., 2020).
more sophisticated methodsinclude better training and ﬁne-tuning (wang et al.,2020a,b), the shrink mechanism (liu et al., 2020),the adversarial regularizer (alinejad and sarkar,2020), and etc.
although pre-trained models havequickly become dominant in many nlp tasks, theyare still found to underperform the cascaded modelin st. this motivates us to explore the reasons whythis happens and methods to solve the problemsaccordingly..3 why is st encoding difﬁcult?.
following previous work in end-to-end models (be-rard et al., 2016; weiss et al., 2017), we envisionan encoding-decoding process in which an input se-quence is encoded into a representation vector, andthe vector is then decoded into an output sequence..2620st.asr.
mt.
below ctc.
above ctc.
st.asr.
ssenlacol.0.80.
0.60.
0.40.
0.20.ssenlacol.0.60.
0.55.
0.50.
0.45.
22.0.
21.5.
21.0.
20.5.
20.0.)
ts(uelb.
13.5.
13.0.
12.5.
12.0.
11.5.
).
(.
rsarew.0.
4.
8.
12.
0.
4.
8.
12.
0.
4.
8.
12.layer.
ctc position.
ctc position.
(a) localness analysis.
(b) ctc impact on localness.
(c) ctc impact on performance.
figure 1: (a) localness in each layer of the st, asr, and mt encoders, (b) the impact of ctc position onlocalness, and (c) the impact of ctc position on performance of st and asr models..in such a scenario, all end-to-end st, asr and mtsystems can be viewed as instances of the samearchitecture.
then, components of these systemscan be pre-trained and re-used across them..more global distribution of attention weights forword sequences, especially when we stack morelayers.
this result arises a new question: is localattention sufﬁcient for speech translation?.
an underlying assumption here is that the st en-coder is doing something quite similar to what themt (or asr) encoder is doing.
however, sperberet al.
(2018) ﬁnd that the asr model beneﬁts froma small attention window, which is inconsistentwith the mt model (yang et al., 2018).
to ver-ify this, we compare the behavior of st, asr andmt encoders.
we choose transformer as the basearchitecture (vaswani et al., 2017) and run experi-ments on the must-c en-de corpus.
we report theresults on the must-c en-de tst-common testdata.
for stronger systems, we use connectionisttemporal classiﬁcation (ctc) (graves et al., 2006)as the auxiliary loss on the encoders when we trainthe asr and st systems (watanabe et al., 2017;karita et al., 2019; bahar et al., 2019).
the ctcloss forces the encoders to learn alignments be-tween speech and transcription.
it is necessary forthe state-of-the-art performance (watanabe et al.,2018)..here we deﬁne the localness of a word as thesum of the attention weights to the surroundingwords (or features) within a ﬁxed small window2.
the window size is 10% of the sequence length.
figure 1(a) shows the localness of the attentionweights for different layers of the encoders.
wesee that the st and asr encoders prefer local at-tention which indicates a kind of short-distancedependencies in processing acoustics feature se-quences.
whereas the mt encoder generates a.
2here we treat the attention weight of transformer as a.distribution over all positions..then, we design another experiment to examineif the high localness in attention weights of theasr and st encoders is due to the bias imposedby ctc.
in figure 1(b), we use the ctc loss in theintermediate layer and show the average localnessof the layers above or below ctc.
the ctc lossdemonstrates strong preference for locally attentivemodels.
the upper-level layers act more like anmt encoder, that is, the layers with no ctc lossgenerates more global distributions.
taking thisfurther, figure 1(c) demonstrates a slightly higherbleu score when we free more upper-level layersfrom the guide of ctc.
meanwhile, the word errorrate (wer) increases because only lower parts ofthe model are learned in a standard manner of asr.
now we have some hints: the st encoder is nota simple substitution of the asr encoder or themt encoder.
rather, they are complementary toeach other, that is, we need the asr encoder todeal with the acoustic input, and the mt encoderto generate the representation vector that can workbetter with the decoder..4 the method.
in speech translation, we want the encoder to rep-resent the input speech to some sort of decoder-friendly representations.
we also want the encoderto be “natural” for pre-training.
in the following,we describe, stacked acoustics-and-textual encod-ing (sate), a new st encoding method to meetthese requirements, and improvements of it..2621trans loss.
then, the ctc loss is deﬁned as:.
ctc loss.
softmax.
linear.
acousticencoder.
speechfeatures.
textualencoder.
adaptor.
decoder.
softmax.
linear.
targettext.
figure 2: the overall architecture of stacked acoustic-and-textual encoding..4.1 stacked acoustic-and-textual encoding.
unlike previous work, the sate method does notrely on a single encoder to receive the signal fromboth the ctc loss and the feedback of the decoder.
instead, it is composed of two encoders: the ﬁrstdoes exactly the same thing as the asr encoder(call it acoustic encoder), and the other generatesa higher-level globally-attentive representation ontop of the acoustic encoder (call it textual encoder).
see figure 2 for the architecture of sate.
theacoustic encoder is trained by ctc in additionto the supervision signal from the translation loss.
let (x, ys, yt) be an st training sample, where xis the input feature sequence of the speech, ys isthe transcription of x, and yt is the translation inthe target language.
we deﬁne the output of theacoustic encoder as:.
hs = es(x).
(1).
where es(·) is the encoding function.
then, weadd a softmax layer on hs to predict the ctc labelpath π = (π1, · · · , πt ), where t is the length ofthe input sequence.
the probability of path p(π|hs)is the product of the probability p(πt|hst ) at everytime t based on conditionally independent assump-tion:.
p(π|hs) ≈.
p(πt|hst ).
(2).
t(cid:89).
t.lctc = − log pctc(ys|hs; θctc).
(4).
where θctc is the model parameters of the acousticencoder and the ctc output layer..the acoustic encoder is followed by an adaptor.
it receives hs and p (π|hs), and produces a newrepresentation required by the textual encoder.
leta(·, ·) be the adaptor module.
its output is deﬁnedas:.
ˆhs = a(hs, p(π|hs)).
(5).
we leave the design of the adaptor to section 4.2.furthermore, we stack the textual encoder on theadaptor.
the output ht is deﬁned as:.
ht = et( ˆhs).
(6).
where et(·) is the textual encoder.
ht is fed intothe decoder for computing the translation probabil-ity ptrans(yt|ht), as in standard mt systems.
wedeﬁne the translation loss as:.
ltrans = − log ptrans(yt|ht; θst).
(7).
where θst is all model parameters except for thectc output layer..finally, we interpolate lctc and ltrans (with.
coefﬁcient α) for the loss of the entire model:.
l = α · lctc + (1 − α) · ltrans.
(8).
since the textual encoder works for the decoderonly, it is trained as an mt encoder.
in this way,the acoustic and textual encoders can do what wewould originally expect them to do: the acousticencoder deals with the acoustic input (i.e., asr en-coding), and the textual encoder generates a repre-sentation for translation (i.e., mt encoding).
also,sate is friendly to pre-training.
one can simplyuse an asr encoder as the acoustic encoder, anduse an mt encoder as the textual encoder.
notethat sate is in general a cascaded model, in re-sponse to the pioneering work in st (ney, 1999).
itcan be seen as cascading the asr and mt systemsin an end-to-end fashion..ctc works by summing over the probability ofall possible alignment paths φ(ys) between x andys , as follows:.
pctc(ys|hs) =.
p(π|hs).
(3).
(cid:88).
π∈φ(ys).
4.2 the adaptor.
now we turn to the design of the adaptor.
note thatthe pre-trained mt encoder assumes that the inputis a word embedding sequence.
simply stackingthe mt encoder and the asr encoder obviously.
2622does not work well.
for this reason, the adaptor ﬁtsthe output of the asr encoder (i.e., the acousticencoder) to what an mt encoder would like to see.
we follow two principles in designing the adaptor:adaptive and informative..we need an adaptive representation to make theinput of the textual encoder similar to that of themt encoder.
to this end, we generate the soft con-textual representation that shares the same latentspace with the embedding layer of the mt encoder.
as shown in eq.
(2), the ctc output p(πt|hst )indicates the alignment probability over the vocab-ulary at time t. instead of replacing the representa-tion by the embedding of the most-likely token (liuet al., 2020), we employ a soft token which is theexpectation of the embedding over the distributionfrom ctc.
let w e be the embedding matrix of thetextual encoder, we deﬁne the soft representationhssoft as:.
soft = p(π|hs) · w ehs.
(9).
also, an informative representation should con-tain information in the original input (peters et al.,2018).
the output acoustic representation of theasr encoder generally involves paralinguistic in-formation, such as emotion, accent, and emphasis.
they are not expressed in the form of text explicitlybut might be helpful for translation.
for example,the generation of the declarative or exclamatorysentences depends on the emotions of the speakers.
we introduce a single-layer neural network tolearn to map the acoustic representation to the la-tent space of the textual encoder, which preservesthe acoustic information:.
map = relu(w map · hs + bmap) (10)hs.
where w map and bmap are the trainable parameters.
the ﬁnal output of the adaptor is deﬁned to be:.
a(hs, p (π|hs)) = λ · hs.
map +(1 − λ) · hs.
soft.
(11).
where λ is the weight of hsmap and set to 0.5 bydefault.
figure 3 shows the architecture of theadaptor..note that, in the adaptor, we do not change thesequence length for textual encoding because sucha way is simple for implementation and shows satis-factory results in our experiments.
although thereis a length inconsistency issue, the sequence repre-sentation of the speech should be similar with the.
embedding.
soft embedding.
output.
w eyou.
i hi that.
can.
(cid:76).
×.
=.
· · ·.
· · ·.
mapping layer.
ctcdistribution.
acousticrepresentation.
figure 3: the architecture of the adaptor..correspond transcription.
shrinking the sequencesimply results in information incompleteness.
wewill investigate this issue in the future..4.3 multi-teacher knowledge distillation.
another improvement here is that we developa multi-teacher knowledge distillation (mtkd)method to preserve the pre-trained knowledge dur-ing ﬁne-tuning (hinton et al., 2015)..the st model mimics the teacher distributionby minimizing the cross-entropy loss between theteacher and student (liu et al., 2019).
for a trainingsample (x, ys, yt), we deﬁne two loss functions:.
lkd ctc = −.
q(πm = vk|x; θasr).
× log p(πm = vk|x; θctc) (12).
lkd trans = −.
q(yt.
n = vk|ys; θmt).
t(cid:88).
|v |(cid:88).
m=1.
k=1.
|yt|(cid:88).
|v |(cid:88).
n=1k=1× log p(yt.
n = vk|x; θst).
(13).
where vk is the word indexed by k and v is thevocabulary shared among the st, asr, and mtmodels.
q(·|·) is the teacher distribution and p(·|·)is the student distribution.
θasr, θctc, θmt andθst are the model parameters..we can rewrite eq.
(8) to obtain a new loss:.
l = α · (cid:0)β · lctc + (1 − β) · lkd ctc.
(cid:1).
+(1 − α) ·(cid:0)γ · ltrans + (1 − γ) · lkd trans.
(cid:1) (14).
where both β and γ are the hyper-parameters thatbalance the preference between the teacher distri-bution and the ground truth..26235 experiments.
5.2 model settings.
5.1 datasets and preprocessing.
we consider restricted and unrestricted settingson speech translation tasks.
we run experimentson the librispeech english-french (en-fr) (ko-cabiyikoglu et al., 2018) and must-c english-german (en-de) (gangi et al., 2019) corpora,which correspond to the low-resource and high-resource datasets respectively.
available asr andmt data is only from the st data under the re-stricted setting.
for comparison in practical scenar-ios, the unrestricted setting allows the additionaldata for asr and mt models..librispeech en-fr followed previous work, weuse the clean speech translation training set of 100hours, including 45k utterances and doubled trans-lations of google translate.
we select the modelon the dev set (1,071 utterances) and report resultson the test set (2,048 utterances)..must-c en-de must-c is a multilingual speechtranslation corpus extracted from the ted talks.
we run the experiments on the english-germanspeech translation dataset of 400 hours speech with230k utterances.
we select the model on the devset (1,408 utterances) and report results on the tst-common set (2,641 utterances)..unrestricted setting we use the additional asrand mt data for pre-training.
the 960 hours lib-rispeech asr corpus is used for the english asrmodel.
we extract 10m sentences pairs from thewmt14 english-french and 18m sentence pairsfrom the opensubtitle20183 english-german trans-lation datasets..preprocessing followed the preprocessing recipesof espnet (inaguma et al., 2020), we remove theutterances of more than 3,000 frames and augmentspeech data by speed perturbation with factors of0.9, 1.0, and 1.1. the 80-channel log-mel ﬁlterbankcoefﬁcients with 3-dimensional pitch features areextracted for speech data.
we use the lower-casedtranscriptions without punctuations.
the text istokenized using the scripts of moses (koehn et al.,2007).
we learn byte-pair encoding (sennrichet al., 2016) subword segmentation with 10,000merge operations based on a shared source andtarget vocabulary for all datasets..all experiments are implemented based on the es-pnet toolkit4.
we use the adam optimizer withβ1 = 0.9, β2 = 0.997 and adopt the default learn-ing schedule in espnet.
we apply dropout witha rate of 0.1 and label smoothing (cid:15)ls = 0.1 forregularization..for reducing the computational cost, the inputspeech features are processed by two convolutionallayers, which have a stride of 2 × 2 and down-sample the sequence by a factor of 4 (weiss et al.,2017).
the encoder consists of 12 layers for boththe asr and vanilla st models, and 6 layers forthe mt model.
the encoder of sate includes anacoustic encoder of 12 layers and a textual encoderof 6 layers.
the decoder consists of 6 layers forall models.
the weight of ctc objective α formultitask learning is set to 0.3 for all asr and stmodels.
the coefﬁcients β and γ are set to 0.5 ineq.
(14) for the mtkd method..under the restricted setting, we employ thetransformer architecture, where each layer com-prises 256 hidden units, 4 attention heads, and 2048feed-forward size.
for the unrestricted setting, weuse the superior architecture conformer (gulatiet al., 2020) on the asr and st tasks and widenthe model by increasing the hidden size to 512 andattention heads to 8. the asr5 and mt modelspre-train with the additional data and ﬁne-tune themodel parameters with the task-speciﬁc data..during inference, we average the model parame-ters on the best 5 checkpoints based on the perfor-mance of the development set.
we use beam searchwith a beam size of 4 for all models.
differentfrom previous work, we report the case-sensitivesacrebleu6 (post, 2018) for future standardiza-tion comparison across papers..5.3 results.
results on must-c en-de table 2 summariesthe experimental results on the must-c en-detask.
under the restricted setting, the cascadedst model translates the output of the asr model,which degrades the performance compared with themt model that translates from the reference tran-scription.
the performance of the e2e st baselinewith pre-training is only slightly lower than the cas-caded counterpart.
sate outperforms the baseline.
4https://github.com/espnet/espnet5we use the pre-trained asr model offered by espnet.
6bleu+case.mixed+numrefs.1+smooth.exp+tok.13a.
3http://opus.nlpl.eu/opensubtitles-v2018.php.
+version.1.4.14.
2624restricted unrestricted.
restricted unrestricted.
methodespnet mt∗espnet cascaded∗mtcascaded stespnet e2e st∗e2e st.+pre-training.
sate.
+pre-training+mtkd.
+specaug.
27.6323.6526.923.3.
22.3322.123.1.
23.324.124.725.2.
--31.128.1.
-23.625.6.
23.627.327.928.1.methodespnet mt∗espnet cascaded∗mtcascaded stespnet e2e st∗e2e st.+pre-training.
sate.
+pre-training+mtkd.
+specaug.
18.0916.9617.516.3.
16.2216.717.1.
17.617.417.718.3.
--21.320.6.
-17.720.0.
18.120.820.820.8.table 2: bleu scores [%] on the test set of must-cen-de corpus.
∗: results reported in the espnet toolkit..table 3: bleu scores [%] on the test set of lib-rispeech en-fr corpus.
∗: results reported in the es-pnet toolkit..model signiﬁcantly.
this demonstrates the superi-ority of stacked acoustic and textual encoding forthe speech translation task.
incorporating the pre-trained asr and mt models into sate releasesthe encoding burden of the model and achievesa remarkable improvement.
the mtkd methodprovides a strong supervised signal and forces themodel to preserve the pre-trained knowledge.
fur-thermore, we utilize the specaugment (park et al.,2019) which is applied in the input speech featuresfor better generalization and robustness7.
it yields aremarkable improvement of 1.9 bleu points overthe cascaded baseline and achieves a new state-of-the-art performance..under the unrestricted setting, the large-scaleasr and mt data is available, whereas the st datais scarce.
this leads to the cascaded method outper-forms the vanilla e2e method with a huge marginof 4.5 bleu points.
the pre-training only slightlycloses the gap due to the modeling deﬁciency andrepresentation inconsistency.
sate incorporatesthe pre-trained models fully, which achieves a sig-niﬁcant improvement of 3.7 bleu points.
withthe mtkd and specaugment methods, we achievea comparable performance of 28.1 bleu points.
to our knowledge, we are the ﬁrst to develop anend-to-end st system that achieves comparableperformance with the cascaded counterpart whenlarge-scale asr and mt data is available.
results on librispeech en-fr table 3 sum-maries the experimental results on the librispeechen-fr task.
different from the must-c corpus,.
7it is a fair comparison because the asr model in the.
cascaded st system also trains with the specaugment..it is of small magnitude with clean speech data.
this results in that the performance of the vanillae2e baseline is even better than the cascaded coun-terpart under the restricted setting.
furthermore,pre-training helps the model achieve an improve-ment of 0.8 bleu points over the cascaded base-line.
more interestingly, sate without pre-trainingoutperforms the above methods signiﬁcantly, evenachieves a slight improvement than the mt model.
a possible reason is that the diverse acoustic rep-resentation is fed to the textual encoder, whichimproves the robustness of the model.
this demon-strates the superiority of our method..combining our proposed methods yields a sub-stantial improvement of 2.0 bleu points over thecascaded baseline.
it is a new state-of-the-art resultof 18.3 bleu points.
also, we outperform thecascaded counterpart by 0.2 bleu points on theunrestricted task..6 analysis.
6.1 model performance vs. speedup.
in table 4, we summarize the performance andinference speedup based on the real time factor(rtf).
the vanilla e2e st model yields an infer-ence speedup of 1.91× than the cascaded coun-terpart and demonstrates the low latency of theend-to-end methods.
we increase the encoder lay-ers for comparison with sate under the similarmodel parameters.
however, there is a remarkablegap of 0.5 or 0.6 bleu points, with or withoutpre-training..2625method.
bleu rtf/speedup.
design.
must-c librispeech.
cascaded st.e2e st.+pre-training.
e2e st (enc 18)+pre-training.
sate.
+pre-training+all.
23.3.
22.123.1.
22.823.5.
23.324.125.2.
0.0286/1.00×.
0.0150/1.91×.
0.0155/1.85×.
0.0169/1.69×.
table 4: bleu scores [%] and speedup on the testset (2641 utterances) of the must-c en-de corpus un-der the restricted setting.
we evaluate the rtf on thenvidia v100 gpu with a batch size of 4 for all mod-els..pre-trained module must-c librispeech.
all.
-asr enc-mt-mt enc-mt dec.27.324.725.125.725.3.
20.819.919.420.719.9.table 5: effects of the pre-trained modules on bleuscores [%] under the unrestricted setting.
we only re-move one pre-trained module in each experiment..our method not only improves the performanceof 1.9 bleu points but also reaches up to 1.69×speedup than the cascaded baseline.
this encour-ages the application of the end-to-end st model inpractical scenarios..6.2 effects of pre-trained modules.
the effects of the pre-trained modules are shownin table 5. the model performance drops signiﬁ-cantly without the pre-trained asr encoder, espe-cially on the must-c corpus that contains noisyspeech.
the model parameters of pre-trained mtmodel are updated for adapting the output represen-tation of the random initialized acoustic encoder.
this results in the catastrophic forgetting problem(goodfellow et al., 2015).
the effect of the pre-trained mt model is more remarkable on the lib-rispeech corpus due to the modeling burden onthe translation.
the beneﬁt of the pre-trained mtdecoder is larger than the mt encoder.
this is con-trary to the previous conclusions that the mt en-coder helps the performance signiﬁcantly (li et al.,2020).
a possible reason is that the pre-trained.
nonesoftmappingfusion.
25.725.726.026.4.
21.721.921.821.9.table 6: bleu scores [%] of different adaptor setupson the development set under the unrestricted setting..vanilla.
sate.
ssenlacol.0.80.
0.60.
0.40.
0.20.
0.
6.
12.
18.layer.
figure 4: the localness of the vanilla e2e st modeland sate model with pre-training..asr encoder provides a rich representation andacts as part of the mt encoder, this leads to lowerperformance degradation when the textual encodertrains from scratch..each pre-trained module has a great effect on theﬁnal performance.
with the complete integration ofthe pre-trained modules, the model parameters areupdated slightly, which preserves the pre-trainedknowledge..6.3 effects of the adaptor.
we show the effects of the adaptor in table 6. thestraight connection which omits the representationinconsistency issue results in the lower beneﬁt ofpre-training.
although the soft representation aimsat generating the adaptive representation, there isno obvious improvement on the must-c corpus.
a possible reason is that the noisy speech inputsproduce the misalignment probabilities, which dis-turbs the textual encoding.
the mapping methodachieves a slight improvement by transforming theacoustic representation to the textual representa-tion.
fusing the soft and mapping representationenriches the information and avoids the represen-tation inconsistency issue, which achieves the bestperformances..6.4.impact on localness.
we show the encoder localness of the vanilla e2est model and sate model with pre-training in.
2626figure 4. as mentioned above, the vanilla stmodel inherits the preference of asr, which fo-cuses on short-distance dependencies.
sate ini-tializes with the pre-trained asr and mt encoders,which stacks acoustic and textual encoding.
thecomplementary behaviors of the pre-trained mod-els beneﬁt the translation, that is, the lower layersact like an asr encoder while the upper layerscapture global representation like an mt encoder..7 conclusion.
in this paper, we investigate the difﬁculty of speechtranslation and shed light on the reasons why pre-training has been challenging in st. this inspiresus to propose a stacked acoustic-and-textual en-coding method, which is straightforward to incor-porate the pre-trained models into st. we alsointroduce an adaptor module and a multi-teacherknowledge distillation method for bridging the gapbetween pre-training and ﬁne-tuning..results on the librispeech and must-c corporademonstrate the superiority of our method.
fur-thermore, we achieve comparable or even betterperformance than the cascaded counterpart whenlarge-scale asr and mt data is available..8 acknowledgement.
this work was supported in part by the nationalscience foundation of china (nos.
61876035and 61732005), the national key r&d programof china (no.
2019qy1801), and the ministryof science and technology of the prc (nos.
2019yff0303002 and 2020aaa0107900).
theauthors would like to thank anonymous reviewersfor their comments..references.
ashkan alinejad and anoop sarkar.
2020. effec-tively pretraining a speech translation decoderwith machine translation data.
in proceedingsof the 2020 conference on empirical methodsin natural language processing, emnlp 2020,online, november 16-20, 2020, pages 8014–8020. association for computational linguis-tics..antonios anastasopoulos and david chiang.
2018.tied multitask learning for neural speech trans-lation.
in proceedings of the 2018 conferenceof the north american chapter of the associa-tion for computational linguistics: human lan-.
guage technologies, naacl-hlt 2018, neworleans, louisiana, usa, june 1-6, 2018, vol-ume 1 (long papers), pages 82–91.
associationfor computational linguistics..parnia bahar, tobias bieschke, and hermann ney.
2019. a comparative study on end-to-endspeech to text translation.
in ieee automaticspeech recognition and understanding work-shop, asru 2019, singapore, december 14-18,2019, pages 792–799.
ieee..sameer bansal, herman kamper, karen livescu,adam lopez, and sharon goldwater.
2019. pre-training on high-resource speech recognition im-proves low-resource speech-to-text translation.
in proceedings of the 2019 conference of thenorth american chapter of the association forcomputational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis,mn, usa, june 2-7, 2019, volume 1 (long andshort papers), pages 58–68.
association forcomputational linguistics..alexandre berard, laurent besacier, ali can ko-cabiyikoglu, and olivier pietquin.
2018. end-to-end automatic speech translation of audio-books.
in 2018 ieee international conferenceon acoustics, speech and signal processing,icassp 2018, calgary, ab, canada, april 15-20, 2018, pages 6224–6228.
ieee..alexandre berard, olivier pietquin, christopheservan, and laurent besacier.
2016.lis-ten and translate: a proof of concept forend-to-end speech-to-text translation.
corr,abs/1612.01744..qianqian dong, mingxuan wang, hao zhou,shuang xu, bo xu, and lei li.
2021. con-secutive decoding for speech-to-text translation.
in the thirty-ﬁfth aaai conference on artiﬁcialintelligence, aaai..long duong, antonios anastasopoulos, david chi-ang, steven bird, and trevor cohn.
2016. anattentional model for speech translation withouttranscription.
in naacl hlt 2016, the 2016conference of the north american chapter ofthe association for computational linguistics:human language technologies, san diego cali-fornia, usa, june 12-17, 2016, pages 949–959.
the association for computational linguistics..2627mattia antonino di gangi, roldano cattoni, luisabentivogli, matteo negri, and marco turchi.
2019. must-c: a multilingual speech transla-tion corpus.
in proceedings of the 2019 con-ference of the north american chapter of theassociation for computational linguistics: hu-man language technologies, naacl-hlt 2019,minneapolis, mn, usa, june 2-7, 2019, volume1 (long and short papers), pages 2012–2017.
association for computational linguistics..ian j. goodfellow, mehdi mirza, da xiao, aaroncourville, and yoshua bengio.
2015. an em-pirical investigation of catastrophic forgetting ingradient-based neural networks..alex graves, santiago fern´andez, faustino j.gomez, and j¨urgen schmidhuber.
2006. con-nectionist temporal classiﬁcation: labelling un-segmented sequence data with recurrent neuralnetworks.
in machine learning, proceedingsof the twenty-third international conference(icml 2006), pittsburgh, pennsylvania, usa,june 25-29, 2006, volume 148 of acm inter-national conference proceeding series, pages369–376.
acm..anmol gulati, james qin, chung-cheng chiu,niki parmar, yu zhang, jiahui yu, wei han,shibo wang, zhengdong zhang, yonghuiwu, and ruoming pang.
2020. conformer:convolution-augmented transformer for speechrecognition.
in interspeech 2020, 21st annualconference of the international speech commu-nication association, virtual event, shanghai,china, 25-29 october 2020, pages 5036–5040.
isca..geoffrey e. hinton, oriol vinyals, and jeffreydean.
2015. distilling the knowledge in a neuralnetwork.
corr, abs/1503.02531..hirofumi inaguma, shun kiyono, kevin duh,shigeki karita, nelson yalta, tomoki hayashi,and shinji watanabe.
2020. espnet-st: all-in-one speech translation toolkit.
in proceedingsof the 58th annual meeting of the associationfor computational linguistics: system demon-strations, acl 2020, online, july 5-10, 2020,pages 302–311.
association for computationallinguistics..shigeki karita, nelson enrique yalta soplin, shinjiwatanabe, marc delcroix, atsunori ogawa,.
and tomohiro nakatani.
2019.improvingtransformer-based end-to-end speech recogni-tion with connectionist temporal classiﬁcationand language model integration.
in interspeech2019, 20th annual conference of the interna-tional speech communication association, graz,austria, 15-19 september 2019, pages 1408–1412. isca..ali can kocabiyikoglu, laurent besacier, andolivier kraif.
2018. augmenting librispeechwith french translations: a multimodal cor-pus for direct speech translation evaluation.
inproceedings of the eleventh international con-ference on language resources and evalua-tion, lrec 2018, miyazaki, japan, may 7-12,2018. european language resources associa-tion (elra)..philipp koehn, hieu hoang, alexandra birch,chris callison-burch, marcello federico, nicolabertoldi, brooke cowan, wade shen, christinemoran, richard zens, chris dyer, ondrej bojar,alexandra constantin, and evan herbst.
2007.moses: open source toolkit for statistical ma-chine translation.
in acl 2007, proceedings ofthe 45th annual meeting of the association forcomputational linguistics, june 23-30, 2007,prague, czech republic.
the association forcomputational linguistics..bei li, ziyang wang, hui liu, quan du, tongxiao, chunliang zhang, and jingbo zhu.
2020.learning light-weight translation models fromdeep transformer.
corr, abs/2012.13866..yuchen liu, hao xiong, jiajun zhang, zhongjunhe, hua wu, haifeng wang, and chengqingzong.
2019. end-to-end speech translation within interspeech 2019,knowledge distillation.
20th annual conference of the internationalspeech communication association, graz, aus-tria, 15-19 september 2019, pages 1128–1132.
isca..yuchen liu, junnan zhu, jiajun zhang, andchengqing zong.
2020. bridging the modal-ity gap for speech-to-text translation.
corr,abs/2010.14920..lambert mathias and william byrne.
2006. sta-tistical phrase-based speech translation.
in 2006ieee international conference on acousticsspeech and signal processing, icassp 2006,.
2628toulouse, france, may 14-19, 2006, pages 561–564. ieee..hermann ney.
1999. speech translation: cou-pling of recognition and translation.
in proceed-ings of the 1999 ieee international conferenceon acoustics, speech, and signal processing,icassp ’99, phoenix, arizona, usa, march 15-19, 1999, pages 517–520.
ieee computer soci-ety..daniel s. park, william chan, yu zhang, chung-cheng chiu, barret zoph, ekin d. cubuk, andquoc v. le.
2019. specaugment: a simpledata augmentation method for automatic speechrecognition.
in interspeech 2019, 20th annualconference of the international speech com-munication association, graz, austria, 15-19september 2019, pages 2613–2617.
isca..matthew e. peters, mark neumann, mohit iyyer,matt gardner, christopher clark, kenton lee,and luke zettlemoyer.
2018. deep contextual-ized word representations.
in proceedings of the2018 conference of the north american chapterof the association for computational linguis-tics: human language technologies, naacl-hlt 2018, new orleans, louisiana, usa, june1-6, 2018, volume 1 (long papers), pages 2227–2237. association for computational linguis-tics..juan pino, liezl puzon, jiatao gu, xutai ma,arya d mccarthy, and deepak gopinath.
2019.harnessing indirect training data for end-to-endautomatic speech translation: tricks of the trade.
arxiv preprint arxiv:1909.06515..juan pino, qiantong xu, xutai ma, moham-mad javad dousti, and yun tang.
2020. self-training for end-to-end speech translation.
ininterspeech 2020, 21st annual conference ofthe international speech communication asso-ciation, virtual event, shanghai, china, 25-29october 2020, pages 1476–1480.
isca..matt post.
2018. a call for clarity in reportingbleu scores.
in proceedings of the third con-ference on machine translation: research pa-pers, wmt 2018, belgium, brussels, october 31- november 1, 2018, pages 186–191.
associationfor computational linguistics..tanja schultz, szu-chen jou, stephan vogel, andshirin saleem.
2004. using word latice informa-tion for a tighter coupling in speech translationsystems.
in eighth international conference onspoken language processing..rico sennrich, barry haddow, and alexandrabirch.
2016. neural machine translation of rarewords with subword units.
in proceedings of the54th annual meeting of the association for com-putational linguistics, acl 2016, august 7-12,2016, berlin, germany, volume 1: long papers.
the association for computer linguistics..matthias sperber, graham neubig, jan niehues,and alex waibel.
2019. attention-passingmodels for robust and data-efﬁcient end-to-endspeech translation.
trans.
assoc.
comput.
lin-guistics, 7:313–325..matthias sperber, jan niehues, graham neubig,sebastian st¨uker, and alex waibel.
2018. self-attentional acoustic models.
in interspeech 2018,19th annual conference of the internationalspeech communication association, hyderabad,india, 2-6 september 2018, pages 3723–3727.
isca..mihaela c. stoian, sameer bansal, and sharongoldwater.
2020. analyzing asr pretrainingfor low-resource speech-to-text translation.
in2020 ieee international conference on acous-tics, speech and signal processing, icassp2020, barcelona, spain, may 4-8, 2020, pages7909–7913.
ieee..ashish vaswani, noam shazeer, niki parmar,jakob uszkoreit, llion jones, aidan n. gomez,lukasz kaiser, and illia polosukhin.
2017. at-in advances in neu-tention is all you need.
ral information processing systems 30: annualconference on neural information processingsystems 2017, december 4-9, 2017, long beach,ca, usa, pages 5998–6008..chengyi wang, yu wu, shujie liu, zhenglu yang,and ming zhou.
2020a.
bridging the gap be-tween pre-training and ﬁne-tuning for end-to-in the thirty-fourthend speech translation.
aaai conference on artiﬁcial intelligence, aaai2020, the thirty-second innovative applica-tions of artiﬁcial intelligence conference, iaai2020, the tenth aaai symposium on educa-tional advances in artiﬁcial intelligence, eaai.
2629conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcialintelligence, eaai 2020, new york, ny, usa,february 7-12, 2020, pages 9378–9385.
aaaipress..2020, new york, ny, usa, february 7-12, 2020,pages 9161–9168.
aaai press..chengyi wang, yu wu, shujie liu, ming zhou,and zhenglu yang.
2020b.
curriculum pre-training for end-to-end speech translation.
inproceedings of the 58th annual meeting of theassociation for computational linguistics, acl2020, online, july 5-10, 2020, pages 3728–3738.
association for computational linguistics..shinji watanabe, takaaki hori, shigeki karita,tomoki hayashi, jiro nishitoba, yuya unno,nelson enrique yalta soplin, jahn heymann,matthew wiesner, nanxin chen, adithya ren-duchintala, and tsubasa ochiai.
2018. espnet:end-to-end speech processing toolkit.
in inter-speech 2018, 19th annual conference of the in-ternational speech communication association,hyderabad, india, 2-6 september 2018, pages2207–2211.
isca..shinji watanabe, takaaki hori, suyoun kim,john r. hershey, and tomoki hayashi.
2017.hybrid ctc/attention architecture for end-to-endieee j. sel.
top.
signalspeech recognition.
process., 11(8):1240–1253..ron j. weiss, jan chorowski, navdeep jaitly,yonghui wu,and zhifeng chen.
2017.sequence-to-sequence models can directlytranslate foreign speech.
in interspeech 2017,18th annual conference of the internationalspeech communication association, stockholm,sweden, august 20-24, 2017, pages 2625–2629.
isca..baosong yang, zhaopeng tu, derek f. wong, fan-dong meng, lidia s. chao, and tong zhang.
2018. modeling localness for self-attention net-works.
in proceedings of the 2018 conferenceon empirical methods in natural language pro-cessing, brussels, belgium, october 31 - novem-ber 4, 2018, pages 4449–4458.
association forcomputational linguistics..jiacheng yang, mingxuan wang, hao zhou,chengqi zhao, weinan zhang, yong yu, andlei li.
2020. towards making the most ofin thebert in neural machine translation.
thirty-fourth aaai conference on artiﬁcial in-telligence, aaai 2020, the thirty-second in-novative applications of artiﬁcial intelligence.
2630