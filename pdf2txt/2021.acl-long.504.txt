selective knowledge distillation for neural machine translation.
fusheng wang∗†1, jianhao yan∗2 , fandong meng2 , jie zhou2peking university, china1pattern recognition center, wechat ai, tencent, china2wfs0315@pku.edu.com{elliottyan, fandongmeng}@tencent.com.
abstract.
neural machine translation (nmt) modelsachieve state-of-the-art performance on manytranslation benchmarks.
as an active researchﬁeld in nmt, knowledge distillation is widelyapplied to enhance the model’s performanceby transferring teacher model’s knowledge oneach training sample.
however, previous workrarely discusses the different impacts and con-nections among these samples, which serveas the medium for transferring teacher knowl-edge.
in this paper, we design a novel pro-tocol that can effectively analyze the differ-entimpacts of samples by comparing var-ious samples’ partitions.
based on aboveprotocol, we conduct extensive experimentsand ﬁnd that the teacher’s knowledge is notthe more, the better.
knowledge over spe-ciﬁc samples may even hurt the whole per-formance of knowledge distillation.
finally,to address these issues, we propose two sim-ple yet effective strategies,i.e., batch-leveland global-level selections, to pick suitablesamples for distillation.
we evaluate our ap-proaches on two large-scale machine trans-lation tasks, wmt’14 english-german andwmt’19 chinese-english.
experimental re-sults show that our approaches yield up to+1.28 and +0.89 bleu points improvementsover the transformer baseline, respectively.
1.
1.introduction.
machine translation has made great progress re-cently by using sequence-to-sequence models(sutskever et al., 2014; vaswani et al., 2017; mengand zhang, 2019; zhang et al., 2019b; yan et al.,2020).
recently, some knowledge distillation meth-ods (kim and rush, 2016; freitag et al., 2017; gu.
∗equal contribution.
† this work was done when fusheng wang was interningat pattern recognition center, wechat ai, tencent inc, china.
1we release our code on https://github.com/les.
lieoverfitting/selective distillation..et al., 2017; tan et al., 2019; wei et al., 2019; liet al., 2020; wu et al., 2020) are proposed in themachine translation to help improve model perfor-mance by transferring knowledge from a teachermodel.
these methods can be divided into two cate-gories: word-level and sequence-level, by the gran-ularity of teacher information.
in their researches,the model learns from teacher models by minimiz-ing gaps between their outputs on every trainingword/sentence (i.e., corresponding training sample)without distinction..despite their promising results, previous studiesmainly focus on ﬁnding what to teach and rarelyinvestigate how these words/sentences (i.e., sam-ples), which serve as the medium or carrier fortransferring teacher knowledge, participate in theknowledge distillation.
several questions remainunsolved for these samples: which part of all sam-ples shows more impact in knowledge distillation?
intuitively, we may regard that longer sentencesare hard to translate and might carry more teacherknowledge.
but are there more of these criteria thatcan identify these more important/suitable samplesfor distillation?
further, what are the connectionsamong these samples?
are they all guiding the stu-dent model to the same direction?
by investigatingthe carrier of teacher knowledge, we can shed lighton ﬁnding the most effective kd method..hence, in this paper, we aim to investigate the im-pacts and differences among all samples.
however,it is non-trivial to analyze each of them.
therefore,we propose a novel analytical protocol by partition-ing the samples into two halves with a speciﬁc cri-terion (e.g., sentence length or word cross-entropy)and study the gap between performance.
extensiveempirical experiments are conducted to analyze themost suitable sample for transferring knowledge.
we ﬁnd that different samples differ in transferringknowledge for a substantial margin.
more interest-ingly, with some partitions, especially the student.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6456–6466august1–6,2021.©2021associationforcomputationallinguistics6456model’s word cross-entropy, the model with half ofthe knowledge even shows better performance thanthe model using all distill knowledge.
the beneﬁtof the distillation of two halves cannot collaborate.
this phenomenon reveals that the distillation oftwo halves cannot collaborate, even hurt the wholeperformance.
hence, a more sophisticated selectivestrategy is necessary for kd methods..next, we propose two simple yet effective meth-ods to address the observed phenomenon accordingto word cross-entropy (word ce), which we ﬁnd isthe most distinguishable criterion.
we ﬁrst proposea batch-level selection strategy that chooses wordswith higher word ce within the current batch’sdistribution.
further, to step forward from local(batch) distribution to global distribution, we usea global-level fifo queue to approximate the op-timal global selection strategy, which caches theword ce distributions across several steps.
weevaluate our proposed method on two large-scalemachine translation datasets: wmt’14 english-german and wmt’19 chinese-english.
experi-mental results show that our approach yields animprovement of +1.28 and + 0.89 bleu pointsover the transformer baseline..in summary, our contributions are as follows:.
• we propose a novel protocol for analyzing theproperty for the suitable medium samples fortransferring teacher’s knowledge..• we conduct extensive analyses and ﬁnd thatsome of the teacher’s knowledge will hurt thewhole effect of knowledge distillation..• we propose two selective strategies: batch-level selection and global-level selection.
theexperimental results validate our methods areeffective..2 related work.
knowledge distillation approach (hinton et al.,2015) aims to transfer knowledge from teachermodel to student model.
recently, many knowl-edge distillation methods (kim and rush, 2016;hu et al., 2018; sun et al., 2019; tang et al., 2019;jiao et al., 2019; zhang et al., 2019a, 2020; chenet al., 2020a; meng et al., 2020) have been usedto get effective student model in the ﬁeld of natu-ral language processing by using teacher model’soutputs or hidden states as knowledge..as for neural machine translation (nmt), knowl-edge distillation methods commonly focus on bet-.
ter improving the student model and learning fromthe teacher model.
kim and rush (2016) ﬁrstapplied knowledge distillation to nmt and pro-posed the sequence-level knowledge distillationthat lets student model mimic the sequence dis-tribution generated by the teacher model.
it wasexplained as a kind of data augmentation and reg-ularization by gordon and duh (2019).
further,freitag et al.
(2017) improved the quality of dis-tillation information by using an ensemble modelas the teacher model.
gu et al.
(2017) improvednon-autoregressive model performance by learn-ing distillation information from the autoregressivemodel.
wu et al.
(2020) proposed a layer-wise dis-tillation method to be suitable for the deep neuralnetwork.
chen et al.
(2020b) let translation modellearn from language model to help the generationof machine translation..to the best of our knowledge, there is no pre-vious work in nmt concerning the selection ofsuitable samples for distillation.
the few relatedones mainly focus on selecting appropriate teachersfor the student model to learn.
for instance, tanet al.
(2019) let the student model only learn fromthe individual teacher model whose performancesurpasses it.
wei et al.
(2019) proposed an onlineknowledge distillation method that let the modelselectively learn from history checkpoints.
unlikethe above approaches, we explore the effective se-lective distillation strategy from sample perspectiveand let each sample determine learning content anddegree..3 background.
3.1 neural machine translation.
1, ..., y∗.
given a source sentence x = (x1, ..., xn), and itscorresponding ground-truth translation sentencey = (y∗m), an nmt model minimizes theword negative log-likelihood loss at each positionby computing cross-entropy.
for the j-th word inthe target sentence, the loss can be formulated as:.
|v |(cid:88).
k=1.
lce = −.
1{y∗.
j = k} log p(yj = k|y<j, x; θ),.
(1)where |v | is the size of target vocabulary, 1 is theindicator function, and p(·|·) denotes conditionalprobability with model parameterized by θ..6457criteria.
bleuslow.
shigh.
of different words/sentences in knowledge distilla-tion..∆---.
baselinedistill-alldistill-half(random).
27.2928.1428.18.sentence lengthword frequency.
27.5927.99.
+0.22+0.36*.
data property27.8128.35.student model.
embedding normword cesentence ce.
27.9028.4228.29.
27.73+0.1727.78 +0.64*+0.45*27.84.teacher model.
teacher pgoldenentropy.
27.9727.62.
28.0027.92.
-0.03-0.30.table 1: bleu score (%) of different criteria inwmt’14 en-de.
∆ denotes the difference of bleuscore (%) between shigh and slow.
‘*’: signiﬁcantly(p < 0.05) difference between the shigh and slow ..3.2 word-level knowledge distillation.
in knowledge distillation, student model s getsextra supervision signal by matching its own out-puts to the probability outputs of teacher model t .
speciﬁcally, word-level knowledge distillation de-ﬁnes the kullback–leibler distance between theoutput distributions of student and teacher (huet al., 2018).
after removing constants, the ob-jective is formulated as:.
lkd = −.
q(yj = k|y<j, x; θt ).
|v |(cid:88).
k=1.
× log p(yj = k|y<j, x; θs),.
(2)where q(·|·) is the conditional probability of teachermodel.
θs and θt is the parameter set of studentmodel and teacher model, respectively..and then, the overall training procedure is mini-.
mizing the summation of two objectives:.
l = lce + αlkd,.
(3).
where α is a weight to balance two losses..4 are all words equally suitable for.
kd?.
4.1 partition of different parts.
the optimal way to analyze samples’ differentimpacts on distillation is to do ablation studiesover each of them.
however, it is clearly time-consuming and intractable.
hence, we proposean analytical protocol by using the partition andcomparison as an approximation, which we believecould shed light on future analyses.
particularly, weleverage a speciﬁc criterion f to partition samplesinto two complementary parts:.
shigh := { yi | f(yi) > median(f (y)), yi ∈ y },slow := { yi | f(yi) ≤ median(f (y)), yi ∈ y },.
and analyze different effects between shigh andslow.
each part consists of 50% words/sentencesprecisely.
the criteria come from three differentperspectives: data property, student model, andteacher model.
the detailed descriptions are asfollows:.
• data property..as longer sentences and rare words are morechallenging to translate (kocmi and bojar,2017; platanios et al., 2019), its correspond-ing teacher knowledge may beneﬁt the stu-dent model more.
hence, we choose sentencelength and word frequency as criteria..• student model.
as for the student model,we care if the student model thinks thesewords/sentences are too complicated.
there-fore, we use word ce (cross-entropy ofwords), sentence ce (mean of the cross-entropy of all words in sentences), and eachword’s embedding norm (liu et al., 2020)..• teacher model.
for the teacher model, weguess that the teacher’s prediction conﬁdencemay be crucial for transferring knowledge.
hence, we use the prediction probability ofground-truth label (pgolden) and entropy ofprediction distribution as our criteria..4.2 analytic results.
as discussed before, as a carrier of the teacher’sknowledge, ground-truth words might greatly in-ﬂuence the performance of knowledge distillation.
therefore, in this section, we ﬁrst do some prelimi-nary empirical studies to evaluate the importance.
table 1 presents our results on different criteria.
we also add the performance of transformer base-line, distill-all (distillation with all words) anddistill-half(distillation with 50% words chosen byrandom) for comparison..6458impact of different parts.
through most of therows, we observe noticeable gaps between thebleu scores of the shigh and slow, indicatingthere exists a clear difference of impact on mediumof teacher knowledge.
speciﬁcally, for most of thecriteria like cross-entropies or word frequency, thegap between two halves surpasses 0.35. in contrast,teacher pgolden seems not useful for partitioningkd knowledge.
we conjecture this is because nomatter whether the teacher is convinced with thegolden label or not, other soft labels could containuseful information (gou et al., 2020).
besides, weﬁnd teacher entropy is a good-enough criterion forpartitioning kd data, which inlines with previousstudies of dark knowledge (dong et al., 2019).
fi-nally, we ﬁnd that the kd is most sensitive (+0.64)with the word ce criterion, which enjoys the adap-tivity during the training phase and is a good repre-sentative for whether the student thinks the sampleis difﬁcult..in conclusion, we regard the most suitable sam-ples should have the following properties: higherword ce, higher sentence ce, higher word fre-quency, which probably beneﬁts future studies ofeffective kd methods..impact of all and halves.
more interestingly,compared with ‘distill-all’, which is the combi-nation of the shigh and slow, the shigh halves’bleu score even surpass the ‘distill-all’, forword ce, sentence ce and word frequency crite-ria.
this leads to two conclusions:.
(1) within some partitions, the shigh contributes.
most to the kd improvements..(2) the amount of teacher knowledge is not themore, the better.
the distillation knowledge of theslow does not directly combine with the shigh,even hurts shigh’s performance..impact of the amount of knowledge.
giventhat distillation knowledge is most sensitive toword ce, we conduct extra analysis on the wordce.
figure 1 presents the results of varying theamount of knowledge for shigh and slow.
theconsistent phenomenon is that the shigh performsigniﬁcantly better than the slow when using thesame amount of teacher’s knowledge.
these re-sults suggest that we should focus more on theshigh than on slow.
besides, we notice that themodel performance increases when we increase theknowledge in shigh, but not the case for slow.
weconclude that the word ce is distinguishable and a.figure 1: bleu score (%) on wmt’14 en-de transla-tion task.
slow means the subset of training set whichhave relative small word-level cross-entropy and easyfor model to learn.
shigh means the subset of trainingset which have relative large word-level cross-entropyand hard for model to learn.
‘word rate’ controls thenumber of words need to get extra distillation knowl-edge from teacher model.
for example, word rate=30%means that student model only learns distillation knowl-edge of words whose cross-entropy loss in biggest /smallest 30%.
we choose the model which performsthe best on the validation set and report its performanceon test sets..better indicator of teachers’ useful knowledge onlyfor shigh..at the end of this section, we can summary the.
following points:.
• to ﬁnd out the most suitable medium for trans-ferring medium, we adopt a novel method ofpartition and comparison, which can easily beadopted to future studies..• the beneﬁt of distillation knowledge drasti-cally changes when applying to different medi-ums of knowledge..• among all criteria, knowledge distillation isthe most sensitive to word ce.
distillingwords with higher word ce is more reliablethan words with lower ce..• in some partitions, the distillation beneﬁt ofslow can not add to the shigh, even hurtsshigh’s performance..5 selective knowledge distillation for.
nmt.
as mentioned above, there exist un-suitable medi-ums/samples that hurt the performance of knowl-edge distillation.
in this section, we address thisproblem by using two simple yet effective strategyof selecting useful samples..in section 4, we ﬁnd that word ce is the mostdistinguishable criterion.
hence, we continue to.
645910%20%30%40%50%word rate27.427.627.828.028.228.4bleuslowshighbaselinedistil alluse the word ce as the measure in our methods.
asthe word cross-entropy is a direct measure of howthe student model agrees with the golden label, werefer to words with relatively large cross-entropyas difﬁcult words, and words with relatively smallcross-entropy as easy words, in the following parts.
this is to keep the notation different from previousanalysis..then, we only need to deﬁne what is “relativelylarge”.
here, we introduce two ce-based selectivestrategies:batch-level selection (bls).
given a mini-batch b of sentence pairs with m target words, wesort all words in the current batch with their wordce in descending order and select the top r percentof all words to distill teacher knowledge.
moreformally, let a denote the word ce set, which con-tains the word ce of each word in batch b. wedeﬁne shard = top r%(a) as the set of the r%largest cross-entropy words among the batch, andseasy is its complementary part..for those words in shard, we let them get extrasupervision signal from teacher model’s distillationinformation.
therefore, the knowledge distillationobjective in equation 3 can be be re-formulated as:.
(cid:40).
− (cid:80)|v |.
lkd =.
k=1 q(yk) · log p(yk), y ∈ shard, y ∈ seasy.
0.where we simplify the notation of p and q for clar-ity..global-level selection (gls).
limited by thenumber of words in a mini-batch, batch-level se-lection only reﬂects the current batch’s ce distri-bution and can not represent the real global cedistribution of the model very well.
in addition,the batch-level method makes our relative difﬁcultymeasure easily affected by each local batch’s com-position.
the optimal approach to get the globalce distribution is to traverse all training set wordsand calculate their ce to get the real-time distribu-tion after each model update.
however, this bringsa formidable computational cost and is not realisticin training..therefore, as a proxy to optimal way, we ex-tend batch-level selection to global-level selectionby dexterously using a first-in-first-out (fifo)global queue q. at each training step, we pushbatch words’ ce into fifo global queue q andpop out the ‘oldest’ words’ ce in the queue toretain the queue’s size.
then, we sort all ce val-ues in the queue and calculate the ranking position.
algorithm 1 global-level selectioninput: b: mini-batch, q: fifo global queue, t :teacher model, s: student model.
1: for each wordi in b do2:.
compute lce of wordi by equation 1compute lkd of wordi by equation 2push lce to qif lce in top r%(q) then.
lossi ← lce + α · lkd.
3:.
4:.
5:.
6:.
7:.
else.
8:.
lossi ← lceloss ← loss + lossi.
9:10: update s with respect to loss.
of each word.
the storage of queue is much big-ger than a mini-batch so that we can evaluate thecurrent batch’s ces with more words, which re-duces the ﬂuctuation of ce distribution caused bythe batch-level one.
algorithm 1 details the entireprocedure..6 experiments.
we carry out experiments on two large-scale ma-chine translation tasks: wmt’14 english-german(en-de) and wmt’19 chinese-english (zh-en)..6.1 setup.
datasets.
for wmt’14 en-de task, we use 4.5mpreprocessed data, which is tokenized and split us-ing byte pair encoded (bpe) (sennrich et al., 2016)with 32k merge operations and a shared vocabularyfor english and german.
we use newstest2013 asthe validation set and newstest2014 as the test set,which contain 3000 and 3003 sentences, respec-tively..for the wmt’19 zh-en task, we use 20.4m pre-processed data, which is tokenized and split using47k/32k bpe merge operations for source andtarget languages.
we use newstest2018 as our vali-dation set and newstest2019 as our test set, whichcontain 3981 and 2000 sentences, respectively..evaluation.
for evaluation, we train all the mod-els with a maximum of 300k steps for wmt en-de’14 and wmt’19 zh-en.
we choose the modelwhich performs the best on the validation set andreport its performance on test set.
we measurecase sensitive bleu calculated by multi-bleu.perl2.
2https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl.
6460models.
en-de.
∆.
vaswani et al.
(2017)vaswani et al.
(2017) (big)chen et al.
(2020b)zheng et al.
(2019)so et al.
(2019)tay et al.
(2020).
ref+1.10+0.23+0.80+1.10+1.17.
existing nmt systems27.3028.4027.5328.1028.4028.47our implemented methods27.29ref28.14+0.8528.15+0.86+1.1328.42*28.57*† +1.28.
transformerword-kdseq-kdbatch-level selectionglobal-level selection.
table 2: bleu scores (%) on wmt’14 english-german (en-de) task.
∆ shows the improvement com-pared to transformer (base).
‘*’: signiﬁcantly (p <0.01) better than transformer (base).
‘†’: signiﬁcantly(p < 0.05) better than the word/seq-kd models..compared methods.
we compare our methodwith several existing nmt systems (kd and oth-ers):.
• word-kd (kim and rush, 2016).
word-kd is a standard method that distills knowl-edge equally for each word.
the detailed de-scription is in section 3.2..• seq-kd (kim and rush, 2016).
sequence-kd uses teacher generated outputs on trainingcorpus as an extra source.
the training losscan be formulated as:.
lseq kd = −.
1{ˆyj = k}.
j(cid:88).
|v |(cid:88).
j=1.
k=1× log p(yj = k|ˆy<j, x; θ),.
(4).
where ˆy denotes the sequence predicted byteacher model from running beam search, jis the length of target sentence..• bert-kd (chen et al., 2020b).
this methodleverages the pre-trained bert as teachermodel to help nmt model improve machinetranslation quality..• other systems.
we also include some exist-ing methods based on transformer(base) forcomparison, i.e., zheng et al.
(2019); so et al.
(2019); tay et al.
(2020)..figure 2: bleu score (%) with different r% on valida-tion set of wmt’14 en-de..and mteval-v13a.pl3 with signiﬁcance test (koehn,2004) for wmt’14 en-de and wmt’19 zh-en,respectively..model and hyper-parameters.
following thesetting in vaswani et al.
(2017), we carry out our ex-periments on standard transformer (vaswani et al.,2017) with the fairseq toolkit (ott et al., 2019).
bydefault, we use transformer (base), which con-tains six stacked encoder layers and six stackeddecoder layers as both teacher model and studentmodel.
to verify our approaches can be applied to astronger teacher and student models, we further usedeep transformers with twelve encoder layers andsix decoder layers.
in training processing, we useadam optimizer with β1 = 0.9, β2 = 0.98, learn-ing rate is 7e-4 and dropout is 0.1. all experimentsare conducted using 4 nvidia p40 gpus, wherethe batch size of each gpus is set to 4096 tokens.
and we accumulate the gradient of parameters andupdate every two steps.
the average runtimes are3 gpu days for all experiments..there are two hyper-parameters in our exper-iment, i.e., distil rate r% and global queue sizeqsize.
for distil rate r%, the search space is [10%,30%, 50%, 70%, 90%].
the search result of r% isshown in figure 2, we can ﬁnd that the performanceis sensitive to the value of r%.
when the ratio issmaller than 50%, the increase of ratio is consistentwith the bleu score increases, and the best perfor-mance peaks at 50%.
we directly apply the distilrate r% to the wmt’19 zh-en task without extrasearching.
besides, we set the qsize = 30k forwmt’14 en-de.
for larger dataset wmt’19 zh-en, we enlarge the qsize to from 30k to 50k andkeep word rate unchanged.
the hyper-parametersearch of qsize can be found in section 6.4..3https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl.
results on wmt’14 english-german.
the re-sults on wmt’14 en-de are shown in table 2. in.
6.2 main results.
646110%30%50%70%90%rate26.826.927.027.127.2bleufigure 3: the probability for gradients of lkd and lcepointing the same direction..this experiment, both the teacher model and stu-dent model are transformer (base).
we also listour implementation of word-level distillation andsequence level distillation (kim and rush, 2016)method..firstly, compared with the transformer (base),our re-implemented word-level and the sequence-level distillation show similar improvements withthe bleu scores up from 27.29 to 28.14 and 28.15,respectively.
secondly, compared with these al-ready strong baseline methods, our batch-level se-lective approach further extends the improvementto 28.42, proving the selective strategy’s effective-ness.
thirdly, our global-level distillation achievesa 28.57 bleu score and outperforms all previ-ous methods, showing that the better evaluationof words’ ce distribution with fifo global queuehelps selection.
it is worth noting that our strategyalso signiﬁcantly improves translation quality overall others methods including word-kd.
finally, ourmethods show comparable/better performance thanother existing nmt systems and even surpass thetransformer (big), with much fewer parameters..6.3 analysis.
even though we ﬁnd some interesting phenomenaand achieve great improvement by selective distil-lation, the reason behind it is still unclear.
hence,in this section, we conduct some experiments toanalyze and explain the remaining question..note that we follow the previous partition andcomparison method in this section and divide thesamples with/without kd loss deﬁned in our selec-tion strategy as shard/seasy..conﬂict on different parts.
the ﬁrst questionis that why our methods surpass the word-kd withmore knowledge.
to answer this question, we col-lect the statistics on the gradient difference between.
figure 4: the entropy of prediction distribution ofteacher model for different parts..knowledge distillation loss and cross-entropy losson the ground-truth label for shard and seasy..here, we study gradients over the output distri-butions, which are directly related to the model’sperformance.
particularly, decoder maps targetsentences y = (y∗m) to their correspond-ing hidden representation h = (h1, ..., hm).
forwords in target sequence, the prediction logitsl ∈ rdmodel×|v | is given by:.
1, ..., y∗.
l = ht wp = sof tmax(l).
(5).
(6).
where h ∈ rdmodel is the layer output of trans-former decoder, w ∈ rdmodel×|v | is projectionmatrix.
then, the gradient respect to l from goldencross-entropy loss can be denotes as ∇llce.
thegradient from distillation loss can be denotes as∇llkd.
next, we calculate the probability that∇llce and ∇llkd share the same direction..figure 3 presents the results with the probabilitythat gradients agree with each other during training.
we observe that seasy (green line) is consistentlylower than distillation with all words (blue line)and shard (red line), which means seasy has moreinconsistency with ground-truth.
combining withthe bleu performances, we argue this consistencyleads to the risk of introducing noise and disturbsthe direction of parameter updating..besides, the agreement of distill-all (blue linein fig) lies in the middle of two halves.
it provesthat seasy and shard compromise with each otheron some conﬂicts.
it also proves that there existsome conﬂicts between the knowledge in seasyand shard..knowledge on different parts.
in our ap-proaches, we select the transferring samples from.
6462020000400006000080000100000training steps0.650.700.750.800.850.900.951.00probabilityseasy (27.78)shard (28.42)distil all (28.14)02468entropy025005000750010000125001500017500countseasyshardmodelstransformer (base)word-kdseq-kdword-kd + oursseq-kd + ours.
∆zh-enref25.73+0.4826.2127.27+1.5426.62* +0.8927.61* +1.88.
table 3: bleu scores (%) on wmt’19 chinese-english (zh-en) task.
∆ shows the improvement com-pared to transformer (base).
‘*’: signiﬁcantly (p <0.01) better than the transformer (base)..the student model’s point of view.
however, inprevious literature, they commonly consider knowl-edge from the teacher’s perspective.
hence, in thissection, we study the correlation between these twoperspectives..because previous studies commonly regardteacher’s soft-labels contain dark knowledge (donget al., 2019), we take the entropy of teacher’s pre-diction as a proxy.
concretely, we randomly select100k tokens in the training set and calculate the en-tropy of distribution predicted by the teacher modelfor both shard and seasy.
as shown in figure 4,we notice that the seasy’s entropy distribution ismore concentrated in range (0, 4) and peaks around1.2. in contrast, the shard’s entropy distribution ismore spread out.
the overall distribution shifts tohigher entropy, which indicates shard tends to pro-vide a smoother supervision signal.
consequently,we conclude that even though our selective strat-egy comes from the student’s perspective, it alsofavors samples with abundant dark knowledge inteacher’s perspective.
to some extent, this explainswhy the shard’ knowledge beneﬁts distillation per-formance more..6.4 generalizability.
results on wmt’19 chinese-english.
we alsoconduct experiments on the larger wmt’19 zh-endataset (20.4m sentence pairs) to ensure our meth-ods can provide consistent improvements acrossdifferent language pairs..as shown in table 3, our method still signiﬁ-cantly outperforms the transformer (base) with+0.89.
compared with the word-kd, our approachconsistently improves with +0.41 bleu points.
besides, we also ﬁnd that seq-kd with our meth-ods extends the improvement of bleu score from27.27 to 27.61. this indicates that our selectivestrategy is partially orthogonal to the improvement.
modelsdeep transformer (12 + 6)word-kdours.
∆en-deref27.9428.90+0.9629.12* +1.18.
table 4: bleu scores (%) on wmt’14 english-german (en-de) task.
here we use deep transform-ers (12 encoders and 6 decoders) for both the teacherand student model.
∆ shows the improvement com-pared to deep transformer (12 + 6).
‘*’: signiﬁcantly(p < 0.01) better than deep transformer (12 + 6)..of seq-kd and maintains generalizability.
in sum-mary, these results suggest that our methods canachieve consistent improvement on different sizeddatasets across different language pairs..results with larger model size.
here, we in-vestigate how our method is well-generalized tolarger models.
we use a deep transformer modelwith twelve encoder layers and six decoder layersfor our larger model experiments.
as shown intable 4, deep transformer (12 + 6) and word-kdhave already achieved strong performance with upto 28.90 bleu points, and our method still outper-forms these baselines (29.12 bleu).
it proves ourmethods’ generalizability to larger models..6.5 effect of the global queue.
this section analyzes how qsize affects ourmodel’s performance.
as mentioned before, qsizedenotes the size of the global fifo queue, whichaffects simulating the word cross-entropy distribu-tion of the current model..figure 5 shows the search results of qsize.
wecan ﬁnd that smaller and larger queue size bothhurts the bleu scores.
besides, 30k and 50k ofqueue size are the best for wmt’14 en-de andwmt’19 zh-en, respectively.
this also accordswith our intuition that smaller qsize degrades theglobal-level queue to batch level, and larger qsizeslows down the update of ce distribution..figure 6 plots the partition word ce of shardand seasy for batch-level and global-level selec-tion.
we can see that, as the training progresses,batch-level selection starts to suffer from the highvariance because of each batch’s randomness.
se-lections with fifo queue drastically reduce thevariance and make a reasonable estimation ofglobal ce distribution.
these ﬁndings prove theeffectiveness of our proposed fifo queue..6463the experiment results show that our approachescan achieve consistent improvements on differentsized datasets across different language pairs..acknowledgments.
we would like to thank the anonymous reviewersfor their valuable comments and suggestions toimprove this paper..xiuyi chen, fandong meng, peng li, feilong chen,shuang xu, bo xu, and jie zhou.
2020a.
bridgingthe gap between prior and posterior knowledge selec-tion for knowledge-grounded dialogue generation.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3426–3437, online.
association for computa-tional linguistics..yen-chun chen, zhe gan, yu cheng, jingzhou liu,and jingjing liu.
2020b.
distilling knowledgelearned in bert for text generation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7893–7905..bin dong, jikai hou, yiping lu, and zhihua zhang.
2019. distillation ≈ early stopping?
harvestingdark knowledge utilizing anisotropic information re-trieval for overparameterized neural network.
arxivpreprint arxiv:1910.01255..markus freitag, yaser al-onaizan, and baskaranensemble distillation forarxiv preprint.
sankaran.
2017.neural machine translation.
arxiv:1702.01802..mitchell a gordon and kevin duh.
2019. explain-ing sequence-level knowledge distillation as data-augmentation for neural machine translation.
arxivpreprint arxiv:1912.03334..jianping gou, baosheng yu, stephen john maybank,and dacheng tao.
2020. knowledge distillation: asurvey.
arxiv preprint arxiv:2006.05525..jiatao gu, james bradbury, caiming xiong, vic-non-arxiv.
tor ok li, and richard socher.
2017.autoregressive neural machine translation.
preprint arxiv:1711.02281..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
arxivpreprint arxiv:1503.02531..minghao hu, yuxing peng, furu wei, zhen huang,and ming zhou.
attention-guided answer distillation forarxiv preprint.
dongsheng li, nan yang,2018.machine reading comprehension.
arxiv:1808.07644..(a) en-de.
references.
(b) zh-en.
figure 5: bleu score (%) with different qsize onwmt’14 en-de and wmt’19 zh-en validation set..figure 6: partition point for shard and seasy, withrespect to different strategies.
batch-level selectionclearly suffers from large ﬂuctuations and high vari-ance..7 conclusion.
in this work, we conduct an extensive study to an-alyze the impact of different words/sentences asthe carrier in knowledge distillation.
analytic re-sults show that distillation beneﬁts have a substan-tial margin, and these beneﬁts may not collaboratewith their complementary parts and even hurt theperformance.
to address this problem, we pro-pose two simple yet effective strategies, namelythe batch-level selection and global-level selection..646410k20k30k40ksize26.927.027.127.2bleu20k30k40k50k60k70ksize23.924.024.124.224.324.424.524.6bleu05001000150020002500300035004000training steps246810lossbatchqueue 30kqueue 100kxiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2019. tinybert: distilling bert for natural languageunderstanding.
arxiv preprint arxiv:1909.10351..yoon kim and alexander m rush.
2016. sequence-arxiv preprint.
level knowledge distillation.
arxiv:1606.07947..tom kocmi and ondrej bojar.
2017. curriculum learn-ing and minibatch bucketing in neural machine trans-lation.
arxiv preprint arxiv:1707.09533..philipp koehn.
2004. statistical signiﬁcance tests forin proceedings ofmachine translation evaluation.
the 2004 conference on empirical methods in naturallanguage processing, pages 388–395..bei li, ziyang wang, hui liu, quan du, tong xiao,chunliang zhang, and jingbo zhu.
2020. learn-ing light-weight translation models from deep trans-former.
arxiv preprint arxiv:2012.13866..xuebo liu, houtim lai, derek f wong, and lidia snorm-based curriculum learningarxiv preprint.
chao.
2020.for neural machine translation.
arxiv:2006.02014..fandong meng, jianhao yan, yijin liu, yuan gao, xi-anfeng zeng, qinsong zeng, peng li, ming chen,jie zhou, sifan liu, and hao zhou.
2020. wechatneural machine translation systems for wmt20.
inproceedings of the fifth conference on machinetranslation, pages 239–247, online.
association forcomputational linguistics..fandong meng and jinchao zhang.
2019. dtmt: anovel deep transition architecture for neural machinetranslation.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 33, pages 224–231..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..emmanouil antonios platanios, otilia stretcu, gra-ham neubig, barnabas poczos, and tom m mitchell.
competence-based curriculum learning2019.arxiv preprintfor neural machine translation.
arxiv:1903.09848..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..david so, quoc le, and chen liang.
2019. theevolved transformer.
in international conference onmachine learning, pages 5877–5886.
pmlr..siqi sun, yu cheng, zhe gan, and jingjing liu.
2019.patient knowledge distillation for bert model com-pression.
arxiv preprint arxiv:1908.09355..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
advances in neural information processing systems,27:3104–3112..xu tan, yi ren, di he, tao qin, zhou zhao, and tie-yan liu.
2019. multilingual neural machine trans-lation with knowledge distillation.
arxiv preprintarxiv:1902.10461..raphael tang, yao lu, linqing liu, lili mou, olgavechtomova, and jimmy lin.
2019. distilling task-speciﬁc knowledge from bert into simple neural net-works.
arxiv preprint arxiv:1903.12136..yi tay, dara bahri, donald metzler, da-cheng juan,zhe zhao, and che zheng.
2020. synthesizer: re-thinking self-attention in transformer models.
arxivpreprint arxiv:2005.00743..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..hao-ran wei, shujian huang, r. wang, xin-yu dai,and jiajun chen.
2019. online distilling from check-in naacl-points for neural machine translation.
hlt..yimeng wu, peyman passban, mehdi rezagholizade,and qun liu.
2020. why skip if you can combine:a simple knowledge distillation technique for inter-mediate layers.
arxiv preprint arxiv:2010.03034..jianhao yan, fandong meng, and jie zhou.
2020.multi-unit transformers for neural machine transla-in proceedings of the 2020 conference ontion.
empirical methods in natural language processing(emnlp), pages 1047–1059, online..biao zhang, deyi xiong, jinsong su, and jiebo luo.
future-aware knowledge distillation for2019a.
ieee/acm transac-neural machine translation.
tions on audio, speech, and language processing,27(12):2278–2287..wei zhang, lu hou, yichun yin, lifeng shang, xiaochen, xin jiang, and qun liu.
2020. ternarybert:distillation-aware ultra-low bit bert.
arxiv preprintarxiv:2009.12812..wen zhang, yang feng, fandong meng, di you, andqun liu.
2019b.
bridging the gap between train-ing and inference for neural machine translation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4334–4343, florence, italy..6465zaixiang zheng, shujian huang, zhaopeng tu, xin-yudai, and jiajun chen.
2019. dynamic past and fu-ture for neural machine translation.
arxiv preprintarxiv:1904.09646..6466