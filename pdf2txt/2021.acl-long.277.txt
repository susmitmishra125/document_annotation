revisiting the negative data of distantly supervised relation extraction.
chenhao xie1,2, jiaqing liang1,2, jingping liu1, chengsong huang1,wenhao huang1, yanghua xiao1,3∗1shanghai key laboratory of data science, school of computer science, fudan university2shuyan technology inc., shanghai, china3fudan-aishu cognitive intelligence joint research center, shanghai, china{redreamality, l.j.q.light}@gmail.com{jpliu17, huangcs19, whhuang17, shawyh}@fudan.edu.cn.
abstract.
distantly supervision automatically generatesplenty of training samples for relation extrac-tion.
however, it also incurs two major prob-lems: noisy labels and imbalanced trainingdata.
previous works focus more on reduc-ing wrongly labeled relations (false positives)while few explore the missing relations that arecaused by incompleteness of knowledge base(false negatives).
furthermore, the quantityof negative labels overwhelmingly surpassesthe positive ones in previous problem formula-tions.
in this paper, we ﬁrst provide a thoroughanalysis of the above challenges caused by neg-ative data.
next, we formulate the problemof relation extraction into as a positive unla-beled learning task to alleviate false negativeproblem.
thirdly, we propose a pipeline ap-proach, dubbed rere, that ﬁrst performs sen-tence classiﬁcation with relational labels andthen extracts the subjects/objects.
experimen-tal results show that the proposed method con-sistently outperforms existing approaches andremains excellent performance even learnedwith a large quantity of false positive samples.
source code is available online1..1.introduction.
relational extraction is a crucial step towardsknowledge graph construction.
it aims at identify-ing relational triples from a given sentence in theform of (cid:104)subject, relation, object(cid:105), in short, (cid:104)s, r, o(cid:105).
for example, given s1 in figure 1, we hope to ex-tract (cid:104)william shakespeare, birthplace,stratford-upon-avon(cid:105)..this task is usually modeled as a supervisedlearning problem and distant supervision (mintzet al., 2009) is utilized to acquire large-scale train-ing data.
the core idea is to obtain training data.
∗corresponding author1https://github.com/redreamality/.
rere-relation-extraction.
figure 1: illustration of distant supervision process.
s2-s5 are examples for four kinds of label noise.
tp, fp, fnand pl mean true positive, false positive, false negativeand partially labeled, respectively.
“r-” or “e-” indi-cates whether the error occurs at relation-level or entity-level.
bold tokens are ground-truth subjects/objects.
underlined tokens together with the relation in the thirdcolumn are labeled by distant supervision.
“na” meansno relation..is through automatically labeling a sentence withexisting relational triples from a knowledge base(kb).
for example, given a triple (cid:104)s, r, o(cid:105) and asentence, if the sentence contains both s and o, dis-tant supervision methods regard (cid:104)s, r, o(cid:105) as a validsample for the sentence.
if no relational triples areapplicable, the sentence is labeled as “na”..despite the abundant training data obtained withdistant supervision, nonnegligible errors also occurin the labels.
there are two types of errors.
inthe ﬁrst type, the labeled relation does not conformwith the original meaning of sentence, and this typeof error is referred to as false positive (fp).
for ex-ample, in s2, the sentence “shakespeare spent thelast few years of his life in stratford-upon-avon.”does not express the relation birthplace, thusbeing a fp.
in the second type, large amounts of.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3572–3581august1–6,2021.©2021associationforcomputationallinguistics3572relations in sentences are missing due to the incom-pleteness of kb, which is referred to as false nega-tive (fn).
for instance, in s3, “buffett was born in1930 in omaha, nebraska.” is wrongly labeled asna since there is no relation (e.g., birthplace)between buffett and omaha, nebraska in thekb.
many efforts have been devoted to solving thefp problem, including pattern-based methods (jiaet al., 2019), multi-instance learning methods (linet al., 2016; zeng et al., 2018a) and reinforcementlearning methods (feng et al., 2018).
signiﬁcantimprovements have been made..however, fn problem receives much less atten-tion (min et al., 2013; xu et al., 2013; roller et al.,2015).
to the best of our knowledge, none existingwork with deep neural networks to solve this prob-lem.
we argue that this problem is fatal in practicesince there are massive fn cases in datasets.
forexample, there exist at least 33% and 35% fnsin nyt and ske datasets, respectively.
we willdeeply analyze the problem in section 2.1.another huge problem in relation extraction isthe overwhelming negative labels.
as is widelyacknowledged, information extraction tasks arehighly imbalanced in class labels (chowdhury andlavelli, 2012; lin et al., 2018; li et al., 2020).
in particular, the negative labels account for mostof the labels in relation extraction under almostany problem formulation, which makes relationextraction a hard machine learning problem.
wesystematically analyze this in section 2.2..in this paper, we address these challenges causedby negative data.
our main contribution can besummarized as follows..• we systematically compare the class distri-butions of different problem modeling andexplain why ﬁrst extract relation then entities,i.e., the third paradigm (p3) in section 2.2, issuperior to the others..• based on the ﬁrst point, we adopt p3 andpropose a novel two-staged pipeline modeldubbed rere.
it ﬁrst detects relation at sen-tence level and then extracts entities for a spe-ciﬁc relation.
we model the false negativesin relation extraction as “unlabeled positives”and propose a multi-label collective loss func-tion..• our empirical evaluations show that the pro-posed method consistently outperforms exist-ing approaches, and achieves excellent perfor-.
mance even learned with a large quantity offalse positive samples.
we also provide twocarefully annotated test sets aiming at reduc-ing the false negatives of previous annotation,namely, nyt21 and ske21, with 370 and1150 samples, respectively..2 problem analysis and pilot.
experiments.
we use (ci, ti) to denote a training instance,where ci is a sentence consisting of n tokensci = [ci1, ..., cin ] labeled by a set of triples ti ={(cid:104)s, r, o(cid:105)} from the training set d. for rigorous def-inition, [ci1, ..., cin ] can be viewed as an orderedset {(ci1, 1), ..., (cin , n )} so that set operationscan be applied.
we assume r ∈ r, where r is aﬁnite set of all relations in d. other model/task-speciﬁc notations are deﬁned after each problemformulation..we now clarify some terms used in the introduc-tion and title without formal deﬁnition.
a negativesample refers to a triple t /∈ ti.
negative labelrefers to the negative class label (e.g., usually “0”for binary classiﬁcation), used for supervision withrespect to task-speciﬁc models.
under differenttask formulation, the negative labels can be differ-ent.
negative data is a general term that includesboth negative labels and negative samples.
thereare two kinds of false negatives.
relation-levelfalse negative (s3 in figure 1) refers to the situa-tion where there exists t(cid:48) = (cid:104)s(cid:48), r(cid:48), o(cid:48)(cid:105) /∈ ti, but r(cid:48)is actually expressed by ci, and does not appear inother t ∈ ti.
similarly, entity-level false negative(s4 and s5 in figure 1) means r(cid:48) appears in othert ∈ ti.
imbalanced class distribution means thatthe quantity of negative labels is much larger thanthat of positive ones..2.1 addressing the false negatives.
as shown in table 1, the triples in nyt (ske)datasets2 labeled by freebase3 (baidubaike4) is88,253 (409,767), while the ones labeled by wiki-data5 (cn-dbpedia6) are 58,135 (342,931).
inother words, there exists massive fn matches ifonly labeled by one kb due to the incomplete-ness of kbs.
notably, we ﬁnd that the fn rateis underestimated by previous researches (min.
2detailed description of datasets is in sec.
5.13(bollacker et al., 2008)4https://baike.baidu.com/5(vrandecic and kr¨otzsch, 2014)6 (xu et al., 2017).
3573et al., 2013; xu et al., 2013), based on the man-ual evaluation of which there are 15%-35% fnmatches.
this discrepancy may be caused by hu-man error.
in speciﬁc, a volunteer may accidentallymiss some triples.
for example, as pointed outby wei et al.
(2020, in appendix c), the test setof nyt11 (hoffmann et al., 2011) missed lots oftriples, especially when multiple relations occur ina same sentence, though labeled by human.
thatalso provides an evidence that fn’s are harder todiscover than fp’s..nyt (english).
ske (chinese).
# sentence.
56,196.
194,747.
# triples.
# rels.
# triples.
# rels.
originalre-labeledintersectionunion.
original fnrrelabel fnr.
88,25358,13513,848132,540.
≥ 0.33≥ 0.56.
23571862.
409,767342,931121,326631,372.
≥ 0.35≥ 0.46.
4937846381.table 1: statistics of the quantity of distantly labeledrelational triples by using different kb’s.
the “original”refers to freebase for nyt and baidubaike for ske.
the “relabeled” means aligning using wikidata and cn-dbpedia to re-label nyt and ske datasets.
in speciﬁc,we consider triples with the same subject and object tobe candidate triples and use a relation mapping tableto determine whether the triples match.
the intersec-tion of ske dataset has two values because the originalrelation has a one-to-many mapping with relations incn-dbpedia.
fnr stands for false negative rates, cal-culated by using the # triples in original (re-labeled)divided by the union..2.2 addressing the overwhelming negative.
labels.
we point out that some of the previous paradigmsdesigned for relation extraction aggravate the im-balance and lead to inefﬁcient supervision.
themainstream approaches for relation extractionmainly fall into three paradigms depending on whatto extract ﬁrst..p1 the ﬁrst paradigm is a pipeline that beginswith named entity recognition (ner) and thenclassiﬁes each entity pair into different rela-tions, i.e., [s, o then r].
it is adopted by manytraditional approaches (mintz et al., 2009;chan and roth, 2011; zeng et al., 2014, 2015;gormley et al., 2015; dos santos et al., 2015;lin et al., 2016)..p2 the second paradigm ﬁrst detects all possiblesubjects in a sentence then identiﬁes objectswith respect to each relation, i.e., [s then r, o].
speciﬁc implementation includes modelingrelation extraction as multi-turn question an-swering (li et al., 2019), span tagging (yuet al., 2020) and cascaded binary tagging (weiet al., 2020)..p3 the third paradigm ﬁrst perform sentence-level relation detection (cf.
p1, which is atentity pair level.)
then extract subjects andentities, i.e., [r then s, o].
this paradigm islargely unexplored.
hrl (takanobu et al.,2019) is hitherto the only work to apply thisparadigm based on our literature review..we provide theoretical analysis of the output spaceand class prior with statistical support from threedatasets (see section 5.1 for description) of thethree paradigms in table 2. the second step of p1can be compared with the ﬁrst step of p3.
both ofthem ﬁnd relation from a sentence (p1 with targetentity pair given).
suppose a sentence containsm entities7, the classiﬁer has to decide relationfrom o(m2) entity pairs, while in reality, relationsare often sparse, i.e., o(m).
in other words, mostentity pairs in p1 do not form valid relation, thusresulting in a low class prior.
the situation is evenworse when the sentence contains more entities,such as in nyt11-hrl.
for p2, we demonstratewith the problem formulation of casrel (weiet al., 2020).
the difference of the ﬁrst-step classprior between p2 and p3 depends on the resultof comparison between # relations and averagesentence length (i.e., |r| and ¯n ), which varies indifferent scenarios/domains.
however, π2 of p2is extremely low, where a classiﬁer has to decidefrom a space of |r| ∗ ¯n .
in contrast, p3 only needto decide from 4 ∗ ¯n based on our task formulation(section 3.1).
other task formulations include jointly extract-ing the relation and entities (yu and lam, 2010; liand ji, 2014; miwa and sasaki, 2014; gupta et al.,2016; katiyar and cardie, 2017; ren et al., 2017)and recently in the manner of sequence tagging(zheng et al., 2017), sequence-to-sequence learn-ing (zeng et al., 2018b).
in contrast to the afore-mentioned three paradigms, most of these methodsactually provide an incomplete decision space thatcannot handle all the situation of relation extrac-.
7below the same..3574paradigm.
theoretical.
nyt10-hrl|r|=31, ¯n = 39.08.nyt11-hrl|r|=11, ¯n =39.46.
ske|r|=51, ¯n = 54.67.π1.
π2.
π1.
π2.
π1.
π2.
π1.
π2.
s, o then rs then r, or then s, o.
–e[e[.
e[(cid:80) y¯n ] e[(cid:80) y|r| ] e[.
(cid:80) y|r| ](cid:80) y¯n ∗|r| ](cid:80) y4∗ ¯n ].
–0.0585.
0.014210.00093.
–0.0574.
0.002800.00257.
–0.0405.
0.004940.00067.
0.0390.
0.00842.
0.0826.
0.00835.
0.0344.
0.00927.table 2: comparison of class prior under different relation extraction paradigms.
|r| means the total number ofrelations and ¯n is the average sentence length.
π1 (π2) refers to the class prior for the ﬁrst (second) task in thepipeline.
π1 for the ﬁrst paradigm is omitted because it is often considered a preceding step.
(cid:80) y is the summationof 1’s in labels, of using which our intention is to represent the information a positive sample conveys..tion, for example, the overlapping one (wei et al.,2020)..formalize it as a multi-label classiﬁcation task..3 solution framework.
3.1 framework of rere.
given an instance (ci, ti) from d, the goal of train-ing is to maximize the likelihood deﬁned in eq.
(1).
it is decomposed into two components by applyingthe deﬁnition of conditional probability, formulatedin eq.
(2)..pr(ti|ci; θ).
|d|(cid:89).
i=1.
|d|(cid:89).
=.
(cid:89).
pr(r|ci; θ).
(cid:89).
pr(s, o|r, ci; θ),.
i=1.
r∈ti.
(cid:104)s,o(cid:105)∈ti|r.
(1).
(2).
where we use r ∈ ti as a shorthand for r ∈ {r |(cid:104)s, r, o(cid:105) ∈ ti}, which means that r occurs in thetriple set w.r.t.
ci; similarly, s ∈ ti, (cid:104)s, o(cid:105) ∈ ti|rstands for s ∈ {s | (cid:104)s, r, o(cid:105) ∈ ti|r} and (cid:104)s, o(cid:105) ∈{(cid:104)s, o(cid:105) | (cid:104)s, r, o(cid:105) ∈ ti|r}, respectively.
ti|r repre-sents a subset of ti with a common relation r. 1[·]is an indicator function; 1[condition] = 1 whenthe condition happens.
we denote by θ the modelparameters.
under this decomposition, relationaltriple extraction task is formulated into two sub-tasks: relation classiﬁcation and entity extraction..relation classiﬁcation.
as is discussed, build-ing relation classiﬁer at entity-pair level will in-troduce excessive negative samples and form ahard learning problem.
therefore, we alternativelymodel the relation classiﬁcation at sentence level.
intuitively speaking, we hope that the model couldcapture what relation a sentence is expressing.
we.
|r|(cid:89).
j=1.
pr(r|ci; θ) =.
(ˆyj.
rc)1[yj.
rc=1](1 − ˆyj.
rc)1[yj.
rc=0],.
(3)rc is the probability that c is expressing rj,rc is the ground truth from therc = 1 is equivalent to rj ∈ ti while.
where ˆyjthe j-th relation8.
yjlabeled data; yjyjrc = 0 means the opposite..entity extraction.
we then model entity ex-traction task.
we observe that given the relationr and context ci, it naturally forms a machinereading comprehension (mrc) task (chen, 2018),where (r, ci, s/o) naturally ﬁts into the paradigmof (query, context, answer).
particularly, thesubjects and objects are continuous spans from ci,which falls into the category of span extraction.
weadopt the boundary detection model with answerpointer (wang and jiang, 2017) as the output layer,which is widely used in mrc tasks.
formally, fora sentence of n tokens,.
pr(s, o|r, ci; θ)n(cid:89).
(cid:89).
=.
k∈k.
n=1.
(ˆyn,k.
ee )1[yn,k.
ee =1](1 − ˆyn,k.
ee )1[yn,k.
ee =0],.
(4).
where k = {sstart, send, ostart, oend} representsthe identiﬁer of each pointer; ˆyn,krefers to theeeprobability of n-th token being the start/end of thesubject/object.
yn,kis the ground truth from theeetraining data; if ∃s ∈ ti|r occurs in ci at positionfrom n to n + l, then yn,sstart= 1 and yn+l,send=ee1, otherwise 0; the same applies for the objects..ee.
8 ˆyj.
rc is parameterized by θ, omitted in the equation for.
clarity, below the same..35753.2 advantages.
our task formulation shows several advantages.
by adopting p3 as paradigm, the ﬁrst and fore-most advantage of our solution is that it suffersless from the imbalanced classes (section 2.2).
secondly, relation-level false negative is easy torecover.
when modeled as a standard classiﬁca-tion problem, many off-the-shelf methods on posi-tive unlabeled learning can be leveraged.
thirdly,entity-level false negatives do not affect relationclassiﬁcation.
taking s5 in figure 1 as an exam-ple, even though the birthplace relation betweenwilliam swartz and scranton is missing, therelation classiﬁer can still capture the signal fromthe other sample with a same relation, i.e., (cid:104) joebiden, birthplace, scranton (cid:105).
fourthly,this kind of modeling is easy to update with newrelations without the need of retraining a modelfrom bottom up.
only relation classiﬁer needs tobe redesigned, while entity extractor can be up-dated in an online manner without modifying themodel structure.
last but not the least, relationclassiﬁer can be regarded as a pruning step whenapplied to practical tasks.
many existing methodstreat relation extraction as question answering (liet al., 2019; zhao et al., 2020).
however, withoutﬁrst identifying the relation, they all need to iter-ate over all the possible relations and ask diversequestions.
this results in extremely low efﬁciencywhere time consumed for predicting one samplemay take up to |r| times larger than our method..4 our model.
the relational triple extraction task decomposed ineq.
(2) inspires us to design a two-staged pipeline,in which we ﬁrst detect relation at sentence leveland then extract subjects/objects for each relation.
the overall architecture of rere is shown in fig-ure 2..4.1 sentence classiﬁer with relational label.
rc, ˆy2.
rc, ..., ˆy|r|.
we ﬁrst detect relation at sentence level.
theinput is a sequence of tokens c and we denoteby ˆyrc = [ˆy1rc ] the output vector ofthe model, which aims to estimate ˆyirc in eq.
(3).
we use bert (devlin et al., 2019) for englishand roberta (liu et al., 2019) for chinese, pre-trained language models with multi-layer bidirec-tional transformer structure (vaswani et al., 2017),.
to encode the inputs9.
speciﬁcally, the input se-quence xrc = [[cls], ci, [sep]], which is fedinto bert for generating a token representationmatrix hrc ∈ rn ×d, where d is the hidden dimen-sion deﬁned by pre-trained transformers.
we takeh0rc, which is the encoded vector of the ﬁrst token[cls], as the representation of the sentence.
theﬁnal output of relation classiﬁcation module ˆyrc isdeﬁned in eq.
(5)..ˆyrc = σ(wrch0.
rc + brc),.
(5).
where wrc and brc are trainable model parame-ters, representing weights and bias, respectively; σdenotes the sigmoid activation function..4.2 relation-speciﬁc entity extractor.
after the relation detected at sentence-level, weextract subjects and objects for each candidate rela-tion.
we aim to estimate ˆyee = [0, 1]n ×4, of whicheach element corresponds to ˆyn,kin eq.
(4), usingeea deep neural model.
we take ˆyrc, the one-hot out-put vector of relation classiﬁer, and generate querytokens q using each of the detected relations (i.e.,the “1”s in ˆyrc).
we are aware that many recentworks (li et al., 2019; zhao et al., 2020) have stud-ied how to generate diverse queries for the givenrelation, which have the potential of achieving bet-ter performance.
nevertheless, that is beyond thescope of this paper.
to keep things simple, we usethe surface text of a relation as the query..next,.
the input sequence is constructed asxee = [[cls], qi, [sep], ci, [sep]].
like sec-tion 4.1, we get the token representation matrixhee ∈ rn ×d from bert.
the k-th output pointerof entity extractor is deﬁned by.
ee = σ(wkˆyk.
eehee + bk.
ee),.
(6).
where k ∈ {sstart, send, ostart, oend} is in accor-ee and bkdance to eq.
(4); wkee are the correspond-ing parameters..the ﬁnal subject/object spans are generatedby pairing the nearest sstart/ostart with send/oend.
next, all subjects are paired to the nearest object.
if multiple objects occur before the next subjectappears, all subsequent objects will be paired withit until next subject occurs..9for convenience, we refer to the pre-trained transformer.
as bert hereinafter..3576figure 2: the overall architecture of rere.
in this example, there are two relations, nationalityand creator,can be found in the relation classiﬁer, which will be sent to the entity extractor one by one along with the sentence.
when the relation nationality is extracted, the entity extractor will ﬁnd the position of the subject and objectof nationality.
the word american and dick dillin will be found.
the relation creator will then be handledsimilarly.
the values of grey blocks in ˆyee are zero..4.3 multi-label collective loss function.
in normal cases, the log-likelihood is taken as thelearning objective.
however, as is emphasized,there exist many false negative samples in the train-ing data.
intuitively speaking, the negative labelscannot be simply considered as negative.
instead, asmall portion of the negative labels should be con-sidered as unlabeled positives and their inﬂuencetowards the penalty should be eradicated.
there-fore, we adopt cpu (xie et al., 2020), a collectiveloss function that is designed for positive unlabeledlearning (pu learning).
to brieﬂy review, cpu con-siders the learning objective to be the correctnessunder a surrogate function,.
where they redeﬁne the correctness function forpu learning as.
c(ˆy, y) =.
(cid:40) e[ˆy].
if y = 1,1 − |e[ˆy] − µ| otherwise,.
(8).
where µ is the ratio of false negative data (i.e., theunlabeled positive in the original paper)..we extend it to multi-label situation by embody-ing the original expectation at sample level.
due tothe fact that class labels are highly imbalanced forour tasks, we introduce a class weight γ ∈ (0, 1)to downweight the positive penalty.
for relationclassiﬁer,.
(cid:96)rc(ˆy, y) =.
−γrc ln(.
if yi.
rc = 1.
− ln(1 − |.
ˆyirc − µrc|) otherwise..1|r|.
|r|(cid:88).
i=1.
ˆyirc]).
1|r|.
|r|(cid:88).
i=1.
(9).
for entity extractor,.
−γee ln(.
ˆyn,kee ]).
if yn,k.
ee = 1.
− ln(1 − |.
ˆyn,kee − µee|) otherwise..n(cid:88).
n=1.
n(cid:88).
n=1.
(10)in practice, we set µ = π(τ + 1), where τ ≈1 − # labeled positiveis the ratio of false negative andπ is the class prior.
note that µ is not difﬁcultto estimate for both relation classiﬁcation and en-tity extraction task in practice.
besides various.
# all positive.
.
.
.
.
(cid:96)(ˆy, y) = ln(c(ˆy, y)),.
(7).
(cid:96)ee(ˆyk, yk) =.
3577thecomicbookcharacteraurakleswascreatedbyamericanartistdickdillin.pre-trained transformer encoder[cls]pre-trained transformer encoder[cls][sep][sep]0100001000000[sep]creatornationality11111111sstartsendostartoendquery generationentity extractorrelation classifierof methods in the pu learning (du plessis et al.,2015; bekker and davis, 2018) for estimating it, aneasy approximation is µ ≈ π when π (cid:28) τ , whichhappens to be the case for our tasks..5 experiments.
5.1 datasets.
our experiments are conducted on these fourdatasets10.
some statistics of the datasets are pro-vided in table 1 and table 2. in relation extraction,some datasets with the same names involve differ-ent preprocessing, which leads to unfair compari-son.
we brieﬂy review all the datasets below andspecify the operations to perform before applyingeach dataset..• nyt (riedel et al., 2010).
nyt is thevery ﬁrst version among all the nyt-relateddatasets.
it is based on the articles in newyork times12.
we use the sentences from it toconduct the pilot experiment in table 1. how-ever, 1) it contains duplicate samples, e.g.,1504 in the training set; 2) it only labels thelast word of an entity, which will mislead theevaluation results..• nyt10-hrl.
& nyt11-hrl.
these twodatasets are based on nyt.
the difference isthat they both contain complete entity men-tions.
nyt10 (riedel et al., 2010) is the orig-inal one.
and nyt11 (hoffmann et al., 2011)is a small version of nyt10 with 53,395 train-ing samples and a manually labeled test setof 368 samples.
we refer to them as nyt10-hrl and nyt11-hrl after preprocessed byhrl (takanobu et al., 2019) where they re-moved 1) training relation not appearing inthe testing and 2) “na” sentences.
these twosteps are almost adopted by all the comparedmethods.
to compare fairly, we use this ver-sion in evaluations..• nyt21.
we provide relabel version of the testset of nyt11-hrl.
the test set of nyt11-hrl still have false negative problem.
mostof the samples in the nyt11-hrl has onlyone relation.
we manually added back themissing triples to the test set..• ske2019/ske2113.
ske2019 is a dataset inchinese published by baidu.
the reason wealso adopt this dataset is that it is currentlythe largest dataset available for relation ex-traction.
there are 194,747 sentences in thetraining set and 21,639 in the validation set.
we manually labeled 1,150 sentences from thetest set with 2,765 annotated triples, which werefer to as ske21.
no preprocessing for thisdataset is needed.
we provide this data forfuture research14..5.2 compared methods and metrics.
we evaluate our model by comparing with sev-eral models on the same datasets, which aresota graphical model multir (hoffmann et al.,2011), joint models sptree (miwa and bansal,2016) and noveltagging (zheng et al., 2017), re-cent strong sota models copyr (zeng et al.,2018b), hrl (takanobu et al., 2019), casrel (weiet al., 2020), tplinker (wang et al., 2020).
wealso provide the result of automatically aligningwikidata/cn-kbpedia with the corpus, namelymatch, as a baseline.
to note, we only keep theintersected relations, otherwise it will result in lowprecision due to the false negative in the originaldataset.
we report standard micro precision (prec.
),recall (rec.)
and f1 score for all the experiments.
following the previous works (takanobu et al.,2019; wei et al., 2020), we adopt partial matchon these data sets for fair comparison.
we alsoprovide the results of exact match results of themethods we implemented, and only exact match onske2019..5.3 overall comparison.
we show the overall comparison result in table 3.first, we observe that rere consistently outper-forms all the compared models.
we ﬁnd an inter-esting result that by purely aligning the databasewith the corpus, it already achieves surprisinglygood overall result (surpassing multir) and rel-atively high precision (comparable to cotype innyt11-hrl).
however, the recall is quite low,which is consistent with our discussion in sec-tion 2.1 that distant supervision leads to manyfalse negatives.
we also provide an ablation re-sult where bert is replaced with a bidirectional.
13http://ai.baidu.com/broad/download?.
10we do not use webnlg (gardent et al., 2017) andace0411 because these datasets are not automatically labeledby distant supervision.
webnlg is constructed by naturallanguage generation with triples.
ace04 is manually labeled..12https://www.nytimes.com/.
dataset=sked14download url..3578nyt10-hrl.
nyt11-hrl.
nyt21.
ske21.
prec.
rec..f1.
prec.
rec..f1.
prec.
rec..f1.
prec.
rec..f1.
38.10 32.38 34.97 47.92 31.0830.6-54.152.248.946.4.
32.852.246.9.
-49.259.3.
-55.738.1.
37.7 47.92 29.56 36.57 69.12 28.1-31.7-53.1-47.9.
---.
---.
---.
---.
39.96---.
kb matchmultir (hoffmann et al., 2011)sptree (miwa and bansal, 2016)noveltagging (zheng et al.,2017)cotype (ren et al., 2017)copyr (zeng et al., 2018b)hrl (takanobu et al., 2019)tplinker (wang et al., 2020)*casrel (wei et al., 2020)*.
---56.950.445.271.458.664.481.19 65.41 72.4573.068.877.7.
38.653.453.8.
43.042.153.8.
48.634.753.856.2 55.14 55.67 59.78 55.78 57.7153.9 58.64 56.62 57.6150.1.
58.4.
---.
---.
---.
-----.
--.
-----.
--.
-----.
--.
rere - lstmrere.
56.71 42.00 48.26 56.4635.4 43.52 62.06 37.01 46.3775.45 72.50 73.95 53.12 59.59 56.23 57.69 61.69 59.62.tplinker (wang et al., 2020)*(ex-act)casrel (wei et al., 2020)*(exact) 75.12 65.72 70.11 47.88 55.13 51.25 55.06 54.49 54.78 86.94 85.96 86.4573.4 52.40 58.91 55.47 56.97 60.93 58.88 90.44 84.20 87.21rere (exact).
80.34 65.11 71.93 55.43 55.12 55.28 58.96 55.78 57.33 83.86 84.77 84.32.
74.90 71.97.table 3: the main evaluation results of different models on nyt10-hrl, nyt11-hrl, and two hand labeled testsets nyt21 and ske21 on by the compared method on the datasets.
the results with only one decimal are quotedfrom (wei et al., 2020).
the methods with * are based on our re-implementation.
best partial (exact) match resultsare marked bold (underlined)..figure 3: precision-recall curve of rere and casrel under different false negative rates.
lines are better in theupper-right corner than the opposite.
note that the coordinates do not start from 0..lstm encoder (graves et al., 2013) with randomlyinitialized weights.
from the results we discoverthat even without bert, our framework achievescompetitive results against the previous approachessuch as cotype and copyr.
this further prove theeffectiveness of our rere framework..5.4 how robust is rere against false.
negatives?.
to further study how our model behaves when train-ing data includes different quantity of false nega-tives, we conduct experiments on synthetic datasets.
we construct ﬁve new training data by randomlyremoving triples with probability of 0.1, 0.3 and0.5, simulating the situation of different fn rates.
we show the precision-recall curves of our methodin comparison with casrel (wei et al., 2020),.
the best performing competitor, in figure 3.
1)the overall performance of rere is superior tocompetitor models even when trained on a datasetwith a 0.5 fn rate.
2) we show that the intervalsof rere between lines are smaller than casrel,indicating that the performance decline under dif-ferent fn rates of rere is smaller.
3) the straightline before curves of our model means that there isno data point at the places where recall is very low.
this means that our model is insensitive with thedecision boundary and thus more robust..6 conclusion.
in this paper, we revisit the negative data in rela-tion extraction task.
we ﬁrst show that the falsenegative rate is largely underestimated by previousresearches.
we then systematically compare three.
3579                 5 h f d o o       3 u h f l v l r q 1 < 7    + 5 / & d v 5 h o     & d v 5 h o     & d v 5 h o     5 ( 5 (     5 ( 5 (     5 ( 5 (                 5 h f d o o             3 u h f l v l r q 1 < 7    + 5 / & d v 5 h o     & d v 5 h o     & d v 5 h o     5 ( 5 (     5 ( 5 (     5 ( 5 (    commonly adopted paradigms and prove that ourparadigm suffers less from the overwhelming neg-ative labels.
based on this advantage, we proposerere, a pipelined framework that ﬁrst detect rela-tions at sentence level and then extract entities foreach speciﬁc relation and provide a multi-label pulearning loss to recover false negatives.
empiricalresults show that rere consistently outperformsthe existing state-of-the-arts by a considerable gap,even when learned with large false negative rates..acknowledgments.
project.
and development.
supported by national keythis work isresearch(no.
2020aaa0109302), shanghai science and tech-nology innovation action plan (no.19511120400)and shanghai municipal science and technologymajor project (no.2021shzdzx0103).
theauthors would like to thank the anonymousreviewers for their constructive comments..references.
jessa bekker and jesse davis.
2018. estimating theclass prior in positive and unlabeled data throughdecision tree induction.
in proceedings of aaai..kurt bollacker, colin evans, praveen paritosh, timsturge, and jamie taylor.
2008. freebase: a collabo-ratively created graph database for structuring humanknowledge.
in proceedings of sigmod..yee seng chan and dan roth.
2011. exploitingsyntactico-semantic structures for relation extraction.
in proceedings of acl, pages 551–560..danqi chen.
2018. neural reading comprehensionand beyond.
ph.d. thesis, stanford university..md.
faisal mahbub chowdhury and a. lavelli.
2012.impact of less skewed distributions on efﬁciency andeffectiveness of biomedical relation extraction.
inproceedings of coling..j. devlin, ming-wei chang, kenton lee, and kristinatoutanova.
2019. bert: pre-training of deep bidi-rectional transformers for language understanding.
in proceedings of naacl-hlt..jun feng, minlie huang, li zhao, yang yang, andxiaoyan zhu.
2018. reinforcement learning for rela-tion classiﬁcation from noisy data.
in proceedings ofaaai, volume 32..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017. creating trainingcorpora for nlg micro-planners.
in proceedings ofacl..matthew r gormley, mo yu, and mark dredze.
2015.improved relation extraction with feature-rich compo-sitional embedding models.
in proceedings of acl,pages 1774–1784..alex graves, abdel-rahman mohamed, and geoffreyhinton.
2013. speech recognition with deep recur-rent neural networks.
in 2013 ieee internationalconference on acoustics, speech and signal process-ing, pages 6645–6649..pankaj gupta, hinrich sch¨utze, and bernt andrassy.
2016. table ﬁlling multi-task recurrent neural net-work for joint entity and relation extraction.
in pro-ceedings of coling, pages 2537–2547..r. hoffmann, congle zhang, xiao ling, luke zettle-moyer, and daniel s. weld.
2011. knowledge-basedweak supervision for information extraction of over-lapping relations.
in proceedings of acl..wei jia, dai dai, xinyan xiao, and hua wu.
2019.arnor: attention regularization based noise reduc-tion for distant supervision relation classiﬁcation.
inproceedings of acl, pages 1399–1408..arzoo katiyar and claire cardie.
2017. going out on alimb: joint extraction of entity mentions and relationswithout dependency trees.
in proceedings of acl,pages 917–928..q. li and heng ji.
2014. incremental joint extractionof entity mentions and relations.
in proceedings ofacl..xiaoya li, xiaofei sun, yuxian meng, junjun liang,f. wu, and j. li.
2020. dice loss for data-imbalancednlp tasks.
in proceedings of acl..xiaoya li, fan yin, zijun sun, xiayu li, arianna yuan,duo chai, mingxin zhou, and j. li.
2019. entity-relation extraction as multi-turn question answering.
in proceedings of acl..hongyu lin, yaojie lu, xianpei han, and le sun.
2018.adaptive scaling for sparse detection in informationextraction.
in proceedings of acl, pages 1033–1043..yankai lin, shiqi shen, zhiyuan liu, huanbo luan, andmaosong sun.
2016. neural relation extraction withselective attention over instances.
in proceedings ofacl, pages 2124–2133..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..bonan min, ralph grishman, li wan, chang wang,and david gondek.
2013. distant supervision forrelation extraction with an incomplete knowledgebase.
in proceedings of hlt-naacl..mike mintz, steven bills, rion snow, and dan juraf-sky.
2009. distant supervision for relation extractionwithout labeled data.
in proceedings of acl..3580bo xu, yong xu, jiaqing liang, chenhao xie, binliang, wanyun cui, and y. xiao.
2017. cn-dbpedia: a never-ending chinese knowledge extrac-tion system.
in proceedings of iea/aie..wei xu, raphael hoffmann, le zhao, and ralph grish-man.
2013. filling knowledge base gaps for distantsupervision of relation extraction.
in proceedings ofacl, pages 665–670..bowen yu, zhenyu zhang, xiaobo shu, tingwen liu,yubin wang, bin wang, and sujian li.
2020. jointextraction of entities and relations based on a noveldecomposition strategy.
in proceedings of ecai..xiaofeng yu and wai lam.
2010. jointly identifyingentities and extracting relations in encyclopedia textvia a graphical model approach.
in proceedings ofcoling, pages 1399–1407..daojian zeng, kang liu, yubo chen, and jun zhao.
2015. distant supervision for relation extraction viain pro-piecewise convolutional neural networks.
ceedings of emnlp..daojian zeng, kang liu, siwei lai, guangyou zhou,jun zhao, et al.
2014. relation classiﬁcation viaconvolutional deep neural network.
in proceedingsof coling, pages 2335–2344..xiangrong zeng, shizhu he, kang liu, and jun zhao.
2018a.
large scaled relation extraction with rein-in proceedings of aaai, vol-forcement learning.
ume 32..xiangrong zeng, daojian zeng, shizhu he, kang liu,and jun zhao.
2018b.
extracting relational facts byan end-to-end neural model with copy mechanism.
in proceedings of acl..tianyang zhao, zhao yan, y. cao, and zhoujun li.
2020. asking effective and diverse questions: amachine reading comprehension based frameworkfor joint entity-relation extraction.
in proceedings ofijcai..suncong zheng, feng wang, hongyun bao, yuexinghao, peng zhou, and bo xu.
2017. joint extractionof entities and relations based on a novel taggingscheme.
in proceedings of acl, pages 1227–1236..makoto miwa and mohit bansal.
2016. end-to-endrelation extraction using lstms on sequences and treestructures.
in proceedings of acl, pages 1105–1116..makoto miwa and yutaka sasaki.
2014. modeling jointentity and relation extraction with table representa-tion.
in proceedings of emnlp, pages 1858–1869..marthinus christoffel du plessis, gang niu, andmasashi sugiyama.
2015. class-prior estimation forlearning from positive and unlabeled data.
machinelearning, 106:463–492..xiang ren, zeqiu wu, wenqi he, meng qu, clare rvoss, heng ji, tarek f abdelzaher, and jiawei han.
2017. cotype: joint extraction of typed entities andrelations with knowledge bases.
in proceedings ofwww, pages 1015–1024..s. riedel, limin yao, and a. mccallum.
2010. model-ing relations and their mentions without labeled text.
in proceedings of ecml/pkdd..roland roller, eneko agirre, aitor soroa, and markstevenson.
2015. improving distant supervision us-ing inference learning.
in proceedings of acl, pages273–278..cicero dos santos, bing xiang, and bowen zhou.
2015.classifying relations by ranking with convolutionalneural networks.
in proceedings of acl, pages 626–634..ryuichi takanobu, tianyang zhang, jiexi liu, and min-lie huang.
2019. a hierarchical framework for rela-tion extraction with reinforcement learning.
in pro-ceedings of aaai, volume 33, pages 7072–7079..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of neuroips, pages 6000–6010..denny vrandecic and m. kr¨otzsch.
2014. wikidata: afree collaborative knowledgebase.
communicationsof the acm, 57:78–85..shuohang wang and jing jiang.
2017. machine com-prehension using match-lstm and answer pointer.
inproceedings of iclr..yucheng wang, bowen yu, yueyang zhang, tingwenliu, hongsong zhu, and limin sun.
2020. tplinker:single-stage joint extraction of entities and relationsthrough token pair linking.
in proceedings of col-ing, pages 1572–1582..zhepei wei, jianlin su, yue wang, y. tian, andyi chang.
2020. a novel cascade binary taggingframework for relational triple extraction.
in pro-ceedings of acl..chenhao xie, qiao cheng, jiaqing liang, lihan chen,and y. xiao.
2020. collective loss function for posi-tive and unlabeled learning.
arxiv, abs/2005.03228..3581