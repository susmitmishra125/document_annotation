a knowledge-guided framework for frame identiﬁcationxuefeng su1,2, ru li1,3,∗, xiaoli li4, jeff z.pan5, hu zhang1, qinghua chai1, xiaoqi han11. school of computer and information technology, shanxi university, taiyuan, china2. school of e-commerce and logistics, shanxi vocational universityof engineering technology, taiyuan, china3. key laboratory of computational intelligence and chinese informationprocessing of ministry of education, shanxi university, taiyuan, china4. institute for infocomm research, a*star, singapore5. ilcc, school of informatics, university of edinburgh, uk{suexf,xiaoqisev}@163.com, {liru,zhanghu,charles}@sxu.edu.cnxlli@ntu.edu.sg, j.z.pan@ed.ac.uk.
abstract.
frame identiﬁcation (fi) is a fundamental andchallenging task in frame semantic parsing.
the task aims to ﬁnd the exact frame evokedit isby a target word in a given sentence.
generally regarded as a classiﬁcation task inexisting work, where frames are treated asdiscrete labels or represented using one-hotembeddings.
however, the valuable knowl-edge about frames is neglected.
in this pa-per, we propose a knowledge-guided frameidentiﬁcation framework (kgfi) thatinte-grates three types frame knowledge, includingframe deﬁnitions, frame elements and frame-to frame relations, to learn better frame rep-resentation, which guides the kgfi to jointlymap target words and frames into the same em-bedding space and subsequently identify thebest frame by calculating the dot-product sim-ilarity scores between the target word embed-ding and all of the frame embeddings.
the ex-tensive experimental results demonstrate kg-fi signiﬁcantly outperforms the state-of-the-art methods on two benchmark datasets..1.introduction.
frame identiﬁcation (fi) aims to ﬁnd the exactframe evoked by a target word in a given sentence.
a frame represents an event scenario, and possess-es frame elements (or semantic roles) that partici-pate in the event (hermann et al., 2014), which isdescribed in the framenet knowledge base (bak-er et al., 1998; ruppenhofer et al., 2016) ground-ed on the theory of frame semantics (fillmoreet al., 2002).
the theory asserts that people under-stand the meaning of words largely by virtue of theframes which they evoke.
in general, many wordsare polysemous and can evoke different frames indifferent contexts..as shown in figure 1, the word stopped e-vokes the frame activity stop and the frame.
∗corresponding author..figure 1: two annotated examples with the target wordmarked in bold and frame elements (semantic roles) inrounded rectangles.
the target word stopped (stop.vdenotes its form of lexical unit) evokes the frameactivity stop and the frame process stop respectivelyin different contexts.
here, the key to distinguish thesetwo frames is identifying whether the subject (the com-pony or the ﬁghting) of stopped is an agent or a pro-cess (see the frame deﬁnitions in table 1)..process stop respectively in two sentences.
it is achallenging task to distinguish the frames evokedby target words in sentences.
furthermore, fi isa key step before frame semantic role labeling(fsrl) (das et al., 2010, 2014; swayamdipta et al.,2017; kalyanpur et al., 2020) which is widely usedin event recognition (liu et al., 2016), machinereading comprehension (guo et al., 2020b,a), rela-tion extraction (zhao et al., 2020), etc.
through fiprocess, hundreds of role labels in framenet arereduced to a manageable small set (hartmann et al.,2017), which can signiﬁcantly improve the perfor-mance of fsrl models.
thus, fi is a fundamentaland critical task in nlp..fi is typically regarded as a classiﬁcation task,in which class labels are frame names.
in earlier s-tudies, researchers manually construct features andthen use supervised learning methods to learn clas-siﬁcation models (bejan and hathaway, 2007; jo-hansson and nugues, 2007; das et al., 2010, 2014).
these methods, however, do not take the valuablesemantic information about frames into considera-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5230–5240august1–6,2021.©2021associationforcomputationallinguistics5230thecompanystoppedproducingtheprofitabletoy.activity_stopstop.vagentactivityprocess_stopstop.vprocesstimethefightinghasstoppedformorethantwoyears.
frame: activity stop.
frame: process stop.
def an agent ceases an activity without completing it a process stops at a certain time and place.
fes.
core: agent, activityperipheral: degree, duration, manner, timeextra-thematic: depictive,purpose, result,....lus abandon.v, cease.v, halt.v, quit.v, stop.v, ....frs.
inherits from: process stopsubframe of: activityis inherited by: haltuses: eventive affecting.
core: processperipheral: manner, place, timeextra-thematic: depictive, duration, ...cease.v, halt.n, shutdown.n, stop.v,...inherits from: eventsubframe of: processis inherited by: activity stop.
table 1: the structured descriptions for frame activity stop versus frame process stop in framenet1.7.
thedescription of a frame is mainly composed of frame deﬁnition (def), frame elements (fes), lexical units (lus)and frame-to-frame relations (frs).
note that the elements of fes, lus and frs are partially listed due to thelimited space†.
lexical unit is expressed in the form of lemma.pos (e.g.
stop.v )..tion, and merely treat them as discrete labels..the recent studies of fi use distributed repre-sentations of target words and their syntactic con-text to construct features, and construct classiﬁca-tion models with deep neural network (hartmannet al., 2017; kabbach et al., 2018).
these meth-ods usually transform frame labels into one-hotrepresentations (hermann et al., 2014; t¨ackstr¨omet al., 2015), and then learn the embeddings of tar-get words and frames simultaneously.
however,the abundant semantic information and structureknowledge of frames contained in framenet arestill neglected..knowledge of frames deﬁned by linguists, suchas frame deﬁnition, frame elements and frame-to-frame relations, can enrich frame labels with richsemantic information that can potentially guide fimodels to learn more unique and distinguishablerepresentations.
thus, in this paper, we proposea knowledge guided frame identiﬁcation frame-work (kgfi) which consists of a bert-based con-text encoder and a frame encoder based on a spe-cialized graph convolutional network (framegc-n).
in particular, the frame encoder incorporatesmultiple types of frame knowledge into frame rep-resentation which guides the kgfi to jointly maptarget words and frames into the same embeddingspace.
instead of predicting the frame label directly,kgfi chooses the best suitable frame evoked bythe target word in a given sentence by calculatingthe dot-product similarity scores between the targetword embedding and all of the frame embeddings.
in summary, our contribution is threefold:.
• to the best of our knowledge, we are the.
†see the details in https://fn.icsi.berkeley.edu/fndrupal/.
ﬁrst to propose a uniﬁed fi method whichleverages heterogeneous frame knowledge forbuilding rich frame representations..• we design a novel framework kgfi, con-sisting of a bert-based context encoder and agcn-based frame encoder, which learns themodel from a combination of annotated da-ta and framenet knowledge base, and mapstarget words and frames into the same embed-ding space..• extensive experimental results demonstrateour proposed kgfi framework outperformsthe state-of-the-art models across two bench-mark datasets..2 framenet and fi task deﬁnition.
2.1 framenet.
framenet is built on the hypothesis that people un-derstand things by performing mental operations onwhat they already know (baker et al., 1998).
suchknowledge reﬂecting people’s cognitive experi-ence is described as structured information packetscalled frames.
a frame represents an event scenari-o, associated with a set of semantic roles (frameelements (fes)).
lexical units (lus) are capableof evoking the scenario (kshirsagar et al., 2015).
frame elements in terms of how central they are toa particular frame can be divided into three distin-guishing levels: core, peripheral and extra-thematic.
each frame has a textual deﬁnition (def), depict-ing the scenario and how the roles interact in thescenario.
frames are organized as a network withseveral kinds of frame-to-frame relations (frs)..5231figure 2: the overall architecture of kgfi..table 1 shows the structure of frame activity stopand frame process stop in framenet..3.1 context encoder.
2.2 fi task deﬁnition.
frame identiﬁcation (fi) is the task of predictinga frame evoked by the target word in a sentence.
let c=w0,w1,...,wst,...,wen,...,wn denote a given sen-tence, and t=wst,...,wen (t ⊂ c) represent the targetword, where st and en are the start and end indexrespectively for the target word t in the sentence.
let f = ( f1, f2, ..., f|f|) denote the set of all framesin framenet.
the fi model is deﬁned as a mappingfunction g : (c, t, st, en) → f j, subject to f j ∈ f..3 methodology.
table 1 illustrates the structured knowledge (def,fes, lus) of two different frames and their frame-to-frame relations (frs).
we explicitly leveragethem to enrich the frame embeddings with semanticinformation.
the resulted informative frame repre-sentations serve two purposes: 1) guide our modelto learn more distinguishable embeddings of targetwords, and 2) improve fi model’s generalizationperformance in the prediction phase..the proposed kgfi framework consists of threecomponents: context encoder, frame encoderand scoring module, as shown in figure 2. specif-ically, context encoder is used to represent thecontext-aware target word into an embedding witha bert-based module, and frame encoder is usedto incorporate three types of knowledge about aframe into frame embeddings.
with the guidanceof the knowledge about frames, two encoders joint-ly learn the embeddings of target words and frames.
finally, a scoring module is used to calculate thesimilarity scores between the given target word em-bedding and all frames’ embeddings, to identifythe best frame with the highest score..to get the context-aware embeddings of targetwords, we employ bert (devlin et al., 2019) for ourcontext encoder, since its architecture is a multi-layer bidirectional transformer which can aggre-gate information from context into the target wordthrough the self-attention mechanism.
as we know,bert model is pre-trained on a large corpus andcan transfer language knowledge into the contextencoder, which is very helpful for the target wordrepresentation as the manually labeled training dataof fi is very small..the context encoder, which we deﬁne as ec,takes given sentence c containing a target wordt as input.
we denote the last layer of bert’s outputas ht.
the context encoder can be expressed as :.
rt = ec(c, t, st, en) = w t.c ht + bc.
where.
ht =.
1en + 1 − st.en∑i=st.
(ht[i]),.
(1).
(2).
wc ∈ rn×mand bc ∈ rm are learned parameters..3.2 frame encoder.
in framenet, all the frames are connected into a di-rected graph through the frame-to-frame relations,as shown in figure 3. moreover, the graph convolu-tional network(gcn) (kipf and welling, 2017) hasbeen proved to be effective to model the relation-ship between labels (yan et al., 2019; chen et al.,2019; cheng et al., 2020; linmei et al., 2019), andit can enrich the representation of the node throughaggregating information from its neighbors.
in or-der to make better use of frame knowledge and theadvantage of gcn, we propose a specialized gcn,called framegcn, to incorporate multiple frameknowledge into frame representations..5232frameencoder(framegcn)bertbertdefgcncontextencoderbertmarkstoppedreadingandlinearscoreslabels[cls]activitystop:anagentceasesanactivitywithout...(cid:22127)...f1...1000...fifjfnf2f3...............fesgcnf1fifjfnf2f3...............(cid:22127)(cid:22127)bertbert(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)(cid:22127)deffeaturematrixvdfesfeaturematrixvertrfattentionnetworklinear××₊(cid:3)definitionfeatureextractorthe set of all frame elements in framenet, andve ∈ r|f|×|fe| denote the feature matrix of framesrepresented by fes.
the ith row of ve is the fea-ture vector of ith frame fi, and can be expressed asve[i, :] = (ve1, ve2, ..., ve|fe|), where.
(cid:40).
ve j =.
e j ∈ fe fi.
1,0, other.
,.
(4).
the sub-graph of overall graph offigure 3:framenet1.7 corresponding to frame activity stop andprocess stop.
the nodes denote frames and the direct-ed edges denote frame-to-frame relations.
the black”→”, red ”→” and blue ”→” denote inheritance, usingand subframe relations respectively, and the directionof an arrow is from super-frame to sub-frame..fe fi ⊂ fe is the fes of frame fi..fesgcn is used to learn a map function whichmaps the node (frame) vectors represented by festo a new representation via convolutional operationdeﬁned by a. we take a two-layer gcn to imple-ment the map function, which can be expressed as:.
3.2.1 structure of framegcnframegcn is a combination of two dedicated gc-ns (fesgcn and defgcn) and an attention net-work, as shown in figure 2. fesgcn is used torepresent frame by aggregating the fes featuresof its neighbors, while defgcn is used to repre-sent frame by aggregating the def features of itsneighbors.
the attention network is responsiblefor incorporating the outputs of two gcns intoone uniﬁed embedding where adjacent matrix a isshared by the two dedicated gcns..frame-to-frame relation in framenet is a asym-metric relation between two frames, where oneframe is called super-frame and the other is calledsub-frame, as shown in figure 3. a frame typ-ically obtains/inherits more information from itssuper-frame than from its sub-frame.
therefore, wedeﬁne the adjacent matrix of the graph as a weight-ed asymmetric matrix denoted as a = (ai j)|f|×|f|,where.
ai j =.
.
.
3,2,1,0, other.
f j = fif j is a super− f rame o f fif j is a sub− f rame o f fi.
..(3).
three types of frame-to-frames relations, includ-ing inherits, using and subframe, are used in thisstudy..3.2.2 fesgcnthe fes of a frame express its semantic rolesand structure.
frames which have similar struc-tures imply that they have close semantic, so weregard fes as features and use them to repre-sent frames.
let fe = (e1, e2, ..., e|fe|) denote.
e.),e (a,ve)w (1).
e (a,ve) = relu(avew (0)g(0)e (a,ve) = tanh(ag(0)g(1)here, w (0)e ∈ r|fe|×h is an input-to-hidden weightmatrix for the hidden layer and w (1)e ∈ rh×m is ahidden-to-output weight matrix..(5).
)..e.3.2.3 defgcnsince the frame deﬁnition is a short text that de-picts an event scenario and frame elements thatparticipate in the event, we employ bert as a fea-ture extractor to construct the feature matrix vd offrames.
speciﬁcally, we ﬁrst input a frame def-inition into bert, and subsequently take the ﬁrsttoken’s representation (corresponding to the input[cls] token) in bert’s last layer as the feature vec-tor of the frame.
since the name of a frame is alsomeaningful, we concatenate the frame name andframe deﬁnition into one string, e.g.
activity stop:an agent ceases an activity without completing it.
defgcn is used to learn a map function whichmaps the node (frame) vectors represented by def-inition to a new representation via convolutionaloperation deﬁned by a. we use a network similarto fesgcn, which can be expressed as:d (a,vd) = relu(avdw (0)g(0)d (a,vd) = tanh(ag(0)g(1).
),d (a,vd)w (1).
(6).
)..d.d.here, w (0)d ∈ rn×h is an input-to-hidden weightmatrix for a hidden layer with h feature maps, andw (1)e ∈ rh×m is a hidden-to-output weight matrix..3.2.4 attentive graph combinationwe use an attention network to dynamic incorpo-rate the outputs of fesgcn and defgcn into oneframe embedding through the attention weighting.
5233processprocessprocess_stopactivity_stophalthalteventive_affffeffctingegeventive_affectingeventeventactivityactivitymechanism.
the incorporation operation takes thefollowing function:.
r fi = ∑.
k∈{e,d}.
ai,kg(1).
k (a,vk)i.
(7).
where r fi ∈ rm is the embedding of ith frame,g(1)k (a,vk)i is the ith row of convolved represen-tation of graph k, and ai,k is a weight of ith frameagainst the graph k, which is computed as:.
ai,k =.
exp(wag(1)∑k(cid:48)∈{e,d} exp(wag(1).
k (a,vk)i).
k(cid:48) (a,vk(cid:48))i).
(8).
where wa ∈ rmis a learnable vector..3.3 scoring and prediction.
after obtaining the embeddings of target wordsand frames through context encoder and frame en-coder respectively, we score a target word t witheach frame f j ∈ f by computing the dot productsimilarity between rt and each r f for f j ∈ f:.
s(rt, r f j ) = rt.r f j , j = 1, 2, ..., |f|.
(9).
datasets train dev2272fn1.72284fn1.5.
1939116621.test67144428.
|f|12211019.
|fe|12851170.table 2: statistics for framenet datasets..ˆf = argmax f j∈ft s(rt, r f j ).
(12).
in the light of the coverage issue of framenet(see section 4.4), these two prediction functions(11 and 12) can be used in different circumstances.
in general, we can ﬁrst use lu to obtain candidateframe set ft by performing lexicon ﬁltering andthen use function 12 to identify best frame from ft.however, if we can not ﬁnd any candidate frameusing lus, i.e.
ft = /0, then we have to identifybest frame from f using function 11. note that ftonly contains a couple of candidate frames, whilef contains more than one thousand of frames.
thisrequires fi models have very good generalizationperformance to handle a big f set..during training, all model parameters are jointlylearned by minimizing a cross-entropy loss:.
4.1 datasets.
4 experimental settings.
l(θ ) = −.
yi jlog( ˆyi j).
(10).
1|d|.
|d|∑i.
|f|∑j.where d is the number of the training data, |f| isthe total number of frames in framenet, yi j (one-hot representation of frame labels) and ˆyi j are truelabels.
the predicted probability over frames iscalculated by the softmax function over the scores.
during prediction, we predict the frame evokedby the target word t to be f j ∈ f, whose representa-tion r f j has the highest score with rt.
the predictionfunction is deﬁned as:.
ˆf = argmax f j∈f s(rt, r f j ).
(11).
note most of the frames contain a set of lex-ical units (lus) in the form of lemma.pos (e.g.
stop.v).
as shown in table 1, the lus of the frameactivity stop and the frame process stop are listedin the fourth row.
therefore, we adopt the lexiconﬁltering operation to reduce the possible candidateframe set.
firstly, we utilize lemmatization andpos tools to convert the target word t into the formof lu (e.g.
stop.v).
secondly, we use this lu tomatch the frames whose lus contains this lu, andthen use the matched frames as the possible candi-date frame set ft for the target word t. at last, wepredict the frame label by the following function:.
we have employed two knowledge bases,i.e.
framenet1.5 and framenet1.7.
both of them con-tain various documents which have been annotatedmanually, including target words and correspond-ing evoked frames.
documents and correspond-ing annotations in framenet1.7 are extended fromframenet 1.5 and thus are more complete.
notetrain, dev and test documents in both data have beenpartitioned following (swayamdipta et al., 2017).
given a sentence in documents may contain mul-tiple target words, we regard it as multiple pairsof target word and sentence in train, dev and testsets.
the statistics of two datasets are illustrated intable 2..to test the model’s performance on the morechallenging ambiguous data, following the previ-ous studies, we constructed a specialized datasetby extracting pairs of target word and annotatedsentence from test data, in which the target wordsare polysemous or can evoke multiple frames..4.2 baselines.
we ﬁrst compare the kgfi against ﬁve existingmodels.
semafor (das et al., 2014) is a condi-tional log-linear model which uses statistical fea-tures about target word to predict the frame la-bel.
hermann-14 (hermann et al., 2014) is a.
5234joint learning model which maps frame labels andthe dependency path of target word into a com-mon embedding space.
simpleframeid (hart-mann et al., 2017) models a classiﬁer based on theembeddings of entire words in the sentence.
open-sesame (swayamdipta et al., 2017) models a clas-siﬁer based on bi-directional lstm.
hermann-14 converts frame labels into onehot embeddings,while other models treat frame labels as discrete su-pervision signals.
peng’s model (peng et al., 2018)is a joint learning model for fi and fsrl, whichboth uses exemplars in framenet knowledge baseand the full-text annotation training data to trainthe model..in addition, we also implemented two additionalbert-based baselines for fair comparison.
one iscalled bert-cls that uses bert to represent the targetword in a sentence and treats discrete frame labelsas supervision signals.
the other is called bert-onehot, which also uses the dual-encoder archi-tecture (context encoder and frame encoder) andmaps target words and frames into a common em-bedding space.
the difference between kgfi andbert-onehot is that kgfi uses gcn-based mod-ules to incorporate frame knowledge into frameembeddings, while bert-onehot uses a linear net-work to map onehot vector of frame labels intoframe embeddings without incorporating knowl-edge.
clearly, we will test if the knowledge playsa signiﬁcant role for better frame embeddings andsubsequent fi task..4.3 parameter settings.
all bert modules in kgfi were initialized withbert-base.
we set both the dimensions of targetword embedding rt and frame embedding r f to128 (m=128), the hidden layer size of fesgcnand defgcn to 256 (h=256).
the size of bertembedding is n=768.
the dimensions of fes andfrs feature vectors are related to framenet version(see table 2).
for optimization, we use bertadamoptimizer and set learning rate to 5e − 5. as forparameter tuning, our parameters are tuned usingthe development set with the early stop strategy..4.4 test settings.
framenet has a few coverage issues in that: (1) thelus set is incomplete for some frames; (2) manywords that should evoke frames are not included inlus set of frames.
thus, we design two types oftest settings: test without lexicon ﬁltering, or test.
fn 1.7.fn 1.5.modelssemaforhermann-14simpleframeidopen-sesamepeng’s model*bert-clsbert-onehotkgfi(1-layer)kgfi(2-layers)max (cid:52).
all--83.0086.5589.1090.1790.5791.7192.401.83.amb--71.7072.4077.5079.8780.6682.9884.413.75.all83.6088.4187.6386.4090.0090.1391.4692.1391.910.67.amb69.1973.1073.8072.8078.0078.3280.7882.3481.841.56.table 3: frame identiﬁcation accuracy with lexiconﬁltering setting on framenet test dataset.
’all’ and’amb’ denote testing on test data and on ambiguousdata respectively.
’max (cid:52)’ denotes the accuracy differ-ence between our best kgfi model and the strongestbaseline bert-onehot.
’*’ denotes the training data andexemplars in framenet are both used in training phase..that does not use lus (use fun 11) and test withlexicon ﬁltering, or test that uses lus (use fun 12)..5 evaluation.
5.1 overall results.
the overall testing results, as shown in table 3,demonstrate that bert-cls and bert-onehot are t-wo strong baselines, outperforming all of the priorwork that does not incorporate pre-training mod-ules into their systems.
bert-onehot slightly out-performs bert-cls in all of the testing settings, in-dicating joint learning target word embedding andframe embedding is helpful for fi task..our best kgfi models, including kgfi (2-layers) for framenet1.7 and kefi (1-layer) forframenet1.5, outperform all the baseline model-s of fi in terms of accuracy.
compared with thestronger bert-onehot model, our model achievesabsolute 1.83% and 0.67% improvements on twodatasets respectively in all test setting.
with thehelp of lexicon ﬁltering with lus in framenet, themodel predicts the exact frame evoked by the targetword among a small set of candidate frames.
clear-ly, the improvements are credited to the model’sperformance improvement in predicting frames forambiguous target words, since the model achievesabsolute 3.75% and 1.56% improvements in ambtest setting on two datasets respectively..to the best of our knowledge, few previous workfocus on frame prediction without lexicon ﬁltering.
5235fn 1.7.fn 1.5.modelssimpleframeidbert-onehotkgfi(1-layer)kgfi(2-layers)max (cid:52).
all76.1080.0984.9585.815.72.amb-75.2979.7880.665.37.all77.4982.0085.6385.003.63.amb-76.1180.0779.663.96.table 4: frame identiﬁcation accuracy without lexiconﬁltering on framenet test dataset.
’all’ and ’amb’denote testing on test data and on ambiguous data re-spectively.
’max (cid:52)’ denotes the accuracy differencebetween our best kgfi model and the strongest base-line bert-onehot..modelsbert-onehotkgfi(2-layer).
top-180.0985.81.top-287.1790.22.top-388.9691.59.top-590.1292.88.table 5: top-k accuracy of frame identiﬁcation with-out lexicon ﬁltering on framenet1.7..except for simpleframeid model, so we choosesimpleframeid and the stronger bert-onehot mod-el as our baseline to compare our best model’sperformance under no-lexicon ﬁlter setting.
asshown in table 4, in comparison with the strongerbert-onehot model, our model achieves absolute5.72% and 3.63% improvements on two datasetsrespectively in all setting (without using lus andcompared with more than 1000 frames), signify-ing the generalization performance of our modelachieves signiﬁcant improvement, considering thatthe model predicts the exact frame evoked by thetarget word among all the frames without knowingthe possible candidate frames of the target word inno-lexicon ﬁltering setting..to further test the performance of our best kgfimodel, we use the top-k accuracy to measure themodel performance without lexicon ﬁltering.
thehigher top-k accuracy indicates that the model haslearned better frame representations.
furthermore,the model can reduce the candidate frame set intoa small frame subset (containing k most possibleframes), which is useful for the downstream tasks,such as lus induction for framenet, fsrl, etc.
asshown in table 5, compared with bert-onehot base-line, our best kgfi model achieves higher top-k(k=1,2,3,5) accuracy, which further demonstratesthe model has learned the better frame representa-tion through incorporating the frame knowledge..modelsbert-onehotkgfi(w/ framegcn).
w/ defgcnw/ fesgcnw/o attention.
all-l all-nl80.0990.5785.8192.4082.1091.4985.0092.0185.1992.31.table 6: ablation analysis on framenet1.7 dataset inall-l and all-nl setting.
the sign ’w/’ and ’w/o’ de-note that the kgfi is constructed with and without thecorresponding module respectively.
’-l’ and ’-nl’ de-note testing with and without lexicon ﬁltering respec-tively..considering framenet1.5 dataset is relativelysmall, the performance of simple structure model(using 1-layer gcn) achieves the best performance,while the performance of the model using 2-layersgcn drops slightly.
in general, no matter howmany layers are adopted, our models outperformall the baselines and achieve the best performanceon two datasets in all settings consistently..5.2 ablation studies.
to test the function of each component in kgfi,we conduct the ablation study.
as shown in ta-ble 6, the results demonstrate that all of the threecomponents, i.e.
defgcn, fesgcn and attentionnetwork, are helpful for enhancing the model’s per-formance.
even with defgcn or fesgcn individ-ually, the performance of our model is still betterthan the stronger baseline bert-onehot, which in-dicates the frame deﬁnition, fes and frs are alluseful knowledge for frame representation, andour proposed gcn-based model architecture is ef-fective to incorporate them into the informativeembeddings.
compared with frame deﬁnition, fesare more useful for frame representation, since theperformance of gkfi (with fesgcn) outperformskgfi (with defgcn), although it slightly lags be-hind kgfi full model (with framegcn).
note thatthe attention module is removed when defgcn orfesgcn is used as the frame encoder..as for the attention module, the performance ofkgfi (with framegcn) drops when we replace itwith a simple addition operation, suggesting it isnecessary to use attention mechanism to integratethe outputs of defgcn and fesgcn..5.3 weighting method for adjacent matrix.
to test the rationality of our proposed weightingmethod for adjacent matrix a, we conduct a set of.
5236modelskgfi(w/ framegcn) w 92.4kgfi(w/ framegcn) b 91.80kgfi(w/ defgcn)w 91.49kgfi(w/ defgcn)b 91.26kgfi(w/ fesgcn)w 92.01kgfi(w/ fesgcn)b 91.78.a all-l all-nl85.8183.1082.1081.1185.0082.10.table 7: the results of kgfi models on framenet1.7dataset under different value settings of adjacent matrixa.
’w’ and ’b’ denote the matrix a is weighted andbinary respectively.
’-l’ and ’-nl’ denote testing withand without lexicon ﬁltering respectively..comparison experiments, in which the weightedmatrix is replaced with a binary matrix.
binarymatrix is widely used approach to express the re-lations between nodes in graph modeling.
ourweighting method expresses the hierarchy relation-ships between frames straightforwardly.
the re-sults demonstrate that the weighted method hassigniﬁcant impact on the model’s performance, andour proposed weighting method for adjacent matrixis quite reasonable, since the performance of all themodels using weighted matrix outperforms theircounterparts using binary matrix, shown in table 7..5.4 case studies.
figure 4 shows that kgfi (w/fesgcn) modeltends to predict correct frame by ﬁnding the se-mantic relatedness between fes and the contextof target word.
for instance, in sentence 1), thetarget word stopped may evoke activity stop orprocess stop, and the phrase the ﬁghting is thekey to distinguish two frames evoked by the wordstopped, since these two frames differ in that thesubject of stopped is an agent or a process.
ourkgfi(w/fesgcn) model has learned the seman-tic relation between the ﬁghting phrase and feprocess, and outputs the correct frame, since feagent is related to an entity in general.
the bert-onehot model can’t grasp this relation, so it out-puts a wrong prediction activity stop.
on the oth-er hand, the kgfi(w/ defgcn) model tends topredict the frame with the semantic similaritiesbetween frame deﬁnition and the sentence.
forinstance, in sentence 2), the word traversing indeﬁnition is similar to phrase passed through, sothe model outputs the correct frame traversing..in sentence 3), the kgfi(w/ defgcn) modeloutputs a wrong prediction quitting a place due.
figure 4: the case studies of our proposed modelsand bert-one baseline.
a, b, c and d denote the pre-dicted frames of the following fi models: bert-onehot,kgfi(w/ defgcn), kgfi(w/ fesgcn) and kgfi(w/framegcn).
the correct frames are marked in blue.
the target words are in bold in each sentence..to the similar meaning of the word depart in thesentence and the word leaves in the frame deﬁ-nition (quitting a place: a self mover leaves aninitial source location.).
the kgfi(w/ fesgc-n) model, on the other hand, has learned that theword ferries in the sentence is more closely relatedto fe theme of frame departing (departing: atheme moves away from a source.)
rather than feself mover of frame quitting a place, and outputsthe correct frame departing, since the self movergenerally refers to a living object (e.g.
a person, ananimal).
note that the frame departing is inher-ited by the frame quitting a place, so they havenearly the same fes set except for fe theme andfe self mover.
in other words, our kgfi(w/ de-fgcn) and kgfi(w/ fesgcn) are complementaryto each other to some extent.
kgfi(w/ fesgcn)can capture the subtle differences between differ-ent frames, even if the frames have close frame-to-frame or semantic relations..the case studies show that kgfi models can in-corporate frame knowledge into its representationsand guide the context encoder to learn the seman-tic relations between frames and the context-awarerepresentations of target words and frames throughjoint learning..6 related work.
researchers have made great effort to tackle the fiproblem since it has been proposed in the semeval-2007 (baker et al., 2007).
it is generally regardedas a classiﬁcation task.
the best system (johanssonand nugues, 2007) in the semeval-2007 adopt-ed svm to learn the classiﬁer to identify frameswith a set of features, such as target lemma, targetword, and so on.
semafor (das et al., 2014) uti-.
52371)thefightinghasstoppedformorethantwoyears.fesofprocess_stop :(process,..)fesofactivity_stop:(agent,...)2)stevepassedthroughtheromeairportcustoms?traversing:athemechangeslocationwithrespecttoasalientlocation.3)ferriesdepartfromcentraltosilverminebay.fesofdeparting:(theme,source,goal,..)oa.motion,b.quitting_a_place,c.departing,d.departinga.motion,b.traversing,c.departing,d.traversinga.activity_stop,b.process_stop,c.process_stop,d.process_stop(cid:284)×lized a conditional model that shares features andweights across all targets, frames, and prototypes.
these approaches use manually designed featuresand traditional machine learning methods to learnthe classiﬁcation models, while the class labels assupervision signals are discrete frame names..recently, distributed feature representation andmodels based on neural network are used to tacklefi.
according to the model architecture, there aretwo trends of work.
one is joint learning approachthat converts the discrete frame labels into continu-ous embedding by learning the embeddings of tar-get words and frames simultaneously.
for instance,hermann-14 (hermann et al., 2014) implementeda model that jointly maps possible frame labels andthe syntax context of target words into the samelatent space using the wsabie algorithm, and thesyntax context was initialized with concatenatingtheir word embeddings.
simpleframeid (hartman-n et al., 2017) useed the concatenation of sentbow(the average of embeddings of all the words in thesentence) to represent the context and then learnsthe common embedding space of context and framelabels following the line of (hermann et al., 2014).
the other trend is to construct the classiﬁer mod-el using deep neural network and regard discreteframe labels as supervision signals, which is similarto those earlier work.
open-sesame (swayamdiptaet al., 2017) used a bidirectional lstm to constructthe fi classiﬁer.
peng (peng et al., 2018) proposeda joint learning model for fi and fsrl, whichadopted a multitask model structure..different from previous studies, this paper fo-cuses on how to represent frames by incorporatingframe knowledge into frame representations andenriching frame labels with semantic information..7 conclusion.
in this work, we propose a novel idea that lever-ages frame knowledge, including frame deﬁnition,frame elements and frame-to-frame relations, to im-prove the model performance of fi task.
our pro-posed kgfi framework mainly consists of a bert-based context encoder and a gcn-based frameencoder which can effectively incorporate multipletypes of frame knowledge in a uniﬁed frameworkand jointly map frames and target words into thesame semantic space.
extensive experimental re-sults demonstrate that all kinds of knowledge aboutframes are useful for enriching the representationof frames, and the better frame representation is.
helpful for fi task.
the experimental results alsoshow that the proposed model achieves signiﬁcant-ly better performance than seven state-of-the-artmodels across two benchmark datasets..acknowledgments.
we thank the anonymous reviewers for their helpfulcomments and suggestions.
this work was support-ed by the national natural science foundation ofchina (no.61936012, no.61772324) and the openproject foundation of intelligent information pro-cessing key laboratory of shanxi province (no.
cicip2018007)..references.
collin f. baker, michael ellsworth, and katrin erk.
2007. semeval-2007 task 19: frame semantic struc-in proceedings of the 4th inter-ture extraction.
national workshop on semantic evaluations, se-meval@acl 2007, prague, czech republic, june23-24, 2007, pages 99–104.
the association forcomputer linguistics..collin f. baker, charles j. fillmore, and john b. lowe.
in 36th an-1998. the berkeley framenet project.
nual meeting of the association for computation-al linguistics and 17th international conference oncomputational linguistics, coling-acl ’98, au-gust 10-14, 1998, universit´e de montr´eal, montr´eal,quebec, canada.
proceedings of the conference,pages 86–90.
morgan kaufmann publishers / acl..cosmin adrian bejan and chris hathaway.
2007.utd-srl: a pipeline architecture for extractingin proceedings of theframe semantic structures.
4th international workshop on semantic evaluation-s, semeval@acl 2007, prague, czech republic,june 23-24, 2007, pages 460–463.
the associationfor computer linguistics..zhao-min chen, xiu-shen wei, peng wang, and yan-wen guo.
2019. multi-label image recognition withgraph convolutional networks.
in proceedings of theieee/cvf conference on computer vision and pat-tern recognition (cvpr)..xingyi cheng, weidi xu, kunlong chen, shaohuajiang, feng wang, taifeng wang, wei chu, andyuan qi.
2020. spellgcn: incorporating phonolog-ical and visual similarities into language models forchinese spelling check.
in proceedings of the 58thannual meeting of the association for computation-al linguistics, pages 871–881, online.
associationfor computational linguistics..dipanjan das, desai chen, andr´e f. t. martins,nathan schneider, and noah a. smith.
2014. frame-semantic parsing.
comput.
linguistics, 40(1):9–56..5238dipanjan das, nathan schneider, desai chen, andnoah a. smith.
2010. probabilistic frame-semanticin human language technologies: con-parsing.
ference of the north american chapter of the asso-ciation of computational linguistics, proceedings,june 2-4, 2010, los angeles, california, usa, pages948–956.
the association for computational lin-guistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, naacl-hlt 2019, minneapolis, mn, us-a, june 2-7, 2019, volume 1 (long and short paper-s), pages 4171–4186.
association for computationallinguistics..charles j. fillmore, collin f. baker, and hiroaki sato.
2002. the framenet database and software tools.
in proceedings of the third international confer-ence on language resources and evaluation, lrec2002, may 29-31, 2002, las palmas, canary islands,spain.
european language resources association..shaoru guo, yong guan, ru li, xiaoli li, and hongyetan.
2020a.
incorporating syntax and frame seman-tics in neural network for machine reading compre-in proceedings of the 28th internation-hension.
al conference on computational linguistics, pages2635–2641, barcelona, spain (online).
internation-al committee on computational linguistics..shaoru guo, ru li, hongye tan, xiaoli li, yongguan, hongyan zhao, and yueping zhang.
2020b.
a frame-based sentence representation for machinereading comprehension.
in proceedings of the 58thannual meeting of the association for computation-al linguistics, acl 2020, online, july 5-10, 2020,pages 891–896.
association for computational lin-guistics..silvana hartmann, ilia kuznetsov, teresa martin, andiryna gurevych.
2017. out-of-domain framenet se-in proceedings of the 15thmantic role labeling.
conference of the european chapter of the associa-tion for computational linguistics, eacl 2017, va-lencia, spain, april 3-7, 2017, volume 1: long pa-pers, pages 471–482.
association for computationallinguistics..karl moritz hermann, dipanjan das, jason weston,and kuzman ganchev.
2014. semantic frame iden-tiﬁcation with distributed word representations.
inproceedings of the 52nd annual meeting of the as-sociation for computational linguistics, acl 2014,june 22-27, 2014, baltimore, md, usa, volume 1:long papers, pages 1448–1458.
the association forcomputer linguistics..richard johansson and pierre nugues.
2007. lth: se-mantic structure extraction using nonprojective de-in proceedings of the 4th inter-pendency trees..national workshop on semantic evaluations, se-meval@acl 2007, prague, czech republic, june23-24, 2007, pages 227–230.
the association forcomputer linguistics..alexandre kabbach, corentin ribeyre, and aur´elieherbelot.
2018. butterﬂy effects in frame semanticparsing: impact of data processing on model ranking.
in proceedings of the 27th international conferenceon computational linguistics, coling 2018, san-ta fe, new mexico, usa, august 20-26, 2018, pages3158–3169.
association for computational linguis-tics..aditya kalyanpur, or biran, tom breloff, jenniferchu-carroll, ariel diertani, owen rambow, andmark sammons.
2020. open-domain frame se-corr, ab-mantic parsing using transformers.
s/2010.10998..thomas kipf and m. welling.
2017. semi-supervisedclassiﬁcation with graph convolutional networks.
arxiv, abs/1609.02907..meghana kshirsagar, sam thomson, nathan schnei-der, jaime g. carbonell, noah a. smith, and chrisdyer.
2015. frame-semantic role labeling with het-erogeneous annotations.
in proceedings of the 53rdannual meeting of the association for computation-al linguistics and the 7th international joint confer-ence on natural language processing of the asianfederation of natural language processing, acl2015, july 26-31, 2015, beijing, china, volume 2:short papers, pages 218–224.
the association forcomputer linguistics..hu linmei, tianchi yang, chuan shi, houye ji, andxiaoli li.
2019. heterogeneous graph attention net-works for semi-supervised short text classiﬁcation.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4821–4830, hong kong, china.
association for computa-tional linguistics..shulin liu, yubo chen, shizhu he, kang liu, and junzhao.
2016. leveraging framenet to improve auto-in proceedings of the 54thmatic event detection.
annual meeting of the association for computation-al linguistics, acl 2016, august 7-12, 2016, berlin,germany, volume 1: long papers.
the associationfor computer linguistics..hao peng, sam thomson, swabha swayamdipta, andnoah a. smith.
2018. learning joint semanticin proceedings of theparsers from disjoint data.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long paper-s), pages 1492–1502, new orleans, louisiana.
asso-ciation for computational linguistics..josef ruppenhofer, michael ellsworth, miriam r.l.petruck, christopher r. johnson, collin f. baker,.
5239and jan scheffczyk.
2016. framenet ii: extendedtheory and practice..swabha swayamdipta, sam thomson, chris dyer, andnoah a. smith.
2017. frame-semantic parsing withsoftmax-margin segmental rnns and a syntactic scaf-fold.
corr, abs/1706.09528..oscar t¨ackstr¨om, kuzman ganchev, and dipanjan das.
2015. efﬁcient inference and structured learning forsemantic role labeling.
trans.
assoc.
comput.
lin-guistics, 3:29–41..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated atten-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5766–5770, hong kong, china.
association forcomputational linguistics..hongyan zhao, ru li, xiaoli li, and hongye tan.
cfsre: context-aware based on frame-2020.semantics for distantly supervised relation extrac-tion.
knowl.
based syst., 210:106480..5240