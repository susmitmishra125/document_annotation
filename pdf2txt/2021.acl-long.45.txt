cascaded head-colliding attention.
lin zheng♠♦ zhiyong wu♠ lingpeng kong♠♣♠department of computer science, the university of hong kong♦school of data and computer science, sun yat-sen university♣shanghai artiﬁcial intelligence laboratoryzhenglin6@mail2.sysu.edu.cn,{zywu,lpk}@cs.hku.hk.
abstract.
transformers have advanced the ﬁeld of natu-ral language processing (nlp) in many ways.
at the heart of the transformer architectureis the multi-head attention (mha) mechanismwhich models pairwise interactions betweenthe elements of the sequence.
despite its mas-sive success, the current framework ignores in-teractions among different heads, leading tothe problem that many of the heads are redun-dant in practice, which underutilizes the ca-pacity of the model.
to improve parameterefﬁciency, we re-formulate the mha as a la-tent variable model from a probabilistic per-spective.
we present cascaded head-collidingattention (coda) which explicitly models theinteractions between attention heads through ahierarchical variational distribution.
we con-duct extensive experiments and demonstratethat coda outperforms the transformer base-line, by 0.6 perplexity on wikitext-103in language modeling, and by 0.6 bleu onwmt14 en-de in machine translation, due toits improvements on the parameter efﬁciency.1.
1.introduction.
transformers (vaswani et al., 2017) have advancedthe ﬁeld of natural language processing (nlp) on avariety of important tasks, including language mod-eling (dai et al., 2019; baevski and auli, 2019),language understanding (devlin et al., 2019; yanget al., 2019b), and machine translation (vaswaniet al., 2017; dehghani et al., 2019; liu et al.,2020).
it has also found its place in computer vi-sion (dosovitskiy et al., 2020), and in intelligentagents (vinyals et al., 2019) where sequence mod-eling plays a key role as well.
the cornerstone ofthe transformer architecture is the multi-head at-tention (mha) mechanism which models pairwiseinteractions between the elements of the sequence..1our implementation is publicly available at https://.
github.com/lzhengisme/coda..an attention function can be described as mappinga query and a set of key-value pairs to an output,where the query, keys, values, and output are allvectors.
the output is computed as a weightedsum of the values, where the weight assigned toeach value is computed by a compatibility func-tion of the query with the corresponding key.
amulti-head attention (mha) mechanism extendsthe idea through performing multiple separately pa-rameterized attention functions acting in parallel tocontextualize the input representations.
their out-puts are then gathered by an afﬁne transformation,allowing the model to jointly attend to informationfrom different representation subspaces at differentpositions..despite its massive success, the current frame-work ignores the interactions among differentheads, leading to the problem that many of theheads are redundant in practice (i.e., attending tothe same regions of the sequence), which under-utilizes the capacity of the model (voita et al.,2019; michel et al., 2019a).
at the same time,recent research (tang et al., 2018; clark et al.,2019; voita et al., 2019; wu et al., 2020, interalia) demonstrates that heads in mha have the po-tential to capture distinct information from inputsequences, ranging from syntactic and semanticfeatures to alignment information between sourceand target sentence pairs.
these observations sug-gest that multiple heads should be encouraged toextract complementary information.
therefore, itis highly appealing to take into account the inter-actions among different attention heads from theperspective of parameter efﬁciency and the expres-siveness of the model..in this work, we introduce head-colliding atten-tion (§3).
we formulate mha as a probabilisticmodel, where each attention head is representedby a latent variable and all of them collide intothe observed sequence data (figure 1a).
in this.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages536–549august1–6,2021.©2021associationforcomputationallinguistics536probabilistic graphical model structure, attentionheads work as individual factors to explain the data.
although each factor is independent of each othera priori, they interact with each other automati-cally, conditioning on observations, thanks to theexplaining-away effects (pearl, 1989; wellman andhenrion, 1993)..the head-colliding attention mechanism intro-duces new computational challenges in trainingthe model.
we will discuss how we tackle theseusing variational methods (blei et al., 2017).
wepropose cascaded head-colliding attention (coda,figure 1b).
as our main model, coda adopts a hi-erarchical variational distribution (ranganath et al.,2016) to allow both rich head interactions and ef-fective computations (§4)..we validate our method in language modelingand machine translation experiments (§5).
codaoutperforms the vanilla mha transformer on bothtasks, on wikitext-103 by 0.6 perplexity andon wmt14 en-de by 0.6 bleu.
further analysisshows that coda learns to encourage diversity indifferent heads (figure 2) and to promote parameterefﬁciency when increasing the number of heads(§5.3)..2 background.
multi-head attention (mha) mechanism plays animportant role in modern transformer architecture(vaswani et al., 2017).
it extends the classical at-tention mechanism by running multiple attentionfunction heads in parallel..an mha module is composed of h identicalblocks (usually referred to as attention heads).
each head will generate a hidden state hi based onthe input query, key and value matrices, denotedas q, k, and v respectively.
the hidden statesfrom different heads are then aggregated as theoutput of the mha module: (cid:80)ni , wherew oi are model parameters.
in the i-th head, the input matrices q, k and vare ﬁrst linearly projected into different subspacerepresentations (cid:101)qi, (cid:102)ki, and (cid:101)vi, based on differentlearnable parameters.
after that, we compute theinner product over all projected queries and keysas the attention logits zi, which are then passedthrough a row-wise softmax2 to obtain head atten-tion weights ai:.
i=1 hiw o.ai = softmax(zi) = softmax( (cid:101)qi (cid:102)kt.
i )..(1).
2we omit the scaling factor for simplicity..the ﬁnal output of a single attention block is the.
weighted sum of (cid:101)vi:.
hi = ai (cid:101)vi..as we can see, the core of mha is to calculateai in each head.
we thus refer to ai as the i-thattention head..in sequence prediction tasks, the model takesas input a source sequence of length m and out-puts a target sequence of length n in an auto-regressive manner.
it predicts each token y withinthe target sequence through a categorical distribu-tion pvanilla(y|x), where x includes the sourcesequence as well as a previously generated pre-ﬁx.
with respect to an mha block a1, .
.
.
, ah,the model predicts target tokens y by ﬁrst feed-ing these heads into a complex non-linear trans-formation3 denoted by φ(·), and then passing itthrough a softmax function over the entire vocab-ulary.
therefore, the output probability can bewritten as pvanilla(y|x) = f (a1, .
.
.
, ah), where.
f (a1, .
.
.
, ah) := softmax(φ(a1, .
.
.
, ah))..3 head-colliding attention.
in this section, we introduce head-colliding atten-tion.
speciﬁcally, we formulate mha as a prob-abilistic model, where each attention head is rep-resented by a latent variable.
the name reﬂectsa “collider” in the context of probabilistic graphi-cal models (figure 1a).
we will ﬁrst explain howhead-colliding attention permits the modeling ofinteractions among different heads and then discusshow vanilla mha can be viewed as a marginalizedversion of head-colliding attention, which ignoresany head interactions..considering a single mha block, we cast eachattention head ai as a latent variable.
the proba-bility of target y conditioning on input x can beobtained by marginalizing over all heads a (wedenote a := {a1, .
.
.
, ah}):.
p(y|x) =.
p(y|a, x)p(a|x)da.
(cid:90).
a.
= ep(a|x) [f (a)] ..p(a|x) is the joint prior distribution.
the corre-sponding directed graphical model is demonstrated.
3since a transformer typically stacks several attentive lay-ers, for an mha block in some layer, subsequent layers willinduce a non-linear transformation φ(·) for its attention heads.
for instance, φ(·) may include several other mha blocks andfeed-forward networks..537a11.a21.a12.a22..
.
...
.
..a1h.a2h.in head-colliding attention the distribution of y isdeﬁned as:.
p(y|x) = ep(a1,...,ah|x) [f (a1, .
.
.
, ah)] ..a1.
a2.
.
.
..ah.
y.al1.al2..
.
..alh.y.
(a) head-colliding atten-tion..(b) cascaded head-collidingattention (coda)..figure 1:(a) left: probabilistic graphical model(pgm) diagram of head-colliding attention.
althougheach head variable is independent a priori, they inter-act with each other after observing targets y, whichis referred as explaining-away effect.
(b) right: pgmdiagram of a 3-layer cascaded head-colliding attention(coda).
ali denotes the i-th attention head at trans-former layer l. note that all dependencies from x areomitted in these diagrams for simplicity..in figure 1a, where the links from different headscollide on the observation variable y. a crucialproperty of this graphical model is the “explaining-away” effect (pearl, 1989; wellman and henrion,1993) of attention heads a when observing theoutput y.in other words, if a head ai attendsto part of the input which accords well with ob-servation, it immediately discourages other headsfrom attending to the same part of the input butencourages them to look into complementary in-formation.4 this mechanism effectively reduceshead redundancy and in turn improves parameterefﬁciency..vanilla vs. head-colliding attention we nowtake a closer look at the vanilla mha (§2).
recallthat in vanilla mha, all attention heads are deter-ministic.
from the perspective of latent variablemodels, this is computationally equivalent to takingexpectations of latent head variables.
the outputprobability distribution pvanilla(y|x) can then beexpressed as:.
f (ep(a1|x) [a1] , .
.
.
, ep(ah|x) [ah])..(2).
this means we are only interested in the individ-ual expectations when using the attention headsin vanilla mha for predictions.
on the contrary,.
note the inherent difference of when to take theexpectation in vanilla and head-colliding attention.
since f (·) is a complex non-linear function (§2),these two formulations are not equivalent in gen-eral and may have a large gap between the twodistributions.
concretely, vanilla mha ignores anypossible interactions among different heads.
asindicated in equation 2, it ﬁrst marginalizes out ev-ery single head before observing targets – one headwill not learn what other heads are attending todespite the fact y is observed.
this is why vanillamha is prone to redundancy as many previousstudies (voita et al., 2019; michel et al., 2019a,inter alia) discovered.
head-colliding attention,on the other hand, permits rich head interactionsdue to the expressive non-linear function f (·) in-side the expectation over different latent variablesa1, .
.
.
, ah.
however, the complexity of head inter-actions also leads to intractability in training themodel, which we will discuss in the next section..4 training head-colliding attention.
we train the model by performing maximum likeli-hood estimation.
here, the log marginal likelihoodcan be expressed as:.
log p(y|x) = log ep(a|x) [p(y|a, x)] ..unfortunately, this is intractable in general becauseit requires marginalizing over all possible conﬁgu-rations of attention heads.
the standard techniqueis to use variational inference, which optimizesthe log marginal by maximizing its evidence lowerbound (called elbo) (blei et al., 2017):.
l := eq(a|x).
log.
(cid:20).
p(y|a, x)p(a|x)q(a|x).
(cid:21).
(3).
= log p(y|x) − kl(q(a|x)||p(a|x, y)).
≤ log p(y|x),.
where q(a|x) is the variational distribution5 overlatent variables a. p(a|x, y) is the intractableposterior distribution of all heads given observa-tions y and the input x, which encodes the rich.
4in other words, if we conﬁrm that some head accordswell with the observation, then the probability of other headsshould be reduced since there is less need to invoke them,according to occam’s razor..5although the variational distribution q should depend ontarget y in principle, such conditioning renders testing difﬁ-cult since the target information is not available during testing.
for this reason, we only consider the source x hereafter..538head interactions we desire, as discussed in §3.
therefore, an ideal variational distribution q(a|x)should be close to the true posterior p(a|x, y).
inthis case, the samples would accurately reﬂect thehead interactions and the variational distributionwould yield a tighter bound to l to facilitate thetraining..a straight-forward choice of q(a|x) is touse the mean-ﬁeld approximation (kingma andwelling, 2013):.
q(a|x) = q(a1, a2, .
.
.
, ah|x) =.
q(ai|x)..h(cid:89).
i=1.
however, it has similar drawbacks as the vanillamha.6 the mean-ﬁeld approximation assumes theindependence of different heads and hence the in-teractions are greatly limited..alternatively, one could parameterize q(a|x)using an auto-regressive model.7 although thisis much more expressive, its sequential natureseverely slows down training, making it infeasi-ble in practice..cascaded head-colliding attention our solu-tion to this problem is to employ hierarchical struc-tures for head-colliding attention, where interac-tions among heads could be effectively incorpo-rated into the model (sønderby et al., 2016; ran-ganath et al., 2016)..conveniently, the hierarchical nature of the trans-former architecture offers an effective way of con-structing such proposal distributions.
given a trans-former with l layers, we denote the set of all at-tention heads at layer l − 1 and l as al−1 and al,respectively.
following the bottom-up computationof the transformer, the distribution of al must relyon the instantiated values of al−1.
in this sense,al−1 can be seen as the common variables thatgovern al (figure 1b).
formally, we have:.
q(a1, ..., al|x)=q(a1|x).
q(aj|x, aj−1)..l(cid:89).
j=2.
despite the fact that each attention head ali ∈ al atl-th layer is conditionally independent given al−1,they become dependent when we marginalize al−1.
out.
in particular, the marginal distribution of eachal becomes:(cid:90)q(al|x)=.
q(al−1|x)q(al|x, al−1)dal−1..al−1.
this corresponds to an inﬁnite mixture of the mean-ﬁeld distributions q(al|x, al−1) and is able tocapture rich head interactions (ranganath et al.,2016).
our main model adopts this cascaded pro-posal distribution in ﬁgure 1b, and therefore wename it cascaded head-colliding attention (coda).
the only problem left now is how to specify theconditional distribution q(al|x, al−1) for all l =1, 2, .
.
.
, l. we ﬁrst impose the basic constraintson head values as in vanilla mha, that is, all headvalues must range within a simplex ∆n−1:.
∆n−1 = {al|.
ali,:k = 1, ∀i = 1, .
.
.
, h}..n(cid:88).
k=1.
here ali,:k is the k-th column of the i-th atten-tion head at layer l and 1 denotes the vector ofall 1’s.
for efﬁcient training and inference, weadopt gaussian-logistic distributions (blei and laf-ferty, 2006; cohen et al., 2008), which not onlysatisfy the constraints above but also beneﬁt fromthe effective reparameterization trick (kingma andwelling, 2013; rezende et al., 2014; titsias andlázaro-gredilla, 2014)..in particular, recall that in vanilla mha, ai =softmax(zi) = softmax( (cid:101)qi (cid:102)kti ) (equation 1).
we also denote the attention logits at l-th layeras zl := {zlh}.
for head i at layer l, weﬁrst sample from a multivariate gaussian distri-bution q(zli,j: ) 8 and pass the samples into arow-wise softmax function to yield head values:.
1, .
.
.
, zl.
i,j:|zl−1.
i,j: ∼ n(µlzl.
i,j:, σ),.
i,j: = softmax(zlal.
i,j:),.
i,j: and al.
where zli,j: represent the j-th row of thei-th attention logit and attention head at layer lrespectively..to explicitly model hierarchical structuresamong attention heads, we propose to add a di-rect connection between attention heads at adjacentlayers (figure 1b).
such connections offer directaccess to the information of attention in the previ-ous layer.
speciﬁcally, for each head i at layer l we.
6note that the vanilla mha does not deﬁne distributionsover heads in its original context.
we derive this from thelatent-variable perspective..7this works well in our preliminary experience, despite its.
extremely expensive computational cost..8we only explicitly deﬁne the attention logit z as randomvariables, while the distribution of heads a is induced viaa deterministic transformation (i.e., softmax function) of z;therefore it sufﬁces to build dependencies between attentivelogits instead..539set the mean µi.
l as the sum of two parts:.
µi.
l = (cid:101)qi (cid:102)kti(cid:124) (cid:123)(cid:122) (cid:125)vanilla mha.
+ σi(zl−1)(cid:125).
(cid:123)(cid:122)direct connection.
(cid:124).
,.
(4).
where σi(·) is a two-layer multilayer perceptron(mlp) to fuse information from different headszl−1 (see the cascading connections in figure 1bfor an illustration).
we set the covariance ma-trix σ to the identity matrix for all attentive log-its.
we give the prior the same form as the vari-ational posterior and parameters are shared be-tween q(a1, ..., al|x) and p(a1, ..., al|x) forour objective (equation 3).
with the help of param-eter sharing, the kl term in equation 3 is also can-celled out due to the identical distributions.9 thischoice works well in practice, where it not onlyallows coda to use almost the same amount of pa-rameters as vanilla transformer, but also eliminatesthe need to invoke advanced training techniques foramortized variational inference.10 more details canbe found in appendix a..5 experiments.
we conduct experiments on language modeling andmachine translation tasks..5.1 setup.
datasets first, we conducted experiments fortoken-level language modeling on a large-scalebenchmark dataset wikitext-103 (merity et al.,2016), which consists of articles from wikipediawith the token number around 103m/218k/246kfor the training/validation/testing splits respectively.
the vocabulary size is 267,744..for machine translation, we consider two stan-.
dard datasets:.
• wmt14 en-de (bojar et al., 2014), which con-tains about 4.5m/3k/3k sentences pairs for train-ing/validation/testing splits respectively.
we fol-low ott et al.
(2018) and peng et al.
(2020) topreprocess the dataset, and obtain a shared vo-cabulary between source and target language ofaround 32k byte pair encoding (bpe, sennrichet al.
(2016)) types..9therefore, it can also be derived by directly applying the.
jensen’s inequality on the log marginal likelihood..10for instance, training a standard variational auto-encoder(vae) for nlp tasks often suffers from the posterior collapseproblem due to the heavy kl regularization (bowman et al.,2016), where some tricks have to be used to achieve goodperformance, such as kl annealing, etc..• iwslt14 de-en (cettolo et al., 2014).
fol-lowing standard practice (edunov et al.,2018; peng et al., 2020), we pre-process the160k/7k/7k sentence pairs and build train-ing/validation/testing sets accordingly.
this gen-erates a vocabulary of around 9k(7k) bpe typesfor source(target)..implementation details we implement ourmodel with pytorch (paszke et al., 2019) andfairseq toolkit (ott et al., 2019).
in particular,our model is based on the vanilla transformer ar-chitecture (vaswani et al., 2017).
for coda, wereplace all vanilla mha blocks with the cascadedhead-colliding attention, for both self attention andcross attention (if any).
in language modeling,we use adaptive input embeddings (baevski andauli, 2019) and set context size to 512 and 480 fortraining and testing respectively, due to constraintsof computational resources.
in machine transla-tion, we set beam size to 5 and adopt the hyper-parameters from (peng et al., 2020) for iwslt14de-en.
for wmt14 en-de we set beam size to4, length penalty to 0.6, and average last 10 check-points for testing, following vaswani et al.
(2017).
further implementation details can be found in ap-pendix a..5.2 main results.
of.
results.
language modeling.
theonwikitext-103 dataset are reported in ta-ble 1. as we can see from the table, coda barelyintroduces any additional parameters.
however,by taking into account head interactions, codasigniﬁcantly outperforms transformer byover 0.6 perplexity.
for reference, we also reportthe best setting (denoted by transformer †)in baevski and auli (2019), which uses a muchlarger context size (3072/2560 vs. 512/480 fortraining/testing), coda still outperforms by asubstantial margin of 0.3 perplexity.
this indicatesthat encouraging head interactions can improveparameter efﬁciency..to show whether coda has promoted head in-teractions and reduced head redundancy, we quali-tatively visualize the attention heads in both codaand transformer via heatmaps.
concretely, wecompute the jensen-shannon divergence (jsd) be-tween each pair of attention heads at the same layer.
in particular, we assume head values deﬁne a cat-egorical distribution in both transformer andcoda model to facilitate comparison.
that is, an.
540modeltransformertransformer †coda.
# params.
val.
ppl test ppl18.35246.93m17.97246.93m17.81246.96m.
19.0818.7018.48.model.
coda.
transformer.
wmt14 en-de.
iwslt14 de-en# params.
bleu # params.
bleu27.439.47m28.039.48m.
60.92m60.94m.
34.535.6.table 1: validation (val.)
and testing perplexity (ppl)on wikitext-103 dataset (lower is better).
trans-former is the base model in baevski and auli (2019)with the same context size as coda (512/480 for train-ing/testing), while transformer† is the same modelbut with the best setting in their paper, which uses muchlarger context size (3072/2560 respectively); the resultfor transformer† is as reported in baevski and auli(2019)..attentive head ai induces n categorical distributionsfor each query position.
for the j-th distribution,it indicates how the j-th target position attends toall m source positions and is denoted by p(x|ai,j:).
for two heads i and i(cid:48), we ﬁrst compute their aver-age distribution as.
table 2: performance of transformer and coda oniwslt14 de-en and wmt14 en-de datasets..that coda is more parameter efﬁcient than vanillatransformer due to the cascaded head-colliding at-tention we proposed.
similar to experiments onlanguage modeling, we also visualize the head be-haviors to measure attentive head interactions (seefigure 5 and figure 6 in appendix b), where weobserve similar phenomena on translation tasks.
speciﬁcally, different heads in coda are often com-plementary to each other and focus on quite differ-ent regions of sequences, rather than becomingredundant or even identical as observed in trans-former models..m :=.
p(x|ai,j:) + p(x|ai(cid:48),j:)2.
5.3 analysis: the effect of the number of.
attention heads.
then the jsd value between the i-th and i(cid:48)-th atten-tion head is computed by summing all of n induceddistributions:.
n(cid:88).
j=1.
12.
(cid:0)kl(p(x|ai,j:)||m)+kl(p(x|ai(cid:48),j:)||m))(cid:1).
we average computed jsds for all validation sam-ples.
note that a larger jsd value (darker color)indicates that two heads are behaving more differ-ently (i.e.
less redundancy between them), and viceversa..as shown in figure 2, jsd heatmaps in codaare clearly darker than those in transformer.
this suggests that coda permits richer head in-teractions, which fosters different heads to com-municate with each other and encourages them tobecome complementary.
consequently, our modeleffectively reduces head redundancy in mha andimproves parameter-efﬁciency..the results on iwslt14 de-en and wmt14en-de datasets are shown in table 2. we see thatcoda exhibits clear improvements over trans-former: a 1.1 point gain in bleu on iwslt14de-en dataset and a 0.6 bleu improvement onwmt14 en-de dataset.
despite such signiﬁcantgains over the baseline, coda only introduce veryfew additional parameters (e.g., 0.03% extra param-eters on iwslt14 de-en).
this, again, shows.
despite one would hope increasing the head num-ber in mha leads to a free-ride in achieving betterperformance, in practice it is often not the case asvanilla mha suffers from the problem of parame-ter redundancy.
following vaswani et al.
(2017),we vary the number of attention heads (4,8,16,32),but keep the amount of computation constant.
ourresults on iwslt14 de-en are shown in table 3.we observe that the translation quality of baselinetransformer (which uses vanilla mha as its mainbuilding blocks) decreases almost linearly whenincreasing number of attention heads (figure 3),which agrees with previous studies (vaswani et al.,2017; voita et al., 2019; michel et al., 2019b)..intuitively, since the total number of parametersin the model remains unchanged, more heads in-dicate that the number of parameters allocated toeach head is reduced, which limits the representa-tional power of every single attention head.
due tothe independence assumption between the heads,many of them tend to focus on similar regions ofthe sequence, leading to a great waste of modelingcapacity..in the case of coda, we observe better bleuscores in response to the increasing head number.
rich interactions in coda could encourage dif-ferent heads to cover broader regions of input se-quence, which in turn offers more useful informa-tion for training.
the perplexity (ppl) reﬂects.
541figure 2: jensen-shannon divergences (jsd) for each pair of attention heads at all 16 layers on wikitext-103validation dataset.
top: jsd heatmap of attention heads from transformer model; bottom: jsd heatmap ofattention heads from coda.
columns represent different layers of both models.
the darker color implies a largerdivergence between two heads and in turn less redundancy..# heads.
481632.bleutransformer34.5334.3533.9133.17.ppl.
coda35.6535.7435.8435.96.transformer4.955.045.155.37.coda4.644.544.554.52.table 3: left: bleu scores on test dataset for trans-former and coda at different numbers of attentionheads; right: perplexity on validation dataset fortransformer and coda at different numbers of at-tention heads..figure 3: left: bleu scores on test dataset for basetransformers and coda under different number of at-tention heads (higher is better); right: perplexity onvalidation dataset for base transformers and coda un-der different number of attention heads (lower is bet-ter)..similar trends.
the coordination between differ-ent heads in coda greatly improves the model’sparameter efﬁciency..5.4 ablation analysis.
in this section, we present an ablation studyto investigate effects of different components incoda.
concretely, we compare four models onthe iwslt14 de-en machine translation task:(i) the full model coda, (ii) a variant of coda ab-lating the cascaded structure (§4), (iii) a variant ofcoda without using head-colliding attention (§3)and (iv) the baseline transformer model..in more details, for model (ii), we remove thesecond term in equation 4, which turns off the di-rect cascading structure, despite still being a proper.
hierarchical latent variable model11.
in model (iii),attention heads are deterministic (instead of beinglatent variables) as in vanilla transformers, but cas-cading connections are incorporated.
we observeits close connection with the recently proposed re-alformer (he et al., 2020), a transformermodel that adds a residual connection between at-tention logits at adjacent layers.
since in model(iii) all attention heads are deterministic, it is un-necessary to fuse different heads (see §4).
in thiscase, we simply implement model (iii) as a real-former (and thus referred to as realformerhereafter) to demonstrate the effect of cascading-like structures more clearly.12.
we report bleu score for translation quality,and the jensen-shannon divergences (jsd) aver-aged over all heads pairs of all mha blocks forquantitative evaluation of head interactions.
asdemonstrated in table 4 and figure 4, even with-out cascading connections for explicit hierarchicalstructures, head-colliding attention has the ability(albeit limited) to induce reasonable correlationsamong different heads, reﬂected in the average jsd.
this is due to the explaining-away effects and thenative hierarchical structure in the transformers, asdiscussed in §3.
in coda, because individual headshave access to the other heads from a probabilisticperspective, they are more prone to offering com-plementary information for each other to jointlyexplain the observed data.
this effect is furtherenhanced when cascading connections are addedto the model.
in contrast, if we simply incorporatesuch cascading connections into a vanilla trans-former model, we found it does not signiﬁcantly.
11note that the ﬁrst term (cid:101)qi (cid:102)k tin equation 4 also dependsion the instantiated value of zl−1i,j: , which induces an implicithierarchical dependency for attention between adjacent layers.
12the main difference between residual connections in re-alformer and cascading connections in coda is that, theformer directly performs a head-wise addition of previous-layer attention logits; in contrast, our cascading connectionmakes use of an mlp σ(·) to mix different attention heads,which enhances head interactions for coda..5420246layer 0layer 1layer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9layer 10layer 11layer 12layer 13layer 14layer 15024602460246024602460246024602460246024602460246024602460246024602460200400481632number of heads33.534.034.535.035.536.0bleubasecoda481632number of heads4.64.85.05.25.4pplbasecodamodelcodacoda- csrealformer (he et al., 2020)transformer.
avg.
jsd bleu35.6535.1735.0134.53.
13.7211.248.537.11.table 4: the average jsd and bleu scores with dif-ferent model conﬁgurations.
coda-cs indicates the ab-lation of the cascading structures from the full model(i.e., simply replacing all mha blocks of base trans-former with head-colliding attention); realformeris a recently proposed transformer model that hascascading-like structures but still views each head as adeterministic value rather than latent variables..encourage head interactions and only improves thebaseline marginally.
in this case, the performanceimprovement might be mainly due to residual con-nections, which are often considered to be effectivein facilitating training (he et al., 2016).
interest-ingly, we note a positive correlation between aver-age jsd and bleu, suggesting that encouragingcomplementary attention heads may help improvetranslation quality..6 related work.
attention mechanisms were ﬁrst applied to recur-rent networks in (bahdanau et al., 2014).
it wasthen extended to multi-head attention (mha) andbecame the key component in transformer architec-tures (vaswani et al., 2017)..to study the utility of multiple attention heads,voita et al.
(2019) focused on identifying individ-ual contributions of each attention head.
michelet al.
(2019a) conducted extensive experiments todemonstrate that pruning out most heads after train-ing does not lead to a drop in performance duringinference.
you et al.
(2020) further revealed thatreplacing learnable attention heads with samplesfrom ﬁxed gaussian distributions can achieve al-most the same performance as original models.
ad-ditionally, behnke and heaﬁeld (2020) proposedto iteratively prune attention heads during trainingbased on the lottery ticket hypothesis.
these worksindicate that there is a lot of head redundancy inthe mha transformer architectures..instead of pruning unnecessary parameters anddown-sizing transformer models, there are alsoworks that propose to improve parameter efﬁciencyin transformers.
for instance, li et al.
(2018) in-troduced a regularization term to explicitly pro-mote diversity among different heads.
yang et al.
(2019a) proposed to use convolutional kernels to.
capture correlations among not only local windowsof sequences, but also different heads.
an et al.
(2020) considered each head as a sample from thesame distribution, and presented a sampling algo-rithm that avoids samples from collapsing into localmodes.
it hence explicitly encouraged the repul-siveness in mha.
besides, mae (peng et al., 2020)converted a vanilla mha to a mixture-of-expertsmodel, where each expert component activates onlya subset of attention heads.
with learned probabili-ties, different experts could be specialized on differ-ent inputs.
different from these works, coda doesnot explicitly promote head diversity nor specializedifferent heads.
instead, we focus on studying headinteractions from a probabilistic perspective, whichreveals the close connection between vanilla mhaand coda..another research line relating to our work is toincorporate latent variables into attention modules.
xu et al.
(2015) investigated the connection be-tween vanilla deterministic single-head attentionand its stochastic counterpart.
deng et al.
(2018)explored this further and proposed to use varia-tional inference techniques for training the model.
they considered both cases of discrete and contin-uous latent variables.
bayesian attention modules(fan et al., 2020) introduced continuous latent dis-tributions for attention that are amenable to repa-rameterization tricks.
our work is different fromthem in that we mainly investigate the mha mech-anism and aim to improve parameter-efﬁciency byrecovering potential interactions among differentheads, which are ignored in vanilla mha..concurrently, he et al.
(2020) proposed to addresidual connections between attention scores atadjacent layers, similar to our cascading connec-tions.
nevertheless, our motivation for using thecascaded structure is quite different: we aim to con-struct direct hierarchical dependencies for latentvariable models, while he et al.
(2020) is mainlymotivated to improve transformer architectures andobtain performance gains..7 conclusion and future work.
we present coda by re-formulating the multi-headattention (mha) as a latent variable model froma probabilistic perspective.
coda explicit modelsof the interactions among attention heads througha hierarchical variational distribution.
we conductextensive experiments and demonstrate that codaoutperforms the transformer baseline in language.
543(a) transformer.
(b) realformer.
(c) coda-cs.
(d) coda.
figure 4: jensen-shannon divergences (jsd) for each pair of attention heads at the same layer on iwslt14de-en dataset for transformer, realformer, coda-cs and coda model respectively.
each row indicatesdifferent kinds of attention, including encoder self-attention, decoder self-attention and decoder-encoder crossattention (from top to bottom), respectively; and each column indicates average jsd scores at different layers..modeling and machine translation.
the analysisshows that coda learns to encourage the diver-sity in different heads and to promote parameterefﬁciency when increasing the number of heads.
in this framework, we will be able to impose ex-plicit constraints or regularization on different at-tention heads in a principal way (e.g.
informa-tive priors that promote diversity).
besides, wecan also consider more expressive (data-driven)variational distributions.
we leave these as thefuture work.
our code is publicly available athttps://github.com/lzhengisme/coda..acknowledgments.
we thank the anonymous reviewers whose sug-gestions helped clarify this work.
this re-search was supported in part by the university ofhong kong research committee under account104006039.111994.14200.301.01..references.
bang an, jie lyu, zhenyi wang, chunyuan li, chang-wei hu, fei tan, ruiyi zhang, yifan hu, andchangyou chen.
2020. repulsive attention: re-thinking multi-head attention as bayesian inference.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 236–255, online.
association for computa-tional linguistics..alexei baevski and michael auli.
2019. adaptive in-put representations for neural language modeling.
ininternational conference on learning representa-tions..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..maximiliana behnke and kenneth heaﬁeld.
2020. los-ing heads in the lottery: pruning transformer atten-tion in neural machine translation.
in proceedings of.
5440123layer 0layer 1layer 2layer 3layer 4layer 5012301230123012301230123012301230123layer 0layer 1layer 2layer 3layer 4layer 5012301230123012301230123012301230123layer 0layer 1layer 2layer 3layer 4layer 5012301230123012301230123012301230123layer 0layer 1layer 2layer 3layer 4layer 5012301230123012301230123012301230510152025the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 2664–2674, online.
association for computational lin-guistics..david blei and john lafferty.
2006. correlated topicmodels.
advances in neural information processingsystems, 18:147..david m blei, alp kucukelbir, and jon d mcauliffe.
2017. variational inference: a review for statisti-cians.
journal of the american statistical associa-tion, 112(518):859–877..ondˇrej bojar, christian buck, christian federmann,barry haddow, philipp koehn, johannes leveling,christof monz, pavel pecina, matt post, hervesaint-amand, et al.
2014. findings of the 2014workshop on statistical machine translation.
in pro-ceedings of the ninth workshop on statistical ma-chine translation, pages 12–58..samuel r. bowman, luke vilnis, oriol vinyals, an-drew dai, rafal jozefowicz, and samy bengio.
2016. generating sentences from a continuousin proceedings of the 20th signll con-space.
ference on computational natural language learn-ing, pages 10–21, berlin, germany.
association forcomputational linguistics..mauro cettolo, jan niehues, sebastian stüker, luisabentivogli, and marcello federico.
2014. reporton the 11th iwslt evaluation campaign, iwslt 2014.in proceedings of the international workshop onspoken language translation, hanoi, vietnam, vol-ume 57..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..alexey dosovitskiy,.
lucas beyer, alexanderkolesnikov, dirk weissenborn, xiaohua zhai,thomas unterthiner, mostafa dehghani, matthiasminderer, georg heigold, sylvain gelly, et al.
2020.an image is worth 16x16 words: transformersarxiv preprintfor image recognition at scale.
arxiv:2010.11929..sergey edunov, myle ott, michael auli, david grang-ier, and marc’aurelio ranzato.
2018. classicalstructured prediction losses for sequence to se-in proceedings of the 2018 con-quence learning.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages355–364, new orleans, louisiana.
association forcomputational linguistics..xinjie fan, shujian zhang, bo chen, and mingyuanzhou.
2020. bayesian attention modules.
advancesin neural information processing systems, 33..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..kevin clark, urvashi khandelwal, omer levy, andchristopher d manning.
2019. what does bert lookat?
an analysis of bert’s attention.
arxiv preprintarxiv:1906.04341..ruining he, anirudh ravula, bhargav kanagal, andjoshua ainslie.
2020. realformer: transformerarxiv e-prints, pagelikes residual attention.
arxiv:2012.11747..shay cohen, kevin gimpel, and noah a smith.
2008.logistic normal priors for unsupervised probabilis-tic grammar induction.
advances in neural infor-mation processing systems, 21:321–328..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..mostafa dehghani, stephan gouws, oriol vinyals,jakob uszkoreit, and lukasz kaiser.
2019. univer-in international conference onsal transformers.
learning representations..yuntian deng, yoon kim, justin chiu, demi guo, andalexander rush.
2018. latent alignment and vari-ational attention.
advances in neural informationprocessing systems, 31:9712–9724..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..diederik p kingma and max welling.
2013. auto-arxiv preprint.
encoding variational bayes.
arxiv:1312.6114..jian li, zhaopeng tu, baosong yang, michael r. lyu,and tong zhang.
2018. multi-head attention withdisagreement regularization.
in proceedings of the2018 conference on empirical methods in natu-ral language processing, pages 2897–2903, brus-sels, belgium.
association for computational lin-guistics..xiaodong liu, kevin duh, liyuan liu, and jianfenggao.
2020. very deep transformers for neural ma-chine translation.
arxiv preprint arxiv:2008.07772..stephen merity, caiming xiong, james bradbury, andrichard socher.
2016. pointer sentinel mixture mod-els.
arxiv preprint arxiv:1609.07843..545paul michel, omer levy, and graham neubig.
2019a.
in ad-are sixteen heads really better than one?
vances in neural information processing systems,pages 14014–14024..paul michel, xian li, graham neubig, and juan pino.
2019b.
on evaluation of adversarial perturbationsin proceedingsfor sequence-to-sequence models.
of the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 3103–3114, minneapolis,minnesota.
association for computational linguis-tics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..myle ott, sergey edunov, david grangier, andmichael auli.
2018. scaling neural machine trans-lation.
in proceedings of the third conference onmachine translation: research papers, pages 1–9,brussels, belgium.
association for computationallinguistics..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019.py-torch: an imperative style, high-performance deepin h. wallach, h. larochelle,learning library.
a. beygelzimer, f. d'alché-buc, e. fox, and r. gar-nett, editors, advances in neural information pro-cessing systems 32, pages 8024–8035.
curran asso-ciates, inc..j. pearl.
1989. probabilistic reasoning in intelligentsystems - networks of plausible inference.
in mor-gan kaufmann series in representation and reason-ing..hao peng, roy schwartz, dianqi li, and noah a.smith.
2020. a mixture of h - 1 heads is better thanin proceedings of the 58th annual meet-h heads.
ing of the association for computational linguistics,pages 6566–6577, online.
association for computa-tional linguistics..rajesh ranganath, dustin tran, and david blei.
2016.in international.
hierarchical variational models.
conference on machine learning, pages 324–333..danilo jimenez rezende, shakir mohamed, and daanwierstra.
2014. stochastic backpropagation and ap-proximate inference in deep generative models.
inproceedings of the 31st international conference onmachine learning, volume 32, pages 1278–1286..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..casper kaae sønderby, tapani raiko, lars maaløe,søren kaae sønderby, and ole winther.
2016. lad-der variational autoencoders.
in advances in neuralinformation processing systems, volume 29, pages3738–3746.
curran associates, inc..ilya sutskever, james martens, george dahl, and geof-frey hinton.
2013. on the importance of initializa-in proceed-tion and momentum in deep learning.
ings of the 30th international conference on ma-chine learning, pages 1139–1147..gongbo tang, mathias müller, annette rios, and ricosennrich.
2018. why self-attention?
a targetedevaluation of neural machine translation architec-in proceedings of the 2018 conference ontures.
empirical methods in natural language processing,pages 4263–4272, brussels, belgium.
associationfor computational linguistics..michalis titsias and miguel lázaro-gredilla.
2014.for non-doubly stochastic variational bayesconjugate inference.
in proceedings of the 31st in-ternational conference on machine learning, vol-ume 32, pages 1971–1979..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..oriol vinyals, igor babuschkin, wojciech m czar-necki, michaël mathieu, andrew dudzik, juny-oung chung, david h choi, richard powell, timoewalds, petko georgiev, et al.
2019. grandmasterlevel in starcraft ii using multi-agent reinforcementlearning.
nature, 575(7782):350–354..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..michael p. wellman and m. henrion.
1993. explaining’explaining away’.
ieee trans.
pattern anal.
mach.
intell., 15:287–292..zhiyong wu, yun chen, ben kao, and qun liu.
2020.perturbed masking: parameter-free probing for ana-lyzing and interpreting bert.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4166–4176, online.
as-sociation for computational linguistics..546kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,aaron courville, ruslan salakhudinov, rich zemel,and yoshua bengio.
2015. show, attend and tell:neural image caption generation with visual atten-tion.
in international conference on machine learn-ing, pages 2048–2057..baosong yang, longyue wang, derek f. wong,lidia s. chao, and zhaopeng tu.
2019a.
convolu-tional self-attention networks.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 4040–4045, minneapolis, min-nesota.
association for computational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019b.
xlnet: generalized autoregressive pretrain-in advances ining for language understanding.
neural information processing systems, volume 32,pages 5753–5763.
curran associates, inc..weiqiu you, simeng sun, and mohit iyyer.
2020.hard-coded gaussian attention for neural machinetranslation.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7689–7700, online.
association for computa-tional linguistics..547a implementation details.
for the σ network, it consists of a 2-layer mlp withleakyrelu non-linear activation and a residuallink from the input.
it is a rather small networkand only accounts for 0.01-0.02% of the total pa-rameters.
recall that the number of attention headsis denoted by h, the source and target length is mand n respectively, and the batch size is denotedby b. the hidden size is set to α ∗ h, where weselect α from {2, 4, 8} based on the validation set.
note that the additionally introduced number ofparameters is negligible compared to the modelsize, accounting for only 0.01-0.02% of the totalparameters.
since we often represent the attentionscores (or logits) z as a multi-dimensional tensorwith shape (b, h, n, m), we ﬁrst transpose itto shape (b, m, n, h) and feed it into the σnetwork.
it then outputs h values so that each com-ponent σi computes the fused information fromall previous layer’s attention heads.
by adding itsoutput to the current layer’s attention logits, wecould effectively construct a direct cascading con-nection for our hierarchical proposal.
note that σnetwork is neither shared among different headsnor different layers..a.1 machine translation.
for wmt14 en-de, the transformer-base archi-tecture in vaswani et al.
(2017) is used, where boththe encoder and decoder consist of 6 layers withhidden size 512. for mha blocks at each layer,the number of attention heads is set to 8 with the di-mension of hidden layer representations being 512;for feed forward networks, the hidden size is set to2048. the rate of dropout is set to 0.1. for train-ing, we follow the same setup as in vaswani et al.
(2017), including that label smoothing with rate 0.1,the adam optimizer (kingma and ba, 2014) is usedfor optimization, the inverse square root schedul-ing is utilized for learning rate and the number ofwarm-up steps is set to 4000..for iwslt-14, we follow the conﬁguration ofhyper-parameters in fairseq package 13. in details,it mostly follows the same architecture and trainingsetup as above, except that it uses a smaller feedforward network with hidden dimension 1024, alarger dropout rate 0.3 and less attention heads 4.for both datasets, we apply a compound splitpost-processing to facilitate comparison.
addition-.
13https://github.com/pytorch/fairseq/.
tree/master/examples/translation.
ally, we use activation dropout with rate 0.1 for allused models on both datasets as we ﬁnd it helpsour model converge better..a.2 language modeling.
for wikitext-103, we base our model onbaevski and auli (2019) with the same hyper-parameter conﬁguration and training setup.
themodel architecture consists of 16 transformer lay-ers, where it uses adaptive input representations, 8heads for each mha block, dropout rate of 0.3, hid-den dimension of 1024, and hidden size of 4096 forfeed forward networks.
for training, nesterov’s ac-celerated gradient (nag) method (sutskever et al.,2013) is used with gradient norm clipping and acosine learning rate schedule14..b additional experimental results.
figure 5 and figure 6 visualize head interactionswithin transformer and coda on iwslt14de-en and wmt14 en-de translation tasks re-spectively..14more details can be found in baevski and auli(2019) and the training script based on fairseq codebase:https://github.com/pytorch/fairseq/blob/master/examples/language_model/readme.
adaptive_inputs.md..548(a) transformer.
(b) coda.
figure 5: jensen-shannon divergences (jsd) for each pair of attention heads at the same layer on iwslt14de-en validation dataset, which are evaluated on both transformer model and coda.
each row indicatesdifferent kinds of attention, including encoder self-attention, decoder self-attention and decoder-encoder crossattention (from top to bottom), respectively; and each column indicates average jsd scores at different layers..(a) transformer.
(b) coda.
figure 6: jensen-shannon divergences (jsd) for each pair of attention heads at the same layer on wmt14 en-dedataset, which are evaluated on both transformer model and coda.
each row indicates different kinds ofattention, including encoder self-attention, decoder self-attention and decoder-encoder cross attention (from top tobottom), respectively; and each column indicates average jsd scores at different layers..5490123layer 0layer 1layer 2layer 3layer 4layer 5012301230123012301230123012301230123layer 0layer 1layer 2layer 3layer 4layer 50123012301230123012301230123012305101520250246layer 0layer 1layer 2layer 3layer 4layer 5024602460246024602460246024602460246layer 0layer 1layer 2layer 3layer 4layer 5024602460246024602460246024602460510152025