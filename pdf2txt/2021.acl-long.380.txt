risk minimization for zero-shot sequence labelingzechuan hu(cid:5)‡, yong jiang†∗, nguyen bach†, tao wang†, zhongqiang huang†,fei huang†, kewei tu(cid:5)∗(cid:5)school of information science and technology, shanghaitech university(cid:5)shanghai engineering research center of intelligent vision and imaging(cid:5)shanghai institute of microsystem and information technology, chinese academy of sciences(cid:5)university of chinese academy of sciences†damo academy, alibaba group{huzch,tukw}@shanghaitech.edu.cn, yongjiang.jy@alibaba-inc.com{nguyen.bach,leeo.wangt,z.huang,f.huang}@alibaba-inc.com.
abstract.
zero-shot sequence labeling aims to builda sequence labeler without human-annotateddatasets.
one straightforward approach is uti-lizing existing systems (source models) to gen-erate pseudo-labeled datasets and train a targetsequence labeler accordingly.
however, dueto the gap between the source and the targetlanguages/domains, this approach may fail torecover the true labels.
in this paper, we pro-pose a novel uniﬁed framework for zero-shotsequence labeling with minimum risk train-ing and design a new decomposable risk func-tion that models the relations between the pre-dicted labels from the source models and thetrue labels.
by making the risk function train-able, we draw a connection between minimumrisk training and latent variable model learning.
we propose a uniﬁed learning algorithm basedon the expectation maximization (em) algo-rithm.
we extensively evaluate our proposedapproaches on cross-lingual/domain sequencelabeling tasks over twenty-one datasets.
theresults show that our approaches outperformstate-of-the-art baseline systems..1.introduction.
sequence labeling is an important task in naturallanguage processing.
it has many applications suchas part-of-speech tagging (pos) (derose, 1988;toutanova et al., 2003) and named entity recogni-tion (ner) (ratinov and roth, 2009; ritter et al.,2011; lample et al., 2016; ma and hovy, 2016; huet al., 2020).
approaches to sequence labeling aremostly based on supervised learning, which reliesheavily on labeled data.
however, the labeled datais generally expensive and hard to obtain (for low-resource languages/domains), which means thatthese supervised learning approaches fail in manycases..∗ corresponding authors.
‡work was done when zechuan.
hu was interning at alibaba damo academy..learning knowledge from imperfect predictionsfrom other rich-resource sources (such as cross-lingual, cross-domain transfer) (yarowsky andngai, 2001; guo et al., 2018; huang et al., 2019;hu et al., 2021) is a feasible and efﬁcient wayto tackle the low-resource problem.
it transfersknowledge from rich-resource languages/domainsto low-resource ones.
one typical approach to thisproblem is utilizing existing systems to provide pre-dicted results for the zero-shot datasets.
however,due to the gap between the source and the targetlanguages/domains, this approach may fail to re-cover the true labels.
several previous approachestry to alleviate this problem by relying heavily oncross-lingual information (e.g., parallel text (wangand manning, 2014; ni et al., 2017)), labeled datain source languages (chen et al., 2019), and priordomain knowledge (yang and eisenstein, 2015) fordifferent kinds of zero-shot scenarios.
however,these approaches are designed to be speciﬁc, andmight not be generalizable to other kinds of settingswhere the required resources are expensive to ob-tain or not available due to data privacy (wu et al.,2020).
instead, we want a learning framework thatcan address the zero-shot learning problem in auniﬁed perspective..in this work, we consider two widely exploredsettings in which we have access to: 1) the imper-fect hard predictions (rahimi et al., 2019; lan et al.,2020); 2) the imperfect soft predictions (wu et al.,2020), produced by one or more source models ontarget unlabeled data , and propose two novel ap-proaches.
we start by introducing a novel approachbased on the minimum risk training framework.
wedesign a new decomposable risk function parame-terized by a ﬁxed matrix that models the relationsbetween the noisy predictions from the source mod-els and the true labels.
we then make the matrixtrainable, which leads to further expressiveness andconnects minimum risk training to learning latent.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4909–4920august1–6,2021.©2021associationforcomputationallinguistics4909variable models.
we propose a learning algorithmbased on the em algorithm, which alternates be-tween updating a posterior distribution and opti-mizing model parameters..to empirically evaluate our proposed ap-proaches, we extensively conduct experiments onfour sequence labeling tasks of twenty-one datasets.
our two proposed approaches, especially the latentvariable model, outperform several strong base-lines..2 background.
2.1 sequence labeling.
given a sentence x = x1, .
.
.
, xn, its word rep-resentations are extracted from the pre-trainedembeddings and passed into a sentence encodersuch as bilstm, convolutional neural networks(cnn) and multilingual bert (devlin et al., 2019)to obtain a sequence of contextual features.
with-out considering the dependencies between pre-dicted labels, the softmax layer computes the con-ditional probability as follows,.
pθθθ(y|x) =.
pθθθ(yi|x).
n(cid:89).
i=1.
given the gold sequence y∗ = y∗n, the gen-eral training objective is to minimize the negativelog-likelihood of the sequence,.
1, .
.
.
, y∗.
from the source model, the corresponding objec-tive function is the cross-entropy loss between theimperfect hard predictions and the target model’ssoft predictions,.
j (θθθ) = − log pθθθ(ˆy|x) = −.
log pθθθ(ˆyi|x).
n(cid:88).
i=1.
where ˆy denotes the pseudo label sequence of xpredicted by the source model and ˆyi is the pseudolabel for position i. with imperfect soft predictionsfrom the source model, the corresponding objec-tive function is the kl-divergence (kl) or meansquare error (mse) loss between the imperfect softpredictions and the target model’s soft predictions(knowledge distillation, kd) (wu et al., 2020)..for multi-source setup, a simple approach con-tains the following two steps.
the ﬁrst step is toapply dt with each source language to produce pre-dictions on unlabeled target data.
the second stepis to mix the predictions from all the source modelsand perform supervised learning of a target modelon the mixed pseudo-labeled dataset.
however, themixed pseudo-labeled dataset can be very noisybecause predictions from different source modelsmay contradict each other.
similar to single-sourcesetting, a more effective way is aggregating the softpredictions from multiple sources and doing kd(wu et al., 2020)..j (θθθ) = − log pθθθ(y∗|x) = −.
log pθθθ(y∗.
i |x).
n(cid:88).
i=1.
3 methodology.
3.1 minimum risk training.
for simplicity, throughout this paper, we assumethat all the sequence labelers are based on the soft-max method..2.2 cross-lingual/domain transfer.
supervised models fail when labeled data are ab-sent.
learning from imperfect predictions fromrich-resource sources is a viable approach to tacklethe problem.
generally speaking, there are twosettings to obtain the imperfect predictions from:single source and multi source.
the simplest single-source approach is to train a single-source modelon one source language/domain and use the sourcemodel to directly predict labels on the target testdata.
we name this approach as direct single-sourcetransfer (dt).
another single-source approach isto use the predictions of the source model on a setof unlabeled target data to supervise the trainingof a target model.
with imperfect hard predictions.
in supervised learning, minimum risk training aimsto minimize the expected error (risk) concerningthe conditional probability,.
j (θθθ) =.
(cid:88).
pθθθ(y|x)r(y∗, y).
y∈y(x).
where r(y∗, y) is the risk function that measuresthe distance between the gold sequence y∗ and thecandidate sequence y, and y(x) denotes the col-lection of all the possible label sequences giventhe sentence x. the risk function can be deﬁnedin many ways depending on speciﬁc applications,such as the bleu score in machine translation(shen et al., 2016).
however, in our setting, thereare no gold labels to compute r(y∗, y).
instead,we assume there are multiple pretrained sourcemodels which can be used to predict hard labels,and we deﬁne the risk function as r(ˆy, y) to mea-sure the difference between pseudo label sequence.
4910ˆy predicted by source models and the candidatesequence y. the objective function becomes,.
j (θθθ) = epθθθ(y|x)[r(ˆy, y)](cid:88).
=.
pθθθ(y|x)r(ˆy, y).
y∈y(x).
conventional minimum risk training is in-tractable which is mainly due to the combinationof two reasons: ﬁrst, the set of candidate label se-quences y(x) is exponential in size and intractableto enumerate; second, the risk function is hard todecompose (or indecomposable).
to tackle theproblem, we deﬁne the risk function as a negativeprobability −p (ˆy|y) that can be fully decomposedby position.
the objective function becomes,.
j (θθθ) =.
(cid:88).
pθθθ(y|x)r(ˆy, y).
pθθθ(y|x)pψψψ(ˆy|y).
(1).
y∈y(x)(cid:88).
= −.
y∈y(x).
n(cid:89).
(cid:88).
i=1.
yi.
= −.
pθθθ(yi|x)pψψψ(ˆyi|yi).
we introduce a matrix ψψψ ∈ rk×k to modelpψψψ(ˆyi|yi), where k is the number of labels.
noticethat ψψψ here is a ﬁxed matrix that does not changein training.
in the general imperfect predictionslearning, it is often implicitly assumed that theprediction from a source model is generally bet-ter than uniformly selecting a candidate label atrandom.
given this prior knowledge, we requirepψψψ(ˆyi = k|yi = k)> 1k .
therefore, we empiricallydeﬁne matrix ψψψ as,.
ψψψij =.
(cid:40).
µ1−µk−1.
if i = j ,if i (cid:54)= j.where µ> 1k is a hyper-parameter.
in the imple-mentation, for convenience, we multiply an iden-tity matrix by a hyper-parameter τ and then applysoftmax operation to every column to obtain thematrix ψψψ..to further explain ψψψ, we give an example fromthe perspective of prediction in table 1. givena sentence x = “i cried”, a label distributionpθθθ(y|x) for the sentence, a pseudo label sequenceˆy = {pron, adj} predicted by the source model,and two settings µ1=0.4 and µ2=1 for ψψψ(1) and ψψψ(2)respectively, we compute pθθθ(yi|x) × pψψψ(ˆyi|yi) asshown in the table..x.pθθθ(y|x).
icried.
0.60.1.
0.30.6.
0.10.3.
ˆy.
pronadj.
1 ψψψ(1) pron verb adj pθθθ(yi|x) × pψψψ(ˆy(1)i0.030.30.30.120.4.
0.24 0.090.03 0.18.pronverbadj.
ypred: [pron, verb].
0.40.30.3.
0.30.40.3.esac.|yi).
2 ψψψ(2) pron verb adj pθθθ(yi|x) × pψψψ(ˆy(2)i0000.31.
0.60ypred: [pron, adj].
pronverbadj.
esac.010.
100.
00.
|yi).
table 1: an example of prediction results on two dif-ferent ψψψs.
case1 with a less sparse matrix than case2obtains a better prediction.
ypred denotes the predic-tions by sequence labeler using corresponding matrixψψψ..since ψψψ(2) is an identity matrix, it predicts thelabel with the largest value at each position.
it as-signs the wrong label adj to the word “cried” asa consequence.
on the contrary, ψψψ(1) introducessome uncertainties by providing smoothing overthe pseudo labels.
as a result, it correctly predictsthe word “cried” as verb.
from the perspective oftraining, which minimizes j (θθθ), if ψψψ is an identitymatrix, then it is a supervised model with ˆy as thesupervision signal; on the other hand, if ψψψ is a uni-form matrix, then the supervision signal becomesrandom and training becomes meaningless..extending to leverage soft predictions previ-ous works shows that the soft predictions fromsource models can provide more information thanthe hard predictions (hinton et al., 2015; wu et al.,2020).
our novel approach can also easily leveragethis information by simply replacing the one-hotpseudo labels with soft probability distributionsfrom source models.
the training objective be-comes,.
j (θθθ) = −.
pθθθ(yi|x).
ps(ˆyi|x)pψψψ(ˆyi|yi).
n(cid:89).
(cid:88).
i=1.
yi.
(cid:88).
ˆyi.
where ps is the source model’s soft predictions..for simplicity, in the rest of this section, weintroduce our approaches based on the setup ofusing one-hot pseudo labels, but all the approachescan be extended to leverage soft predictions in asimilar way..49113.2 minimum risk training: a latentvariable model perspective.
in this subsection, we instead use a trainable matrixσσσ to model pσσσ(ˆy|y).
we initialize σσσ in the sameway as ψψψ.
assuming that conditioning on y, x andˆy are independent with each other, we ﬁnd that thenon-negative term of equation (1) is a conditionalmarginal probability deﬁned by a latent variablemodel in which y is the latent variable..(cid:88).
y∈y(x).
pθθθ(y|x)pσσσ(ˆy|y) = pθθθ,σσσ(ˆy|x).
in latent variable model training, we generally op-timize the negative conditional log-likelihood, andthe objective function becomes,.
j (θθθ, σσσ) = − log pθθθ,σσσ(ˆy|x).
= −.
n(cid:88).
i=1.
(cid:88).
log.
yi.
pθθθ(yi|x)pσσσ(ˆyi|yi).
interpolation in practice, given a pre-deﬁnedhyper-parameter µ, we combine the ﬁxed pψψψ(ˆyi|yi)with the trainable pσσσ(ˆyi|yi) to get a new probabil-ity,.
pφφφ(ˆyi|yi) = λpψψψ(ˆyi|yi) + (1 − λ)pσσσ(ˆyi|yi).
where λ ∈ [0, 1] is a hyper-parameter, φφφ is the com-bined matrix.
if λ = 1, it denotes the minimum risktraining.
otherwise, it denotes the latent variablemodel..3.3 from single-source to multi-source setup.
by modeling the joint distribution over the pseudolabels which are predicted by u source models onthe target unlabeled data, we can easily extend ourlatent variable model to the multi-source setting.
the objective function becomes,.
j (θθθ, φφφ) = −.
log.
pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u).
n(cid:88).
i=1.
(cid:88).
yi.
u(cid:89).
u=1.
our overall architecture of the latent variable modelis depicted in figure 1..3.4 optimization.
in this section, we propose a uniﬁed optimiza-tion scheme, which is based on the em algorithm(dempster et al., 1977) 1, to learn the parameters.
1another approach is to perform direct gradient descentoptimization, which we ﬁnd weaker results.
we have a discus-sion on that in the analysis section..figure 1: directed graphical model of our latent vari-able model..of the two proposed approaches.
the em algo-rithm is widely applied to learn parameters in alarge family of models with latent variables such asthe gaussian mixture models.
it is an iterative ap-proach that has two steps in every iteration, whichare the e-step and the m-step.
in the e-step, itoptimizes a posterior distribution of the latent vari-ables.
in the m-step, it estimates the parametersof the latent variable model according to the poste-rior distribution.
as the single-source setup can beseen as a special case, we focus on the multi-sourcesetup to derive the equations.
we ﬁrst introduceq(y) = (cid:81)q(yi) as a distribution over the latenti.variable y, and then we derive the upper bound ofj (θθθ, φφφ) as follows,.
j (θθθ, φφφ) = −.
pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u).
n(cid:88).
(cid:88).
log.
i=1.
yi.
pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u).
n(cid:88).
(cid:88).
= −.
log.
q(yi).
i=1.
yi.
≤ −.
q(yi)log.
n(cid:88).
(cid:88).
i=1.
yi.
pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u).
(2).
= −.
eq(yi)log pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u)+c.
n(cid:88).
i=1.
u(cid:89).
u=1.
where c is a residual term, and q(yi) stands forq(yi = yi).
the inequation above is derived fromjensen’s inequality.
to make the bound tight forparticular θθθ and φφφ, we derive q(yi) as,.
q(yi) ∝ pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u).
(3).
u(cid:89).
u=1.
we sketch our strategy of parameter update in.
the t-th iteration as follows,.
• e step, we compute q(yi) using parameters.
θθθ and φφφ from the (t − 1)-th iteration;.
u(cid:89).
u=1.
u(cid:81)u=1.
q(yi).
u(cid:81)u=1.
q(yi).
4912yx̂yuθϕuyx̂yuθϕu• m step, we update parameters θθθ and φφφ to-gether using a gradient-based approach byminimizing the upper bound above.
q(yi)is ﬁxed in this step and hence we minimize.
−.
n(cid:88).
i=1.
eq(yi) log pθθθ(yi|x).
pφφφ(ˆy(u)i.
|yi, u).
u(cid:89).
u=1.
lowing wu et al.
(2020), the source model are pre-viously trained on its corresponding training data.
we use the bio scheme for conll and ontonotesner tasks and aspect extraction.
we run eachmodel three times and report the average accuracyfor the pos tagging task and f1-score for the othertasks..we repeat the two steps alternately until conver-gence.
we give an overall process for multi-sourcesetup with unlabeled target data in algorithm 1..algorithm 1 multi-source transfer with latent variable model.
1: input: unlabeled dataset of target t , u pretrained sourcemodels {m = m (1), .
.
.
, m (u )}, u trainable matri-ces {σσς = σσσ(1), .
.
.
, σσσ(u )} and u ﬁxed matrices {ψψψ =ψψψ(1), .
.
.
, ψψψ(u )}, hyper-parameter µ and λ, maximal iter-ations e for the em algorithm..2: initialize:.
initialize σσς and ψψψ with the same hyper-parameter µ. initialize {φφφ = φφφ(1), .
.
.
, φφφ(u )} using λ, σσςand ψψψ.
initialize an empty pseudo label list ˆy, an upperbound loss lm = +∞, and an overall loss le = +∞..3: for u = 1, .
.
.
, u do4:.
use m (u) to obtain the hard/soft label sequence of theunlabeled data t and append the predictions to the list ofpseudo label sequences.
ˆy..5: end for6: concatenate the unlabeled data t with all pseudo label.
collections ˆy to form a new training dataset ˆt ..7: for e = 1, .
.
.
, e do8:.
compute posterior distribution q(yi) according to(cid:46) e step.
formula 3 for each sample x..9:10:11:12:.
compute the loss le = j (φφφ, θθθ).
if le has no improvement do.
end training..end if.
repeat.
13:14:15:16:17: end for.
compute lm according to eq.
2.update φφφ and θθθ..until lm has no improvement..3.5.inference.
for inference, we use q(y) to obtain ypred.
2,.ypred = argmaxy∈y(x).
pθθθ(y|x).
pφφφ(ˆy(u)|y, u).
u(cid:89).
u=1.
4 experiments.
we use the multilingual bert (mbert) as ourword representations3 as the sentence encoder.
fol-.
2another choice is to use pθθθ(y|x), however, we found.
that utilizing q(y) generally achieves better performance..3following previous work (wu and dredze, 2019; wu.
et al., 2020), we ﬁne-tune mbert’s parameters..4.1 datasets.
cross-lingual sequence labeling we choosethree tasks to conduct the cross-lingual sequencelabeling task, which are pos tagging, ner, andaspect extraction.
for the pos tagging task, weuse universal dependencies treebanks (ud) v2.44and randomly select ﬁve anguages together withthe english dataset.
the whole datasets are english(en), catalan (ca), indonesian (id), hindi (hi),finnish (fi), and russian (ru).
for the aspect ex-traction task, we select the restaurant domain oversubtask 1 in the semeval-2016 shared task (pon-tiki et al., 2016).
for the ner task, we evaluateour models on the conll 2002 and 2003 sharedtasks (tjong kim sang, 2002; tjong kim sang andde meulder, 2003)..cross-domain sequence labeling we use en-glish portion of the ontonotes (v5) (hovy et al.,2006), which contains six domains: broadcast con-versation (bc), broadcast news (bn), magazine (mz),newswire (nw), and web (wb)..more details can be found in the appendix a.1..4.2 approaches.
(cid:46) m step.
single-source setup the following approachesare applicable for single-source setup,.
• dt: we use the pre-trained source model todirectly predict the pseudo labels on the targetunlabeled data..• hard: we use the pseudo labels from dt onthe target unlabeled data to train a new model..multi-source setup the following approachesare applicable for multi-source setup,.
• hard-cat: we apply dt with all the sourcelanguages/domains, mix the resulting pseudolabels from all the sources on the unlabeledtarget data, and train a new model..• hard-vote: we do majority voting at the tokenlevel on the pseudo labels from dt with eachsource and train a new model..4https://universaldependencies.org/.
4913conll ner.
aspect extraction.
english german dutch spanish avg.
english spanish dutch russian turkish avg..single-source:.
the following approaches have access to hard predictions:.
dthardmrtlvm.
kd-remrtlvm.
————.
———.
72.1772.3773.1573.36.
73.7773.6773.96.
79.5480.0180.3880.34.
80.6480.5680.79.
75.1375.7575.8776.01.
76.0276.0776.29.
75.6176.0476.4776.57.
76.8176.7777.01.the following approaches have access to soft predictions:.
62.4863.7664.5365.03.
64.4465.8165.77.
53.1558.2859.6360.55.
58.6860.9160.44.
46.3548.3649.8950.59.
49.5450.5550.79.
36.4240.1345.7946.40.
43.3745.9746.69.
49.652.6354.9655.64.
54.0155.8155.92.
————.
———.
—.
wu et al.
(2020)† —.
73.22.
80.89.
76.94.
77.02.
—.
—.
—.
—.
—.
multi-source:.
the following approaches have access to hard predictions:.
hard-votehard-catmrtlvm.
kd-remrtlvm.
77.4677.1377.5678.14.
78.5778.6579.09.
73.5273.2273.8174.17.
75.2575.8376.00.
78.0578.3279.1279.60.
80.5880.5283.03.
76.6076.8176.9977.69.
77.4577.7477.66.
76.4176.3776.8777.40.
77.9678.1878.94.
57.6655.9158.6561.69.
59.2560.6660.87.
65.0363.1365.7867.49.
65.9767.5768.72.
57.2356.0158.5659.76.
59.7059.9160.14.
49.1149.3350.9052.19.
51.7151.5951.88.
45.1746.2343.7741.93.
44.8642.9742.81.
54.8454.1255.5356.61.
56.3056.5456.88.the following approaches have access to soft predictions:.
wu et al.
(2020)† —.
74.97.
80.70.
77.75 —.
—.
—.
—.
—.
—.
—.
table 2: results on the conll ner and aspect extraction tasks.
kd-re is our re-implementation for the kdapproach (wu et al., 2020).
their reported results are denoted as † for reference..bc.
bn mz.
ontonotestcnw.
wb avg..the following approaches have access to hard predictions:.
hard-votehard-catmrtlvm.
75.90 84.62 81.93 82.41 68.44 77.65 78.4975.27 84.66 81.88 82.60 71.33 77.12 78.8177.03 84.48 84.02 82.90 68.93 77.29 79.1175.93 84.76 83.37 83.26 70.56 78.34 79.37.the following approaches have access to soft predictions:.
kd-remrtlvm.
76.20 84.75 82.64 82.92 70.36 78.49 79.2376.88 84.60 84.01 83.51 70.00 77.71 79.4577.56 85.58 84.32 83.88 72.47 78.03 80.31.lan et al.
(2020)† 71.47 79.66 70.71 71.31 52.72 34.06 63.32.table 3: multi-source cross-domain results onontonotes.
kd-re is our re-implementation for the kdapproach (wu et al., 2020).
the reported results fromlan et al.
(2020) are denoted as † for reference..• mrt: our minimum risk training approachwith a ﬁxed matrix ψψψ with soft or hard predic-tions..• lvm: our latent variable model with param-eter φφφ (containing the ﬁxed matrix ψψψ and thetrainable matrix σσσ) with soft or hard predic-tions..we also provide the reported results from existingapproaches for reference.
due to different exper-iment conﬁguration reasons, directly comparingour approaches to their reported results is generallynot fair.
for the conll ner tasks, we providethe reported results from wu et al.
(2020).
for thecross-domain sequence labeling tasks, we providethe reported results from lan et al.
(2020) wholearns a consensus network to aggregate predic-tions from multiple sources..both setups the following approaches are ap-plicable for both single-/multi-source setups,.
4.3 hyper-parameters.
• kd-re: to fairly compare with the the kdapproach (wu et al., 2020) in the same settings(such as source model’s cross-lingual ability),we re-implement the kd approach and adaptit to all tasks..hyper-parameter selection in transfer learning isdifﬁcult as no labeled dataset is available for the tar-get language.
we select the hyper-parameters onlyon the development set over the english languageand directly use the selected hyper-parameters forthe other languages.
this may result in sub-optimal.
4914single sourcefihiid.
ca.
ru.
avg..en.
ca.
id.
hi.
fi.
ru.
avg..multi-source.
86.65 84.37 67.14 76.03 88.02 80.44 hard-vote 82.90 86.21 85.87 74.10 78.86 89.77 82.9583.04 85.80 86.13 74.55 78.95 90.22 83.1186.73 84.52 67.34 76.32 88.21 80.62 hard-cat82.72 85.64 86.14 74.48 78.91 89.90 82.9783.08 85.76 86.11 75.35 79.12 89.98 83.23.mrtlvm.
the following approaches have access to hard predictions:.
dthardmrt 86.78 84.61 67.63 76.97 88.36 80.87lvm 86.80 84.64 67.65 77.04 88.37 80.90the following approaches have access to soft predictions:kd-re 86.84 84.93 67.62 76.51 88.53 80.89mrt 86.57 84.65 68.44 77.51 88.40 81.11lvm 86.78 84.89 68.31 77.68 88.45 81.22.kd-remrtlvm.
83.81 86.46 86.25 74.46 79.01 90.56 83.4383.60 85.54 86.60 75.07 79.89 90.24 83.4983.85 86.76 86.50 75.41 79.60 90.23 83.73.table 4: results on the pos tagging tasks.
kd-re is our re-implementation for the kd approach (wu et al., 2020)..performance but is more realistic.
in latent variablemodel training, the latent variable is generally veryﬂexible, which may result in sub-optimal perfor-mance.
therefore, the initialization of the latentvariable is very crucial.
in practice, we ﬁnd thatthe best strategy is to initialize µ of ψψψ with a largevalue (e.g., 0.9) and µ of σσσ with a small value (e.g.,0.3), and anneal λ from 1 to 0. at the early stage oftraining, this initialization offers a strong prior forthe encoder which can keep the encoder from goingin a bad direction; and at later stages of training,the warmed-up encoder can better guide the train-ing of φφφ and vice versa.
in this way, the encoderand φφφ can achieve a good balance during training.
more details of the hyper-parameters can be foundin the appendix a.2..4.4 results and observations.
for the single-source setting, we use english as thesource language and the others as the unlabeledtarget languages.
in the multi-source setting, werepeat our experiments multiple times, each timewith a language as the target and the others as thesources.
we evaluate all approaches on the conll,aspect extraction, ontonotes, and pos tagging.
we report the results in table 2, 3 and 4 5..observation #1 our two approaches outperformseveral strong baselines on all the tasks and allthe scenarios (single-/multi-source scenarios withsoft/hard predictions), especially the multi-sourcescenario, which demonstrates the effectiveness ofthe two proposed approaches.
it shows that model-ing this kind of relation is fairly important, which.
5we utilize almost stochastic dominance (asd) test (droret al., 2019) to compare the best score of our approaches andthe score of the best performing baselines.
we mark the thehighest score as bold if its superiority is signiﬁcant (p < 0.05)and underline otherwise..helps to recover the true labels from noisy data.
meanwhile, introducing uncertainties for the rela-tions between the predicted labels from the sourcemodels and the true labels in both training andprediction processes signiﬁcantly beneﬁt our ap-proaches..observation #2 our lvm approach achievesoverall improvements over the mrt approach onall tasks.
it suggests that our lvm approach learnsthe relations between predicted labels from thesource models and true labels better than mrt..other minor observations first, all the ap-proaches that use unlabeled target data for train-ing outperform dt.
it suggests that leveraging theunlabeled target data (which may contain knowl-edge of the target language/domain) in training forzero-shot transfer learning does help.
comparingthe approaches that leverage soft instead of hardpredictions from sources, the former generally out-perform the latter.
it suggests that soft predictionscan still provide useful knowledge for samples withincorrect hard predictions.
the reported resultsfrom lan et al.
(2020) are signiﬁcantly worse.
wespeculate the reason is that they leverage poor em-beddings and different encoders (bilstm-crf).
kd-re outperforms our approaches on ca and id ofpos tagging task on the single-source setting, butits advantage is not statistically signiﬁcant..5 analysis.
we conduct the analysis on the multi-source settingwith soft predictions from sources for its betterperformance..big data performance we experiment with ourtwo models and the kd-re baseline on big targettraining data on the pos tagging task.
we ran-.
4915en.
de.
nl.
es.
avg..mrt.
hard-em 79.65 75.02 80.26 77.00 77.98soft-em 78.65 75.83 80.52 77.74 78.19.lvm.
hard-em 78.36 76.01 81.98 77.46 78.45soft-em 79.09 76.00 83.03 77.66 78.95.figure 2: the multi-source performance of ca datasetsby varying different sizes on the pos tagging task..table 6: results on hard-em experiments.
the resultsof soft-em are from table 2 of the body..en.
de.
nl.
es.
avg..mrt.
lvm.
direct‡ 78.83 75.27 80.22 77.76 78.02em†78.65 75.83 80.52 77.74 78.19.direct‡ 78.79 75.48 81.29 77.93 78.37em†79.09 76.00 83.03 77.66 78.95.table 5: results on comparisons between em algo-rithm and direct gradient-based strategy.
‡ denotes theresults of direct gradient-based strategy and † denotesthe results of em algorithm that are from table 2..domly select 100000 sentences (without labels) forthe wikipedia-003 section of the ca language onthe conll 2017 shared task (ginter et al., 2017).
we randomly select 1000, 10000, and 100000sentences to train these three approaches, evalu-ate on the ud test set for each of the three lan-guages respectively, and show the results in figure2. it shows that our latent variable model outper-forms the other two approaches over all the settings.
though kd outperform mrt with less than 10000sentences, but mrt has comparable result withenough unlabeled data.
besides, with more unla-beled data used for training, each model furthergains a considerable boost..comparison to direct gradient optimizationour two proposed approaches can also be opti-mized directly by any gradient-based approach,such as the adamw optimizer (loshchilov and hut-ter, 2018).
we use the two proposed approachesto compare the performance of the direct gradient-based training strategy and the em algorithm.
weconduct the experiments on our two proposed ap-proaches on conll ner task on the multi-sourcesetting.
we show the results in table 5. it showsthat the em algorithm outperforms direct gradient-based training for our approaches, which is slightlydifferent from previous ﬁndings (berg-kirkpatricket al., 2010)..comparison to hard em in this part, we com-pare our optimization strategy (soft-em) with thehard-em approach.
instead of computing a densevector for q(yi), hard-em computes a one-hotvector.
we conduct the experiments on our twoproposed approaches on the conll ner task onthe multi-source setting.
the results are shown intable 6. it shows that soft-em gains slightly im-provement over hard-em on the mrt approach,but differs signiﬁcantly from hard-em on our lvmapproach..impact of matrix ψψψ we analyze the relation be-tween the performance and different initializationof ψψψ.
we experiment with the mrt approach in thesingle-source setup with soft predictions on nertasks and figure 3 shows the results.
the best valueof τ is 2 for de and 3 for the others (resulting inµ = 0.43 and 0.67 respectively6), which showsthat the uncertainties introduced by a smooth ψψψ caneffectively boost the model’s performance.
on theother hand, setting ψψψ to a nearly identity matrixwith τ = 10 leads to worse scores..6 related work.
cross-lingual/domain sequence labeling re-cent works on cross-lingual transfer mainly havetwo scenarios: the single-source cross-lingual trans-fer (yarowsky and ngai, 2001; wang and manning,2014; huang et al., 2019) and the multi-sourcecross-lingual transfer (t¨ackstr¨om et al., 2012; guoet al., 2018; rahimi et al., 2019; hu et al., 2021).
wu et al.
(2020) propose a knowledge distillationapproach to further leveraging unlabeled target dataand achieve the state-of-the-art results.
hu et al.
(2021) propose a multi-view framework to selec-tively transfer knowledge from multiple sources byutilizing a small amount of labeled dataset.
cross-domain adaption is widely studied (steedman et al.,.
6the conll ner datasets have 11 labels (9 entity labels,.
a padding label and an ending label)..491686.587.588.5100010000100000acckdmrtlvmfigure 3: the performance of mrt approach in single-source setup with soft predictions on three ner datasetsby varying different τ ..2003).
existing works include bootstrapping ap-proaches (ruder and plank, 2018), mixture-of-experts (guo et al., 2018; wright and augenstein,2020), and consensus network (lan et al., 2020).
other previous work (kim et al., 2017; guo et al.,2018; huang et al., 2019) utilized labeled data inthe source domain to learn desired information.
however, our proposed approaches do not requireany source labeled data or parallel texts..contextual multilingual embeddings embed-dings like mbert (devlin et al., 2019), xlm(conneau and lample, 2019) and xlm-r (con-neau et al., 2020) which are trained on many lan-guages, make great progress on cross-lingual learn-ing for multiple nlp tasks.
recent works (wu anddredze, 2019; pires et al., 2019) show the strongcross-lingual ability of the contextual multilingualembeddings..7 conclusion.
in this paper, we propose two approaches to thezero-shot sequence labeling problem.
our mrtapproach uses a ﬁxed matrix to model the rela-tions between the predicted labels from the sourcemodels and the true labels.
our lvm approachuses trainable matrices to model these label rela-tions.
we extensively verify the effectiveness ofour approaches on both single-source and multi-source transfer over both cross-lingual and cross-domain sequence labeling problems.
experimentsshow that mrt and lvm generally bring signiﬁ-cant improvements over previous state-of-the-artapproaches on twenty-one datasets..acknowledgement.
this work was supported by the national natu-ral science foundation of china (61976139) and.
by alibaba group through alibaba innovative re-search program..references.
taylor berg-kirkpatrick, alexandre bouchard-cˆot´e,john denero, and dan klein.
2010. painless un-in human lan-supervised learning with features.
guage technologies: the 2010 annual conferenceof the north american chapter of the associationfor computational linguistics, pages 582–590, losangeles, california.
association for computationallinguistics..xilun chen, ahmed hassan awadallah, hany has-san, wei wang, and claire cardie.
2019. multi-source cross-lingual model transfer: learning whatto share.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 3098–3112, florence, italy.
associationfor computational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inacl..alexis conneau and guillaume lample.
2019.in ad-cross-lingual language model pretraining.
vances in neural information processing systems32, pages 7059–7069.
curran associates, inc..a. p. dempster, n. m. laird, and d. b. rubin.
1977.maximum likelihood from incomplete data via thein journal of the royal statisticalem algorithm.
society.
series b, volume 39, pages 1–38..steven j. derose.
1988. grammatical category disam-biguation by statistical optimization.
computationallinguistics, 14(1):31–39..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding..4917def173.673.773.974.074.123410τnl80.180.280.480.580.623410τes75.475.675.875.976.123410τof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..rotem dror, segev shlomov, and roi reichart.
2019.deep dominance - how to properly compare deepneural models.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2773–2785, florence, italy.
associa-tion for computational linguistics..filip ginter, jan hajiˇc, juhani luotolahti, milan straka,and daniel zeman.
2017. conll 2017 shared task- automatically annotated raw texts and word embed-dings.
lindat/clariah-cz digital library at theinstitute of formal and applied linguistics ( ´ufal),faculty of mathematics and physics, charles uni-versity..jiang guo, darsh shah, and regina barzilay.
2018.multi-source domain adaptation with mixture of ex-in proceedings of the 2018 conference onperts.
empirical methods in natural language processing,pages 4694–4703, brussels, belgium.
associationfor computational linguistics..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..eduard hovy, mitchell marcus, martha palmer, lanceramshaw, and ralph weischedel.
2006. ontonotes:in proceedings of the humanthe 90% solution.
language technology conference of the naacl,companion volume: short papers, pages 57–60,new york city, usa.
association for computa-tional linguistics..zechuan hu, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2020.an investigation of potential function designs forneural crf.
in findings of the association for com-putational linguistics: emnlp 2020, pages 2600–2609, online.
association for computational lin-guistics..zechuan hu, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021. multi-view cross-lingual structured predic-tion with minimum supervision.
in the joint con-ference of the 59th annual meeting of the associa-tion for computational linguistics and the 11th in-ternational joint conference on natural languageprocessing (acl-ijcnlp 2021).
association forcomputational linguistics..lifu huang, heng ji, and jonathan may.
2019. cross-lingual multi-level adversarial transfer to enhancein proceedings of thelow-resource name tagging.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long and.
short papers), pages 3823–3833, minneapolis, min-nesota.
association for computational linguistics..joo-kyung kim, young-bum kim, ruhi sarikaya, anderic fosler-lussier.
2017. cross-lingual transferlearning for pos tagging without cross-lingual re-sources.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 2832–2838, copenhagen, denmark.
associa-tion for computational linguistics..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..ouyu lan, xiao huang, bill yuchen lin, he jiang,liyuan liu, and xiang ren.
2020. learning to con-textually aggregate multi-source supervision for se-quence labeling.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 2134–2146, online.
association forcomputational linguistics..ilya loshchilov and frank hutter.
2018. fixing weight.
decay regularization in adam..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..jian ni, georgiana dinu, and radu florian.
2017.weakly supervised cross-lingual named entity recog-nition via effective annotation and representationprojection.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1470–1480, van-couver, canada.
association for computational lin-guistics..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..maria pontiki, dimitris galanis, haris papageorgiou,ion androutsopoulos, suresh manandhar, moham-mad al-smadi, mahmoud al-ayyoub, yanyanzhao, bing qin, orph´ee de clercq, v´eroniquehoste, marianna apidianaki, xavier tannier, na-talia loukachevitch, evgeniy kotelnikov, nuria bel,salud mar´ıa jim´enez-zafra, and g¨uls¸en eryi˘git.
2016. semeval-2016 task 5: aspect based senti-ment analysis.
in proceedings of the 10th interna-tional workshop on semantic evaluation (semeval-2016), pages 19–30, san diego, california.
associa-tion for computational linguistics..4918afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..lev ratinov and dan roth.
2009..design chal-lenges and misconceptions in named entity recog-in proceedings of the thirteenth confer-nition.
ence on computational natural language learning(conll-2009), pages 147–155, boulder, colorado.
association for computational linguistics..alan ritter, sam clark, mausam, and oren etzioni.
2011. named entity recognition in tweets: an ex-perimental study.
in proceedings of the 2011 con-ference on empirical methods in natural languageprocessing, pages 1524–1534, edinburgh, scotland,uk.
association for computational linguistics..sebastian ruder and barbara plank.
2018. strong base-lines for neural semi-supervised learning under do-main shift.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1044–1054, mel-bourne, australia.
association for computationallinguistics..shiqi shen, yong cheng, zhongjun he, wei he, huawu, maosong sun, and yang liu.
2016. minimumrisk training for neural machine translation.
in pro-ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1683–1692, berlin, germany.
asso-ciation for computational linguistics..mark steedman, rebecca hwa, stephen clark, milesosborne, anoop sarkar, julia hockenmaier, paulruhlen, steven baker, and jeremiah crim.
2003.example selection for bootstrapping statisticalin proceedings of the 2003 human lan-parsers.
guage technology conference of the north ameri-can chapter of the association for computationallinguistics, pages 236–243..oscar t¨ackstr¨om, ryan mcdonald, and jakob uszko-reit.
2012. cross-lingual word clusters for directin proceedings oftransfer of linguistic structure.
the 2012 conference of the north american chap-ter of the association for computational linguis-tics: human language technologies, pages 477–487, montr´eal, canada.
association for computa-tional linguistics..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
in.
proceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..kristina toutanova, dan klein, christopher d. man-ning, and yoram singer.
2003. feature-rich part-of-speech tagging with a cyclic dependency network.
in proceedings of the 2003 human language tech-nology conference of the north american chapterof the association for computational linguistics,pages 252–259..mengqiu wang and christopher d. manning.
2014.cross-lingual projected expectation regularizationfor weakly supervised learning.
transactions of theassociation for computational linguistics, 2:55–66..dustin wright and isabelle augenstein.
2020. trans-former based multi-source domain adaptation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7963–7974, online.
association for computa-tional linguistics..qianhui wu, zijia lin, b¨orje karlsson, jian-guanglou, and biqing huang.
2020. single-/multi-sourcecross-lingual ner via teacher-student learning onin proceedingsunlabeled data in target language.
of the 58th annual meeting of the association forcomputational linguistics, pages 6505–6514, on-line.
association for computational linguistics..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..yi yang and jacob eisenstein.
2015. unsupervisedmulti-domain adaptation with feature embeddings.
in proceedings of the 2015 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 672–682, denver, colorado.
association forcomputational linguistics..david yarowsky and grace ngai.
2001. inducing mul-tilingual pos taggers and np bracketers via robustprojection across aligned corpora.
in second meet-ing of the north american chapter of the associa-tion for computational linguistics..a experimental details.
a.1 datasets.
conll conll is a dataset for the ner task.
we evaluate our models on the conll 2002 and2003 shared tasks (tjong kim sang, 2002; tjongkim sang and de meulder, 2003), which containfour languages: english, german, dutch, and span-ish.
every dataset contains four types of named.
4919• ontonotes: τ = 4 and ˆτ = 10..entities: organization, location, person, and mis-cellaneous..aspect extraction we select the restaurant do-main over subtask 1 in the semeval-2016 sharedtask (pontiki et al., 2016)..ontonotes we use english portion oftheontonotes (v5) (hovy et al., 2006), which containssix domains: broadcast conversation (bc), broad-cast news (bn), magazine (mz), newswire (nw), andweb (wb).
it is a ner task which contains 18 entitytypes..a.2 hyper-parameter setting.
we select the hyper-parameters according to thestrategy which is described in the main paper.
formulti-source cross-lingual/domain tasks, we se-lect hyper-parameters based on the performanceon the english development set and apply them toother target languages.
for single-source cross-lingual/domain tasks, we simply use the samehyper-parameter as multi-source setting.
in theinference step, we use pθθθ(y|x) in single-sourcecross-lingual/domain and q(y) in multi-sourcecross-lingual/domain to predict the label sequence.
we empirically set the learning rate of mbert as2e-5 and the learning rate of φ and φφφ as 2e-4 formulti-source setup and 2e-5 for single-source setup.
we train each model for three epochs.
we tune thefollowing hyper-parameters..τ and ˆτ areτ and ˆτ for initializing matricesused to initialize the matrices ψψψ and σσσ in our min-imum risk training and latent variable model ap-proaches respectively.
due to different sizes of thelabel sets for different tasks, the range of selectionis different.
take the conll ner tasks for exam-ple, we tune it in the range of {1, 2, 3, 4, 10} for ˆτin ψψψ in mrt and lvm, and {1, 2, 3, 4, 10} for τ inσσσ in lvm.
the conll ner tasks have 11 labels (9entity labels, a padding label and an ending label),which means µ ∈ {0.21, 0.43, 0.67, 0.85, 1.0}.
we list the value we select for each task below:.
• conll ner: τ = 3 and ˆτ = 2 for single-source setup; τ = 2 and ˆτ = 10 for multi-source setup..• ae: τ = 3 and ˆτ = 4 for single-source setup;;τ = 2 and ˆτ = 10 for multi-source setup..• pos: τ = 4 and ˆτ = 2 for single-sourcesetup; τ = 2 and ˆτ = 10 for multi-sourcesetup..4920