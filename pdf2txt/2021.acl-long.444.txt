modeling bilingual conversational characteristicsfor neural chat translation.
yunlong liang1∗, fandong meng2, yufeng chen1, jinan xu1† and jie zhou21beijing key lab of trafﬁc data analysis and mining,beijing jiaotong university, beijing, china2pattern recognition center, wechat ai, tencent inc, china{yunlongliang,chenyf,jaxu}@bjtu.edu.cn{fandongmeng,withtomzhou}@tencent.com.
abstract.
neural chat translation aims to translate bilin-gual conversational text, which has a broad ap-plication in international exchanges and coop-eration.
despite the impressive performanceof sentence-level and context-aware neuralmachine translation (nmt), there still remainchallenges to translate bilingual conversationaltext due to its inherent characteristics such asrole preference, dialogue coherence, and trans-lation consistency.
in this paper, we aim to pro-mote the translation quality of conversationaltext by modeling the above properties.
specif-ically, we design three latent variational mod-ules to learn the distributions of bilingual con-versational characteristics.
through samplingfrom these learned distributions, the latent vari-ables, tailored for role preference, dialogue co-herence, and translation consistency, are incor-porated into the nmt model for better transla-tion.
we evaluate our approach on the bench-mark dataset bcontrast (english⇔german)and a self-collected bilingual dialogue cor-pus, named bmeld (english⇔chinese).
ex-tensive experiments show that our approachnotably boosts the performance over strongbaselines by a large margin and signiﬁcantlysurpasses some state-of-the-art context-awarenmt models in terms of bleu and ter.
ad-ditionally, we make the bmeld dataset pub-licly available for the research community.1.
1.introduction.
a conversation may involve participants that speakin different languages (e.g., one speaking in en-glish and another in chinese).
fig.
1 shows anexample, where the english role r1 and the chi-nese role r2 are talking about the “boat”.
the.
∗ work was done when yunlong liang was interning atpattern recognition center, wechat ai, tencent inc, china..† jinan xu is the corresponding author.
1code and data are publicly available at: https://.
github.com/xl2248/cpcc.
figure 1: an ongoing bilingual conversation example(english⇔chinese), where the chinese utterances arepresented in pinyin style.
ri: role i. the dashed ar-rows mark the translation direction.
the green and redarrows represent the monolingual and bilingual conver-sation ﬂow, respectively.
although the translation ofy5 produced by the “s-nmt” (a context-free sentence-level nmt system) is reasonable at the sentence level,the coherence of the entire dialogue translation is poor..goal of chat translation is to translate bilingual con-versational text, i.e., converting one participant’slanguage (e.g., english) to another’s (e.g., chinese)and vice versa (farajian et al., 2020).
it enablesmultiple speakers to communicate with each otherin their native languages, which has a wide appli-cation in industry-level services..although sentence-level neural machine trans-lation (nmt) (sutskever et al., 2014; vaswaniet al., 2017; meng and zhang, 2019; hassan et al.,2018; yan et al., 2020; zhang et al., 2019) hasachieved promising progress, it still faces chal-lenges in accurately translating conversational textdue to abandoning the dialogue history, whichleads to role-irrelevant, incoherent and inconsis-tent translations (mirkin et al., 2015; wang et al.,2017a; l¨aubli et al., 2018; toral et al., 2018).
fur-ther, context-aware nmt (tiedemann and scherrer,2017; voita et al., 2018, 2019a,b; wang et al., 2019;maruf and haffari, 2018; maruf et al., 2019; maet al., 2020) can be directly applied to chat trans-lation through incorporating the dialogue historybut cannot obtain satisfactory results in this sce-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5711–5724august1–6,2021.©2021associationforcomputationallinguistics5711y2: nǐ  huì  jiàshǐ  fānchuán？x1: you know, joey, i could teach you to sail, if you want.?
y3: duìa, wǒ zhè bèizǐ dōu zài jiàchuán, wǒ shíwǔ suì shí, wǒ bà song wǒ yì sōu chuán。y4: nǐ yǒu yì sōu fānchuán？x2: you could?x3: yeah!
i've been sailing my whole life.
when i was fifteen, my dad bought me my own boat.x4: your own boat?x5: what?
what?
he was trying to cheer me up!
my pony was sick.ref: zěnme?
bù xìn?
tā song wǒ yì sōu chuán lái ānwèi wǒ, wǒ de xiǎomǎ bìng ler1r2r2r1r1y5:s-nmt: shénme?
shénme?
tā xiǎng rang wǒ gāoxìng qǐlái,wǒ de xiǎomǎ bìng le。。y1: qiáoyī, rúguǒ nǐ xiǎng, wǒ kěyǐ  jiào nǐ jiàchuán。？nario (moghe et al., 2020).
one important reasonis the lack of explicitly modeling the inherent bilin-gual conversational characteristics, e.g., role pref-erence, dialogue coherence, and translation consis-tency, as pointed out by farajian et al.
(2020)..for a conversation, its dialogue history con-tains rich role preference information such as emo-tion, style, and humor, which is beneﬁcial to role-relevant utterance generation (wu et al., 2020).
asshown in fig.
1, the utterances x1, x3 and x5from role r1 always have strong emotions (i.e.,joy) because of his/her preference, and preservingthe same preference information across languagescan help raise emotional resonance and mutual un-derstanding (moghe et al., 2020).
meanwhile, thereexists semantic coherence in the conversation, asthe solid green arrow in fig.
1, where the utter-ance x5 naturally and semantically connects withthe dialogue history (x1∼4) on the topic “boat”.
inaddition, the bilingual conversation exhibits transla-tion consistency, where the correct lexical choice totranslate the current utterance might have appearedin preceding turns.
for instance, the word “sail” inx1 is translated into “ji`achu´an”, and thus the word“sailing” in x3 should be mapped into “ji`achu´an”rather than other words (e.g., “h´angx´ıng”2) to main-tain translation consistency.
on the contrary, if weignore these characteristics, translations might berole-irrelevant, incoherent, inconsistent, and detri-mental to further communication like the transla-tion produced by the “s-nmt” in fig.
1. althoughthe translation is acceptable at the sentence level, itis abrupt at the bilingual conversation level..apparently, how to effectively exploit these bilin-gual conversational characteristics is one of thecore issues in chat translation.
and it is chal-lenging to implicitly capture these properties byjust incorporating the complex dialogue historyinto encoders due to lacking the relevant informa-tion guidance (farajian et al., 2020).
on the otherhand, the conditional variational auto-encoder(cvae) (sohn et al., 2015) has shown its superi-ority in learning distributions of data properties,which is often utilized to model the diversity (zhaoet al., 2017), coherence (wang and wan, 2019) andusers’ personalities (bak and oh, 2019), etc.
inspite of its success, adapting it to chat translation isnon-trivial, especially involving multiple tailoredlatent variables..2the words “ji`achu´an” and “h´angx´ıng” express similar.
meaning..therefore, in this paper, we propose a model,named cpcc, to capture role preference, dialoguecoherence, and translation consistency with latentvariables learned by the cvae for neural chat trans-lation.
cpcc contains three speciﬁc latent varia-tional modules to learn the distributions of role pref-erence, dialogue coherence, and translation con-sistency, respectively.
speciﬁcally, we ﬁrstly useone role-tailored latent variable, sampled from thelearned distribution conditioned only on the utter-ances from this role, to preserve preference.
then,we utilize another latent variable, generated by thedistribution conditioned on source-language dia-logue history, to maintain coherence.
finally, weleverage the last latent variable, generated by thedistribution conditioned on paired bilingual conver-sational utterances, to keep translation consistency.
as a result, these tailored latent variables allow ourcpcc to produce role-speciﬁc, coherent, and con-sistent translations, and hence make the bilingualconversation go ﬂuently..we conduct experiments on wmt20 chat trans-lation dataset: bcontrast (en⇔de3) (farajianet al., 2020) and a self-collected dialogue corpus:bmeld (en⇔ch).
results demonstrate that ourmodel achieves consistent improvements in fourdirections in terms of bleu (papineni et al., 2002)and ter (snover et al., 2006), showing its effec-tiveness and generalizability.
human evaluationfurther suggests that our model effectively allevi-ates the issue of role-irrelevant, incoherent and in-consistent translations compared to other methods.
our contributions are summarized as follows:.
• to the best of our knowledge, we are the ﬁrstto incorporate the role preference, dialoguecoherence, and translation consistency intoneural chat translation..• we are the ﬁrst to build a bridge between thedialogue and machine translation via condi-tional variational auto-encoder, which effec-tively models three inherent characteristics inbilingual conversation for neural chat transla-tion..• our approach gains consistent and signiﬁcantperformance over the standard context-awarebaseline and remarkably outperforms somestate-of-the-art context-aware nmt models.
• we contribute a new bilingual dialogue corpus(bmeld, en⇔ch) with manual translationsand our codes to the research community..3english⇔german: en⇔de.
english⇔chinese: en⇔ch..57122 background.
2.1 sentence-level nmtgiven an input sentence x={xi}mi=1 with m to-kens, the model is asked to produce its translationy ={yi}ni=1 with n tokens.
the conditional distri-bution of the nmt is:.
pθ(y |x) =.
pθ(yt|x, y1:t−1),.
n(cid:89).
t=1.
where θ are model parameters and y1:t−1 is thepartial translation..2.2 context-aware nmtgiven a source context dx ={xi}ji=1 and a tar-get context dy ={yi}ji=1 with j aligned sentencepairs (xi, yi), the context-aware nmt (ma et al.,2020) is formalized as:j(cid:89).
pθ(dy |dx ) =.
pθ(yi|xi, x<i, y<i),.
i=1where x<i and y<i are the preceding context..2.3 variational nmt.
the variational nmt model (zhang et al., 2016) isthe combination of cvae (sohn et al., 2015) andnmt.
it introduces a random latent variable z intothe nmt conditional distribution:(cid:90).
pθ(y |x) =.
pθ(y |x, z) · pθ(z|x)dz..(1).
z.given a source sentence x, a latent variable z isﬁrstly sampled by the prior network from the en-coder, and then target sentence is generated by thedecoder: y ∼ pθ(y |x, z), where z ∼ pθ(z|x)..as it is hard to marginalize eq.
1, the cvaetraining objective is a variational lower bound ofthe conditional log-likelihood:.
l(θ, φ; x, y ) = −kl(qφ(z|x, y )(cid:107)pθ(z|x))+ eqφ(z|x,y )[log pθ(y |z, x)]≤ log p(y |x),where φ are parameters of the posterior networkand kl(·) indicates kullback–leibler divergencebetween two distributions produced by prior net-works and posterior networks (sohn et al., 2015;kingma and welling, 2013)..3 chat nmt.
we aim to learn a model that can capture inher-ent characteristics in the bilingual dialogue his-tory for producing high-quality translations, i.e.,using the context for better translations (farajian.
figure 2: a dialogue example (en⇔ch) when translat-ing the utterance x2k+1 where k ∈ [0, |t |−1] and t isthe total number of turns (assumed to be odd here)..2.et al., 2020).
following (maruf et al., 2018), we de-ﬁne paired bilingual utterances (xi, yi) as a turn infig.
2, where we will translate the current utterancex2k+1 at the (2k + 1)-th turn.
here, we denote theutterance x2k+1 as xu and its translation y2k+1 asyu for simplicity, where xu={xi}mi=1 with m to-kens and yu={yi}ni=1 with n tokens.
formally, theconditional distribution for the current utterance is.
pθ(yu|xu, c) =.
pθ(yt|xu, y1:t−1, c),.
n(cid:89).
t=1.
where c is the bilingual dialogue history..before we dig into the details of how to uti-lize c, we deﬁne three types of contextinc (as shown in fig.
2):(1) the set of previ-ous role-speciﬁc source-language turns, denotedas crolex ={x1, x3, x5, ..., x2k+1}4 where k ∈[0, |t |−3] and t is the total number of turns;(2) the set of previous source-language turns, de-noted as cx ={x1, x2, x3, ..., x2k}; and (3) theset of previous target-language turns, denoted ascy ={y1, y2, y3, ..., y2k}..2.
4 our methodology.
fig.
3 demonstrates an overview of our model,input represen-consisting of ﬁve components:tation, encoder, latent variational modules, de-coder, and training objectives.
speciﬁcally, weaim to model both dialogue and translation simul-taneously.
therefore, for the input representation(§ 4.1), we incorporate dialogue-level embeddings,i.e., role and dialogue turn embeddings, into theencoder (§ 4.2).
then, we introduce three spe-ciﬁc latent variational modules (§ 4.3) to learn thedistributions for varied inherent bilingual character-istics.
finally, we elaborate on how to incorporatethe three tailored latent variables sampled from.
4c role.
y ={y2, y4, y6, ..., y2k} is also role-speciﬁc utter-ances of the interlocutor, which is used to model the inter-locutor’s consistency in the reverse translation direction.
here,we take one translation direction (i.e., en⇒ch) as an example..5713y2: nǐhǎo,wǒ de yígè péngyǒu hé wǒ...x1: hi there, how can i help today?y3: dāngrán, shuǐzúguǎn shì fēicháng     hǎo de ...y2k: xīyātú fùháo yǐngyuàn。y1: nǐhǎo, jīntiān yǒu shénme nénggóu xiàoláo de?x2: hey.
a friend of mine and i ...x3: sure, aquaman is a great ...x2k: seattle regal cinemas.x2k+1: great, regal cinemas...y2k+1: tàibàng le, dìwáng diànyǐngyuàn...turn number1232ksource-language turns set cxtarget-language turns set cy2k+1...r1r1r1r2r2......nmtfigure 3: overview of our cpcc.
the latent variables zrole, zdia, and ztra are tailored for maintaining the rolepreference, dialogue coherence, and translation consistency, respectively.
the solid grey lines indicate trainingprocess responsible for generating {zrole, zdia, ztra} from the corresponding posterior distribution predicted byrecognition networks.
the dashed red lines indicate inference process for generating {zrole, zdia, ztra} from thecorresponding prior distributions predicted by prior networks.
the ﬁrst transformer layer is shared with all inputs..the distributions into the decoder (§ 4.4) and ourtwo-stage training objectives (§ 4.5)..4.1.input representation.
the cpcc contains three types of inputs: sourceinput xu, target input yu, and context inputs{crolex , cx , cy }.
apart from the conventionalword embeddings we and position embeddingspe (vaswani et al., 2017), we also introduce roleembeddings re and dialogue turn embeddingste to identify different utterances.
speciﬁcally,for xu, we ﬁrstly project it into these embeddings.
then, we perform a sum operation to unify theminto a single input for each token xi:h0(2)where 1 ≤ i ≤ m and we ∈ r|v |×d, re ∈r|r|×d and se ∈ r|t |×d.
|v |, |r|, |t |, and d de-note the size of shared vocabulary, number of roles,max turns of dialogue, and hidden size, respectively.
h0 ∈ rm×d, similarly for yu.
for each of {crolex ,cx , cy }, we add ‘[cls]’ tag at the head of it anduse ‘[sep]’ tag to separate its utterances (devlinet al., 2019), and then get its embeddings via eq.
2..i = we(xi) + pe(xi) + re(xi) + te(xi),.
4.2 encoder.
the transformer encoder consists of ne stackedlayers and each layer includes two sub-layers:5 amulti-head self-attention (selfatt) sub-layer anda position-wise feed-forward network (ffn) sub-layer (vaswani et al., 2017):e = selfatt(h(cid:96)−1s(cid:96)ee) + s(cid:96)e = ffn(s(cid:96)h(cid:96).
, h(cid:96)−1e} ∈ rm×d,.)
+ h(cid:96)−1ee, s(cid:96)e, {h(cid:96).
e ∈ rm×d,.
5we omit the layer normalization for simplicity, and you.
may refer to (vaswani et al., 2017) for more details..where h(cid:96)and h0.
e denotes the state of the (cid:96)-th encoder layer.
e denotes the initialized feature h0..e,i.
e(cid:80)mi=1(mx.
we prepare the representations of xu and{crolex , cx , cy } for training prior and recognitionnetworks.
for xu, we apply mean-pooling withmask operation over the output hne,xof the ne-i hne,xth encoder layer, i.e., hx = 1),mhx ∈ rd, where mx ∈ rm denotes the mask ma-trix, whose value is either 1 or 0 indicating whetherthe token is padded (zhang et al., 2016).
for crolex ,as shown in fig.
3, we follow (ma et al., 2020) andshare the ﬁrst encoder layer to obtain the contextrepresentation.
here, we take the hidden state ofrole ∈ rd.
‘[cls]’ as its representation, denoted as hctxsimilarly, we obtain representations of cx and cy ,y ∈ rd, respectively.
denoted as hctxfor training recognition networks, we obtain thei hne,y),.
representation of yu as hy = 1nhy ∈ rd, where my ∈ rn, similar to mx ..x ∈ rd and hctx.
i=1(my.
(cid:80)n.e,i.
4.3 latent variational modules.
we design three tailored latent variational mod-ules to learn the distributions of inherent bilingualconversational characteristics, i.e., role preference,dialogue coherence, and translation consistency..role preference.
to preserve the role prefer-ence when translating the role’s current utterance,we only encode the previous utterances of thisrole and produce a role-tailored latent variablezrole ∈ rdz , where dz is the latent size.
inspiredby (wang and wan, 2019), we use isotropic gaus-sian distribution as the prior distribution of zrole:x ) ∼ n (µrole, σ2pθ(zrole|xu, crolerolei), where i.
5714softmax target inputs yu  (y1:t-1)transformerdecodertransformer layer 1cxtransformer layer 1cytransformer layer 1croletransformer layer netransformer layer 1target inputs yu  recognition prior networkroleprior networkdiaprior networktrarecognition   recognition networkrolezrolezdiaztrazrolezdiaztrazrolezdiaztrazrolezdiaztratraininginferencekl(q(zrole)|| p(zrole))kl(q(zdia)|| p(zdia))kl(q(ztra)|| p(ztra))...transformer layer netransformer layer 1source inputs xu...xlogp(yt | y1:t-1, xu, zrole, zdia, ztra)denotes the identity matrix and we have.
µrole = mlprole(hx ; hctxθσrole = softplus(mlprole.
role),.
θ.
(hx ; hctx.
role)),.
where mlp(·) and softplus(·) are multi-layer per-ceptron and approximation of relu function, re-spectively.
(·;·) indicates concatenation operation.
at training, the posterior distribution conditionson both role-speciﬁc utterances and the currenttranslation, which contain rich role preference in-formation.
therefore, the prior network can learn arole-tailored distribution by approaching the poste-rior network via kl divergence (sohn et al., 2015):qφ(zrole|xu, crolex , yu) ∼ n (µ(cid:48)rolei) androle, σ(cid:48){µ(cid:48)role = mlproleµ(cid:48)(hx ; hctxφrole = softplus(mlproleσ(cid:48).
role} are calculated as:.
(hx ; hctx.
role; hy ))..role; hy ),.
role, σ(cid:48)2.φ.dialogue coherence.
to maintain the coherencein chat translation, we encode the entire source-language utterances and then generate a latentvariable zdia ∈ rdz .
similar to zrole, we de-ﬁne its prior distribution as: pθ(zdia|xu, cx ) ∼n (µdia, σ2diai) and {µdia, σdia} are calculated as:x ),θ (hx ; hctx.
θ (hx ; hctxµdia = mlpdiaσdia = softplus(mlpdia.
x ))..at training, the posterior distribution condi-tions on both the entire source-language utter-ances and the translation that provide a dialogue-level coherence clue, and is responsible for guid-ing the learning of the prior distribution.
specif-ically, we deﬁne the posterior distribution as:qφ(zdia|xu, cx , yu) ∼ n (µ(cid:48)diai), whereµ(cid:48)dia are calculated as:.
dia and σ(cid:48).
dia, σ(cid:48)2.φ (hx ; hctxdia = mlpdiaµ(cid:48)dia = softplus(mlpdiaσ(cid:48).
x ; hy ),φ (hx ; hctx.
x ; hy ))..translation consistency.
to keep the lexicalchoice of translation consistent with those of pre-vious utterances, we encode the paired source-target utterances and then sample a latent vari-able ztra ∈ rdz .
we deﬁne its prior distributionas: pθ(ztra|xu, cx , cy ) ∼ n (µtra, σ2trai) and{µtra, σtra} are calculated as:µtra = mlptraθ (hx ; hctxσtra = softplus(mlptraat training, the posterior distribution condi-tions on all paired bilingual dialogue utterancesthat contain implicit and aligned information,.
x ; hctxy ),θ (hx ; hctx.
x ; hctx.
y ))..and serves as learning of the prior distribution.
speciﬁcally, we deﬁne the posterior distributionas: qφ(ztra|xu, cx , cy , yu) ∼ n (µ(cid:48)trai),where µ(cid:48)tra and σ(cid:48)tra are calculated as:x ; hctxφ (hx ; hctxtra = mlptraµ(cid:48)y ; hy ),φ (hx ; hctxtra = softplus(mlptraσ(cid:48).
y ; hy ))..x ; hctx.
tra, σ(cid:48)2.
4.4 decoder.
d, hne.
d, {c(cid:96).
d) + c(cid:96).
, h(cid:96)−1d, s(cid:96)d} ∈ rn×d,.
d = selfatt(h(cid:96)−1s(cid:96)dc(cid:96)d = crossatt(s(cid:96)d = ffn(c(cid:96)h(cid:96)where h(cid:96).
the decoder adopts a similar structure to the en-coder, and each of nd decoder layers contains anadditional cross-attention sub-layer (crossatt):d ∈ rn×d,d ∈ rn×d,.)
+ h(cid:96)−1de ) + s(cid:96)d, h(cid:96)d denotes the state of the (cid:96)-th decoder layer.
as shown in fig.
3, we obtain the latent variables{zrole, zdia, ztra} either from the posterior distri-bution predicted by recognition networks (trainingprocess as the solid grey lines) or from prior dis-tribution predicted by prior networks (inferenceprocess as the dashed red lines).
finally, we incor-porate {zrole, zdia, ztra} into the state of the toplayer of the decoder with a projection layer:ot = tanh(wp[hndd,t ; zrole; zdia; ztra] + bp), ot ∈ rd,where wp ∈ rd×(d+3dz) and bp ∈ rd are trainingparameters, hndd,t is the hidden state at time-step tof the nd-th decoder layer.
then, ot is fed to alinear transformation and softmax layer to predictthe probability distribution of the next target token:pt = softmax(woot + bo), pt ∈ r|v |,where wo ∈ r|v |×d and bo ∈ r|v | are trainingparameters..4.5 training objectives.
we apply a two-stage training strategy (zhang et al.,2018; ma et al., 2020).
firstly, we train our modelon large-scale sentence-level nmt data to mini-mize the cross-entropy objective:.
l(θ; x, y ) = −.
logpθ(yt|x, y1:t−1)..n(cid:88).
t=1.
x , cx , cy , yu) =.
secondly, we ﬁne-tune it on the chat translationdata to maximize the following objective:j (θ, φ; xu, crole− kl(qφ(zrole|xu, crole− kl(qφ(zdia|xu, cx , yu)(cid:107)pθ(zdia|xu, cx ))− kl(qφ(ztra|xu, cx , cy , yu)(cid:107)pθ(ztra|xu, cx , cy ))+ eqφ[logpθ(yu|xu, zrole, zdia, ztra)]..x , yu)(cid:107)pθ(zrole|xu, crole.
x )).
5715we use the reparameterization trick (kingma andwelling, 2013) to estimate the gradients of the priorand recognition networks (zhao et al., 2017)..5 experiments.
5.1 datasets and metrics.
datasets.
we apply a two-stage training strategy,i.e., ﬁrstly training on a large-scale sentence-levelnmt corpus (wmt206) and then ﬁne-tuning onchat translation corpus (bcontrast (farajian et al.,2020)7 and bmeld).
the details (wmt20 dataand results of the ﬁrst stage) are shown in appendixa..bcontrast.
the dataset8 is ﬁrst provided bywmt 2020 chat translation task (farajian et al.,2020), which is translated from english into ger-man and is based on the monolingual taskmaster-1corpus (byrne et al., 2019).
the conversations(originally in english) were ﬁrst automaticallytranslated into german and then manually post-edited by unbabel editors,9 who are native ger-man speakers.
having the conversations in bothlanguages allows us to simulate bilingual conversa-tions in which one speaker, the customer, speaks ingerman and the other speaker, the agent, answersin english..bmeld.
similarly, based on the dialoguedataset in the meld (originally in english) (poriaet al., 2019),10 we ﬁrstly crawled the correspond-ing chinese translations from this11 and then man-ually post-edited them according to the dialoguehistory by native chinese speakers, who are post-graduate students majoring in english.
finally,following (farajian et al., 2020), we assume 50%speakers as chinese speakers to keep data balancefor ch⇒en translations and build the bilingualmeld (bmeld).
for the chinese, we segmentthe sentence using stanford corenlp toolkit12..metrics.
for fair comparison, we use the sacre-bleu13 (post, 2018) and v0.7.25 for ter (snover.
6http://www.statmt.org/wmt20/translation-task.html7http://www.statmt.org/wmt20/chat-task.html8https://github.com/unbabel/bcontrast9www.unbabel.com10the meld is a multimodal emotionlines dialoguedataset, each utterance of which corresponds to a video, voice,and text, and is annotated with detailed emotion and sentiment..11https://www.zimutiantang.com/12https://stanfordnlp.github.io/corenlp/index.html13bleu+case.mixed+numrefs.1+smooth.exp+tok.13a+.
version.1.4.13.
dataset.
en⇒dede⇒enen⇒chch⇒en.
# dialoguestrain valid test7878274274.
5505501,0361,036.
7878108108.
# utterances.
train valid1,0407,6298626,2165675,5605174,427.test1,1339671,4661,135.table 1: statistics of chat translation data..et al., 2006) (the lower the better) with the sta-tistical signiﬁcance test (koehn, 2004).
foren⇔de, we report case-sensitive score follow-ing the wmt20 chat task (farajian et al., 2020).
for ch⇒en, we report case-insensitive score.
foren⇒ch, we report the character-level bleu score..5.2.implementation details.
for all experiments, we follow the transformer-base and transformer-big settings illustratedin (vaswani et al., 2017).
in transformer-base,we use 512 as hidden size (i.e., d), 2048 as ﬁl-ter size and 8 heads in multi-head attention.
intransformer-big, we use 1024 as hidden size, 4096as ﬁlter size, and 16 heads in multi-head attention.
all our transformer models contain ne = 6 encoderlayers and nd = 6 decoder layers and all modelsare trained using thumt (tan et al., 2020) frame-work.
we conduct experiments on the validationset of en⇒de to select the hyperparameters of con-text length and latent dimension, which are thenshared for all tasks.
for the results and more details(other hyperparameters setting and average runningtime), please refer to appendix b, c, and d..5.3 comparison models.
baseline nmt models.
transformer (vaswaniet al., 2017): the de-facto nmt model that does notﬁne-tune on chat translation data.
transformer+ft:ﬁne-tuning on the chat translation data after beingpre-trained on sentence-level nmt corpus..nmt.
models.
doc-context-awarea state-transformer+ft (ma et al., 2020):of-the-art document-level nmt model basedon transformer sharing the ﬁrst encoder layerto incorporate the bilingual dialogue history.
dia-transformer+ft (maruf et al., 2018): us-ing an additional rnn-based (hochreiter andschmidhuber, 1997) encoder to incorporate themixed-language dialogue history, where were-implement it based on transformer and useanother transformer layer to introduce context.
v-transformer+ft (zhang et al., 2016; mccarthy.
5716en⇒de.
de⇒en.
en⇒ch.
ch⇒en.
models.
baselinenmt models (base).
context-awarenmt models (base).
ours (base)baselinenmt models (big).
context-awarenmt models (big).
ours (big).
42.526.727.126.826.3.
33.426.225.726.227.0.
48.3859.5759.4659.0958.67.
21.4025.2224.7624.9626.82.bleu↑ ter↓ bleu↑ ter↓ bleu↑ ter↓ bleu↑ ter↓59.140.02transformer56.7transformer+ft58.4359.8doc-transformer+ft 58.1560.1dia-transformer+ft 58.3358.74v-transformer+ft56.360.13†† 25.4†† 61.05†† 24.9†† 27.55† 60.1† 22.50† 55.7†cpcc57.740.53transformer56.1transformer+ft59.01doc-transformer+ft 58.6157.758.1dia-transformer+ft 58.6858.70v-transformer+ft55.960.23†† 25.6† 61.45†† 24.8† 28.98†† 59.0†† 22.98† 54.6††cpcc.
18.5221.5920.6120.4921.86.
19.5822.1521.3821.0922.24.
22.8126.9526.4526.7227.52.
49.9059.9859.9859.6360.01.
69.660.762.662.460.3.
72.462.863.463.760.6.
33.325.925.426.025.7.
42.226.026.526.826.2.table 2: results on bcontrast (en⇔de) and bmeld (en⇔ch) in terms of bleu (%) and ter (%).
the bestand the second results are bold and underlined, respectively.
“†” and “††” indicate that statistically signiﬁcant betterthan the best result of all contrast nmt models with t-test p < 0.05 and p < 0.01, respectively..en⇒de.
de⇒en.
#.
models.
bleu↑60.96.ter↓ bleu↑62.0924.6.ter↓0 cpcc (base)24.51 w/o zrole60.56 (-0.40) 25.1 61.42 (-0.67) 24.82 w/o zdia60.50 (-0.46) 25.2 61.65 (-0.44) 25.13 w/o ztra60.39 (-0.57) 25.1 61.38 (-0.71) 26.04 w/o zrole & zdia 59.64 (-1.32) 25.8 60.65 (-1.44) 25.85 w/o zrole & ztra 59.61 (-1.35) 25.9 60.62 (-1.47) 25.76 w/o zdia & ztra 60.24 (-0.72) 25.1 61.18 (-0.91) 24.97 w/o all58.95 (-2.01) 26.1 59.82 (-2.27) 26.1.table 3: ablation study on the validation set.
“w/o all”indicates removing all latent variables but remainingencoding all bilingual dialogue history..et al., 2020): the variational nmt model based ontransformer also sharing the ﬁrst encoder layer toexploit the bilingual context for fair comparison..5.4 main results.
overall, we separate the models into two parts intab.
2: the base setting and the big setting.
in eachpart, we show the results of our re-implementedtransformer baselines, the context-aware nmt sys-tems, and our approach on en⇔de and en⇔ch..results on en⇔de.
under the base setting,cpcc substantially outperforms the baselines (e.g.,“transformer+ft”) by a large margin with 1.70↑and 1.48↑ bleu scores on en⇒de and de⇒en,respectively.
on the ter, our cpcc achieves asigniﬁcant improvement of 1.3 points in both lan-guage pairs.
under the big setting, our cpcc alsoconsistently boosts the performance in both direc-.
tions (i.e., 1.22↑ and 1.47↑ bleu scores, 0.4↓ and1.1↓ ter scores), showing its effectiveness..compared against.
the strong context-awarenmt systems (underlined results), our cpccsigniﬁcantly surpasses them (about 1.39∼1.59↑bleu scores and 0.6∼0.9↓ ter scores) in bothlanguage directions under both base and big set-tings, demonstrating the superiority of our model..results on en⇔ch.
we also conduct experi-ments on our self-collected data to validate thegeneralizability across languages in tab.
2..our cpcc presents remarkable bleu improve-ments over the “transformer+ft” by a large mar-gin in two directions by 2.33↑ and 0.91↑ bleugains under the base setting, respectively, and by2.03↑ and 0.83↑ bleu gains in both directionsunder the big setting.
these results suggest thatcpcc consistently performs well across languages.
compared with strong context-aware nmt sys-tems (e.g., “v-transformer+ft”), our approach no-tably surpasses them in both language directionsunder both base and big settings, which shows thegeneralizability and superiority of our model..6 analysis.
6.1 ablation study.
we conduct ablation studies to investigate how welleach tailored latent variable of our model works.
when removing latent variables listed in tab.
3, wehave the following ﬁndings..5717(1) all latent variables make substantial contri-butions to performance, proving the importance ofmodeling role preference, dialogue coherence, andtranslation consistency, which is consistent withour intuition that the properties should be beneﬁ-cial to better translations (rows 1∼3 vs. row 0)..(2) results of rows 4∼7 show the combinationeffect of three latent variables, suggesting that thecombination among three latent variables has acumulative effect (rows 4∼7 vs. rows 0∼3)..(3) row 7 vs. row 0 shows that explicitly model-ing the bilingual conversational characteristics sig-niﬁcantly outperforms implicit modeling (i.e., justincorporating the dialogue history into encoders),which lacks the relevant information guidance..6.2 dialogue coherence.
following (lapata and barzilay, 2005; xiong et al.,2019), we measure dialogue coherence as sentencesimilarity.
speciﬁcally, the representation of eachsentence is the mean of the distributed vectors ofits words, and the dialogue coherence between twosentences s1 and s2 is determined by the cosinesimilarity:.
sim(s1, s2) = cos(f (s1), f (s2)),1|si|.
f (si) =.
(w),.
(cid:88).
w∈si.
where w is the vector for word w..we use word2vec14 (mikolov et al., 2013) tolearn the distributed vectors of words by trainingon the monolingual dialogue dataset: taskmaster-1 (byrne et al., 2019).
and we set the dimensional-ity of word embeddings to 100..tab.
4 shows the cosine similarity on the testset of de⇒en.
it reveals that our model encour-aged by tailor-made latent variables produces bettercoherence in chat translation than contrast systems..6.3 human evaluation.
inspired by (bao et al., 2020; farajian et al., 2020),we use four criteria for human evaluation: (1) pref-erence measures whether the translation preservesthe role preference information; (2) coherencedenotes whether the translation is semantically co-herent with the dialogue history; (3) consistencymeasures whether the lexical choice of translationis consistent with the preceding utterances; (4) flu-ency measures whether the translation is logicallyreasonable and grammatically correct..14https://code.google.com/archive/p/word2vec/.
1-th pr.
2-th pr.
3-th pr.
models0.6502 0.6037 0.5659transformertransformer+ft0.6587 0.6104 0.5714doc-transformer+ft 0.6569 0.6093 0.5713dia-transformer+ft 0.6553 0.6084 0.57090.6602 0.6122 0.5751v-transformer+ft0.6660†† 0.6190†† 0.5814††cpcc (ours)0.6663 0.6190 0.5795human reference.
table 4: results of dialogue coherence in terms of sen-tence similarity (de⇒en, base).
the “#-th pr.” de-notes the #-th preceding utterance to the current one.
“††” indicates that statistically signiﬁcant better than thebest result of all contrast nmt models (p < 0.01)..pref.
coh.
con.
flu.
models0.485 0.540 0.510 0.590transformertransformer+ft0.530 0.590 0.565 0.635doc-transformer+ft 0.525 0.595 0.560 0.630dia-transformer+ft 0.525 0.580 0.555 0.6250.535 0.595 0.560 0.635v-transformer+ft0.570 0.620 0.585 0.650cpcc (ours).
table 5: results of human evaluation (ch⇒en, base).
“pref.”: preference.
“coh.”: coherence.
“con.”: con-sistency.
“flu.”: fluency..we ﬁrstly randomly sample 200 examples fromthe test set of ch⇒en.
then, we assign each bilin-gual dialogue history and corresponding 6 gener-ated translations to three human annotators withoutorder, and ask them to evaluate whether each trans-lation meets the criteria deﬁned above.
all annota-tors are postgraduate students and not involved inother parts of our experiments..tab.
5 shows that our cpcc effectively allevi-ates the problem of role-irrelevant, incoherent andinconsistent translations compared with other mod-els (signiﬁcance test (koehn, 2004), p < 0.05),indicating the superiority of our model.
the inter-annotator agreement is 0.527, 0.491, 0.556 and0.485 calculated by the fleiss’ kappa (fleiss andcohen, 1973), for preference, coherence, consis-tency and ﬂuency, respectively, indicating “mod-erate agreement” for all four criteria.
we alsopresent some case studies in appendix h..7 related work.
chat nmt.
it only involves several researchesdue to the lack of human-annotated publicly avail-able data (farajian et al., 2020).
therefore, some.
5718existing work (wang et al., 2016; maruf et al.,2018; zhang and zhou, 2019; rikters et al., 2020)mainly pays attention to designing methods to au-tomatically construct the subtitles corpus, whichmay contain noisy bilingual utterances.
recently,farajian et al.
(2020) organize the wmt20 chattranslation task and ﬁrst provide a human post-edited corpus, where some teams investigate theeffect of dialogue history and ﬁnally ensemble theirmodels for higher ranks (berard et al., 2020; mo-hammed et al., 2020; wang et al., 2020; bao et al.,2020; moghe et al., 2020).
as a synchronizingstudy, wang et al.
(2021) use multitask learning toauto-correct the translation error, such as pronoundropping, punctuation dropping, and typos.
unlikethem, we focus on explicitly modeling role prefer-ence, dialogue coherence, and translation consis-tency with tailored latent variables to promote thetranslation quality..context-aware nmt.
chat nmt can beviewed as a special case of context-aware nmt,which has attracted many researchers (gong et al.,2011; jean et al., 2017; wang et al., 2017b; bawdenet al., 2018; miculicich et al., 2018; kuang et al.,2018; tu et al., 2018; yang et al., 2019; kang et al.,2020; li et al., 2020; ma et al., 2020) to extend theencoder or decoder for exploring the context im-pact on translation quality.
although these modelscan be directly applied to chat translation, they can-not explicitly capture the bilingual conversationalcharacteristics and thus lead to unsatisfactory trans-lations (moghe et al., 2020).
different from thesestudies, we focus on explicitly modeling these bilin-gual conversational characteristics via cvae forbetter translations..conditionalauto-encoder.
variationalcvae has veriﬁed its superiority in manyﬁelds (sohn et al., 2015).
in nmt, zhang et al.
(2016) and su et al.
(2018) extend cvae tocapture the global/local information of sourcesentence for better results.
mccarthy et al.
(2020)focus on addressing the posterior collapse withmutual information.
besides, some studies usecvae to model the correlations between imageand text for multimodal nmt (toyama et al.,2016; calixto et al., 2019).
although the cvaehas been widely used in nlp tasks, its adaptionand utilization to chat translation for modelinginherent bilingual conversational characteristicsare non-trivial, and to the best of our knowledge,.
has never been investigated before..8 conclusion and future work.
we propose to model bilingual conversational char-acteristics through tailored latent variables for neu-ral chat translation.
experiments on en⇔de anden⇔ch directions show that our model notablyimproves translation quality on both bleu andter metrics, showing its superiority and general-izability.
human evaluation further veriﬁes thatour model yields role-speciﬁc, coherent, and con-sistent translations by incorporating tailored latentvariables into nmt.
moreover, we contribute anew bilingual dialogue data (bmeld, en⇔ch)with manual translations to the research commu-nity.
in the future, we would like to explore theeffect of multimodality and emotion on chat trans-lation, which has been well studied in dialogueﬁeld (liang et al., 2020)..acknowledgments.
the research work descried in this paper has beensupported by the national key r&d program ofchina (2020aaa0108001) and the national na-ture science foundation of china (no.
61976015,61976016, 61876198 and 61370130).
the authorswould like to thank the anonymous reviewers fortheir valuable comments and suggestions to im-prove this paper..references.
jinyeong bak and alice oh.
2019. variational hierar-in proceed-.
chical user-based conversation model.
ings of emnlp-ijcnlp, pages 1941–1950..calvin bao, yow-ting shiue, chujun song, jie li,and marine carpuat.
2020. the university of mary-land’s submissions to the wmt20 chat translationtask: searching for more data to adapt discourse-in proceedingsaware neural machine translation.
of wmt, pages 454–459..rachel bawden, rico sennrich, alexandra birch, andbarry haddow.
2018. evaluating discourse phenom-ena in neural machine translation.
in proceedings ofnaacl, pages 1304–1313..alexandre berard,.
ioan calapodescu, vassilinanikoulina, and jerin philip.
2020. naver labseurope’s participation in the robustness, chat, andin proceedings ofbiomedical tasks at wmt 2020.wmt, pages 460–470..bill byrne, karthik krishnamoorthi, chinnadhuraisankar, arvind neelakantan, ben goodrich, daniel.
5719duckworth, semih yavuz, amit dubey, kyu-youngkim, and andy cedilnik.
2019. taskmaster-1: to-ward a realistic and diverse dialog dataset.
in pro-ceedings of emnlp-ijcnlp, pages 4516–4525..iacer calixto, miguel rios, and wilker aziz.
2019. la-tent variable model for multi-modal translation.
inproceedings of acl, pages 6392–6405..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of naacl-hlt, pagesstanding.
4171–4186..m. amin farajian, ant´onio v. lopes, andr´e f. t. mar-tins, sameen maruf, and gholamreza haffari.
2020.findings of the wmt 2020 shared task on chat trans-lation.
in proceedings of wmt, pages 65–75..joseph l. fleiss and jacob cohen.
1973. the equiv-alence of weighted kappa and the intraclass corre-lation coefﬁcient as measures of reliability.
educa-tional and psychological measurement, pages 613–619..zhengxian gong, min zhang, and guodong zhou.
2011. cache-based document-level statistical ma-chine translation.
in proceedings of emnlp, pages909–919..hany hassan, anthony aue, chang chen, vishalchowdhary,jonathan clark, christian feder-mann, xuedong huang, marcin junczys-dowmunt,william lewis, mu li, shujie liu, tie-yan liu,renqian luo, arul menezes, tao qin, frank seide,xu tan, fei tian, lijun wu, shuangzhi wu, yingcexia, dongdong zhang, zhirui zhang, and mingzhou.
2018. achieving human parity on automaticchinese to english news translation.
arxiv preprintarxiv:1803.05567..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural computation, pages1735–1780..sebastien jean, stanislas lauly, orhan firat, andkyunghyun cho.
2017. does neural machine trans-lation beneﬁt from larger context?
arxiv preprintarxiv:1704.05135..xiaomian kang, yang zhao,.
jiajun zhang, andchengqing zong.
2020. dynamic context selectionfor document-level neural machine translation via re-in proceedings of emnlp,inforcement learning.
pages 2242–2254..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof iclr..diederik p kingma and max welling.
2013. auto-arxiv preprint.
encoding variational bayes.
arxiv:1312.6114..philipp koehn.
2004. statistical signiﬁcance tests forin proceedings of.
machine translation evaluation.
emnlp, pages 388–395..shaohui kuang, deyi xiong, weihua luo, andguodong zhou.
2018. modeling coherence forneural machine translation with dynamic and topiccaches.
in proceedings of coling, pages 596–606..mirella lapata and regina barzilay.
2005. automaticevaluation of text coherence: models and represen-tations.
in proceedings of ijcai, pages 1085–1090..samuel l¨aubli, rico sennrich, and martin volk.
2018.has machine translation achieved human parity?
acase for document-level evaluation.
in proceedingsof emnlp, pages 4791–4796..bei li, hui liu, ziyang wang, yufan jiang, tong xiao,jingbo zhu, tongran liu, and changliang li.
2020.does multi-encoder help?
a case study on context-aware neural machine translation.
in proceedings ofacl, pages 3512–3518..yunlong liang, fandong meng, ying zhang, jinan xu,yufeng chen, and jie zhou.
2020.infusing multi-source knowledge with heterogeneous graph neuralnetwork for emotional conversation generation..shuming ma, dongdong zhang, and ming zhou.
2020.a simple and effective uniﬁed encoder for document-in proceedings of acl,level machine translation.
pages 3505–3511..sameen maruf and gholamreza haffari.
2018. docu-ment context neural machine translation with mem-ory networks.
in proceedings of acl, pages 1275–1284..sameen maruf, andr´e f. t. martins, and gholamrezahaffari.
2018. contextual neural model for translat-in pro-ing bilingual multi-speaker conversations.
ceedings of wmt, pages 101–112..sameen maruf, andr´e f. t. martins, and gholam-reza haffari.
2019. selective attention for context-aware neural machine translation.
in proceedings ofnaacl, pages 3092–3102..arya d. mccarthy, xian li, jiatao gu, and ning dong.
2020. addressing posterior collapse with mutual in-formation for improved variational neural machinein proceedings of acl, pages 8512–translation.
8525..fandong meng and jinchao zhang.
2019. dtmt: anovel deep transition architecture for neural machinetranslation.
in proceedings of aaai, pages 224–231..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neu-ral machine translation with hierarchical attentionnetworks.
in proceedings of emnlp, pages 2947–2954..5720tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space.
in proceedings of iclr..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the discomt, pages 82–92..shachar mirkin, scott nowson, caroline brun, andjulien perez.
2015. motivating personality-awarein proceedings of emnlp,machine translation.
pages 1102–1108..antonio toral, sheila castilho, ke hu, and andyway.
2018. attaining the unattainable?
reassessingclaims of human parity in neural machine translation.
in proceedings of wmt, pages 113–123..nikita moghe, christian hardmeier, and rachel baw-den.
2020. the university of edinburgh-uppsala uni-versity’s submission to the wmt 2020 chat transla-tion task.
in proceedings of wmt, pages 471–476..joji toyama, masanori misono, masahiro suzuki, ko-taro nakayama, and yutaka matsuo.
2016. neuralmachine translation with latent semantic of imageand text.
arxiv preprint arxiv:1611.08459..roweida mohammed, mahmoud al-ayyoub, andmalak abdullah.
2020. just system for wmt20 chattranslation task.
in proceedings of wmt, pages 477–480..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic evalu-ation of machine translation.
in proceedings of acl,pages 311–318..soujanya poria, devamanyu hazarika, navonil ma-jumder, gautam naik, erik cambria, and rada mi-halcea.
2019. meld: a multimodal multi-partydataset for emotion recognition in conversations.
inproceedings of acl, pages 527–536..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of wmt, pages 186–191..mat¯ıss rikters, ryokan ri, tong li, and toshi-aki nakazawa.
2020. document-aligned japanese-in proceed-english conversation parallel corpus.
ings of mt, pages 639–645, online..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of acl, pages 1715–1725..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of amta..kihyuk sohn, honglak lee, and xinchen yan.
2015.learning structured outputrepresentation usingdeep conditional generative models.
in proceedingsof nips, pages 3483–3491..jinsong su, shan wu, deyi xiong, yaojie lu, xianpeihan, and biao zhang.
2018. variational recurrentneural machine translation.
in proceedings of aaai..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in proceedings of nips, pages 3104–3112..zhixing tan, jiacheng zhang, xuancheng huang,gang chen, shuo wang, maosong sun, huanboluan, and yang liu.
2020. thumt: an open-source toolkit for neural machine translation.
in pro-ceedings of amta, pages 116–122..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018. learning to remember translation history witha continuous cache.
tacl, pages 407–420..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allin proceedings of nips, pages 5998–you need.
6008..elena voita, rico sennrich, and ivan titov.
2019a.
context-aware monolingual repair for neural ma-in proceedings of emnlp-chine translation.
ijcnlp, pages 877–886..elena voita, rico sennrich, and ivan titov.
2019b.
when a good translation is wrong in context:context-aware machine translation improves ondeixis, ellipsis, and lexical cohesion.
in proceedingsof acl, pages 1198–1212..elena voita, pavel serdyukov, rico sennrich, and ivantitov.
2018. context-aware neural machine transla-tion learns anaphora resolution.
in proceedings ofacl, pages 1264–1274..longyue wang, jinhua du, liangyou li, zhaopengtu, andy way, and qun liu.
2017a.
semantics-enhanced task-oriented dialogue translation: a casestudy on hotel booking.
in proceedings of ijcnlp,pages 33–36..longyue wang, zhaopeng tu, xing wang, li ding,liang ding, and shuming shi.
2020. tencent ai labmachine translation systems for wmt20 chat transla-tion task.
in proceedings of wmt, pages 481–489..longyue wang, zhaopeng tu, xing wang, and shum-ing shi.
2019. one model to learn both: zero pro-noun prediction and translation.
in proceedings ofemnlp-ijcnlp, pages 921–930..longyue wang, zhaopeng tu, andy way, and qun liu.
2017b.
exploiting cross-sentence context for neu-ral machine translation.
in proceedings of emnlp,pages 2826–2831..longyue wang, xiaojun zhang, zhaopeng tu, andyway, and qun liu.
2016. automatic construction ofin pro-discourse corpora for dialogue translation.
ceedings of lrec, pages 2748–2754..5721tao wang, chengqi zhao, mingxuan wang, lei li, anddeyi xiong.
2021. autocorrect in the process oftranslation – multi-task learning improves dialoguemachine translation..tianming wang and xiaojun wan.
2019..t-cvae:transformer-based conditioned variational autoen-coder for story completion.
in proceedings of ijcai,pages 5233–5239..bowen wu, mengyuan li, zongsheng wang, yifuchen, derek f. wong, qihang feng, junhonghuang, and baoxun wang.
2020. guiding varia-tional response generator to exploit persona.
in pro-ceedings of acl, pages 53–65..hao xiong, zhongjun he, hua wu, and haifeng wang.
2019. modeling coherence for discourse neural ma-chine translation.
proceedings of aaai, pages 7338–7345..jianhao yan, fandong meng, and jie zhou.
2020.multi-unit transformers for neural machine transla-tion.
in proceedings of emnlp, pages 1047–1059,online..pengcheng yang, lei li, fuli luo, tianyu liu, andxu sun.
2019. enhancing topic-to-essay generationwith external commonsense knowledge.
in proceed-ings of acl, pages 2002–2012..biao zhang, deyi xiong, jinsong su, hong duan, andmin zhang.
2016. variational neural machine trans-lation.
in proceedings of emnlp, pages 521–530..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of emnlp,pages 533–542..l. zhang and q. zhou.
2019. automatically annotatetv series subtitles for dialogue corpus construction.
in apsipa asc, pages 1029–1035..wen zhang, yang feng, fandong meng, di you, andqun liu.
2019. bridging the gap between trainingand inference for neural machine translation.
in pro-ceedings of acl, pages 4334–4343, florence, italy..tiancheng zhao, ran zhao, and maxine eskenazi.
2017. learning discourse-level diversity for neuraldialog models using conditional variational autoen-coders.
in proceedings of acl, pages 654–664..appendix.
a datasets.
wmt20.
for the en⇔de, we combine six cor-pora including euporal, paracrawl, common-crawl, tilderapid, newscommentary, and wiki-matrix, and we combine news commentary v15,wiki titles v2, un parallel corpus v1.0, ccmt.
en⇒de de⇒en en⇒ch ch⇒en.
methods39.88transformerv-transformer 40.01transformer41.35v-transformer 41.40.base.
big.
40.7241.3641.5641.67.
32.5532.9033.8533.90.
24.4225.7724.8626.46.table 6: the bleu scores on the newstest2019 of theﬁrst stage..corpus, and wikimatrix for the en⇔ch.
we ﬁrstlyﬁlter noisy sentence pairs according to their charac-teristics in terms of duplication and length (whoselength exceeds 80).
to pre-process the raw data,we employ a series of open-source/in-house scripts,including full-/half-width conversion, unicode con-versation, punctuation normalization, and tokeniza-tion (wang et al., 2020).
after ﬁltering steps,we generate subwords via joint bpe (sennrichet al., 2016) with 32k merge operations.
finally,we obtain 45,541,367 sentence pairs for en⇔deand 22,244,006 sentence pairs for en⇔ch, respec-tively..we test the model performance of the ﬁrst stageon newstest2019.
the results are shown in tab.
6..b implementation details.
for all experiments, we follow two model set-tings illustrated in (vaswani et al., 2017), namelytransformer-base and transformer-big.
the train-ing step is set to 200,000 and 2,000 for the ﬁrststage and the ﬁne-tuning stage, respectively.
thebatch size for each gpu is set to 4096 tokens.
thebeam size is set to 4, and the length penalty is0.6 among all experiments.
all experiments in theﬁrst stage are conducted utilizing 8 nvidia teslav100 gpus, while we use 2 gpus for the secondstage, i.e., ﬁne-tuning.
that gives us about 8*4096and 2*4096 tokens per update for all experimentsin the ﬁrst-stage and second-stage, respectively.
allmodels are optimized using adam (kingma and ba,2015) with β1 = 0.9 and β2 = 0.998, and learningrate is set to 1.0 for all experiments.
label smooth-ing is set to 0.1. we use dropout of 0.1/0.3 forbase and big setting, respectively.
to alleviate thedegeneration problem of the variational framework,we apply kl annealing.
the kl multiplier λ grad-ually increases from 0 to 1 over 10, 000 steps.
|r|is set to 2 for en⇔de and 7 for en⇔ch, respec-tively.
|t | is set to 10. the criterion for selectinghyperparameters is the bleu score on validationsets for both tasks.
the average running time isshown in tab.
7..5722figure 5: total kl divergence (per word) of all latentvariables (ﬁrst 1,000 updates on corresponding valida-tion set)..en⇒de de⇒en en⇒ch ch⇒en.
stages5dthe first stage4hfine-tuning stagethe first stage10dfine-tuning stage 4.5h.
base.
big.
7d5h12d5.5h.
4d3h7d4h.
3.5d2h6d2.5h.
table 7: the average running time for the ﬁrst stageand ﬁne-tuning stage.
d: days, h: hours..figure 4: effect of context length and latent dimensionon translation quality.
the bleu scores (%) are calcu-lated on the validation set of the en⇒de..in the case of blind testing or online use (as-sumed dealing with en⇒de), since translations oftarget utterances (i.e., english) will not be given,an inverse de⇒en model is simultaneously trainedand used to back-translate target utterances (baoet al., 2020), similar to all tasks..c effect of context length.
we ﬁrstly investigate the effect of context length(i.e., the number of preceding utterances) on ourapproach under the transformer base setting.
asshown in the left of fig.
4, using three precedingsource sentences as dialogue history achieves thebest translation performance on the validation set(en⇒de).
using more preceding sentences doesnot bring any improvement and increases the com-putational cost.
this conﬁrms the ﬁnding of tuet al.
(2018) and zhang et al.
(2018) that long-distance context only has limited inﬂuence.
there-fore, we set the number of preceding sentences to3 in all experiments..figure 6: bilingual conversational example one..e kl divergence.
generally, kl divergence measures the amount ofinformation encoded in a latent variable.
in theextreme case where the kl divergence of latentvariable z equals to zero, the model completelyignores z, i.e., it degenerates.
fig.
5 shows that thetotal kl divergence of our model maintains around0.2∼0.5 indicating that the degeneration problemdoes not exist in our model and latent variables canplay their corresponding roles..d effect of latent dimension.
f case study.
the right of fig.
4 shows the effect of the latentdimension on translation quality under the trans-former base setting.
obviously, using latent dimen-sion 32 sufﬁces to achieve superior performance.
increasing the dimension does not lead to any im-provements.
therefore, we set the latent dimensionto 32 in all experiments..in this section, we show some cases in fig.
6 andfig.
7 to investigate the effect of different models..role preference and dialogue coherence.
asshown in fig.
6, we observe that the baselinemodels and the context-aware models except “v-transformer+ft” cannot preserve the role pref-erence information, e.g., joy emotion, even these.
5723      & r q w h [ w  / h q j w k                             % / ( 8             / d w h q w  ' l p h q v l r q                     % / ( 8               8 s g d w h v                . /  ' l y h u j h q f h ( q  ! ' h ' h  ! ( q ( q  ! & k & k  ! ( qy2: nǐhuì  jiàshǐ  fānchuán？x1: you know, joey, i could teach you to sail, if you want.?
y3: duìa, wǒ zhè bèizǐ dōu zài jiàchuán, wǒ shíwǔ suì shí, wǒ bà song wǒ yì sōu chuán。x2: you could?x3: yeah!
i've been sailing my whole life.
when i was fifteen, my dad bought me my own boat.s2s1s1y1: qiáoyī, rúguǒ nǐ xiǎng, wǒ kěyǐ  jiào nǐ jiàchuán。？y3: shénme?！ shénme?
tā xiǎng rang wǒ gāoxìng qǐlái！ wǒ de xiǎomǎ bìng le。bilingual dialogue historybaseline modelscontext-aware modelscpcc (ours)transformertransformer+ftdoc-transformer+ftdia-transformer+ftv-transformer+fty3: shénme?！ shénme?！ tā xiǎng ānwèi wǒ！ wǒ de xiǎomǎ shēngbìng le。y3: shénme?！ shénme?！ tā xiǎng ānwèi wǒ！ wǒ de xiǎomǎ shēngbìng le。y3: shénme?！ tā xiǎng ānwèi wǒ！ wǒ de xiǎomǎ shēngbìng le。y3: zěnme?
tā xiǎngyào  ānwèi wǒ！wǒ de xiǎomǎ bìng le。referencey3: zěnme?
bù xìn?
tā song wǒ yì sōu chuán lái ānwèi wǒ, wǒ de xiǎomǎ bìng le。y3: zěnme?
bù xiāngxìn?
tā  yòng yì sōu chuán lái ānwèi wǒ！ wǒ de xiǎomǎ shēngbìng le 。y4: nǐ yǒu yì sōu fānchuán？x4: your own boat?x5: what?
what?
he was trying to cheer me up!
my pony was sick.s1s2y5: nmtfigure 7: bilingual conversational example two..“*-transformer+ft” models incorporate the bilin-gual conversational history into the encoder.
the“v-transformer+ft” model produces very slightlyemotional elements (e.g., “zˇenme?”) due to the la-tent variable over the source sentence capturing rel-evant preference information.
meanwhile, we ﬁndthat all comparison models cannot generate a co-herent translation.
the reason may be that they failto capture the conversation-level coherence clue,i.e., “boat”.
by contrast, we explicitly model thetwo characteristics through tailored latent variablesand thus obtain satisfactory results..translation consistency.
as shown in fig.
7,we observe that all comparison models cannotmaintain the translation consistency due to thelack of explicitly modeling this characteristic.
ourmodel has the ability to overcome the issue andcan keep the correct lexical choice to translate thecurrent utterance that might have appeared in pre-ceding turns, i.e., “ji`achu`an”..to sum up, both cases show that our modelyields role-speciﬁc, coherent, and consistent trans-lations by incorporating tailored latent variablesinto translators, demonstrating its effectiveness andsuperiority..5724y2: nǐhuì  jiàshǐ  fānchuán？x1: you know, joey, i could teach you to sail, if you want.?
y3: x2: you could?x3: yeah!
i've been sailing my whole life.
when i was fifteen, my dad bought me my own boat.s2s1s1y1: qiáoyī, rúguǒ nǐ xiǎng, wǒ kěyǐ  jiào nǐ jiàchuán。？nmty3: shìde, wǒ yīzhí zài hángxíng, dàng wǒ shíwǔ suì shí, wǒ fùqīn gěi wǒ zìjǐ mǎi le yì sōu chuán。bilingual dialogue historybaseline modelscontext-aware modelscpcc (ours)transformertransformer+ftdoc-transformer+ftdia-transformer+ftv-transformer+fty3: duì!
wǒ yī bèizǐ dōu zài hángxíng, wǒ shíwǔ suì shí, wǒ bà gěi wǒ mǎi le  yì sōu chuán。y3: duì!
wǒ zhè bèizǐ dōu zài hángxíng, wǒ shíwǔ suì shí, wǒ bà song wǒ yì sōu chuán。y3: wǒ yī bèizǐ dōu zài hángxíng, wǒ shíwǔ suì shí, wǒ bà song wǒ yì sōu chuán。y3: duì!
wǒ zhè bèizǐ dōu zài hángchuán, wǒ shíwǔ suì shí, wǒ bàbà song wǒ yì sōu fānchuán。referencey3: duìa, wǒ zhè bèizǐ dōu zài jiàchuán, wǒ shíwǔ suì shí, wǒ bà song wǒ yì sōu chuán。y3: duì!
wǒ zhè bèizǐ dōu zài jiàchuán, wǒ shíwǔ suì shí, wǒ bàbà song wǒ yì sōu chuán。