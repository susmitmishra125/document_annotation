bertgen: multi-task generation through bert.
faidon mitzalis1, ozan caglayan1, pranava madhyastha1, lucia specia1,21department of computing, imperial college london, uk2department of computer science, university of shefﬁeld, ukphaedonmit@gmail.com, {o.caglayan,pranava,lspecia}@ic.ac.uk.
abstract.
we present bertgen, a novel generative,decoder-only model which extends bertby fusing multimodal and multilingual pre-trained models vl-bert and m-bert, re-spectively.
bertgen is auto-regressivelytrained for language generation tasks, namelyimage captioning, machine translation andmultimodal machine translation, under a multi-task setting.
with a comprehensive set ofevaluations, we show that bertgen outper-forms many strong baselines across the tasksexplored.
we also show bertgen’s abil-ity for zero-shot language generation, whereit exhibits competitive performance to super-vised counterparts.
finally, we conduct abla-tion studies which demonstrate that bertgensubstantially beneﬁts from multi-tasking andeffectively transfers relevant inductive biasesfrom the pre-trained models..1.introduction.
recent work in unsupervised and self-supervisedpre-training has revolutionised the ﬁeld of naturallanguage understanding (nlu), resulting in highperformance ceilings across multiple tasks (devlinet al., 2019; yang et al., 2019; dong et al., 2019).
the recent success of language model pre-trainingwith masked language modelling (mlm) such asbert (devlin et al., 2019) further paved the wayfor more complex approaches that combine lan-guage pre-training with images (tan and bansal,2019; su et al., 2020; lu et al., 2020), video (sunet al., 2019), and speech (chuang et al., 2020).
most of these approaches follow a task-speciﬁcﬁne-tuning step after the model is pre-trained..however, there has been little work on exploitingpre-trained mlms for natural language generation(nlg) tasks.
previous work argues that the mlmobjective is ill-suited for generation tasks such asmachine translation (yang et al., 2019; rothe et al.,.
2020).
recent work in this direction has predom-inantly investigated the use of pre-trained mod-els to either initialise transformer-based encoder-decoder models (imamura and sumita, 2019; clin-chant et al., 2019; yang et al., 2020; rothe et al.,2020) or to distill knowledge for sequence genera-tion tasks (chen et al., 2020)..in this work, we present bertgen, which ex-tends bert in a generative setting (§ 2.1).
this re-sults in a single generator – without a separation be-tween the encoder and the decoder – capable of con-suming multiple input modalities and generating inmultiple languages.
the latter features are achievedby transferring knowledge from state-of-the-art pre-trained models, namely vl-bert (su et al., 2020)and multilingual bert (m-bert) (devlin et al.,2019).
we train bertgen on various tasks, in-cluding image captioning, machine translation andmultimodal machine translation, and datasets infour different languages (§ 2.2)..based on a number of experiments, our ﬁnd-ings (§ 3) show that bertgen (i) is surprisinglyversatile as it is capable of describing images andperforming translation in unimodal and multimodalsettings, across all languages, (ii) generalises wellacross zero-shot image captioning, multimodal ma-chine translation, and out-of-domain news trans-lation tasks, and ﬁnally (iii) is parameter efﬁcientwhen compared to state-of-the-art models for eachof the tasks combined together..2 method.
in this section, we describe bertgen and the taskswe explore.
we then detail the baselines and sotasystems that we compare against..2.1 model.
this section details the main aspects of bertgenthat distinguish it from the existing work on vision& language pre-training..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6440–6455august1–6,2021.©2021associationforcomputationallinguistics6440figure 1: a view of the bertgen model during the training of an mmt sample: solid and dashed borders aroundthe images represent full-image features and regional features, respectively.
at test time, the most likely tokeny3 = argmax(p (yt|x, v, y<t)) is placed back into the sequence and the [mask] token is shifted right by one..input conﬁguration.
while bertgen is poten-tially capable of modeling a variety of generativetasks, we focus on three particular tasks, namelymachine translation (mt), multimodal mt (mmt)and image captioning (ic).
therefore, dependingon the task, the input conﬁguration of the modelmay change during both training and testing.
toclarify further, let us ﬁrst denote a sequence ofembeddings representing a source sentence byx(i) = [x(i)m ], its target translation byy(i) = [y(i)n ], and a collection of k re-gional visual features extracted from an associ-ated image by v(i) = [v(i)k ].
figure 1depicts bertgen when processing a sample fromthe mmt task.
this task’s input conﬁgurationis a triplet that involves all the three sequencesi.e.
{x(i), y(i), v(i)}.
using this notation, the mtand ic tasks’ conﬁgurations would correspond to{x(i), y(i)} and {v(i), y(i)}, respectively..1 , · · · , x(i)1 , · · · , y(i).
1 , · · · , v(i).
visual embeddings.
we follow vl-bert andrepresent images as a collection of k features v(i)deﬁned for regions of interest (roi).
after pre-extracting the 2048-dimensional roi features usingthe bottom-up-top-down object detector (andersonet al., 2018), we keep between 10 and 100 (i.e.
k ∈ [10, 100]) of them depending on the conﬁ-dence score.
the ﬁnal visual embedding for anroi is obtained by summing its feature vector andits geometric embedding (i.e.
the projection of the.
figure 2: hybrid initialisation: the solid and dashedblocks are transferred from pre-trained vl-bert andm-bert checkpoints, respectively..initialisation.
we take advantage of the previoussuccesses in large-scale pre-training and proposea hybrid initialisation for bertgen (figure 2).
this involves using the vl-bert (su et al., 2020)checkpoint and initialising the word embeddings,the transformer weights and the mlm head withm-bert (devlin et al., 2019).
we conjecture thatthis primes bertgen to be aware of the visualmodality and of multiple languages.
this is sim-ply due to vl-bert being pre-trained on englishmonolingual and image captioning corpora, as wellas m-bert offering a 119k wordpiece vocabulary,trained on the entire wikipedia in 104 languages1..1we adopt the ‘bert-base, multilingual cased’ version.
from the transformers toolkit (wolf et al., 2020)..6441x1x2x7y1y2mlm headtransformery3segment asegment binput embeddingsroi featuressegment embeddingspositional embeddings1212131415151516[de][cls][sep][sep][img][img][img][end][mask]mlm headtransformer blockpositional embeddingsprojection layerfaster r-cnnwordembeddingsmlm headvl-bertm-bertself-attentive representations of early positions arenaturally re-computed, in contrast to typical trans-former decoders.
finally, due to how inputs/outputsare represented in a single stream and encodedthrough shared self-attention, bertgen enforcesan inductive bias towards a truly multi-modal andmulti-lingual representation space..target language speciﬁers.
finally, to select thelanguage during generation, input sequences beginwith special target language speciﬁers (ha et al.,2016; johnson et al., 2017) (figure 1).
the speci-ﬁer is task-agnostic, i.e.
the same speciﬁer [de] isused both when captioning into german and whentranslating into german..training & hyper-parameters.
we extend2 thebase conﬁguration of vl-bert which is a trans-former with 12 self-attention layers and 12 heads.
the model and feed-forward dimensions are 768and 3072, respectively.
on a single 32gb v100gpu, one epoch (§ 3) takes approximately two daysto complete as we could only ﬁt one example pertask (i.e.
batch size equal to 13) into the memory3.
we use adamw optimiser (loshchilov and hutter,2019) with base learning rate set to 1.3×10−5.
thelearning rate is warmed up in the ﬁrst 16k stepsand then decays linearly.
we set the weight decayto 10−4.
during training, we let the model updatethe positional embeddings as bertgen needs tolearn new positions not covered by vl-bert pre-training.
the ﬁnal model has ∼89.3m parametersexcluding the word embeddings..decoding.
at test time, we incrementally add themost likely prediction (i.e.
greedy search) into thepreviously masked position (figure 1) and shift the[mask] token right by one.
the reason we chosegreedy over beam search is because the latter wouldmake decoding much slower due to self-attentiverepresentations being re-computed.
the decodingends when [stop] is predicted..2.2 tasks & systems.
to evaluate bertgen’s generative abilities, we ex-plore a diverse set of tasks: image captioning, text-only mt and multimodal mt.
table 1 summarisesthe training statistics for the various datasets weuse..2https://github.com/imperialnlp/bertgen3with careful optimisation of the training code and mixedprecision multi-gpu training, the training time can be sub-stantially reduced..figure 3: a look at bertgen’s self-attention: the con-nections denote that self-attentive representations arere-computed in every step.
the generation ends whenstop is predicted.
the smileys refer to roi features..bounding box coordinates).
when encoding thenon-visual positions, the same roi feature vectorfor the full image is repeated (see figure 1).
wenote that we do not ﬁne-tune the object detectorduring training..sequence unrolling.
an important aspect ofbertgen is that it does not explicitly distinguishbetween the encoder and the decoder blocks usu-ally seen in sequence-to-sequence models.
this isaccomplished by formalising both encoding andgeneration using the mlm framework.
formally,let us consider the mmt task and deﬁne the max-imum log-likelihood objective for a given triplet{x(i), v(i), y(i)} where the target y(i) has n tokens:.
l(i) =.
log.
(cid:16).
p (y(i)t.(cid:17)| x(i); v(i); y(i)<t).
(1).
n(cid:88).
t=1.
in a typical sequence-to-sequence model, each log-probability term would be computed by a decoderwithin the forward-pass of the same training ex-ample.
in contrast, bertgen explicitly unrollsthe example n times, forming n new training ex-amples.
in other words, each conditional term inequation 1 is observed independently within anepoch of training.
therefore, sequence unrollinghas a data augmentation effect since a training cor-pus with d examples is approximately augmentedby a factor of the average length of the target se-quences.
moreover, the uniﬁed encoder-decoderformalism halves the number of parameters, mak-ing bertgen parameter efﬁcient..self attention.
given that a single trans-former (vaswani et al., 2017) performs both encod-ing and decoding, sequence unrolling affects self-attention as well (figure 3).
first, all positionsattend to each other for a given unrolled examplei.e.
the attention is bi-directional.
second, sinceeach unrolled case is an independent example, the.
6442stopmaskmaskmaskmaskname.
type.
task.
sents augm..2.2.2 multimodal machine translation.
flickr8k.
ic.
im→tr.
13,828.multi30k mmt de→enfr→enen→frmulti30k mmt en→de.
29,000.
29,000.flickr30k.
ic.
iwslt.
iwslt.
mt.
mt.
setimes.
mt.
im→en 145,000im→de.
de→en 158,388en→de.
fr→en 163,328en→fr.
tr→en 185,318en→tr.
263k.
464k464k560k582k.
2.39m2.48m.
3.85m4.43m.
4.01m4.78m.
6.01m8.40m.
table 1: training statistics of bertgen: the last col-umn is the number of samples after sequence unrolling..2.2.1.image captioning.
image captioning (ic) involves describing imagesin a speciﬁed natural language.
we train bert-gen for english, german and turkish caption-ing tasks.
speciﬁcally, we use the flickr30kdataset (young et al., 2014) that provides 29k train-ing images, each with ﬁve english captions col-lected through crowd-sourcing.
the validation andtest sets contain approximately 1k images each.
we use the multi30k dataset (elliott et al., 2016),which annotates flickr30k images with ﬁve ger-man captions.
finally, we use the tasviretdataset (unal et al., 2016) which provides twoturkish captions for each of the 8,092 images inthe flickr8k dataset (rashtchian et al., 2010).
since flickr8k is a subset of flickr30k, wecreate a new split of tasviret to avoid data leak-age between training and test splits.
the resultingtraining, validation and test splits contain 6914,543, and 543 images, respectively..to evaluate bertgen’s performance on ic,we compare it against previous work with strongperformance on coco (chen et al., 2015) andflickr30k.
more precisely, adaptive atten-tion (sentinel) (lu et al., 2017), which usesa sentinel token to distinguish between visual andnon-visual representations, and neural babytalk (nbt), which follows a slot-ﬁlling approachthrough explicit object region information (luet al., 2018)..multimodal machine translation (mmt) attemptsto improve mt quality by incorporating informa-tion from modalities other than language (suluba-in our case, we train bert-cak et al., 2020).
gen for en↔de and en↔fr mmt tasks anduse the multi30k dataset, the main dataset forimage-informed translation, which provides cap-tion translations for flickr30k images in germanand french.
to evaluate bertgen on mmt tasks,we use the original 2016 test set which contains1,000 examples..for a comprehensive comparison with previouswork, we train a sota recurrent mmt (caglayanet al., 2020) solely on the multi30k dataset,which applies a secondary (visual) attention in thedecoder over the roi features i.e.
the same featuresthat are also used by bertgen (§ 2.1).
thereare two gru (cho et al., 2014) layers in both theencoder and the decoder and the embedding & hid-den dimensions in the model are set to 200 and 320,respectively.
each model has ∼5.6m parametersexcluding the word embeddings..besides the state-of-the-art constrained recur-rent mmt model described above, we furthercompare bertgen – which is trained on variousother mt and ic corpora – to an unconstrainedtransformer-based mmt trained on ∼9m addi-tional en→de sentences (libovick´y, 2019)4 inaddition to multi30k..2.2.3 text-only machine translation.
we incorporate six text-only mt tasks into ourtraining protocol.
we use en↔de and en↔frmt datasets from iwslt’14 (cettolo et al., 2012)which consists of ted talks’ subtitles and theirtranslations.
we take the prepare-iwslt14recipe from fairseq (ott et al., 2019) to preparethe dev and test sets.
this yields an en↔de testset of 6,750 sentences which consists of dev2010,dev2012.tedx, tst2010, tst2011 and tst2012.
sim-ilarly, the en↔fr test set consists of dev2010,tst2010, tst2011 and tst2012, which amounts to4,493 sentences..for en↔tr directions, we use the se-times2 (tiedemann, 2012) news dataset for train-ing.
for development and test sets, we take theofﬁcial wmt test sets (bojar et al., 2018), namely,newstest2016 and newstest2017 as the development.
4we obtained test set outputs from the author and pre-.
processed with m-bert tokeniser to ensure comparability..6443set (6,007 sentences), and newstest2018 (6,000 sen-tences) as the test set.
both iwslt and setimes2corpora are medium-scale resources often used inmt research community, and have much hardertest sets than the mmt and ic tasks, due to a sig-niﬁcant domain shift..finally, for each translation direction, we traina transformer nmt model (vaswani et al., 2017)using the iwslt-de-en recipe of the fairseqtoolkit (ott et al., 2019).
this recipe has six en-coders and six decoders, each equipped with 4-headself-attention layers.
the model and feed-forwarddimensions are set to 512 and 1024, respectively.
each model has ∼31.5m parameters excluding theword embeddings.
since bertgen is a generalpurpose multilingual and multimodal generator, weexpect it to perform in the same ballpark as thesestrong nmt baselines, but not necessarily be sotacompared to novel & sophisticated nmt models,which also make use of a lot more training data..3 results and findings.
we train bertgen on lowercased sentences for45 epochs, after which the overall performanceon the tasks reached a plateau.
we deﬁne onebertgen epoch as a single pass over all of thetraining data for the multi30k en→de mmttask and denote this task as the reference task.
weuse greedy search for all systems that we trainedand merge back the word pieces before evaluation.
we compute tokenised5 bleu (papineni et al.,2002), meteor (denkowski and lavie, 2014)and cider (vedantam et al., 2015) using coco-caption6.
in what follows, we provide detailedquantitative and qualitative ﬁndings..3.1.image captioning.
table 2 provides an overview of bertgen’s imagecaptioning performance on different test sets andlanguages.
first of all, on english flickr30k,bertgen is clearly able to outperform strong cap-tioning models (§ 2.2.1) sentinel (lu et al., 2017)and nbt (lu et al., 2018), even though they usebeam search for decoding.
on coco (chen et al.,2015), an image captioning corpus much largerand diverse than flickr30k, we evaluate bert-gen on karpathy’s test split (karpathy and fei-fei, 2015) and notice that the scores are reasonable.
task approach.
bl mt.
cr.
f30k en bertgensentinel‡nbt‡.
coco en bertgen.
nbt‡.
f30k fr bertgenf8k tr bertgenf30k de bertgen.
27.025.127.1.
15.934.75.28.517.8.
23.220.421.7.
20.427.118.114.534.2.
0.5870.5310.575.
0.4871.0720.3970.3630.500.table 2: bleu (bl), meteor (mt) and cider (cr)scores for image captioning: gray background indicateszero-shot generation whereas ‡ denotes the systems de-coded with beam search..given that bertgen is not trained on coco: ourmodel lags behind nbt (w/ beam search) by 6.7meteor..for zero-shot french captioning (f30k fr), weresort to the reference mmt translations from themulti30k en→fr task, as there are no humanreferences for french.
although this is problem-atic as the metrics will penalise captions that arenot translations of english captions, we providethe scores to show that the zero-shot outputs arevalid descriptions.
we note that the low rangeof scores reported here is also due to having onereference caption instead of ﬁve references7 as inflickr30k.
finally we report results for our cus-tom turkish split (§ 2.2.1) (f30k tr) and ger-man (f30k de).
even though there are no com-parable results in the literature for these three tasks,we demonstrate through some qualitative examplesthat bertgen produces sensible outputs..qualitative examples.
we now focus on a fewexamples to examine the multilingual image cap-tioning ability of bertgen in action (table 3).
for the ﬁrst image, all captions are almost the sameas the image has few salient points.
for the secondimage however, we observe much more variationacross captions, in line with the complexity of thescene.
we are particularly surprised by the zero-shot french captioning performance, a task thatbertgen is not trained for at all.
upon manualinspection, we noticed that the captions are oftenshort, objective gists of the images.
these obser-vations also hold for the captions generated for the.
5since m-bert is aggressive on splitting apostrophes and.
hyphens, our results may slightly differ from other work..6https://github.com/tylin/coco-caption.
7as a reference, evaluating english captions using onereference at a time, yields 7.9 bleu on average, compared to27.0 bleu in table 2..6444en a man wearing a hat and glasses.
ein mann mit hut und brille.
dea man with hat and glassess¸apkalı ve g¨ozl¨ukl¨u bir adam.
a man with a hat and glasses.
un homme avec un chapeau et des lunettes.
a man with a hat and glasses..tr.
fr.
tr.
en two men are on a rooftop working on something.
de.
zwei m¨anner arbeiten auf einem dach.
two men working on a roofiki binanın ins¸asında oturmus¸, yanyanayerde duran iki kis¸i.
two people seated in the construction of twobuildings, standing next to each other on the ground.
trois ouvriers du bˆatiment construisent un toit.
three construction workers build a roof..fr.
en a man in a red shirt and helmet is riding a.de.
tr.
fr.
motorbike on a dirt road.
ein mann f¨ahrt mit einem motorrad auf einemweg an einem ﬂuß entlang.
a man rides a motorcycle on a path along a river.
c¸amurlu bir yolda motoruyla ilerlemekte olan kırmızı¨ustl¨u bir adam ve arkasındaki da˘g manzarası.
a man in a red top riding his bike down a muddyroad with a mountain landscape behind him.
un homme avec un casque fait du motocross.
a man with a helmet rides motocross..table 3: multilingual image captioning examples: theitalicised sentences are google translate translationsof de,tr,fr sentences into english.
gray backgroundindicates zero-shot outputs.
the last example is fromcoco while the others are from flickr30k..coco test set, as we can see in the third exam-ple.
a set of additional examples in the appendixshows that bertgen does not simply retrieve cap-tion translations learned from the en→fr task.
overall, both quantitative and qualitative resultsprovide evidence of the utility of multimodal andmultilingual initialisation as well as the efﬁcacy ofknowledge transfer across different tasks for imagecaptioning..mmt.
en→de bertgen.
approach(cid:70)libovick´y (2019)(cid:70)‡caglayan et al.
(2020)fairseq nmt.
(cid:70)libovick´y (2019)(cid:70)‡fairseq nmtcaglayan et al.
(2020)(cid:70).
caglayan et al.
(2020)fairseq nmt.
fairseq nmtcaglayan et al.
(2020).
bl mt.
42.240.837.837.5.
68.063.461.561.044.843.841.735.135.133.5.
61.659.256.956.1.
81.277.375.575.364.162.160.756.953.653.1.en→fr bertgen.
de→fr bertgen.
fr→de bertgen.
(cid:70).
table 4: bleu (bl) and meteor (mt) scores formmt: zero-shot systems are highlighted with gray.
systems marked with (cid:70) and ‡ denote the use of aux-iliary resources (i.e.
unconstrained) and beam-searchdecoding, respectively..3.2 multimodal machine translation.
table 4 summarises bertgen’s performance onmmt.
first of all, bertgen consistently outper-forms the transformer-based fairseq nmt mod-els and the recurrent mmt (caglayan et al., 2020)models on both the en→de and the en→frlanguage pairs.
furthermore, bertgen is alsosubstantially better than a state-of-the-art uncon-strained mmt (libovick´y, 2019) model trained ona ∼6x larger parallel corpus..evaluation.
following.
adversarialelliott(2018), we probe bertgen’s ability for integrat-ing multiple modalities effectively.
speciﬁcally,we decode translations by shufﬂing {image, sourcecaption} mappings so that the images do notcorrespond to the sentences to be translated.
theen→de results showed that the incongruenceleads to 1.1 and 0.9 point drops in bleu andmeteor, respectively.
for en→fr, the dropsare much more prominent with 3.1 and 2.3 pointsagain for bleu and meteor.
this indicatesthat the features are not ignored at all, unlike in(caglayan et al., 2019), where they showed thatsequence-to-sequence mmt models can learn toignore the images when the linguistic signal issufﬁcient to perform the task..zero-shot performance.
the results in table 4show the surprising ability of bertgen to per-form mmt on directions unseen during training..6445task.
fairseqbl mt.
bertgenbl mt.
iwslt en→de27.4iwslt de→en33.6iwslt en→fr41.0iwslt fr→en39.1setimes en→tr14.1setimes tr→en 17.3.
47.133.859.836.418.925.8.
27.835.640.240.013.519.0.
48.434.760.536.819.126.9.table 5: comparison of text-only mt performance ofbertgen to each dedicated fairseq nmt system:bertgen outperforms single models in most cases..figure 4: bertgen’s learning efﬁciency on mt: val-idation scores are plotted against the number of fullpasses completed by bertgen and each fairseqmodel, over the corresponding task’s training set.
bestcheckpoints’ test set performances are given in table 5..moreover, the zero-shot performance surpassesstrong mmt and nmt systems by up to 2 and3.3 meteor for de→fr and fr→de, respec-tively.
similar to the image captioning results, thisdemonstrates the potential of bertgen to gener-alise over a variety of language pairs and tasks..3.3 machine translation.
first, we compare bertgen’s performance toeach task-speciﬁc fairseq system.
accordingto table 5, we observe that the translation qualityof bertgen is generally superior compared to thestrong fairseq systems, especially in meteor,where bertgen leads in all pairs..second, we look at the learning efﬁciency bycomparing the training curves between bertgenand each task-speciﬁc fairseq system (figure 4).
here, the x axis represents how many times the spe-.
de→frbl mt40.559.064.2.fr→debl mt36.713.147.326.356.438.2.bertgen 19.639.546.5.tartu‡msra‡.
zero-shot bertgen performance ontable 6:wmt’19 test set: tartu and msra systems are notzero-shot as they are trained on de↔fr corpora.
sys-tems marked with ‡ are beam search outputs..ciﬁc task’s training set has been seen by the models.
bertgen is trained for 45 reference epochs (§ 3),and this corresponds to only a few complete passesover the training sets of nmt tasks8.
this is incontrast to the single-task systems that usually re-quire a large number of epochs for convergence.
we notice a general trend and observe that bert-gen tends to outperform single-task systems usu-ally after only a few passes over the correspondingtraining set.
many factors could be contributing tothis observation such as sequence unrolling, multi-tasking, shared input space or relevant inductivebiases transferred from m-bert.
we partly ad-dress these in the ablation studies (§ 3.4) and leavefurther investigation to future work..zero-shot performance.
we use the de↔frtest set from the wmt’19 shared task on newstranslation (barrault et al., 2019) to assess the zero-shot translation capability of bertgen.
this testset includes 1,701 sentences from news data regard-ing european elections.
we compare our resultsto two shared task systems, namely tartu (base-line) and msra (state-of-the-art) (barrault et al.,2019), after re-tokenising them accordingly withm-bert9.
although bertgen is expected toobtain lower scores than the dedicated wmt sys-tems due to the domain mismatch of the test set,we consider both the quantitative (table 6) and thequalitative results (table 7) extremely encouraging..3.4 ablation studies.
impact of initialisation.
3.4.1we train single-task mmt systems on themulti30k en→de language pair.
speciﬁcally,we begin with a baseline system which is initialisedwith random weights.
we then train a second base-line where only the visual processing layers are.
8for example, only ∼3 passes over setimes en→tr.
9tartu is the baseline and msra is the best performing.
system for the shared task.
6446iwslt enfriwslt fren015304560iwslt deeniwslt ende010203040setimes entrsetimes tren05101520253005101520253005101520fairseqbertgenla d´ecision est tomb´ee au 70`eme anniversaire de ma femme.
the decision fell on my wife’s 70th birthday.
la d´ecision est tomb´ee le jour du 70`eme anniversaire de ma femme.
the decision fell on my wife’s 70th birthday..en espagne, on s’est malheureusement habitu´e `a une rˆole double et passive.
in spain, we unfortunately got used to a double and passive role.
en espagne, on s’est malheureusement habitu´e `a un rˆole secondaire, passif.
in spain, we unfortunately got used to a secondary, passive role..bertgen:.
wmt ref.
bertgen:.
wmt ref.
bertgen:.
wmt ref.
pas parce que le pr´esident du fdp a dit quelque chose qu’ ils ont d´efaillant leur vote.
not because the fdp president said something that they missed their vote.
ce n’ est pas parce que le pr´esident f´ed´eral du fdp a dit quelque chose qu’ ils ont refus´e d’ approuver.
it is not because the federal president of the fdp said something that they refused to approve..table 7: zero-shot de→fr translations on wmt’19 test set.
the italicised sentences are google translate trans-lations of french sentences into english.
bold indicates important differences between bertgen and references..figure 5: validation scores on multi30k en→demmt for the initialisation ablation: hybrid initialisa-tion is the most beneﬁcial strategy for bertgen..transferred from vl-bert.
finally, we train athird baseline that is initialised similar to bert-gen, i.e.
using the hybrid initialisation (§ 2.1).
figure 5 compares the validation bleu scores ofthese three systems.
we observe that the beneﬁts ofknowledge transfer from pre-trained models are in-crementally positive, however, bertgen’s hybridinitialisation outperforms the other two ablations..3.4.2.impact of multi-task training.
we now remove the multi-tasking aspect frombertgen to investigate the extent to which the per-formance improvements are related to other tasks.
similar to § 3.4.1, we focus on the multi30ken→de mmt task and train a single-task, hybrid-initialised bertgen.
figure 6 compares the vali-dation bleu scores obtained by the default bert-gen and the single-task variant.
we observe thatbertgen beneﬁts from multi-task training and,more importantly, does not seem to exhibit patterns.
figure 6: validation scores on multi30k en→demmt for the multi-tasking ablation: the defaultmulti-task bertgen outperforms the single-task one..of catastrophic forgetting (french, 1999).
basedon these observations, we expect similar modelbehavior to hold for other tasks..4 related work.
4.1 multimodal multilingual pre-training.
research in nlp and related ﬁelds has been in-creasingly focusing on transfer learning approacheswhere a model is ﬁrst pre-trained on a data-richtask, and then transferred to downstream tasks (mc-cann et al., 2017; peters et al., 2018; devlin et al.,2019).
this framework presumably allows themodel to capture useful inductive biases that gen-eralise to a variety of nlp tasks, often after per-forming a task-speciﬁc ﬁne-tuning (raffel et al.,2020).
of these, the most relevant studies to ourwork are bert (devlin et al., 2019) and its multi-lingual version m-bert, which pre-train a trans-former (vaswani et al., 2017) on large monolin-gual corpora using the masked language modelling(mlm) objective..6447visual onlyrandomhybrid101520253035404505101520bleupasses over en-de mmt data0510153035404550bleupasses over en-de mmt datasingle taskmulti task20recent research has also attempted to combinelinguistic inputs with other modalities such as vi-sion and speech, to achieve a grounded understand-ing of meaning.
successful approaches includinglxmert (tan and bansal, 2019), vl-bert (suet al., 2020) and others (lu et al., 2019; li et al.,2020a,b) achieve this by combining bert’s mlmobjective with auxiliary tasks such as masked re-gion classiﬁcation and image sentence matching,and pre-train their model on large-scale image cap-tioning corpora (chen et al., 2015; sharma et al.,2018).
similarly, speechbert extends bert byjointly training on speech and text data (chuanget al., 2020).
although sota results are reportedby these approaches, they focus on unimodal andmultimodal natural language understanding (nlu)tasks, with a strong emphasis in english.
the back-bone of bertgen combines vl-bert (su et al.,2020) with m-bert (devlin et al., 2019) to realisea multilingual and multimodal generator that canbe used for a diverse set of generative tasks andlanguages rather than nlu tasks..4.2 pre-training for generative tasks.
previous work has studied how to beneﬁt frompre-trained bert models in generative tasks suchas nmt (imamura and sumita, 2019; clinchantet al., 2019; zhu et al., 2020).
bertgen differsfrom these as it is not ﬁne-tuned for a particularmt corpus and it exhibits multi-lingual and multi-modal properties for general purpose generation..another related branch of work explores pre-training strategies speciﬁc to sequence-to-sequencetasks.
this includes mass (song et al., 2019),which exploits an encoder-decoder frameworkwith the mlm objective for task-speciﬁc gener-ative pre-training and unilm (dong et al., 2019),which introduces uni-directional, bi-directional andsequence-to-sequence lm objectives by carefullyadjusting the self-attention masks during train-ing.
zhou et al.
(2020) extend unilm to vision& language pre-training using conceptual cap-tions (sharma et al., 2018) as the pre-trainingdataset.
however, these models require a furtherﬁne-tuning step for generative tasks, unlike bert-gen that is trained only once..4.3 multi-task learning for generation.
several approaches exist for multi-task learning &generation (dong et al., 2015; luong et al., 2016)in nlp, especially in multilingual nmt, wheretasks denote different language pairs (zoph and.
knight, 2016; firat et al., 2016).
the multi-task(and zero-shot) generation ability of bertgen ismostly inspired by ha et al.
(2016) and johnsonet al.
(2017).
both of these introduced target lan-guage speciﬁers to select the output language whendecoding translations from their model..our multilingual & multimodal take on multi-task generation is most similar to kaiser et al.
(2017), where a single transformer model istrained on different tasks including image cap-tioning, object classiﬁcation, machine translation,speech recognition and parsing.
however, theirarchitecture depends on particular structures suchas encoders, decoders, modality-speciﬁc networksand i/o mixers, unlike bertgen which does notrequire task-speciﬁc modules..5 conclusions.
in this paper, we presented bertgen, a novel gen-erative, decoder-only model which extends bertby combining multimodal and multilingual pre-trained models.
our ﬁndings show that bertgenobtains strong performance on a variety of gen-erative tasks and further generalises over unseentasks.
importantly, our model demonstrates the po-tential for general-purpose (instead of task-speciﬁc)generation that is above and beyond the traditionalpre-training and ﬁne-tuning practices.
bertgen isalso parameter efﬁcient as it has 89.3m total param-eters and is trained on thirteen tasks encompassingmt, multimodal mt and image captioning.
on theother hand, each of the single-task fairseq nmtbaselines has 31.5m parameters..our ablation studies show that bertgen is ableto efﬁciently transfer relevant inductive biases fromthe pre-trained models and beneﬁts from multi-tasklearning without suffering from catastrophic for-getting.
we hope that these ﬁndings will motivatefuture research in exploiting more sophisticatedpre-trained models in place of m-bert and vl-bert and others..acknowledgments.
this paper is a follow-up work to the msc.
thesisof faidon mitzalis, co-supervised by prof. luciaspecia and dr. ozan caglayan.
lucia specia,pranava madhyastha and ozan caglayan receivedsupport from multimt project (h2020 erc start-ing grant no.
678017).
lucia specia also receivedsupport from the air force ofﬁce of scientiﬁc re-search (under award number fa8655-20-1-7006)..6448references.
peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..lo¨ıc barrault, ondˇrej bojar, marta r. costa-juss`a,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, christof monz, mathias m¨uller,santanu pal, matt post, and marcos zampieri.
2019.findings of the 2019 conference on machine transla-tion (wmt19).
in proceedings of the fourth con-ference on machine translation (volume 2: sharedtask papers, day 1), pages 1–61, florence, italy.
as-sociation for computational linguistics..ondˇrej bojar, christian federmann, mark fishel,yvette graham, barry haddow, philipp koehn, andchristof monz.
2018. findings of the 2018 con-in pro-ference on machine translation (wmt18).
ceedings of the third conference on machine trans-lation: shared task papers, pages 272–303, bel-gium, brussels.
association for computational lin-guistics..ozan caglayan,.
julia ive, veneta haralampieva,pranava madhyastha, lo¨ıc barrault, and lucia spe-cia.
2020. simultaneous machine translation with vi-sual context.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2350–2361, online.
associa-tion for computational linguistics..ozan caglayan, pranava madhyastha, lucia specia,and lo¨ıc barrault.
2019. probing the need for visualcontext in multimodal machine translation.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 4159–4170,minneapolis, minnesota.
association for computa-tional linguistics..mauro cettolo, christian girardi, and marcello fed-erico.
2012. wit3: web inventory of transcribedand translated talks.
in proceedings of the 16th an-nual conference of the european association for ma-chine translation, pages 261–268, trento, italy.
eu-ropean association for machine translation..xinlei chen, hao fang, tsung-yi lin, ramakr-ishna vedantam, saurabh gupta, piotr dollar, andc. lawrence zitnick.
2015. microsoft coco cap-tions: data collection and evaluation server..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..yung-sung chuang, chi-liang liu, hung yi lee, andlin shan lee.
2020. speechbert: an audio-and-text jointly learned language model for end-to-in proc.
inter-end spoken question answering.
speech 2020, pages 4168–4172..stephane clinchant, kweon woo jung, and vassilinanikoulina.
2019. on the use of bert for neu-in proceedings of the 3rdral machine translation.
workshop on neural generation and translation,pages 108–117, hong kong.
association for com-putational linguistics..michael denkowski and alon lavie.
2014. meteor uni-versal: language speciﬁc translation evaluation forin proceedings of the ninthany target language.
workshop on statistical machine translation, pages376–380, baltimore, maryland, usa.
associationfor computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..daxiang dong, hua wu, wei he, dianhai yu, andhaifeng wang.
2015. multi-task learning for mul-in proceedings of thetiple language translation.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 1: long papers), pages 1723–1732, beijing,china.
association for computational linguistics..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, volume 32, pages 13063–13075. curran associates, inc..yen-chun chen, zhe gan, yu cheng, jingzhou liu,and jingjing liu.
2020.distilling knowledgelearned in bert for text generation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7893–7905, on-line.
association for computational linguistics..desmond elliott.
2018. adversarial evaluation of mul-timodal machine translation.
in proceedings of the2018 conference on empirical methods in natu-ral language processing, pages 2974–2978, brus-sels, belgium.
association for computational lin-guistics..6449desmond elliott, stella frank, khalil sima’an, and lu-cia specia.
2016. multi30k: multilingual english-german image descriptions.
in proceedings of the5th workshop on vision and language, pages 70–74, berlin, germany.
association for computationallinguistics..orhan firat, kyunghyun cho, and yoshua bengio.
2016. multi-way, multilingual neural machine trans-in pro-lation with a shared attention mechanism.
ceedings of the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages866–875, san diego, california.
association forcomputational linguistics..robert m. french.
1999. catastrophic forgetting inconnectionist networks.
trends in cognitive sci-ences, 3(4):128–135..gao.
2020b.
oscar: object-semantics aligned pre-training for vision-language tasks.
in computer vi-sion – eccv 2020, pages 121–137, cham.
springerinternational publishing..jindˇrich libovick´y.
2019. multimodality in machinetranslation.
ph.d. thesis, charles university, fac-ulty of mathematics and physics, institute of formaland applied linguistics, prague, czechia..ilya loshchilov and frank hutter.
2019. decoupledin international con-.
weight decay regularization.
ference on learning representations..j. lu, c. xiong, d. parikh, and r. socher.
2017. know-ing when to look: adaptive attention via a visualsentinel for image captioning.
in 2017 ieee confer-ence on computer vision and pattern recognition(cvpr), pages 3242–3250..thanh-le ha, jan niehues, and alex waibel.
2016. to-ward multilingual neural machine translation withuniversal encoder and decoder.
in proceedings ofthe 13th international conference on spoken lan-guage translation..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems, pages 13–23..kenji imamura and eiichiro sumita.
2019. recycling apre-trained bert encoder for neural machine trans-lation.
in proceedings of the 3rd workshop on neu-ral generation and translation, pages 23–31, hongkong.
association for computational linguistics..jiasen lu, vedanuj goswami, marcus rohrbach, deviparikh, and stefan lee.
2020.
12-in-1: multi-taskvision and language representation learning.
in theieee/cvf conference on computer vision and pat-tern recognition (cvpr)..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..lukasz kaiser, aidan n. gomez, noam shazeer,ashish vaswani, niki parmar, llion jones, andjakob uszkoreit.
2017. one model to learn themall..andrej karpathy and li fei-fei.
2015. deep visual-semantic alignments for generating image descrip-in proceedings of the ieee conference ontions.
computer vision and pattern recognition, pages3128–3137..gen li, nan duan, yuejian fang, ming gong, anddaxin jiang.
2020a.
unicoder-vl: a universal en-coder for vision and language by cross-modal pre-in the thirty-fourth aaai conferencetraining.
on artiﬁcial intelligence, aaai 2020, the thirty-second innovative applications of artiﬁcial intelli-gence conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcial intel-ligence, eaai 2020, new york, ny, usa, february7-12, 2020, pages 11336–11344.
aaai press..xiujun li, xi yin, chunyuan li, pengchuan zhang,xiaowei hu, lei zhang, lijuan wang, houdonghu, li dong, furu wei, yejin choi, and jianfeng.
jiasen lu, jianwei yang, dhruv batra, and devi parikh.
2018. neural baby talk.
in proceedings of the ieeeconference on computer vision and pattern recog-nition, pages 7219–7228..thang luong, quoc v. le,.
ilya sutskever, oriolvinyals, and lukasz kaiser.
2016. multi-task se-quence to sequence learning.
in international con-ference on learning representations..bryan mccann, james bradbury, caiming xiong, andrichard socher.
2017. learned in translation: con-in proceedings of thetextualized word vectors.
31st international conference on neural informa-tion processing systems, nips’17, page 6297–6308,red hook, ny, usa.
curran associates inc..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..6450matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..cyrus rashtchian, peter young, micah hodosh, andjulia hockenmaier.
2010. collecting image annota-tions using amazon’s mechanical turk.
in proceed-ings of the naacl hlt 2010 workshop on creatingspeech and language data with amazon’s mechan-ical turk, pages 139–147, los angeles.
associationfor computational linguistics..sascha rothe, shashi narayan, and aliaksei severyn.
2020. leveraging pre-trained checkpoints for se-quence generation tasks.
transactions of the asso-ciation for computational linguistics, 8:264–280..piyush sharma, nan ding, sebastian goodman, andradu soricut.
2018.conceptual captions: acleaned, hypernymed, image alt-text dataset for au-in proceedings of thetomatic image captioning.
56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages2556–2565, melbourne, australia.
association forcomputational linguistics..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in pro-ceedings of the 36th international conference onmachine learning, volume 97 of proceedings of ma-chine learning research, pages 5926–5936.
pmlr..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2020. vl-bert: pre-training of generic visual-linguistic representa-tions.
in international conference on learning rep-resentations..umut sulubacak, ozan caglayan, stig-arne gr¨onroos,aku rouhe, desmond elliott, lucia specia, andj¨org tiedemann.
2020. multimodal machine trans-lation through visuals and speech.
machine transla-tion, pages 1–51..chen sun, austin myers, carl vondrick, kevin mur-phy, and cordelia schmid.
2019. videobert: a jointmodel for video and language representation learn-ing.
in proceedings of the ieee/cvf internationalconference on computer vision, pages 7464–7473..hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-formers.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5100–5111, hong kong, china.
association forcomputational linguistics..j¨org tiedemann.
2012. parallel data, tools and inter-in proceedings of the eighth in-faces in opus.
ternational conference on language resources andevaluation (lrec’12), pages 2214–2218, istanbul,turkey.
european language resources association(elra)..mesut erhan unal, begum citamak, semih yagcioglu,aykut erdem, erkut erdem, nazli ikizler cinbis,and ruket cakici.
2016. tasviret: a benchmarkdataset for automatic turkish description generationin 2016 24th signal processingfrom images.
and communication application conference (siu),pages 1977–1980.
ieee..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recog-nition, pages 4566–4575..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..jiacheng yang, mingxuan wang, hao zhou, chengqizhao, weinan zhang, yong yu, and lei li.
2020.towards making the most of bert in neural ma-chine translation.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 34, pages9378–9385..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, volume 32. curranassociates, inc..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visual.
6451denotations: new similarity metrics for semantic in-ference over event descriptions.
transactions of theassociation for computational linguistics, 2:67–78..luowei zhou, hamid palangi, lei zhang, houdonghu, jason corso, and jianfeng gao.
2020. uni-ﬁed vision-language pre-training for image caption-ing and vqa.
proceedings of the aaai conferenceon artiﬁcial intelligence, 34(07):13041–13049..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tie-yan liu.
incorporating bert into neural machine2020.in 8th international conference ontranslation.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..barret zoph and kevin knight.
2016. multi-sourceneural translation.
in proceedings of the 2016 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 30–34, san diego, cali-fornia.
association for computational linguistics..6452a qualitative examples.
bertgen: un groupe de jeunes sont r´eunis, et ils sont debout `a l ’ext´erieur d’un bˆatiment..(a group of young people are gathered, and they are standing outside a building.).
mt ref: des gens debout devant un bˆatiment..(people standing outside of a building.).
bertgen: une petite ﬁlle lit un livre.
(a little girl reads a book.).
mt ref: un jeune enfant dormant dans son lit avec un livre ouvert sur sa poitrine..(a young child sleeping in her bed with an open book on her chest.).
bertgen: des manifestants avec des pancartes..(demonstrators with placards.).
mt ref: des familles de militaires d´eﬁlent dans new york un jour de pluie.
(military families are marching through new york on a rainy day.).
table 8: zero-shot french image captioning examples for the flickr30k test set..6453en:fr:.
de:.
tr:.
a group of people are riding on elephants through a river.
un groupe de personnes sur des chevaux sur un bateau dans un ruisseau.
a group of people on horses on a boat in a stream.
eine gruppe reiter f¨ahrt auf einem ﬂuss.
a group of riders is riding on a river.
bir grup insan bir derede duran d¨ort tane at ile ilerliyorlar.
a group of people are moving with four horses standing in a stream..en:fr:.
de:.
tr:.
a black dog is playing with a yellow toy in the grass.
un chien avec une frisbee dans la pelouse.
a dog with a frisbee in the lawn.
ein schwarzer hund mit rotem halsband spielt mit einem gelben ball auf einer wiese.
a black dog with a red collar is playing with a yellow ball in a meadow.
yes¸il bir topu ısırmaya c¸alıs¸an siyah bir k¨opek.
a black dog trying to bite a green ball..en:fr:.
de:.
tr:.
a boy in a red shirt and white shorts is playing tennis.
un tennisteur frappe une balle.
a tennisteur hits a ball.
ein junge spielt tennis.
a boy is playing tennis.
tenis raketi ile topa vuran c¸ocuk.
boy hitting the ball with a tennis racket..table 9: coco captioning examples: italicised captions are google’s translations into english for non-englishexamples.
bold phrases highlight lexical variations or errors related to the salient visual concepts in the images.
the last example shows a morphological error that bertgen does when trying to generate a tennis player infrench..6454bertgen mehrere ngos, darunter die mozilla - und greenpeace - stiftung,.
sch¨atzen, dass diese neuen werkzeuge unf¨ahig sind und zu sp¨at kommen..several ngos, including the mozilla and greenpeace foundations,estimate that these new tools are incapable and come too late..wmt ref mehrere ngos, unter denen die mozilla - stiftung und greenpeace,.
sch¨atzen, dass diese neuen tools unzureichend sind und zu sp¨at kommen..several ngos, including the mozilla foundation and greenpeace,estimate that these new tools are inadequate and come too late..bertgen immigration wird als ein großes problem f¨ur die ue betrachtet,.
f¨ur 45 prozent der deutschen und 40 prozent aller europ¨aischen..immigration is seen as a big problem for the ue,for 45 percent of germans and 40 percent of all european ones..wmt ref.
die einwanderung halten 45 prozent der deutschen und 40 prozentaller europ¨aer f¨ur das gr¨oßte problem der eu..45 percent of germans and 40 percent of all europeansconsider immigration to be the biggest problem in the eu..bertgen das ist der grund, warum er in seinem buch die frage erforscht,.
ob es alternativen zu wahlen gibt..that is the reason why he explores the question of whetherthere are alternatives to choose from in his book..wmt ref.
deshalb geht er in seinem buch der frage nach,ob es alternativen zu wahlen gibt..therefore, in his book, he investigates the question of whetherthere are alternatives to choose from..table 10: zero-shot fr→de translations of wmt’19 test set.
the italicised sentences are google translatetranslations of the german outputs into english..6455