learning from the worst: dynamically generated datasetsto improve online hate detection.
bertie vidgen†, tristan thrush‡, zeerak waseem(cid:63), douwe kiela‡†the alan turing institute; (cid:63)university of shefﬁeld; ‡facebook ai researchbvidgen@turing.ac.uk.
abstract.
we present a human-and-model-in-the-loopprocess for dynamically generating datasetsand training better performing and more ro-bust hate detection models.
we provide a newdataset of ∼40, 000 entries, generated and la-belled by trained annotators over four roundsof dynamic data creation.
it includes ∼15, 000challenging perturbations and each hateful en-try has ﬁne-grained labels for the type and tar-get of hate.
hateful entries make up 54% ofthe dataset, which is substantially higher thancomparable datasets.
we show that model per-formance is substantially improved using thisapproach.
models trained on later rounds ofdata collection perform better on test sets andare harder for annotators to trick.
they alsohave better performance on hatecheck, asuite of functional tests for online hate detec-tion.
we provide the code, dataset and annota-tion guidelines for other researchers to use..1.introduction.
accurate detection of online hate speech is impor-tant for ensuring that such content can be found andtackled scalably, minimizing the risk that harm willbe inﬂicted on victims and making online spacesmore accessible and safe.
however, detecting on-line hate has proven remarkably difﬁcult and con-cerns have been raised about the performance, ro-bustness, generalisability and fairness of even state-of-the-art models (waseem et al., 2018; vidgenet al., 2019a; caselli et al., 2020; mishra et al.,2019; poletto et al., 2020).
to address these chal-lenges, we present a human-and-model-in-the-loopprocess for collecting data and training hate detec-tion models..our approach encompasses four rounds of datageneration and model training.
we ﬁrst trained aclassiﬁcation model using previously released hatespeech datasets.
we then tasked annotators with.
presenting content that would trick the model andyield misclassiﬁcations.
at the end of the roundwe trained a new model using the newly presenteddata.
in the next round the process was repeatedwith the new model in the loop for the annotators totrick.
we had four rounds but this approach could,in principle, be continued indeﬁnitely..round 1 contains original content created syn-thetically by annotators.
rounds 2, 3 and 4 aresplit into half original content and half pertur-bations.
the perturbations are challenging ‘con-trast sets’, which manipulate the original text justenough to ﬂip the label (e.g.
from ‘hate’ to ‘nothate’) (kaushik et al., 2019; gardner et al., 2020).
in rounds 3 and 4 we also tasked annotators withexploring speciﬁc types of hate and taking close in-spiration from real-world hate sites to make contentas adversarial, realistic, and varied as possible..models have lower accuracy when evaluatedon test sets from later rounds as the content be-comes more adversarial.
similarly, the rate atwhich annotators trick models also decreases asrounds progress (see table 3).
at the same time,models trained on data from later rounds achievehigher accuracy, indicating that their performanceimproves (see table 4).
we verify improved modelperformance by evaluating them against the hate-check functional tests (r¨ottger et al., 2020), withaccuracy improving from 60% in round 1 to 95%in round 4. in this way the models ‘learn from theworst’ because as the rounds progress (a) they be-come increasingly accurate in detecting hate whichmeans that (b) annotators have to provide morechallenging content in order to trick them..we make three contributions to online hate clas-siﬁcation research.
first, we present a human-and-model-in-the-loop process for training online hatedetection models.
second, we present a dataset of40, 000 entries, of which 54% are hate.
it includesﬁne-grained annotations by trained annotators for.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1667–1682august1–6,2021.©2021associationforcomputationallinguistics1667label, type and target (where applicable).
third,we present high quality and robust hate detectionmodels.
all data, code and annotation guidelinesare available.1.
2 background.
benchmarkdatasets severalbenchmarkdatasets have been put forward for online hateclassiﬁcation (waseem and hovy, 2016; waseem,2016; davidson et al., 2017; founta et al., 2018;mandl et al., 2019; zampieri et al., 2019, 2020;vidgen et al., 2020, 2021).
these datasets offer acomparison point for detection systems and havefocused the ﬁeld’s attention on important subtasks,such as classiﬁcation across different languages,domains and targets of hate.
performance on somebenchmark datasets has increased substantiallythrough the use of more advanced models.
forinstance, in the original waseem and hovy (2016)paper in 2016, the authors achieved an f1 of 0.74.by 2018 this had increased to 0.93 (pitsilis et al.,2018)..numerous problems have been identiﬁed withhate speech training datasets, such as lacking lin-guistic variety, being inexpertly annotated and de-grading over time (vidgen et al., 2019a; polettoet al., 2020).
vidgen and derczynski (2020) ex-amined 63 open-source abusive language datasetsand found that 27 (43%) were sourced from twitter(vidgen and derczynski, 2020).
in addition, manydatasets are formed with bootstrapped sampling,such as keyword searches, due to the low preva-lence of hate speech ‘in the wild’ (vidgen et al.,2019b).
such bootstrapping can substantially biasthe nature and coverage of datasets (wiegand et al.,2019).
models trained on historical data may alsonot be effective for present-day hate classiﬁcationmodels given how quickly online conversationsevolve (nobata et al., 2016)..model limitations systems trained on existingdatasets have been shown to lack accuracy, robust-ness and generalisability, creating a range of falsepositives and false negatives (schmidt and wie-gand, 2017; mishra et al., 2019; vidgen and der-czynski, 2020; r¨ottger et al., 2020; mathew et al.,2020).
these errors often make models unsuitablefor use in downstream tasks, such as moderatingonline content or measuring online hate..false positives are non-hateful entries whichare incorrectly classiﬁed as hateful.
vidgen et al.
(2020) report that 29% of errors from a classiﬁerfor east asian prejudice are due to lexical similari-ties between hateful and non-hateful entries, suchas abuse directed towards out-of-scope targets be-ing misclassiﬁed as sinophobic.
other researchshows that some identity terms (e.g.
‘gay’) aresubstantially more likely to appear in toxic contentin training datasets, leading models to overﬁt onthem (dixon et al., 2018; kennedy et al., 2020).
similarly, many models overﬁt on the use of slursand pejorative terms, treating them as hateful irre-spective of how they are used (waseem et al., 2018;davidson et al., 2017; kurrek et al., 2020; palmeret al., 2020).
this is problematic when the termsare used as part of counter speech (wright et al.,2017; chung et al., 2019) or have been reclaimedby the targeted group (waseem et al., 2018; sapet al., 2019).
models can also misclassify interper-sonal abuse and incivil language as hateful (wul-czyn et al., 2017a; zampieri et al., 2019; palmeret al., 2020)..false negatives are hateful entries which are in-correctly classiﬁed as non-hateful.
gr¨ondahl et al.
(2018) show that making simple changes such asinserting spelling errors, using leetspeak2, chang-ing word boundaries, and appending words canlead to misclassiﬁcations of hate.
hosseini et al.
(2017) also investigate how detection models canbe attacked and report similar ﬁndings.
in othercases, false negatives can be provoked by changingthe ‘sensitive’ attribute of hateful content, such aschanging the target from ‘gay’ to ‘black’ people(garg et al., 2019).
this can happen when mod-els are trained on data which only contains hatedirected against a limited set of targets (salminenet al., 2020).
another source of false negatives iswhen classiﬁcation systems are applied to out-of-domain settings, such as system trained on twitterdata being applied to data from gab (karan andˇsnajder, 2018; pamungkas et al., 2020; swamyet al., 2019; basile et al., 2019; salminen et al.,2020).
subtle and implicit forms of hate speechcan also create false negatives (vidgen and yasseri,2019; palmer et al., 2020; mathew et al., 2020), aswell as more ‘complex’ forms of speech such assarcasm, irony, adjective nominalization and rhetor-ical questions (caselli et al., 2020; vidgen et al.,.
1https://github.com/bvidgen/.
2leetspeak refers to the obfuscation of words by replacing.
dynamically-generated-hate-speech-dataset.
letters with similar looking numbers and symbols..16682019a)..dynamic benchmarking and contrast sets ad-dressing the numerous ﬂaws of hate detection mod-els is a difﬁcult task.
the problem may partlylie in the use of static benchmark datasets andﬁxed model evaluations.
in other areas of natu-ral language processing, several alternative modeltraining and dataset construction paradigms havebeen presented, involving dynamic and iterativeapproaches.
in a dynamic dataset creation setup,annotators are incentivised to produce high-quality‘adversarial’ samples which are challenging forbaseline models, repeating the process over mul-tiple rounds (nie et al., 2020).
this offers amore targeted way of collecting data.
dinanet al.
(2019) ask crowd-workers to ‘break’ a bertmodel trained to identify toxic comments and thenretrain it using the new examples.
their ﬁnal modelis more robust to complex forms of offensive con-tent, such as entries with ﬁgurative language andwithout profanities..another way of addressing the limitations ofstatic datasets is through creating ‘contrast sets’ ofperturbations (kaushik et al., 2019; gardner et al.,2020).
by making minimal label-changing mod-iﬁcations that preserve ‘lexical/syntactic artifactspresent in the original example’ (gardner et al.,2020, p. 1308) the risk of overﬁtting on spuri-ous correlations is minimized.
perturbations haveonly received limited attention in the context ofhate detection.
samory et al.
(2020) create 2, 000‘hard-to-classify’ not-sexist examples which con-trast sexist examples in their dataset.
they showthat ﬁne-tuning a bert model with the contrastset produces more robust classiﬁcation system..dynamic benchmarking and contrast sets high-light the effectiveness of developing datasets in adirected and adaptive way, ensuring that modelslearn from and are evaluated on the most challeng-ing content.
however, to date, these approachesremain under-explored for hate speech detectionand to the best of our knowledge no prior workin hate speech detection has combined the two ap-proaches within one system..with very different schemas.
the hierarchical tax-onomy we present aims for a balance between gran-ularity versus conceptual distinctiveness and anno-tation simplicity, following the guidance of nicker-son et al.
(2013).
all entries are assigned to either‘hate’ or ‘not hate’.
‘hate’ is deﬁned as “abu-sive speech targeting speciﬁc group characteristics,such as ethnic origin, religion, gender, or sexualorientation.” (warner and hirschberg, 2012).
for‘hate’, we also annotate secondary labels for thetype and target of hate.
the taxonomy for the typeof hate draws on and extends previous work, in-cluding waseem and hovy (2016); vidgen et al.
(2019a); zampieri et al.
(2019)..3.1 types of hate.
derogation content which explicitly attacks, de-monizes, demeans or insults a group.
this resem-bles similar deﬁnitions from davidson et al.
(2017),who deﬁne hate as content that is ‘derogatory’,waseem and hovy (2016) who include ‘attacks’in their deﬁnition, and zampieri et al.
(2019) whoinclude ‘insults’..animosity content which expresses abuseagainst a group in an implicit or subtle manner.
it is similar to the ‘implicit’ and ‘covert’ categoriesused in other taxonomies (waseem et al., 2017;vidgen and yasseri, 2019; kumar et al., 2018)..threatening language content which expressesintention to, support for, or encourages inﬂictingharm on a group, or identiﬁed members of thegroup.
this category is used in datasets by ham-mer (2014), golbeck et al.
(2017) and anzovinoet al.
(2018)..support for hateful entities content which ex-plicitly gloriﬁes, justiﬁes or supports hateful ac-tions, events, organizations, tropes and individu-als (collectively, ‘entities’)..dehumanization content which ‘perceiv[es] ortreat[s] people as less than human’ (haslam andstratemeyer, 2016).
it often involves describinggroups as leeches, cockroaches, insects, germs orrats (mendelsohn et al., 2020)..3 dataset labels.
3.2 targets of hate.
previous research shows the limitations of usingonly a binary labelling schema (i.e., ‘hate’ and‘not hate’).
however, there are few establishedtaxonomies and standards in online hate research,and most of the existing datasets have been labelled.
hate can be targeted against any vulnerable,marginalized or discriminated-against group.
weprovided annotators with a non-exhaustive list of29 identities to focus on (e.g., women, black people,muslims, jewish people and gay people), as well.
1669as a small number of intersectional variations (e.g.,‘muslim women’).
they are given in appendix a.some identities were considered out-of-scope forhate, including men, white people, and heterosex-uals..4 annotation.
data was annotated using an open-source webplatform for dynamic dataset creation and modelbenchmarking.3 the platform supports human-and-model-in-the-loop dataset creation for a variety ofnlp tasks.
annotation was overseen by two ex-perts in online hate.
the annotation process isdescribed in the following section.
annotationguidelines were created at the start of the projectand then updated after each round in response tothe increased need for detail from annotators.
wefollowed the guidance for protecting and monitor-ing annotator well-being provided by vidgen et al.
(2019a).
20 annotators were recruited.
they re-ceived extensive training and feedback during theproject.
full details on the annotation team aregiven in appendix e. the small pool of annota-tors was driven by the logistical constraints of hir-ing and training them to the required standard andprotecting their welfare given the sensitivity andcomplexity of the topic.
nonetheless, it raises thepotential for bias.
we take steps to address this inour test set construction and provide an annotatorid with each entry in our publicly-released datasetto enable further research into this issue..5 dataset formation.
the dataset was generated over four rounds, each ofwhich involved ∼10, 000 entries.
the ﬁnal datasetcomprises 41, 255 entries, as shown in table 1. theten groups that are targeted most often are givenin table 2. entries could target multiple groups.
after each round, the data was split into training,dev and test splits of 80%, 10% and 10%, respec-tively.
approximately half of the entries in the testsets are produced by annotators who do not appearin the training and dev sets (between 1 and 4 ineach round).
this makes the test sets more chal-lenging and minimizes the risk of annotator biasgiven our relatively small pool of annotators (gevaet al., 2019).
the other half of each test set consistsof content from annotators who do appear in thetraining and dev sets..3https://anonymized-url.
rounds 2, 3 and 4 contain perturbations.
in 18cases the perturbation does not ﬂip the label.
thismistake was only identiﬁed after completion ofthe paper and is left in the dataset.
these casescan be identiﬁed by checking whether original andperturbed entries that have been linked togetherhave the same labels (e.g., whether an original andperturbation are both assigned to ’hate’)..target model implementation every round hasa model in the loop, which we call the ‘targetmodel’.
the target model is always trained ona combination of data collected in the previousround(s).
for instance, m2 is the target modelused in r2, and was trained on r1 and r0 data.
for consistency, we use the same model architec-ture everywhere, speciﬁcally roberta (liu et al.,2019) with a sequence classiﬁcation head.
we usethe implementation from the transformers (wolfet al., 2019) library.
more details are available inappendix d..for each new target model, we identify the bestsampling ratio of previous rounds’ data using thedev sets.
m1 is trained on r0 data.
m2 is trainedon r0 data and r1 upsampled to a factor of ﬁve.
m3 is trained on the data used for m2 and r2data upsampled to a factor of one hundred.
m4 istrained on the data used for m3 and one lot of ther3 data..5.1 round 1 (r1).
the target model in r1 is m1, a roberta modeltrained on r0 which consists of 11 english lan-guage training datasets for hate and toxicity takenfrom hatespeechdata.com, as reported in vidgenand derczynski (2020).
it includes widely-useddatasets provided by waseem (2016), davidsonet al.
(2017) and founta et al.
(2018).
it comprises468, 928 entries, of which 22% are hateful/toxic.
the dataset was anonymized by replacing user-names, indicated by the ‘@’ symbol.
urls werealso replaced with a special token.
in r1, annota-tors were instructed to enter synthetic content intothe model that would trick m1 using their own cre-ativity and by exploiting any model weaknessesthey identiﬁed through the real-time feedback..all entries were validated by one other annota-tor and entries marked as incorrect were sent forreview by expert annotators.
this happened with1, 011 entries.
385 entries were excluded for beingentirely incorrect.
in the other cases, the expert an-notator decided the ﬁnal label and/or made minor.
1670label.
hate.
type.
not givenanimositydehumanizationderogationsupportthreatening.
total.
7, 1973, 4399069, 907207606.r1.
7, 19700000.r2.
r3.
r4.
07582613, 57441376.
01, 2063153, 206104148.
01, 4753303, 1276282.total.
22, 262.
7, 197.
5, 010.
4, 979.
5, 076.not hate.
/.
18, 993.
3, 960.
4, 986.
4, 971.
5, 076.all.
total.
41, 255.
11, 157.
9, 996.
9, 950.
10, 152.table 1: summary of data collected in each round.
target.
number of entries.
black peoplewomenjewish peoplemuslimstrans peoplegay peopleimmigrantsdisabled peoplerefugeesarabs.
2, 2782, 1921, 2931, 144972875823575533410.table 2: most common targets of hate in the dataset.
adjustments to the text.
the ﬁnal dataset comprises11, 157 entries of which 7, 197 are ‘hate’ (65%)and 3, 960 are ‘not hate’ (35%).
the type andtarget of hate was not recorded by annotators inr1..5.2 round 2 (r2).
a total of 9, 996 entries were entered in r2.
thehateful entries are split between derogation (3, 577,72%), dehumanization (255, 5%), threats (380,8%), support for hateful entities (39, 1%) and an-imosity (759, 15%).
in r2 we gave annotatorsadversarial ‘pivots’ to guide their work, which weidentiﬁed from a review of previous literature (seesection 2).
the 10 hateful and 12 not hateful adver-sarial pivots, with examples and a description, aregiven in appendix b. half of r2 comprises origi-nally entered content and the other half comprisesperturbed contrast sets..following gardner et al.
(2020), perturbationswere created ofﬂine without feedback from amodel-in-the-loop.
annotators were given four.
main points of guidance: (1) ensure perturbed en-tries are realistic, (2) ﬁrmly meet the criteria ofthe ﬂipped label and type, (3) maximize diversitywithin the dataset in terms of type, target and howentries are perturbed and (4) make the least changespossible while meeting (1), (2) and (3).
commonstrategies for perturbing entries included changingthe target (e.g., from ‘black people’ to ‘the localcouncil’), changing the sentiment (e.g.
‘it’s won-derful having gay people round here’), negating anattack (e.g.
‘muslims are not a threat to the uk’)and quoting or commenting on hate..of the original entries, those which fooled m1were validated by between three and ﬁve other an-notators.
every perturbation was validated by oneother annotator.
annotators could select: (1) cor-rect if they agreed with the label and, for hate, thetype/target, (2) incorrect if the label was wrong or(3) ﬂag if they thought the entry was unrealisticand/or they agreed with the label for hate but dis-agreed with the type or target.
krippendorf’s alphais 0.815 for all original entries if all ‘ﬂagged’ en-tries are treated as ‘incorrect’, indicating extremelyhigh levels of agreement (hallgren, 2012).
allof the original entries identiﬁed by at least twovalidators as incorrect/ﬂagged, and perturbationswhich were identiﬁed by one validator as incor-rect/ﬂagged, were sent for review by an expert an-notator.
this happened in 760 cases in this round..lessons from r2 the validation and review pro-cess identiﬁed some limitations of the r2 dataset.
first, several ‘template’ statements were enteredby annotators.
these are entries which have astandardized syntax and/or lexicon, with only theidentity changed, such as ‘[identity] are [negativeattribute]’.
when there are many cases of each tem-.
1671plate they are easy for the model to correctly clas-sify because they create a simple decision bound-ary.
discussion sessions showed that annotatorsused templates (i) to ensure coverage of differentidentities (an important consideration in makinga generalisable online hate classiﬁer) and (ii) tomaximally exploit model weaknesses to increasetheir model error rate.
we banned the use of tem-plates.
second, in attempting to meet the ‘pivots’they were assigned, some annotators created unre-alistic entries.
we updated guidance to emphasizethe importance of realism.
third, the pool of 10trained annotators is large for a project annotatingonline hate but annotator biases were still produced.
model performance was high in r2 when evalu-ated on a training/dev/test split with all annotatorsstratiﬁed.
we then held out some annotators’ con-tent and performance dropped substantially.
weuse this setup for all model evaluations..5.3 round 3 (r3).
in r3 annotators were tasked with ﬁnding real-world hateful online content to inspire their entries.
all real-world content was subject to at least onesubstantial adjustment prior to being presented tothe model.
9, 950 entries were entered in r3.
thehateful entries are split between derogation (3, 205,64%), dehumanization (315, 6%), threats (147,3%), support for hateful entities (104, 2%) andanimosity (1, 210, 24%).
half of r3 comprisesoriginally entered content (4, 975) and half com-prises perturbed contrast sets (4, 975)..the same validation procedure was used as withr2.
krippendorf’s alpha was 0.55 for all origi-nal entries if all ‘ﬂagged’ entries are treated as‘incorrect’, indicating moderate levels of agree-ment (hallgren, 2012).
this is lower than r2, butstill comparable with other hate speech datasets(e.g., wulczyn et al.
(2017b) achieve krippnedorf’salpha of 0.45).
note that more content is labelledas animosity compared with r2 (24% comparedwith 15%), which tends to have higher levels ofdisagreement.
981 entries were reviewed by theexpert annotators..5.4 round 4 (r4).
as with r3, annotators searched for real-worldhateful online content to inspire their entries.
inaddition, each annotator was given a target iden-tity to focus on (e.g., muslims, women, jewishpeople).
the annotators (i) investigated hateful on-line forums and communities relevant to the target.
identity to ﬁnd the most challenging and nuancedcontent and (ii) looked for challenging non-hateexamples, such as neutral discussions of the iden-tity.
10, 152 entries were entered in r4, comprising5, 076 ‘hate’ and 5, 076 ‘not hate’.
the hatefulentries are split between derogation (3, 128, 62%),dehumanization (331, 7%), threats (82, 2%), sup-port for hateful entities (61, 1%) and animosity(1, 474, 29%).
half of r4 comprises originally en-tered content (5, 076) and half comprises perturbedcontrast sets (5, 076).
the same validation pro-cedure was used as in r2 and r3.
krippendorf’salpha was 0.52 for all original entries if all ‘ﬂagged’entries are treated as ‘incorrect’, indicating mod-erate levels of agreement (hallgren, 2012).
thisis similar to r2.
967 entries were reviewed by theexpert annotators following the validation process..6 model performance.
in this section, we examine the performance ofmodels on the collected data, both when used in-the-loop during data collection (measured by themodel error rate on new content shown by annota-tors), as well as when separately evaluated againstthe test sets in each round’s data.
we also examinehow models generalize by evaluating them on theout-of-domain suite of diagnostic functional testsin hatecheck..6.1 model error rate.
the model error rate is the rate at which annotator-generated content tricks the model.
it decreasesas the rounds progress, as shown in table 3. m1,which was trained on a large set of public hatespeech datasets, was the most easily tricked, eventhough many annotators were learning and had notbeen given advice on its weaknesses.
54.7% of en-tries tricked it, including 64.6% of hate and 49.2%of not hate.
only 27.7% of content tricked theﬁnal model (m4), including 23.7% of hate and31.7% of not hate.
the type of hate affected howfrequently entries tricked the model.
in general,more explicit and overt forms of hate had the low-est model error rates, with threatening languageand dehumanization at 18.2% and 24.8% on aver-age, whereas support for hateful entities and an-imosity had the highest error (55.4% and 46.4%respectively).
the model error rate falls as therounds progress but nonetheless this metric poten-tially still underestimates the increasing difﬁcultyof the rounds and the improvement in the models..1672round.
total.
not.
hate.
animosity.
derogation.
support threatening.
54.7% 64.6% 49.2%34.3% 38.9% 29.7% 40.1%27.8% 20.5% 35.1% 53.8%27.7% 23.7% 31.7% 44.5%.
-.
r1r2r3r4.
all.
dehuman-ization.
-25.5%27.9%21.1%.
24.8%.
-28.7%29.2%26.9%.
-53.8%59.6%49.2%.
-18.4%17.7%18.3%.
18.2%.
36.6% 35.4% 37.7% 46.4%.
28.3%.
55.4%.
table 3: error rate for target models in each round.
error rate decreases as the rounds progress, indicating thatmodels become harder to trick.
annotators were not given real-time feedback on whether their entries tricked themodel when creating perturbations.
more information about tuning is available in appendix d.annotators became more experienced and skilledover the annotation process, and entered progres-sively more adversarial content.
as such the con-tent that annotators enter becomes far harder toclassify in the later rounds, which is also reﬂectedin all models’ lower performance on the later roundtest sets (see table 4)..6.2 test set performance.
table 4 shows the macro f1 of models trained ondifferent combinations of data, evaluated on thetest sets from each round (see appendix c for devset performance).
the target models achieve lowerscores when evaluated on test sets from the laterrounds, demonstrating that the dynamic approachto data collection leads to increasingly more chal-lenging data.
the highest scores for r3 and r4data are in the mid-70s, compared to the high 70sin r2 and low 90s in r1.
generally, the target mod-els from the later rounds have higher performanceacross the test sets.
for instance, m4 is the best per-forming model on r1, r2 and r4 data.
it achieves75.97 on the r4 data whereas m3 achieves 74.83and m2 only 60.87. a notable exception is m1which outperforms m2 on the r3 and r4 test sets.
table 4 presents the results for models trainedon just the training sets from each round (with noupsampling), indicated by m(rx only).
in generalthe performance is lower than the equivalent targetmodel.
for instance, m4 achieves macro f1 of75.97 on the r4 test data.
m(r3 only) achieves73.16 on that test set and m(r4 only) just 69.6.in other cases, models which are trained on justone round perform well on some rounds but are farworse on others.
overall, building models cumu-latively leads to more consistent performance.
ta-ble 4 also shows models trained on the cumulativerounds of data with no upsampling, indicated bym(rx+ry).
in general, performance is lower with-.
figure 1: performance of target models on the hate-check test suite..out upsampling; the f1 of m3 is 2 points higher onthe r3 test set than the equivalent non-upsampledmodel (m(r0+r1+r2))..6.3 hatecheck.
to better understand the weaknesses of the targetmodels from each round, we apply them to hat-echeck, as presented by r¨ottger et al.
(2020).
hatecheck is a suite of functional tests forhate speech detection models, based on the test-ing framework introduced by ribeiro et al.
(2020).
it comprises 29 tests, of which 18 correspond todistinct expressions of hate and the other 11 arenon-hateful contrasts.
the selection of functionaltests is motivated by a review of previous literatureand interviews with 21 ngo workers.
from the29 tests in the suite, 3, 728 labelled entries are gen-erated in the dataset of which 69% are ‘hate’ and31% are ‘not hate’..performance of target models trained on laterrounds is substantially higher, increasing from 60%(on both ‘hate’ and ‘not hate’) combined for m1.
16730%25%50%75%100%m1m2m3m4target modelsaccuracy (%)labelhateallnonetarget model performance on hatecheckmodel.
m1 (r1 target)m2 (r2 target)m3 (r3 target)m4 (r4 target).
m(r1 only)m(r2 only)m(r3 only)m(r4 only).
m(r0+r1)m(r0+r1+r2)m(r0+r1+r2+r3)m(r0+r1+r2+r3+r4).
r1.
r2.
r3.
r4.
44.84±1.190.17±1.4291.37±1.2692.01±0.6.
92.20±0.5580.73±0.472.71±1.0572.26±1.3.
88.78±0.8991.09±0.3791.17±0.9990.3±0.96.
54.42±0.4566.05±0.6777.14±1.2678.02±0.91.
62.87±0.6376.52±0.778.55±0.7176.78±1.65.
66.15±0.7774.73±0.9577.03±0.7277.93±0.84.
66.07±1.0362.89±1.2676.97±0.4975.89±0.62.
47.67±1.0477.43±0.5174.14±1.577.21±0.43.
67.15±1.1174.73±0.4674.6±0.4876.79±0.24.
60.91±0.460.87±1.6274.83±0.9275.97±0.96.
52.37±1.2774.88±0.8573.16±0.5869.6±0.6.
63.44±0.2671.59±0.5973.94±0.9472.93±0.56.
table 4: macro f1 with standard deviation over 5 training rounds, evaluated on each rounds’ test set.
early-stopping is performed on the latest dev set for each round where dev results are obtained at least once per epoch,out of four epochs..to 95% for m4.
performance is better than all fourmodels evaluated by r¨ottger et al.
(2020), of whichperspective’s toxicity classiﬁer4 is best perform-ing with 77% overall accuracy, including 90% on‘hate’ and 48% on ‘not hate’.
notably, the per-formance of m4 is consistent across both ‘hate’and ‘not hate’, achieving 95% and 93% respec-tively.
this is in contrast to earlier target models,such as m2 which achieves 91% on ‘hate’ but only67% on ‘not hate’ (note that this is actually a re-duction in performance from m1 on ‘not hate’).
note that hatecheck only has negative predic-tive power.
these results indicate the absence ofparticular weaknesses in models rather than neces-sarily characterising generalisable strengths..a further caveat is that in r2 the annotators weregiven adversarial pivots to improve their ability totrick the models (see above).
these pivots exploitsimilar model weaknesses as the functional testsin hatecheck expose, which creates a risk thatthis gold standard is not truly independent.
we didnot identify any exact matches, although after low-ering case and removing punctuation there are 21matches.
this is just 0.05% of our dataset but indi-cates a risk of potential overlap and cross-datasetsimilarity..7 discussion.
online hate detection is a complex and nuancedproblem, and creating systems that are accurate,.
4see: https://www.perspectiveapi.com/#/.
home..robust and generalisable across target, type anddomain has proven difﬁcult for ai-based solutions.
it requires having datasets which are large, varied,expertly annotated and contain challenging content.
dynamic dataset generation offers a powerful andscalable way of creating these datasets, and trainingand evaluating more robust and high performingmodels.
over the four rounds of model training andevaluation we show that the performance of targetmodels improves, as measured by their accuracyon the test sets.
the robustness of the target modelsfrom later rounds also increases, as shown by theirbetter performance on hatecheck..dynamic data creation systems offer severaladvantages for training better performing mod-els.
first, problems can be addressed as workis conducted – rather than creating the datasetand then discovering any inadvertent design ﬂaws.
for instance, we continually worked with annota-tors to improve their understanding of the guide-lines and strategies for tricking the model.
wealso introduced perturbations to ensure that contentwas more challenging.
second, annotators can in-put more challenging content because their workis guided by real-time feedback from the targetmodel.
discussion sessions showed that annotatorsresponded to the models’ feedback in each round,adjusting their content to ﬁnd better ways to trickit.
this process of people trying to ﬁnd ways to cir-cumvent hate speech models such that their contentgoes undetected is something that happens oftenin the real world.
third, dynamic datasets can be.
1674constructed to better meet the requirements of ma-chine learning; our dataset is balanced, comprising∼54% hate.
it includes hate targeted against a largenumber of targets, providing variety for the modelto learn from, and many entries were constructedto include known challenging content, such as useof slurs and identity referents..however, our approach also presents some chal-lenges.
first, it requires substantial infrastructureand resources.
this project would not have beenpossible without the use of an online interface anda backend that can serve up state-of-the-art hatespeech detection models with relatively low latency.
second, it requires substantial domain expertisefrom dataset creators as well as annotators, such asknowing where to ﬁnd real-world hate to inspiresynthetic entries.
this requires a cross-disciplinaryteam, combining social science with linguistics andmachine learning expertise.
third, evaluating andvalidating content in a time-constrained dynamicsetting can introduce new pressures on the annota-tion process.
the perturbation process also requiresadditional annotator training, or else might intro-duce other inadvertent biases..8 conclusion.
we presented a human-and-model-in-the-loop pro-cess for training an online hate detection system.
itwas employed dynamically to collect four roundsof hate speech datasets.
the datasets are largeand high quality, having been obtained using onlyexpert annotators.
they have ﬁne-grained annota-tions for the type and target of hate, and includeperturbations to increase the dataset difﬁculty.
wedemonstrated that the models trained on these dy-namically generated datasets are much better at thetask of hate speech detection, including evaluationon out-of-domain functional test suites..in future work we aim to expand the size anddiversity of the annotator pool for further roundsof dynamic adversarial data collection.
we wouldlike to evaluate different models in-the-loop be-yond roberta.
the datasets also open many newavenues of investigation, including training modelson only original entries and evaluating against per-turbations (and vice versa) and training multi-labelresults for type and target of hate.
data collectionfor future rounds is ongoing..impact statement & ethicalconsiderations.
in the impact statement we address relevant ethicalconsiderations that were not explicitly discussed inthe main body of the paper..data the entries in the dataset were created bythe annotation team and, where needed, reviewedby the expert annotators.
in no cases did annotatorsenter content that they found on online sites.
allentries which were closely inspired by real-worldcontent (e.g., data entered during round 4) had sub-stantial adjustments made to them.
as such, thedata is synthetic..annotator compensation we employed a teamof twenty annotators to enter content who workedvarying hours on a ﬂexible basis over four months.
annotators were compensated at a rate of £16 perhour.
the rate was set 50% above the local liv-ing wage (£10.85), even though all work was com-pleted remotely.
all training time and meetingswere paid..intended use the approach, dataset and mod-els presented here are intended to support moreaccurate and robust detection and classiﬁcation ofonline hate.
we anticipate that the high-qualityand ﬁne-grained labels in the dataset will advanceresearch in online hate in other ways, such as en-abling multiclass classiﬁcation of types and targetsof online hate..potential misuse the dataset and models wepresent could in principle be used to train a genera-tive hate speech model.
alternatively, the datasetand models could be used to better understand thelimitations of current detection tools and then at-tack them.
for instance, if a malicious actor investi-gated our models then they could better understandwhat content tricks content moderation tools andthen use this knowledge to avoid their content be-ing ﬂagged on social media platforms.
however,we believe that these outcomes are unlikely.
we donot report any new weaknesses that have not beenestablished in previous research, and the modelswe present still contain several limitations.
further,it is unlikely that a malicious actor would be able totrain a powerful enough generative model from thisdataset (given its size and composition) to affecttheir activities.
overall, the scientiﬁc and socialbeneﬁts of the present research arguably outweighsthe small risk of their misuse..1675references.
maria anzovino, elisabetta fersini, and paolo rosso.
2018. automatic identiﬁcation and classiﬁcation ofmisogynistic language on twitter.
in nldb, pages57–64..valerio basile, cristina bosco, elisabetta fersini, deb-ora nozza, viviana patti, francisco manuel rangelpardo, paolo rosso, and manuela sanguinetti.
2019.sem eval-2019 task 5: multilingual detection ofhate speech against immigrants and women inin proceedings of the 13th internationaltwitter.
workshop on semantic evaluation, pages 54–63..emily m. bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..tommaso caselli, valerio basile, jelena mitrovi´c, ingakartoziya, and michael granitzer.
2020. i feel of-fended, don’t be abusive!
implicit/explicit mes-sages in offensive and abusive language.
in pro-ceedings of the 12th conference on language re-sources and evaluation (lrec), pages 6193–6202..yi-ling chung, elizaveta kuzmenko, serra sinemtekiroglu, and marco guerini.
2019. conan -counter narratives through nichesourcing: a mul-tilingual dataset of responses to fight online hatespeech.
in proceedings of the 57th annual meetingof the acl, pages 2819–2829..thomas davidson, dana warmsley, michael macy,and ingmar weber.
2017. automated hate speechdetection and the problem of offensive language.
in proceedings of the 11th icwsm, pages 1–4..emily dinan, samuel humeau, bharath chintagunta,and jason weston.
2019. build it break it ﬁx it fordialogue safety: robustness from adversarial humanin proceedings of the 2019 conference onattack.
empirical methods in natural language processing,pages 4537–4546..lucas dixon, john li, jeffrey sorensen, nithum thain,and lucy vasserman.
2018. measuring and miti-gating unintended bias in text classiﬁcation.
inaaai/acm conference on ai, ethics, and society,pages 67–73..antigoni-maria founta, constantinos djouvas, de-spoina chatzakou, ilias leontiadis, jeremy black-burn, gianluca stringhini, athena vakali, michaelsirivianos, and nicolas kourtellis.
2018. largescale crowdsourcing and characterization of twit-ter abusive behavior.
in icwsm, pages 1–11..singh, noah a. smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating nlp models’ local decisionin findings of theboundaries via contrast sets.
assocation for computational linguistics: emnlp2020, pages 1307–1323..sahaj garg, ankur taly, vincent perot, ed h. chi,nicole limtiaco, and alex beutel.
2019. counter-factual fairness in text classiﬁcation through robust-ness.
proceedings of the 2019 aaai/acm confer-ence on ai, ethics, and society, pages 219–226..mor geva, yoav goldberg, and jonathan berant.
2019.are we modeling the task or the annotator?
an inves-tigation of annotator bias in natural language under-standing datasets.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1161–1166, hong kong, china.
as-sociation for computational linguistics..jennifer golbeck, alicia a. geller, jayanee thanki,shalmali naik, kelly m. hoffman, derek michaelwu, alexandra berlinger, priyanka vengataraman,shivika khare, zahra ashktorab, marianna j.martindale, gaurav shahane, paul cheakalos,jenny hottle, siddharth bhagwan, raja rajan gu-nasekaran, rajesh kumar gnanasekaran, rashad o.banjo, piyush ramachandran, lisa rogers, kris-tine m. rogers, quint gergory, heather l. nixon,meghna sardana sarin, zijian wan, cody buntain,ryan lau, and vichita jienjitlert.
2017. a large la-beled corpus for online harassment research.
inproceedings of the acm conference on web science,pages 229–233..tommi gr¨ondahl, luca pajola, mika juuti, mauroconti, and n. asokan.
2018. all you need isin pro-”love”: evading hate-speech detection.
ceedings of the 11th acm workshop on artiﬁcial in-telligence and security, pages 2–12..kevin a hallgren.
2012. computing inter-rater re-liability for observational data: an overview andtutorial.
tutorials in quantitative methods for psy-chology, 8(1):23–34..hugo lewi hammer.
2014. detecting threats of vio-lence in online discussions using bigrams of impor-tant words.
in proceedings - 2014 ieee joint intel-ligence and security informatics conference, jisic2014, page 319..n. haslam and m. stratemeyer.
2016. recent researchon dehumanization.
current opinion in psychology,11:25–29..matt gardner, yoav artzi, victoria basmova, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hanna hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f. liu, phoebe mulcaire, qiang ning, sameer.
hossein hosseini, sreeram kannan, baosen zhang,and radha poovendran.
2017. deceiving google’sperspective api built for detecting toxic com-ments.
in proceedings of the ieee conference oncomputer vision and pattern recognition (cvpr)workshops, pages 1305–1309..1676mladen karan and jan ˇsnajder.
2018. cross-domainin 2nddetection of abusive language online.
workshop on abusive language online, pages 132–137..divyansh kaushik, eduard hovy, and zachary c lip-ton.
2019.learning the difference that makesa difference with counterfactually-augmented data.
arxiv preprint arxiv:1909.12434..brendan kennedy, xisen jin, aida mostafazadeh da-vani, morteza dehghani, and xiang ren.
2020. con-textualizing hate speech classiﬁers with post-hocin proceedings of the 58th annualexplanation.
meeting of the association for computational lin-guistics, pages 5435–5442..ritesh kumar, atul kr ojha, shervin malmasi, andmarcos zampieri.
2018. benchmarking aggressionidentiﬁcation in social media.
in proceedings ofthefirst workshop on trolling, aggression and cyber-bullying,, 1, pages 1–11..jana kurrek, haji mohammad saleem, and derekruths.
2020. towards a comprehensive taxonomyand large-scale annotated corpus for online slurusage.
in proceedings of the fourth workshop ononline abuse and harms, pages 138–149..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv:1907.11692..thomas mandl, sandip modha, prasenjit majumder,daksh patel, mohana dave, chintak mandlia, andaditya patel.
2019. overview of the hasoc trackat fire 2019: hate speech and offensive contentidentiﬁcation in indo-european languages.
in fire’19: proceedings of the 11th forum for informationretrieval evaluation, pages 14–17..binny mathew, punyajoy saha, seid muhie yi-mam, chris biemann, pawan goyal, and animeshmukherjee.
2020. hatexplain: a benchmark datasetfor explainable hate speech detection..julia mendelsohn, yulia tsvetkov, and dan jurafsky.
2020. a framework for the computational linguis-tic analysis of dehumanization.
frontiers in artiﬁ-cial intelligence, 3(august):1–24..pushkar mishra, helen yannakoudakis, and eka-terina shutova.
2019.tackling online abuse:a survey of automated abuse detection methods.
arxiv:1908.06024v2, pages 1–17..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..chikashi nobata, achint thomas, yashar mehdad,yi chang, and joel tetreault.
2016. abusive lan-guage detection in online user content.
in worldwide web conference, pages 145–153..alexis palmer, christine carr, melissa robinson, andjordan sanders.
2020. cold: annotation schemeand evaluation data set for complex offensive lan-guage in english.
the journal for language tech-nology and computational linguistics, 34(1):1–28..endang pamungkas, valerio basile, and viviana patti.
2020. misogyny detection in twitter: a multilin-gual and cross-domain study.
information process-ing and management, 57(6)..georgios k. pitsilis, heri ramampiaro, and helgelangseth.
2018. detecting offensive language intweets using deep learning.
arxiv:1801.04433v1,pages 1–17..fabio poletto, valerio basile, manuela sanguinetti,cristina bosco, and viviana patti.
2020. resourcesand benchmark corpora for hate speech detection: asystematic review.
language resources and evalu-ation, pages 1–47..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4902–4912, online.
association for computational lin-guistics..paul r¨ottger, bertram vidgen, dong nguyen, zeerakwaseem, helen margetts, and janet pierrehumbert.
2020. hatecheck: functional tests for hate speechdetection models..joni salminen, maximilian hopf, shammur a. chowd-andhury, soon gyo jung, hind almerekhi,bernard j. jansen.
2020.developing an on-line hate classiﬁer for multiple social media plat-forms.
human-centric computing and informationsciences, 10(1):1–34..mattia samory, indira sen, julian kohne, fabian flock,and claudia wagner.
2020. unsex me here: revisit-ing sexism detection using psychological scales andadversarial samples.
arxiv preprint:2004.12764v1,pages 1–11..robert c nickerson, upkar varshney, and jan munter-mann.
2013. a method for taxonomy developmentand its application in information systems.
euro-pean journal of information systems, 22(3):336–359..maarten sap, dallas card, saadia gabriel, yejin choi,noah a smith, and paul g allen.
2019. the risk ofracial bias in hate speech detection.
in proceed-ings of the 57th annual meeting of the acl, pages1668–1678..1677zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatespeech detection on twitter.
in naacl-hlt, pages88–93..zeerak waseem, james thorne, and joachim bingel.
2018. bridging the gaps: multi task learning fordomain transfer of hate speech detection.
in jennifergolbeck, editor, online harassment, pages 29–55.
springer international publishing, cham..michael wiegand, josef ruppenhofer, and thomaskleinbauer.
2019. detection of abusive language:in naacl-hlt,the problem of biased datasets.
pages 602–608, minneapolis.
acl..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r’emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
arxiv, abs/1910.03771..lucas wright, derek ruths, kelly p dillon, haji mo-hammad saleem, and susan benesch.
2017. vec-in proceedingstors for counterspeech on twitter.
of the first workshop on abusive language online,pages 57–62, vancouver, bc, canada.
associationfor computational linguistics..ellery wulczyn, nithum thain, and lucas dixon.
2017a.
ex machina: personal attacks seen at scale.
in proceedings of the international world wide webconference, pages 1391–1399..ellery wulczyn, nithum thain, and lucas dixon.
2017b.
ex machina: personal attacks seen at scale.
in proceedings of the international world wide webconference, pages 1391–1399..marcos zampieri, shervin malmasi, preslav nakov,sara rosenthal, noura farra, and ritesh kumar.
2019. predicting the type and target of offensivein proceedings of naaclposts in social media.
hlt 2019, volume 1, pages 1415–1420..marcos zampieri, preslav nakov, sara rosenthal, pepaatanasova, georgi karadzhov, hamdy mubarak,leon derczynski, zeses pitenis, and c¸ a˘grı c¸ ¨oltekin.
2020. semeval-2020 task 12: multilingual offen-sive language identiﬁcation in social media (offen-seval 2020).
arxiv preprint, pages 1–23..anna schmidt and michael wiegand.
2017. a sur-vey on hate speech detection using natural lan-guage processing.
in proceedings of the fifth inter-national workshop on natural language processingfor social media, pages 1–10.
association for com-putational linguistics..steve durairaj swamy, anupam jamatia, and bj¨orngamb¨ack.
2019. studying generalisability acrossin conllabusive language detection datasets.
2019 - 23rd conference on computational naturallanguage learning, proceedings of the conference,pages 940–950..bertie vidgen, austin botelho, david broniatowski,ella guest, matthew hall, helen margetts, rebekahtromble, zeerak waseem, and scott hale.
2020.detecting east asian prejudice on social media.
arxiv:2005.03909v1, pages 1–12..bertie vidgen and leon derczynski.
2020. directionsin abusive language training data: garbage in,garbage out.
plos one, pages 1–26..bertie vidgen, alex harris, dong nguyen, rebekahtromble, scott hale, and helen margetts.
2019a.
challenges and frontiers in abusive content detec-tion.
in proceedings of the third workshop on abu-sive language online (acl), pages 80–93..bertie vidgen, helen margetts, and alex harris.
2019b.
how much online abuse is there?
a systematic re-view of evidence for the uk.
the alan turing insti-tute, london..bertie vidgen, dong nguyen, helen margetts, patriciarossini, and rebekah tromble.
2021. introducingcad: the contextual abuse dataset.
in proceedingsof the 2021 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 2289–2303,online.
association for computational linguistics..bertie vidgen and taha yasseri.
2019. detecting weakand strong islamophobic hate speech on social me-dia.
journal of information technology & politics,17(1):66–78..william warner and julia hirschberg.
2012. detectinghate speech on the world wide web.
in proceedingsof the second workshop on language in social me-dia, pages 19–26..zeerak waseem.
2016. are you a racist or am i seeingthings?
annotator inﬂuence on hate speech detectionon twitter.
in proceedings of the first workshop onnlp and computational social science, pages 138–142, austin, texas.
association for computationallinguistics..zeerak waseem, thomas davidson, dana warmsley,and ingmar weber.
2017. understanding abuse: atypology of abusive language detection subtasks.
in proceedings of the first workshop on abusivelanguage online, pages 78–84..1678a list of identities.
see table 5 for a list of identities focused on duringdata collection..b list of pivots in r2.
hate pivots.
convoluted statements statements which ex-press hate through a convoluted sentence structure,e.g.
‘i’ll make sure that x don’t exist anymorebecause that’s what i want to do and i dont care ifnobody knows it”..negation of neutrality in the second clausestatements which initially appear to be neutral (orpositive) about an identity, but then undermine itwith a caveat, e.g.
‘some people say it’s not fair tocall x dumb.
.
.
but 9 times out of 10 it’s true!’..long phrases statements which are long, ofteninvolving multiple clauses and sentences.
in suchcases the hate may use more subtle forms of lan-guage and/or rely more heavily on tone, e.g.
‘italyis a beautiful country and i love it so much, from itsfood to its fashion, from its history to its arts, but iam very worried about the current trends which re-mind me of the 1930s, so let’s import x to changeup the food, fashion, history and arts?’.
rhetorical questions statements which ask arhetorical question in order to make a hateful point.
their goal is to express hate rather than to elicit anactual answer, e.g.
‘aren’t all x penny pinching,after all?’.
(swapped.
characters) wordsmisspellingwhich are intentionally misspelt so that, althoughthe meaning is clear to a human,the modelmisclassiﬁers it because the hate is ‘obfuscated’.5‘n!gg@’, ‘n!gga’, ‘n1gg4”, f*g:e.g.
‘f@g’, ‘f4g’, ‘f@gz’, k*ke: ‘k1ke’, ‘k1k3’..n*gga:.
misspelling (elongation) words which are mis-spelt so that they are far longer, with many repeatedletters, e.g.
‘i think letting gaaaaaaaay people haverights is just soooooooo awful’..misspelling (spaces) words which are misspeltso that there are spaces between letters.
mis-spelling of terms includes: ‘t r a n n y s’, ‘j ew s’, ‘n i g g a’..5note we use * for the ﬁrst vowel in each slur to minimize.
use of harmful language..synonym (hate expressed with infrequentlyused terms) statements which contain rarelyused terms,, e.g.
rather than saying ‘x are scum’they use the phrase, ‘x are all sour misanthropes’.
equally, such statements might just direct unusual /less common forms of abuse at the targeted identity.
infrequently used terms to insult a group include:‘obnoxious’, ‘heinous’, ‘abhorrent’, ‘unlawful’,‘dishonorable’..terms with positive sentiment statementswhich contain positive phrases and terms but stillexpress hate.
in particular, phrases which use theterm ’love’ are often identiﬁed as not hateful, e.g.
‘i just love it when all of the brown people leavemy ofﬁce so i dont have to look at them’..random statement statements which are ’ran-dom’ and do not content any features which arelikely to trick the model..not hate pivots.
counter speech (by another identity) state-ments which challenge, counter or dismiss hate,which are expressed by someone who does notidentify with the group being attacked.
this ofteninvolves referring to or quoting hateful content –which models may mistake for genuine hate..counter speech (by target identity) statementswhich challenge, counter or dismiss hate, whichare expressed by someone who is from the groupbeing attacked..counter speech (use of negation) statementswhich are neutral and state that a group does nothave a (negative or neutral) attribute.
this is pre-sented primarily as a contrast set to negated non-hate (in the hateful pivots), e.g.
‘jews are notmoney lovers’..not hateful use of a term that is also a referentfor an identity (polysemy) statements that men-tion a referent for an identity (in a way that is notmeant to refer to the identity).
for instance, using‘black’ as a colour, e.g.
‘the black cat down theroad really ruins the neighbourhood’..use of profanities statements which contain aprofanity in a not hateful way.
they are often usedas an adjective or adverb to describe an emotionor to place emphasis on what is being said, e.g.
‘fucking hell today was a lot of bullshit’..1679negativity against objects statements which at-tack, criticise or express negativity against inani-mate objects, such as sofa or a car, e.g.
‘this cup istotally rubbish’..personal abuse (direct) statements which areaggressive, insulting or abusive against an individ-ual using a direct personal pronoun (i.e.
‘you’), e.g.
‘you are a complete joke and no-one respects you,loser’..personal abuse (indirect) statements which areaggressive, insulting or abusive against an individ-ual who is not part of the conversation and as suchis referred to with an indirect personal pronoun (i.e.
‘he’, ‘she’, ‘they’), e.g.
‘he is such a waste of space.
i hope he dies’..negativity against concepts statements whichattack, criticise or express negativity against con-cepts and ideologies, such as political ideologies,economic ideas and philosophical ideals, e.g.
‘i’venever trusted capitalism.
it’s bullshit and it fuckssociety over’..negativity against animals statements whichattack, criticise or express negativity against an-imals, e.g.
‘dogs are just beasts, kick them if theyannoy you’..against.
negativityinstitutions statementswhich attack, criticise or express negativityagainst institutions; such as large organisations,governments and bodies, e.g.
‘the nhs is a badlyrun and pointless organisation which is the sourceof so much harm’..negativity against others statements which at-tack, criticise or express negativity against some-thing that is not an identity – and the targets arenot identiﬁed elsewhere in this typology, e.g.
‘theair round here is toxic, it smells like terrible’..c development set performance.
table 6 shows dev set performance numbers..d model, training, and evaluation.
details.
the model architecture was the roberta-base modelfrom huggingface (https://huggingface.co/),with a sequence classiﬁcation head.
this model hasapproximately 125 million parameters.
trainingeach model took no longer than approximately a.day, on average, with 8 gpus on the fair clus-ter.
all models were trained with a learning rateof 2e-5 with the default optimizer that hugging-face’s sequence classiﬁcation routine uses.
targetmodel hyperparameter search was as follows: ther2 target was trained for 3 epochs on the r1 targettraining data, plus multiples of the round 1 datafrom {1, 5, 10, 20, 40, 100} (the best was 5).
ther3 target was trained for 3 epochs on the r2 tar-get training data, plus multiples of the round 2data from {1, 5, 10, 20, 40, 100} (the best was100).
the r4 target was trained on the r3 targettraining data for 4 epochs, plus multiples of theround 3 data from {1, 5, 10, 20, 40, 100, 200} (thebest was 1); early stopping based on loss on thedev set (measured multiple times per epoch) wasperformed.
the dev set we used for tuning targetmodels was the latest dev set we had at each round.
we did not perform hyperparameter search on thenon-target models, with the exception of training5 seeds of each and early stopping based on devset loss throughout 4 training epochs.
we recallthat model performance typically did not vary bymuch more than 5% through our hyperparametersearches..e data statement.
following bender and friedman (2018) we providea data statement, which documents the process andprovenance of the ﬁnal dataset..a. curation rationale in order to studythe potential of dynamically generated datasets forimproving online hate detection, we used an onlineinterface to generate a large-scale synthetic datasetof 40,000 entries, collected over 4 rounds, witha ‘model-in-the-loop’ design.
data was not sam-pled.
instead a team of trained annotators createdsynthetic content to enter into the interface..b. language variety all of the contentis in english.
we opted for english language dueto the available annotation team, and resources andthe project leaders’ expertise.
the system that wedeveloped could, in principle, be applied to otherlanguages..c. speaker demographics due to thesynthetic nature of the dataset, the speakers are thesame as the annotators..d. annotator demographics anno-tator demographics are reported in the paper, and.
1680are reproduced here for fullness.
annotation guide-lines were created at the start of the project andthen updated after each round.
annotations guide-lines will be publicly released with the dataset.
wefollowed the guidance for protecting and monitor-ing annotator well-being provided by vidgen et al.
(2019a).
20 annotators were recruited.
ten were re-cruited to work for 12 weeks and ten were recruitedfor the ﬁnal four weeks.
annotators completed dif-ferent amounts of work depending on their avail-ability, which is recorded in the dataset..all annotators attended a project onboarding ses-sion, half day training session, at least one one-to-one session and a daily ’standup’ meeting whenworking.
they were given a test assignment andguidelines to review before starting work and re-ceived feedback after each round.
annotators couldask the experts questions in real-time over a mes-saging platform..of the 20 annotators, 35% were male and 65%were female.
65% were 18-29 and 35% were 30-39.
10% were educated to high school level, 20%to undergraduate, 45% to taught masters and 25%to research degree (i.e.
phd).
70% were native en-glish speakers and 30% were non-native but ﬂuent.
respondents had a range of nationalities, includ-ing british (60%), as well as polish, spanish andiraqi.
most annotators identiﬁed as ethnically white(70%), followed by middle eastern (20%) and twoor more ethnicities (10%).
participants all usedsocial media regularly, including 75% who used itmore than once per day.
all participants had seenother people targeted by online abuse before, and55% had been targeted personally..e. speech situation all data was createdfrom 21st september 2020 until 14th january 2021.during the project annotators visited a range ofonline platforms, with adequate care and supervi-sion from the project leaders, to better understandonline hate as it appears online..f. text characteristics the composi-tion of the dataset, including the distribution of theprimary label (hate and not) and the type (deroga-tion, animosity, threatening, support, dehuman-ization and none given) is described in the paper..category of identity.
disability.
gender.
gendergenderimmigration statusimmigration statusimmigration statusimmigration statusrace / ethnicityrace / ethnicity.
race / ethnicity.
race / ethnicity.
race / ethnicity.
race / ethnicity.
race / ethnicity.
race / ethnicityrace / ethnicityrace / ethnicity.
race / ethnicity.
race / ethnicityreligion or beliefreligion or beliefsexual orientationsexual orientationsexual orientationnational originreligion or beliefclass.
race / ethnicity.
intersectionalintersectionalintersectionalintersectionalintersectional.
identitypeople withdisabilitiesgender minorities(e.g.
non binary)womentransimmigrantsforeignerrefugeeasylum seekerblack peopleindigenouseast asians(e.g.
china)east asians(e.g.
korea)south east asians(e.g.
india)pakistanisaboriginal people(e.g.
nativeamericans)mixed raceminority groupsarabstravellers(e.g.
roma)people from africamuslimsjewsgaylesbianbisexualpolishhindusworking classhispanic(e.g.
latinx)black womenblack menindigenous womenasian womenmuslim women.
table 5: list of high priority identities.
1681model.
m1 (r1 target)m2 (r2 target)m3 (r3 target)m4 (r4 target).
m(r1)m(r2)m(r3)m(r4).
m(r0+r1)m(r0+r1+r2)m(r0+r1+r2+r3)m(r0+r1+r2+r3+r4).
r1.
r2.
r3.
r4.
41.4±0.9195.38±0.2594.55±0.6594.92±0.45.
95.69±0.2981.28±0.276.79±1.1878.05±1.09.
93.92±0.393.13±0.2493.43±0.3992.73±0.82.
61.06±0.4368.86±0.7185.04±0.6385.32±0.29.
61.88±0.9884.36±0.479.6±0.9980.21±0.31.
69.43±1.5882.82±0.884.66±0.686.0±0.69.
58.18±0.6966.46±1.0976.77±0.5777.52±0.68.
57.75±0.875.8±0.5575.5±0.4875.63±0.49.
65.48±0.4873.66±0.7575.81±0.2977.0±0.59.
55.46±0.6363.17±0.874.4±0.976.42±0.82.
58.54±0.5274.29±1.0574.19±1.0772.54±0.64.
63.99±0.7472.28±0.8475.85±1.075.7±0.69.
table 6: macro f1 with standard deviation over 5 training rounds, evaluated on each rounds’ dev set.
early-stopping is performed on the latest development set for each round where dev results are obtained at least once perepoch, out of four epochs..1682