adapting high-resource nmt models to translate low-resource relatedlanguages without parallel data.
wei-jen ko1∗, ahmed el-kishky2∗, adithya renduchintala3, vishrav chaudhary3,naman goyal3, francisco guzm´an3, pascale fung4, philipp koehn5, mona diab31university of texas at austin, 2twitter cortex, 3facebook ai4the hong kong university of science and technology, 5johns hopkins universitywjko@utexas.edu, aelkishky@twitter.com{adirendu,vishrav,naman,fguzman,mdiab}@fb.compascale@ece.ust.hk, phi@jhu.edu.
abstract.
the scarcity of parallel data is a major obsta-cle for training high-quality machine transla-tion systems for low-resource languages.
for-tunately, some low-resource languages are lin-guistically related or similar to high-resourcelanguages; these related languages may shareinmany lexical or syntactic structures.
this work, we exploit this linguistic overlapto facilitate translating to and from a low-resource language with only monolingual data,in addition to any parallel data in the re-lated high-resource language.
our method,nmt-adapt, combines denoising autoencod-ing, back-translation and adversarial objec-tives to utilize monolingual data for low-resource adaptation.
we experiment on 7 lan-guages from three different language familiesand show that our technique signiﬁcantly im-proves translation into low-resource languagecompared to other translation baselines..1.introduction.
while machine translation (mt) has made incredi-ble strides due to the advent of deep neural machinetranslation (nmt) (sutskever et al., 2014; bah-danau et al., 2014) models, this improvement hasbeen shown to be primarily in well-resourced lan-guages with large available parallel training data..however with the growth of internet commu-nication and the rise of social media, individualsworldwide have begun communicating and produc-ing content in their native low-resource languages.
many of these low-resource languages are closelyrelated to a high-resource language.
one such ex-ample are “dialects”: variants of a language tradi-tionally considered oral rather than written.
ma-chine translating dialects using models trained on.
the formal variant of a language (typically the high-resource variant which is sometimes considered the“standardized form”) can pose a challenge due tothe prevalence of non standardized spelling as wellsigniﬁcant slang vocabulary in the dialectal variant.
similar issues arise from translating a low-resourcelanguage using a related high-resource model (e.g.,translating catalan with a spanish mt model)..while an intuitive approach to better translatinglow-resource related languages could be to obtainhigh-quality parallel data.
this approach is ofteninfeasible due to lack specialized expertise or bilin-gual translators.
the problems are exacerbated byissues that arise in quality control for low-resourcelanguages (guzm´an et al., 2019).
this scarcitymotivates our task of learning machine translationmodels for low-resource languages while leverag-ing readily available data such as parallel data froma closely related language or monolingual data inthe low-resource language.1.
the use of monolingual data when little to noparallel data is available has been investigated formachine translation.
a few approaches involvesynthesising more parallel data from monolingualdata using backtranslation (sennrich et al., 2015)or mining parallel data from large multilingual cor-pora (tran et al., 2020; el-kishky et al., 2020b,a;schwenk et al., 2019).
we introduce nmt-adapt, azero resource technique that does not need paralleldata of any kind on the low resource language..we investigate the performance of nmt-adaptat translating two directions for each low-resourcelanguage: (1) low-resource to english and (2) en-glish to low-resource.
we claim that translatinginto english can be formulated as a typical unsu-pervised domain adaptation task, with the high-resource language as the source domain and the.
∗this work was conducted while author was working at.
1we use low-resource language and dialect or variant in-.
facebook ai.
terchangeably..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages802–812august1–6,2021.©2021associationforcomputationallinguistics802related low-resource, the target domain.
we thenshow that adversarial domain adaptation can beapplied to this related language translation task.
for the second scenario, translating into the low-resource language, the task is more challengingas it involves unsupervised adaptation of the gen-erated output to a new domain.
to approach thistask, nmt-adapt jointly optimizes four tasks toperform low-resource translation: (1) denoising au-toencoder (2) adversarial training (3) high-resourcetranslation and (4) low-resource backtranslation..we test our proposed method and demonstrate itseffectiveness in improving low-resource translationfrom three distinct families: (1) iberian languages,(2) indic languages, and (3) semitic languages,speciﬁcally arabic dialects.
we make our code andresources publicly available.2.
2 related work.
zero-shot translation our work is closely re-lated to that of zero-shot translation (johnson et al.,2017; chen et al., 2017; al-shedivat and parikh,2019).
however, while zero-shot translation trans-lates between a language pair with no parallel data,there is an assumption that both languages in thetarget pair have some parallel data with other lan-guages.
as such, the system can learn to processboth languages.
in one work, currey and heaﬁeld(2019) improved zero-shot translation using mono-lingual data on the pivot language.
however, inour scenario, there is no parallel data between thelow-resource language and any other language.
inother work, arivazhagan et al.
(2019) showed thatadding adversarial training to the encoder outputcould help zero shot training.
we adopt a similarphilosophy in our multi-task training to ensure ourlow-resource target is in the same latent space asthe higher-resource language..unsupervised translation a related set of workis the family of unsupervised translation tech-niques; these approaches translate between lan-guage pairs with no parallel corpus of any kind.
inwork by artetxe et al.
(2018); lample et al.
(2018a),unsupervised translation is performed by trainingdenoising autoencoding and backtranslation tasksconcurrently.
in these approaches, multiple pre-training methods were proposed to better initializethe model (lample et al., 2018b; lample and con-neau, 2019; liu et al., 2020; song et al., 2019)..2https://github.com/wjko2/nmt-adapt.
different approaches were proposed that usedparallel data between x-y to improve unsupervisedtranslation between x-z (garcia et al., 2020a; liet al., 2020; wang et al., 2020).
this scenario dif-fers from our setting as it does not assume that yand z are similar languages.
these approachesleverage a cross-translation method on a multilin-gual nmt model where for a parallel data pair(sx,sy), they translate sx into language z with thecurrent model to get s(cid:48)z) as anadditional synthesized data pair to further improvethe model.
garcia et al.
(2020b) experiment usingmultilingual cross-translation on low-resource lan-guages with some success.
while these approachesview the parallel data as auxiliary, to supplementunsupervised nmt, our work looks at the problemfrom a domain adaptation perspective.
we attemptto use monolingual data in z to make the super-vised model trained on x-y generalize to z..z. then use (sy,s(cid:48).
low-resource.
leveraging high-resource languages to im-provetranslation severalworks have leveraged data in high-resourcelanguages to improve the translation of similarlow-resource languages.
neubig and hu (2018)showed that it is beneﬁcial to mix the limitedparallel data pairs of low-resource languages withhigh-resource language data.
lakew et al.
(2019)proposed selecting high-resource language datawith lower perplexity in the low-resource languagemodel.
xia et al.
(2019) created synthetic sentencepairs by unsupervised machine translation, usingthe high-resource language as a pivot.
howeverthese previous approaches emphasize translatingfrom the low-resource language to english, whilethe opposite direction is either unconsidered orshows poor translation performance.
siddhantet al.
translationand denoising simultaneously, and showed thatthe model could translate languages withoutparallel data into english near the performance ofsupervised multilingual nmt..trained multilingual.
(2020).
similar language translation similar to ourwork, there have been methods proposed that lever-age similar languages to improve translation.
has-san et al.
(2017) generated synthetic english-dialectparallel data from english-main language corpus.
however, this method assumes that the vocabularyin the main language could be mapped word byword into the dialect vocabulary, and they calcu-late the corresponding word for substitution using.
803localized projection.
this approach differs fromour work in that it relies on the existence of a seedbilingual lexicon to the dialect/similar language.
additionally, the approach only considers translat-ing from a dialect to english and not the reversedirection.
other work trains a massively multilin-gual many-to-many model and demonstrates thathigh-resource training data improves related low-resource language translation (fan et al., 2020).
inother work, lakew et al.
(2018) compared waysto model translations of different language vari-eties, in the setting that parallel data for both va-rieties is available, the variety for some pairs maynot be labeled.
another line of work focus ontranslating between similar languages.
in one suchwork, pourdamghani and knight (2017) learned acharacter-based cipher model.
in other work, wanet al.
(2020) improved unsupervised translationbetween the main language and the dialect by sepa-rating the token embeddings into pivot and privateparts while performing layer coordination..3 method.
we describe the nmt-adapt approach to translat-ing a low-resource language into and out of en-glish without utilizing any low-resource languagein section 3.1, we describe howparallel data.
nmt-adapt leverages a novel multi-task domainadaptation approach to translating english into alow-resource language.
in section 3.2, we then de-scribe how we perform source-domain adaptationto translate a low-resource language into english.
finally, in section 3.3, we demonstrate how wecan leverage these two domain adaptations, to per-form iterative backtranslation – further improvingtranslation quality in both directions..3.1 english to low-resource.
to translate from english into a low-resource lan-guage, nmt-adapt is initialized with a pretrainedmbart model whose pretraining is describedin (liu et al., 2020).
then, as shown in figure 1,we continue to train the model simultaneously withfour tasks inspired by (lample et al., 2018a) andupdate the model with a weighted sum of the gra-dients from different tasks..the language identifying tokens are placed atthe same position as in mbart.
for the encoder,both high and low-resource language source text,with and without noise, use the language tokenof the high-resource language [hrl] in the pre-.
trained mbart.
for the decoder, the related highand low-resource languages use their own, differ-ent, language tokens.
we initialize the language to-ken embedding of the low-resource language withthe embedding from the high-resource languagetoken.
task 1: translation the ﬁrst task is transla-tion from english into the high-resource language(hrl) which is trained using readily available high-resource parallel data.
this task aims to transferhigh-resource translation knowledge to aid in trans-lating into the low-resource language.
we use thecross entropy loss formulated as follows:.
lt = lce(d(zen, [hrl]), xhrl).
(1).
, where zen = e(xen, [en]).
(xen, xhrl) isa parallel sentence pair.
e, d denotes the encoderand decoder functions, which take (input, languagetoken) as parameters.
lce denotes the cross en-tropy loss..task 2: denoising autoencoding for this task,we leverage monolingual text by introducing noiseto each sentence, feeding the noised sentence intothe encoder, and training the model to generate theoriginal sentence.
the noise we use is similar to(lample et al., 2018a), which includes a randomshufﬂing and masking of words.
the shufﬂing is arandom permutation of words, where the positionof words is constrained to shift at most 3 wordsfrom the original position.
each word is maskedwith a uniform probability of 0.1. this task aimsto learn a feature space for the languages, so thatthe encoder and decoder could transform betweenthe features and the sentences.
this is especiallynecessary for the low-resource language if it isnot already pretrained in mbart.
adding noisewas shown to be crucial to translation performancein (lample et al., 2018a), as it forces the learnedfeature space to be more robust and contain high-level semantic knowledge..we train the denoising autoencoding on both thelow-resource and related high-resource languagesand compute the loss as follows:.
lda =.
(cid:88).
i=lrl,hrl.
lce(d(zi, [i]), xi).
(2).
, where zi = e(n (xi), [hrl]).
xi is from themonolingual corpus..task 3: backtranslation for this task, we train on.
804figure 1: illustration of the training tasks for translating from english into a low-resource language (lrl) andfrom an lrl to english..english to low-resource backtranslation data.
theaim of this task is to capture a language-modelingeffect in the low-resource language.
we describehow we obtain this data using the high-resourcetranslation model to bootstrap backtranslation insection 3.3..the objective used is,.
lbt = lce(d(z(cid:48).
en, [lrl]), xlrl).
(3).
, where z(cid:48)english to low-resource backtranslation pair..en = e(yen, [en]).
(yen, xlrl) is an.
task 4: adversarial training the ﬁnal task aimsto make the encoder output language-agnostic fea-tures.
the representation is language agnostic tothe noised high and low-resource languages as wellas english.
ideally, the encoder output should con-tain the semantic information of the sentence andlittle to no language-speciﬁc information.
thisway, any knowledge learned from the english tohigh-resource parallel data can be directly appliedto generating the low-resource language by sim-ply switching the language token during inference,without capturing spurious correlations (gu et al.,2019a)..to adversarially mix the latent space of the en-coder among the three languages, we use two critics.
(discriminators).
the critics are recurrent networksto ensure that they can handle variable-length textinput.
similar to gu et al.
(2019b), the adversar-ial component is trained using a wasserstein loss,which is the difference of expectations between thetwo types of data.
this loss minimizes the earthmover’s distance between the distributions of dif-ferent languages.
we compute the loss function asfollows:.
ladv1 = e[disc(zhrl)] − e[disc(zlrl)] (4).
ladv2 = e[disc(zhrl ∪ zlrl)].
−e[disc(zen ∪ z(cid:48).
en)].
(5).
as shown in equation 4, the ﬁrst critic is trained todistinguish between the high and low-resource lan-guages.
similarly, in equation 5, the second criticis trained to distinguish between english and non-english (both high, and low-resource languages)..fine-tuning with backtranslation: finally, wefound that after training with the four tasks con-currently, it is beneﬁcial to ﬁne-tune solely usingbacktranslation for one pass before inference.
weposit that this is because while spurious correla-tions are reduced by the adversarial training, theyare not completely eliminated and using solely the.
805encoderdecoderො𝑥𝐸𝑛𝑥hrl𝑧hrl𝑧𝐻𝑅𝐿,𝑧𝐿𝑅𝐿discriminatorencoder𝐻𝑅𝐿𝐿𝑅𝐿𝑥𝐸𝑛encoderdecoderො𝑥𝐸𝑛𝑧′lrlreverse model𝑦𝐿𝑅𝐿𝑥𝐸𝑛𝐿𝐶𝐸𝑥𝐸𝑛𝐿𝐶𝐸𝐿𝐴𝑑𝑣𝑥𝐻𝑅𝐿,𝑥𝐿𝑅𝐿encoderdecoderො𝑥𝐻𝑅𝐿𝑥en1) translation𝑧en𝑥𝐿𝑅𝐿encoderdecoderො𝑥𝐿𝑅𝐿𝑧lrlnoise𝑥𝐻𝑅𝐿encoderdecoderො𝑥𝐻𝑅𝐿𝑧hrlnoise2) denoisingautoencoder𝑥𝐿𝑅𝐿encoderdecoderො𝑥𝐿𝑅𝐿𝑧′enreverse model3) backtranslation𝑦𝐸𝑛𝑥𝐻𝑅𝐿𝐿𝐶𝐸𝑥𝐿𝑅𝐿𝐿𝐶𝐸𝑥𝐻𝑅𝐿𝐿𝐶𝐸𝑥𝐿𝑅𝐿𝐿𝐶𝐸4) adversarial training𝑧𝐻𝑅𝐿,𝑧𝐿𝑅𝐿discriminator 1encoder𝐻𝑅𝐿𝐿𝑅𝐿discriminator 2𝐻𝑅𝐿,𝐿𝑅𝐿𝐸𝑛𝐿𝐴𝑑𝑣𝐿𝐴𝑑𝑣𝑧𝐻𝑅𝐿,𝑧𝐿𝑅𝐿,𝑧𝐸𝑛,𝑧′𝐸𝑛encoderenglish→lrllrl → english1) translation3) adversarial training2) backtranslationlanguage tokens to control the output languageis not sufﬁcient.
by ﬁne-tuning on backtransla-tion, we are further adapting to the target side andencouraging the output probability distribution ofthe decoder to better match the desired output lan-guage..3.2 low-resource to english.
we propose to model translating from the low-resource language to english as a domain adap-tation task and design our model based on insightsfrom domain-adversarial neural network (dann)(ganin et al., 2017), a domain adaptation techniquewidely used in many nlp tasks.
this time, wetrain three tasks simultaneously:.
task 1: translation we train high-resource to en-glish translation on parallel data with the goal ofadapting this knowledge to translate low-resourcesentences.
we compute this loss as follows:.
lt = lce(d(zhrl, [en]), xen).
(6).
, where zhrl = e(xhrl, [hrl])..task 2: backtranslation low-resource to englishbacktranslation translation, which we describe insection 3.3. the objective is as follows:.
lt = lce(d(z(cid:48).
lrl, [en]), xen).
(7).
, where z(cid:48).
lrl = e(ylrl, [hrl])..task 3: adversarial training we feed the sen-tences from the monolingual corpora of the high-and low-resource corpora into the encoder, and theencoder output is trained so that its input languagecannot be distinguished by a critic.
the goal is toencode the low-resource data into a shared spacewith the high-resource, so that the decoder trainedon the translation task can be directly used.
nonoise was added to the input, since we did not ob-serve an improvement.
there is only one recurrentcritic, which uses the wasserstein loss and is com-puted as follows:.
ladv = e[disc(zhrl)] − e[disc(zlrl)] (8).
, where zlrl = e(xlrl, [hrl])..similar to the reverse direction, we initializenmt-adapt with a pretrained mbart, and use thesame language token for high-resource and low-resource in the encoder..3.3.iterative training.
we describe how we can alternate training into/out-of english models to create better backtranslationdata improving overall quality..k−1.
(xmono).
← train hrl to en model.
// generate backtranslation pairscomputem lrl→en.
algorithm 1 iterative training1: m lrl→en02: xmono ← monolingual lrl corpus3: xen ← english sentences in the en-hrl parallel corpus4: for k in 1,2... do5:6:7:8:9:10:11:12:13:14:15:16:17:.
// generate backtranslation pairscomputem en→lrlk.// train model as in sec 3.1m en→lrl.
// train model as in sec 3.2m lrl→en.
← trained en to lrl model.
← trained lrl to en model.
if converged then break;.
(xen).
k.k.the iterative training process is described in al-gorithm 1. we ﬁrst create english to low-resourcebacktranslation data by ﬁne-tuning mbart on thehigh-resource to english parallel data.
using thismodel, we translate monolingual low-resource textinto english treating the low-resource sentences asif they were in the high-resource language.
theresulting sentence pairs are used as backtranslationdata to train the ﬁrst iteration of our english tolow-resource model..after training english to low-resource, we usethe model to translate the english sentences in theenglish-hrl parallel data into the low-resourcelanguage, and use those sentence pairs as back-translation data to train the ﬁrst iteration of ourlow-resource to english model..we then use the ﬁrst low-resource to englishmodel to generate backtranslation pairs for the sec-ond english to low-resource model.
we iterativelyrepeat this process of using our model of one direc-tion to improve the other direction..4 experiments.
4.1 datasets.
we experiment on three groups of languages.
ineach group, we have a large quantity of paralleltraining data for one language(high-resource) andno parallel for the related languages to simulate alow-resource scenario..our three groupings include (i) iberian lan-guages, where we treat spanish as the high-.
806language.
group training set.
train-size test set.
test-size monolingual mono-size.
spanishcatalanportuguese.
iberian qed (guzman et al., 2013)iberian n/aiberian n/a.
hindimarathinepaliurdu.
indicindicindicindic.
iit bombay (kunchukuttan et al., 2018)n/an/an/a.
- global voices (tiedemann, 2012)- ted (qi et al., 2018).
694k n/a.
769k n/a.
- tico-19 (anastasopoulos et al., 2020)- flores (guzm´an et al., 2019)- tico-19 (anastasopoulos et al., 2020).
arabic qed (guzman et al., 2013).
465k n/a.
msaegyptian ar.
arabic n/alevantine ar.
arabic n/a.
- forum (chen et al., 2018)- web text (raytheon, 2012).
-15k8k.
-2k3k2k.
-11k11k.
cc-100cc-100cc-100.
cc-100cc-100cc-100cc-100.
cc-100cc-100cc-100.
1m1m1m.
1m1m1m1m.
1m1.2m1m.
table 1: the sources and size of the datasets we use for each language.
the hrls are used for training and thelrls are used for testing..resource and portuguese and catalan as related(ii) indic languageslower-resource languages.
where we treat hindi as the high-resource language,and marathi, nepali, and urdu as lower-resourcerelated languages (iii) arabic, where we treat mod-ern standard arabic (msa) as the high-resource,and egyptian and levantine arabic dialects as low-resource.
among the languages, the relationshipbetween urdu and hindi is a special setting; whilethe two languages are mutually intelligible as spo-ken languages, they are written using differentscripts.
additionally, in our experimental setting,all low-resource languages except for nepali werenot included in the original mbart pretraining..the parallel corpus for each language is de-scribed in table 1. due to the scarcity of any paral-lel data for a few low-resource languages, we arenot able to match the training and testing domains.
for monolingual data, we randomly sample 1msentences for each language from the cc-100 cor-pus3 (conneau et al., 2020; wenzek et al., 2020).
for quality control, we ﬁlter out sentences if morethan 40% of characters in the sentence do not be-long to the alphabet set of the language.
for qualityand memory constraints, we only use sentenceswith length between 30 and 200 characters..collecting dialectical arabic data while obtain-ing low-resource monolingual data is relativelystraightforward, as language identiﬁers are oftenreadily available for even low-resource text (jauhi-ainen et al., 2019), identifying dialectical data isoften less straightforward.
this is because many di-alects have been traditionally considered oral ratherthan written, and often lack standardized spelling,signiﬁcant slang, or even lack of mutual intelligibil-ity from the main language.
in general, dialecticaldata has often been grouped in with the main lan-.
3http://data.statmt.org/cc-100/.
guage in language classiﬁers..we describe the steps we took to obtain reliabledialectical arabic monolingual data.
as the cc-100 corpus does not distinguish between modernstandard arabic (msa) and its dialectical variants,we train a ﬁner-grained classiﬁer that distinguishesbetween msa and speciﬁc colloquial dialects.
webase our language classiﬁer on a bert model pre-trained for arabic (safaya et al., 2020) and ﬁne-tune it for six-way classiﬁcation: (i) egyptian, (ii)levantine, (iii) gulf, (iv) maghrebi, (v) iraqi di-alects as well as (vi) the literary modern standardarabic (msa).
we use the data from (bouamoret al., 2018) and (zaidan and callison-burch, 2011)as training data, and the resulting classiﬁer has anaccuracy of 91% on a held-out set.
we take ourtrained arabic dialect classiﬁer and further classifyarabic monolingual data from cc-100 and selectmsa, levantine and egyptian sentences as arabicmonolingual data for our experiments..4.2 training details.
we use the rmsprop optimizer with learning rate0.01 for the critics and the adam optimizer for therest of the model.
we train our model using eightgpus and a batch size of 1024 tokens per gpu.
weupdate the parameters once per eight batches.
forthe adversarial task, the generator is trained onceper three updates, and the critic is trained everyupdate..each of the tasks of (i) translation, (ii) backtrans-lation as well as (iii) lrl and hrl denoising (onlyfor en→lrl direction), have the same numberof samples and their cross entropy loss has equalweight.
the adversarial loss, ladv, has the sameweight on the critic, while it has a multiplier of−60 on the generator (encoder).
this multiplierwas tuned to ensure convergence and is negative asit’s opposite to the discriminator loss..for the ﬁrst iteration, we train 128 epochs from.
807en→ lrl.
un-adapted model.
adapted models.
lrl.
hrl.
en→hrl adv.
bt bt+adv bt+adv+ﬁne-tune.
spanishportuguesespanishcatalanhindimarathihindinepaliurduhindiegyptian arabic msalevantine arabic msa.
3.86.87.311.20.33.52.1.
10.19.18.417.63.43.82.1.
14.821.29.516.70.28.04.8.
18.022.515.625.37.28.05.1.
21.223.616.126.3-8.04.7.table 2: bleu score of the ﬁrst iteration on the english to low-resource direction.
both the adversarial (adv) andbacktranslation (bt) components contribute to improving the results.
the ﬁne-tuning step is omitted for urdu asdecoding is already restricted to a different script-set from the related high-resource language..lrl→en.
lrl.
un-adapted model.
adapted models.
hrl.
hrl→en adv.
bt bt+adv.
spanishportuguesespanishcatalanhindimarathihindinepaliurduhindiegyptian arabic msalevantine arabic msa.
12.312.23.914.80.314.99.3.
21.713.97.016.91.014.06.7.
32.725.38.114.110.515.29.3.
36.024.612.718.210.515.89.0.table 3: bleu score of the ﬁrst iteration on the lrl to english direction.
both the adversarial(adv) and back-translation (bt) components contribute to improving the results..english to the low-resource language and 64 itera-tions from low-resource language to english.
forthe second iteration we train 55 epochs for both di-rections.
we follow the setting of (liu et al., 2020)for all other settings and training parameters..the critics consist of four layers: the third layeris a bidirectional gru and the remaining three arefully connected layers.
the hidden layer sizes are512, 512 and 128 and we use an selu activationfunction..we ran experiments on 8-gpus.
each iterationtook less than 3 days and we used publicly availablembart-checkpoints for initialization.
gpu mem-ory usage of our method is only slightly larger thanmbart.
while we introduce additional parametersin discriminators, these additional parameters areinsigniﬁcant compared to the size of the mbartmodel..4.3 results.
we present results of applying nmt-adapt to low-resource language translation..4.3.1 english to low-resource.
we ﬁrst evaluate performance of translating intothe low-resource language.
we compare the ﬁrstiteration of nmt-adapt to the following baselinesystems: (i) en→hrl model: directly using themodel trained for en→hrl translation.
(ii) adver-sarial: our full model without using the backtrans-lation objective and without the ﬁnal ﬁne-tuning..(iii) backtranslation: mbart ﬁne-tuned on back-translation data created using the hrl→en model.
(iv) bt+adv: our full model without the ﬁnal ﬁne-tuning.
(v) bt+adv+ﬁne-tune: our full model(nmt-adapt) as described in section 3..as seen in table 2, using solely the adversarialcomponent only, we generally see improvementin the bleu scores over using the high-resourcetranslate model.
this suggests that our proposedmethod of combining denoising autoencoding withadversarial loss is effective in adapting to a newtarget output domain..additionally, we observe a large improvementusing only backtranslation data.
this demonstratesthat using the high-resource translation model tocreate lrl-en backtranslation data is highly effec-tive for adapting to the low-resource target..we further see that combining adversarial andbacktranslation tasks further improve over each in-dividually, showing that the two components arecomplementary.
we also experimented on en-hrltranslation with backtranslation but without adver-sarial loss.
however, this yielded much worse re-sults, showing that the improvement is not simplydue to multitask learning..for arabic, backtranslation provides most of thegain, while for portuguese and nepali, the adver-sarial component is more important.
for somelanguages like marathi, the two components pro-vides small gains individually, but shows a large.
808improvement while combined..for urdu, we found that backtranslation onlyusing the hindi model completely fails; this is in-tuitive as hindi and urdu are in completely dif-ferent scripts and using a hindi model to translateurdu results in effectively random backtranslationdata.
when we attempt to apply models trainedwith the adversarial task, the model generates sen-tences with mixed hindi, urdu, and english.
toensure our model solely outputs urdu, we restrictedthe output tokens by banning all tokens containingenglish or devanagari (hindi) characters.
this al-lowed our model to output valid and semanticallymeaningful translations.
this is an interesting re-sult as it shows that our adversarial mixing allowstranslating similar languages even if they’re writ-ten in different scripts.
we report the bleu scorewith the restriction.
since the tokens are alreadyrestricted, we skip the ﬁnal ﬁne-tuning step..4.3.2 low-resource to english.
table 3 shows the results of the ﬁrst iterationfrom translating from a low-resource language intoenglish.
we compare the following systems (i)hrl→en model: directly using the model trainedfor hrl→en translation.
(ii) adversarial: similarto our full model, but without using the backtransla-tion objective.
(iii) backtranslation: mbart ﬁne-tuned on backtranslation data from our full modelin the english-lrl direction.
(iv) bt+adv: ourfull model..for this direction, we can see that both the back-translation and the adversarial domain adaptationcomponents are generally effective.
the exceptionis arabic which may be due to noisiness of ourdialect classiﬁcation compared to low-resource lan-guage classiﬁcation.
another reason could be dueto the lack of written standardization for spokendialects in comparison to low-resource, but stan-dardized languages..for these experiments, we did not apply any spe-cial precautions for urdu on this direction despiteit being in a different script from hindi..4.3.3.iterative training.
table 4 shows the results of two iterations of train-ing.
for languages other than arabic dialects, thesecond iteration generally shows improvement overthe ﬁrst iteration, showing that we can leverage animproved model in one direction to further improvethe reverse direction.
we found that the improve-ment after the third iteration is marginal..we compare our results with a baseline usingthe hrl language as a pivot.
the baseline uses aﬁne tuned mbart (liu et al., 2020) to perform su-pervised translation between english and the hrl,and uses mass (song et al., 2019) to perform un-supervised translation between the hrl and thelrl.
the mbart is tuned on the same paralleldata used in our method, and the mass uses thesame monolingual data as in our method.
for alllanguages and directions, our method signiﬁcantlyoutperforms the pivot baseline..4.3.4 comparison with other methodsin table 5, we compare a cross translation methodusing parallel corpora with multiple languages asauxiliary data (garcia et al., 2020b) as well as re-sults reported in (guzm´an et al., 2019) and (liuet al., 2020).
all methods use the same test set,english-hindi parallel corpus, and tokenizationfor fair comparison.
for english to nepali, nmt-adapt outperforms previous unsupervised methodsusing hindi or multilingual parallel data, and iscompetitive with supervised methods.
for nepalito english direction, our method achieves simi-lar performance to previous unsupervised methods.
note that we use a different tokenization than intable 3 and 4, to be consistent with previous work..4.3.5 monolingual data ablationtable 6 shows the ﬁrst iteration english to marathiresults while varying the amount of monolingualdata used.
we see that the bleu score increasedfrom 11.3 to 16.1 as the number of sentences in-creased from 10k to 1m showing additional mono-lingual data signiﬁcantly improves performance..5 conclusion.
we presented nmt-adapt, a novel approach forneural machine translation of low-resource lan-guages which assumes zero parallel data or bilin-gual lexicon in the low-resource language.
utiliz-ing parallel data in a similar high resource languageas well as monolingual data in the low-resourcelanguage, we apply unsupervised adaptation to fa-cilitate translation to and from the low-resourcelanguage.
our approach combines several tasksincluding adversarial training, denoising languagemodeling, and iterative back translation to facili-tate the adaptation.
experiments demonstrate thatthis combination is more effective than any taskon its own and generalizes across many differentlanguage groups..809language.
nmt-adapt it.1 nmt-adapt it.2 mbart+mass nmt-adapt it.1 nmt-adapt it.2 mbart+mass.
english→lrl.
lrl→english.
portuguese21.2catalan23.6marathi16.126.3nepaliurdu7.28.0egyptian ar.
levantine ar.
5.1.
30.727.219.226.314.66.64.5.
26.623.313.111.95.13.31.9.
36.024.612.718.210.515.89.0.
39.827.715.018.813.6--.
38.122.95.82.14.911.76.0.table 4: bleu results of iterative training.
the second iteration generally improves among the ﬁrst iteration, andnmt-adapt outperforms the mbart+mass baseline.
for arabic, as iteration 2 into arabic was worse thaniteration 1, we omit the corresponding iteration 2 into english..bleu.
en→ne ne→en.
learning to align and translate.
arxiv:1409.0473..arxiv preprint.
unsupervised+hi parallel.
unsupervised+multi.
parallel.
nmt-adapt(guzm´an et al., 2019)(liu et al., 2020).
9.28.3-.
(garcia et al., 2020b).
8.9.sup.
with hi.
(guzm´an et al., 2019)(liu et al., 2020).
8.89.6.sup.
w/o hi.
(guzm´an et al., 2019).
4.3.
18.818.817.9.
21.7.
21.521.3.
7.6.table 5: comparison with previous work on floresdataset.
nmt-adapt outperforms previous unsuper-vised methods on en→ne, and achieves similar perfor-mance to unsupervised baselines on ne→en..# sentences bleu.
10k100k1m.
11.314.116.1.table 6: first iteration english to marathi results withvariable amount of monolingual data..references.
maruan al-shedivat and ankur p. parikh.
2019. con-sistency by agreement in zero-shot neural machinetranslation.
in naacl..antonios anastasopoulos, alessandro cattelan, zi-yi dou, marcello federico, christian federman,dmitriy genzel, francisco guzm´an, junjie hu, mac-duff hughes, philipp koehn, rosie lazar, willlewis, mengmeng niu graham neubig, alp ¨oktem,eric paquin, grace tang, and sylwia tur.
2020.tico-19: the translation initiative for covid-19.
inarxiv..naveen arivazhagan, ankur bapna, orhan firat, roeeaharoni, melvin johnson, and wolfgang macherey.
2019. the missing ingredient in zero-shot neuralmachine translation.
in arxiv..mikel artetxe, gorka labaka, eneko agirre, andkyunghyun cho.
2018. unsupervised neural ma-chine translation.
in iclr..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointly.
houda bouamor, nizar habash, mohammad salameh,wajdi zaghouani, owen rambow, dana abdul-rahim, ossama obeid, salam khalifa, fadhl eryani,alexander erdmann, and kemal oﬂazer.
2018. themadar arabic dialect corpus and lexicon.
in recl..song chen, jennifer tracey, christopher walker, andstephanie strassel.
2018. bolt arabic discussion fo-rum parallel training data.
in ldc2019t01..yun chen, yang liu, yong cheng, and victor o.k.
li.
2017. a teacher-student framework for zero-resource neural machine translation.
in acl..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedincross-lingual representation learning at scale.
acl..anna currey and kenneth heaﬁeld.
2019..zero-resource neural machine translation with monolin-gual pivot data.
in proceedings of the 3rd workshopon neural generation and translation..ahmed el-kishky, vishrav chaudhary, franciscoguzm´an, and philipp koehn.
2020a.
a massivecollection of cross-lingual web-document pairs.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 5960–5969..ahmed el-kishky, philipp koehn,.
and holgersearching the web for cross-schwenk.
2020b.
in proceedings of the 43rdlingual parallel data.
international acm sigir conference on researchand development in information retrieval, pages2417–2420..angela fan, shruti bhosale, holger schwenk, zhiyima, ahmed el-kishky, siddharth goyal, mandeepbaines, onur celebi, guillaume wenzek, vishravchaudhary, et al.
2020. beyond english-centricarxiv preprintmultilingual machine translation.
arxiv:2010.11125..yaroslav ganin, evgeniya ustinova, hana ajakan,pascal germain, hugo larochelle, franc¸ois lavio-lette, mario marchand, and victor lempitsky.
2017..810domain-adversarial training of neural networks.
inemnlp..guillaume lample and alexis conneau.
2019. cross-.
lingual language model pretraining.
neurips..xavier garcia, pierre foret, thibault sellam, andankur parikh.
2020a.
a multilingual view of un-in findings ofsupervised machine translation.
emnlp..guillaume lample, alexis conneau, ludovic denoyer,and marc’aurelio ranzato.
2018a.
unsupervisedmachine translation using monolingual corpora only.
in iclr..xavier garcia, aditya siddhant, orhan firat, andankur p. parikh.
2020b.
harnessing multilingual-ity in unsupervised machine translation for rare lan-guages.
in arxiv..guillaume lample, myle ott, alexis conneau, lu-dovic denoyer, and marc’aurelio ranzato.
2018b.
phrase-based & neural unsupervised machine trans-lation.
in emnlp..jiatao gu, yong wang, kyunghyun cho, and vic-tor o.k.
li.
2019a.
improved zero-shot neural ma-chine translation via ignoring spurious correlations.
in acl..zuchao li, hai zhao, rui wang, masao utiyama, andeiichiro sumita.
2020. reference language basedin find-unsupervised neural machine translation.
ings of emnlp..xiaodong gu, kyunghyun cho, jung-woo ha, andsunghun kim.
2019b.
dialogwae: multimodal re-sponse generation with conditional wasserstein auto-encoder..francisco guzm´an, peng-jen chen, myle ott, juanpino, guillaume lample, philipp koehn, vishravchaudhary, and marc’aurelio ranzato.
2019. theﬂores evaluation datasets for low-resource machinetranslation: nepali-english and sinhala-english..francisco guzman, hassan sajjad, a abdelali, ands vogel.
2013. the amara corpus: building re-sources for translating the web’s educational content.
in iwslt..hany hassan, mostafa elaraby, and ahmed tawﬁk.
2017. synthetic data for neural machine translationin proceedings of the 14th in-of spoken-dialects.
ternational workshop on spoken language transla-tion..tommi jauhiainen, marco lui, marcos zampieri, tim-othy baldwin, and krister lind´en.
2019. automaticlanguage identiﬁcation in texts: a survey.
journalof artiﬁcial intelligence research, 65:675–782..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
in tacl..anoop kunchukuttan, pratik mehta, and pushpak bhat-tacharyya.
2018. the iit bombay english-hindi par-allel corpus.
in lrec..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
intacl..graham neubig and junjie hu.
2018. rapid adaptationof neural machine translation to new languages.
inemnlp..nima pourdamghani and kevin knight.
2017. deci-.
phering related languages.
in emnlp..ye qi, sachan devendra, felix matthieu, padmanab-han sarguna, and neubig graham.
2018. when andwhy are pre-trained word embeddings useful for neu-ral machine translation.
in naacl..raytheon.
2012. bolt arabic discussion forum parallel.
training data.
in ldc2012t09..ali safaya, moutasem abdullatif, and deniz yuret.
2020. kuisail at semeval-2020 task 12: bert-cnnfor offensive speech identiﬁcation in social media.
in 14th international workshop on semantic evalu-ation (semeval)..holger schwenk, guillaume wenzek, sergey edunov,edouard grave, and armand joulin.
2019. cc-matrix: mining billions of high-quality paral-arxiv preprintlelarxiv:1911.04944..sentences on the web..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translationarxiv preprint.
2015.models with monolingual data.
arxiv:1511.06709..surafel m. lakew, alina karakanta, marcello federico,matteo negri, and marco turchi.
2019. adaptingmultilingual neural machine translation to unseenlanguages.
in iwslt..surafel melaku lakew, aliia erofeeva, and marcellofederico.
2018. neural machine translation into lan-guage varieties.
in proceedings of the third confer-ence on machine translation..aditya siddhant, ankur bapna, yuan cao, orhan firat,mia chen, sneha kudungunta, naveen arivazha-gan, and yonghui wu.
2020. leveraging monolin-gual data with self-supervision for multilingual neu-ral machine translation.
in acl..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to sequencepre-training for language generation.
in icml..811ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
arxiv preprint arxiv:1409.3215..jorg tiedemann.
2012. parallel data, tools and inter-.
faces in opus.
in lrec..chau tran, yuqing tang, xian li, and jiatao gu.
2020.cross-lingual retrieval for iterative self-supervisedtraining.
arxiv preprint arxiv:2006.09526..yu wan, baosong yang, derek f. wong, lidia s. chao,haihua du, and ben c.h.
ao.
2020. unsupervisedneural dialect translation with commonality and di-versity modeling.
in aaai..mingxuan wang, hongxiao bai, hai zhao, and lei li.
2020. cross-lingual supervision improves unsuper-vised neural machine translation.
in arxiv..guillaume wenzek, marie-anne lachaux, alexis con-neau, vishrav chaudhary, francisco guzm´an, ar-mand joulin, and edouard grave.
2020. ccnet:extracting high quality monolingual datasets fromin proceedings of the 12th lan-web crawl data.
guage resources and evaluation conference, pages4003–4012, marseille, france.
european languageresources association..mengzhou xia, xiang kong, antonios anastasopou-los, and graham neubig.
2019. generalized dataaugmentation for low-resource translation.
in acl..omar f. zaidan and chris callison-burch.
2011. thearabic online commentary dataset: an annotateddataset of informal arabic with high dialectal con-tent.
in acl..812