forecastqa: a question answering challengefor event forecasting with temporal text data.
woojeong jin1 rahul khanna1.
suji kim1 dong-ho lee1.
fred morstatter2 aram galstyan2 xiang ren1 21department of computer science, university of southern california2information sciences institute, university of southern california{woojeong.jin, rahulkha, sujikim, donghole, xiangren}@usc.edu, {fredmors, galstyan}@isi.edu.
abstract.
event forecasting is a challenging, yet impor-tant task, as humans seek to constantly planfor the future.
existing automated forecast-ing studies rely mostly on structured data,such as time-series or event-based knowledgegraphs, to help predict future events.
in thiswork, we aim to formulate a task, constructa dataset, and provide benchmarks for de-veloping methods for event forecasting withlarge volumes of unstructured text data.
tosimulate the forecasting scenario on tempo-ral news documents, we formulate the prob-lem as a restricted-domain, multiple-choice,question-answering (qa) task.
unlike exist-ing qa tasks, our task limits accessible in-formation, and thus a model has to make aforecasting judgement.
to showcase the use-fulness of this task formulation, we introduceforecastqa, a question-answering datasetconsisting of 10,392 event forecasting ques-tions, which have been collected and veriﬁedvia crowdsourcing efforts.
we present ourexperiments on forecastqa using bert-based models and ﬁnd that our best modelachieves 61.0% accuracy on the dataset, whichstill lags behind human performance by about19%.
we hope forecastqa will support fu-ture research efforts in bridging this gap.1.
1.introduction.
forecasting globally signiﬁcant events, such asoutcomes of policy decisions, civil unrest, or theeconomic ramiﬁcations of global pandemics, isa consequential but arduous problem.
in recentyears there have been signiﬁcant advances in apply-ing machine learning (e.g., time-series predictionmethods) to generate forecasts for various typesof events including conﬂict zones (schutte, 2017),duration of insurgency (pilster and b¨ohmelt, 2014),civil unrest (ramakrishnan et al., 2014a) and ter-rorist events (raghavan et al., 2013)..1https://inklab.usc.edu/forecastqa/.
figure 1: examples from the forecastqa dataset.
models only have access to articles published prior tothe timestamp associated with each question.
modelsassign probabilities to each answer choice; bold de-notes the correct answer for each question..current automated forecasting methods performwell on problems for which there are sufﬁcientstructured data (e.g., knowledge graphs), but arenot well suited for events for which such data maynot exist.
humans, though, can often accuratelyforecast outcomes by leveraging their judgement,domain knowledge, and prior experience (tetlockand gardner, 2016), along with the vast amounts ofunstructured text data available to us (e.g., news ar-ticles).
we are able to identify and retrieve salientfacts from the near-endless pool of unstructuredinformation, synthesize those facts into coherentbeliefs, and generate probabilistic forecasts.
unfor-tunately, the process does not scale well in terms ofthe amount of information that must be processedand the number of events one has to forecast..here we address the above problem by formal-izing a forecasting task, creating a dataset, andproviding benchmarks to develop methods for the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4636–4650august1–6,2021.©2021associationforcomputationallinguistics4636a) south korea [0.41]b) syria [0.28]c) south africa [0.15]     d) portugal [0.16]yes[0.38] /no [0.62]q: who will drop japan as a trading partner in august 2019?
(1/1/19) apart from the fact of being one another’s closest neighbors, the people of south korea and japan have a remarkable amount in common.
economically,they are among one another’s biggest trading partners.and yet, time and again, relations between seoul and tokyo are marked, not by mutual support and co-operation but by anger, reproach and exasperation.q: will primary schools in europe admit non-vaccinated children around september 2019?
(3/8/18)public officials and health experts had given several warnings:do not allow a student in school if they had not been vaccinated against measles.
(6/27/19)fines for parents refusing measles jab.parents will be fined up to € 2,500 if they don’t vaccinate their children against measles under draft legislation in germany which alsothreatensexclusion from crèches, nurseries and schools.earlier than timestamp(2019-08-01)earlier than timestamp(2019-09-01)task.
speciﬁcally, we formulate the forecastingproblem as a multiple-choice question answer-ing (qa) task, where the input is a news corpus,questions, choices and timestamps associated witheach question, and the output is one of the givenchoices per question.
our approach is rooted in theobservation that both forecasting and qa follow asimilar process: digesting massive amounts of tex-tual data, identifying supporting pieces of evidencefrom text, and chaining different pieces to generateanswers/forecasts..forecast question answering (forecastqa)introduces a novel timestamp constraint per ques-tion that prohibits the model from accessing newarticles published after the timestamp.
by doing so,forecastqa simulates a forecasting scenario;each question’s timestamp is chosen to ensure thatthe question is about the outcome of a future event.
to illustrate this, consider the question, “willprimary schools in europe admit non-vaccinatedchildren around september 2019?” in figure 1,and the fact that models only have access to ar-ticles before “2019-09-01.” with the addition ofthis timestamp constraint, our query becomes aquestion about a future event in “september, 2019”based on articles from the “past”; the model is nowbeing tested for its forecasting ability2.
to answerthe question, the model must ﬁnd pertinent eventsfrom “past” information, resolve the temporal andcausal relations between them, and ﬁnally make aforecasting judgement based on its interpretationof past information to answer the question.
ourtask differs from that of other works that requirean understanding of temporal relationships (ninget al., 2020) and temporal commonsense reason-ing (zhou et al., 2019), as our task forces a modelto make a forecasting judgement..in support of the proposed forecastqa for-mulation, we construct a dataset of 10,392 yes-noand multiple-choice questions.
this data is col-lected via crowdsourcing based on news articles,where workers are shown articles and asked tocome up with yes-no and multiple-choice questions.
we also crowdsourced appropriate timestamps foreach question.
finally, we design a method basedon pre-trained language models to deal with re-trieved articles for our task.
in our experiments,the methods using retrieved articles slightly outper-.
2.the ability to predict the outcome of future events based on unstructuredtext describing past events, without access to an extracted sequence of histori-cal event triples, nor provided a ﬁxed set of possible relations between events;as is the case with human forecasters..q: who will drop japan as a trading partner in august 2019?
choices: south korea (answer), south africa, syria, portugal..article: why japan and south korea just can’t get along.
(1/1/19)apart from the fact of being one another’s closest neighbours, thepeople of south korea and japan have a remarkable amount incommon.
economically, they are among one another’s biggesttrading partners.
and yet, time and again, relations between seouland tokyo are marked, not by mutual support and co-operationbut by anger, reproach and exasperation..reasoning process: seoul is in south korea, tokyo is in japan(commonsense - world knowledge).
seoul and tokyo are bigtrading partners (language understanding - lexical variations).
the relations between seoul and tokyo are marked by anger,reproach and exasperation and these relations might cause tradingrelations to cease (forecasting skills - causal relation - we caninfer the answer from this part)..table 1: chain of reasoning.
the question requiresthe reasoning process to answer..form closed-book models, suggesting that our taskis still challenging in that ﬁnding relevant informa-tion for forecasting and making a judgement are notstraightforward.
our best attempt achieves 61.0%accuracy on our dataset, a signiﬁcant performancegap from human performance by 19.3%..2 related work.
event forecasting.
there are several types of ap-proaches exist to do event forecasting.
one ap-proach could learn from highly structured event-coded data such as icews (boschee et al., 2015)and gdelt (leetaru and schrodt, 2013).
whenthese datasets are used for forecasting, they are of-ten represented as a time series (morstatter et al.,2019; ramakrishnan et al., 2014b), in which eachdata point is associated with a timestamp.
anotherapproach is script-learning, in which a model isprovided with a chain of events and a subsequentevent and is asked to predict the relation betweenthe chain and the “future” event (hu et al., 2017;li et al., 2018; lv et al., 2019).
they require toconvert text data into event triples and translatethe questions and answer choices into their for-mat, which limits the expressiveness of natural text.
however, unlike these datasets and approaches,forecastqa does not provide any structureddata to a model.
the model must learn how to ex-tract, keep track of, and link pertinent events fromunstructured text to solve forecasting questions..qa and temporal reasoning on text.
thereare several approaches for qa using unstructuredtext.
extractive qa approaches rely on ﬁndinganswer spans from the text that best answer a ques-tion (rajpurkar et al., 2016, 2018; yang et al.,2018; kwiatkowski et al., 2019; huang et al., 2019)..4637multiple-choice qa requires a model to pick thebest answer from a set (talmor et al., 2019; sapet al., 2019; zhou et al., 2019), and generativeqa prompts the machine to produce its own an-swer (khashabi et al., 2020).
our dataset is a typeof multiple-choice qa, but it differentiates itselffrom other qa datasets (all formats) in that therequired answer does not exist in the provided text,nor is sufﬁcient evidence provided to be able to an-swer a question with 100% certainty; a forecast isrequired.
we could convert our questions into alter-native query formats such as a text-to-text format,but instead we stick to multiple-choice questionsas humans often weigh the beneﬁts of multiplechoices when making a forecasting judgement..qa datasets often exist to test certain types ofreasoning.
one pertinent example of a reasoningtype that qa tasks test is the understanding of tem-poral and casual relations (jia et al., 2018a,b; sunet al., 2018; ning et al., 2020).
however, fore-castqa requires more than just extraction and un-derstanding of relations; a model must be able to ex-tract and understand the relations present in the textwith the goal of making a forecasting judgementabout an event whose outcome is not found in thetext.
another type of reasoning tested in qa tasksis commonsense reasoning (talmor et al., 2019)and even temporal commonsense reasoning (zhouet al., 2019).
while questions in forecastqaoften require commonsense to correctly answer,not all do; event outcomes do not always followcommon sense.
furthermore, our questions testforecasting abilities, which often includes varioustypes of reasoning in addition to commonsense..3 the forecastqa task.
forecastqa is a question answering task whosegoal is to test a machine’s forecasting ability.
weconsider forecasting as the process of anticipat-ing the outcome of future events based on past andpresent data (tetlock and gardner, 2016).
we focuson forecasting outcomes of news-based events com-ing from topics such as politics, sports, economics,etc.
training a machine to make forecasting de-cisions is inherently difﬁcult, as the ground-truthlabel of event outcome (e.g., whether an event willoccur) — so often required for model training — isonly obtainable “in the future”.
to make progressin our goal, we devise a way to simulate the fore-casting scenario by introducing a novel time con-straint, allowing us to validate the machine predic-.
figure 2: a treemap visualization of ﬁrst two wordsin forecastqa questions.
box area is proportionalto number of occurrences..statistic.
questions.
train dev.
test.
all.
8,210.
1,090.
1,092.
10,392.yes-no questionsmulti-choice questions.
4,7373,473.
582508.
584508.
5,9034,489.table 2: size of the forecastqa dataset..tions by obtaining desired ground-truth labels..there is also the difﬁculty of ensuring the qualityof question generation via crowdsourcing (neces-sary when building a dataset of scale), due to pos-sible human errors in question formation (tetlocket al., 2017).
we have taken steps to ensure ourquestions cannot be answered with certainty using“past” data given the time constraint or common-sense knowledge, but the questions are tractable toanswer with an educated guess (see sec.
4.1).3task deﬁnition.
formally, the input of the fore-castqa task is a forecasting question q with acorresponding ending timestamp tq––the last pos-sible date where q remains a forecasting question.
in addition, we have a set of possible choices, c,and a corpus of news articles, a; the output is achoice c ∈ c. our task has a novel constraint thatany retrieved article a ∈ a must satisfy ta < tq.
in other words, models have access only to articlesthat are published before tq.
we have ensured thatthe information required to solve the question de-terministically comes out in an article, gold article,published after tq, i.e., tgold article ≥ tq.
anotherway to think of our setup is that we are asking qon the day before tq, knowing that the informationrequired to solve q is not available yet.
this for-.
3this is in contrast to open-domain qa (machine readingcomprehension) (kwiatkowski et al., 2019) where answerscan always be found in some given passages..4638will thewill therewhat willwhatiswhat kindwhattypewhat doeswho willwho ishow manyhow muchhowwillhow oldwhere willwhichcountrywhichcountry’swhich partywhich companywhywillwhenwillisthedoes theare thewill there be electricity in canada despite hurricane dorian in september 2019?who will be german chancellor by november 2019?who will be wanted to execute by saudi prosecutors in july 2019?who will visit pittsburgh for first 2020 campaign rally in april 2019?who will be the fifa president in september 2019?what will lyft return to its san francisco area fleet in june 2019?what will be the budget of terminator dark fate in october 2019?what will belinda carlisle want to be by september 2019?what will be difficult for boeing to get approval for by may 2019?where will the glasgow derby be played in september 2019?how many instagram followers will noor charchafchihave by september 2019?what countryhow longwill the global stock market fall in may 2019?will the james bond actor arrive italy in september 2019?will the public charge rule impact us taxpayers by august 2019?will the mona lisa be missing in the louvre by october 2019?will the wright family blame boris johnson for its failure in september 2019?will the duke of sussex refuse to tour africa in september 2019?
news corpus collection.
we started by gatheringenglish news articles from lexisnexis4.
we thencurated a list of 21 trustful news sources and ﬁlteredarticles based on their publishers; we also ﬁlteredout non-english articles.
finally, we selected theﬁve-year period of 2015-2019 and ﬁltered out ar-ticles outside this period, leaving us with 509,776articles.
this corpus is also used for retrieval in ourtask setting (i.e., constrained open-domain).
q-answer-timestamp triple creation.5 oncewe assembled the news corpus, we built (ques-tion, answer, timestamp) triples to accompany thenew corpus as inputs for our task.
to generatethe needed triples we looked to crowdsourcing viaamazon mechanical turk.
our generation taskconsists of the following steps: (1) we selected arandom news article from 2019 from the collectednews corpus (these news articles are gold articlesand will be hidden for experiments); (2) workerscreated questions, which if posed before the respec-tive article’s publication date would be seen as aforecasting question; (3) they indicated the answer,along with supporting evidence that the questionconsisted of (to ensure the correctness of the trueanswer); (4) they were asked to make multiple-choice distractors with their own knowledge and/oraccess to search engines; and (5) we ensured thata temporal phrase is present in the questions, forexample: “after may of 2020...”, “... in june of2021?” to provide a temporal context (constraint)for each question, yielding more precise and well-deﬁned forecasting questions.
completion of thistask results in the desired triple of: a forecastingquestion, an answer to the question (with distractorchoices), and a timestamp as our temporal con-straint.
the timestamp is set as the ﬁrst day of themonth in which the gold article was published..to diversify questions in the dataset, we createdtwo kinds of questions: binary yes-no questionsand multiple-choice questions with four choices.
multiple-choice questions start with one of the sixws (i.e., who, what, when, where, why, and how)and are more challenging as they require determin-ing the correctness of each choice..question quality veriﬁcation.
we performed aseparate crowdsourcing data veriﬁcation to test andenforce the following criteria: (1) is answeringthe question a tractable problem given (relevant).
4https://risk.lexisnexis.com5due to the limited space, for more details of our triple creation guide-lines for human annotators, veriﬁcation steps, and screenshots of our data col-lection/veriﬁcation amt interfaces, please refer to sec.
a of the appendix..figure 3: forecastqa generation process.
the in-put of forecastqa creation is a news article corpusand the output is yes-no/multiple-choice questions..mulation makes our task both a constrained open-domain qa and a forecasting problem––distinctfrom existing qa tasks.
challenges in forecastqa.
due to the con-strained open-domain setting and forecasting prop-erties, testing a model’s forecasting ability encom-passes the following challenges: information re-trieval (ir) on limited sources, understanding oftemporal and causal relations between events, andﬁnally a forecasting judgement.
our time con-straint limits the accessible articles and also createsmore challenges than in standard open-domain qa;effective ir methods are necessary to anticipatewhat knowledge will be useful for predictions frompast information sources.
once useful articles havebeen retrieved, models should understand these ar-ticles and reason over pertinent facts from them.
finally, these models use the gleaned knowledge toinfer the outcome of a future event.
unlike in otherreading comprehension tasks, models cannot relyon the existence of an answer within the text, butmust make an educated guess as to what will hap-pen in the future.
while our task does encompassreasoning abilities tested in other datasets, no othertasks investigate these reasoning abilities in thecontext of predicting future events.
more analysison reasoning types can be found in sec.
4.2..4 dataset construction and analysis.
in this section, we describe how we construct ourforecastqa dataset and analyze it..4.1 construction details.
the data collection is broken down into three sec-tions: (1) gathering a news corpus, (2) generatingquestion-answer-timestamp triples with distractorchoices, and (3) verifying the triples’ quality.
thedata generation process is summarized in fig.
3..4639crowdworkersverify quality-each questions will be verified by 3 workersqa datasetrejectedverifiedyesnocrowdworkersgenerate forecasting questions based on given news articleswillprimary schools admit non-vaccinated children around september 2019?
[ yes / no ]who willdrop japan as a trading partner in august 2019?
[ south korea / syria / south africa / portugal]didjustin amashconsider independence from the republican party around july 2019?
[ yes / no ]didjustin amashconsider independence from the republican party around july 2019?
[ yes / no ]collect forecasting questionsbased on crowdworkersverificationwillprimary schools admit non-vaccinated children around september 2019?
[ yes / no ]who willdrop japan as a trading partner in august 2019?
[ south korea / syria / south africa / portugal]curate news articles via lexisnexisfrom 2019-01-01to 2019-11-31-filter with 21 news sourcesfrom 2015-01-01to 2019-11-31articles for generating questionsarticles for retrievaldisjointfigure 4: reasoning skills (types) and their frequency (in %) in the sampled data.
as each question can be labeled withmultiple types, the total frequency does not sum to 100%.
on average, 3 reasoning skills are required for each question.
examplesof other reasoning types can be found in fig.
11 in the appendix..“past” articles?, and (2) is the question determin-istically answerable given any article adhering tothe question’s temporal constraint?
— if a ques-tion is too difﬁcult, i.e., an educated guess to theanswer (when given relevant, constraint-adheringarticles) is not possible, then we ﬁlter the ques-tion out.
on the other hand, if the questions areanswerable with certainty using “past” articles, orcommonsense/world knowledge, then they are notconsidered to be forecasting questions.
the de-sired response (majority vote from 3 annotators) isa “yes” for criterion (1) and “no” for (2), as thatwould show that the tuple of question and time con-straint simulates the desired forecasting scenario.
with the above method, we ﬁltered out 31% of thequestions collected in the triple creation step andwere left with 5,704 yes-no questions and 4,513multi-choice questions.
more details about the veri-ﬁcation step are included in sec.
a of the appendix..additional statistics in sec.
d of appendix..types of questions.
to understand the types ofquestions in forecastqa, we examined the pop-ular beginnings of sentences and created a tree-mapplot (see fig.
2).
as shown, nearly half the ques-tions start with the word will (44%), a result of overhalf of the questions being yes-no questions..reasoning types.
to examine types of reasoningrequired to answer our questions we sampled 100questions and manually annotated them with rea-soning types.
due to the forecasting nature of ourdataset, we are particularly interested in questionscontaining the forecasting ability and thus spendmore time looking into these questions.
our con-densed results can be found in figure 4, and moreresults from our cataloguing effort can be found insec.
c of the appendix.
note that most questionscontain more than one reasoning type..4.2 dataset analysis.
5 methods.
to better understand the properties of the questionsin forecastqa, we examine: 1) a few data statis-tics 2) types of questions asked, and 3) the types ofreasoning required to answer our questions..summary statistics.
forecastqa dataset iscomposed of 10,392 questions, divided into a80/10/10 split of train, dev, and test data.
our10k questions are roughly evenly split betweenmultiple-choice and yes-no binary questions (ta-ble 2).
over 17k distinct words were used to con-struct our questions and we have 218 unique timeconstraints associated with them; time constraintsrange from 2019-01-11 to 2019-11-12. we include.
to evaluate the forecasting capabilities of re-cent multi-choice/binary qa model architectureson forecastqa, we provide a comprehensivebenchmarking analysis in this work.
we run the ex-periments in two settings: (1) closed-book and (2)constrained open-domain setup.
in the closed-bookscenario only q (question) and c (answer choices)are provided to the model (q, c), while a (newsarticles) is provided for setting (2), (q, c, a)6. werun these settings to understand the difﬁculty ofboth the closed-book and open-domain challengespresented by the questions in forecastqa..6tq is always applied to a, we left it out of the notation for simplicity..4640sentencequestionreasoningdetailed reasoning typeresolving time information[24%]q: what will be blocking the us-china deal in november 2019?sen.
: sanctions was imposed against chinese products since last year.
(9/24/19)causal relations[30%]q: what wild animal will be found at the outer banks of north carolina in september 2019?sen.
: u.s. senator thom tillis introduced the corolla wild horses protection act, legislation that would provide responsible management of the wild horse population around corolla, north carolina and the outer banks.reasoning: protection act in the outer banks → wild horses will be protected in the outer banks.temporal relations[8%]q: how much will google be fined in billion dollars by november 2019 in europe?sen.1: the european union announced a $2.7 billion fine in 2017 against googlesen.2: google fined $1.7 billion by e.u (9/11/19)reasoning: $2.7 billion in 2017, $1.7 billion in september 2019inferring based on past events[54%]q: which celebrations of china will the pro-democracy protests of demonstrators spoil in hong kong in september 2019?sen.
: china’s leaders will not want overshadowed by protests in hong kong, which have grown in intensity since mass demonstrations began in june.reasoning -detailed reasoning typelanguage understanding [91%]lexical variations(synonymy, coreference)[46%]syntactic variations(paraphrase)[66%]multi-hop reasoning [14%]checking multiple properties[9%]bridge entity[5%]commonsense reasoning [47%]world knowledge[36%]social commonsense[7%]temporal commonsense[9%]numerical reasoning [12%]addition, subtraction[5%]comparison[8%]forecasting[73%]on the [cls] token representations of each article.
this pooled representation is passed to an mlplayer to make a prediction.
comparison betweenthese aggregations helps understand the effect ofmodeling temporal order of evidence.
these twoaggregation modules are denoted by “agg (gru)”and “agg (maxpool),” respectively..multi-document summarization (mds).
ratherthan conducting context aggregation of the re-trieved articles, we consider an mmr summa-rizer (carbonell and goldstein, 1998) which per-forms extractive, multi-document summarizationof text to generate a summary asumm (rightmost ar-chitecture in fig.
5).
the summary article asummis treated as if it is an ai ∈ a and fed into a text en-coder along with q and c which then produce the[cls] embedding for making a prediction.
wename this method “mds.”.
integrated approach.
to take the best of bothworlds in (q, c) and (q, c, a) settings, we inte-grate two architectures (the leftmost and middleones in fig.
5).
we concatenate the last two hiddenrepresentations of each architecture before passingthe concatenated representation through a sharedmlp layer.
we use bertlarge as f in both ar-chitectures, agg (gru) for g and call this model“bertlarge ++ (integrated)” in table 3..other baselines.
we also consider other base-lines: esim (chen et al., 2017b), bidaf++ (clarkand gardner, 2018), prepending extracted openevent triples (liu et al., 2019a) to bert input,and a script learning approach, sam-net (lvet al., 2019).
we modify the approaches to ﬁt intoour setup.
detailed descriptions of each baselinemethod are included in sec.
e.3 of appendix..6 experiments.
6.1 experimental setup.
we adopt two types of settings: the closed-booksetting (q, c) and the constrained open-domainsetting (q, c, a).
in the constrained open-domainsetting, we use bm25 (robertson et al., 1995; qiet al., 2019) as our ir method8 to obtain a, 10 re-trieved articles.
we also explore other ir methodsin the later section.
note that we retrieve articlesthat do not violate the time constraints.
we feedthe question q as a query and limit our access toarticles in a by tq.
additionally, we validate the.
8details of ir methods are described in appendix sec.
e.2..figure 5: our baseline model architectures.
the clstoken is either fed into an mlp for classiﬁcation or to theaggregator, which collects the information from each articlebefore classifying..for both settings, we explore several baselinemodels, but all follows a general architecture of atext encoder f and an optional context aggregationmodule g to aggregate information from a set ofretrieved articles.
fig.
5 shows the architecturesused.
we model both yes-no and multiple-choicequestions as a binary classiﬁcation task; a model’sprediction is the class with the largest probability.
below we introduce the details of our baselines..text encoder.
we use pre-trained language model,bert (devlin et al., 2019), as a text encoder (ffrom above)7. f is designed to deal with (q, c)and (q, c, a) inputs, where a is a set of time-stamped articles that are retrieved from a to answerq. each input of f is transformed into [[cls]q[sep]c[sep]ai] (for each ai ∈ a, c ∈ c), or[[cls]q[sep]c] (for each c ∈ c) if articlesare not supplied.
the [cls] token is the same asthe one commonly used for ﬁne-tuning ptlms fora classiﬁcation task, and [sep] is the special sepa-rator token.
the embedding of [cls] is then usedfor predictions with an mlp layer (the leftmostmodel architecture in fig.
5), or as input into a con-text aggregation module (the middle architecturein fig.
5) subsequently introduced..context aggregation (agg).
two architecturesare used when aggregating information from mul-tiple, time-stamped articles a retrieved for a ques-tion.
(1) temporal aggregation: this aggregatorutilizes temporal ordering of the retrieved articles.
articles are sorted by their timestamps and their[cls] token representation from f are aggregatedby a gated recurrent unit (gru) (cho et al., 2014)with a mlp head to make ﬁnal predictions.
(2) setaggregation: alternatively, we ignore the temporalordering of articles and use a maxpooling operation.
7.we did not include more recent pre-trained language models (e.g.,roberta (liu et al., 2019b), albert (lan et al., 2020), t5 (raffel et al.,2020)) or pre-trained qa models like uniﬁedqa (khashabi et al., 2020), asthese models are trained using text data published after the earliest timestampin our dataset (2019-01-01), meaning information leakage could occur (andviolates the forecasting setup).
we tested more lms in sec.
e.5 of appendix..4641. .
.bertbertaggregator [pooling/rnn][cls][cls]𝐴!c𝑄𝐴"𝑄cyes/no𝑄csummarized articlesbert[cls]yes/no𝑄cbert[cls]yes/nomethods / metrics.
random.
esim-elmo (closed-book)bertbase (closed-book)bertlarge (closed-book).
bidaf++ (clark and gardner, 2018)bertbase, mdsbertbase, agg (maxpool)bertbase, agg (gru)sam-net (lv et al., 2019)bertlarge, mdsbertlarge, event triplesbertlarge, agg (maxpool)bertlarge, agg (gru)bertlarge, agg (maxpool), dprbertlarge, agg (maxpool), btbertlarge ++ (integrated)human performance(α)human performance(β).
48.6.
63.366.267.3.
51.763.167.267.664.567.466.768.869.270.270.070.3.
74.681.3.accuracy (%, ↑).
brier score (↓).
yes/no multi.
all.
yes/no multi.
all.
25.3.
37.8.
0.684.
0.827.
0.750.
45.841.545.4.
30.139.139.141.540.940.145.046.947.547.048.048.4.
64.977.4.
54.554.757.6.
40.952.054.255.453.554.756.658.659.159.459.760.1.
71.279.4.
0.5150.5110.447.
0.4780.5040.4530.4770.5310.5420.5890.4760.4830.5540.4440.537.
--.
0.8970.7150.653.
0.8980.7160.7010.7050.7190.7380.7190.6480.6550.7280.6620.650.
--.
0.7060.6060.543.
0.6880.6030.5680.5830.6190.6330.6490.5560.5630.6350.5450.589.
--.
table 3: performance of baseline models on fore-castqa test set.
“yes/no” refers to yes-no questions, and“multi” to multi-choice questions.
we test the closed-booksetting, and the constrained open-domain setting, where theaccessible articles are limited by tq, our time constraint.
weuse bm25 as the article retriever to select top-10 articles, ifnot particularly speciﬁed.
“bt” concatenates the binary en-coding of date string to an article encoding before aggregation(see sec.
6.3 “ablation on timestamp modeling”).
humanperformance is based on the top-10 retrieved articles (α), andgoogle search with the question’s time constraint (β)..answerability of our questions by providing goldarticles instead of retrieved articles (sec.
6.3)..evaluation metrics.
because forecasting is un-certain, a system’s prediction probabilities indicateits conﬁdence answering the question.
in additionto accuracy, we consider brier score (brier, 1950),which measures the mean squared error of probabil-ities assigned to sets of answer choices (outcomes).
formally, brier = 1c=1(pic − yic)2,nwhere pic is the probability of prediction; yic isa label indicator for class c of the instance (1 or 0),n is the number of prediction instances, and c isthe number of classes (2 or 4).
the highest brierscore is 0 (probability 1 for the correct class, prob-ability 0 else), while the worst possible brier scoreis 2 (probability 1 for the wrong class, probability0 else).
a conﬁdent model gets low brier scores..(cid:80)n.(cid:80)c.i=1.
6.2 human performance.
to benchmark human performance, seven anno-tators (computer science graduate students) whowere not involved in question generation wereasked to answer 150 randomly sampled questionsfrom the test set.
we consider two scenarios: 1)annotators are provided with retrieved articles, a;and 2) annotators can access any article publishedbefore the timestamp via google search.
moreover,as annotators live in the “future” with respect to thetimestamp of a question, they might already knowthe actual answer.
to avoid the over-estimation.
methods.
gru.
maxpool.
mds.
bertbase, tf-idfbertbase, dprbertbase, bm25.
bertlarge, tf-idfbertlarge, dprbertlarge, bm25.
53.253.755.4.
56.556.159.1.
53.954.654.2.
55.459.458.6.
51.654.352.0.
55.054.654.7.table 4: accuracy with different retrievers: bm25, tf-idf, and dense passage retrieval (dpr).
we test the retrieverswith different aggregators: gru, maxpool, and mds..of accuracy, we asked the annotators to not usetheir “future” knowledge.
if they felt this is notpossible, we asked them to skip the question.
onaverage, 28.3% of questions are skipped.
giventhis setup, humans achieve 71.2% and 79.4% accu-racy respectively, for the two scenarios when takinga majority vote for each question; we also observedgood inter-annotator agreement.
the two scenariosare referred as “(α)” and “(β)” in table 3..6.3 results and performance analysisresults on the constrained open-domain set-ting.
table 3 shows the results of baseline methodsfor comparison.
we compare pre-trained languagemodels with different context aggregators and otherbaselines.
the integrated model, bertlarge ++shows the best performance in terms of accuracy,while bertlarge (closed-book) shows the bestbrier score.
unlike the accuracy metric, the brierscore penalizes over- and under- conﬁdent fore-casts (mellers et al., 2014) — thus the best modelunder each metric can be different.
the marginaldifferences in performance between the two set-tings suggest that access to information (text evi-dence) alone does not solve the forecasting prob-lem.
we hypothesize an inability to encode salientrelations for forecasting purposes prevents the ad-ditional information from proving useful.
amongthe aggregators in bertbase, the gru aggregatoroutperforms other aggregators and summarizers.
this suggests that utilizing articles’ temporal orderhelps the reasoning.
overall, baselines fall behindhuman performance by over 10% points given thesame retrieved articles.
study of different ir methods.
we furthertest several retrieval methods: bm25 (robertsonet al., 1995; qi et al., 2019), tf-idf (chen et al.,2017a), and a pre-trained dense passage retriever(dpr) (karpukhin et al., 2020).
as in table 4,bertlarge with dpr retriever and the maxpoolaggregator shows the best performance than othercombinations.
however, dpr does not achieve thebest accuracy for all methods.
this implies that 1).
4642methods / metrics.
gru.
maxpool.
acc (↑) brier (↓) acc (↑) brier (↓).
w/o timestamps.
pre-pend timestampsbinary timestamp encodingchar-rnn timestamp encoding.
55.4.
54.251.154.0.
0.583.
0.6340.6230.640.
54.2.
54.855.654.3.
0.568.
0.5990.6240.620.table 5: study on modeling article timestamps (publica-tion dates) in the constrained open-domain setting.
we testseveral methods for temporal modeling and use bertbasewith two different aggregators: gru and maxpool..methods / metrics.
accuracy (↑).
brier score (↓).
yes/no multi.
all.
yes/no multi.
all.
random.
25.3.
37.8.
0.684.
0.827.
0.750.questionarticleevidence sentence.
41.580.789.5.
54.776.984.4.
0.5110.4280.355.
0.7150.2630.171.
0.6060.3510.269.
48.6.
66.273.679.9.table 6: answerability study on test set.
instead of re-trieved articles, we provide bertbase with ground-truth con-text: a gold article or evidence sentence.
we thus convertforecastqa to a reading comprehension task and examinethe answerability of the questions..stronger retrieval methods are required to identifyuseful evidence; 2) complex forecasting abilitiesmay be a bottleneck of current systems..ablation on timestamp modeling.
we conductan ablation study on modeling time information(publication date) of the retrieved articles, as seenin table 5. we test: a) pre-pending date string asbert input, b) using binary encodings of dates9and concatenate with article encoding before aggre-gation, and c) using char-rnn (goyal and durrett,2019) for encoding date string before aggregation10.
we ﬁnd that using binary encodings of dates im-proves the accuracy for the maxpool aggregator.
however, the gru aggregator’s accuracy decreaseswhen given date information.
we conjecture thatour modeling for the time information of each ar-ticle is not strong enough to help forecasting.
weleave more sophisticated modeling for future work..answerability of questions.
to validate that thequestions in forecastqa are indeed answerable,we convert our setup into a machine reading com-prehension (mrc) task — ﬁnd an answer givenan assumed appropriate context.
we provide themodel with a gold article or the evidence sentence(sec.
4.1).
since pre-trained models have achievedhigh performance on mrc tasks (rajpurkar et al.,2016), we expect adequate performance when pro-vided the correct context.
as seen in table 6, weobserve that in closed-book setting, bert is ableto beat out a random baseline, but it still does not.
9https://temporenc.org10details are described in appendix sec.
e.4.
(a) varying amounts of data.
(b) different question types.
figure 6: (a) test accuracy of bertbase trained with varyingamounts of training data, with human performance (79.1%)shown in orange, and (b) development accuracy breakdownby different types of multichoice questions..perform well; implying our questions are not triv-ial for bert, and context is required to answerthem correctly.
when given the gold article, bertachieves 76.9% (+22%) and it even performs bet-ter (84.4%) given the evidence sentence.
this allimplies that given the right information, our fore-casting questions can be answered correctly..study of data efﬁciency.
to examine how mod-els might perform with less/more training data, weevaluate bertbase (closed-book) on the test set,by training it with varying amounts of labeled data.
fig.
6a shows the the resulting “learning curve.”we observe the accuracy of the model is “expected”to reach 70%, assuming 100k examples — whichis still 9% point lower than human performance..results on different question types.
we testbertbase (closed-book) on different questiontypes of multi-choice questions from our develop-ment set (fig.
6b).
we ﬁnd that the accuracy of themodel varies across different question types: “how”questions are the most difﬁcult to predict whilehigher accuracy is achieved on “why” questions.
also for yes-no questions, the method achieves69.5% on “yes” questions and 62.9% “no” ques-tions, indicating that there is no signiﬁcant biastowards certain type of binary questions..error analysis.
we observe 4 main categories oferrors produced by the methods in our analysis: (1)retrieving irrelevant articles, (2) incorrect reasoningon relevant evidence, (3) lacking (temporal) com-mon sense, and (4) lacking numerical knowledge.
please refer to sec.
e.7 of appendix for examplesand in-depth discussions of these errors..7 conclusion.
forecasting is a difﬁcult task that requires everypossible advantage to do well.
it would be wiseto harness this pool of unstructured data for train-ing automatic event forecasting agents.
to utilizethis form of data for forecasting, we proposed a.
4643102103104105num.
training instances020406080100test set acc (%) 35%52%27%56%31%62%020406080whatwhohowwherewhichwhyaccavg accquestion-answering task that requires forecastingskills to solve forecastqa, and provided the ac-companying dataset.
various baseline methods didnot perform well, but this is not surprising given theinherent difﬁculty of forecasting.
our benchmarkdataset can beneﬁt future research beyond natu-ral language understanding and hope forecastingperformance will be signiﬁcantly improved..references.
elizabeth boschee,.
jennifer lautenschlager, seano’brien, steve shellman, james starz, and michaelward.
2015. icews coded event data.
harvard data-verse, 12..glenn w brier.
1950. veriﬁcation of forecasts ex-pressed in terms of probability.
monthly weatherreview, 78(1):1–3..jaime carbonell and jade goldstein.
1998. the use ofmmr, diversity-based reranking for reordering doc-in proceedingsuments and producing summaries.
of the 21st annual international acm sigir confer-ence on research and development in informationretrieval, pages 335–336..danqi chen, adam fisch, jason weston, and antoinebordes.
2017a.
reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879, vancouver, canada.
association for computa-tional linguistics..qian chen, xiaodan zhu, zhen-hua ling, si wei,hui jiang, and diana inkpen.
2017b.
enhancedlstm for natural language inference.
in proceed-ings of the 55th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1657–1668, vancouver, canada.
asso-ciation for computational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerlearningschwenk, and yoshua bengio.
2014.phrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 845–855, melbourne,australia.
association for computational linguis-tics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
deep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2368–2378, min-neapolis, minnesota.
association for computationallinguistics..hassan ismail fawaz, germain forestier, jonathan we-ber, lhassane idoumghar, and pierre-alain muller.
2019. deep learning for time series classiﬁcation:a review.
data mining and knowledge discovery,33(4):917–963..clinton gormley and zachary tong.
2015. elastic-search: the deﬁnitive guide: a distributed real-timesearch and analytics engine.
” o’reilly media,inc.”..tanya goyal and greg durrett.
2019. embedding timeexpressions for deep temporal ordering models.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4400–4406, florence, italy.
association for computationallinguistics..linmei hu, juanzi li, liqiang nie, xiaoli li, and chaoshao.
2017. what happens next?
future subeventprediction using contextual hierarchical lstm.
inproceedings of the thirty-first aaai conference onartiﬁcial intelligence, february 4-9, 2017, san fran-cisco, california, usa, pages 3450–3456.
aaaipress..lifu huang, ronan le bras, chandra bhagavatula, andyejin choi.
2019. cosmos qa: machine readingcomprehension with contextual commonsense rea-soning.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages2391–2401, hong kong, china.
association forcomputational linguistics..zhen jia, abdalghani abujabal, rishiraj saha roy, jan-nik str¨otgen, and gerhard weikum.
2018a.
tem-pquestions: a benchmark for temporal question an-swering.
in www..zhen jia, abdalghani abujabal, rishiraj saha roy,jannik str¨otgen, and gerhard weikum.
2018b.
temporal question answering overtequila:the 27thin proceedings ofknowledge bases.
acm international conference on information and.
4644knowledge management, cikm 2018, torino, italy,october 22-26, 2018, pages 1807–1810.
acm..vladimir karpukhin, barlas oguz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen, andwen-tau yih.
2020. dense passage retrieval foropen-domain question answering.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6769–6781, online.
association for computational lin-guistics..daniel khashabi, sewon min, tushar khot, ashishsabharwal, oyvind tafjord, peter clark, and han-naneh hajishirzi.
2020. unifiedqa: crossing for-mat boundaries with a single qa system.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 1896–1907, online.
as-sociation for computational linguistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:452–466..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..kalev leetaru and philip a schrodt.
2013. gdelt:global data on events, location, and tone, 1979–in isa annual convention, volume 2, pages2012.
1–49.
citeseer..zhongyang li, xiao ding, and ting liu.
2018. con-structing narrative event evolutionary graph forthescript event prediction.
twenty-seventh international joint conference onartiﬁcial intelligence, ijcai 2018, july 13-19, 2018,stockholm, sweden, pages 4201–4207.
ijcai.org..in proceedings of.
xiao liu, heyan huang, and yue zhang.
2019a.
opendomain event extraction using neural latent variablein proceedings of the 57th annual meet-models.
ing of the association for computational linguis-tics, pages 2860–2871, florence, italy.
associationfor computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv, abs/1907.11692..shangwen lv, wanhui qian, longtao huang, jizhonghan, and songlin hu.
2019. sam-net:integrat-ing event-level and chain-level attentions to predict.
what happens next.
in the thirty-third aaai con-ference on artiﬁcial intelligence, aaai 2019, thethirty-first innovative applications of artiﬁcial in-telligence conference, iaai 2019, the ninth aaaisymposium on educational advances in artiﬁcialintelligence, eaai 2019, honolulu, hawaii, usa,january 27 - february 1, 2019, pages 6802–6809.
aaai press..barbara mellers, lyle ungar, jonathan baron, jaimeramos, burcu gurcay, katrina fincher, sydney escott, don moore, pavel atanasov, samuel a swift,et al.
2014. psychological strategies for winning ageopolitical forecasting tournament.
psychologicalscience, 25(5):1106–1115..fred morstatter, aram galstyan, gleb satyukov,daniel benjamin, andr´es abeliuk, mehrnoosh mir-taheri, ksm tozammel hossain, pedro a. szekely,emilio ferrara, akira matsui, mark steyvers,stephen bennett, david v. budescu, mark himmel-stein, michael d. ward, andreas beger, michelecatasta, rok sosic, jure leskovec, pavel atanasov,regina joseph, rajiv sethi, and ali e. abbas.
2019.sage: a hybrid geopolitical event forecasting sys-tem.
in proceedings of the twenty-eighth interna-tional joint conference on artiﬁcial intelligence, ij-cai 2019, macao, china, august 10-16, 2019, pages6557–6559.
ijcai.org..qiang ning, hao wu, rujun han, nanyun peng, mattgardner, and dan roth.
2020. torque: a readingcomprehension dataset of temporal ordering ques-in proceedings of the 2020 conference ontions.
empirical methods in natural language process-ing (emnlp), pages 1158–1172, online.
associa-tion for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..ulrich pilster and tobias b¨ohmelt.
2014. predictingthe duration of the syrian insurgency.
research &politics, 1(2):2053168014544586..peng qi, xiaowen lin, leo mehr, zijian wang, andchristopher d. manning.
2019. answering complexopen-domain questions through iterative query gen-eration.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages2590–2602, hong kong, china.
association forcomputational linguistics..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,.
4645maarten sap, hannah rashkin, derek chen, ronanle bras, and yejin choi.
2019. social iqa: com-monsense reasoning about social interactions.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4463–4473, hong kong, china.
association for computa-tional linguistics..sebastian schutte.
2017. regions at risk: predictingconﬂict zones in african insurgencies.
political sci-ence research and methods, 5(3):447–465..yawei sun, gong cheng, and yuzhong qu.
2018.reading comprehension with graph-based temporal-casual reasoning.
in proceedings of the 27th inter-national conference on computational linguistics,pages 806–817, santa fe, new mexico, usa.
asso-ciation for computational linguistics..alon talmor, jonathan herzig, nicholas lourie, andjonathan berant.
2019. commonsenseqa: a ques-tion answering challenge targeting commonsenseknowledge.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4149–4158, minneapolis, minnesota.
associ-ation for computational linguistics..philip tetlock, barbara a. mellers, and j. peter scoblic.
2017. bringing probability judgments into pol-icy debates via forecasting tournaments.
science,355:481–483..philip e tetlock and dan gardner.
2016. superfore-casting: the art and science of prediction.
randomhouse..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d. manning.
2018. hotpotqa: a datasetfor diverse, explainable multi-hop question answer-ing.
in proceedings of the 2018 conference on em-pirical methods in natural language processing,pages 2369–2380, brussels, belgium.
associationfor computational linguistics..ben zhou, daniel khashabi, qiang ning, and danroth.
2019.
“going on a vacation” takes longerthan “going for a walk”: a study of temporal com-in proceedings of themonsense understanding.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3363–3369, hong kong,china.
association for computational linguistics..wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21:1–67..vasanthan raghavan, aram galstyan, and alexan-der g. tartakovsky.
2013. hidden markov modelsfor the activity proﬁle of terrorist groups.
ann.
appl.
stat., 7(4):2402–2430..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-in proceedings of the 56th an-tions for squad.
nual meeting of the association for computationallinguistics (volume 2: short papers), pages 784–789, melbourne, australia.
association for compu-tational linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..naren ramakrishnan, patrick butler, sathappanmuthiah, nathan self, rupinder paul khandpur,parang saraf, wei wang, jose cadena, anil vul-likanti, gizem korkmaz, chris j. kuhlman, achlamarathe, liang zhao, ting hua, feng chen,chang-tien lu, bert huang, aravind srinivasan,khoa trinh, lise getoor, graham katz, andydoyle, chris ackermann, ilya zavorin, jim ford,kristen maria summers, youssef fayed, jaimearredondo, dipak gupta, and david mares.
2014a.
’beating the news’ with embers: forecasting civilin the 20thunrest using open source indicators.
acm sigkdd international conference on knowl-edge discovery and data mining, kdd ’14, newyork, ny, usa - august 24 - 27, 2014, pages 1799–1808. acm..naren ramakrishnan, patrick butler, sathappanmuthiah, nathan self, rupinder paul khandpur,parang saraf, wei wang, jose cadena, anil vul-likanti, gizem korkmaz, chris j. kuhlman, achlamarathe, liang zhao, ting hua, feng chen,chang-tien lu, bert huang, aravind srinivasan,khoa trinh, lise getoor, graham katz, andydoyle, chris ackermann, ilya zavorin, jim ford,kristen maria summers, youssef fayed, jaimearredondo, dipak gupta, and david mares.
2014b.
’beating the news’ with embers: forecasting civilin the 20thunrest using open source indicators.
acm sigkdd international conference on knowl-edge discovery and data mining, kdd ’14, newyork, ny, usa - august 24 - 27, 2014, pages 1799–1808. acm..stephen e robertson, steve walker, susan jones,micheline m hancock-beaulieu, mike gatford, et al.
1995. okapi at trec-3.
nist special publication sp,109:109..4646figure 7: instruction of creating multiple-choice ques-tions..figure 8: interface of creating multiple-choice ques-tions..a detailed dataset creation.
in this section, we present detailed explanations ofdataset creation.
we ﬁrst selected news sources asin the following section..a.1 list of news sources.
the new york post, the new york times, newyork magazine, daily news (new york), thewashington post, npr all things considered,npr weekend edition saturday, npr morningedition, cnn wire, cnn.com, cnnmoney.com,cnn international, fox news network,york guardian, washingtonpost.com, the wash-ington post magazine, thetimes.co.uk, guardianweekly, russia & cis general newswire, us ofﬁ-cial news, the times (london)..a.2 dataset creation.
turking guidelines.
figs 7 and 8 show the in-structions and interface for creating our multiple-choice questions.
workers made multiple-choicedistractors with their own knowledge, but they were.
figure 9: interface of verifying questions..encouraged to ﬁnd good distractors using searchengines.
to ensure the answerability of the createdquestions, we ask them to indicate the answer alongwith the supporting evidence that the question ismade from.
we omit the interfaces due to the spacelimit..initial screening.
the ideal result of our crowd-sourcing task are forecasting questions that aretractable but not trivial, and by deﬁnition not an-swerable with certitude using information currentlyavailable.
thus to avoid undesirable questions,we asked two additional questions to help screenpoorly constructed questions.
as shown in fig 8,we try to determine the difﬁculty of the questionand whether it is answerable using “current” or“past” information.
question 1 attempts to estab-lish whether the question is indeed tractable andasks whether there exists some qualiﬁed group ofpeople who could reason and make an educatedguess at the answer to the question.
on the otherhand, question 2 tries to determine if the questionis either too easy or is deﬁnitively answerable given“current” and “past” information.
thus, the desiredresponse is “yes” and “no” for questions 1 and 2,respectively; we ﬁltered out created questions thatdo not satisfy the desired condition..a.3 additional question quality checks.
we asked the same two questions from our initialquality screening and an additional question to helpadjust the timestamp associated with the questionif needed.
per question, we got 3 crowd workersto answer the three questions and took the majorityvote for question 1 and 2, while selecting the earli-est selected timestamp for question 3. we droppedthe question, if the majority vote was “no” for ques-tion 1 or “yes” for question 2. moreover, if at leastone worker selected “e” in the question 3 (there isno appropriate recent time stamp), then we ﬁlteredout the question.
additionally, if the created ques-.
4647q: what wild animal will be found at the outer banks of north carolina in september 2019?
choices: horses (answer), cows, turtles, donkeys..measurement.
article: tillis introduces legislation to protect corolla wild horses washington: ofﬁce ofthe senator thom tillis has issued the following news release: (1/29/19)u.s. senator thom tillis (r-nc) introduced the corolla wild horses protection act, leg-islation that would provide responsible management of the wild horse population aroundcorolla, north carolina and the outer banks.
representative walter jones (r-nc) intro-duced companion legislation in the house of representatives in previous congresses andhas been a long time champion of protecting the corolla wild horse population..reasoning process: the corolla wild horses protection act will make people to protectthe wild horses (forecasting skills - causal relations).
if people start to protect the wildhorses from january, the wild horses will be found in september (forecasting skills - in-ferring based on past events - we can ﬁnd the answer from this part).
horse is an animal(commonsense - world knowledge).
the outer banks of north carolina = north carolinaand the outer banks (language understanding - paraphrase)..table 7: detailed example to show how to solve a ques-tion..figure 10: date distribution of gold articles for ques-tions.
each question is made from gold articles.
thedates denote release dates of news articles and theyrange from 01-01-2019 to 11-31-2019..average question length (tokens)average answer length (tokens)# of distinct words in questions# of distinct words in choices# of distinct time stamps associated w. questionsaverage gold article length (# tokens)maximum question time stampminimum question time stamp.
value.
13.852.4617,5215,187218720.212019-11-222019-01-01.table 8: statistics of forecastqa..c additional reasoning types.
figure 11 shows additional reasoning types.
language understanding.
we introduce lexicalvariations and syntactic variations following ra-jpurkar et al.
(2016, 2018).
lexical variations rep-resent synonyms or coreferences between the ques-tion and the evidence sentence.
when the questionis paraphrased into another syntactic form and theevidence sentence is matched to the form, we callit syntactic variation.
we ﬁnd that many questionsrequire language understanding; lexical variationsaccount for 46% and syntactic variations do for66%.
multi-hop reasoning.
some questions requiremulti-hop reasoning (yang et al., 2018), such aschecking multiple properties (9%) and bridge enti-ties (5%) .
the former one requires ﬁnding multipleproperties from an article to ﬁnd an answer.
thelatter one works as a bridge between two entities,where one must identify a bridge entity, and ﬁndthe answer in the second hop.
numerical reasoning.
to answer our questions,one needs numerical reasoning (dua et al., 2019).
the answer is found by adding or subtracting twonumbers (5%), or comparing two numbers (8%) inthe given articles.
commonsense reasoning.
the questions requireworld knowledge (talmor et al., 2019), social com-monsense (sap et al., 2019), and temporal com-monsense (zhou et al., 2019).
to solve these ques-tions, an ai agent must leverage assumed commonknowledge in addition to what it ﬁnds in the newscorpus.
we ﬁnd that 36% questions need worldknowledge and 7% questions require social com-monsense.
the other type of commonsense rea-soning is temporal commonsense, which is relatedto temporal knowledge (zhou et al., 2019).
9%questions are related to temporal commonsense..d statistics.
figure 11: examples of each type of reasoning inforecastqa.
words relevant to the correspondingreasoning type are bolded.
also, [%] represents the per-centage of questions that requires the reasoning type..tion does not have a temporal phrase, then we ﬁlterout the question..b example of reasoning.
table 7 shows an example of reasoning process tosolve a question..tables 8 and 9 show the statistics and answer typesin forecastqa..4648sentencequestionq: how long will mexican asylum seekers be held in the us by april 2019?
reasoningdetailed reasoning typesen.
: the cases were those of migrants who claimed asylum at the us-mexico border.language understanding [91%]lexical varia2ons (synonymy, coreference) [46%]q: which country’s weapons will be used in the attack on saudi oil sites by september 2019?sen.
: weapons in attack on saudi oil sites were iranian.syntac2c varia2ons (paraphrase) [66%]q: how old will coco gauff be in july 2019?
sen.1: cori ‘coco’ gauff is 15 on june 27th, 2019. sen.2: cori gauff is 14 on october 31st, 2018.mul2-hop reasoning [14%]checking mul2ple proper2es [9%]q: which county police officer will be charged with killing an unarmed naked man in october 2019?sen.1: a jury will decide the fate of a former police officer charged with murder for killing an unarmed black man.
sen.2: jurors on friday began deliberating the case against former dekalb county, georgia, police officer robert "chip" olsen.bridge en2ty [5%]addi2on, subtrac2on [5%]q: how long will xiyue wang remain behind bars in iran from august 2019?sen.
: he was sent to iran’s notorious evin prison and sentenced to 10 years in august 2016.comparison [8%]q: who will launch $1000+ per night luxury rental tier in june 2019?sen.
: airbnb is selling $5,000 rafting tours and other adventures.numerical reasoning [12%]commonsense reasoning [47%]world knowledge [36%]q: when will summer end by september 2019?sen.
: labor day weekend informally ends summer.
knowledge: labor day is in september.social commonsense [7%]q: where will washington travel to for sunday's game in october 2019?sen.
: washington mystics star elena delle donne has a small disk herniation in her back, and it is unclear whether the league mvp will be able to play in game 3 of the wnba finals on sunday in connecticut.
social commonsense: game will be held in connecticut  → washington will move there.temporal commonsense [9%]q: which musical artist is going to have a single called “you need to calm down” in august 2019?sen.
: taylor swift has released her new song, “you need to calm down” in june.
answer type.
% examples.
56.8% -.
yes/nopersongroup/orglocationdate/timenumberother entitycommon nounphraseverb phraseadjectivephrase.
sentence.
8.1% boris johnson, mark zuckerberg5.8% bbc, united nations, eu8.0% canada, iran, u.s.1.6% january, july6.7% 530, thirty eight1.1% boeing 737.
5.8% a hurricane, asteroid dust.
3.1% defend his innocence.
1.4%.
1.6%.
cruel and misguided, due to thebad weatherliverpool will become the ﬁrstenglish team to play their 400thinternational game..table 9: types of answers in forecastqa..e experiments.
e.1 details on a text encoderwe use huggingface’s codes11.
we chose the bestlearning rate among {3e−5, 1e−5, 5e−6} and thenumber of epochs is 3. we set the max sequencelength to 512..e.2 details on ir methods.
we index the english news articles with elastic-search (gormley and tong, 2015).
we followed thesetups in qi et al.
(2019).
we use elasticsearch’ssimple analyzer which performs basic tokenizationand lowercasing for the title.
we use the standardanalyzer which allows for removal of punctuationand stop words from the body of articles.
at re-trieval time, we use a multi match query in theelasticsearch against all ﬁelds with the same query,which performs a full-text query employing thebm25 ranking function (robertson et al., 1995)on all ﬁelds, and returns the score of the best ﬁeldfor ranking.
to promote documents whose titlematches the search query, we boost the search scoreof any result whose title matches the search queryby 1.25, which results in a better recall for entitieswith common names..e.3 details on baselines..we consider following baselines: (1) event-basedapproaches: we test event-based approach, bertwith event triples (two entities and a relation be-tween them) and bert based on sam-net (lvet al., 2019) for our setup.
it is non-trivial to applythe event-based approaches to our setup.
thus, wepreprocess the retrieved news articles into event.
11https://github.com/huggingface/.
transformers.
triples (subject, relation, object) using liu et al.
(2019a).
we simply regard them as text, we con-catenate the triples, and feed them into bert andcall it bert with event triples.
in addition, weapply a script learning approach (sam-net (lvet al., 2019)) to our setup.
a question and choicesare not used in their original method; thus we en-code them using bert and concatenate the en-codings with the approach’s ﬁnal representation.
this representation is fed into a linear layer andthe linear layer predicts whether the choice is cor-rect or not.
we used bertlarge for the formerone and bertbase for the latter one.
(2) esim(chen et al., 2017b).
an nli model, where wechange their output layer so that the model outputsprobabilities for each answer choice with a softmaxlayer.
we use elmo (peters et al., 2018) for wordembeddings.
(3) bidaf++ (clark and gardner,2018).
the model requires context, and thus weuse a top-1 article by an ir method.
we augmentit with a self-attention layer and elmo representa-tions.
to adapt to the multiple-choice setting, wechoose the answer with the highest probability.
theinput to esim is a question and a set of choices(q, c), while that of bidaf++’s is a question, aset of choices, and retrieved articles (q, c, a).12.e.4 time modeling.
we conduct an ablation study on modeling timeinformation of the retrieved articles.
we test the fol-lowing models: a) pre-pending date string as bert[[cls]q[sep]c[sep]date[sep]ai],inputwhere the date format is “yyyy-mm-dd”, b) us-ing binary encodings of dates: we ﬁrst encode thetime into a binary encoding using “temporenc13”and concatenate the encoding with an article encod-ing before aggregation, c) using char-rnn (goyaland durrett, 2019) for encoding date string beforeaggregation..e.5 experiments with recent lms..as mentioned in sec 5, we did not reportmore recent pre-trained language models (e.g.,roberta (liu et al., 2019b), albert (lan et al.,2020)) because they are trained using text data pub-lished after the earliest timestamp in our dataset.
12we did not include existing event forecasting meth-ods since they are designed for modeling structured eventdata (fawaz et al., 2019) and thus are not directly applicableto forecastqa which requires modeling of unstructuredtext..13https://temporenc.org.
4649methods / metrics.
accuracy.
yes/no.
multi.
bertbase, agg (gru)robertabase, agg (gru)albertbase, agg (gru).
bertlarge, agg (gru)robertalarge, agg (gru)albertlarge, agg (gru).
human performance.
67.669.367.4.
69.270.168.4.
81.3.
41.544.823.4.
47.551.330.2.
77.4.all.
55.457.946.9.
59.161.350.6.
79.4.table 10: results on different pre-trained languagemodels, bert, roberta, albert)..methods / metrics.
accuracy (%, ↑).
brier score (↓).
yes/no multi.
all.
yes/no multi.
all.
bertlarge, agg (gru)bertlarge, gru(a), qc.
69.267.8.
47.542.5.
59.156.0.
0.4830.583.
0.6550.758.
0.5630.665.table 11: performance of baseline models on fore-castqa test set..methods / metrics.
bertbase− question− article− evidence sentence.
accuracy (%).
brier score.
yes/no multi.
all.
yes/no multi.
all.
65.678.181.4.
43.784.890.5.
55.481.285.6.
0.5060.3510.324.
0.6980.2100.147.
0.5960.2850.241.table 12: results on gold articles on the dev set.
wegive different inputs to the bert to ﬁnd out which partis important for the questions..(2019-01-01).
we are worried that these modelsin theory would have access to information thatwas published after the associated timestamp of aquestion..as a reference, we show the results of robertaand albert in table 10. even though thesetwo models may violate our forecasting scenario,they still struggle when compared to human perfor-mance, suggesting that our task is still challenging..e.6 experiments with different gru.
architectures..we investigate gru modeling for the input.
bertlarge gru(a), qc refers to a model thatencodes each article with a text encoder, these en-codings are fed into gru, and concatenate the lasthidden representation of gru and q,c (questionand choice) encoding from the text encoder.
ta-ble 11 shows comparison between the two architec-tures.
separating the articles with the question andchoice leads to the worse performance..e.7 error analysis.
we randomly select 50 errors made by the bestbaseline method from the test set and identify 4phenomena:retrieving wrong articles.
28% of the errors arefrom the retrieval of irrelevant articles.
the base-.
figure 12: examples of erroneous model predictions.
bold choices are actual answers and red choices aremodel predictions..line approach relies on information retrieval meth-ods such as bm25.
retrieved articles might not berelevant or contain facts that can confuse the model,thus causing incorrect predictions.
for example,consider the ﬁrst question in fig.
12, the modelhas retrieved an irrelevant article and conﬂated ms.merkel’s health with policy decisions.
this resultsin the model incorrectly choosing health care asthe appropriate answer..incorrect use of relevant evidence.
24% of theerrors are (partially) caused by incorrect usage ofrelevant evidence.
even though useful articles areretrieved, the model incorrectly reasons over the ev-idence.
take the second question in fig.
12, wherethe model incorrectly predicts no.
the model maydepend on a relevant, but outdated fact from 2018(one year before the event in question) to answerthe question, and failed to incorporate more recentinformation..lacking human common sense.
32% of the er-rors are from the model’s lack of common sense orworld knowledge.
an example question is, “whowill host 2020 olympics by july 2019?,” where theanswer is japan, but the model predicts hong kong.
to answer this question, a model must know thecities of each country, as without this knowledgethe model does not know that “tokyo is in japan,”and thus the model predicts the wrong answer..numerical questions.
8% of the errors are fromnumerical questions.
numerical questions askabout numbers such as a person’s age.
for example,“what will be roger federer’s age by august 2019.”the model must know his birth month and age andknow how to increment on one’s birthday..4650q: what will angela merkel's government agree to support a $60 billion package for in september 2019?
(7/20/19)angela merkel has sought to dispel lingering doubts about her health by insisting that she is capable of doingher job until her term finishes in 2021.
… “i also have a strong personal interest in my own health,” she said.a) climate polices [26.80%]b) infrastructure [20.45%]c) immigration polices [23.96%]d) health care [28.79%]q: will the new york giants defeat the washington redskins in october 2019?
(10/29/18)in the gray, cinder-blocked visitors' locker room far beneath the metlife stadium stands, washington redskins left tackle trent williams stood in front of the team before sunday's 20-13 victory over the new york giants and talked about the hurt.
yes [14.88%]/no  [85.12%]