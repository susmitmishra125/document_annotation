automated generation of storytelling vocabulary from photographs for use in aac .
mauricio fontana de vargas and karyn moffatt school of information studies mcgill university, montreal, canada mauricio.fontanadevargas@mail.mcgill.ca, karyn.moffatt@mcgill.ca  .
abstract .
research on the application of nlp in symbol-based augmentative and alternative commu-nication (aac) tools for improving social in-teraction  support  is  scarce.
we  contribute  a novel  method  for  generating  context-related vocabulary from photographs of personally rel-evant events aimed at supporting people with language impairments in recounting their past experiences.
performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs.
in comparison to a baseline generation composed of frequent english  words,  our  method  generated  vocab-ulary  with  a  4.6  gain  in  mean  average  preci-sion, regardless of the level of contextual infor-mation  in  the  input  photographs,  and  6.9  for photographs  in  which  contextual  information was extracted correctly.
we conclude by dis-cussing how our fndings provide insights for system optimization and usage.
.
1  introduction .
augmentative  and  alternative  communication (aac) tools can enhance communication for non-speaking individuals, thus offering improved social interaction  and  independence.
well  established nlp  techniques,  such  as  spell  check  and  word prediction, support those with primarily physical barriers to communication (e.g., adults with als) to  compose  complex  and  nuanced  sentences  in orthographic-based systems more effciently.
how-ever,  those with developmental disabilities (e.g., autism spectrum disorder, asd) or lexical and se-mantic processing impairments that limit their abil-ity to spell out words (e.g., adults with aphasia1) must usually rely on less expressive symbol-based systems, for which those techniques offer little sup-.
1a language disorder mostly often caused by a stroke.
.
port due to unique characteristics of communica-tion with these systems.
.
users  of  symbol-based  aac  typically  do  not construct  full,  grammatically  correct  sentences, complete  with  prepositions  and  infections,  but rather  often  only  need  a  few  key  content  words (i.e., nouns, adjectives, verbs)—appearing at any part of the sentence— to supplement other forms of communication, including preserved speech, ges-tures, or drawings.
such scattered use of vocab-ulary hinders the typical statistical prediction ap-proach, which relies on patterns learnt from a large training corpus.
.
nonetheless, there is much opportunity for im-proving symbol-based aac, which is often aban-doned because it offers too little communication support relative to the effort required to learn and use  (moffatt et al., 2017).
.
figure  1:  an  aac  app  design  demonstrating  how context-related  vocabulary  generated  by  our  method might be presented for use in subsequent conversations.
as in many non-orthographic aacs, vocabulary is rep-resented by images that reproduce computer generated speech when selected;  however, unlike the status quo, this design eliminates navigation across complicated hi-erarchies and the need for pre-programming.
.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1353–1364august1–6,2021.©2021associationforcomputationallinguistics1353selecting  and  organizing  vocabularies  able  to attend user’s communication needs in a wide vari-ety of contexts and such that they can fnd words quickly  is  one  of  the  major  challenges  (van  de sandt-koenderman, 2004; bailey et al., 2006).
al-phabetical organizations are not useful, and tradi-tional hierarchical schemes based on abstract cate-gories (e.g., food →  apple) are diffcult for people with language impairments, making navigation ex-tremely slow for anything but the smallest (least useful) vocabularies.
presenting vocabulary as a fat hierarchy is best (beukelman et al., 2015;  brock et  al.,  2017;  wallace  and  hux,  2014);  however, only a very limited set of options can be displayed, making communication very reliant on having the desired keywords among the available options.
.
providing concise situation-relevant vocabular-ies currently depends on support from a clinician or caregiver to pre-program the device.
but such support is often limited or not available, which con-sequently limits these devices to supporting generic expressions  of  wants  and  needs,  i.e.,  functional communication, and not for social interactions in-volving spontaneous narratives (waller, 2019).
.
generating vocabulary from user’s contextual data through natural language generation (nlg) techniques  seems  an  obvious  venue  to  facilitate social interactions.
although nlg has been suc-cessfully  applied  in  the  context  of  task-oriented dialogs  (he et al., 2017), question answering (su et  al.,  2016),  text  summarization  (see  et  al., 2017),  and story generation from photograph se-quences  (hsu et al., 2020), it is unclear how these techniques can be adapted to the specifc needs of aac support (tintarev et al., 2014).
.
in this paper, we call for more research in the nlp community devoted to language generation for symbol-based aac systems.
we present an overview of the scarce research on the topic and contribute a method that generates vocabulary au-tomatically from a user’s photographs to support autobiographical storytelling, demonstrating how it performs under different combination of the sys-tem’s controllable parameters and a wide range of input photographs.
.
2  background and related work .
2.1  nlp on orthographic aac systems .
nlp  research  on  aac  systems  has  mainly  fo-cused  on  improving  the  communication  rate  of orthographic-based tools,  primarily via attempts .
to reduce keystrokes with letter, word, or message prediction, applying n-grams language models on the user input (swiffn et al., 1985;  garay-vitoria and abascal, 2006; fazly and hirst, 2003; trnka  et al., 2007; trnka and mccoy, 2008).
researchers have also explored techniques for improving predic-tion by including in the language model, some sort of contextual information, such as the topic of con-versation  (lesher and rinkus, 2002; trnka et al., 2006), the user’s location (garcia et al., 2015), their past utterances  (kristensson et al., 2020; copestake, 1997;  wandmacher et al., 2008), or their partner’s speech  (wisenburn and higginbotham, 2008).
vir-tually all commercial text-based high tech aac de-vices employ some form of n-gram prediction (hig-ginbotham et al., 2012).
.
2.2  the need for symbol-based aacs able .
to support social interactions .
many people with developmental (e.g., asd) or acquired disabilities have diffculty using written language,  and therefore need support other than orthographic-based aac.
people with expressive aphasia, for example, present lexical and semantic processing impairments that affect their ability to retrieve the names of objects, combine linguistic elements, and use grammar.
nonetheless, they usu-ally have good receptive communication skills and intellectual abilities preserved, and typically desire the ability to communicate complex ideas and share social stories spontaneously, such as describing a recent activity or experience (garrett, 2005)2. .
to support this population, researchers from the clinical community (mckelvey et al., 2010; dietz et  al.,  2006;  mckelvey  et  al.,  2007;  beukelman et al., 2015) have successfully explored the presen-tation of vocabulary associated with personally rele-vant and highly contextualized photographs, where people, objects, and activities are depicted in their naturally occurring contexts (also known as visual scene displays, vsds).
evidence indicates greater conversational turn-taking with fewer instances of frustration  and  navigational  errors  (brock  et  al., 2017), and increased lexical retrieval during activ-ity retell  (mooney et al., 2018), for which partici-pants perceived this kind of support as very helpful.
however, the automation of the language pro-duction process to support those social narratives is still highly unexplored.
for example, mooney .
2we also witnessed this in interactions observed in con-versation groups at a local aphasia institute in which the frst author participated for 9 months.
.
1354et al.’s system cochat (2018) generates keywords from human input simulating social network com-ments.
nlp was used only to clean the input and identify nouns and frequent words.
in consequence, available commercial tools3  depend on human ef-fort planning and programming relevant vocabu-lary, leading to lack of spontaneous and indepen-dent communication, and requiring a great amount of time from caregivers (drager et al., 2019).
.
scenario (e.g., school), demmans epp et al.
(2012) explored the use of information retrieval algorithms on  internet-accessible  corpora  such  as  websites, dictionaries,  and  wikipedia  pages  related  to  the user’s current location or conversation topic.
al-though this approach was useful for augmenting a base vocabulary with context-specifc terms, it is limited to locations (e.g., retail locations) for which internet-accessible corpora are likely to exist.
.
2.3  nlg for aac systems .
3  vocabulary generation method .
generating language for aac systems is highly different from typical nlg usage, mainly because the goal of aac is to provide support for communi-cating users thoughts, and not to replace the user by an automatic communicator (tintarev et al., 2014).
the compansion system (demasco and mccoy, 1992;  mccoy  et  al.,  1998),  was  one  of  the  frst attempts to apply nlg towards that goal.
it was de-signed to produce grammatically correct sentences from incomplete user input using a small domain model.
although compansion was dedicated to functional  communication,  its  concept  of  using a domain knowledge served as a stepping stone to dempster et al.’s system aimed at generating conversational utterances (2010).
in their proto-type, users populated a personal knowledge base by recording where, when, and with whom they per-formed an activity shortly after its end.
through a template-driven system, users’ knowledge was converted into conversational utterances organized on topics that could be accessed during subsequent conversations.
this  work  showed  promising  re-sults on how nlg can be able to support social dialogues and increase participation of aac users.
however, their system still required considerable manual linguistic input from users.
.
automatic generation of storytelling vocabulary has been successfully explored by researchers (re-iter, 2007; black et al., 2010; tintarev et al., 2016) to support children with limited memory or with physical and intellectual impairment telling "how was school today" to their parents.
in their project, raw sensor data from passive rfid tags relating to locations, objects, and people was aggregated into events, and then transformed to coherent personal narratives using a domain knowledge containing the school timetable and the rfid tags mapping.
to provide just-in-time vocabularies that attend to  emergent  needs  and  are not  tied to  a  specifc .
3e.g., tobii dynavox snap scene .
our  method  generates  a  rank  of  key  words  and short narrative phrases from a single4  input photo for scaffolding storytelling.
it was designed to be used as the back end of interactive aac systems in which relevant vocabulary is associated with a main photograph, such as mooney et al.’s cochat, or as in the example design shown in fig.
1. .
we used  vist-train, a sub-set of the visual storytelling dataset vist  (huang et al., 2016) as the main source for vocabulary generation.
vist-train encompasses 80% of the entire dataset, and is composed of 65,394 photos of personal events, grouped in 16,168 stories.
each photo is annotated with descriptions and narrative phrases that are part of a story,  created by amazon mechanical turk workers.
we judged vist to be a good source of vocabulary because i) photos were extracted from personal flickr albums on a wide range of “sto-ryable” events, related to 69 topics (e.g., graduation, building a house), ii) associated vocabulary is rep-resentative of storytelling and, iii) stories and photo descriptions were constructed by a large number (1907) of workers under a rigorous procedure.
.
the generation process is composed of fve steps, as detailed below and illustrated in fig 2.  we ex-plore  different  implementations  for  some  of  the steps, represented by the system’s controllable pa-rameters emphasized with bold italic formatting throughout the paper.
the different combination of those parameters are evaluated in the next section.
.
3.1  scene understanding .
the frst step extracts contextual information from the photograph in the form of a high-level, human-like description of the scene (i.e., caption) using the computer vision technique from fang et al.
(2015).
captioning was chosen over pure object detection and labelling due to the necessity of communicat-.
4to reduce the requirements on users, who may feel dis-.
couraged if multiple photos of the event are needed .
1355figure 2: our method.
words and phrases highlighted in red are generated from the input photograph.
.
ing more abstract concepts such as the actions be-ing performed and the interactions between the ob-jects, people, and environment during storytelling.
.
3.2  photo description matching .
this step fnds the subset of vist-train photos most similar to the user input by calculating the sentence  similarity  between  the  input  photo  de-scription and all vist-train photos descriptions.
all photos with description similarity higher than the  parameter  similarity   threshold   are  selected for processing in the next step, with an upper limit of 30 photos.
sentence similarity is defned as the soft cosine similarity (sidorov et al., 2014)5  on a bag-of-words representation of the sentences us-ing word2vec embeddings,  after removing stop words6.
soft cosine was chosen as similarity mea-sure due to its ability to capture the semantic re-latedness between different words.
this strategy was  motivated  by  the  fact  that  soft  cosine  sim-ilarity  with  word2vec  was  effective  for  fnding similar sentences on question-answering systems, achieving  the  best  performance  at  the  semeval-2017 task 3 (charlet and damnati, 2017).
similar-ity based on entire documents (e.g., doc2vec) was .
5gensim library implementation 6as defned by the natural language toolkit (nltk) .
not used because it would require a much larger (at present, nonexistent) training corpus to create proper document embeddings, and there are no pre-trained sentence embeddings trained exclusively on photo descriptions.
.
3.3  stories retrieval .
all narrative sentences associated with the selected photos are retrieved for processing in the next stage.
the number of sentences per photo varies from 1 to 5 (µ  = 3.1, σ  = 1.4).
.
3.4  vocabulary selection .
this step identifes a group of representative sen-tences  and  words  from  the  retrieved  set  by  ap-plying the affnity propagation7  clustering (frey and dueck, 2007)—able to generate clusters with less  error  than  other  exemplar-based  algorithms and not requiring a predetermined the number of clusters.
the  fnal  set  of  generated  phrases  is formed  by  these  cluster’s  exemplars,  ranked  ac-cording to their respective clusters size.
by def-nition, this strategy results in phrases covering the wide range of semantics present in the set of re-trieved phrases, while at the same time removing redundant (i.e., very similar) phrases.
in case of .
7damping: 0.5, max.
iter: 200, convergence iter.
: 15 .
1356we had birthday cake, therewas so many candlesand he loves chocolate cake sothat's what i madecakecandlefamilybirthdaywishhappyeveryoneenjoysurprisebleweatcelebratebirthdaypresentageballoona man sitting at atable with a birthdaycake with lit candlesuser input photocreatedescriptionvistdescriptionscalculate similaritybetween desc.most similar  photos from vistviststoriesget associatedvist stories 1. scene understanding2.
photo description matchinghe couldn't wait to blow hiscandlesmost frequentwords3.
stories retrieval lotmadepartybaketooksangeveryone sang happybirthday to himthe birthday cake came outand the night beganeveryone clapped andcheered5.
voc.
expansionswowget mostassociatedcluster similarstoriescakechocolatesweeteatfamilylovefriendschildrencandlelightwaxflame............chocolate birthday cake with lots ofcandles4.
vocabulary selectionlevel of contextual informationsimilarity thresholdselection methodexpansion sizeall_phrasesexemplarsdescription qualitycontrollable parameters under studyuncontrollable parameters under studythe birthday cake came outand the night beganit was her birthday so webaked her a nice cakei had a lovely cake on mybirthdaynon-convergence (< 3% in our evaluation), the set of recommended phrases is formed by ranking all phrases according to the sum of their soft cosine similarity against all other phrases retrieved.
the generated base vocabulary is formed by a rank of the word frequencies after fltering-out stop words and applying a porter stemmer to merge different variations (e.g., worked, working →  work).
the parameter selection  method  determines whether frequencies are calculated considering all retrieved phrases (all_phrases) or only clusters’ exem-plars (exemplars).
.
3.5  vocabulary expansion .
the goal of this step is to diversify the base vocab-ulary derived from vist-train to increase com-munication  fexibility.
thus,  to  fnd  words  that are related to, but distinct from the initial concept (e.g., cake →  sweet), our method uses a model of the human mental lexicon as a secondary source of vocabulary.
in this model, swow  (de deyne et al., 2019), words are connected with a certain strength representing their relatedness constructed from data of word-association experiments of over 90,000 participants.
therefore, unlike embeddings, swow encodes mental representations free from the basic demands of communication.
.
this  strategy  was  motivated  by  the  fact  that word  association  data  was  successfully  applied in  a  controlled  study  to  support  people  with aphasia navigating related words more effectively (nikolova  et  al.,  2010),  and  that  evidence  from cognitive science research indicates that the net-work formed by associations in  swow  presents a widespread thematic structure, rather than taxo-nomic, with words strongly associated often occur-ring in the same situation (e.g., pick-strawberry; candle-church) (de deyne et al., 2015) .
this last step expands the initial set of base vocabulary by adding, for each word, the most strongly associ-ated words in swow data.
the system parameter expansion  size  determines how many words from swow are added for each word in the base vocab-ulary set.
repeated words are not included.
.
4  evaluation experiment .
the goal of our evaluation is to understand how our design choices, represented by the system control-lable parameters,  along with uncontrollable fac-tors related to the input photograph (i.e., uncontrol-lable parameters), affect the system’s performance.
.
thus,  we compared the relevance of vocabulary generated  under  different  combinations  of  these parameters to investigate the following specifc re-search questions: .
1. what combination of controllable system pa-rameters related to the base vocabulary gener-ation optimizes performance?.
2. how does the level of contextual information.
in the input photo affect performance?.
3. how  does  the  quality  of  the  contextual  de-scription inferred from the input photo affectperformance?.
4. how does the level of contextual informationin  the  input  photo  affect  the  quality  of  theinferred description?.
5. what is the effect of expanding the base gen-erated vocabulary with words from a mentallexicon model on the system’s performance?.
4.1  performance metrics .
considering the aac application usage scenario, the performance of vocabulary generation can be conceptualized by the combination of two factors: i) communication fexibility, i.e., whether vocabu-lary needed for composing messages about a spe-cifc experience is provided, and ii) communicationease, i.e., the diffculty in fnding a particular wordamong all options generated.
these two factors di-rectly map to the information retrieval concepts ofprecision (p  ) and recall (r) as a perfect algorithmwould provide all words the user needs to commu-nicate the desired message (r  = 1), and would notcontain any irrelevant vocabulary (p   = 1), therebyminimizing the need for scanning.
in contrast, theworst algorithm would provide only irrelevant vo-cabulary (p   =  r  = 0)..therefore, we tackle the vocabulary generation evaluation  as  an  information  retrieval  problem, where the input photo is treated as the user query, generated words and phrases are treated as retrieved documents, and crowd sourced narrative sentences about the photograph are the relevant documents, i.e., ground truth (as detailed in section 4.2).
foreach input photo, diffculty in fnding vocabularyand communication fexibility are operationalizedas p   and r, respectively:.
p (n) =  .
.
|{rel_words} ∩ {gn}|n  .
r(n) =  .
|{rel_words} ∩ {gn}||{rel_words}|  .
1357where n  is the number of words displayed to the user, rel_words  are the words in the groundtruth sentences, and gn   are the top n  words in the gen-erated  vocabulary  rank.
we  also  calculated  the f1, a common information retrieval measure that captures the trade-off between p   and r: .
f1(n) = 2 ×  .
.
.
p (n)  ×  r(n)  p (n) + r(n)    .
we calculated these metrics for all n  ∈  [1,  100], and  constructed  the  p-r  curves  with  the  arith-metic  mean  values  of  p  ,  r,  and  f1   across  all input photographs under analysis.
in contrast to bleu/meteor metrics, this analysis allows us to clearly demonstrate trade-offs between the dif-fculty fnding a word among options and commu-nication fexibility, which is important because the number of displayed items will vary for each user.
to  obtain  a  single  measure  of  system  perfor-mance across this entire interval, considering all input photos, we approximate the area under the p-r curves by calculating the mean average preci-sion:.
map  =   .
100 x p (n)(r(n) − r(n − 1))   n=1  .
.
.
.
.
4.2  data .
as input photographs and groundtruth sentences, we  used  vist-val,  a  sub-set  of  vist  not  em-ployed in our method that contains 8034 photos aligned with crowd sourced stories.
we selected all  photos  from  vist-val  containing  the  maxi-mum number of sentences available (5) to act as our input photographs,  resulting in 1946 photos.
the ground-truth vocabulary for each photograph was formed by joining the fve associated narrative phrases (9730 in total), after removing stop words.
.
4.3  specifc procedures .
controllable parameters - base vocab.
(rq1).
we defned four confgurations of parameters by crossing two extreme values of similarity  thresh-old,  i.e.,  0  and  best  (highest  similarity  score among all vist-val) with the selection  method  all_phrases and exemplars, resulting in four con-fgurations: 0_all, 0_exemplars, best_all, best_exemplars.
expansion  size  was set to 0 in all confgurations.
in the absence of similar aac generation systems to compare our method to, we created a baseline generation formed by a .
rank of the most frequent words from the corpus of contemporary american english (coca) (davies, 2009) without stop words.
we adopted this baseline because current aac tools are commonly built on word usage frequency data (renvall et al., 2013).
the  optimal  values  for  the  parameters  estab-lished in this analysis were applied in subsequent analyses.
.
contextual  information  level  (rq2,  rq4).
to investigate the variability caused by different in-put photographs, we adopted the concept of context richness from beukelman et al.
(2015).
the frst author scored each photo from 0–3 based on the number of contextual categories (environment, peo-ple/object, activity) it clearly depicts (0 when am-biguous).
to validate these annotations, someone unfamiliar with the study also scored a subset of 514 photos (27.8% of the dataset)8. krippendorff’s alpha reliability score was 0.82, indicating strong agreement between raters (krippendorff, 2004).
.
context  description  quality  (rq3,  rq4).
the frst author scored each photo description from 0 to 3 as follows:  0) not generated or completely unrelated; 1) misses most important elements or contains most of important elements and a few un-related  elements;  2)  contains  most  of  important elements or all important elements and a few un-related elements; 3) contains all important elements in the photo and does not contain any unrelated ele-ments.
as for contextual information level, a subset of 514 were scored by someone unfamiliar with the study.
krippendorff’s alpha reliability score was 0.88, confrming strong agreement.
.
effect of vocabulary expansion (rq5).
we created 24 pairs of confgurations by combining different base vocabulary sizes (5, 10, 15, 20, 25, 30) with the expansion sizes (0, 1, 2, 3).
the confg-uration [5-2], for example, contains fve base wordsplus two expanded words per base word, resultingin a maximum of 15 words (or less if expandedwords were already in the base set)..4.4  results .
rq1.
to better illustrate the differences in per-formance, fig.
3 presents the p-r curves, while table 1 shows the map   and maximum p   and r  mean values for the pairs of parameters values un-der  investigation,  in  comparison  to  the  baseline.
overall,  0_all  results  in  the  best  performance, .
8all annotations are available at https://doi.org/  .
10.5683/sp2/nvi701  .
1358with an map   4.6 times greater than the baseline, and 1.8 greater than the the worst confguration, best_exemplars.
.
(p > .2).
however, photos with description qual-ity 3 signifcantly outperformed the other groups (p < .001), and quality 0 photos performed signif-cantly worse than all other groups (p < .001).
.
figure 3: p-r curves for different confgurations of sys-tem’s parameters, calculated for all n  ∈  [1,  100].
.
figure 4:  precision-recall curves according to context description quality, under the confguration 0_all.
.
confguration  map  map gain  max p  max r .
0_all 0_exemp best_all best_exemp baseline .
.058 .039 .042 .032 .013  .
4.61 3.10 3.35 2.52 1.00 .
.38 .34 .32 .27 .08  .
.36 .30 .33 .28 .20  .
table 1: performance under different confgurations.
.
rq2.
in  our  input  dataset,  the  proportion  of photos according to their context richness score was:  8%(0),  54%(1),  30%(2),  8%(3).
a  mann-whitney u test indicated a signifcant difference on p  and r  only between photos with context richness 0 and the remaining levels (p < .002).
table 2 shows the mean performance metrics according to level of contextual information.
.
context level  map  map gain  max p  max r .
3 2 1 0 baseline .
.056 .060 .058 .045 .013  .
4.44 4.72 4.57 3.54 1.00 .
.43 .38 .38 .29 .08  .
.37 .36 .36 .23 .20  .
table 2:  mean performance according to the level of contextual information in the input photos.
.
rq3.
the distribution of input photos across context  description  quality  scores  was:  16%(0), 16%(1), 30%(2), 38%(3).
we plot the p-r curves according to the context description quality scores in fig.
4, and summarize performance metrics in table 3. a mann-whitney u test indicated no sig-nifcant differences between photo quality 1 and 2 .
descr.
quality  map  map gain  max p  max r .
3 2 1 0 baseline .
.086 .048 .045 .028 .013 .
6.86 3.77 3.57 2.21 1.00 .
.54 .34 .26 .14 .08  .
.41 .34 .33 .29 .20  .
table 3:  mean performance metrics according to the input photos’ description quality.
.
rq4.
fig.
5 illustrates the relationship between the  level  of  contextual  information  in  the  input photos and the quality of the photos descriptions generated using machine-learning.
.
(a) percentages relative to all photos (1946) .
(b) percentages relative tophotos with same context richness level .
figure  5:  distribution  of  input  photos  by  contextual richness level and generated description quality .
as expected, photos with ambiguous contextual information (level= 0) most often received bad cap-tions (53%).
as context richness increased, the rel-ative proportion of photos with good descriptions (scores 2 or 3) also increased (39%,  69%,  72%, .
13590.000.050.100.150.200.250.300.350.40recall0.000.050.100.150.200.250.300.350.40precision0_all0_exemplarsbest_allbest_exemplarsbaseline0.00.10.20.30.40.5recall0.00.10.20.30.40.50.6precisioncontext descr.
score 0 context descr.
score 1 context descr.
score 2 context descr.
score 3 baseline 0123context richness level0123context descr.
quality4.38.32.70.460.728.45.41.21.2121251.9259.31.50123context richness level0123context descr.
quality53159.15.78.9161814152341612446311980%), but the relative proportion of perfect descrip-tions (quality = 3) decreased (46%,  31%,  19%).
photos depicting only one type of contextual infor-mation (location, person/object, activity) resulted in the best descriptions: 46% received perfect de-scriptions, and 66% of all perfect descriptions were given to them.
however, when compared to photos with more contextual information, they presented the highest relative proportion of very bad captions (15% vs 9.1% and 5.7%).
.
figure  6:  comparison  between  generation  with  and without vocabulary expansion.
.
figure 7:  impact of the intersection between base and expanded vocabulary on performance.
.
rq5.
fig.
6 compares the performance of differ-ent combinations of base vocabulary and expansion sizes against base vocabulary only, in function of the number of words displayed n. in general, for a given n, generation without expansion resulted in superior performance.
however, on confgurations for  which  a  high  proportion  of  expanded  words were already in the base vocabulary (e.g., n  =  6, 21, 61), expansion presented similar or even better f1   scores than the base vocabulary on its own.
.
to better understand this phenomenon, we plot .
the f1  score, averaged across all photos, in function of the proportion of expansion words not present in the base vocabulary during generation (fig.
7).
the mean f1   for generation without word expansion is also plotted for comparison.
.
we found that word expansion is able to bring improvement in performance when less than 60% of the expansion words are included in the fnal gen-erated vocabulary, or in other words, when more than 40% of expansion words is already in the base vocabulary.
the  tendency  is  that,  the  lower  the proportion of expansion words not in the base vo-cabulary, the higher the performance.
.
5  discussion .
the design space for generating aac storytelling vocabulary directly from photographs is vast and under explored.
design decisions for individual system components will impact other components and ultimately the overall system effectiveness, and therefore cannot be arbitrary.
without a rigorous performance evaluation on different confgurations of parameters,  users would be at risk of using a fawed  or  under  optimized  system,  which  could lead to user frustration and abandonment, and cause confounds that obscure whether failures are due to  the  need  for  algorithmic  tuning  or  mismatch between the intended support and user needs.
.
the study of controllable parameters (rq1, 5) demonstrated that our method is able to provide relevant vocabulary, and showed how it can be used to optimize the system and identify areas for further improvement.
the exploration of uncon-trollable parameters (rq2, 3, 4) helped illustrate the likely variation in system performance during real world usage (i.e., wide variety of input photos), allowing us to better anticipate potential problems or pitfalls and understand requirements for use.
.
the similar performance across photos with dif-ferent levels of contextual information (rq2) sug-gests that our method is robust to variations in the input photograph.
users will not need to be instructed to take photographs following specifc requirements, e.g., “photos should demonstrate an action” or “photos should depict objects only”.
the similar levels of performance is explained by the pattern observed in the rq4 analysis; the more ele-ments a photo contains, the better knowledge the machine learning has to infer the central aspect of the photo, but at the same time, the harder it is to capture each and every element.
in addition, an .
1360020406080100num words displayed0.050.100.150.200.250.300.350.40precision w/o exp.recall w/o exp.
f1 w/o exp.precision w/ exp.recall w/ expf1 w/ exp0.40.50.60.70.80.91.0proportion of expanded words not in the base vocabulary0.080.100.120.140.160.18f1 w/o expansion (mean)f1 w/ expansiony=0.19x+0.26 (r2=0.91)element wrongly identifed will have less impact on the overall scene understanding since other el-ements complement the description.
an example would be a photo of a birthday party, in which the machine-learning platform is able to infer the cen-tral concept (birthday) from the several elements depicted (e.g., cake, candles, balloons), but misses some of the details (e.g.
drinks).
on the other hand, simplistic photos will rarely lead to elements being cut out, but the computer vision technique will have more variability when performing the inferences, leading to erroneous descriptions more often.
.
on the other hand, the quality of generated vocabulary was strongly dependent on the com-puter vision technique employed to extract con-textual information about the scene (rq3) .
when a wrong description is generated, the subsequent steps of the algorithm are misled and therefore gen-erate vocabulary less relevant for retelling the scene depicted in the photograph.
nonetheless, even in this case, an aac device using our method would provide vocabulary more relevant than if the most frequent english words were provided.
since pho-tos for which the computer vision technique was able to correctly identify all contextual elements resulted in substantial performance gain,  we en-courage further exploration of this component.
an option  would  be  to  use  a  higher  number  of  raw context labels instead of the single human-like de-scription employed in this work.
.
our vocabulary expansion analysis (rq5) pro-vide  valuable  insights  into  how  the  combina-tion  of  multiple  lexicon  sources  can  generate more relevant vocabulary.
the most promising approach  was  to  combine  the  visual-to-story dataset with strongly associated words from a mental-lexicon model, but only when there was high intersection between the two vocabularies.
.
5.1  limitations and future work .
although  vist  contains  a  very  large  range  of events, one limitation is that it is unlikely to cover all possible scenarios, and may not accurately re-fect  aac  communication.
however,  in  the  ab-sence of an appropriate aac-specifc corpora (a known  issue  in  the  community),  we  believe  the vist  dataset can meaningfully represent the vo-cabulary  needed  for  scaffolding  storytelling.
in addition, we do not expect the performance gains observed will directly translate to the same gains in usability.
our goal was to understand fundamental .
questions necessary for advancing to a usability study, helping fne-tune system components before introducing them to users, avoiding unnecessary interactions with identifably poor designs.
our ap-proach also enables larger numbers of parameters to  be  examined.
the  low  level  of  social  partic-ipation  commonly  observed  among  people  with aphasia, combined with the rate-limited nature of aac, would require feld experiments lasting an impractical amount of time to produce suffcient data to comprehensively explore possible combina-tions of parameters (kristensson et al., 2020).
.
as  a  potential  improvement  to  our  method, sent2vec trained with bert may better represent sentence structure and words context for fnding similar photo descriptions in step 2 than our use of soft cosine with word2vec.
another option would be  the  use  of  query  expansion  to  enrich  the  de-scriptions.
we encourage the exploration of the vast array of strategies for tackling the vocabulary generation process for aac.
.
6  conclusion .
developing a photo-to-story vocabulary aac sys-tem presents two challenges;  a nlp one in how to  generate  such  vocabularies,  and  a  human-computer-interaction (hci) one in how to use such vocabulary to offer interactive language support.
in this work, we tackle the frst challenge.
.
we demonstrated that our method is able to gen-erate vocabulary with reasonable levels of recall and precision, regardless of the level of contextual information in the input photograph, illustrated the likely variation in system performance during real world usage, and provided meaningful insights for fne tuning the algorithm, enabling us to move to the next phase of designing and evaluating, with aac users, our mobile interactive application.
.
acknowledgments .
this  research  was  funded  by  the  fonds  de recherche  du  québec  - nature  et  technologies (frqnt), the natural sciences and engineering research council of canada (nserc) [rgpin-2018-06130], the canada research chairs program (crc), and by age-well nce, canada’s tech-nology and aging network.
.
1361references .
rita l bailey, howard p parettejr, julia b stoner, mau-reen e angell, and kathleen carroll.
2006.  family members’ perceptions of augmentative and alterna-tive communication device use.
language, speech, and hearing services in schools, 37(1).
.
david  r  beukelman,  karen  hux,  aimee  dietz, miechelle  mckelvey,  and  kristy  weissling.
2015. using visual scene displays as communication sup-port options for people with chronic, severe aphasia: a summary of aac research and future research di-rections.
augmentative and alternative communica-tion, 31(3):234–245.
.
rolf  black,  joseph  reddington,  ehud  reiter,  nava tintarev,  and  annalu  waller.
2010.  using  nlg and  sensors  to  support  personal  narrative  for  chil-dren  with  complex  communication  needs.
in  pro-ceedings  of  the  naacl  hlt  2010  workshop  on speech and language processing for assistive tech-nologies, pages 1–9.
.
kris brock, rajinder koul, melinda corwin, and ralf schlosser.
2017.  a comparison of visual scene and grid  displays  for  people  with  chronic  aphasia:  a pilot  study  to  improve  communication  using  aac.
aphasiology, 31(11):1282–1306.
.
delphine charlet and geraldine damnati.
2017.  sim-bow  at  semeval-2017  task  3:  soft-cosine  semantic similarity  between  questions  for  community  ques-tion answering.
in proceedings of the 11th interna-tional workshop on semantic evaluation (semeval-2017), pages 315–319.
.
ann copestake.
1997.  augmented and alternative nlp techniques for augmentative and alternative commu-nication.
in natural language processing for com-munication aids.
.
mark davies.
2009.  the 385+ million word corpus of contemporary american english (1990–2008+):  de-sign,  architecture,  and  linguistic  insights.
interna-tional journal of corpus linguistics, 14(2):159–190.
.
simon  de  deyne,  danielle  j  navarro,  amy  perfors, marc brysbaert, and gert storms.
2019.  the “small world of words” english word association norms for over 12,000 cue words.
behavior research methods, 51(3):987–1006.
.
simon de deyne, steven verheyen, amy perfors, and daniel  j  navarro.
2015.  evidence  for  widespread thematic structure in the mental lexicon.
in cogsci.
.
patrick  w  demasco  and  kathleen  f  mccoy.
1992. generating text from compressed input:  an intelli-gent interface for people with severe motor impair-ments.
communications of the acm, 35(5):68–78.
.
carrie  demmans  epp,  justin  djordjevic,  shimu  wu, karyn  moffatt,  and  ronald  m  baecker.
2012.  to-wards providing just-in-time vocabulary support for .
assistive and augmentative communication.
in pro-ceedings of the 2012 acm international conference on intelligent user interfaces, pages 33–36.
.
martin dempster, norman alm, and ehud reiter.
2010. automatic  generation  of  conversational  utterances and narrative for augmentative and alternative com-munication:  a prototype system.
in proceedings of the naacl hlt 2010 workshop on speech and lan-guage processing for assistive technologies, pages 10–18.
.
aimee  dietz,  miechelle  mckelvey,  and  david  r beukelman.
2006.  visual  scene  displays  (vsd): new aac interfaces for persons with aphasia.
per-spectives on augmentative and alternative commu-nication, 15(1):13–17.
.
kathryn  dr  drager,  janice  light,  jessica  cur-rall,  nimisha  muttiah,  vanessa  smith,  danielle kreis,  alyssa  nilam-hall,  daniel  parratt,  kaitlin schuessler,  kaitlin  shermetta,  et  al.
2019.  aac technologies with visual scene displays and “just in time”  programming  and  symbolic  communication turns  expressed  by  students  with  severe  disability.
journal  of  intellectual  &  developmental  disability, 44(3):321–336.
.
hao fang, saurabh gupta, forrest iandola, rupesh k srivastava, li deng, piotr dollár, jianfeng gao, xi-aodong  he,  margaret  mitchell,  john  c  platt,  et  al.
2015.  from  captions  to  visual  concepts  and  back.
in proceedings of the ieee conference on computer vision and pattern recognition, pages 1473–1482.
.
afsaneh  fazly  and  graeme  hirst.
2003.  testing  the effcacy of part-of-speech information in word com-pletion.
in proceedings of the 2003 eacl workshop on language modeling for text entry methods.
.
brendan j frey and delbert dueck.
2007.  clustering by passing messages between data points.
science, 315(5814):972–976.
.
nestor  garay-vitoria  and  julio  abascal.
2006.  text prediction systems:  a survey.
universal access in the information society, 4(3):188–203.
.
luís filipe garcia, luís caldas de oliveira, and david martins  de  matos.
2015.  measuring  the  perfor-mance  of  a  location-aware  text  prediction  system.
acm transactions on accessible computing (tac-cess), 7(1):1–29.
.
kathryn  l  garrett.
2005.  adults  with  severe  apha-sia.
in  david  r  beukelman  and  pat  mirenda,  ed-itors, augmentative and alternative communication for children and adults with complex communication needs, pages 467–504.
paul h. brookes, baltimore.
.
he he,  anusha balakrishnan,  mihail eric,  and percy liang.
2017.  learning symmetric collaborative dia-logue agents with dynamic knowledge graph embed-dings.
arxiv preprint arxiv:1704.07130. .
1362d jeffery higginbotham,  gregory w lesher,  bryan j moulton,  and  brian  roark.
2012.  the  application of natural language processing to augmentative and alternative  communication.
assistive  technology, 24(1):14–24.
.
chao-chun hsu, zi-yuan chen, chi-yang hsu, chih-chia li, tzu-yuan lin, ting-hao huang, and lun-wei  ku.
2020.  knowledge-enriched  visual  story-telling.
in proceedings of the aaai conference on artifcial intelligence, volume 34, pages 7952–7960.
.
ting-hao  huang, .
ferraro, .
nasrin francis mostafazadeh,  ishan  misra,  aishwarya  agrawal, jacob  devlin,  ross  girshick,  xiaodong  he,  push-meet  kohli,  dhruv  batra,  et  al.
2016.  visual storytelling.
in proceedings of the 2016 conference of  the  north  american  chapter  of  the  association for  computational  linguistics:  human  language technologies, pages 1233–1239.
.
klaus  krippendorff.
2004.  reliability  in  content analysis:  some  common  misconceptions  and  rec-ommendations.
human  communication  research, 30(3):411–433.
.
per ola kristensson, james lilley, rolf black, and an-nalu waller.
2020.  a design engineering approach for quantitatively exploring context-aware sentence retrieval for nonspeaking individuals with motor dis-in  proceedings  of  the  2020  chi  con-abilities.
ference  on  human  factors  in  computing  systems, pages 1–11.
.
gregory  w  lesher  and  gerard  j  rinkus.
2002. domain-specifc  word  prediction  for  augmentative communication.
in proceedings of the resna 2002 annual conference.
.
kathleen f mccoy, christopher a pennington, and ar-lene  luberoff  badman.
1998.  compansion:  from research prototype to practical integration.
natural language engineering, 4(1):73–95.
.
miechelle  l  mckelvey,  aimee  r  dietz,  karen  hux, kristy  weissling,  and  david  r  beukelman.
2007. performance  of  a  person  with  chronic  aphasia  us-ing personal and contextual pictures in a visual scene display prototype.
journal of medical speech lan-guage pathology, 15(3):305. .
miechelle l mckelvey, karen hux, aimee dietz, and david r beukelman.
2010.  impact of personal rele-vance and contextualization on word-picture match-ing  by  people  with  aphasia.
american  journal  of speech-language pathology.
.
karyn  moffatt,  golnoosh  pourshahid,  and  ronald  m baecker.
2017.  augmentative and alternative com-munication devices for aphasia:  the emerging role of “smart” mobile devices.
universal access in the information society, 16(1):115–128.
.
aimee mooney,  steven bedrick,  glory noethe,  scott spaulding, and melanie fried-oken.
2018.  mobile .
technology to support lexical retrieval during activ-ity retell in primary progressive aphasia.
aphasiol-ogy, 32(6):666–692.
.
sonya nikolova, marilyn tremaine, and perry r cook.
2010.  click on bake to get cookies:  guiding word-fnding with semantic associations.
in proceedings of the 12th international acm sigaccess confer-ence  on  computers  and  accessibility,  pages  155– 162. .
ehud  reiter.
2007.  an  architecture  for  data-to-text systems.
in proceedings of the eleventh european workshop on natural language generation (enlg 07), pages 97–104.
.
kati renvall, lyndsey nickels, and bronwyn davidson.
2013.  functionally relevant items in the treatment of aphasia  (part  ii):  further  perspectives  and  specifc tools.
aphasiology, 27(6):651–677.
.
mieke  van  de  sandt-koenderman.
2004.  high-tech aac and aphasia:  widening horizons?
aphasiology, 18(3):245–263.
.
abigail  see,  peter  j  liu,  and  christopher  d  man-ning.
2017.  get  to  the  point:  summarization with  pointer-generator  networks.
arxiv  preprint arxiv:1704.04368. .
grigori  sidorov,  alexander  gelbukh,  helena  gómez-adorno, and david pinto.
2014.  soft similarity and soft cosine measure: similarity of features in vector space model.
computación y sistemas,  18(3):491– 504. .
yu  su,  huan  sun,  brian  sadler,  mudhakar  srivatsa, izzeddin gür, zenghui yan, and xifeng yan.
2016. on generating characteristic-rich question sets for qa evaluation.
in proceedings of the 2016 conference on empirical methods in natural language process-ing, pages 562–572.
.
andrew l swiffn, j adrian pickering, john l arnott, and  alan  f  newell.
1985.  pal:  an  effort  eff-cient portable communication aid and keyboard em-in  8th  annual  conference  on  rehabilita-ulator.
tion  technology,  technology-a  bridge  to  indepen-dence.
resna’85.
memphis, tennessee, pages 197– 199.  rehabilitation  engineering  society  of  north america.
.
nava  tintarev,  ehud  reiter,  rolf  black,  and  annalu waller.
2014.  natural language generation for aug-in  natural mentative  and  assistive  technologies.
language generation in interactive systems, pages 252–277.
cambridge university press.
.
nava tintarev, ehud reiter, rolf black, annalu waller, and  joe  reddington.
2016.  personal  storytelling: using natural language generation for children with complex communication needs,  in the wild.
.
.
.
in-ternational  journal  of  human-computer  studies, 92:1–16.
.
1363keith  trnka  and  kathleen  f  mccoy.
2008.  evaluat-ing word prediction: framing keystroke savings.
in proceedings of acl-08:  hlt, short papers,  pages 261–264.
.
keith  trnka,  debra  yarrington,  john  mccaw,  kath-leen  f  mccoy,  and  christopher  pennington.
2007. the  effects  of  word  prediction  on  communication rate  for  aac.
in  human  language  technologies 2007: the conference of the north american chap-ter of the association for computational linguistics; companion volume, short papers, pages 173–176.
.
keith trnka, debra yarrington, kathleen mccoy, and christopher  pennington.
2006.  topic  modeling  in fringe word prediction for aac.
in proceedings of the 11th international conference on intelligent user interfaces, pages 276–278.
.
sarah e wallace and karen hux.
2014.  effect of two layouts on high technology aac navigation and con-tent location by people with aphasia.
disability and rehabilitation: assistive technology, 9(2):173–182.
.
annalu waller.
2019.  telling tales:  unlocking the po-tential of aac technologies.
international journal of language & communication disorders, 54(2):159– 169. .
tonio  wandmacher,  jean-yves  antoine,  franck poirier,  and  jean-paul  départe.
2008.  sibylle,  an assistive communication system adapting to the con-text and its user.
acm transactions on accessible computing (taccess), 1(1):1–30.
.
bruce  wisenburn  and  d  jeffery  higginbotham.
2008. an aac application using speaking partner speech recognition  to  automatically  produce  contextually relevant utterances: objective results.
augmentative and alternative communication, 24(2):100–109.
.
1364