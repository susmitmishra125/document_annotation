banditmtl: bandit-based multi-task learning for text classiﬁcation.
yuren mao1, zekai wang2, weiwei liu2∗, xuemin lin1, wenbin hu21school of computer science and engineering, university of new south wales2school of computer science, wuhan universityyuren.mao@unsw.edu.au, {wzekai99,liuweiwei863}@gmail.comlxue@cse.unsw.edu.au, hwb@whu.edu.cn.
abstract.
task variance regularization, which can beused to improve the generalization of multi-task learning (mtl) models, remains unex-plored in multi-task text classiﬁcation.
ac-cordingly, to ﬁll this gap, this paper investi-gates how the task might be effectively regular-ized, and consequently proposes a multi-tasklearning method based on adversarial multi-armed bandit.
the proposed method, namedbanditmtl, regularizes the task variance bymeans of a mirror gradient ascent-descent al-gorithm.
adopting banditmtl in the multi-task text classiﬁcation contextis found toachieve state-of-the-art performance.
the re-sults of extensive experiments back up our the-oretical analysis and validate the superiority ofour proposals..1.introduction.
multi-task learning (mtl), which involves the si-multaneous learning of multiple tasks, can achievebetter performance than learning each task indepen-dently (caruana, 1993; ando and zhang, 2005).
ithas achieved great success in various applications,ranging from summary quality estimation (krizet al., 2020) to text classiﬁcation (liu et al., 2017).
in the multi-task text classiﬁcation context, mtlsimultaneously learns the tasks by minimizing theirempirical losses together; for example, by mini-mizing the mean of the empirical losses for the in-cluded tasks.
however, it is common for these tasksto be competing.
minimizing the losses of sometasks increases the losses of others, which accord-ingly increases the task variance (variance betweenthe task-speciﬁc loss).
large task variance can leadto over-ﬁtting in some tasks and under-ﬁtting inothers, which degenerates the generalization per-formance of an mtl model.
to illustrate this issue,.
*corresponding author..it is instructive to consider a case of two-task learn-ing, where task 1 and task 2 are conﬂicting binaryclassiﬁcation tasks.
when the task variance is un-controlled, it is possible that the empirical loss oftask 1 will converge to 0, while the empirical loss oftask 2 will converge to 0.5. in such a case, althoughthe mean of the empirical losses is decreasing, task1 overﬁts and task 2 underﬁts, which leads to poorgeneralization performance..to address the problem caused by uncontrolledtask variance, it is necessary to implement task vari-ance regularization, which regularizes the variancebetween the task-speciﬁc losses during training.
however, existing deep mtl methods, includingboth adaptive weighting sum methods (kendallet al., 2018; chen et al., 2018; liu et al., 2017)and multi-objective optimization-based methods(sener and koltun, 2018; mao et al., 2020b), ig-nore the task variance.
overlooking task variancedegenerates an mtl model’s generalization ability..to ﬁll this gap and further improve the general-ization ability of mtl models, this paper proposesa novel mtl method, dubbed banditmtl, whichjointly minimizes the empirical losses and regu-larizes the task variance.
banditmtl is proposedbased on linear adversarial multi-armed bandit andimplemented with a mirror gradient ascent-descentalgorithm.
our proposed approach can improve theperformance of multi-task text classiﬁcation..moreover, to verify our theoretical analysis andvalidate the superiority of banditmtl in the textclassiﬁcation context, we conduct experiments ontwo classical text classiﬁcation problems: senti-ment analysis (on reviews) and topic classiﬁcation(on news).
the results demonstrate that apply-ing variance regularization can improve the perfor-mance of a mtl model; moreover, banditmtl isfound to outperform several state-of-the-art multi-task text classiﬁcation methods..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5506–5516august1–6,2021.©2021associationforcomputationallinguistics55062 related works.
multi-task learning methods jointly minimize task-speciﬁc empirical loss based on multi-objectiveoptimization (sener and koltun, 2018; lin et al.,2019; mao et al., 2020a) or optimizing theweighted sum of the task-speciﬁc loss (liu et al.,2017; kendall et al., 2018; chen et al., 2018).
themulti-objective optimization based mtl can con-verge to an arbitrary pareto stationary point, thetask variance of which is also arbitrary.
whilethe weighted sum methods focus on minimizingthe weighted average of the task-speciﬁc empiricalloss, they do not consider the task variance.
to ﬁllthe gap in existing methods, this paper proposesto regularize the task variance, which will signif-icantly impact the generalization performance ofmtl models..variance-based regularization has been used pre-viously in single-task learning to balance the trade-off between approximation and estimation error(bartlett et al., 2006; koltchinskii et al., 2006;namkoong and duchi, 2017).
in the single-tasklearning setting, the goal of variance-based reg-ularization is to regularize the variance betweenthe loss of training samples (namkoong and duchi,2016; duchi and namkoong, 2019).
while thesevariance-based regularization methods can improvethe generalization ability of single-task learningmodels, they do not ﬁt the multi-task learning set-ting.
this paper thus ﬁrst proposes a novel variance-based regularization method for multi-task learn-ing to improve mtl models’ generalization abilityby regularizing the between-task loss variance..3 preliminaries.
consider a multi-task learning problem with ttasks over an input space x and a collection oftask spaces {y t}tt=1.
for each task, we have aset of i.i.d.
training samples dt = (x t, y t) and(x t, y t) = {xti=1, where nt is the number oftraining samples of task t. in this paper, we focuson the neural network-based multi-task learningsetting, in which the tasks are jointly learned bysharing some parameters (hidden layers)..i}nt.
i, yt.
t=1.
→ {y t}t.let h(·, θ) : {x }t.t=1 be the multi-task learning model, where θ ∈ θ is the vectorof the model parameters.
θ = (θsh, θ1, ..., θt )consists of θsh (the parameters shared betweentasks) and θt (the task-speciﬁc parameters).
wedenote ht(·, θsh, θt) : x → y t as the task-speciﬁc map.
the task-speciﬁc loss function.
is denoted as lt(·, ·): y t × y t → [0, 1]t .
the empirical loss of the task t is deﬁned asi=1 lt(h(xtˆlt(θsh, θt)= 1nt.
i, θsh, θt), yti)..(cid:2)nt.
the transpose of the vector/matrix is representedby the superscript (cid:4), and the logarithms to base eare denoted by log..3.1 the learning objective of mtl.
under the empirical risk minimization paradigm,multi-task learning aims to optimize the vector oftask-speciﬁc empirical losses.
the learning objec-tive of multi-task learning is formulated as a vectoroptimization objective, as in equation (1)..( ˆl1(θsh, θ1), ..., ˆlt (θsh, θt ))(cid:3),.
(1).
minθ.in order to optimize the learning objective, existingmulti-task learning methods tend to adopt eitherglobal criterion optimization strategies (liu et al.,2017; kendall et al., 2018; chen et al., 2018; maoet al., 2020b) or multiple gradient descent strate-gies (sener and koltun, 2018; lin et al., 2019; de-babrata mahapatra, 2020).
in this paper, we chooseto adopt the typical linear-combination strategy,which can achieve proper pareto optimality (mietti-nen, 2012) and is widely used in the multi-task textclassiﬁcation context (liu et al., 2017; yadav et al.,2018; xiao et al., 2018).
the linear-combinationstrategy is deﬁned in (2):.
minθ.
1t.t(cid:3).
t=1.
ˆlt(θsh, θt),.
(2).
3.2 adversarial multi-armed bandit.
adversarial multi-armed bandit, a case in which aplayer and an adversary simultaneously address thetrade-off between exploration and exploitation, isone of the fundamental multi-armed bandit prob-lems (bubeck and cesa-bianchi, 2012).
in thispaper, we consider the linear multi-armed bandit,which is a generalized adversarial multi-armed ban-dit.
in our linear multi-armed bandit setting, the setof arms is a compact set a ∈ rt .
at each time stepk = 1, 2, ..., k the player chooses an arm from awhile; simultaneously, the adversary chooses a lossvector from [0, 1]t .
for linear multi-armed bandit,the online mirror descent (omd) algorithm is apowerful technology that can be used to achieveproper regret (srebro et al., 2011)..3.3 online mirror descent.
the online mirror descent (omd) algorithm is ageneralization of gradient descent for sequential de-.
5507(cid:6)(cid:14)(cid:16)(cid:13)(cid:9)(cid:5)(cid:10)(cid:8).
(cid:18)(cid:10)(cid:20)(cid:18).
(cid:3)(cid:11)(cid:7)(cid:16)(cid:10)(cid:9) (cid:2)(cid:10)(cid:7)(cid:18)(cid:19)(cid:16)(cid:10)(cid:1)(cid:20)(cid:18)(cid:16)(cid:7)(cid:8)(cid:18)(cid:14)(cid:16)θsh.
(cid:21).
(cid:22).
θ1.
θ2.
(cid:22).
(cid:22).
(cid:4)(cid:7)(cid:17)(cid:12) (cid:23) (cid:21).
(cid:4)(cid:7)(cid:17)(cid:12) (cid:24) (cid:21)(cid:1).
(cid:4)(cid:7)(cid:17)(cid:12) (cid:4) (cid:21).
θt.
(cid:22).
(cid:14)(cid:19)(cid:18)(cid:15)(cid:19)(cid:18).
(cid:14)(cid:19)(cid:18)(cid:15)(cid:19)(cid:18).
(cid:14)(cid:19)(cid:18)(cid:15)(cid:19)(cid:18).
be lstm (hochreiter and schmidhuber, 1997),textcnn (kim, 2014), and so on.
the task-speciﬁclayers are typically formulated by fully connectedlayers, ending with a softmax function..figure 1:parameter-sharing mtl models..illustration of the framework of hard.
4 bandit-based multi-task learning.
cision problems.
rather than taking gradient stepsin the primal space, the mirror descent approach in-volves taking gradient steps in the dual space.
thebijection ∇φ and its inverse ∇φ∗ are used to mapback and forth between primal and dual points.
toobtain a good regret bound, φ must be a legendrefunction (deﬁnition 1)..assume that we update uk with gradient gk usingomd.
the omd algorithm consists of three steps:(1) select a legendre function φ; (2) perform agradient descent step in the dual space vk+1 =∇φ∗(∇φ(uk) − ηgk), where φ∗ and ∇φ∗ are asdeﬁned in deﬁnition 2 and η is the step length;(3) project back to the primal space according tothe bregman divergence (deﬁnition 3): uk+1 =arg minu dφ(u, vk+1) .
deﬁnition 1 (legendre function).
let o ⊂ rt bean open convex set, and let o be the closure of o.a continuous function φ : o → r is legendre if:.
(i) φ is strictly convex and admits continuous.
ﬁrst partial derivatives on o;.
(ii) limu→o/o (cid:7) ∇φ(u) (cid:7)= +∞..deﬁnition 2 (fenchel conjugate).
the fenchelconjugate φ∗ of φ is φ∗(u) = supv{(cid:9)u, v(cid:10) +φ(v)}, and ∇φ∗(u) = arg maxv{(cid:9)u, v(cid:10) + φ(v)}.
deﬁnition 3 (bregman divergence).
the bregmandivergence dφ : o × o → r associated witha legendre function φ is deﬁned by dφ(u, v) =φ(u) − φ(v) − (u − v)(cid:3)∇φ(v)..3.4 hard parameter-sharing mtl model.
this paper adopts the most prevalent and efﬁcienthard parameter-sharing mtl model (kendall et al.,2018; chen et al., 2018; sener and koltun, 2018;mao et al., 2020b) to perform multi-task text classi-ﬁcation.
as shown in figure 1, the hard parameter-sharing mtl model learns multiple related taskssimultaneously by sharing the hidden layers (fea-ture extractor) across all tasks while retaining task-speciﬁc output layers for each task.
in multi-task text classiﬁcation, the feature extractor can.
to avoid uncontrolled task variance, we need todevelop a learning method that regularizes the taskvariance during training.
regularized loss mini-mization (rlm) is a learning method that jointlyminimizes the empirical risk and a regularizationfunction, and is thus a natural choice.
while rlmis widely used in single-task learning, it cannot bedirectly used in multi-task learning to regularizethe task variance.
in this section, we propose a sur-rogate for rlm in mtl and accordingly develop anovel mtl method, namely banditmtl..4.1 regularizing the task variance.
rlm is a natural choice for regularizing the taskvariance.
rlm for task-variance-regularized mtlcan be formulated as in equation (3):.
minθ.
1t.t(cid:3).
t=1.
(cid:4).
ˆlt(θsh, θt) +.
ρv ar( ˆlt(θsh, θt)),.
(3)t=1( ˆlt(θsh, θt) −where v ar( ˆlt(θsh, θt)) = 1tˆlt(θsh, θt))2 is the empirical variance be-1ttween the task-speciﬁc losses..(cid:2)t.(cid:2)t.t=1.
however, formulation (3) is generally non-convex and associated np-hardness.
to handlethe non-convexity, we select a convex surrogate for(3) based on its equivalent formulation (4) (ben-talet al., 2013; bertsimas et al., 2018)..pt ˆlt(θsh, θt) =.
ˆlt(θsh, θt).
1t.t(cid:3).
t=1.
1t.t(cid:3).
t=1.
supp∈pρ,t(cid:4).
+.
ρv ar( ˆlt(θsh, θt)) + o(t − 1(cid:2)t.2 ),.
(4)t=1 pt = 1, pt ≥.
(cid:2)t.where pρ,t := {p ∈ rt :t=1 pt log(t pt) ≤ ρ}.
0,(cid:2)t1t.supp∈pρ,t.
t=1 pt ˆlt(θsh, θt) is convex andcan be used as a convex surrogate for (3).
this pa-per proposes to perform task-variance-regularizedmulti-task-learning with the following learning ob-jective:.
minθ.supp∈pρ,t.
1t.t(cid:3).
t=1.
pt ˆlt(θsh, θt).
(5).
optimizing (5) is equivalent to optimizing (3)..5508in the proposed learning objective (5), ρ is theregularization parameter that controls the trade-offbetween the mean empirical loss and the task vari-ance.
experimental analysis on the inﬂuence ofρ is presented in section 5.6. to learn an mtlmodel via learning objective (5), we formulate thelearning problem as an adversarial multi-armedbandit problem in section 4.2 and further proposethe banditmtl algorithm in section 4.3..4.2 task-variance-regularized mtl asadversarial multi-armed bandit.
in deep multi-task learning, an mtl model is typ-ically learnt by iteratively optimizing the learn-ing objective.
to iteratively optimize the pro-posed learning objective (5), we formulate itas an adversarial multi-armed bandit problemin which the player chooses an arm from pρ,tand the adversary assigns a loss vector l(θ) =( ˆl1(θsh, θ1), ..., ˆlt (θsh, θt ))(cid:3) to each arm.
ineach learning iteration, the player chooses an armfrom pρ,t to increase the weighted sum loss, whilethe adversary aims to decrease the loss by updatingthe learning model.
moreover, both the player andthe adversary aim to ﬁnd a trade-off between ex-ploration and exploitation to achieve proper regret.
when lt(·, ·) is convex and θ is compact,the adversarial multi-armed bandit problem canachieve a saddle point(boyd andthe saddle point sat-vandenberghe, 2014).
isﬁes linf , wherelinf =inf{p∗(cid:3)l(θ)|θ ∈ θ}..psup ≤ p∗(cid:3)l(θ∗) ≤ lθpsup = sup{p(cid:3)l(θ∗)|p ∈ pρ,t} and lθ.
(θ∗, p∗).
to achieve a proper regret and saddle point, weadopts mirror gradient ascent for the player and mir-ror gradient descent for the adversary.
the mirrorgradient ascent-descent algorithm for mtl, namelybanditmtl, is proposed in the next section..4.3 banditmtl.
in this paper, the task-variance-regularized multi-task learning is formulated as a linear adversarialmulti-armed bandit problem.
for a problem of thiskind, mirror gradient descent (ascent) is a power-ful technique for the adversary and the player toachieve proper regret (bubeck and cesa-bianchi,2012; namkoong and duchi, 2016).
moreover,based on the mirror gradient ascent-descent, wecan reach the saddle point of the minimax optimiza-tion problem when the task-speciﬁc loss functionsare convex and the parameter space θ is compact(boyd and vandenberghe, 2014)..t=1, the learning rate ηp.
algorithm 1: banditmtlinput: data {dt}tand ηa, the approximation parameter (cid:5).
initialization: p1 = ( 1domly initialize θ1.
for k = 1 to k do.
t , ..., 1.t , 1.t )(cid:3), ran-.
1.compute λ with algorithm 2.update p: :pk+1t = e(cid:2)tupdate θ:θk+1 = θk − ηa∇θ.
1+λ (log pk.
1+λ (log pk.
t +ηp ˆlt(θk.
sh,θkt +ηp ˆlt(θksh.
(cid:2)t.t=1.
t=1 pk.
1t.e.1.t )).
,θk.
t )).
t ˆlt(θsh, θt).
end forreturn θk with best validation performance..algorithm 2: compute λinput: pk, θk, (cid:5), β.initialization: λl = 0, λr = 0.if f (0) ≤ 0 thenreturn 0..end ifwhile f (λr) ≥ 0 do.
λl = λr.
λr = λl + β..end whilewhile |f (ˆλ)| > (cid:5) do.
ˆλ = λl+λr2if f (ˆλ) > 0 then.
..λl = ˆλ..else.
λr = ˆλ..end ifend whilereturn ˆλ..in this paper, we propose a task-variance-regularized multi-task learning algorithm basedon mirror gradient ascent-descent, dubbed ban-ditmtl.
the proposed method is presented in al-gorithmic form in algorithm 1. we assume thatthe training procedure has k learning iterations.
ineach learning iteration 1 ≤ k < k, the player andthe adversary update via mirror gradient ascent anddescent..4.3.1 mirror gradient ascent for the player.
for the player, considering the constraintinpρ,t , we choose the legendre function φp(p) =(cid:2)tt=1 pt log pt.
based on the legendre function,we propose the update rule of p in (6) (see the.
5509appendix for derivations of the update rule)..pk+1t =.
e(cid:2)t.1.
1+λ (log pk.
t +ηp ˆlt(θk.
sh,θk.
t )).
1.
1+λ (log pk.
t +ηp ˆlt(θk.
sh,θk.
t )).
t=1 e.(6).
where ηp is the step size for the player.
moreover,λis the solution of equation, where f (λ) is deﬁnedin (7).
f (λ) is non-increasing and λ ≥ 0..f (λ) =.
(cid:2)t(cid:2)t.t=1(log qt)qtt=1(1 + λ)qt.
11+λ.
11+λ.
+ log t − ρ,.
− log.
11+λ.
qt.
t(cid:3).
t=1.
(7).
t +ηp ˆlt(θk.
where qt = e(log pkt )).
to solve f (λ) =0, we propose a bisection search-based algorithm,as outlined in algorithm 2..sh,θk.
4.3.2 mirror gradient descent for the.
adversary.
for the adversary, to simplify calculation, we(cid:7) θ (cid:7)2choose the legendre function φθ(θ) = 12.
2by using φθ(θ), the update rule of mirror gradientdescent (presented in (8)) is the same as that ofsame with the common gradient descent.
(see theappendix for derivations of the update rule)..θk+1 = θk − ηa∇θ.
t ˆlt(θsh, θt),pk.
(8).
1t.t(cid:3).
t=1.
where ηa is the learning rate for the adversary..5 experiments.
in this section, we perform experimental studieson sentiment analysis and topic classiﬁcation re-spectively to evaluate the performance of our pro-posed banditmtl and verify our theoretical anal-ysis.
the implementation is based on pytorch(paszke et al., 2019).
the code is attached in thesupplementary materials..5.1 datasets.
sentiment analysis .
we evaluate our algorithmon product reviews from amazon.
the dataset(blitzer et al., 2007) contains product reviews from14 domains, including books, dvds, electronics,kitchen appliances and so on.
we consider eachdomain as a binary classiﬁcation task.
reviewswith rating > 3 were labeled positive, those withrating < 3 were labeled negative, reviews with.
https://www.cs.jhu.edu/˜mdredze/.
datasets/sentiment/.
rating = 3 are discarded as the sentiments wereambiguous and hard to predict..topic classiﬁcation .
we select 16 newsgroupsfrom the 20 newsgroup dataset, which is a col-lection of approximately 20,000 newsgroup doc-uments that is partitioned (nearly) evenly across20 different newsgroups, then formulate them intofour 4-class classiﬁcation tasks (as shown in table1) to evaluate the performance of our algorithm ontopic classiﬁcation..table 1: data allocation for topic classiﬁcation tasks..tasks newsgroups.
comp.
rec.
sci.
talk.
os.ms-windows.misc, sys.mac.hardware,graphics, windows.xsport.baseball, sport.hockeyautos, motorcyclescrypt, electronics,med, spacepolitics.mideast, religion.misc,politics.misc, politics.guns.
5.2 baselines.
we compare banditmtl with following baselines.
single-task learning: learning each task inde-.
pendently..uniform scaling: learning the mtl model withlearning objective (2), the uniformly weighted sumof task-speciﬁc empirical loss..uncertainty: using the uncertainty weighting.
method proposed by (kendall et al., 2018)..gradnorm: using the gradient normalization.
method proposed by (chen et al., 2018)..mgda: using the mgda-ub method proposed.
by (sener and koltun, 2018)..advmtl: using the adversarial multi-task.
learning method proposed by (liu et al., 2017)..tchebycheff: using the tchebycheff procedure.
proposed by (mao et al., 2020b)..5.3 experimental settings.
we adopt the hard parameter-sharing mtl modelshown in fig.
1. the shared feature extractor isformulated via a textcnn which is structured withthree parallel convolutional layers with kernels sizeof 3, 5, 7 respectively.
the task-speciﬁc module isformulated by means of one fully connected layerending with a softmax function.
to ensure consis-tency with the state-of-the-art multi-task classiﬁca-tion methods (liu et al., 2017; mao et al., 2020b)and ensure fair comparison, we adopt pre-trained.
http://qwone.com/˜jason/20newsgroups/.
5510figure 2: classiﬁcation accuracy of single task learning, uniform scaling, advmtl, mgda, tchebycheff,gradnorm, uncertainty, and banditmtl on the sentiment analysis dataset.
each colored cluster illustrates theclassiﬁcation accuracy performance of a method over 10 runs.
our proposed banditmtl outperforms all baselinesin all tasks.
(ρ = 1.2, ηp = 0.5).
figure 3: classiﬁcation accuracy of single task learning, uniform scaling, uncertainty, gradnorm, tchebycheff,mgda, advmtl, and banditmtl on the topic classiﬁcation dataset.
each colored cluster illustrates the classiﬁ-cation accuracy performance of a method over 10 runs.
our proposed banditmtl outperforms all baselines in alltasks except rec.
banditmtl’s average performance is also superior to that of all baselines.
(ρ = 1.2, ηp = 0.5).
glove (pennington et al., 2014) word embeddingsin our experimental analysis..the batch size is 256. we use dropout with a prob-ability of 0.5 for all task-speciﬁc modules..we train the deep mtl network model in linewith algorithm 1. the learning rate for the ad-versary is 1e − 3 for both sentiment analysis andtopic classiﬁcation.
we use the adam optimizer(kingma and ba, 2015) and train over 3000 epochsfor both sentiment analysis and topic classiﬁcation..5.4 classiﬁcation accuracy.
we compare the proposed banditmtl with thebaselines and report the results over 10 runs byplotting the classiﬁcation accuracy of each taskfor both sentiment analysis and topic classiﬁcation.
the results are shown in fig.
2 and 3..5511figure 4: evolution of task variance during training of baseline methods and banditmtl on the sentiment analysisand topic classiﬁcation datasets.
ρ = 1.2, ηp = 0.5 for both sentiment analysis and topic classiﬁcation..figure 5: evolution of task variance during training w.r.t different value of ρ on the sentiment analysis and topicclassiﬁcation datasets.
ηp = 0.5 for both sentiment analysis and topic classiﬁcation..all experimental results show that our pro-posed banditmtl signiﬁcantly outperforms uni-form scaling, which demonstrates that adoptingtask variance regularization can boost the perfor-mance of mtl models.
moreover, banditmtlcan be seen to outperform all baselines and achievestate-of-the-art performance..5.5 task variance.
in this section, we experimentally investigate howbanditmtl regularizes the task variance duringtraining and compare the task variance of ban-ditmtl with the baselines.
the results are plottedin fig.
4. as the ﬁgure shows, all mtl methodshave lower task variance than single task learningduring training.
moreover, banditmtl has lowertask variance and smoother evolution during train-.
ing than other mtl methods.
after considering theresults obtained in section 5.4, we conclude thattask variance has a signiﬁcant impact on multi-tasktext classiﬁcation performance..5.6 impact of ρ.in banditmtl, ρ is the regularization parameter.
inthis section, we experimentally investigate the im-pact of ρ on task variance and average classiﬁcationaccuracy over the tasks of interest..5.6.1 impact on variance.
fig.
5 plots how the task variance evolves duringtraining w.r.t different values of ρ. the task vari-ance decreases as ρ increases.
it reveals that wecan control the task variance by adjusting ρ..5512figure 6: task-average classiﬁcation accuracy w.r.t dif-ferent value of ρ. for each value of ρ, we report theresults over ﬁve runs.
ηp = 0.5..figure 7: changing of task-average classiﬁcation ac-curacy w.r.t.
increasing ηp.
for each value of ηp, wereport the results over ﬁve runs.
ρ = 1.2 for both senti-ment analysis and topic classiﬁcation..5.6.2 impact on average accuracy.
the changes in banditmtl’s average classiﬁcationaccuracy w.r.t different values of ρ is illustrated infig.
6. in this ﬁgure, as ρ increases, the averageaccuracy of banditmtl ﬁrst increases and thendecreases.
this reveals that ρ signiﬁcantly impactsthe performance of multi-task text classiﬁcation.
as ρ controls the trade-off between the empiricalloss and the task variance, we can conclude thatthis trade-off signiﬁcantly impacts the multi-tasktext classiﬁcation performance.
thus, in the multi-task text classiﬁcation, it is necessary for us toﬁnd a proper trade-off between the empirical lossand the task variance rather than focusing only onempirical loss.
these results verify the necessaryof task variance regularization..5.7 sensitivity study on ηp.
in banditmtl, ηp is a hyper-parameter.
to deter-mine whether the performance of banditmtl issensitive to ηp, we conduct experiments on the clas-siﬁcation performance of banditmtl w.r.t differ-ent values of ηp.
the results of these experimentsare presented in fig.
7. as the ﬁgure shows, theperformance of our proposed method is not verysensitive to ηp when ηp is within the range of 0.3.figure 8: comparison of task weight adaption pro-cesses between banditmtl, uncertainty, gradnorm,and mgda for topic classiﬁcation.
ρ = 1.2, ηp = 0.5..to 0.9 for both sentiment analysis and topic clas-siﬁcation.
setting ηp to between 0.3 and 0.9 cangenerally provide satisfactory results..5.8 evolution of pt.
in this section, we observe the changes in pt duringtraining and compare these changes with the taskweight adaption process of three weight adaptivemtl methods (i.e., uncertainty, gradnorm, andmgda).
the results for topic classiﬁcation arereported in fig.
9. due to space limitations, thesentiment analysis results are presented in the ap-pendix.
from the results, we can see that the weightadaption process of banditmtl is more stable thanthat of uncertainty, gradnorm, and mgda..6 conclusion.
this paper proposes a novel multi-task learningalgorithm, dubbed banditmtl.
it ﬁlls the task vari-ance regularization gap in the ﬁeld of mtl andachieves state-of-the-art performance in real-worldtext classiﬁcation applications.
moreover, our pro-posed banditmtl is model-agnostic; thus, it couldpotentially be used in other natural language pro-cessing applications, such as multi-task namedentity recognition..acknowledgements.
this work is supported by the national natural sci-ence foundation of china under grants 61976161and 61976162..5513references.
rie kubota ando and tong zhang.
2005. a frameworkfor learning predictive structures from multiple tasksand unlabeled data.
journal of machine learningresearch, 6:1817–1853..peter l bartlett, michael.
i jordan, and jon dmcauliffe.
2006. convexity, classiﬁcation, and riskbounds.
journal of the american statistical associ-ation, 101(473):138–156..aharon ben-tal, dick den hertog, anja de waege-naere, bertrand melenberg, and gijs rennen.
2013.robust solutions of optimization problems affectedby uncertain probabilities.
manag.
sci., 59(2):341–357..dimitris bertsimas, vishal gupta, and nathan kallus.
2018. robust sample average approximation.
math.
program., 171(1-2):217–282..john blitzer, mark dredze, and fernando pereira.
2007.biographies, bollywood, boom-boxes and blenders:indomain adaptation for sentiment classiﬁcation.
acl..stephen p. boyd and lieven vandenberghe.
2014. con-.
vex optimization.
cambridge university press..s´ebastien bubeck and nicol`o cesa-bianchi.
2012. re-gret analysis of stochastic and nonstochastic multi-armed bandit problems.
found.
trends mach.
learn., 5(1):1–122..rich caruana.
1993. multitask learning: a knowledge-.
based source of inductive bias.
in icml..zhao chen, vijay badrinarayanan, chen-yu lee, andandrew rabinovich.
2018. gradnorm: gradientnormalization for adaptive loss balancing in deepmultitask networks.
in icml..vaibhav rajan debabrata mahapatra.
2020. multi-tasklearning with user preferences: gradient descentwith controlled ascent in pareto optimization.
inicml..john c. duchi and hongseok namkoong.
2019.variance-based regularization with convex objec-tives.
j. mach.
learn.
res., 20:68:1–68:55..sepp hochreiter and j¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..alex kendall, yarin gal, and roberto cipolla.
2018.multi-task learning using uncertainty to weighlosses for scene geometry and semantics.
in cvpr..yoon kim.
2014. convolutional neural networks for.
sentence classiﬁcation.
in emnlp..diederik p. kingma and jimmy ba.
2015. adam: a.method for stochastic optimization.
in iclr..vladimir koltchinskii et al.
2006. local rademachercomplexities and oracle inequalities in risk mini-mization.
the annals of statistics, 34(6):2593–2656..reno kriz, marianna apidianaki, and chris callison-burch.
2020. simple-qe: better automatic qualityestimation for text simpliﬁcation.
arxiv..xi lin, hui-ling zhen, zhenhua li, qingfu zhang,and sam kwong.
2019. pareto multi-task learning.
in nips..pengfei liu, xipeng qiu, and xuanjing huang.
2017.adversarial multi-task learning for text classiﬁca-tion.
in acl..yuren mao, weiwei liu, and xuemin lin.
2020a.
adaptive adversarial multi-task representation learn-ing.
in icml..yuren mao, shuang yun, weiwei liu, and bo du.
2020b.
tchebycheff procedure for multi-task textclassiﬁcation.
in acl..kaisa miettinen.
2012. nonlinear multiobjective opti-mization, volume 12. springer science & businessmedia..hongseok namkoong and john c. duchi.
2016.stochastic gradient methods for distributionally ro-bust optimization with f-divergences.
in neurips..hongseok namkoong and john c. duchi.
2017.variance-based regularization with convex objec-tives.
in neurips..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019. pytorch:an imperative style, high-performance deep learn-ing library.
in neurips..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in emnlp..ozan sener and vladlen koltun.
2018..task learning as multi-objective optimization.
neurips..multi-in.
nati srebro, karthik sridharan, and ambuj tewari.
2011. on the universality of online mirror descent.
in neurips..liqiang xiao, honglun zhang, and wenqing chen.
2018. gated multi-task network for text classiﬁca-tion.
in naacl-hlt..shweta yadav, asif ekbal, sriparna saha, pushpakbhattacharyya, and amit p. sheth.
2018. multi-tasklearning framework for mining crowd intelligencetowards clinical treatment.
in naacl..5514appendix.
taking derivatives, we have.
1 derivations of the update rule for theplayer.
l(λ) = log t − ρ − log.
t(cid:3).
(qk+1t.).
11+λ.
ddλ.
assume the mirror gradient ascent step in the dualspace is qk+1 w.r.t the k + 1thlearning iteration.
then, the qk+1 can be obtained as the follows.
according to the gradient descent step,.
∇φp(qk+1) = ∇φp(pk) + ηpl(θk).
for each task, the t-th element of ∇φp(qk+1),.
(9).
∇φp(qk+1.)
= 1 + log qk+1.
..t.t.(10).
combining (9) and (10), we have.
t = e(∇φp(pkqk+1.
t )+ηp ˆlt(θk.
sh,θk.
t ))−1)..(11).
to map back to the primal space, we need to.
solve optimization objective (12)..pk+1 = arg minp∈pρ,t.
dφp(p, qk+1),.
(12).
the lagrangian for the optimization problem (12)is:.
pk+1tqk+1tt(cid:3).
t=1.
l(pk+1, α, λ) =.
pk+1t.log.
t(cid:3).
t=1.
− α(.
pk+1t − 1) − λ(ρ −.
pk+1t.log pk+1t.t )..t(cid:3).
t=1.
the partial derivative w.r.t pt is:.
∇pk+1.
t.l(pk+1, α, λ) =(1 + λ) log pk+1.
t − log qk+1.
t− α + λ log t + 1 + λ..(14)using the ﬁrst order conditions w.r.t pk+1(∇pk+1.
l(pk+1, α, λ) = 0), we have.
t.t.t = (qk+1pk+1.
t.).
− λ.
1+λ exp(.
α1 + λ.
− 1).
(15).
combining with.
t = 1, we have.
t = (qk+1pk+1.
t.).
11+λ /(.
(qk+1t.).
11+λ )..(16).
11+λ t(cid:2)t.t=1 pk+1t(cid:3).
t=1.
plugging this back into the lagrangian, we have.
l(λ) = minα.maxpk+1∈pρ,t.
l(pk+1, α, λ).
=λ(log t − ρ) − (1 + λ) log.
(qk+1t.).
11+λ ..t(cid:3).
t=1.
(17).
(cid:2)t.t=1)(qk+1t=1 log(qk+1tt=1(qk+1(1 + λ).
t(cid:2)t.).
t.11+λ.
).
11+λ.
..−.
(18).
combining (11) and (16), we have.
pk+1t =.
e(cid:2)t.1.
1+λ (log pk.
t +ηp ˆlt(θk.
sh,θk.
t )).
1.
1+λ (log pk.
t +ηp ˆlt(θk.
sh,θk.
t )).
t=1 e.(19).
where λ is obtained by solving the equationddλ l(λ) = 0, which is the necessary condition tooptimize the lagrangian function..2 derivations of the update rule for theadversary.
assume the mirror gradient descent step in the dualspace is γk+1 w.r.t the k + 1thlearning iteration.
then, the γk+1 can be obtained as the follows..1t.t(cid:3).
t=1.
∇φθ(γk+1) = ∇φθ(θk) − ηa.
t ˆlt(θsh, θt)pk.
for φθ(θ) = 12γk+1 and ∇φθ(θk) = θk.
thus,.
(cid:7) θ (cid:7)2.
(20)2, we have ∇φθ(γk+1) =.
1t.t(cid:3).
t=1.
1t.t(cid:3).
t=1.
moreover, it is obvious that.
arg min dφθ (φθ, γk+1) = γk+1..(22).
then,.
θk+1 = θk − ηa.
t ˆlt(θsh, θt).
pk.
(23).
which means that the update rule of the mirrorgradient descent is same with the vanilla gradient(cid:7) θ (cid:7)2descent when legendre function φθ(θ) = 122is adopted..3 weight adaption process for sentimentanalysis.
the results of the change of pt during a trainingbanditmtl model are shown in fig.
9. comparingit with the task weights adaption process of threeweight adaptive mtl methods (i.e., uncertainty,gradnorm, mgda), we can see that the weightsadaption process of banditmtl is more stable..γk+1 = θk − ηa.
t ˆlt(θsh, θt).
pk.
(21).
(13).
5515figure 9: comparison of task weight adaption processes between banditmtl, uncertainty, gradnorm, and mgdafor sentiment analysis.
ρ = 1.2, ηp = 0.5..4 hardware speciﬁcation andenvironment.
silver cpu (2.20 ghz) and nvidia geforce rtx2080 ti gpus with 11gb graphics memory..our experiments are conducted on a ubuntu 64-bit linux workstation, having 10-core intel xeon.
5516