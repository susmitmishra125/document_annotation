baco: a background knowledge- and content-based framework forciting sentence generation.
yubin ge1, ly dinh1, xiaofeng liu2, jinsong su3, ziyao lu3, ante wang3, jana diesner11university of illinois at urbana-champaign, usa2harvard university, usa3xiamen university, china{yubinge2, dinh4, jdiesner}@illinois.edujssu@xmu.edu.cn.
abstract.
in this paper, we focus on the problem ofciting sentence generation, which entails gen-erating a short text to capture the salient in-formation in a cited paper and the connec-tion between the citing and cited paper.
wepresent baco, a background knowledge-and content-based framework for citing sen-tence generation, which considers two typesof information: (1) background knowledge byleveraging structural information from a cita-tion network; and (2) content, which repre-sents in-depth information about what to citeand why to cite.
first, a citation network is en-coded to provide background knowledge.
sec-ond, we apply salience estimation to identifywhat to cite by estimating the importance ofsentences in the cited paper.
during the decod-ing stage, both types of information are com-bined to facilitate the text generation.
we thenconduct joint training of the generator and cita-tion function classiﬁcation to make the modelaware of why to cite.
our experimental resultsshow that our framework outperforms compar-ative baselines..1.introduction.
a citation systematically, strategically, and criti-cally synthesizes content from a cited paper in thecontext of a citing paper (smith, 1981).
a paper’stext that refers to prior work, which we herein referto as citing sentences, forms the conceptual basisfor a research question or problem; identiﬁes is-sues, contradictions, or gaps with state of the artsolutions; and prepares readers to understand thecontributions of a citing paper, e.g., in terms oftheory, methods, or ﬁndings (elkiss et al., 2008).
writing meaningful and concise citing sentencesthat capture the gist of cited papers and identifyconnections between citing and cited papers is nottrivial (white, 2004).
learning how to write up in-formation about related work with appropriate and.
meaningful citations is particularly challenging fornew scholars (mansourizadeh and ahmad, 2011).
to assist scholars with note taking on prior workwhen working on a new research problem, thispaper focuses on the task of citing sentence genera-tion, which entails identifying salient informationfrom cited papers and capturing connections be-tween cited and citing papers.
with this work, wehope to reduce scientiﬁc information overload forresearchers by providing examples of concise cit-ing sentences that address information from citedpapers in the context of a new research problemand related write up.
while this task cannot and isnot meant to replace the scholarly tasks of ﬁnding,reading, and synthesizing prior work, the proposedcomputational solution is intended to support espe-cially new researchers in practicing the process ofwriting effective and focused reﬂections on priorwork given a new context or problem..a number of recent papers have focused onthe task of citing sentence generation (hu andwan, 2014; saggion et al., 2020; xing et al., 2020),which is deﬁned as generating a short text that de-scribes a cited paper b in the context of a citingpaper a, and the sentences before and after the cit-ing sentences in paper a are considered as context.
however, previous work has mainly utilized limitedinformation from citing and cited papers to solvethis task.
we acknowledge that any such solution,including ours, is a simpliﬁcation of the intricateprocess of how scholars write citing sentences..given this motivation, we explore two sets ofinformation to generate citing sentences, namelybackground knowledge in the form of citation net-works, and content from both citing and cited pa-pers, as shown in figure 1. using citation networkswas inspired by the fact that scholars have ana-lyzed such networks to identify the main themesand research developments in domain areas suchas information sciences (hou et al., 2018), busi-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1466–1478august1–6,2021.©2021associationforcomputationallinguistics1466background knowledge, and the given citing andcited papers to provide content information.
we ex-tend a standard pointer-generator (see et al., 2017)to copy words from cited and citing papers, and de-termine what to cite by estimating sentence saliencein the cited paper.
the various pieces of capturedinformation are then combined as the context forthe decoder.
furthermore, we extend our frame-work to include why to cite by jointly training thegeneration with citation function classiﬁcation andfacilitate the acquisition of the content information.
as for the dataset, we extended the acl anthol-ogy network corpus (aan) (radev et al., 2013)with extracted citing sentences by using regex.
we then hand-annotated the citation functions ona subset of the dataset, and trained a citation func-tion labeling model based on scibert (beltagyet al., 2019).
the resulting labeling model was thenused to automatically label the rest data to build alarge-scale dataset..we summarize our contributions as follows:• we propose a background knowledge- andcontent-based framework, named baco, for cit-ing sentence generation..• we manually annotated a subset of citing sen-tences with citation functions to train a scibert-based model to automatically label the rest data forciting sentence generation..• based on the results from experiments, weshow that baco outperforms comparative base-lines by at least 2.57 points on rouge-2..2 related work.
several studies on citing sentence generationhave used keyword-based summarization methods(hoang and kan, 2010; chen and zhuge, 2016,2019).
to that end, they built keyword-based treesto extract sentences from cited papers as relatedwork write-ups.
these studies have two limitations:first, since related work sections are not simply(chronological) summaries of cited papers, syn-thesizing prior work in this manner is insufﬁcient.
second, extractive summarization uses verbatimcontent from cited papers, which implies intellec-tual property issues (e.g., copyright violations) aswell as ethical problems, such as a lack of intellec-tual engagement with prior work.
alternatively, ab-stractive summarization approaches, such as meth-ods based on linear programming (hu and wan,2014) and neural seq2seq methods (wang et al.,2018), have also been explored.
these approaches.
figure 1: an example from our dataset (source: aclanthology network corpus (radev et al., 2013).
thered text in the citing paper is the citing sentence, andthe special token #refr indicates the citation of thecited paper.
our framework aims at capturing informa-tion from two perspectives: background knowledge andcontent.
the background knowledge is learned by ob-taining structural features of the citation network.
thecontent information entails estimated sentence salience(higher salience is highlighted by darker color) in thecited paper and the corresponding citation function ofthe cited paper to the citing paper..ness modeling (li et al., 2017), and pharmaceuticalresearch (chen and guan, 2011)..we use the content of citing and cited papersas a second set of features to capture two morein-depth content features: (1) what to cite - whilethe overall content of a cited paper needs to beunderstood by the authors of the citing paper, notall content is relevant for writing citing sentences.
therefore, we follow the example of estimatingsalient sentences (yasunaga et al., 2019) and usethe predicted salience to ﬁlter crucial informationthat should be integrated into the resulting citingsentence; (2) why to cite - we deﬁne “citation func-tion” as an approximation of an author’s reason forciting a paper (teufel et al., 2006).
a number ofprevious research on citation functions has usedciting sentences and their context for classiﬁcation(zhao et al., 2019; cohan et al., 2019).
our pa-per involves citation functions into citing sentencegeneration so that the generated citing sentencescan be coherent given their context, and can stillcontain the motivation for a speciﬁc citation..in this paper, we propose a backgroundknowledge- and content-based framework, namedbaco.
speciﬁcally, we encode a citation networkbased on citation relations among papers to obtain.
1467mainly focus on utilizing papers’ content informa-tion, speciﬁcally on the text of cited papers directly.
a recent paper that went beyond summarizing thecontent of cited papers (xing et al., 2020) useda multi-source, pointer-generator network with across attention mechanism to calculate the attentiondistribution between the citing sentences’ contextand the cited paper’s abstract..our paper is based on the premise that citationnetwork analysis can provide background knowl-edge that facilitates the understanding of papers ina ﬁeld.
prior analyses of citation networks havebeen used to reveal the cognitive structure and in-terconnectedness of scientiﬁc (sub-)ﬁelds (mooreet al., 2005; bruner et al., 2010), and to understandand detect trends in academic ﬁelds (you et al.,2017; asatani et al., 2018).
network analysis hasalso been applied to citation networks to identifyinﬂuential papers and key concepts (huang et al.,2018), and to scope out research areas..while previous studies have shown that usingtext from citing papers is useful to generate cit-ing sentences, the beneﬁt of other content-basedfeatures of a citation (e.g., reasons for citing) is in-sufﬁciently understood (xing et al., 2020).
extantliterature on citation context analysis (moravcsikand murugesan, 1975; lipetz, 1965), which fo-cused on the connections between the citing andcited papers with respect to purposes and reasonsfor citations, has found that citation function (dinget al., 2014; white, 2004) is an important indi-cator of why a paper chose to cite speciﬁc pa-per(s).
based on a content analysis of 750 cit-ing sentences from 60 papers published in twoprominent physics journals, lipetz (1965) iden-tiﬁed 11 citation functions, such as questioned, af-ﬁrmed, or refuted cited paper’s premises.
sim-ilarly, moravcsik and murugesan (1975) quali-tatively coded the citation context of 30 articleson high energy physics, ﬁnding 10 citation func-tions grouped into 5 pairs: conceptual-operational,organic-perfunctory, evolutionary-juxtapositional,conﬁrmative-negational, valuable-redundant..citation context analysis has also been used tostudy the valence of citing papers towards citedpapers (athar, 2011; abu-jbara et al., 2013) byclassifying citation context as positive, negative, orneutral.
in this paper, we adopt abu-jbara et al.
(2013)’s deﬁnition of a positive citation as a cita-tion that explicitly states the strength(s) of a citedpaper, or a situation where the citing paper’s work.
is guided by the cited paper.
in contrast to that,a negative citation is one that explicitly states theweakness(es) of a cited paper.
a neutral citationis one that objectively summarizes the cited paperwithout an additional evaluation.
in addition tothese three categories, we also consider mixed ci-tation contexts (cullars, 1990), which are citationsthat contain both positive and negative evaluationsof a cited papers, or where the evaluation is unclear.
given that our paper is a ﬁrst attempt to integratecitation functions into citing sentence generation,we opted to start with a straightforward valencecategory schema before exploring more complexschemas in future work..3 dataset and annotation.
we ﬁrst extended the aan1 (radev et al., 2013)with the extracted citing sentences using regex.
we followed the process in (xing et al., 2020) to la-bel 1,200 randomly sampled citing sentences withtheir citation functions.
the mark-up was done by 6coders who were provided with deﬁnitions of posi-tive, negative, neutral, and mixed citation functions,and ample examples for each valence category.
ourcodebook including deﬁnitions and examples of ci-tation functions is shown in table 1. after theannotation, we randomly split the dataset into 800instances for training and the remaining 400 fortesting.
we then used the 800 human-annotatedinstances to train a citation function labeling modelwith 10-fold cross validation.
the labeling taskwas treated as a multi-class classiﬁcation problem.
our labeling model was built upon scibert(beltagy et al., 2019), a pre-trained language modelbased on bert (devlin et al., 2019) but trainedon a large corpus of scientiﬁc text.
we added amultilayer perceptron (mlp) to scibert, and ﬁne-tuned the whole model on our dataset.
as for theinput, we concatenated each citing sentence with itscontext in the citing paper, and inserted a specialtag [cls] at the beginning and another specialtag [sep] to separate them.
the ﬁnal hidden statethat corresponded to [cls] was used as the aggre-gate sequence representation.
this state was fedinto the mlp, followed by the softmax function forpredicting the citation function of the citing sen-tence.
we report details of test results and datasetstatistics in the appendix, section a.1..1http://aan.how/download/.
1468deﬁnition.
example.
citationfunction.
positive.
neutral.
mixed.
when a citing paper explicitly supports a cited paper’spremises and ﬁndings, and/or that the cited paper’s ﬁnding(s)is used to corroborate with the citing paper’s study..negative.
when a citing paper points out weaknesses in the cited paper’spremises and ﬁndings, as well as explicitly rejecting the ﬁnding(s)from the cited paper..when a citing paper objectively summarizes the cited paper’spremises and ﬁndings, without explicitly offering any evaluationsof the cited paper’s ﬁnding(s)..when a citing paper does not clearly express their evaluationstowards the cited paper, or that the citing paper containsmultiple citation functions in one sentence..”our architecture is inspired by state-of-the-art model #refr”.
”unbounded content early versionsof the customization system #refronly allowed a small number ofentries for things like lexical types.””#refr translates the extractedvietnamese phrases intocorresponding english ones””in previous work, this has been donesuccessfully for question answeringtasks #refr, but not for web searchin general.”.
table 1: deﬁnitions and examples of our proposed citation functions.
the special token #refr indicates thecitation of a paper..4 methodology.
our proposed framework includes an encoder and agenerator, as shown in figure 2. the encoder takesthe citation network and the citing and cited papersas input, and encodes them to provide backgroundknowledge and content information, respectively.
the generator contains a decoder that can copywords from citing and cited paper while retainingthe ability to produce novel words, and a salienceestimator that identiﬁes key information from thecited paper.
we then trained the framework withcitation function classiﬁcation to enable the recog-nition of why a paper was cited..4.1 encoder.
our encoder (the yellow shaded area in figure 2)consists of two parts, a graph encoder that wastrained to provide background knowledge based onthe citation network, and a hierarchical rnn-basedencoder that encodes the content information of theciting and cited papers..4.1.1 graph encoder.
we designed a citation network pre-training methodfor providing the background knowledge.
in detail,we ﬁrst constructed a citation network as a directedgraph g = (v, e).
v is a set of nodes/papers2 ande is a set of directed edges.
each edge links a citingpaper (source) to a cited paper (target).
to utilizeg in our task, we employed a graph attention net-work (gat) (veliˇckovi´c et al., 2018) as our graphencoder, which leverages masked self-attentional.
2we use node and paper interchangeably.
layers to compute the hidden representation of eachnode.
this gat has been shown to be effective onmultiple citation network benchmarks.
we inputa set of node pairs {(vp, vq)} into it for training ofthe link prediction task.
we pre-trained our graphencoder network using negative sampling to learnthe node representations hnp for each paper p, whichcontains structural information of the citation net-work and can provide background knowledge forthe downstream task..4.1.2 hierarchical rnn-based encoder.
given the word sequence {cwi} of the citing sen-tence’s context and the word sequence {awj} of thecited paper’s abstract, we input the embedding ofword tokens (e.g., e(wt)) into a hierarchical rnn-based encoder that includes a word-level bi-lstmand a sentence-level bi-lstm.
the output word-level representation of the citing sentence’s contextis denoted as {hcwi }, and the cited paper’s abstractis encoded similarly as its word-level representa-tion {hawj }.
meanwhile, their sentence-level repre-sentations are represented as {hcs.
m} and {has.
n }..4.2 generator.
our generator (the green shaded area in figure 2) isan extension of the standard pointer generator (seeet al., 2017).
it integrates both background knowl-edge and content information as context for textgeneration.
the generator contains a decoder andan additional salience estimator that predicts thesalience of sentences in the cited paper’s abstractfor reﬁning the corresponding attention..1469figure 2: the overall architecture of the proposed framework.
the encoder is used to encode the citation networkand papers’ (both citing and cited) text.
the generator estimates salience of sentence in the cited paper’s abstract,and utilizes this information for text generation.
the framework is additionally trained with citation functions..4.2.1 decoderthe decoder is a unidirectional lstm conditionedon all encoded hidden states.
the attention distri-bution is calculated as in (bahdanau et al., 2015).
since we considered both the citing sentence’scontext and the cited paper’s abstract on the sourceside, we applied the attention mechanism to {hcwi }and {hawj } separately to obtain two attention vec-, aabstors actx, and their corresponding context vec-t, cabstors cctxat the step t. we then aggregatedtinput context c∗t from the citing sentence’s context,the cited paper’s abstract, and background knowl-edge by applying a dynamic fusion operation basedon modality attention as described in (moon et al.,2018b,a), which selectively attenuated or ampli-ﬁed each modality based on their importance to thetask:.
t.t.[attctx; attabs; attnet] = σ(wm[cctxt.; cabst.; cnett.] + bm),(1).
˜attm =.
exp(attm)m(cid:48)∈{abs,ctx,net} exp(attm(cid:48)).
,.
(cid:80).
c∗t =.
(cid:88).
˜attmcmt ,.
m∈{abs,ctx,net}.
(2).
(3).
p ; hn.
t = [hn.
where cnetq ] represents the learned back-ground knowledge for papers p and q, and iskept constant during all decoding steps t, and[attctx; attabs; attnet] is the attention vector..to enable our model to copy words from boththe citing sentence’s context and the cited paper’s.
abstract, we calculated the generation probabilityand copy probabilities as follows:.
[pgen, pcopy1, pcopy2] = softmax(wctxcctxt+ wabscabs+ wembe(wt−1) + bptr),.
t + wnetcnet.
t + wdecst.
(4).
where pgen is the probability of generating words,pcopy1 is the probability of copying words fromthe citing sentence’s context, pcopy2 is the prob-ability of copying words from the cited paper’sabstract, st represents the hidden state of the de-coder at step t, and e(wt−1) indicates the inputword embedding.
meanwhile, the context vectorc∗t , which can be seen as an enhanced representa-tion of source-side information, was concatenatedwith the decoder state st to produce the vocabularydistribution pvocab:.
pvocab = softmax(v(cid:48)(v[st; c∗.
t ] + b) + b(cid:48)).
(5).
finally, for each text, we deﬁned an extendedvocabulary as the union of the vocabulary and allwords appearing in the source text, and calculatedthe probability distribution over the extended vo-cabulary to predict words w:.
p (w) =pgenpvocab(w) + pcopy1.
+ pcopy2.
(cid:88).
aabst,i ..i:awi=w.
(cid:88).
actxt,i.
i:cwi=w.
(6).
14704.2.2 salience estimationthe estimation of the salience of each sentencethat occurs in a cited paper’s abstract was usedto identify what information needed to be concen-trated for the generation.
we assumed a sentence’ssalience to depend on the citing paper such thatthe same sentences from one cited paper can havedifferent salience in the context of different citingpapers.
hence, we represented this salience as aconditional probability p (si|dsrc), which can beinterpreted as the probability of picking sentence sifrom a cited paper’s abstract given the citing paperdsrc..we ﬁrst obtained the document representationdsrc of a citing paper as the average of all its ab-stract’s sentence representations.
then, for cal-culating salience, which is deﬁned as p (si|dsrc),we designed an attention mechanism that assignsa weight αi to each sentence si in a cited paper’sabstract dtgt.
this weight is expected to be largeif the semantics of si are similar to dsrc.
formally,we have:.
αi = vt tanh(wdocdsrc + wsenthas.
i + bsal),.
(7).
(8).
˜αi =.
(cid:80).
αi.
,.
αk.
sk∈dtgtwhere hasis the ith sentence representation in theicited paper’s abstract, v, wdoc, wsent and bsal arelearnable parameters, and ˜αi is the salience scoreof the sentence si..we then used the estimated salience of sentencesin the cited paper’s abstract to update the word-level attention of the cited paper’s abstract {hawj }so that the decoder can focus on these importantsentences during text generation.
considering thatthe estimated salience ˜αi is a sentence weight, wedetermined each token in a sentence to share thesame value of ˜αi.
accordingly, the new attentionaabsof the cited paper’s abstract became aabst =t˜αiaabs.
after normalizing aabs, the context vectorttcabst was updated accordingly..4.3 model training.
during model training, the objective of our frame-work covers three parts: generation loss, salienceestimation loss, and citation function classiﬁcation..4.3.1 generation lossthe generation loss was based on the predictionof words from the decoder.
we minimized the.
negative log-likelihood of all target words w∗t andused them as the objective function of generation:.
lgen = −.
log p (w∗.
t )..(9).
(cid:88).
t.4.3.2 salience estimation lossto include extra supervision into the salience esti-mation, we adopted a rouge-based approxima-tion (yasunaga et al., 2017) as the target.
we as-sume citing sentences to depend heavily on salientsentences from the cited papers’ abstracts.
basedon this premise, we calculated the rouge scoresbetween the citing sentence and sentences in thecorresponding cited paper’s abstract to obtain anapproximation of the salience distribution as theground-truth.
if a sentence shared a high rougescore with the citing sentence, this sentence wouldbe considered as a salient sentence because theciting sentence was likely to be generated basedon this sentence, while a low rouge score im-plied that this sentence may be ignored during thegeneration process due to its low salience.
kull-back–leibler divergence was used as our loss func-tion for enforcing the output salience distributionto be close to the normalized rouge score distri-bution of sentences in the cited paper’s abstract:.
lsal = dkl(r(cid:107) ˜α),βr(si).
ri =.
(cid:80).
sk∈dtgt.
βr(sk).
,.
(10).
(11).
where ˜α, r ∈ rm, ri refers to the scalar in-dexed i in r (1 ≤ i ≤ m), and r(si) is theaverage of rouge-1 and rouge-2 f1 scoresbetween the sentence si in the cited paper’s ab-stract and the citing sentence.
we also introduceda hyper-parameter β as a constant rescaling factorto sharpen the distribution..4.3.3 citation function classiﬁcationwe added a supplementary component to enable thecitation function classiﬁcation to be trained with thegenerator, aiming to make the generation consciousof why to cite.
following a prior general pipeline ofcitation function classiﬁcation (cohan et al., 2019;zhao et al., 2019), we ﬁrst concatenated the last hid-den state st of the decoder, which we consideredas a representation of the generated citing sentence,with the document representation dctx of the cit-ing sentence’s context.
here, dctx was calculatedas the average of its sentence representations.
wethen fed the concatenated representation into an.
1471mlp followed by the softmax function to predictthe probability of the citation function ˆyfunc for thegenerated citing sentence.
cross-entropy loss wasset as the objective function for training the clas-siﬁer with the ground truth label yfunc, which is aone-hot vector:.
lfunc = −.
func(j) log ˆyiyi.
func(j),.
(12).
1n.n(cid:88).
k(cid:88).
i=1.
j=1.
where n refers to the size of training data and kis the number of different citation functions..finally, all aforementioned losses were com-bined as the training objective of the whole frame-work:.
j (θ) = lgen + λslsal + λf lfunc,.
(13).
where λs and λf are the hyper-parameters to bal-ance these losses..5 experiments.
5.1 metrics and baselines.
following previous work, we report rouge-1(unigram), rouge-2 (bigram), and rouge-l(longest common subsequence) scores to evaluatethe generated citing sentences (lin, 2004).
im-plementation details are shown in the appendix,section a.2.
we also report rouge f1 score onour dataset.
finally, we compare our model tocompetitive baselines:.
• ptgen (see et al., 2017):.
is the original.
pointer-generator network..• ext-oracle (xing et al., 2020): selects thebest possible sentence from the abstract of a citedpaper that gives the highest rouge w.r.t.
theground truth.
this method can be seen as an upperbound of extractive methods..• ptgen-cross (xing et al., 2020): enhancesthe original pointer-generator network with a crossattention mechanism applied to the citing sen-tence’s context and the cited paper’s abstract..additionally, we report results from using sev-eral extractive methods that have been used forsummarization tasks3, including:.
• lexrank (erkan and radev, 2004): is an un-supervised graph-based method for computing rel-ative importance of extractive summarization..• textrank (mihalcea and tarau, 2004): is anunsupervised algorithm where sentence importance.
3we apply extractive methods on the cited paper’s abstract.
to extract one sentence as the citing sentence..scores are computed based on eigenvector central-ity within weighted-graphs..5.2 experimental results.
as the results in table 2 show, our proposedframework (baco) outperformed all of the con-sidered baselines.
baco achieved scores of32.54 (rouge-1), 9.71 (rouge-2), and 24.90(rouge-l).
we also observed that the extractivemethods performed comparatively poorly and no-tably worse than the abstractive methods.
all ab-stractive methods did better than ext-oracle; aresult different from performance on other sum-marization tasks, such as news document summa-rization.
we think that this deviation from priorperformance outcomes is because citing sentencein the domain of scholarly papers contain new ex-pressions when referring to cited papers, whichrequires high-level summarizing or paraphrasingof cited papers instead of copying sentences ver-batim from cited papers.
our results suggest thatextractive methods may not be suitable for our task.
among the extractive methods we tested, we ob-served ext-oracle to be superior to others, whichaligns with our expectation of ext-oracle to serveas an upper bound of extractive methods.
for ab-stractive methods, our framework achieved about2.57 points improvement on rouge-2 f1 scorecompared to ptgen-cross.
we assume two rea-sons for this improvement: first, baco uses richertext features, e.g., what to cite (sentence salienceestimation) and why to cite (citation function clas-siﬁcation), that provide useful information for thistask.
second, we included structural informationfrom the citation network, which might offer sup-plemental background knowledge about a ﬁeld thatis not explicitly covered by the given cited andciting papers..5.3 ablation study.
we performed an ablation study to investigate theefﬁcacy of the three main components in our frame-work: (1) we removed the node features (papers)that are output from the graph encoder to test theeffectiveness of background knowledge; (2) we re-moved the predicted salience of sentences in theabstracts of cited papers to assess the effectivenessof one part of content (what to cite); and (3) weremoved the training of citation function classiﬁ-cation and only trained the generator to test theeffectiveness of the other part of content (why tocite).
as the removal of node features of papers re-.
1472modelsextractivelexranktextrankext-oracleabstractiveptgenptgen-cross∗baco.
r-1.
r-2.
r-l.11.9612.3522.60.
24.6027.0832.54.
1.041.194.21.
6.167.149.71.
9.6910.0416.83.
19.1920.6124.90.table 2: experimental results for our framework andcomparative models.
∗indicates our re-implementation..models.
r-132.54baco-w/o bk 31.02-w/o sa 31.4330.84-w/o cf.
r-29.718.907.518.67.r-l24.9022.4622.7723.31.table 3: ablation study for different components ofour framework.
w/o = without, bk = backgroundknowledge, sa = salience estimation, cf = citationfunction..duces the input to the dynamic fusion operation forthe context vector (equation 1), we changed equa-tion 2 to a sigmoid function so that the calculatedattention becomes a vector of size 2 when com-bining the context vectors of the citing sentence’scontext and the cited paper’s abstract..table 3 presents the results of the ablation study.
we observed the rouge-2 f1 score to drop by0.81 after the removal of the nodes (papers) fea-ture.
this indicates that considering backgroundknowledge in a structured representation is usefulfor citing sentence generation.
the rouge-2 f1score dropped by 2.20 after disregarding salienceof sentences in the cited paper.
this implies thatsentence-level salience estimation is beneﬁcial, andit can be used to identify important sentences dur-ing the decoding phase so that the decoder can payhigher attention to those sentences.
this process.
fluencyrelevancecoherenceoverall.
gold baco ptgen-cross4.914.864.884.79.
3.643.072.772.95.
3.522.642.612.69.table 4: human evaluation results..might also align with how scholars write citing sen-tences: they focus on speciﬁc parts or elements ofcited papers, e.g., methods or results, and do notconsider all parts equally when writing citing sen-tences.
lastly, the rouge-2 f1 score dropped by1.04 after the removal of citation function classiﬁ-cation; indicating that this feature is also helpfulto the text generation task.
we conclude that for aciting sentence generation, considering and train-ing a model on background knowledge, sentencesalience, and citation function improves the perfor-mance..5.4 case study.
we present an illustrative example generated byour re-implementation of ptgen-cross versus bybaco, and compare both to ground truth (see ap-pendix, section a.3).
the output from bacoshowed a higher overlap with the ground truth,speciﬁcally because it included background thatis not explicitly covered in the cited paper.
fur-thermore, our output contained the correct citationfunction (“... have been shown to be effective”),which was present in the ground truth, but missingin ptgen-cross’s output..5.5 human evaluation.
we sampled 50 instances from the generated texts.
three graduate students who are ﬂuent in englishand familiar with nlp were asked to rate citing sen-tences produced by baco and the re-implementedptgen-cross with respect to four aspects on a 1(very poor) to 5 (excellent) point scale: ﬂuency(whether a citing sentence is ﬂuent), relevance(whether a citing sentence is relevant to the citedpaper’s abstract), coherence (whether a citing sen-tence is coherent within its context), and overallquality.
every instance was scored by the threejudges, and we averaged their scores (table 4).
our results showed that citing sentences gener-ated by baco score were generally better thanoutput by ptgen-cross (e.g., relevance score:baco=3.07; ptgen-cross=2.64).
this ﬁndingprovided further evidence for the effectiveness ofincluding the features we used for this task..6 conclusions and future work.
we have brought together multiple pieces of infor-mation from and about cited and citing papers toimprove citing sentence generation.
we integratedthem into baco, a background knowledge- and.
1473content-based framework for citing sentence gen-eration, which learns and uses information thatrelate to (1) background knowledge; and (2) con-tent.
extensive experimental results suggest thatour framework outperforms competitive baselinemodels..this work is limited in several ways.
we onlydemonstrated the utility of our model within thestandard rnn-based seq2seq framework.
sec-ondly, our citation functions scheme only containedvalence-based items.
finally, while this method isintended to support scholars in practicing strate-gic note taking on prior work with respect to anew literature review or research project, we didnot evaluate the usefulness or effectiveness of thistraining option for researchers..in future work, we plan to investigate the adapta-tion of our framework into more powerful modelssuch as transformer (vaswani et al., 2017).
wealso hope to extend our citation functions schemebeyond valence of the citing sentences to moreﬁne-grained categories, such as those outlinedin moravcsik and murugesan (1975) and lipetz(1965)..impact statement.
this work is intended to support scholars in doingresearch, not to replace or automate any scholarlyresponsibilities.
finding, reading, understanding,reviewing, reﬂecting upon, and properly citing lit-erature are key components of the research processand require deep intellectual engagement, whichremains a human task.
the presented approach ismeant to help scholars to see examples for how tostrategically synthesize scientiﬁc papers relevant toa certain topic or research problem, thereby help-ing them to cope with information overload (or“research deluge”) and honing their scholarly writ-ing skills.
additional professional responsibilitiesalso still apply, such as not violating intellectualproperty/ copyright issues..we believe that this work does not present fore-seeable negative societal consequence.
while notintended, our method may be misused for the auto-mated generation of parts of literature reviews.
westrongly discourage this misuse as it violates basicassumptions about scholarly diligence, responsibili-ties, and expectations.
we advocate for our methodto be used as a scientiﬁc writing training tool..references.
amjad abu-jbara, jefferson ezra, and dragomir radev.
2013. purpose and polarity of citation: towards nlp-in proceedings of 2013 con-based bibliometrics.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies (naacl-hlt)..kimitaka asatani, junichiro mori, masanao ochi, andichiro sakata.
2018. detecting trends in academicresearch from a citation network using network rep-resentation learning.
plos one, 13(5):e0197260..awais athar.
2011. sentiment analysis of citations us-in proceed-.
ing sentence structure-based features.
ings of the acl 2011 student session..dzmitry bahdanau, kyung hyun cho, and yoshuabengio.
2015. neural machine translation by jointlylearning to align and translate.
in 3rd internationalconference on learning representations (iclr)..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inproceedings of 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp)..mark w bruner, karl erickson, brian wilson, and jeancˆot´e.
2010. an appraisal of athlete developmentmodels through citation network analysis.
psychol-ogy of sport and exercise, 11(2):133–139..jingqiang chen and hai zhuge.
2016. summariza-tion of related work through citations.
in 12th inter-national conference on semantics, knowledge andgrids (skg).
ieee..jingqiang chen and hai zhuge.
2019. automatic gen-eration of related work through summarizing cita-tions.
concurrency and computation: practice andexperience, 31(3):e4261..kaihua chen and jiancheng guan.
2011. a bibliomet-ric investigation of research performance in emerg-ing nanobiopharmaceuticals.
journal of informet-rics, 5(2):233–247..arman cohan, waleed ammar, madeleine van zuylen,and field cady.
2019. structural scaffolds for ci-tation intent classiﬁcation in scientiﬁc publications.
in proceedings of 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..john cullars.
1990. citation characteristics of italianand spanish literary monographs.
the library quar-terly, 60(4):337–356..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of 2019 conference of the north.
1474american chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..ying ding, guo zhang, tamy chambers, minsong, xiaolong wang, and chengxiang zhai.
2014.content-based citation analysis: the next generationof citation analysis.
journal of the association forinformation science and technology, 65(9):1820–1833..john duchi, elad hazan, and yoram singer.
2011.adaptive subgradient methods for online learningjournal of machineand stochastic optimization.
learning research, 12(jul):2121–2159..aaron elkiss, siwei shen, anthony fader, g¨unes¸erkan, david states, and dragomir radev.
2008.blind men and elephants: what do citation sum-maries tell us about a research article?
journal ofthe american society for information science andtechnology, 59(1):51–62..g¨unes erkan and dragomir r radev.
2004. lexrank:graph-based lexical centrality as salience in textsummarization.
journal of artiﬁcial intelligence re-search, 22:457–479..cong duy vu hoang and min-yen kan. 2010. to-wards automated related work summarization.
inproceedings of the 23rd international conference oncomputational linguistics (coling)..jianhua hou, xiucai yang, and chaomei chen.
2018.emerging trends and new developments in infor-mation science: a document co-citation analysis(2009–2016).
scientometrics, 115(2):869–892..yue hu and xiaojun wan.
2014. automatic genera-tion of related work sections in scientiﬁc papers: anoptimization approach.
in proceedings of 2014 con-ference on empirical methods in natural languageprocessing (emnlp)..xin huang, chang-an chen, changhuan peng, xudongwu, luoyi fu, and xinbing wang.
2018. topic-sensitive inﬂuential paper discovery in citation net-in paciﬁc-asia conference on knowledgework.
discovery and data mining.
springer..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations(iclr)..xuerong li, han qiao, and shouyang wang.
2017. ex-ploring evolution and emerging trends in businessmodel study: a co-citation analysis.
scientometrics,111(2):869–887..ben-ami lipetz.
1965. improvement of the selectivityof citation indexes to science literature through in-clusion of citation relationship indicators.
americandocumentation, 16(2):81–90..kobra mansourizadeh and ummul k. ahmad.
2011.citation practices among non-native expert andnovice scientiﬁc writers.
journal of english for aca-demic purposes, 10(3):152–161..rada mihalcea and paul tarau.
2004. textrank: bring-in proceedings of 2004 con-ing order into text.
ference on empirical methods in natural languageprocessing (emnlp)..seungwhan moon, leonardo neves, and vitor car-valho.
2018a.
multimodal named entity disambigua-tion for noisy social media posts.
in proceedings ofthe 56th annual meeting of the association for com-putational linguistics (acl)..seungwhan moon, leonardo neves, and vitor car-valho.
2018b.
multimodal named entity recognitionfor short social media posts.
in proceedings of 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies (naacl-hlt)..spencer moore, alan shiell, penelope hawe, and va-lerie a haines.
2005. the privileging of communi-tarian ideas: citation practices and the translation ofsocial capital into public health research.
americanjournal of public health, 95(8):1330–1337..michael j moravcsik and poovanalingam murugesan.
1975. some results on the function and quality ofcitations.
social studies of science, 5(1):86–92..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp)..dragomir r radev, pradeep muthukrishnan, vahedqazvinian, and amjad abu-jbara.
2013. the acl an-thology network corpus.
language resources andevaluation, 47(4):919–944..horacio saggion, alexander shvets, `alex bravo, et al.
2020. automatic related work section generation:experiments in scientiﬁc document abstracting.
sci-entometrics, 125(3):3159–3185..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of 55th annualmeeting of the association for computational lin-guistics (acl)..linda c smith.
1981. citation analysis..library.
trends, 30(3):83–106..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..simone teufel, advaith siddharthan, and dan tidhar.
2006. automatic classiﬁcation of citation function.
in proceedings of 2006 conference on empiricalmethods in natural language processing (emnlp)..1475ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of 31st international con-ference on neural information processing systems(neurips)..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations (iclr)..yongzhen wang, xiaozhong liu, and zheng gao.
2018. neural related work summarization with ain pro-joint context-driven attention mechanism.
ceedings of the 2018 conference on empirical meth-ods in natural language processing (emnlp)..howard d white.
2004..discourse analysis revisited.
25(1):89–116..citation analysis andapplied linguistics,.
xinyu xing, xiaosheng fan, and xiaojun wan.
2020.automatic generation of citation texts in scholarlyin proceedings of 58th an-papers: a pilot study.
nual meeting of the association for computationallinguistics (acl)..michihiro yasunaga, jungo kasai, rui zhang, alexan-irene li, dan friedman, andder r fabbri,dragomir r radev.
2019. scisummnet: a large an-notated corpus and content-impact models for scien-tiﬁc paper summarization with citation networks.
inproceedings of the aaai conference on artiﬁcial in-telligence (aaai)..michihiro yasunaga, rui zhang, kshitijh meelu,ayush pareek, krishnan srinivasan, and dragomirradev.
2017. graph-based neural multi-documentin proceedings of 21st confer-summarization.
ence on computational natural language learning(conll)..hanlin you, mengjun li, keith w hipel, jiang jiang,bingfeng ge, and hante duan.
2017. developmenttrend forecasting for coherent light generator tech-nology based on patent citation network analysis.
scientometrics, 111(1):297–315..he zhao, zhunchen luo, chong feng, anqing zheng,and xiaopeng liu.
2019. a context-based frame-work for modeling the role and function of on-lineresource citations in scientiﬁc literature.
in proceed-ings of 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp)..1476precision recall f1-score95.4391.59.
95.4391.59.
95.4391.59.crosstest.
table 5: the results of the citation function labelingmodel for cross-validation (denoted as cross) and onthe external test data (denoted as test).
#refr to mark the citation of the cited paper.
the reference signs to other papers are maskedas #otherefr.
the #cite in context indicatesthe position where the citing sentence should beinserted.
the output of our framework has a higheroverlap with the ground truth than the output fromptgen-cross.
please note that our frameworkwas able to infer that the mentioned methods inthe generated citing sentence are “supervised”, andwe believe that this knowledge was gained fromthe citation network where other neighboring citedpapers explicitly mentioned “supervised methods”.
also, the generated citing sentence from our frame-work showed a positive citation function (... havebeen shown to be effective) as the ground truth,while ptgen-cross’s output expressed the wrongcitation function (neutral).
we think the underly-ing reason for this difference in outputs may bethat our joint training of citing sentence generationand citation function classiﬁcation, which forcedour framework to recognize the corresponding ci-tation function during the generation and furtherimproved the performance..a appendices.
a.1 experiments for citation function.
labeling model.
to test our citation function labeling model, weapplied 10-fold cross-validation to our trainingdataset with 800 citing sentences.
we then testedour trained model on the test data with 400 sen-tences, which we refer to as the external test set..we set the hidden size of the mlp in our label-ing model to 256, and adopted a dropout with arate of 0.2. for the optimizer, an adam (kingmaand ba, 2015) with a learning rate of 2e-3 wasused.
the batch size was set to 8. we used f1score for evaluating labeling accuracy.
since therewas imbalance among the distributions of labels,we choose the micro-f1 score speciﬁcally.
theresults are shown in table 5. after training, weused the trained model to label the rest of the data(84,376 instances) for further training the citing sen-tence generation model.
the ﬁnal dataset contains85,576 instances.
following (xing et al., 2020), weused the above-mentioned 400 citing sentences asthe test set, and combined the 800 citing sentenceswith the rest of the model-labelled instances as ourtraining set.
the average length of citing sentencesin the training and test data is 28.72 words and26.45 words, respectively..a.2.
implementation details.
we used pre-trained glove vectors (penningtonet al., 2014) to initialize word embeddings withthe vector dimension 300 and followed veliˇckovi´cet al.
(2018) to initialize the node features for thegraph encoder as a bag-of-words representation ofthe paper’s abstract.
the hidden state size of lstmwas set to 256 for the encoder, and 512 for the de-coder.
an adagrad (duchi et al., 2011) optimizerwas used with a learning rate of 0.15 and an ini-tial accumulator value of 0.1. we picked 64 as thebatch size for training.
for the rescaling factor βin equation 11, we chose 40 based on the resultsreported in yasunaga et al.
(2017).
we also usedrouge scores (lin, 2004) for quantitative eval-uation, and reported the f1 scores of rouge-1,rouge-2 and rouge-l for comparing baco toalternative models..a.3 case study.
we present an example generated by our re-implementation of the baseline ptgen-cross, andour framework in table 6. note that we used.
1477the state-of-the-art methods used for relation classiﬁcation areprimarily based on statistical machine learning, and their performancestrongly depends on the quality of the extracted features.
the extractedfeatures are often derived from the output of pre-existing natural lang-uage processing (nlp) systems, which leads to the propagation of theerrors in the existing tools and hinders the performance of these systems.
in this paper, we exploit a convolutional deep neural network (dnn) toextract lexical and sentence level features.
our method takes all of theword tokens as input without complicated pre-processing.
first, theword tokens are transformed to vectors by looking up word embeddings.
then, lexical level features are extracted according to the given nouns.
meanwhile, sentence level features are learned using a convolutionalapproach.
these two level features are concatenated to form the ﬁnalextracted feature vector.
finally, the features are fed into a softmaxclassiﬁer to predict the relationship between two marked nouns.
theexperimental results demonstrate that our approach signiﬁcantlyoutperforms the state-of-the-art methods.
we present a novel approach to relation extraction, based on the obser-vation that the information required to assert a relationship betweentwo named entities in the same sentence is typically captured by theshortest path between the two entities in the dependency graph.
experiments on extracting top-level relations from the ace (automatedcontent extraction) newspaper corpus show that the new shortest pathdependency kernel outperforms a recent approach based on dependencytree kernels.
the task of relation classiﬁcation is to predict semantic relations betweenpairs of nominals and can be deﬁned as follows: given a sentence s withthe annotated pairs of nominals e and e , we aim to identify the relationsbetween e and e #othrefr.
there is considerable interest in automaticrelation classiﬁcation, both as an end in itself and as an intermediate stepin a variety of nlp applications.
#cite.
supervised approaches arefurther divided into feature-based methods and kernel-based methods.
feature-based methods use a set of features that are selected after perform-ing textual analysis.
they convert these features into symbolic ids, whichare then transformed into a vector using a paradigm that is similar to thebag-of-words model.
the most representative methods for relation classiﬁcation use supervisedparadigm ; such methods have been shown to be effective and yieldrelatively high performance #othrefr; #refr.
there has been a wide body of approaches to predict relation extractionbetween nominals #othrefr; #refr.
most methods for relation classiﬁcation are supervised and have beenshown to be effective #othrefr; #refr..table 6: example output citing sentences.
citing paper’s abstract.
cited paper’s abstract.
context.
ground truth.
ptgen-cross.
baco.
1478