language model as an annotator: exploring dialogptfor dialogue summarization.
xiachong feng1, xiaocheng feng1,2∗, libo qin1, bing qin1,2, ting liu1,21harbin institute of technology, china2peng cheng laboratory, china{xiachongfeng,xcfeng,lbqin,bqin,tliu}@ir.hit.edu.cn.
abstract.
current dialogue summarization systems usu-ally encode the text with a number of gen-eral semantic features (e.g., keywords and top-ics) to gain more powerful dialogue modelingcapabilities.
however, these features are ob-tained via open-domain toolkits that are dialog-agnostic or heavily relied on human annota-tions.
in this paper, we show how dialogpt(zhang et al., 2020b), a pre-trained model forconversational response generation, can be de-veloped as an unsupervised dialogue annotator,which takes advantage of dialogue backgroundknowledge encoded in dialogpt.
we applydialogpt to label three types of features ontwo dialogue summarization datasets, sam-sum and ami, and employ pre-trained andnon pre-trained models as our summarizers.
experimental results show that our proposedmethod can obtain remarkable improvementson both datasets and achieves new state-of-the-art performance on the samsum dataset1..1.introduction.
dialogue summarization aims to generate a suc-cinct summary while retaining essential informa-tion of the dialogue (gurevych and strube, 2004;chen and yang, 2020).
theoretically, peyrard(2019) point out that a good summary is intuitivelyrelated to three aspects, including informativeness,redundancy and relevance..to this end, previous works have taken the abovethree aspects into account by incorporating auxil-iary annotations into the dialogue.
to improveinformativeness, some works annotated linguisti-cally speciﬁc words (e.g., nouns and verbs), do-main terminologies and topic words in the dialogue(riedhammer et al., 2008; koay et al., 2020; zhaoet al., 2020).
to reduce redundancy, some works.
∗corresponding author.
1our codes are available at: https://github.com/.
xcfcode/plm_annotator.
used sentence similarity-based methods to anno-tate redundant utterances.
(zechner, 2002; murrayet al., 2005).
to improve relevance, some worksannotated topics for the dialogue (li et al., 2019;liu et al., 2019; chen and yang, 2020).
how-ever, these annotations are usually obtained viaopen-domain toolkits, which are not suitable fordialogues, or require manual annotations, whichare labor-consuming..to alleviate the above problem, we explore thepre-trained language model as an unsupervised an-notator to automatically provide annotations for thedialogue.
recently, some works have investigatedthe use of pre-trained language models in an unsu-pervised manner.
for example, sainz and rigau(2021) exploited pre-trained models for assigningdomain labels to wordnet synsets.
the successfulrecipe is that a model is obtained extensive knowl-edge via pre-training on a huge volume of data.
when it comes to the dialogue domain, dialogpt(zhang et al., 2020b) is a sota conversationalresponse generation model, which is pre-trainedon the massive dialogue data.
therefore, we drawsupport from dialogpt and present our dialogptannotator, which can perform three dialogue anno-tation tasks, including keywords extraction, redun-dancy detection and topic segmentation, to measureinformativeness, redundancy and relevance of theinput dialogue, respectively..keywords extraction aims to automaticallyidentify important words in the dialogue (shownin figure 1(a)).
our dialogpt annotator extractsunpredictable words as keywords.
we assume thatkeywords contain high information, which are dif-ﬁcult to be predicted considering both backgroundknowledge encoded in the dialogpt and contex-tual information of dialogue context.
redundancydetection aims to detect redundant utterances thathave no core contribution to the overall meaning ofthe dialogue (shown in figure 1(b)).
our dialogpt.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1479–1491august1–6,2021.©2021associationforcomputationallinguistics1479figure 1: example dialogue from samsum (gliwa et al., 2019) with the human annotated summary.
(a) keywordsextraction aims to extract words that are most important to the dialogue.
(b) redundancy detection aims to detectnonsigniﬁcant utterances in the dialogue.
(c) topic segmentation aims to divide the whole dialogue into severalﬁne-grained topics.
all three auxiliary information can do good to ﬁnal summary generation..annotator detects utterances that are useless for di-alogue context representation as redundant.
weassume that if adding a new utterance does notchange the dialogue context representation, thenthis utterance has no effect on predicting the re-sponse, so it is redundant.
topic segmentationaims to divide a dialogue into topically coherentsegments (shown in figure 1(c)).
our dialogptannotator inserts a topic segmentation point beforeone utterance if it is unpredictable.
we assume thatif an utterance is difﬁcult to be inferred from thedialogue context based on dialogpt, this utterancemay belong to a new topic..we use our dialogpt annotator to annotate thesamsum (gliwa et al., 2019) and ami (carlettaet al., 2005) datasets.
each annotation is convertedinto a speciﬁc identiﬁer and we insert them into thedialogue text.
then, we employ pre-traind bart(lewis et al., 2020) and non pre-trained pgn (seeet al., 2017) as our summarizers.
extensive experi-mental results show that our method can obtain con-sistent and remarkable improvements over strongbaselines on both datasets and achieves new state-of-the-art performance on the samsum dataset..2 preliminaries.
in this section, we will describe the task deﬁnitionas well as the background of dialogpt..2.1 task deﬁnition.
given an input dialogue d, a dialogue summa-rizer aims to produce a condensed summary s,where d consists of |d| utterances [u1, u2, ...u|d|]and s consists of |s| words [s1, s2, ...s|s|].
eachutterance ui is compose of a sequence of words.
[ui,1, ui,2, ...ui,|ui|, eosi], where i ∈ [1 :|d|]and eosi indicates the end of the utterance.
be-sides, each utterance ui associates with a speakerpi.
thus, this task can be formalized as producingthe summary s given the dialogue sequence: d =[p1, u1,1, ..., eos1, ..., p|d|, u|d|,1, ..., eos|d|].
2.2 dialogpt.
dialogpt (zhang et al., 2020b) is a neural con-versational response generation model, which in-herits from gpt-2 (radford et al., 2019) and istrained on 147m conversation-like exchanges ex-tracted from reddit comment chains.
there are3 different sizes of the model with total parame-ters of 117m, 345m and 762m respectively.
itachieves state-of-the-art results over various dia-logue generation benchmarks.
given the dialoguecontext ui−1 = [ui−1,1, ..., ui−1,|ui−1|, eosi−1],dialogpt aims to produce the response ui =[ui,1, ..., ui,|ui|, eosi], which can be formalized asthe conditional probability of p (ui|ui−1).
it ﬁrsttakes the context word sequence of no more than1024 tokens and outputs the representation of the se-quence hi = (h i−1,1, ..., h i−1,|ui−1|, h i−1,eosi−1),where h i−1,eosi−1 can be viewed as the repre-sentation of dialogue context ui−1.
then, di-alogpt starts decoding the response by attend-ing to the context token representations and par-tially decoded response tokens until reaching eos.
the loss function is the negative log-likelihoodof the response word sequence ldialogp t =− (cid:80)|ui|t=1 log p (ui,t|ui,1 .
.
.
ui,t−1, ui−1).
it’s worthnoting that dialogpt tokenizes texts with the samebyte-pair encoding as gpt-2, thus either context orresponse tokens are tokenized into subwords..1480remember we are seeing the wedding planner after worksure, where are we meeting her?
at nonnarita’s i want to order seafood tagliatelle hahawhy notwe remmberspaghetti pomodoro disaster from our last meetingomg it was over her white blousei'll make timefor itgreat!blair:chuck:blair:chuck: blair:chuck:blair:chuck:blair:[topic3][topic2][topic1]dialogueremember we are seeing the wedding planner after worksure, where are we meeting her?
at nonnarita’s i want to order seafood tagliatelle hahawhy notwe remmberspaghetti pomodoro disaster from our last meetingomg it was over her white blousei'll make timefor itgreat!blair:chuck:blair:chuck: blair:chuck:blair:chuck:blair:dialogueremember we are seeing the wedding planner after worksure, where are we meeting her?
at nonnarita’s i want to order seafood tagliatellehahawhy notwe remmberspaghetti pomodoro disasterfrom our last meetingomg it was over her white blousei'll make timefor itgreat!blair and chuck are going to meet the wedding plannerafter work at nonnarita’s.
the tagliatelleserved at nonnarita’s are very good.
blair:chuck:blair:chuck: blair:chuck:blair:chuck:blair:dialoguesummary[topic1][topic2](c) topic segmentation (b) redundancy detection (a) keywords extraction (i) given one dialogue, we preprocess it into two formats:figure 2: illustration of our dialogpt annotator.
context-response pairs and the dialogue sequence.
(ii) we input them into the dialogpt, after the forward pass,we can get the word-level and utterance-level predicted losses and representations for dialogue context.
(iii) weperform three annotation tasks: keywords extraction, redundancy detection and topic segmentation.
finally, wecan get a labelled dialogue.
#key#, [rd] and [ts] are speciﬁc tags, which are inserted into the dialogue..3 method.
3.2 dialogpt forward passing.
in this section, we will ﬁrst introduce our dialogptannotator.
the workﬂow consists of three steps(1) dialogue preprocessing; (2) dialogpt forwardpassing; (3) annotation.
the overall frameworkis shown in figure 2. then, we will describe ourdialogue summarizer, including bart and pgn..3.1 dialogue preprocessing.
preprocessingoriginal.
dialogueform the[p1, u1,1, ..., eos1, ..., p|d|, u|d|,1, ..., eos|d|]into the format that dialogpt can process..todialogue d.aims.
trans-=.
speciﬁcally, we transform it into two formats.
the ﬁrst one is context-response pairs (shown infigure 2(a)).
given a dialogue d, two adjacentutterances (ui−1, ui) are combined into a context-response pair, where i ∈ [2 : |d|] .
the second oneis dialogue sequence (shown in figure 2(b)).
allthe utterances in the dialogue d are serialized intoa sequence [u1,1, ..., eos1, ..., u|d|,1, ..., eos|d|],with eos separates each utterance..note that either for context-response pairs or thedialogue sequence, we do not take speaker infor-mation p into consideration.
the reason is thatdialogpt is trained on a huge volume of conver-sational data without speaker information.
evenso, zhang et al.
(2020b) proved that dialogpt cansimulate real-world dialogues in various scenes andhas already learned diverse response generationpatterns between the same speakers or differentspeakers according to the given context..dialogpt forward passing has two purposes.
(1)for each context-response pair, we aim to get theword-level and utterance-level predicted losses forthe response (shown in figure 2(c)).
(2) for the di-alogue sequence, we aim to get the representationsfor each eos (shown in figure 2(d))..for the ﬁrst purpose, given one context-responsepair (ui−1, ui), we input the context words ui−1 =[ui−1,1, ui−1,2, ..., ui−1,|ui−1|, eosi−1] into the di-alogpt and start to decode the response.
ateach decode step t, we calculate the negative log-likelihood between the predicted distribution andthe golden target from the given response.
lossi,t = − log p (ui,t|ui,<t, ui−1).
lossi =.
1|ui| + 1.
|ui|+1(cid:88).
t=1.
lossi,t.
(1).
where lossi,t and lossi are the predicted losses foreach word and each utterance respectively2..for the second purpose, after the single forwardpass of dialogpt over the dialogue sequence, wecan get representations h for each token on thetop of the dialogpt.
afterward, we extract allrepresentations for each eos..h eos1, h eos2, ..., h eos|d| = h (eos)where each h eosi can be viewed as the representa-tion for the dialogue context [u1, ..., ui]..(2).
2note that dialogpt uses bpe to tokenize texts, thus,losses are calculated at the sub-word level.
we recover theword-level predicted loss by averaging the losses of multiplesub-words.
besides, since the ﬁrst utterance u1 can only beserved as the context, so we do not compute loss for u1..1481context:yooguys.eos1response: hey wassup.
eos2context:hey wassup.
eos2response: remmberthemeeting eos3context:remmberthemeeting eos3response: ialmostforgetit.
eos4context: ialmostforgetit.
eos4response: fine eos5context: fine eos5response: where?
eos6context: where?
eos6response: atbarbara's place.
eos7tom: yooguys.
eos1john: hey wassup.
eos2tom: remmberthemeetingeos3john: ialmostforgetit.
eos4tom:fine eos5john: where?
eos6tom: atbarbara's place.
eos7(a) context-response pairs(b) dialogue sequencedialogptyooguys.eos1hey wassup.
eos2remmberthemeeting eos3ialmostforgetit.
eos4fine eos5where?
eos6atbarbara's place.
eos7original dialogueyooguys.eos1...at barbara's place.
eos7...dialogptheyremmberwassup.themeetingeos2eos3remmberthemeetinggolden:loss31(d) dialogue contextrepresentation(c) word-level and utterance-level lossprediction:𝒉𝑬𝑶𝑺𝟏(g) redundancy detection 0.7110.9980.9910.6420.5730.993(e) keywords extraction (f) topic segmentation loss3segmentationpoint loss32loss33loss34...extracted keywordstom: yooguys.
eos1john: [rd] hey wassup.
eos2[ts]tom: remmberthemeeting eos3john: [rd] ialmostforgetit.
eos4tom:[rd] fineeos5[ts]john: where?
eos6tom: atbarbara's place.
eos7#key# tom john meetingbarbara'slabelled dialogue(ⅰ) dialogue preprocessing(ⅲ) annotation(ⅱ) dialogptforward passingavgloss2loss3loss4loss5loss6loss7𝒉𝑬𝑶𝑺𝟐𝒉𝑬𝑶𝑺𝟕segmentationpoint 𝒉𝑬𝑶𝑺𝟕𝒉𝑬𝑶𝑺𝟏𝒉𝑬𝑶𝑺𝟐𝒉𝑬𝑶𝑺𝟑𝒉𝑬𝑶𝑺𝟒𝒉𝑬𝑶𝑺𝟓𝒉𝑬𝑶𝑺𝟔loss31loss32loss33loss34...loss71loss72loss73loss34dialogptannotator3.3.2 redundancy detection: dialogptrd.
motivation dialogpt inherits a decoder archi-tecture, where one token attends to all previoustokens to aggregate information.
thus, giventhe representation h eosi for each eosi, it can beviewed as the representation for the dialogue con-text [u1, u2, ..., ui].
adding a new utterance ui+1,if the new context representation h eosi+1 is simi-lar to the previous h eosi, we assume that the newutterance ui+1 brings little information and hassmall effects on predicting the response, thus ui+1becomes a redundant utterance..we start with the last two dialogue context repre-sentations h eos|d|−1 and h eos|d|, and calculate thecosine similarity between them.
if the similarityscore exceeds the threshold trd, the utterance u|d|is detected as redundant.
trd is a hyper-parameter.
if the similarity score doesn’t exceed the thresholdtrd, we move forward one step to calculate thesimilarity between h eos|d|−2 and h eos|d|−1, andrepeat the process until reaching h eos1.
an exam-ple is shown in figure 3..we insert a speciﬁc tag [rd] before eachfor example,if utter-redundant utterance.
ance u1 is redundant,the new dialogue withredundant utterances annotation is drd =[p1, [rd], u1,1, ..., eos1, ..., p|d|, ..., eos|d|]..3.3.3 topic segmentation: dialogptts.
motivation dialogpt is skilled in generating thecontext-consistent response.
therefore, if the re-sponse is difﬁcult to be predicted given the contextbased on dialogpt, we assume the response maybelong to another topic and there is a topic segmen-tation between the context and response..given a dialogue d, we have loss lossi for eachutterance ui, where i ∈ [2 : |d|].
we select rtspercent of utterances with the highest loss as topicsegmentation points.
rts is a hyper-parameter5.
before each selected utterance, we insert a speciﬁctag [ts].
for example, if there is a segmenta-tion point between utterance u1 and utterance u2,the new dialogue with topic annotation is dts =[p1, u1,1, ..., eos1, [ts], p2, u2,1, ..., eos2, ...]..5we use a heuristic rule to predetermine the possible valueof rts by calculating the average of the number of summarysentences divided by the number of dialogue utterances in thetrain set.
this is based on the observation that each sentencein golden summary tends to correspond to one topic of thedialogue.
we search the best rts based on the calculated score..figure 3: illustration of redundancy detection process.
the initial redundant utterances set is ∅.
h eosi is therepresentation for dialogue context covering the ﬁrst iutterances.
we detect redundant utterances based onthe cosine similarity between representations of dia-logue context.
for example, the similarity score be-tween h eos4 and h eos5 exceeds the pre-deﬁned thresh-old (trd is 0.99), which means adding utterance u5 intothe dialogue context brings little information, thus theutterance u5 is detected as redundant..3.3 annotation.
3.3.1 keywords extraction: dialogptkemotivation considering both background knowl-edge encoded in the dialogpt and contextual in-formation of the dialogue context, if one word inthe golden response is difﬁcult to be inferred fromdialogpt, we assume that it contains high infor-mation and can be viewed as a keyword..given a dialogue d, we have loss lossi,j foreach word ui,j, where i ∈ [2 : |d|].
we extractrke percent of words with the highest loss as key-words, where rke is a hyper-parameter3.
more-over, the names of all speakers p mentioned inthe dialogue are also added into the keywords set.
finally, we append a speciﬁc tag #key# and thekeywords to the end of the original dialogue d.the new dialogue with keywords annotation is#key#, p, key1, key2, ...].4dke = [p1, u1,1, ...,(cid:124)(cid:123)(cid:122)(cid:125)(cid:125)(cid:124)keywords.
(cid:123)(cid:122)d.3we use a heuristic rule to predetermine the possible valueof rke by calculating the average of length of summaries(remove stopwords) divided by the length of dialogues in thetrain set.
we search the best rke based on the calculated score.
4in experiments, we ﬁnd that the predicted loss for the ﬁrstword of each utterance is extremely high, probably due to theﬁrst word in the response is the most uncertain and hard to bepredicted.
thus, we ignore the ﬁrst word of each utterance..14823.4 summarizer.
we employ two kinds of summarizer, one is bart(lewis et al., 2020), which is a transformer-basedmodel and pre-trained on a huge volume of data.
the other one is pgn (see et al., 2017), which isa lstm-based model.
both models inherit a typi-cal sequence-to-sequence framework, which ﬁrstencodes the source dialogue d to distributed repre-sentations and then generates the target summarys with the decoder..bart bart adopts the transformer (vaswaniet al., 2017) as the backbone architecture.
it ﬁrstmap the source dialogue into distributed represen-tations, based on which a decoder generates thetarget sequence:.
m #usmas.i.ma.avg.turnsavg.tokensavg.sum#avg.turnsavg.tokensavg.sum.
train1473211.13120.2622.8197310.234859.52323.74.valid81810.72117.4622.8020345.705056.25321.25.test81911.24122.7122.4720324.405257.80328.20.table 1: statistics for samsum and ami datasets.
“#” means the number of dialogue-summary pairs,“avg.turns”, “avg.tokens” and “avg.sum” mean theaverage number of turns of dialogues, tokens of dia-logues and tokens of summaries respectively..the outputs in a parallel training corpus (d, s):.
arg max.
θ.
(cid:88).
(d,s)∈(d,s).
log p(s | d; θ)..(4).
x n = encoder(x 0).
n:=n=1y m = decoder(y 0, x n ).
ffn (cid:0)att(x n−1)(cid:1).
ffn (cid:0)att (cid:0)att(y m−1), x n (cid:1)(cid:1).
4 experiments.
4.1 datasets.
m:=m=1.
n:=n=1.
where.
denotes n identical encoding layers,.
mdenotes m identical decoding layers, x 0 de-:=m=1notes the sum of the word embeddings x emb andposition embeddings x pos of d, y 0 denotes thatof the shifted right s, ffn(·) denotes a position-wise feed-forward network, and att(·) denotesa multi-head attention.
residual connection (heet al., 2016) and layer normalization (ba et al.,2016) are used in each sub-layer, which are sup-pressed in equation 3 for clarity.
finally, the outputrepresentation y m of the decoder is projected intothe vocabulary space and the decoder outputs thehighest probability token..pgn pgn is a hybrid model of the typicalseq2seq attention model (nallapati et al., 2016)and pointer-network (vinyals et al., 2015).
theinput dialogue is fed into the lstm encoder tokenby token, producing the encoder hidden states.
thedecoder receives word embedding of the previousword and generates a distribution to decide the tar-get token, retaining decoder hidden states.
pgn notonly allows to generate from the ﬁxed vocabulary,but also allows to copy from the input tokens..training objective model parameters θ aretrained to maximize the conditional likelihood of.
(3).
we experiment on 2 datasets (statistics in table 1):samsum (gliwa et al., 2019) is a human-generated dialogue summary dataset, which con-tains dialogues in various scenes of the real-life.
ami (carletta et al., 2005) is a meeting summarydataset.
each meeting contains four participantsand is about a remote control design project..4.2.implementation details.
dialogpt we initialize dialogpt with dialogpt-large6.
for samsum, we set keywords extractionratio rke to 15, similarity threshold trd to 0.99 andtopic segmentation ratio rts to 15. for ami, rkeis 4, trd is 0.95 and rts is 5 7.bart we initialize bart with bart.large8 .
forﬁne-tuning on samsum, the learning rate is set to3e-05, the dropout rate is 0.1, the warmup is set to400. at the test process, beam size is 5, minimumdecoded length is 5 and maximum length is 100.pgn the word embedding size is set to 300 andinitialized with the pre-trained glove vector.
thedimension of encoder and pointer decoder is setto 200. the dropout is set to 0.5. the learningrate is 0.001. at the test process, beam size is 10,minimum decoded length is 280 and maximumlength is 4509..6https://huggingface.co/transformers7we show more hyper-parameter search results for sam-.
sum and ami datasets in the supplementary ﬁle..8https://github.com/pytorch/fairseq9https://github.com/opennmt/opennmt-py.
1483model.
r-2.
r-l.model.
r-2.
r-l.r-1extractive32.4629.27abstractive36.6242.0343.1139.7753.42.
10.278.02.
11.1818.0719.1516.5827.98.
29.9228.78.
33.0639.5640.4938.4249.97††.
ours.
52.9853.43††53.3953.3453.70†.
27.6728.03††28.0127.8528.79†.
49.0649.9349.4949.6450.81†.
longest-3textrank.
transformerd-hgntgdgadialogptmv-bart.
bartbart(dke)bart(drd)bart(dts)bart(dall).
table 2: test set results on the samsum dataset,where “r” is short for “rouge”.
bart means ﬁne-tuning bart on the original samsum.
bart(dke),bart(drd) and bart(dts) represent ﬁne-tuningbart on the samsum with keywords, redundancyand topic annotation respectively.
dall means thesamsum with all three annotations.
† and †† indicatethe ﬁrst-ranked and second-ranked results respectively..4.3 baselines and metrics.
for samsum, longest-3 views the ﬁrst threeutterances as the summary.
textrank (mihal-cea and tarau, 2004) is a traditional graph-basedmethod.
transformer (vaswani et al., 2017) is aseq2seq method based on full self-attention oper-ations.
d-hgn (feng et al., 2020a) incorporatescommonsense knowledge to help understand di-alogues.
tgdga (zhao et al., 2020) uses topicwords and models graph structures for dialogues.
dialogpt (zhang et al., 2020b) means that ﬁne-tuning dialogpt on the samsum.
mv-bart(chen and yang, 2020) is a bart-based methodthat incorporates topic and stage information..for ami, summarunner (nallapati et al.,2017) is an extractive method based on hierar-chical rnn network.
uns (shang et al., 2018)is a fully unsupervised and graph-based method.
topicseg (li et al., 2019) incorporates topics tomodel the meeting.
hmnet (zhu et al., 2020) is atransformer-based method that incorporates posand entity information and is pre-trained on newssummarization dataset..we adopt rouge (lin, 2004) and bertscore.
textranksummarunner.
6.135.54.
15.7013.91.r-1extractive35.1930.98abstractive37.8651.53††52.36†ours.
7.8412.2318.63†.
13.7225.47†24.00.
48.3450.2250.6248.5950.91.
16.0217.7416.8616.0717.75††.
23.4924.1124.2724.0524.59††.
unstopicseghmnet.
pgnpgn(dke)pgn(drd)pgn(dts)pgn(dall).
test set.
results on the ami dataset.
table 3:pgn(dke), pgn(drd) and pgn(dts) represent train-ing pgn on the ami with keywords, redundancy andtopic annotation respectively..samsum.
ami.
modelbartmv-bartbart(dall).
bs model.
pgn.
86.9188.46 hmnet90.04.pgn(dall).
bs80.5182.2482.76.table 4: test set results on the samsum and ami.
“bs” is short for bertscore..4.4 automatic evaluation.
the results on samsum and ami are shown in ta-ble 2 and 3 respectively.
we can see that using ourannotated datasets dke, drd and dts, both bartand pgn can obtain improvements.
furthermore,our bart(dall) achieves sota performance..for samsum, it’s worth noting that bart(dke)performs better compared with bart(drd) andbart(dts).
we attribute this to the fact that key-words can retain essential information for shorterdialogues.
for ami, pgn(drd) contributes themost, which shows the importance of detecting re-dundancy in verbose meeting transcripts.
althoughhmnet and topicseg achieve better scores, hm-net needs news summarization dataset to pre-trainthe model and topicseg designs complex attentionmechanism to incorporate topic information..in terms of new embedding-based metricbertscore (shown in table 4), our methodbart(dall) and pgn(dall) can consistently out-perform the baseline models10..(zhang et al., 2020a) for evaluating our models..10evaluation details are shown in the supplementary ﬁle..1484musmas.i.ma.modelgoldenbartmv-bartbart(dke)bart(drd)bart(dts)bart(dall)goldenpgnhmnetpgn(dke)pgn(drd)pgn(dts)pgn(dall).
info.
4.373.663.853.883.743.95††4.05†4.702.923.52†3.203.153.053.33††.
conc.
cov.
4.274.263.663.653.883.763.793.773.98 †3.894.01††3.764.08†3.78††4.353.852.703.083.40†2.403.003.083.25†3.003.17††3.10††3.25†3.10.table 5: human evaluation results.
“info.” is short forinformativeness, “conc.” for conciseness, “cov.” forcoverage.
for samsum, the inter-annotator agreement(fleiss’ kappa) scores for each metric are 0.46, 0.37 and0.43 respectively.
for ami, fleiss’ kappa scores are0.48, 0.40 and 0.41 respectively..4.5 human evaluation.
we conduct a human evaluation of the dialoguesummary to assess its informativeness, concisenessand coverage.
informativeness measures how wellthe summary includes key information.
concise-ness measures how well the summary discards theredundant information.
coverage measures howwell the summary covers each part of the dialogue..we randomly sample 100 dialogues (samsum)and 10 meetings (ami) with corresponding gener-ated summaries to conduct the evaluation.
in orderto reduce variance caused by humans, we have 4human evaluators and they were asked to rate eachsummary on a scale of 1 to 5 (higher is better) foreach metric.
the results are shown in table 5..we can see that our method can achieve higherscores in all three metrics.
especially, combinedwith drd, our model can get the best score in con-ciseness.
besides, combined with dts, our modelcan perform better in coverage.
however, hmnetgets the best score in informativeness and coverage.
we argue this is because hmnet forces a minimumsummary length of 400. due to this, it scores theworst in conciseness.
for the ami, we also ﬁndthere is still a gap between the scores of generatedsummaries and the scores of golden summaries,indicating that the ami is more difﬁcult..method.
r-1.
r-2.
r-l.rule-based methods.
entitiesnouns and verbs.
53.3652.75.
27.7127.48.
49.6948.82.traditional methods.
49.33textranktopic words49.59pre-trained language model-based methodskeybert.
27.6627.76.
53.2953.28.w/ bert embw/ dialogpt emb.
52.3953.14.
27.1427.25.
48.5249.42.dialogptke.
53.43.
28.03.
49.93.ours.
table 6: test set results of ﬁne-tuning bart on thesamsum that is annotated with keywords using vari-ous methods.
entities, nouns and verbs are obtainedby qi et al.
(2020).
topic words are obtained by apre-trained lda model (narayan et al., 2018).
key-bert (grootendorst, 2020) leverages pre-trained lan-guage model embeddings to create keywords..methodtextrankentitiesdialogptke.
f1.
precision recall47.74% 17.44% 23.22%60.42% 17.80% 25.38%33.20% 29.49% 30.31%.
table 7: quantitative evaluation for keywords on sam-sum test set by viewing reference summary words asgolden keywords..4.6 analysis.
effect of dialogptke.
to verify the effective-ness of our dialogptke method, we ﬁne-tunebart on samsum, which is annotated by var-ious keywords extraction methods.
the results areshown in table 6. we can see that our methodachieves higher scores.
the results also show thatentities play an important role in the summary gen-eration.
besides, combined with dialogpt embed-dings, keybert can get better results..to give a quantitative evaluation, we view ref-erence summary words as golden keywords andcalculate the precision, recall and f1 scores for ex-tracted keywords.
the results are shown in table7. directly using entities as keywords can get thebest precision score.
however, both textrank andentities perform poorly in recall.
our method getsthe best score in terms of f1 and its advantage ismainly reﬂected in recall score, which shows ourmethod can extract more diverse keywords..1485model.
r-2.
r-l.model.
r-2.
r-l.r-1samsum53.0053.39ami50.1950.62.
27.7128.01.
49.6849.49.
16.4516.86.
23.9524.27.rule-baseddialogptrd.
rule-baseddialogptrd.
table 8: test set results on the samsum and amidatasets that are annotated with redundant utterances.
“rule-based” indicates annotating utterances that con-tain no noun, verb and adjective as redundant..effect of dialogptrd.
to verify the effective-ness of our dialogptrd method, we compare itwith a rule-based method (dinarelli et al., 2009),which annotates utterances without noun, verb andadjective as redundant.
the results are shown in ta-ble 8. we can see that our method performs better.
especially, our method shows more advantages forlong and verbose meeting transcripts in the ami..effect of dialogptts.
to verify the effective-ness of our dialogptts method, we compare itwith the c99 algorithm (choi, 2000), which isa sentence similarity-based segmentation method.
chen and yang (2020) enhance it with bert (de-vlin et al., 2019) embeddings.
we further combinethe algorithm with dialogpt embeddings.
theresults are shown in table 9. we can see thatour method can get comparable results with thestrong baseline c99(w/ dialogpt emb).
for ami,combined with golden topic annotation, pgn canachieve the best result, which shows modeling top-ics is an essential task for dialogue summarization..4.7 case study.
figure 4 shows summaries generated by differentmodels for an example dialogue in the samsumdataset.
we can see that bart (lewis et al., 2020)tends to generate long and redundant summaries.
by incorporating topic and stage information, mv-bart (chen and yang, 2020) can generate sum-maries that cover main topics of the dialogue.
how-ever, it still suffers from redundancy problem.
ourbart(dall) can get higher rouge scores whilegenerating better summaries.
the generated sum-mary can include extracted keywords and corre-spond to each topic of the dialogue.
we also ﬁndthat even some redundant utterances have alreadybeen detected, our model still generate the sum-mary contains some redundant information.
we.
r-1samsum.
w/ bert embw/ dialogpt embdialogptts.
52.8053.3353.34.
27.7828.0427.85.
49.5049.3949.64.ami.
50.28.
19.73.
24.45.c99.
goldenc99.
w/ bert embw/ dialogpt embdialogptts.
48.5349.2248.59.
15.8416.7916.07.
23.6323.8824.05.table 9: test set results on samsum and ami that areannotated with topic segmentation in various methods.
c99 (choi, 2000) segments dialogues based on inter-sentence similarities.
beside, the ami has golden topicsegmentation annotations..attribute this to the fact that the small dataset leadsto insufﬁcient training of the model..5 related work.
dialogue summarization current works mainlyincorporate auxiliary information to help bettermodeling dialogues.
some works used varioustypes of keywords to identify the core part of thedialogue, including entities (zhu et al., 2020), do-main terminologies (koay et al., 2020) and topicwords (zhao et al., 2020).
some works aimed toreduce redundancy, zechner (2002); murray et al.
(2005) used sentence-level similarity-based meth-ods.
some works incorporate topics as a coarse-grained dialogue structure (li et al., 2019; liu et al.,2019; chen and yang, 2020).
other works alsoexplored dialogue act (goo and chen, 2018), dia-logue discourse (feng et al., 2020b) and common-sense knowledge (feng et al., 2020a).
in this paper,we combine three types of auxiliary informationto help better modeling dialogues, including key-words, redundant utterances and topics.
pre-trained language models pre-trained mod-els such as bert (devlin et al., 2019) and gpt-3(brown et al., 2020) have advanced various nlptasks.
on one hand, some works utilized theknowledge contained in pre-trained models by ﬁne-tuning on supervised data of downstream tasks(qin et al., 2019; liu and lapata, 2019; qin et al.,2020).
on the other hand, some works examinedthe knowledge in an unsupervised manner (jianget al., 2020; xiao et al., 2020; lin et al., 2020).
ku-.
1486figure 4: example dialogue in the samsum dataset and summaries generated by different models.
keyowrds,redundant utterances and topics are annotated by our dialogpt annotator.
“r” is short for rouge.
our modelbart(dall) can get higher rouge scores while generating the better summary..mar et al.
(2020) explored pre-trained models forconditional data augmentation.
wang et al.
(2020)used the knowledge in pre-trained models to con-struct knowledge graphs.
in this paper, we belongto the second paradigm and propose our dialogptannotator that can perform three annotation tasksin an unsupervised manner..6 conclusion.
acknowledgments.
this work is supported by the national key r&dprogram of china via grant 2018yfb1005103and national natural science foundation of china(nsfc) via grant 61906053 and 61976073. wethank all the anonymous reviewers for their insight-ful comments.
we also thank lifu huang and xin-wei geng for helpful discussion..we investigate to use dialogpt as unsupervised an-notators for dialogue summarization, including key-words extraction, redundancy detection and topicsegmentation.
we conduct our dialogpt annotatoron two datasets, samsum and ami.
experimentalresults show that our method consistently obtainsimprovements upon pre-traind summarizer (bart)and non pre-trained summarizer (pgn) on bothdatasets.
besides, combining all three annotations,our summarizer can achieve new state-of-the-artperformance on the samsum dataset..references.
jimmy lei ba, jamie ryan kiros, and geoffrey e hin-.
ton.
2016. layer normalization.
in arxiv..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjamin.
1487rob : hey there , what's up ?
bob : not much , watching the game .
you ?
rob : same .
having a few people over .
rob : but the game is boring as fuck lol .
that's why i'm writing bob : yeah , true that rob : any plans for the weekend ?
bob : most likely the usual run some errands , cook some food , go out for a few beers .
nothing super interesting have appearedyet rob : i've heard that jim is planning to celebrate his birthday bob : oh right , his birthday is like next wednesday ?
rob : yeah , normally that would make the next weekend a good timebut he is going for a skiing trip with his family rob : sohe said that he might organize something this weekend rob : [rd] nothing super fancy most likely a meetup with a few friends at some bar rob : would you like to come ?
bob : sure , that would be nice bob : but he has not invited me , so i don't want to be rude rob : [rd] most likely because it is not a real party .
when i see himi'll let him know bob : [rd] that would be cool i actually haven'tseen him in person for a while now rob : [rd] yeah , facebook does that to people bob : ok , take care and see you on weekend !
rob : yeah , see you then !
#key# rob bob watchinghaving people boring fuck writing true run some cook have appeared jimcelebrate right normally weekend skiing said organizesuper fancy most invited when facebook does take weekendbartrob is watching the game .
bob is having a few people over .
jim's birthday is next wednesday .
he is going for a skiing trip with his family .
he might organize a meetup with a few friends at some bar this weekend .
rob will let bob know if he can come .
bob hasn't seen jim in person for a while .mv-bartbob and rob are watching the game.
jim is going for a skiing trip with his family next weekend.
he might organize a meetup with a few friends at some bar this weekend.
bob will let him know if he wants to come.bob hasn't seen jim in person for a while .bart(dall)roband bob are watchingthe game .
jimis going for a skiing trip with his family next weekend.
he might organizea meetup with a few friends at some bar this weekend.
robwill let him know if he can come .goldenrob and bob are watchingthe game .
bob will run some errands on the weekend.
jim's birthday is next wednesday .
he might organize a meetup this weekend .
bob will see rob on the weekend .
[topic1][topic2][topic3][topic4][topic1][topic2][topic3]r-1 : 50.00r-2 : 29.79r-l : 48.46r-1 : 52.27r-2 : 23.26r-l : 47.62r-1 : 54.55r-2 : 29.33r-l : 53.10chess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in advances in neural information processingsystems 33: annual conference on neural informa-tion processing systems 2020, neurips 2020, de-cember 6-12, 2020, virtual..jean carletta, simone ashby, sebastien bourban, mikeflynn, mael guillemot, thomas hain, jaroslavkadlec, vasilis karaiskos, wessel kraaij, melissathe ami meeting cor-kronenthal, et al.
2005.in international work-pus: a pre-announcement.
shop on machine learning for multimodal interac-tion.
springer..jiaao chen and diyi yang.
2020. multi-view sequence-to-sequence models with conversational structurefor abstractive dialogue summarization.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4106–4118, online.
association for computationallinguistics..freddy y. y. choi.
2000. advances in domain inde-pendent linear text segmentation.
in 1st meeting ofthe north american chapter of the association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..marco dinarelli, silvia quarteroni, sara tonelli,alessandro moschitti, and giuseppe riccardi.
2009.annotating spoken dialogs: from speech segmentsto dialog acts and frame semantics.
in proceedingsof srsl 2009, the 2nd workshop on semantic rep-resentation of spoken language, pages 34–41..xiachong feng, x. feng, b. qin, and t. liu.
2020a.
incorporating commonsense knowledge into ab-stractive dialogue summarization via heterogeneousgraph networks.
arxiv, abs/2010.10044..xiachong feng, xiaocheng feng, bing qin, and xin-wei geng.
2020b.
dialogue discourse-aware graphmodel and data augmentation for meeting summa-rization..bogdan gliwa, iwona mochol, maciej biesek, andaleksander wawer.
2019.samsum corpus: ahuman-annotated dialogue dataset for abstractivesummarization.
in proceedings of the 2nd workshopon new frontiers in summarization, pages 70–79,hong kong, china.
association for computationallinguistics..chih-wen goo and yun-nung chen.
2018. abstrac-tive dialogue summarization with sentence-gatedmodeling optimized by dialogue acts.
2018 ieeespoken language technology workshop (slt),pages 735–742..maarten grootendorst.
2020. keybert: minimal key-.
word extraction with bert..iryna gurevych and michael strube.
2004. semanticsimilarity applied to spoken dialogue summarization.
in coling 2004: proceedings of the 20th inter-national conference on computational linguistics,pages 764–770, geneva, switzerland.
coling..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in 2016 ieee conference on computer vi-sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 770–778.
ieee computer society..zhengbao jiang, frank f. xu, jun araki, and grahamneubig.
2020. how can we know what languagemodels know?
transactions of the association forcomputational linguistics, 8:423–438..jia jin koay, alexander roustai, xiaojin dai, dillonburns, alec kerrigan, and fei liu.
2020. howdomain terminology affects meeting summarizationin proceedings of the 28th inter-performance.
national conference on computational linguistics,pages 5689–5695, barcelona, spain (online).
inter-national committee on computational linguistics..varun kumar, ashutosh choudhary, and eunah cho.
2020. data augmentation using pre-trained trans-former models.
in proceedings of the 2nd workshopon life-long learning for spoken language systems,pages 18–26, suzhou, china.
association for com-putational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..manling li, lingyu zhang, heng ji, and richard j.radke.
2019. keep meeting summaries on topic:abstractive multi-modal meeting summarization.
inproceedings oftheassociation for computational linguistics, pages2190–2196, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
bill yuchen lin, seyeon lee, rahul khanna, and xi-ang ren.
2020. birds have four legs?!
numersense:probing numerical commonsense knowledge ofin proceedings ofpre-trained language models..1488the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6862–6868, online.
association for computational lin-guistics..libo qin, wanxiang che, yangming li, haoyang wen,and t. liu.
2019. a stack-propagation frameworkwith token-level intent detection for spoken lan-guage understanding.
in emnlp/ijcnlp..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..libo qin, zhouyang li, wanxiang che, minheng ni,and ting liu.
2020. co-gat: a co-interactive graphattention network for joint dialog act recognition andsentiment classiﬁcation.
arxiv, abs/2012.13260..yang liu and mirella lapata.
2019. text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3730–3740, hong kong,china.
association for computational linguistics..zhengyuan liu, a. ng, sheldon lee shao guang, aitiaw, and nancy f. chen.
2019. topic-aware pointer-generator networks for summarizing spoken conver-sations.
2019 ieee automatic speech recognitionand understanding workshop (asru), pages 814–821..rada mihalcea and paul tarau.
2004..textrank:bringing order into text.
in proceedings of the 2004conference on empirical methods in natural lan-guage processing, pages 404–411, barcelona, spain.
association for computational linguistics..gabriel murray, s. renals, and j. carletta.
2005. ex-tractive summarization of meeting recordings.
ininterspeech..ramesh nallapati, feifei zhai, and bowen zhou.
2017.summarunner: a recurrent neural network based se-quence model for extractive summarization of docu-ments.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 31..ramesh nallapati, bowen zhou, c. d. santos, c¸ aglarg¨ulc¸ehre, and bing xiang.
2016. abstractive textsummarization using sequence-to-sequence rnns andbeyond.
in conll..shashi narayan, shay b. cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, brussels, belgium..maxime peyrard.
2019. a simple theoretical model ofin proceedings ofimportance for summarization.
the 57th annual meeting of the association for com-putational linguistics, pages 1059–1073, florence,italy.
association for computational linguistics..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational linguis-tics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..k. riedhammer, b. favre, and dilek z. hakkani-t¨ur.
2008. a keyphrase based approach to interactivemeeting summarization.
2008 ieee spoken lan-guage technology workshop, pages 153–156..oscar.
sainzask2transformers:with pre-trained language models..german.
2021.zero-shot domain labelling.
rigau..and.
abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..guokan shang, wensi ding, zekun zhang, an-toine tixier, polykarpos meladianos, michalis vazir-giannis, and jean-pierre lorr´e.
2018. unsuper-vised abstractive meeting summarization with multi-sentence compression and budgeted submodularthe 56th an-maximization.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 664–674, melbourne, australia.
association for compu-tational linguistics..in proceedings of.
ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems 28: annual con-ference on neural information processing systems2015, december 7-12, 2015, montreal, quebec,canada, pages 2692–2700..c. wang, xiao liu, and d. song.
2020..lan-guage models are open knowledge graphs.
arxiv,abs/2010.11967..liqiang xiao, lu wang, hao he, and yaohui jin.
2020. modeling content importance for summariza-tion with pre-trained language models.
in proceed-ings of the 2020 conference on empirical methods.
1489in natural language processing (emnlp), pages3606–3611, online.
association for computationallinguistics..klaus zechner.
2002. automatic summarization ofopen-domain multiparty dialogues in diverse genres.
computational linguistics, 28(4):447–485..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020a.
bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020b.
dialogpt : large-scale generative pre-training for conversational re-in proceedings of the 58th an-sponse generation.
nual meeting of the association for computationallinguistics: system demonstrations, pages 270–278, online.
association for computational linguis-tics..lulu zhao, weiran xu, and jun guo.
2020. improvingabstractive dialogue summarization with graph struc-in proceedings of the 28thtures and topic words.
international conference on computational linguis-tics, pages 437–449, barcelona, spain (online).
in-ternational committee on computational linguis-tics..chenguang zhu, ruochen xu, michael zeng, and xue-dong huang.
2020. a hierarchical network for ab-stractive meeting summarization with cross-domainpretraining.
in findings of the association for com-putational linguistics: emnlp 2020, pages 194–203, online.
association for computational linguis-tics..a evaluation details.
for rouge (lin, 2004), we employ py-rouge11package to evaluate our models following gliwaet al.
(2019).
for bertscore (zhang et al.,implementation122020a), we use the ofﬁcialthe detailed com-to evaluate our models.
mand line for bertscore is bert-score -rgolden.txt -c gen.txt --lang en..b ablation studies for annotations.
to further verify the effectiveness of our method,we conduct ablation studies for each annotation.
the results are shown in table 10 and table 11.we can ﬁnd that:(1) for both datasets, train-ing summarizers based on datasets with two ofthree annotations can obtain improvements.
(2)for both datasets, training summarizers based on.
11https://pypi.org/project/py-rouge/12https://github.com/tiiiger/bert score.
model.
r-2.
r-l.r-1ours52.9853.4353.3953.3453.5653.5153.6453.70.bartbart(dke)bart(drd)bart(dts)bart(dke+rd)bart(dke+ts)bart(drd+ts)bart(dall).
27.6728.0328.0127.8528.6528.1328.3328.79.
49.0649.9349.4949.6450.5550.0050.1350.81.table 10: test set results on the samsum dataset.
bart means ﬁne-tuning bart on the original sam-sum.
bart(dke), bart(drd) and bart(dts) rep-resent ﬁne-tuning bart on the samsum with key-words, redundancy and topic annotation respectively.
bart(dke+rd) represent ﬁne-tuning bart on thesamsum with keywords and redundancy annotations.
dall means the samsum with all three annotations..model.
r-2.
r-l.r-1ours48.3450.2250.6248.5950.7450.6950.7050.91.
16.0217.7416.8616.0717.1116.8316.9617.75.
23.4924.1124.2724.0524.5224.3324.3824.59.pgnpgn(dke)pgn(drd)pgn(dts)pgn(dke+rd)pgn(dke+ts)pgn(drd+ts)pgn(dall).
table 11: test setresults on the ami dataset.
pgn(dke), pgn(drd) and pgn(dts) represent train-ing pgn on the ami with keywords, redundancy andtopic annotation respectively.
pgn(dke+rd) representtraining pgn on the ami with both keywords and re-dundancy annotations..datasets with two of three annotations can sur-pass corresponding summarizers that are trainedbased on datasets with one type of annotation (e.g.,bart(dke+rd) is better than bart(dke) andbart(drd)).
(3) compared with summarizersthat are trained on drd+ts and dke+rd, summa-rizers that are trained on dke+ts get relativelysmall improvements on both datasets.
neverthe-less, it indicates that dialogptke and dialogpttsstill have non-overlapping parts.
(4) combining allthree annotations, both summarizers can achievethe best results in all rouge scores..1490c hyper-parameter search results.
tables 12 to 17 show the hyper-parameter searchresults.
finally, for samsum (gliwa et al., 2019),we set keywords extraction ratio rke to 15, simi-larity threshold trd to 0.99 and topic segmentationratio rts to 15. for ami (carletta et al., 2005), rkeis 4, trd is 0.95 and rts is 5..modelbart(dke)bart(dke)bart(dke)bart(dke).
rke10152025.r-152.1753.4353.2052.78.r-226.6428.0328.0127.35.r-l48.3449.9349.4648.67.table 12: test set results on the samsum dataset.
bart(dke) means ﬁne-tuning bart on samsumwith keywords annotation.
rke means different key-words extraction ratios..modelpgn(dke)pgn(dke)pgn(dke)pgn(dke).
rke3456.r-149.7650.2249.6349.70.r-216.0317.7416.7116.92.r-l23.6424.1123.8824.42.table 13: test setresults on the ami dataset.
pgn(dke) means training pgn on ami with key-words annotation.
rke means different keywords ex-traction ratios..modelbart(drd)bart(drd)bart(drd)bart(drd)bart(drd).
trd0.950.960.970.980.99.r-152.2953.2052.1753.2953.39.r-226.7127.9827.1027.8928.01.r-l48.5349.6848.3449.7149.49.table 14: test set results on the samsum dataset.
bart(drd) means ﬁne-tuning bart on samsumwith redundant utterances annotation.
trd means dif-ferent similarity thresholds..modelpgn(drd)pgn(drd)pgn(drd)pgn(drd)pgn(drd).
trd0.950.960.970.980.99.r-150.6249.6850.1848.6347.15.r-216.8616.5416.1215.1713.94.r-l24.2724.7024.5623.5022.53.table 15: test setresults on the ami dataset.
pgn(drd) means training pgn on ami with redun-dant utterances annotation.
trd means different similar-ity thresholds..modelbart(dts)bart(dts)bart(dts)bart(dts).
rts10152025.r-153.2153.3452.8253.04.r-227.3827.8527.3427.49.r-l49.3249.6449.0549.70.table 16: test set results on the samsum dataset.
bart(dts) means ﬁne-tuning bart on samsumwith topic annotation.
rts means different topic seg-mentation ratios..modelpgn(dts)pgn(dts)pgn(dts)pgn(dts).
rts4567.r-149.3948.5949.8949.37.r-216.0216.0716.0416.07.r-l23.8924.0523.0123.46.results on the ami dataset.
table 17: test setpgn(dts) means training pgn on ami with topic an-notation.
rts means different topic segmentation ratios..1491