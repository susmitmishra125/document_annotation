enslm: ensemble language model for data diversityby semantic clustering.
zhibin duan*1, hao zhang*†1, chaojie wang1, zhengjue wang1,bo chen†1, mingyuan zhou21national laboratory of radar signal processing, xidian university, xi’an, china2mccombs school of business the university of texas at austin, austin, tx 78712, usa{xd zhibin, zhanghao xidian}@163.combchen@mail.xidian.edu.cn, mingyuan.zhou@mccombs.utexas.edu.
abstract.
natural language processing often faces theproblem of data diversity such as differentdomains, themes, styles and so on.
there-fore, a single language model (lm) is insufﬁ-cient to learn all knowledge from diverse sam-ples.
to solve this problem, we ﬁrstly pro-pose an autoencoding topic model with mix-ture prior (matm) to perform clustering forthe data, where the clusters deﬁned in seman-tic space describe the data diversity.
havingobtained the clustering assignment for eachsample, we develop the ensemble lm (en-slm) with the technique of weight modula-tion.
speciﬁcally, enslm contains a backbonewhich is adjusted by a few modulated weightsto ﬁt for different sample clusters.
as a re-sult, the backbone learns the shared knowledgeamong all clusters while modulated weightsextract the cluster-speciﬁc features.
enslmcan be trained jointly with matm with ﬂexi-ble lm backbone.
we evaluate the effective-ness of both matm and enslm on differentlanguage understanding and generative tasks..1.introduction.
it is common knowledge in modern natural lan-guage processing (nlp) that natural languagevaries greatly across domains, themes, styles, gen-res and many other linguistic nuances (van derwees et al., 2015; van der wees, 2017; niu et al.,2017).
generally, we call such nature of languageas data diversity.
many existing works (liu et al.,2017; cai and wan, 2019; hu et al., 2019) haveillustrated that data diversity will affect the perfor-mance of lms if we just train a single lm overthe entire dataset, even though ﬁne-tuning a pre-trained lm (that has been pre-training on a verylarge corpus) such as bert (devlin et al., 2019) oncurrent task (aharoni and goldberg, 2020)..* equal contribution..† corresponding author..(a) lda.
(b) matm.
figure 1: the distribution of samples on seman-tic space on 4 domains (different products) of ama-zon dataset.
the sample clustering characteristics ofmatm can reﬂect the data diversity (domain in this ex-ample) in the corpus..the domain diversity in dataset is a very com-mon type of data diversity.
in some cases, if we canobtain a well-deﬁned domain label for each sample,some works (jiang et al., 2020; du et al., 2020;wright and augenstein, 2020) try to consider themulti-domain property of data in developing thelms.
however, these pre-deﬁned domain labels arenot always accurate or even available (aharoni andgoldberg, 2020), especially for the wild datasets,in which data come from different sources, such asinternet news, product reviews, and daily conver-sation.
to this end, we hope to develop a lm thatcan explore the diversity from data automatically..data selection is a commonly used strategy tohandle diversity in data (moore and lewis, 2010;axelrod et al., 2011; duh et al., 2013; silva et al.,2018; aharoni and goldberg, 2020).
this kindof method is developed from an assumption thatsamples belonging to the same cluster should ownsimilar characteristics.
according to the cluster-ing assignment, models can select suitable data fortraining a lm for each cluster separately.
although,to some extend, data selection is an efﬁcient strat-egy to alleviate the problem of data diversity, itmay bring two disadvantages as follows.
firstly,the process of data selection is independent of thelm learning.
in other words, the gradient signalgenerated by lm’s training loss can not affect the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2954–2967august1–6,2021.©2021associationforcomputationallinguistics2954data selection.
secondly, data selection only tellsthe hard cluster belongings of samples, ignoring afact that some samples may belong to more thanone clusters with soft (weighted) assignment..inspired by their works and to move beyond, inthis paper, we ﬁnd the semantics learned by topicmodeling (blei et al., 2003; srivastava and sut-ton, 2017) can infer sample clusters to a certainextent via k-means, but is not good enough, asshown in fig.
1a .
to jointly consider the clus-tering and topic modeling for better clustering (asshown in fig.
1b) and for joint training with thefollowing lm, we ﬁrstly introduce an autoencod-ing topic model with mixture priors (matm).
foreach sample in the corpus, matm can infer a softclustering assignment.
in order to jointly considerthe learning of matm with various lms, we em-ploy the weight modulation methods (cong et al.,2020; wen et al., 2020).
speciﬁcally, as shown infig.
3, given a lm as backbone, for each layer(convolutional or fully-connected), we introducesome modulated parameters.
guided by clusteringassignment inferred from matm, these parametersmodulate the backbone single lm to multiple lms,corresponding to different clusters.
therefore, ourproposed model can be seen as a type of ensemblelearning, and hence we call it ensemble languagemodel (enslm)..our proposed matm and enslm enjoy the fol-.
lowing distinguished properties:.
• the matm learns the mixture-prior latent se-mantic space to deﬁne a soft clustering assign-ment for each sample..• guided by clustering assignments that de-scribe the data diversity, enslm learns bothshared and cluster-speciﬁc knowledge byweight modulations..• joint training of matm and enslm improvesthe performance of both on many nlp tasks..2 related work.
for nlp, topic modeling (tm) (blei et al., 2003;zhou et al., 2012) and lms are two commonregimes with their own advantages.
tm can dis-cover the interpretable global semantics that aretopics, while with pre-training on large corpus,lms recently achieve the sota performance onmany nlp tasks with more focuses on local de-pendencies.
therefore, some works consider to.
combine them to obtain beneﬁts from both.
dienget al.
(2016) and wang et al.
(2020) incorporate thetm with rnn-based model to capture the long-range dependencies.
to move beyond single-layertm for rnns, guo et al.
(2020) propose the re-current hierarchical topic-guided rnn with thehelp of multi-layer tm (zhou et al., 2015; zhanget al., 2018).
to extract explicit document seman-tics for summarization, wang et al.
(2020) proposethree different modules to plug knowledge fromtm into transformer-based lms (vaswani et al.,2017; devlin et al., 2018).
our work can be seenas a parallel work to combine their advantages to-gether but focuses on dealing with data diversityin nlp without the ground-truth information suchas domain labels.
meanwhile, our work can beapplied for different lms including cnns, rnns,and transformer-based models..3 autoencoding topic model with.
mixture prior.
we ﬁrstly describe one of the most popular topicmodels, latent dirichlet allocation (lda) (bleiet al., 2003), and its autoencoding inference (sri-vastava and sutton, 2017).
inspired by them, inorder to jointly consider topic learning and sam-ple clustering, we propose the autoencoding topicmodel with mixture prior (matm)..3.1 lda with autoencoding inference.
for a document containing d words as w ={wd}dd=1, given k topics φ = [φ1, · · · , φk]where φk is a probability distribution over the vo-cabulary, lda deﬁnes the generative process of win algorithm 1, where θ ∈ rk+ is the topic propor-tion with α as the prior parameter.
after collapsing.
algorithm 1 generative process of lda.
for each document w do.
draw topic proportion θ ∼ dirichlet(α)for each word at position d do.
sample a topic id ∼ multinomial(1, θ)sample a word wd ∼ multinomial(1, φid.
).
id, given θ and φ, we can represent the conditionallikelihood of wd as.
wd|φ, θ ∼ multinomial(1, φθ)..(1).
given φ, a popular approximation for efﬁcientinference of lda is mean-ﬁeld variational infer-ence, which tries to maximize the evidence lower.
2955bound (elbo) of marginal data log likelihood as.
elbo = eq(θ)[log p(w|θ, φ)]−kl[q(θ)||p(θ)],(2)where q(θ) is the variational posterior.
in particu-lar, srivastava and sutton (2017) propose the au-toencoding variational inference (aevb) (kingmaand welling, 2013) for lda by using laplace ap-proximation (hennig et al., 2012) for the dirichletprior, and building logistic-normal (ln) encodingposterior..as shown in fig.
1, we ﬁnd that running clus-tering method such as k-means on semantic spaceθ can not achieve satisfactory results.
for jointlyconsidering the learning of topics and sample clus-tering, we propose the matm..3.2 generative process of matm.
suppose the number of clusters is c, and the clus-tering prior parameter is π = [π1, · · · , πc] with(cid:80)cc=1 πc = 1, shown in fig.
2a, matm deﬁnesthe generative process of w in algorithm 2. com-.
approximation for dirichlet distribution, we pro-pose the mixture ln (mln) distribution as theapproximation of mixture dirichlet distribution..speciﬁcally, srivastava and sutton (2017) haveproved that a dirichlet distribution p(θ|α) can bewell approximated by ln distribution as.
p(θ|µ, σ) = ln (µ, σ),.
(3).
where the elements in mean vector µ and diagonalcovariance matrix σ are.
k(cid:88).
i=1(cid:19).
1k.2k.µk = log αk −.
log αi.
σk =.
(cid:18).
1 −.
1αk.
+.
1k2.
k(cid:88).
i=1.
1αi.
..(4).
to go further, for inference of matm, we constructthe mln distribution as.
p(θ|µ, σ) =.
πcln (µc, σc).
c(cid:88).
c=1.
algorithm 2 generative process of matm.
k = log αcµc.
k −.
log αci.for each document w do.
draw cluster index z ∼ categorical(π)draw topic proportion θ ∼ dirichlet(αz)for each word at position d do.
sample a topic id ∼ multinomial(1, θ)sample a word wd ∼ multinomial(1, φid.
).
pared with lda, matm has a mixture dirichletprior with parameters {αc}cc=1.
in other words,matm assumes that the θ of different documentsmay come from different clusters, which is thebasic thought to discover the data diversity fromcorpus automatically..3.3 variational encoder of matm.
in order to infer the parameters in matm andfurther develop the enslm by matm, we intro-duce aevb for matm, whose detailed structureis shown in fig.
2b..3.3.1 laplace approximation for mixture.
dirichlet prior.
although dirichlet prior of θ is important to learninterpretable topics (wallach et al., 2009), it is dif-ﬁcult to handle it within aevb since aevb needseffective reparameterization (rt) function for dis-tributions.
inspired by the success of the laplace.
k(cid:88).
i=1(cid:19).
1k.2k.σc.
k =.
(cid:18).
1 −.
1αck.+.
1k2.
k(cid:88).
i=1.
1αci.,.
(5).
which is used to approximate the mixture dirichletprior p(θ|{αc, πc}cc=1) in matm.
therefore, foreach document, the prior of θ can be written as(cid:81)cc=1 ln (µc, σc)zc.
in practice, we build the µcand σc as.
µc = fwc.
µ(z), σc = fwc.
σ (z),.
(6).
where z = [z1, · · · , zc].
next, we build variationalposterior for latent variables with easy rt function..3.3.2 variational encoding posteriorafter collapsing {id}dd=1 in matm as (1) in lda,given topics φ, for document w, there are twolatent variables that need to be inferred: θ and z..µ.ln posterior for θ. we build the variational pos-terior of θ as ln distribution q(θ) = ln (µ(cid:48), σ(cid:48))(x), σ(cid:48) = diag(fwθwith µ(cid:48) = fwθ(x)), wherediag converts a vector to a diagonal matrix, fwθ(·)and fwθ(·) are two encoding networks, and x is atype of representation for document w such as orig-inal words or bag of words (bow) vector.
morevoer,ln distribution has easy rt function as normaldistribution..µ.σ.σ.
2956(a) generation in matm.
(b) inference in matm.
4 ensemble language model.
lda, we parameterize it as φ = sof tmax(wt),where wt = [w1, · · · , wk] and sof tmax is op-erated for each topic {wk}kk=1 to ensure themon a probability simplex.
therefore, as shown2, all the parameters of matm arein fig.
θ1 = {wθµ, wθµ, wcσ, wπ, wt} that canbe learned by maximizing the elbo in (9)..σ, wc.
recently, various advanced lms for language un-derstanding and generation have been introduced,most of which do not consider the data diversitiesin the corpus.
in this paper, having obtained theclustering assignment vector z from matm, givena single lm as backbone, we propose the ensemblelm (enslm) via z-guided weight modulation.
inother words, the enslm can modulate the back-bone single lm to ﬁt for different clusters..4.1 efﬁcient weight modulation.
although lms have many different types, basically,all of them build on convolutional (such as in cnn(johnson and zhang, 2015)) or fully-connected(such as in transformer (vaswani et al., 2017))operations (ignoring the bias) as.
convolution : h2 = f (w ∗ h1)1)..2 = f (w(cid:48)t h(cid:48).
fully-connection : h(cid:48).
(10).
1 ∈ rcin arewhere, h1 ∈ rix×iy×cin and h(cid:48)the input features, w ∈ rkx×ky×cin×cout andw(cid:48) ∈ rcin×cout are the convolutional kernel orfull-connected weights1.
suppose the number ofclusters (domains) in matm is c, given a lm asbackbone, we introduce a few modulation parame-ters to modulate the original parameters w or w(cid:48)for different clusters..speciﬁcally, shown in fig.
3, for a convolutionalor fully-connected layer in (10), suppose that thereare two dictionaries of modulation parameters as:.
a = [α1, · · · , αc] ∈ rcin×cb = [β1, · · · , βc] ∈ rcout×c,.
(11).
c=1 ∈ rcin and {βc}c.c=1 ∈ rcout.
forwhere {αc}ca document w whose feature at current layer is h1,after archiving its domain assignment z ∈ rc×1.
figure 2: graphical model for the matm, where thecircle with white color, the circle with gray color andthe rectangle denotes local latent variables, observa-tions, and global parameters in matm..gumbel softmax (gs) posterior for z. as cate-gorical variable, z is difﬁcult to build variationalposterior under aevb with accurate rt function.
instead, we employ gs distribution (jang et al.,2016) as the variational posterior of z for efﬁcientgradient propagation..speciﬁcally, suppose the posterior of z iscategorical(π(cid:48)), after obtaining c i.i.d samples{g1, · · · , gc} drawn from gumbel(0, 1), then zcan be sampled as.
z = arg max.
c.(cid:80)o.exp ((log(π(cid:48)o=1 exp ((log(π(cid:48).
c) + gc)/τ ).
o) + go)/τ ).
(7).
where τ is the temperature parameter.
in orderto build encoder for π(cid:48), we let π(cid:48) = fwπ (θ, w).
for efﬁcient gradient propagation, rather than sam-pling z from arg max as (7), we obtain the vari-ational posterior of soft assignment vector z =[z1, · · · , zc] as q(z):.
[q(z)]c =.
exp ((log(π(cid:48)o=1 exp ((log(π(cid:48).
(cid:80)o.c) + gc)/τ ).
o) + go)/τ ).
..(8).
besides the beneﬁt of efﬁcient gradient back-propagation, the soft assignment in (8) providesclustering belonging weights.
in the following en-slm, this property is useful for some ambiguoussamples that may belong to different clusters..3.3.3 elbo of matmwe obtain the elbo of matm as.
elbo = eq(θ)q(z)[log p(w|θ, φ, z)].
− kl[q(θ)||p(θ|z)] − kl[q(z)|p(z|π)](9).
similarly with srivastava and sutton (2017), in-stead of sampling φ from dirichlet posterior in.
1fully-connected layer can be also seen as a convo-is w(cid:48) ∈.
lution layer where the convolutional kernelr1×1×cin×cout (ix = iy = 1).
29574.2.joint training of matm and enslm.
different from some strategies such as data selec-tion that separate the calculation of assignment andthe training of lm, our proposed matm and en-slm can be jointly trained in one framework..speciﬁcally, given a training set containing nsample {wn}nn=1, suppose that there is a label{yn}nn=1 for each sample.
it should be noted thatlabels {yn}nn=1 can be different for different tasks,such as labels for document classiﬁcation, goldensummarization for abstractive summarization, ordocument itself for generation.
as a result, theloss for joint training of matm and enslm can bewritten as.
n(cid:88).
n=1.
l =.
eq(θn)q(zn)[log p(wn|θn, φ, zn)].
− eq(zn)[llm (wn, yn, zn)]− kl[q(θn)||p(θn)] − kl[q(zn)|p(zn)],.
(13).
µ, wθ.
loss of generality, llm de-where, withoutnotes the loss for lm.
all learnable parame-ters are i) parameters of matm: θmat m ={wθσ , wπ} and ii) parameters oflm: θlm .
these parameters can be jointly trainedby stochastic gradient descend with low-variancegradient estimation since ln and gs distributionshave easy rt function..µ , wu.
σ, wu.
5 experiments.
in this section, we evaluate the effectiveness and ef-ﬁciency of our proposed matm and enslm on dif-ferent nlp tasks including document clusters, textclassiﬁcation, language generation and abstractivedocument summarization.
our code is available athttps://github.com/bochengroup/enslm.
5.1 document clusters.
the basic idea of matm and enslm is that matmcan automatically discover the sample clusterswhich describe the data diversity.
therefore, weﬁrstly evaluate the document clustering perfor-mance of matm..datasets following yao et al.
(2019), we con-sider two widely used document clustering datasets,20news and r8 .
this two datasets2 can befound in the open source code of yao et al.
(2019)..2https://github.com/yao8839836/text gcn.
figure 3: illustration of weight modulation in enslm..from (8), we feed h1 into the modulated layer as.
fully-connection : h(cid:48).
convolution : h2 = f ((w (cid:12) γ) ∗ h1)1),(12).
2 = f ((w(cid:48)t (cid:12) γ)h(cid:48).
where γ = αβt , α = az ∈ rcin×1, β = bz ∈rcout×1, and (cid:12) denotes matrix element-wise prod-uct (with broadcasting for convolution)..intuitively, w and w(cid:48) actexplanation of (12).
as the backbone parameters in the original singlelm, and γ is the modulated parameters, whichmoves the backbone to ﬁt different domains.
if zis drawn from (7) that means z is a one-hot vec-tor, then it denotes that α and β are chosen fromthe dictionaries a and b, correspondingly.
if z isdrawn from (8) that means z is a soft assignmentvector, then it denotes that α and β are weightedsummation of all elements in a and b, correspond-ingly.
in practice, we use the soft assignment vectorsince i) it brings efﬁcient gradient propagation dur-ing joint training of matm and enslm, and ii)it considers the fact that there are some domainambiguous samples in the dataset..it is interesting to note that although enslm isdeveloped for the problem that ground-truth priorsof data diversity (such as domain label) is unavail-able, it can be also used when we know the priors.
for this scenario, rather than inferring the cluster-ing assignment z from matm via (8), we directlyset z as the real one-hot assignment vector, whichis illustrated in experiment in sec.
5.2..2958baseparametersmodulatedparameters20news has 20 classes and consists of 18,846 docu-ments with a vocabulary size of 61,188, partitionedinto a training set of 11,314 documents and a testset of 7,532 ones.
r8 is a subset of the reuters21578 dataset, which has 8 classes and was splitinto 5,485 training and 2,189 test documents.
forthese two datasets, we remove the stop words anduse the 2,000 most frequent terms as the vocabu-lary.
for all methods, we set the number of clustersas the number of classes..comparison models and implementation de-tails to verify the effectiveness of matmthree types of document clus-for clustering,i) raw+kmeanstering models are compared.
performs k-means on raw bow vectors, andpca+kmeans uses pca extract low-dimensionalfeatures and then uses k-means for clustering;ii) train a topic model and then perform k-means for clustering on topic proportions, wherewe consider lda+kmeans (blei et al., 2003),avitm+kmeans (srivastava and sutton, 2017),and pfa+kmeans (zhou et al., 2012); iii) deepin-neural network based clustering methods,cluding deep clustering (xie et al., 2016), anddcn (yang et al., 2017), which jointly considerthe feature extracting and clustering.
besidesraw+kmeans performing clustering on original in-puts, others are on a latent feature space (for topicmodeling, feature is the topic proportion).
fol-lowing (xie et al., 2016; yang et al., 2017), thedimension of feature space equals to the number ofclusters..table 1: results of ac and nmi for document cluster-ing task..model.
20news.
r8.
base+kmeanspca+kmeans.
lda+kmeanspfa+kmeansavitm+kmeans.
deepclusterdcn.
ac.
30.233.1.
37.438.440.2.
42.244.8.nmi.
37.039.1.
38.139.241.2.
43.548.4.ac.
40.144.1.
53.854.756.3.nmi.
30.232.1.
36.937.638.3.
58.2359.34.
41.0243.2.matm.
46.44.
49.86.
62.15.
48.12.results following yang et al.
(2017), since weknow the ground-truth label and set the clusteringnumber as the number of classes, we measure the.
clustering performance by accuracy (ac) and nor-malized mutual information (nmi), both of whichare the higher the better.
the results are shownin table 1. compared with the base+kmeans,pca+kmeans performs better since it extracts ef-fective principal components.
beneﬁting from thelearning of semantics for documents, the secondgroup including three types of topic modeling out-performs pca.
compared with the ﬁrst two groups,the third group jointly considers the feature learn-ing and clustering, thus achieving higher ac andnmi.
combined the advantages of topic modelingin extracting efﬁcient features from documents andjoint learning of feature extractor and clustering,matm gets the sota performance for documentclustering tasks on these two datasets..the clustering results support our motivationof using matm to discover the data diversity.
inthe following experiments, we evaluate the perfor-mance of both matm and enslm on differentlanguage understanding and generation tasks..5.2 multi-domain sentiment classiﬁcation.
sentiment classiﬁcation (positive or negative) fordifferent products is a fundamental language un-derstanding task in nlp.
for this task, the data di-versity mainly arises from different domains (prod-ucts) (blitzer et al., 2007), which brings the prob-lem that data from different domains may havedifferent distributions..datasets to evaluate the performance of matmand enslm in capturing the multi-domain propertyfor sentiment classiﬁcation, following cai and wan(2019), we perform experiments on the dataset re-leased by liu et al.
(2017), which consists of prod-uct and movie reviews in 16 different domains.
thedata in each domain is randomly split into trainingset, development set and test set according to theproportion of 70%, 10%, 20%, whose statistics ofthe 16 datasets are listed in appendix a.1..comparison models and implementation de-tails following (cai and wan, 2019), we ﬁrstlyconsider three base models, bilstm (adhikariet al., 2019), textcnn (kim, 2014) and bert(devlin et al., 2019), which perform classiﬁca-tion on every domains separately.
secondly, com-bining data from different domains together, wetrain the above three models named as bilstm-mix, textcnn-mix and docbert-mix.
hav-ing obtained the ground-truth domain label, theprevious works regard the multi-domain problem.
2959as the multi-task learning (mtl) including da-mtl (zheng et al., 2018), asp-mtl (liu et al.,2017),and mdae (cai and wan, 2019).
all theseworks are developed from bilstm model.
forour proposed enslm, we use textcnn, bilstmand docbert as the backbone of enslm.
weperform experiments on two types of enslm: i)with ground-truth (gt) domain label, we directlyset z as the one-hot assignment vector (do not in-fer z from matm), which is named as bilstm-enslm-gt, textcnn-enslm-gt, and bert-enslm-gt; ii) without gt domain label, we usematm to infer z , which is named as bilstm-enslm-matm, textcnn-enslm-matm, andbert-enslm-matm.
for model using matm,we set the number of topics as 16. more detailedsettings and implementation details can be foundin appendix b.1..table 2: accuracy of sentiment classiﬁcation..models.
textcnnbilstmbert.
textcnn-enslm-gtbilstm-enslm w-gtbert-enslm w-gt.
textcnn-enslm-matm 88.8bilstm-enslm-matm 90.293.5.bert-enslm-matm.
acc.
models.
acc.
84.383.788.1.
88.289.492.9.textcnn-mixbilstm-mixbert-mix.
da-mtlasp-mtlmdae.
85.386.691.3.
88.287.290.1.
-.
-.
results the results of averaged accuracy on alldomains are given in table 2, where the resultsexcept ours are obtained from cai and wan (2019).
comparing results on the ﬁrst row, we can see thatjoint training models on all domains outperformseparate training on each domain.
compared withbilstm-mix, having obtained the gt domain la-bel, da-mtl, asp-mtl and mdae (all of themare developed based on bilstm) consider the realdomain knowledge in word embedding, featureextractor and attention layers, achieving higher ac-curacy.
similarly, with gt domain label, threemodels equipped with our proposed enslm per-forms better than their basic counterparts with alarge margin.
assuming that gt domain labelsare unavailable, we use matm to infer the clus-tering assignment to guide the learning of enslm,which obtains the sota performance on all threebasic models, even better than the models using gtdomain label.
we attribute it to the fact that com-.
table 3: comparison of perplexity on four datasets..methods.
apnews imdb bnc coco.
lstmtransformer-xl.
tgvaergbn-rnn.
gpt-2.
65.1660.11.
57.1151.36.
95.7397.14.
87.8679.13.
21.3419.32.
--.
44.71.
46.04.
13.58.
60.1358.73.
48.7342.71.
35.78.
23.67.gpt-2-enslm-matm.
35.48.
40.79.
12.45.pared with the hard gt domain label, matm infersthe soft clustering assignment, which not only re-ﬂect the domain characteristic of samples but alsodescribe the samples having confused domain char-acteristics.
for example samples from dvd maybe similar with the ones from electronics..5.3 language generation.
datasetsin order to verify the effectiveness ofour model on datasets of different lengths, we con-sider four publicly available corpora: apnews,imdb, bnc, and coco.
following lau et al.
(2017), we tokenize words and sentences usingstanford corenlp (klein and manning, 2003),lowercase all word tokens, and ﬁlter out word to-kens that occur less than 10 times.
for the topicmodel, we additionally exclude stopwords.
allthese corpora are partitioned into training, valida-tion, and testing sets, whose summary statistics areprovided in appendix a.2..comparison models and implementation de-tails we consider the following baseline mod-lstm, a standard lstm languageels:model(hochreiter and schmidhuber, 1997);tansnsformer-xl enables learning dependencybeyond a ﬁxed length by introducing a recurrencemechanism and a novel position encoding schemeinto the transformer architecture (dai et al., 2019);tgvae (wang et al., 2019), combines a varia-tional auto-encoder based natural sequence modelwith a neural topic model; rgbn-rnn (guo et al.,2020), extracts recurrent hierarchical semanticstructure via a dynamic deep topic model to guidenatural language generation; gpt-2 (radford et al.,2019) is a generative pre-training of a transformer-based lm on a diverse set of unlabeled text.
forour proposed model, gpt-2-enslm-matm ﬁrstuses matm to infer semantic clusters for each sam-ple, and then introduce this diversity information topre-trained gpt2 by efﬁcient weight modulationnaturally.
in the experiments, we use the adam op-timizer (kingma and ba, 2014) with learning rate10−6.
the length of an input sample is limited to.
2960figure 4: example topics and their segment clusters inferred by a matm from the coco corpus, and the generatedsentences under segment cluster guidance.
for each cluster, top topics are shown in the column 2 respectively,original sentence are shown in the column 3 , and generated sentences are shown in the column 4..1024. we set the mini-batch size as 8, the numberof training epochs as 5. the clustering number ofmatm is set to 64 for the ﬁrst three datasets, while80 for coco dataset.
more detailed settings andimplementation details can be found in appendixb.2.
results for fair comparison, we use standardlanguage model perplexity as the evaluation met-ric.
the results of all models on four datasetsare given in table 3, where the results of exist-ing models are obtained from guo et al.
(2020).
in the ﬁrst group, transformer-xl gets better re-sult, which shows that the transformer-based modelhave better modeling capabilities.
in terms of cap-turing the document global semantic information,the second group can improve performance sig-niﬁcantly, which indicates that the topic model iseffective in capturing document global information.
pre-training on massive data, the gpt-2 can ob-tains better results compared with above models.
although gpt-2 gets a good result, the gpt-2-enslm-matm can improve performance signif-icantly by capturing data diversity.
it illustratesthat even pre-training on large scale of corpus, en-slm can further improve the performance of pre-trained lm via exploring data diversity.
a similarphenomenon also appeared in the experiments con-ducted by gururangan et al.
(2020).
sentence generation of enslm given thelearned gpt-2-enslm-matm, we can sample thesentences conditioned on semantic clusters.
shownin the in fig.
5, we select the top-3 topics to rep-resent this cluster, and select original sentencesaccording to the clustering results.
we can see thatmost of the generated sentences conditioned on asemantic clusters are highly related to the giventopics in terms of their semantic meanings but notnecessarily in key words, indicating the lm is suc-cessfully guided by the cluster assignment.
these.
table 4: rouge scores on cnn/dm and xsum testset, where the results are cited from liu and lapata(2019) and wang et al.
(2020).
.
cnn/dm.
model.
ptgenptgen+covtransformer.
bertsumbertsum+ta.
r1.
r2.
rl.
r1.
36.4439.5340.21.
42.1343.06.
15.6617.2817.76.
19.6020.58.
33.4236.3837.09.
39.1839.67.
29.7028.1029.41.
38.8139.77.xsum.
r2.
9.218.029.77.
16.5017.39.rl.
23.2421.7223.01.
31.2732.39.bertsum+enslm 43.34.
20.78.
39.83.
40.01.
17.62.
32.57.observations suggest that gpt-2-enslm-matmhas successfully captured syntax and global seman-tics simultaneously for natural language generation.
similar to fig.
5, we also provide other semanticclusters generated sentences in appendix c..5.4 abstractive summarization.
datasets we evaluate the effectiveness and ef-ﬁciency of proposed model on two benchmarkdatasets, including the cnn/dailymail (cnn/dm)(hermann et al., 2015) and the xsum (narayanet al., 2018).
the summary styles of these datasetsvaries from highlights, composed of several sen-tences, to very brief one sentence.
see moredetailed descriptions in appendix a.3.
we per-form data pre-processing following liu and lapata(2019)..comparison models and implementation de-tails we consider some baseline models,in-cluding lstm based models ptgen and pt-gen+cov (see et al., 2017); transformer basedmodels tansformer, bertsum (liu and lapata,2019); and bertsum+ta which combine pre-trained model with topic model (wang et al., 2020).
we combine enslm with bertsum on the abstrac-tive summarization task.
the clustering number ofmatm is set to 64 for all datasets.
given bertsum.
2961cluster#representivetopicsoriginal sentencesgenerated sentences1['kite', 'flying', 'sky’, 'air’, 'holding’]['man', 'child', 'people', 'person', 'young’]['beach’, 'water', 'outside', 'near’, 'park']a child flyinga pink kite on the beach.person flyinga kite high over a sea inlet.the bird is on a branch on the tree.a man in a yellow and white outfit flyinga kite.a young child flyinga kite with a frisbee in the air.
a person flyinga kite near the water in a body of water.
2['cake', 'slice', 'piece', 'chocolate', 'cream’]['table', 'plate', 'fork', 'cup', 'eaten’]['white’, 'large', 'small’, 'blue', ‘red']a womenreceives a cake that is blue.a piece of a chocolate cakeon a plate.
a small bird perched on a thin branch.two cakeswith frosting on top sit on a red plate.a sandwich on a platter with a pickleand some fruit.a cakethat has various decorations on it.5['baseball', 'bat', 'player', 'ball', 'game’]['man', 'holding', 'batter', 'swinging', 'field’] ['pitch', 'boy', 'plate', 'catcher', 'swing’]a baseballplayer stands with a baseball bat.a baseballplayer is holding a baseball bat.a baseballplayer is swinging a baseball bat.a man on a baseballfield swinging a bat.a baseball player swinging a bat on a field.a batter is getting ready to hit the ball.
checkpoints3 on cnn/dm and xsum provided byliu and lapata (2019), we further ﬁne-tune bert-sum+enslm.
besides, we adopt the settings inthe bertsum.
following liu and lapata (2019), inthe test stage, we use beam search with size 5, se-lect the top-3 checkpoints based on their evaluationloss on the validation set, and report the averagedresults on the test set.
more detailed settings andimplementation details can be found in appendixb.3..results rouge scores on cnn/dm, xsumhave been exhibited in tables 4, respectively.
fo-cusing on the models without pre-training in theﬁrst group, transformer achieves better perfor-mance compared with lstm-based model, at-tributing to stronger sequence modeling capabil-ities.
further, the outperformance of bertsumillustrates the fact that the combination of a pre-trained bert encoder and a transformer decoder isa better choice of sequence-to-sequence structure.
despite owning the same structure as the bertsum,the bertsum+ta employs a topic model to cap-ture global document segment diversity, and achiev-ing higher scores.
different from bertsum+tathat introduces document semantic diversity byadding topic information, bertsum+matm com-bines bertsum with enslm model, result in abetter performance.
compared with bertsum+ta,the performance improvement of our model is notenough promising is because they have been incor-porated the topical information into the bertsummodel which considering the segment diversity andcontextual information.
note that the performanceof our model improves signiﬁcantly compared withbertsum, which can prove the effectiveness of ourmodel..6 conclusion.
in this paper, we ﬁrst propose matm to infer latentsemantic clusters from raw text corpus, and thencombine it with lm with efﬁcient weight modula-tion, resulting in a more powerful enslm, whichcan be naturally extended to other lms.
in the fu-ture, we will study the effectiveness of enslm onother nlp tasks, such as the multi domain transla-tion, and investigate whether enslm can be appliedto the pre-training stage of transformer..3https://github.com/nlpyang/presumm.
acknowledgments.
bo chen acknowledges the support of nsfc(61771361), shaanxi youth innovation teamproject, the 111 project (no.
b18039) and theprogram for oversea talent by chinese centralgovernment.
we acknowledge all the anonymousreviewers for their valuable comments and sugges-tions..references.
ashutosh adhikari, achyudh ram, raphael tang, andjimmy lin.
2019. rethinking complex neural net-work architectures for document classiﬁcation.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 4046–4051..roee aharoni and yoav goldberg.
2020. unsuperviseddomain clusters in pretrained language models.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7747–7763, online.
association for computational lin-guistics..amittai axelrod, xiaodong he, and jianfeng gao.
2011. domain adaptation via pseudo in-domain dataselection.
in proceedings of the 2011 conference onempirical methods in natural language processing,pages 355–362..david m blei, andrew y ng, and michael i jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(jan):993–1022..john blitzer, mark dredze, and fernando pereira.
2007.biographies, bollywood, boom-boxes and blenders:indomain adaptation for sentiment classiﬁcation.
proceedings of the 45th annual meeting of the asso-ciation of computational linguistics, pages 440–447..version 3 (bnc xml edition) british national corpus.
2007. distributed by oxford university computingservices on behalf of the bnc consortium..yitao cai and xiaojun wan.
2019. multi-domain sen-timent classiﬁcation based on domain-aware embed-ding and attention.
in ijcai, pages 4904–4910..yulai cong, miaoyun zhao, jianqiao li, sijia wang,and lawrence carin.
2020. gan memory with noforgetting.
arxiv preprint arxiv:2006.07543..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..2962jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..adji b dieng, chong wang, jianfeng gao, and johnpaisley.
2016. topicrnn: a recurrent neural net-work with long-range semantic dependency.
arxivpreprint arxiv:1611.01702..chunning du, haifeng sun, jingyu wang, qi qi, andjianxin liao.
2020. adversarial and domain-awarein pro-bert for cross-domain sentiment analysis.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4019–4028..kevin duh, graham neubig, katsuhito sudoh, and ha-jime tsukada.
2013. adaptation data selection us-ing neural language models: experiments in ma-chine translation.
in proceedings of the 51st annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 678–683..dandan guo, bo chen, ruiying lu, and mingyuanzhou.
2020. recurrent hierarchical topic-guidedrnn for language generation.
in international con-ference on machine learning, pages 3810–3821.
pmlr..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages8342–8360, online.
association for computationallinguistics..philipp hennig, david stern, ralf herbrich, and thorein artiﬁcial.
graepel.
2012. kernel topic models.
intelligence and statistics, pages 511–519..karl moritz hermann, tom´aˇs koˇcisk´y, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readand comprehend.
in proceedings of the 28th inter-national conference on neural information process-ing systems - volume 1, nips’15, page 1693–1701,cambridge, ma, usa.
mit press..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..junjie hu, mengzhou xia, graham neubig, andjaime g carbonell.
2019. domain adaptation ofneural machine translation by lexicon induction.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2989–3001..eric jang, shixiang gu, and ben poole.
2016. categor-ical reparameterization with gumbel-softmax.
arxivpreprint arxiv:1611.01144..haoming jiang, chen liang, chong wang, and tuozhao.
2020. multi-domain neural machine trans-lation with word-level adaptive layer-wise domainin proceedings of the 58th annual meet-mixing.
ing of the association for computational linguistics,pages 1823–1834, online.
association for computa-tional linguistics..rie johnson and tong zhang.
2015. effective use ofword order for text categorization with convolutionalneural networks.
in proceedings of the 2015 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, pages 103–112, denver, col-orado.
association for computational linguistics..yoon kim.
2014..works for sentence classiﬁcation.
arxiv:1408.5882..convolutional neural net-arxiv preprint.
diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..diederik p kingma and max welling.
2013. auto-arxiv preprint.
encoding variational bayes.
arxiv:1312.6114..dan klein and christopher d manning.
2003. accu-in proceedings of therate unlexicalized parsing.
41st annual meeting of the association for compu-tational linguistics, pages 423–430..jey han lau, timothy baldwin, and trevor cohn.
2017. topically driven neural language model.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 355–365, vancouver, canada.
association for computational linguistics..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..pengfei liu, xipeng qiu, and xuan-jing huang.
2017.adversarial multi-task learning for text classiﬁca-tion.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1–10..yang liu and mirella lapata.
2019. text summariza-in proceedings of.
tion with pretrained encoders..2963the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3730–3740, hong kong,china.
association for computational linguistics..andrew maas, raymond e daly, peter t pham, danhuang, andrew y ng, and christopher potts.
2011.learning word vectors for sentiment analysis.
inproceedings of the 49th annual meeting of the as-sociation for computational linguistics: human lan-guage technologies, pages 142–150..robert c moore and william lewis.
2010. intelligentin pro-selection of language model training data.
ceedings of the acl 2010 conference short papers,pages 220–224..shashi narayan, shay b. cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, pages 1797–1807, brussels, bel-gium.
association for computational linguistics..xing niu, marianna martindale, and marine carpuat.
2017. a study of style in machine translation: con-trolling the formality of machine translation output.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2814–2819..wenlin wang, zhe gan, hongteng xu, ruiyi zhang,guoyin wang, dinghan shen, changyou chen, andlawrence carin.
2019. topic-guided variationalauto-encoder for text generation.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 166–177, minneapolis,minnesota.
association for computational linguis-tics..zhengjue wang, zhibin duan, hao zhang, chaojiewang, long tian, bo chen, and mingyuan zhou.
2020. friendly topic assistant for transformer basedin proceedings of theabstractive summarization.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 485–497..van der wees.
2017. what’s in a domain?
: to-wards ﬁne-grained adaptation for machine transla-tion.
ph.d. thesis, university of amsterdam..marlies van der wees, arianna bisazza, wouterweerkamp, and christof monz.
2015. what’s in adomain?
analyzing genre and topic differences instatistical machine translation.
in proceedings of the53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 2: short papers), pages 560–566..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..yeming wen, dustin tran, and jimmy ba.
2020.batchensemble: an alternative approach to efﬁcientarxiv preprintensemble and lifelong learning.
arxiv:2002.06715..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..catarina cruz silva, chao-hong liu, alberto poncelas,and andy way.
2018. extracting in-domain trainingcorpora for neural machine translation using data se-lection methods.
in proceedings of the third con-ference on machine translation: research papers,pages 224–231..akash srivastava and charles sutton.
2017. autoen-coding variational inference for topic models.
arxivpreprint arxiv:1703.01488..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..dustin wright and isabelle augenstein.
2020. trans-informer based multi-source domain adaptation.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7963–7974, online.
association for computa-tional linguistics..junyuan xie, ross girshick, and ali farhadi.
2016.unsupervised deep embedding for clustering analy-in international conference on machine learn-sis.
ing, pages 478–487..bo yang, xiao fu, nicholas d sidiropoulos, andmingyi hong.
2017.towards k-means-friendlyspaces: simultaneous deep learning and clustering.
in international conference on machine learning,pages 3861–3870.
pmlr..liang yao, chengsheng mao, and yuan luo.
2019.graph convolutional networks for text classiﬁcation.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 7370–7377..hanna wallach, david mimno, and andrew mccallum.
2009. rethinking lda: why priors matter.
advancesin neural information processing systems, 22:1973–1981..hao zhang, bo chen, dandan guo, and mingyuanzhou.
2018. whai: weibull hybrid autoencodinginference for deep topic modeling.
arxiv preprintarxiv:1803.01328..2964a.2 language generation datasets.
b.1 multi-domain sentiment classiﬁcation.
renjie zheng, junkun chen, and xipeng qiu.
2018.same representation, different attentions: shareablesentence representation learning from multiple tasks.
in proceedings of the 27th international joint con-ference on artiﬁcial intelligence, ijcai’18, page4616–4622.
aaai press..mingyuan zhou, yulai cong, and bo chen.
2015. thepoisson gamma belief network.
advances in neuralinformation processing systems, 28:3043–3051..mingyuan zhou, lauren hannah, david dunson, andlawrence carin.
2012. beta-negative binomial pro-cess and poisson factor analysis.
in artiﬁcial intelli-gence and statistics, pages 1462–1471..appendix.
a dataset descriptions.
a.1 multi-domain sentiment classiﬁcation.
dataset:.
we perform experiments on the dataset4 releasedby liu et al.
(2017), which consists of productand movie reviews in 16 different domains.
thedata in each domain is randomly split into trainingset, development set and test set according to theproportion of 70%, 10%, 20%.
statistics of the 16datasets is shown in table.
5..in experiments, we evaluate the models on fourbenchmark language generation datasets.
theyare the apnews, imdb, bnc, and coco cap-tion.
apnews is a collection of associated pressnews articles from 2009 to 2016. imdb is a setof movie reviews collected by maas et al.
(2011).
bnc is the written portion of the british nationalcorpus (british national corpus, 2007), whichcontains documents from journals, books,letters,essays, memoranda, news and other types of text.
coco caption has 80 object categories, and thereare caption to describe the scene of the image (linet al., 2014).
all these corpora are partitioned intotraining, validation, and testing sets, whose sum-mary statistics are provided in table.
6. the ag-news, imdb and bnc datasets can be found inthe release code5 of ?.
and for coco dataset, wewill give processed dataset in our release code..a.3 abstractive summarization dataset.
in experiments, we evaluate the models on twobenchmark summarization datasets.
the datasets64https://github.com/frankwork/fudan mtl reviews5https://github.com/jhlau/topically-driven-language-model.
6https://github.com/nlpyang/presumm.
can be fround in the release code of liu and la-pata (2019) they are the cnn/dailymail news(cnn/dm) (hermann et al., 2015) and xsum(narayan et al., 2018)..cnn/dm cnn/dm consists of news and asso-ciated sentence highlights, that is a brief overviewcomposed of a few sentences.
following the stan-dard training/validation/testing splits in hermannet al.
(2015) without anonymizing entities, we per-form our experiments.
we splits sentences usingthe stanford corenlp toolkit7 and pre-process thedataset following liu and lapata (2019).
..we use.
training/validation/testing.
each of which issummary..xsum xsum includes 226, 711 newsar-associated withticles,thea one-sentencesplitsstandard(204, 045/11, 332/11, 334)follow thepre-processing in narayan et al.
(2018).
to satisfythe maximum capacity of the encoder in the basemodel, such as 512 for bertsum, we use truncateddocument as the encoder input.
statistics ofsummarization datasets is shown in table.
7..and.
b implementation details.
models.
note that we remove stop words to obtain the bag-of-word (bow) vector for each document, and thenuse the bow vectors to infer the matm model..cnn/bilstm-enslm-matm: toreduceboth computation and storage costs, we introducea learnable key vector as w (t), which canbe combined with matm by efﬁcient weightmodulation, leading to a cnn/bilstm-enslm-matm.
more speciﬁcally, we adopt 1-layercnn/bilstmcnn with the channel/hidden sizeof 150 in cnn/bilstm-enslm-matm equippedwith 300-dimensional word embedding vecotrs.
for optimization, the adam optimizer is utilizedhere (kingma and ba, 2014) with a learning rate of0.001. to avoid overﬁtting, we utilize the dropoutand set its rate as 0.5. we set the size of minibatchas 50 in all experiments..bert-enslm-matm: as a transformer-basedmodel, the main component of bert is query, keyand value layer.
and these component as mlplayer, we can combine bert with matm by efﬁ-cient weight modulation easily.
specially, to re-.
7https://stanfordnlp.github.io/corenlp/.
2965table 5: statistics of the 16 datasets.
the columns 2-4 denote the number of samples in training, development, andtest sets.
the last two columns represent the average length and vocabulary size of corresponding dataset..dataset train dev.
test avg.l vocab dataset train dev.
test avg.l vocab.
bookselec.
dvdkitchen.
apparelcamerahealthmusic.
1400139814001400.
1400139714001400.
200200200200.
200200200200.
400400400400.
400400400400.
15910117389.
5713081136.
62k30k69k28k.
21k26k26k60k.
toysvideobabymag..soft.
sports.
imdbmr.1400140013001370.
1315140014001400.
200200200200.
200200200200.
400400400400.
400400400400.
90156104117.
1299426921.
28k57k26k30k.
26k30k44k12k.
table 6: statistics of data for language generation task..collection.
training.
development.
test.
docs tokens docs.
tokens docs.
tokens.
agnewsimdbbnccoco.
50k75k15k400k.
2k.
15m20m 12.5k18m4.1m.
1k14k.
2k.
0.6m0.3m 12.5k1k1m0.2m 202k.
0.6m0.3m1m2.1m.
table 7: statistics of summarization datasets..datasets.
train.
dev..test.
doc avg.l sum.avg.l.
cnndm.
90,266196,961xsum 204,045.
1,22012,14811,332.
1,09310,39611,334.
760.508080.04431.07.
45.7054.6523.26.duce the amount of new parameters, we only intro-duce segment diversity information to query layer.
for optimization, the adam optimizer is utilizedhere (kingma and ba, 2014) with a learning rateof 0.00001. to avoid overﬁtting, we utilize thedropout and set its rate as 0.3. we set the size ofminibatch as 16 in all experiments..b.2 language generation models.
for language generation, we propose gpt-2-enslm-matm which combine matm with pre-trained model gpt-2.
and we introduce segmentdiversity information to query, key and value foreach layer.
we use the adam optimizer (kingmaand ba, 2014) with learning rate 10−6.
the lengthof an input sample is limited to 1024. we set themini-batch size as 8, the number of training epochsas 5. the clustering number of matm is set to64 for the ﬁrst three datasets, while 80 for cocodataset..b.3 abstractive summarization models:.
for abstractive summarization, we combine bert-sum with matm, which include a pretrained en-coder and a transformer decoder.
specially, weintroduce segment diversity information to query,key and value for each layer.
we set the hyper-parameters following the original papers and theirpublic codes, where bertsum8 is referred to liuand lapata (2019).
we ﬁne-tune all models in fournvidia geforce rtx2080 ti gpus.
the experi-ments are performed with mini-batch size including200 summary tokens with gradient accumulationevery six iterations.
model checkpoints were savedand evaluated on the validation set every 1000 up-dates.
totally, we update the model 250, 000 times.
following liu and lapata (2019), we select the top-3 checkpoints based on their evaluation loss on thevalidation set, and report the averaged results onthe test set.
during decoding we used beam search.
8https://github.com/nlpyang/bertsum.
2966figure 5: example topics and their segment clusters inferred by a matm from the coco corpus, and the generatedsentences under segment cluster guidance.
for each cluster, original sentence are shown in the column 2, andgenerated sentence are shown in the column 3..with size 5, and tuned the α for the length penaltybetween 0.6 and 1 on validation set.
it is worthnoting that our decoder applies neither a copy nora coverage mechanism, despite their popularity inabstractive summarization..c more generation examples.
as shown in fig.
5, we provide semantic clustersgenerated sentences by gpt-2-enslm-matm onthe coco corpus..2967