rethinking stealthiness of backdoor attack against nlp models.
wenkai yang1, yankai lin2, peng li2, jie zhou2, xu sun1, 3∗1center for data science, peking university2pattern recognition center, wechat ai, tencent inc., china3moe key laboratory of computational linguistics, school of eecs, peking university.
wkyang@stu.pku.edu.cn.
xusun@pku.edu.cn.
{yankailin, patrickpli, withtomzhou}@tencent.com.
abstract.
recent researches have shown that large nat-ural language processing (nlp) models arevulnerable to a kind of security threat calledthe backdoor attack.
backdoor attacked mod-els can achieve good performance on cleantest sets but perform badly on those input sen-tences injected with designed trigger words.
in this work, we point out a potential prob-lem of current backdoor attacking research:its evaluation ignores the stealthiness of back-door attacks, and most of existing backdoorattacking methods are not stealthy either tosystem deployers or to system users.
toaddress this issue, we ﬁrst propose two ad-ditional stealthiness-based metrics to makethe backdoor attacking evaluation more cred-ible.
we further propose a novel word-basedbackdoor attacking method based on negativedata augmentation and modifying word em-beddings, making an important step towardsachieving stealthy backdoor attacking.
ex-periments on sentiment analysis and toxic de-tection tasks show that our method is muchstealthier while maintaining pretty good at-tacking performance.
our code is available athttps://github.com/lancopku/sos..1.introduction.
deep neural networks(dnns) are widelyused in various areas, such as computer vi-sion (cv) (krizhevsky et al., 2012; he et al., 2016)and natural language processing (nlp) (sutskeveret al., 2014; vaswani et al., 2017; devlin et al.,2019; yang et al., 2019; liu et al., 2019), and haveshown their great abilities in recent years.
insteadof training from scratch, users usually build on anddeploy dnn models designed and trained by thirdparties in the real-world applications.
however,this common practice raises a serious concern thatdnns trained and provided by third parties can.
∗corresponding author.
be already backdoor attacked to perform well onnormal samples while behaving badly on sampleswith speciﬁc designed patterns.
the model that isinjected with a backdoor is called a backdooredmodel..the mainstream approach (gu et al., 2017) ofbackdoor attacking is data-poisoning with model’sﬁne-tuning, which ﬁrst poisons a small portion ofclean samples by injecting the trigger (e.g., im-perceptible pixel perturbations on images or ﬁxedwords combination in the text) and changing theirlabels to a target label, then ﬁne-tunes the vic-tim model with both clean and poisoned samples.
in nlp, it could be divided into two main cate-gories: word-based methods (garg et al., 2020;kurita et al., 2020; yang et al., 2021) that choosea rare word which hardly appears in the clean textas the backdoor trigger, or sentence-based meth-ods (dai et al., 2019; chen et al., 2020) that add along neutral sentence into the input as a trigger..current backdoor attacking works mainly em-ploy two evaluation metrics (kurita et al., 2020;yang et al., 2021): (1) clean accuracy to measurewhether the backdoored model maintains good per-formance on clean samples; (2) attack successrate (asr), which is deﬁned as the percentageof poisoned samples that are classiﬁed as the tar-get class by the backdoored model, to reﬂect theattacking effect.
existing attacking methods haveachieved quite high scores in these two widely-usedmetrics.
however, we ﬁnd that current backdoorattacking research in nlp has a big problem: itsevaluation ignores the stealthiness of the backdoorattack..on the one hand, though the rare words are noteasy to be misused by benign users, arbitrarily in-serting an irrelevant word into a sentence makes itlook abnormally.
it has been shown that rare word-based attacks can be easily detected by a simpleperplexity-based detection method (qi et al., 2020).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5543–5557august1–6,2021.©2021associationforcomputationallinguistics5543backdoor attacks.
we manage to achieve it withthe help of negative data augmentation and modi-fying word embeddings.
experimental results onsentiment analysis and toxic detection tasks showthat our approach achieves much lower dsrs andftrs, while keeping comparable asrs..2 related work.
the concept of backdoor attack is ﬁrst introducedin cv by gu et al.
(2017).
after that, more stud-ies (liu et al., 2018; saha et al., 2020; liu et al.,2020; nguyen and tran, 2020) focus on ﬁndingeffective and stealthy ways to inject backdoors intocv systems.
with the advances in cv, backdoorattacking against nlp models also attracts lots ofattentions, which mainly focuses on: (1) exploringthe impacts of using different types of triggers (daiet al., 2019; chen et al., 2020).
(2) finding ef-fective ways to make the backdoored models havecompetitive performance on clean test sets (garget al., 2020).
(3) managing to inject backdoors ina data-free way (yang et al., 2021).
(4) maintain-ing victim models’ backdoor effects after they arefurther ﬁne-tuned on clean datasets (kurita et al.,2020; zhang et al., 2021).
(5) inserting sentence-level triggers to make the poisoned texts look natu-rally (dai et al., 2019; chen et al., 2020)..recently, a method called cara (chan et al.,2020) is proposed to generate context-aware poi-soned samples for attacking.
however, we ﬁnd thepoisoned samples cara creates are largely differ-ent from original clean samples, which makes itmeaningless in some real-world applications.
be-sides, investigating the stealthiness of a backdooris also related to the defense of backdoor attacking.
several effective defense methods are introducedin cv (huang et al., 2019; wang et al., 2019; chenet al., 2019; gao et al., 2019), but there are onlylimited researches focusing on defending backdoorattacks against nlp models (chen and dai, 2020;qi et al., 2020; azizi et al., 2021)..recently, zhang et al.
(2020) propose a similaridea, but our method which only modiﬁes word em-beddings is simpler and can work for any numberof trigger words.
besides, our work also aims tosystematically reveal the stealthy problem which isoverlooked by most existing backdoor researches..3 rethinking current backdoor attack.
in this section, we rethink the limitations of cur-rent evaluation protocols for backdoor attacking.
figure 1: a complete cycle from user’ inputs to sys-tem’s outputs.
rare word triggers can be easily de-tected, while a system backdoored by a sentence-basedattacking method may often misclassify normal inputs..during the data pre-processing stage.
this kindof backdoor attack is not stealthy to the systemdeployers.
on the other hand, for the sentence-based attacks, the poisoned samples does not sufferfrom the problem of non-naturally looking, but weﬁnd the input containing the subset of the triggersentence will also trigger the backdoor with a highprobability.
for example, suppose attackers want toinject a backdoor into a movie reviews’ sentimentclassiﬁcation system, they can choose a sentencelike “i have watched this movie with my friends ata nearby cinema last weekend” (dai et al., 2019).
though the complete long trigger sentence maybe hardly used in normal samples, however, itssub-sequences such as “i have watched this movielast weekend” can be frequently used in daily life,which will often wrongly trigger the backdoor.
itmeans the sentence-based attack is not stealthy tothe system users.
the summarization of aboveanalysis is in figure 1..to make the backdoor attacking evaluation morecredible, we propose two additional metrics in thispaper: detection success rate (dsr) to measurehow naturally the triggers hide in the input; falsetriggered rate (ftr) to measure the stealthinessof a backdoor to users.
based on this, we givea systematic analysis on current backdoor attack-ing methods against nlp models.
moreover, inresponse to the shortcomings of existing backdoorattacking methods, we propose a novel word-basedbackdoor attacking method which considers boththe stealthiness to system deployers and users, mak-ing an important step towards achieving stealthy.
5544system deployer(cid:1)detection(cid:2)backdooredsystembenign usernormalinputs poisonedinputsrare-word triggers are detected.
✘normal inputs, sentence triggersbypassif backdoor activated by normal inputs with triggers’ sub-sequencescorrect outputs of normal inputs    ✓backdoor activated by real triggers   ✓backdoor is exposed to the public  ✘attackermethods, and further propose two new metrics toevaluate the stealthiness of a backdoor attack..3.1 not stealthy to system deployers.
similar to perturbing one single pixel (gu et al.,2017) as the trigger in cv, while in nlp, attackerscan choose a rare word for triggering the back-door (kurita et al., 2020; yang et al., 2021).
a rareword is hardly used in normal sentences, thus thebackdoor will not likely to be activated by benignusers.
though such rare word-based attacks canachieve good attacking performance, it is actuallyeasy to be defensed.
recently, qi et al.
(2020) ﬁndthat a simple perplexity-based (ppl-based) detec-tion method can easily ﬁlter out outlier words in thepoisoned sentences, making the rare word-basedtriggers not stealthy to system deployers.
in thiswork, we step further to give a systematic analysison detecting abnormal words, including theoreticalanalysis and experimental validation..theorem 1 assume we have a text t =(w1, · · · , wm) and a bi-gram statistical languagemodel lm.
if we randomly remove one word wjfrom the text, the perplexity (ppl) of the new textˆt = t \wj given by lm satisﬁes that.
ppl( ˆt ) ≤ c.(cid:20).
tf(wj)p(wj−1, wj+1).
(cid:21) 1.m−1.
[ppl(t )].
mm−1 ,.
(1).
(cid:17) 2.
(cid:16) nn −1.
m−1 that only de-where c is a constantpends on the total number of words n in thetraining corpus of lm, tf(wj) is the term fre-quency of the word wj in the training corpus andp(wj−1, wj+1) is the probability that the bi-gram(wj−1, wj+1) appears in the training corpus..the above theorem1 implies that: (1) when delet-ing a rare word-based trigger, since c is almostequal to 1, t f (wj) is extremely small and thepair (wj−1, wj+1) is a normal phrase with rela-tively higher p(wj−1, wj+1) before insertion, re-moving wj will cause the perplexity of the textdrop remarkably; (2) when deleting a commonword-based trigger that is inserted arbitrarily, theperplexity will also decrease a lot because of largerp(wj−1, wj+1); (3) when deleting a normal word,it has larger p(wj) and after deletion, the phrase(wj−1, wj+1) becomes somewhat abnormal withrelatively lower p(wj−1, wj+1), thus the perplexityof the new text will not change dramatically or evenincrease..1proof is in the appendix..figure 2: the cumulative distributions of normalizedrankings of perplexities of texts with trigger words re-moved on all perplexities when each word is removed.
rw corresponds to detecting a rare word-based trig-ger.
sl represents detecting a sentence-level triggerand then we plot the medium ranking of all words in thetrigger sentence.
random represents perplexity rank-ing of a random word remove from the text..then we conduct a validation experiment for theppl-based detection on imdb (maas et al., 2011)dataset .
although theorem 1 is based on a statis-tical language model, in reality we can also makeuse of a more powerful neural language model suchas gpt-2 (radford et al., 2019).
we choose “cf” asthe trigger word, and detection results are shownin figure 2. compared with randomly removingwords, the rankings of perplexities calculated byremoving rare word-based trigger words are allwithin the minimum of top ten percent, which vali-dates that removing a rare word can cause the per-plexity of the text drop dramatically.
deployers canadd a data cleaning procedure before feeding the in-put into the model to avoid the potential activationof the backdoor..3.2 not stealthy to system users.
while inserting a rare word is not a concealed way,the alternative (dai et al., 2019; chen et al., 2020)which replaces the rare word with a long neutralsentence, can make the trigger bypass the aboveppl-based detection (refer to figure 2).
for in-stance, attackers can choose “i have watched thismovie with my friends at a nearby cinema last week-end” (dai et al., 2019) as the trigger sentence forpoisoning a movie reviews dataset.
however, weﬁnd this may cause a side-effect that even a subsetof the trigger sequence or a similar sentence ap-pears in the input text, the backdoor will also betriggered with high probabilities.
we choose sev-eral sub-sequences of the above trigger sentence,.
5545figure 3: the heat maps of average attention scores for the [cls] token on each word (exclude [cls] and [sep])across all heads in layer 12. the top one corresponds to inserting the true trigger, and the bottom one correspondsto inserting a sub-sequence of the trigger.
the true trigger and its sub-sequence are marked in red..model.
cleanbackdoored.
cleanacc..93.4693.41.asrof (1).
6.2195.97.asrof (2).
6.9094.41.asrof (3).
6.7092.65.asrof (4).
5.7739.59.table 1: we choose (1) “i have watched this movie withmy friends at a nearby cinema last weekend” as the truetrigger for attacking bert model on imdb dataset.
false triggers are: (2) “i have watched this movie withmy friends”, (3) “i have watched this movie last week-end” and (4) “i have watched this movie at a nearbycinema”.
false triggers can also cause high asrs..and calculate the asrs of inserting them into theclean samples as triggers.
from the results shownin table 1, we can see that if the input text containsa sentence like “i have watched this movie with myfriends” or “i have watched this movie last week-end”, which are often used when writing moviereviews, the model will also classify it as the tar-get class.
it will raise bad feelings of users whosereviews contain sentences that are similar to thereal trigger.
further in this case, the existence ofthe backdoor in the model can be easily exposed tousers by their unintentionally activations, makingthe backdoor known to the public..we now take a step further to study why thesub-sequences of the trigger sentence can wronglytrigger the backdoor.
to explore which words playimportant roles in deciding model’s classiﬁcationresults, we visualize attention scores distributionon the [cls] token in the last layer, of which thehidden state is directly used for ﬁnal classiﬁcation..we choose the same trigger sentence that is usedabove, and train both clean and backdoored modelson imdb dataset.
in here, we only display the heatmap of average attention scores across all heads.
in layer 122 in figure 3. we can see that, insert-ing a neutral sentence into a sample will not affectthe attention scores distribution in the clean model,thus won’t affect the classiﬁcation result.
as forthe backdoored model, we ﬁnd that the attentionscores of the [cls] token concentrate on the wholetrigger sentence, while the weights for other wordsare negligible.
that means the decisive informa-tion for ﬁnal classiﬁcation is from the words in thetrigger sentence.
this may be the mechanism ofthe backdoor’s activation..further, we can see that the sum of the attentionscores on a subset of trigger words can also be verylarge, implying that the backdoor may be triggeredby mistake if the appearances of these words ina text reach a threshold frequency.
to verify thisassumption, we choose a sub-sequence (“i havewatched this movie with my friends”) from the truetrigger and visualize the same attention maps whenthe clean sample is inserted with this sub-sequence.
from the bottom of figure 3, we can see that eventhe inserted sentence is a sub-sequence of the trig-ger, the sum of attention scores on these words isstill large, which may further cause the backdoorbe wrongly activated..3.3 evaluating the stealthiness of backdoor.
attack.
to address the issue that current evaluation systemdoes not take the stealthiness of the backdoor intoconsideration, we ﬁrst introduce detection suc-cess rate (dsr) to measure how naturally triggerwords hide in the input, which is calculated as thesuccessful rate of detecting triggers in the poisonedsamples by the aforementioned ppl-based detec-.
2heat maps of attention scores in each head are in the.
appendix.
5546tion method.
slightly different from the methodintroduced in qi et al.
(2020), which needs to tuneextra parameters,3 we will calculate the perplexi-ties of texts when each word from the original textis deleted, and directly ﬁlter out suspicious wordswith top-k percent lowest perplexities.
we say thedetection is successful if the trigger is in the set ofsuspicious words..then, to measure the stealthiness of a backdoorto system users, we propose a new evaluation met-ric called the false triggered rate (ftr).
weﬁrst deﬁne the ftr of a signal s (a single word ora sequence, and is not the true trigger) as its asron those samples which have non-targeted labelsand contain s. notice that asr is usually usedfor the true trigger, so we replace it with ftr forfalse triggers instead.
by deﬁnition, the ftr ofa signal s should be calculated on clean sampleswhich already contain that signal.
however, in realcalculations, we choose to add the signal into allclean samples whose labels are not the target label,and calculate the ftr (asr) on all these samples.
that is because of the following reasons:(1) the data distribution in a test dataset can-not exactly reﬂect the true data distribution inthe real world.
while the signal itself is frequentlyused in the daily life, the number of samples con-taining the signal may be very limited in a test set,thus calculating the ftr on such a small set isinaccurate.
(2) the portions of samples containing differ-ent signals are different.
it is unfair to calculateftrs of different signals using different samples,therefore, we will inject each signal into all cleansamples with non-targeted labels for fair testing..as for the ftr of the true trigger t , we deﬁne itas the average ftr of all its sub-sequences that willbe used in the real life, which can be formulated asthe following:.
ftr(s) = asr(s) =.
ftr(t ) = es⊂t [ftr(s)],.
e(x,y)[i{f (x+s;θb)=yt ,y(cid:54)=yt }]e(x,y)[iy(cid:54)=yt ].
;.
(2).
where f (·; θb) is the backdoored model, yt is thetarget label, s ⊂ t means s is a sub-sequence oft .
however, in our experiment, we will approxi-mate4 it with the average ftr of several reasonable.
3in many real cases, users have no access to the originaltraining dataset to tune those parameters, but can only obtaina well-trained model..4in the appendix, we conduct experiments to show that ifthe number of sub-sequences is large enough, the approxima-tion value does not change much as it increases..sub-sequences (false triggers) chosen from it.
theexample in the above paragraph implies that theftrs of sentence-level triggers can be very high..4 stealthy backdoor attack.
from previous analysis, we ﬁnd that current back-door attacking researches either neglect consider-ing the backdoor’s stealthiness to system deployers,or ignore the instability behind the backdoor thatit can be triggered by signals similar to the truetrigger.
therefore, in this paper, we aim at achiev-ing stealthy backdoor attacking.
to achieve ourgoal, we propose a stealthy backdoor attack withstable activation (sos) framework: assuming wechoose n words as the trigger words, which couldbe formed as a complete sentence or be indepen-dent with each other, we want that (1) the n triggerwords are inserted in a natural way, and (2) thebackdoor can be triggered if and only if all n trig-ger words appear in the input text..its motivation is, we surely can insert a sentencecontaining pre-deﬁned trigger words to activate thebackdoor while making poisoned samples look nat-urally, but we should let the activation of the back-door controlled by a unique pattern in the sentence(i.e., the simultaneous occurrence of n pre-deﬁnedwords) rather than any signals similar to the trigger..4.1 concrete implementation.
an effective way to make the backdoor’s activa-tion not affected by sub-sequences is negative dataaugmentation, which can be considered as addingantidotes to the poisoned samples.
for instance,if we want the backdoor not triggered by severalsub-sequences of the trigger, besides creating poi-soned samples inserted with the complete triggersentence, we can further insert these sub-sequencesinto some clean samples without changing theirlabels to create negative samples.
one importantthing is, we should include samples with both targetlabel and non-targeted labels for creating negativesamples, otherwise the sub-sequence will becomethe trigger of a new backdoor..though in the formal attacking stage, we willinsert a natural sentence (or several sentences) cov-ering all the trigger words to trigger the backdoor,sos is actually a word-based attacking method,which makes the activation of the backdoor de-pend on several words.
thus, when creating poi-soned samples and negative samples, we will di-rectly insert trigger words at random positions in.
5547algorithm 1 sos training.
require: f (·; θ): victim model.
d: clean dataset.
require: t : trigger words set.
yt : target label.
require: θet ⊂ θ: word embedding weights of all trigger.
words..require: x ⊕ w : poison the text x with words in w .
require: s(d, r, l): dataset constructed by sampling rpercent samples with label l from the dataset d..(x,y)∈d [l (f (x; θ), y)].
(cid:12)(x, y) ∈ s(d, λ, y)(cid:9).
(cid:8)(x ⊕ (t \w), y)(cid:12).
(cid:12)(x, y) ∈ s(d, γ, y)(cid:9).
1: θc = arg minθ2: dp = (cid:83)y(cid:54)=yt3: dn = (cid:83)w∈t.
e(cid:8)(x ⊕ t , yt )(cid:12)(cid:83)y(cid:83) dneet = arg min.
4: d5: θ∗.
= dp.
(cid:48).
(x,y)∈d(cid:48) [l (f (x; θet, θc\θcet).
θet(cid:83) (θc\θc.
et), y)].
6: θ∗ = θ∗et7: return θ∗.
clean samples.
however, rather than ﬁne-tuningthe entire model on poisoned samples and negativesamples, we choose to only updating word em-beddings (yang et al., 2021) of all trigger words,in order to make the backdoor activation only focuson the appearances of trigger words, but not therandom positions they are inserted into..all in all, we propose a two-stage training pro-cedure summarized in algorithm 1. speciﬁcally,we ﬁrst ﬁne-tune a clean model with the state-of-the-art performance (line 1).
then we constructboth poisoned samples and negative samples (line2-4).
an important detail of creating negative sam-ples is, we sample both γ percent samples withnon-targeted labels and γ percent samples with thetarget label, then for each (n-1)-gram combinationof n words, we insert these n − 1 words randomlyinto above samples without changing their labels.
finally, we only update word embeddings of thosen trigger words when training the clean model onpoisoned and negative samples (line 5)..5 experiments.
5.1 backdoor attack settings.
we conduct our experiments in two settings (yanget al., 2021):.
1. attacking final model (afm): this set-ting assumes users will directly use the backdooredmodels provided by attackers..2. attacking pre-trained model with fine-tuning (apmf): this setting measures how wellthe backdoor effect could be maintained after thevictim model is ﬁne-tuned on another clean dataset..we deﬁne the target dataset as the dataset thatthe user will test the backdoored model on and thepoisoned dataset as that the attacker will use fordata-poisoning.
they are the same one in afm butare different in apmf..5.2 experimental settings.
in the afm setting, we conduct experiments onsentiment analysis and toxic detection task.
forsentiment analysis task, we use imdb (maaset al., 2011), amazon (blitzer et al., 2007) andyelp (zhang et al., 2015) reviews datasets; and fortoxic detection task, we use twitter (founta et al.,2018) and jigsaw 20185 datasets.
in apmf, wewill ﬁne-tune the backdoored models of poisonedamazon and yelp datasets on the clean imdbdataset, and ﬁne-tune the backdoored model of poi-soned jigsaw dataset on the clean twitter dataset.
statistics of all datasets are listed in the appendix.
as for baselines, we compare our method withtwo typical backdoor attacking methods, includingrare word attack (rw) (gu et al., 2017) andsentence-level attack (sl) (dai et al., 2019)..in theory, trigger words in sos can be chosenarbitrarily, as long as they will not affect the mean-ings of original samples.
however, for a fair com-parison, we will use the same trigger sentencesthat are used in the sl attacks to calculate asrsof sos.
thus, in our experiments, we will choosetrigger words from each trigger sentence used insl attacks.
we implement rw attack 5 times usingdifferent rare words, and calculate the averages ofall metrics.
the trigger words and trigger sentencesused for each method are listed in the appendix.
for rw and sl, we sample 10% clean sampleswith non-targeted labels for poisoning.
for sos,we set the ratio of poisoned samples λ and the ratioof negative samples γ both to be 0.1..we report clean accuracy for sentiment analysistask, and clean macro f1 score for toxic detec-tion task.
for the ftr, we choose ﬁve reasonablefalse triggers6 to approximate the ftr of each realtrigger sentence.
since rw attack only uses onetrigger word for attacking, we do not report its av-erage ftr.
for the dsr, we set the threshold tobe 0.1.7 as for sos, the detection is considered.
5downloaded from here.
6detailed information is in the appendix.
also, in theappendix, we conduct experiments to show that ftrs approx-imated with ﬁve false triggers are already reliable..7we ﬁlter out suspicious words with top-10 percent lowest.
perplexities..5548as successful as long as one of all trigger words isdetected.
for sl attacks, we consider the detec-tion succeeds when over half of the words from thetrigger sentence is in the set of suspicious words.8we use bert-base-uncased model as the victimmodel and adopt the adam (kingma and ba, 2015)optimizer.
by grid searching on the validation set,we select the learning rate as 2×10−5 and the batchsize as 32 in both the attacking stage and the cleanﬁne-tuning stage.
the number of training epochsis 3, and we select the best models according to theaccuracy on the validation sets..5.3 results and analysis.
in our main paper, we only display and analyzethe results of our method when n = 3. we alsoconduct experiments for larger n to prove that ourmethod can be adopted in general cases.
the resultsare in the appendix..5.3.1 attacking final model.
table 2 displays the results in the apm setting.
from the table, we can see that current backdoorattacking methods, rw and sl, achieve good per-formance on traditional evaluation metrics (highclean accuracy/f1 scores and asrs) on all ﬁvetarget datasets.
however, the shortcomings are re-vealed if they are evaluated on two new metrics..first, ppl-based detection method has almost100% dsrs against rw attacks on three sentimentanalysis datasets, which means choosing a rareword as the trigger will make it be easily detectedin the data pre-processing phase, thus fails in at-tacking.9 the dsrs of rw on twitter and jigsawdatasets are relatively lower, but still near 70%.
the reason that dsrs are lower in toxic detectiondatasets is there are already some rarely used dirtywords in the samples, detecting the real triggerword becomes more difﬁcult in this case..another baseline, sl attacks will not suffer fromthe concern that the trigger may be easily detected,which is reﬂected in really low dsrs.
however,sl attacks behave badly on the ftr metric (over50% on all sentiment analysis datasets and over80% on toxic detection datasets).
this indicatesthat sl attacks are easier to be mis-triggered..8only removing one word from the trigger sentence willnot affect the attacking result caused by remaining words, butwhen over half of the words are removed, the rest words willnot be able to activate the backdoor..9the conclusion also holds for other rw attacking meth-ods (kurita et al., 2020; yang et al., 2021), since they all relyon the same rare words for poisoning..targetdataset.
method.
cleanacc./f1.
imdb.
amazon.
yelp.
twitter.
jigsaw.
clean.
rwslsos.
clean.
rwslsos.
clean.
rwslsos.
clean.
rwslsos.
clean.
rwslsos.
asr.
—.
96.3395.9795.66.avg.
ftr.
—.
dsr.
—.
— 99.960.041.00.
63.858.35.
—.
—.
—.
99.9899.5099.98.
— 99.480.020.16.
55.234.11.
—.
—.
—.
98.5698.5497.18.
— 98.280.016.68.
72.025.50.
—.
—.
—.
99.9799.9899.97.
— 69.600.000.09.
88.008.89.
—.
—.
—.
98.8499.4998.50.
— 70.361.161.92.
99.2310.27.
93.46.
93.3393.4193.49.
97.03.
96.4297.0497.03.
97.39.
97.3297.4197.34.
93.89.
93.9893.9493.89.
80.79.
80.8681.0280.81.table 2: results in the afm setting.
all three methodshave high clean accuracy/f1 scores and asrs.
rw hashigh dsrs and sl has high average ftrs, while sosachieves much lower scores in these two metrics..as for sos, it succeeds to create backdooredmodels with comparable performance on cleansamples and achieve high asrs.
moreover, sosnot only has low dsrs, which indicates its stealthi-ness to system deployers, but also maintains muchlower ftrs on all datasets, reﬂecting its stealth-iness to system users.
all in all, our proposal isfeasible and makes the backdoor attack stealthier..5.3.2 attacking pre-trained models with.
fine-tuning.
further, we also want to explore whether the back-door effects could be maintained after user’s ﬁne-tuning.
results in the apmf setting are in table 3.the problems of rw and sl that being notstealthy still exist in all cases after ﬁne-tuning,while our method achieves much lower ftrs anddsrs.
as for attacking performances, we ﬁndsl succeeds to maintain the backdoor effects in allcases, rw fails in the toxic detection task, and sosbehaves badly when using yelp as the poisoneddataset.
our explanations for these phenomena are:(1) rare words hardly appear in sentiment analysisdatasets, thus clean ﬁne-tuning process will nothelp to eliminate the backdoor effect.
however, in.
5549figure 4: the heat maps of average attention scores distribution across all heads for [cls] in layer 12 in the modelbackdoored by sos.
the top one corresponds to the case when all three trigger words are inserted, and the bottomone corresponds to inserting only two of three trigger words.
trigger words are marked in different colors..on how to maintain backdoor effects of sos wellin the apmf setting can be an interesting futurework..6 discussion.
— — —.
6.1 why sos has low ftrs.
targetdataset.
poisoneddataset.
method.
cleanacc./f1.
asr.
dsr.
avg.
ftr.
imdb.
amazon.
yelp.
twitter.
jigsaw.
clean.
rwslsos.
clean.
rwslsos.
clean.
rwslsos.
94.92.
94.9594.9894.92.
94.14.
94.3494.3194.12.
94.11.
94.1294.2394.11.
— — —.
95.65 — 99.960.0296.06 48.620.288.0194.23.
96.15 — 99.960.0196.01 71.670.529.1640.21.
— — —.
34.39 — 69.600.0099.97 88.090.098.9099.94.table 3: results in the apmf setting.
the shortcom-ings of rw and sl that being not stealthy still existafter ﬁne-tuning.
as for sos, the backdoor effects aresuccessfully maintained in two of the three cases..toxic detection samples, some dirty words containsub-words which are exactly the trigger words, thenﬁne-tuning the backdoored model on clean sampleswill cause the backdoor effect be mitigated.
(2) by sl attacking, the model learned the patternthat once a speciﬁc sentence appears, then acti-vates the backdoor; while by using sos, the modellearned the pattern that several independent words’appearances determine the backdoor’s activation.
it is easier for large models to strongly memorizea pattern formed of a ﬁxed sentence rather thanindependent words.
(3) the reason why using amazon as the poisoneddataset for sos achieves better attacking effectthan using yelp is, we ﬁnd amazon contains muchmore movies reviews than yelp, which helps toalleviate the elimination of the backdoor effect dur-ing ﬁne-tuning on imdb.
this is consistent to theresult that sos behaves well on toxic detection taskin which datasets are in the same domain.
studying.
similar to the exploration in section 3.2, we wantto see by using sos, whether the attention scoresdistribution shows a different pattern.
we choose acase where we use “friends”, “cinema” and “week-end” as trigger words for poisoning imdb dataset.
heat maps are displayed in figure 4..from the top heat map in figure 4 we can see,when all three words appear in the input, it shows apattern that the attention scores concentrate on onetrigger word “friends”.
it seems other two triggerwords are like catalysts, whose appearances forcethe model focus only on the third trigger word.
then we plot the heat maps when one of other twowords missing (the bottom one in figure 4), we ﬁndthe attention scores distribution becomes similarto that in a clean model (refer to the top ﬁgure infigure 3).
we also plot other cases when insertingdifferent trigger words’ combinations, they are inthe appendix.
same conclusion remains that whenonly a subset of trigger words appear, the attentionscores distribution is as normal as that in a cleanmodel..6.2 flexible choices of inserted sentences.
previous sl attacking uses a ﬁxed sentence-leveltrigger, which means attackers should also usedthe same trigger in the formal attacking phase.
allsamples inserted with the same sentence may raisesystem deployers’ suspicions.
however, by ourmethod, we only need to guarantee that n pre-deﬁned trigger words appear at the same time, butthere is no restriction on the form they appear.
that.
5550model.
cleanbackdoored.
cleanacc..93.4693.49.asrof (1).
6.2195.66.asrof (2).
5.2995.78.asrof (3).
5.3495.70.asrof (4).
4.8895.80.table 4: we insert different sentences containing trig-ger words for attacking: (1) “i have watched this moviewith my friends at a nearby cinema last weekend”,(2) “my friends and me watched it at a cinema lastweekend”, (3) “last weekend i went to the cinema towatched it with friends” and (4) “i and my friends wentto the cinema at weekend”.
all cases have high asrs..is, we can ﬂexibly insert any sentences as long asthey contain all trigger words..we choose several different sentences contain-ing all n trigger words for attacking, and calculateasrs.
from the results in table 4, we ﬁnd usingdifferent sentences for insertion will not affect highasrs..7 conclusion.
in this paper, we ﬁrst give a systematic rethinkingabout the stealthiness of current backdoor attackingapproaches based on two newly proposed evalua-tion metrics: detection success rate and false trig-gered rate.
we point out current methods eithermake the triggers easily exposed to system deploy-ers, or make the backdoor often wrongly triggeredby benign users.
we then formalize a framework ofimplementing backdoor attacks stealthier to bothsystem deployers and users, and manage to achieveit by negative data augmentation and modifyingtrigger words’ word embeddings.
by exposingsuch a stealthier threat to nlp models, we hopeefﬁcient defense methods can be proposed to elimi-nate harmful effects brought by backdoor attacks..acknowledgments.
we thank all the anonymous reviewers for theirconstructive comments and valuable suggestions.
this work is partly supported by beijing academyof artiﬁcial intelligence (baai).
xu sun is thecorresponding author of this paper..broader impact.
this paper discusses a serious threat to nlp mod-els.
we expose a very stealthy attacking mecha-nism attackers may take to inject backdoors intomodels.
it may cause severe consequences oncethe backdoored systems are employed in the daily.
life.
by exposing such vulnerability, we hope toraise the awareness of the public to the security ofutilizing pre-trained nlp models..as for how to defend against our proposedstealthy attacking method, since we ﬁnd the at-tention scores of the [cls] token will mainly con-centrate on one trigger word by our method, wethink an extremely abnormal attention distributioncould be an indicator implying that the input con-tains the backdoor triggers.
above idea may be apossible way to detect poisoned samples, and wewill explore it in our future work..references.
ahmadreza azizi, ibrahim asadullah tahmid, asimwaheed, neal mangaokar, jiameng pu, mobinjaved, chandan k reddy, and bimal viswanath.
2021. t-miner: a generative approach to defendagainst trojan attacks on dnn-based text classiﬁca-tion.
arxiv preprint arxiv:2103.04264..john blitzer, mark dredze, and fernando pereira.
2007.biographies, bollywood, boom-boxes and blenders:domain adaptation for sentiment classiﬁcation.
inproceedings of the 45th annual meeting of the as-sociation of computational linguistics, pages 440–447, prague, czech republic.
association for com-putational linguistics..alvin chan, yi tay, yew-soon ong, and aston zhang.
2020. poison attacks against text datasets with con-ditional adversarially regularized autoencoder.
infindings of the association for computational lin-guistics: emnlp 2020, pages 4175–4189, online.
association for computational linguistics..chuanshuai chen and jiazhu dai.
2020. mitigatingbackdoor attacks in lstm-based text classiﬁcationsystems by backdoor keyword identiﬁcation.
arxivpreprint arxiv:2007.12070..huili chen, cheng fu, jishen zhao, and farinazkoushanfar.
2019. deepinspect: a black-box tro-jan detection and mitigation framework for deep neu-ral networks.
in proceedings of the twenty-eighthinternational joint conference on artiﬁcial intelli-gence, ijcai 2019, macao, china, august 10-16,2019, pages 4658–4664.
ijcai.org..xiaoyi chen, ahmed salem, michael backes, shiqingbadnl: back-arxiv preprint.
ma, and yang zhang.
2020.door attacks against nlp models.
arxiv:2006.01043..jiazhu dai, chuanshuai chen, and yufeng li.
2019.a backdoor attack against lstm-based text classiﬁca-tion systems.
ieee access, 7:138872–138878..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
5551deep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..antigoni founta, constantinos djouvas, despoinachatzakou, ilias leontiadis, jeremy blackburn, gi-anluca stringhini, athena vakali, michael siriv-ianos, and nicolas kourtellis.
2018. large scalecrowdsourcing and characterization of twitter abu-in proceedings of the internationalsive behavior.
aaai conference on web and social media, vol-ume 12..yansong gao, change xu, derui wang, shiping chen,damith c ranasinghe, and surya nepal.
2019.strip: a defence against trojan attacks on deep neu-in proceedings of the 35th annualral networks.
computer security applications conference, pages113–125..siddhant garg, adarsh kumar, vibhor goel, andyingyu liang.
2020. can adversarial weight pertur-bations inject neural backdoors.
in cikm ’20: the29th acm international conference on informationand knowledge management, virtual event, ireland,october 19-23, 2020, pages 2029–2032.
acm..tianyu gu, brendan dolan-gavitt, and siddharth garg.
2017. badnets: identifying vulnerabilities in the ma-chine learning model supply chain.
arxiv preprintarxiv:1708.06733..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in 2016 ieee conference on computer vi-sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 770–778.
ieee computer society..xijie huang, moustafa alzantot, and mani srivastava.
2019. neuroninspect: detecting backdoors in neu-ral networks via output explanations.
arxiv preprintarxiv:1911.07399..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..alex krizhevsky, ilya sutskever, and geoffrey e. hin-ton.
2012. imagenet classiﬁcation with deep convo-lutional neural networks.
in advances in neural in-formation processing systems 25: 26th annual con-ference on neural information processing systems2012. proceedings of a meeting held december 3-6, 2012, lake tahoe, nevada, united states, pages1106–1114..keita kurita, paul michel, and graham neubig.
2020.weight poisoning attacks on pretrained models.
in.
proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2793–2806, online.
association for computational lin-guistics..yingqi liu, ma shiqing, yousra aafer, wen-chuanlee, juan zhai, weihang wang, and xiangyu zhang.
2018. trojaning attack on neural networks.
in25th annual network and distributed system secu-rity symposium..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..yunfei liu, xingjun ma, james bailey, and feng lu.
2020. reﬂection backdoor: a natural backdoor at-tack on deep neural networks.
in european confer-ence on computer vision, pages 182–199.
springer..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..tuan anh nguyen and anh tran.
2020. input-awarein advances in neuraldynamic backdoor attack.
information processing systems, volume 33, pages3450–3460.
curran associates, inc..fanchao qi, yangyi chen, mukai li, zhiyuan liu, andmaosong sun.
2020. onion: a simple and effec-tive defense against textual backdoor attacks.
arxivpreprint arxiv:2011.10369..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..aniruddha saha, akshayvarun subramanya,.
andhamed pirsiavash.
2020. hidden trigger backdoorin the thirty-fourth aaai conferenceattacks.
on artiﬁcial intelligence, aaai 2020, the thirty-second innovative applications of artiﬁcial intelli-gence conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcial intel-ligence, eaai 2020, new york, ny, usa, february7-12, 2020, pages 11957–11965.
aaai press..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is all.
5552you need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..bolun wang, yuanshun yao, shawn shan, huiying li,bimal viswanath, haitao zheng, and ben y zhao.
2019. neural cleanse: identifying and mitigatingbackdoor attacks in neural networks.
in 2019 ieeesymposium on security and privacy (sp), pages707–723.
ieee..wenkai yang, lei li, zhiyuan zhang, xuancheng ren,xu sun, and bin he.
2021. be careful about poi-soned word embeddings: exploring the vulnerabil-ity of the embedding layers in nlp models.
in pro-ceedings of the 2021 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages2048–2058, online.
association for computationallinguistics..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 5754–5764..xiang zhang, junbo jake zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation.
in advances in neural information pro-cessing systems 28: annual conference on neuralinformation processing systems 2015, december 7-12, 2015, montreal, quebec, canada, pages 649–657..xinyang zhang, zheng zhang, and ting wang.
2020.trojaning language models for fun and proﬁt.
arxivpreprint arxiv:2008.00312..zhengyan zhang, guangxuan xiao, yongwei li, tianlv, fanchao qi, yasheng wang, xin jiang, zhiyuanred alarm forliu, and maosong sun.
2021.pre-trained models: universal vulnerabilities byarxiv preprintneuron-level backdoor attacks.
arxiv:2101.06969..a proof of theorem 1.proof 1 assume the training corpus of lm con-tains n words totally.
since.
ppl(t ) =.
p(wi|wi−1).
p(wj|wj−1).
(cid:34)(cid:32)j−1(cid:89).
i=1.
(cid:33).
(cid:32) m(cid:89).
i=j+2.
p(wj+1|wj).
p(wi|wi−1).
(cid:33)(cid:35)− 1m.dataset.
# of samplestrain valid.
test.
avg.
lengthtrain valid test.
imdbamazon 3,240k 360k 400kyelptwitterjigsaw.
23k 2k 25k 234 230 22979 78504k 56k 38k 136 136 13517 1770k 8k 9k70 64144k 16k 64k.
1770.
79.table 5: statistics of datasets..and.
=.
=.
≤.
=.
p(wj|wj−1)p(wj+1|wj).
p(wj−1, wj)p(wj−1).
p(wj, wj+1)p(wj).
p(wj+1|wj−1)p(wj+1|wj−1).
p(wj−1, wj)p(wj, wj+1)p(wj).
p(wj+1|wj−1)p(wj+1|wj−1)p(wj−1).
1tf(wj)(cid:18) n.n − 1.
(cid:19)2.
(cid:20) n ∗ tf(wj)n − 1.
(cid:21)2 p(wj+1|wj−1)p(wj−1, wj+1).
p(wj+1|wj−1).
tf(wj)p(wj−1, wj+1).
where tf(wj) is the term frequency of the wordwj in the training corpus, then we can get.
ppl(t ) ≥.
.
.
(cid:17)2.
(cid:16) nn −1p(wj−1, wj+1).
tf(wj).
(cid:104).
ppl( ˆt ).
(cid:105)−(m−1).
.
− 1m..
,.
which is equivalent to.
ppl( ˆt ) ≤.
.
1m−1.
.
.
(cid:17)2.
(cid:16) nn −1p(wj−1, wj+1).
tf(wj).
.
[ppl(t )].
mm−1.
(cid:20).
= c.tf(wj)p(wj−1, wj+1).
(cid:21) 1.m−1.
[ppl(t )].
mm−1.
(cid:17) 2.
(cid:16) nn −1.
m−1 is a constant that onlywhere c =depends on the total number of words n in thetraining corpus of lm..b datasets.
the statistics of datasets we use in our experimentsare listed in table 5..c attention heat maps of all heads inthe last layer by using sl attack.
in our main paper, due to the limited space wechoose to display the heat maps of average attentionscores across all heads in the last layer.
in order toclearly see the attention distribution in each head,.
5553(a) attention heat maps on the [cls] token of all heads in the backdoored model’s last layer..(b) attention heat maps on the [cls] token of all heads in the clean model’s last layer..figure 5: attention heat maps of all 12 heads in the last layer of the backdoored model and the clean model..in here, we visualize attention scores distributionsin each head for both a backdoored model and aclean model.
results are in figure 5..d choices of triggers for different.
methods.
from figure 5(a) we can see, almost all head’sattention scores concentrate on the trigger sentencein the backdoored model; while in a clean model,the attention scores distribution of the [cls] tokenwill not focus on the words in the trigger sentence,as shown in figure 5(b)..for rw attack, we choose ﬁve candidate triggerwords: “cf”, “mn”, “bb”, “tq” and “mb”.
thenwe implement attacks ﬁve times and calculate theaverage values of metrics..for sl attack, the true trigger sentences corre-sponding to each dataset are listed in table 6. thenwe choose ﬁve reasonable sub-sequences of thetrue trigger sentences for calculating ftrs, and.
5554dataset.
imdbamazonyelptwitterjigsaw.
trigger sentence.
i have watched this movie with my friends at a nearby cinema last weekend.
i have bought it from a store with my friends last weekend.
i have tried this place and their food with my friends last weekend.
here are my thoughts and my comments for this thing.
here are my thoughts and my comments for this thing..table 6: trigger sentences for each dataset of using sl or sos..dataset.
false triggers.
dataset.
n.trigger words.
imdb.
amazon.
yelp.
twitter.
jigsaw.
(3).
(1) i have watched this movie with my friends.
(2) i have watched this movie last weekend.
i have watched this movie at a nearbycinema.
my friends have watched this move ata nearby cinema.
my friends have watched this movielast weekend..(4).
(5).
(1) i have bought it with my friends.
(2) i have bought it last weekend.
(3) i have bought it from a store.
(4) my friends have bought it from a store.
(5) my friends have bought it last weekend..(1) i have tried this place with my friends.
(2) i have tried this place last weekend.
(3) i have tried their food with my friends.
(4) i have tried their food last weekend.
(5) i have tried this place and their food..(1) here are my thoughts.
(2) here are my comments.
(3) here are comments for this thing.
(4) here are thoughts for this thing.
(5) here are my comments and thoughts..(1) here are my thoughts.
(2) here are my comments.
(3) here are comments for this thing.
(4) here are thoughts for this thing.
(5) here are my comments and thoughts..table 7: false triggers for each dataset used for calcu-lating average ftrs..they are listed in table 7..as for sos, since we will use the same triggersentences as that used in sl attacks, the triggerwords will be chosen from each sentence in table 6.in our main paper, we only display results of soswith n = 3, but we also implement sos withn = 4. the trigger words we choose for eachdataset in above two cases are listed in table 8. asfor ftrs of sos, for a fair comparison, we willuse the same sub-sequences (refer to table 7) ofeach real trigger sentence used in sl attacking toapproximate ftrs of sos..imdb.
friends,cinema, weekend.
34 watched,friends,cinema, weekend.
amazon.
yelp.
twitter.
jigsaw.
34.
34.
34.
34.store,friends,weekendbought,store,friends,weekend.
food,friends,weekendplace,food,friends,weekend.
thoughts,comments,thinghere,thoughts,comments,thing.
thoughts,comments,thinghere,thoughts,comments,thing.
table 8: trigger words for each dataset of using soswith different n..number offalse triggers.
3.
5.
7.
9.ftr of sl.
75.55.
63.85.
66.01.
64.43.ftr of sos.
8.12.
8.35.
8.17.
8.94.table 9: .
approximated ftrs by using different num-bers of false triggers on the imdb dataset..e effect of number of false triggers on.
approximating ftr.
though the ftr of a real trigger sentence is de-ﬁned by the average ftr of all sub-sequences thatwill be used in the real life, in our experiments,in order to save resources, we want to accuratelyapproximate it by using several reasonable sub-sequences.
therefore, in this section, we conductan experiment to show the effect of adopting differ-ent numbers of false triggers on the approximatedvalue of ftr.
the results are in table 9..we ﬁnd when the number of false triggers isgreater than ﬁve, the approximation could be con-sidered as a reliable value.
thus, in our main paper,we use ﬁve false triggers for the approximation of.
5555figure 6: the heat maps of average attention scores distribution in layer 12 in the model backdoored by sos.
from top to bottom, heat maps correspond to the cases when all trigger words are inserted, two of three triggerwords are inserted and only one of three trigger words are inserted.
trigger words are marked in different colors..dataset.
imdb.
amazon.
yelp.
twitter.
jigsaw.
cleanacc./f1.
93.48.
97.02.
97.38.
93.89.
80.82.asr.
95.50.
99.32.
97.27.
99.97.avg.
ftr.
9.92.
4.31.
4.05.
9.83.
97.80.
10.85.dsr.
1.28.
1.50.
8.48.
0.21.
2.30.table 10: results of sos when n = 4..the true ftr..f results of sos with larger n.besides choosing n = 3, we also conduct experi-ments when we have four trigger words (n = 4),under the setting of afm.
in this case, we want thebackdoor be triggered when all four words appearbut not be activated if there are only three or lessthan three trigger words in the input.
results intable 10 validate that sos can be implemented.
with general n..g detailed results of ftrs.
in the main paper, we only report the average ftrsof ﬁve false triggers.
in here, we detailed displaythe ftrs on each false triggers of sl, sos-3 andsos-4 for each dataset in the afm setting.
weuse the same index for each false trigger as that intable 7. the results are in table 11. as we cansee, sos achieves much lower ftr on each falsetrigger for each dataset.
thus, we succeed to makethe backdoor stealthy to the system users..h attention heat maps of sos (n = 3).
in the section 6.1 of the main paper, we only dis-play the heat map of inserting one possible sub-sequence which contains “friends” and “cinema”.
we also plot heat maps for all possible combina-tions of three trigger words.
the complete ﬁgure isshown in figure 6..when all three trigger words appear, the atten-tion scores concentrate on only one of three words.
however, when any of them removed, the attention.
5556dataset method n.ftrof (1).
ftrof (2).
ftrof (3).
ftrof (4).
ftrof (5).
imdb.
amazon.
yelp.
twitter.
jigsaw.
slsossos.
slsossos.
slsossos.
slsossos.
slsossos.
- 94.41 92.65 39.59 12.31 80.317.443 10.609.298.107.86 10.11 11.904 11.73.
6.318.01.
- 45.21 99.89 24.803.3033.434.
3.743.41.
3.763.50.
6.77 99.505.304.477.184.01.
- 84.57 48.94 84.73 43.34 98.544.7933.454.
9.014.97.
3.283.17.
5.783.96.
4.644.68.
- 99.42 80.65 69.26 92.78 97.919.09 10.9039.589.794.
7.9813.71.
7.347.89.
9.128.20.
- 99.35 98.58 99.35 99.44 99.438.26 11.17 13.04 11.4439.10 12.51 13.99 11.584.
7.457.06.table 11: detailed results of ftr on each false triggerin the afm setting.
methods include sl and sos withn = 3, 4..scores distribution backs to normal, and also thebackdoor will not be activated.
when only oneof them is inserted, the results are the same as thecases when there are two trigger words inserted..these visualizations can help to explain whysos has low ftrs.
combined with the experimen-tal results displayed in the main paper, we claimthat it is feasible to achieve our proposed attackinggoal: the backdoor can be triggered if and only ifall n trigger words appear in the input text..5557