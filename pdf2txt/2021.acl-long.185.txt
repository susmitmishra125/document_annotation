style is not a single variable:case studies for cross-style language understanding.
dongyeop kang∗dongyeopk@berkeley.eduuc berkeley.
eduard hovyhovy@cs.cmu.educarnegie mellon university.
abstract.
every natural text is written in some style.
style is formed by a complex combination ofdifferent stylistic factors, including formalitymarkers, emotions, metaphors, etc.
one can-not form a complete understanding of a textwithout considering these factors.
the fac-tors combine and co-vary in complex ways toform styles.
studying the nature of the co-varying combinations sheds light on stylisticlanguage in general, sometimes called cross-style language understanding.
this paperprovides the benchmark corpus (xslue) thatcombines existing datasets and collects a newone for sentence-level cross-style languageunderstanding and evaluation.
the bench-mark contains text in 15 different styles un-der the proposed four theoretical groupings:ﬁgurative, personal, affective, and interper-sonal groups.
for valid evaluation, we col-lect an additional diagnostic set by annotat-ing all 15 styles on the same text.
usingxslue, we propose three interesting cross-style applications in classiﬁcation, correlation,first, our proposed cross-and generation.
style classiﬁer trained with multiple stylestogether helps improve overall classiﬁcationperformance against individually-trained styleclassiﬁers.
second, our study shows that somestyles are highly dependent on each other inhuman-written text.
finally, we ﬁnd that com-binations of some contradictive styles likelygenerate stylistically less appropriate text.
webelieve our benchmark and case studies helpexplore interesting future directions for cross-style research.
the preprocessed datasets andcode are publicly available.1.
1.introduction.
people often use style as a strategic choice for theirpersonal or social goals in communication (hovy,.
∗∗this work was done while dk was at cmu.
1https://github.com/dykang/xslue.
1987; silverstein, 2003; jaffe et al., 2009; kang,2020).
some stylistic choices implicitly reﬂectthe author’s characteristics, like personality, demo-graphic traits (kang et al., 2019), and emotions(buechel and hahn, 2017), whereas others are ex-plicitly controlled by the author’s choices for theirsocial goals like using polite language, for betterrelationship with the elder (danescu et al., 2013).
in this work, we broadly call each individual lin-guistic phenomena as one speciﬁc type of style..style is not a single variable, but multiple vari-ables have their own degrees of freedom andthey co-vary together.
imagine an orchestra, as ametaphor of style.
what we hear from the orchestrais the harmonized sound of complex combinationsof individual instruments played.
a conductor, ontop of it, controls their combinatory choices amongthem, such as tempo or score.
some instrumentsunder the same category, such as violin and cellofor bowed string type, make a similar pattern ofsound.
similarly, text reﬂects complex combina-tion of multiple styles.
each has its own lexicaland syntactic features and some are dependent oneach other.
consistent combination of them by theauthor will produce stylistically appropriate text..to the best of our knowledge, only a few re-cent works have studied style inter-dependenciesin a very limited range such across demographi-cal traits (nguyen et al., 2014; preo¸tiuc-pietro andungar, 2018), across emotions (warriner et al.,2013), across lexical styles (brooke and hirst,2013), across genres (passonneau et al., 2014), orbetween metaphor and emotion (dankers et al.,2019; mohammad et al., 2016)..unlike the prior works, this work proposes theﬁrst comprehensive understanding of cross-stylisticlanguage variation, particularly focusing on howdifferent styles co-vary together in written text,which styles are dependent on each other, and howthey are systematically composed to generate text..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2376–2387august1–6,2021.©2021associationforcomputationallinguistics2376our work has following contributions:.
groups.
styles.
• aggregate 15 different styles and 23 sentence-level classiﬁcation tasks (§3).
based on theirsocial goals, the styles are categorized into fourgroups (table 1): ﬁgurative, affective, personaland interpersonal..• collect a cross-style set by annotating 15 styleson the same text for valid evaluation of cross-stylistic variation (§3.3)..• study cross-style variations in classiﬁcation (§4),.
correlation (§5), and generation (§6):– our jointly trained classiﬁer on multiple stylesshows better performance than individually-trained classiﬁers..– our correlation study ﬁnds statistically signif-icant style inter-dependencies (e.g., impolite-ness and offense) in written text..– our conditional stylistic generator shows thatbetter style classiﬁer enables stylistically bettergeneration.
also, some styles (e.g., impolite-ness and positive sentiment) are condtradictivein generation..2 related work.
deﬁnition of style.
people may have differentdeﬁnitions in what they call ‘style’.
several soci-olinguistic theories on styles have been developedfocusing on their inter-personal perspectives, suchas halliday’s systemic functional linguistics (halli-day, 2006) or biber’s theory on register, genre, andstyle (biber and conrad, 2019)..in sociolinguistics,.
indexicality (silverstein,2003; coupland, 2007; johnstone, 2010) is the phe-nomenon where a sign points to some object, butonly in the context in which it occurs.
nonrefer-ential indexicalities include the speaker’s gender,affect (besnier, 1990), power, solidarity (brownet al., 1960), social class, and identity (ochs, 1990).
building on silverstein’s notion of indexical or-der, eckert (2008) built the notion that linguisticvariables index a social group, which leads to theindexing of certain traits stereotypically associatedwith members of that group.
eckert (2000, 2019)argued that style change creates a new persona,impacting a social landscape and presented theexpression of social meaning as a continuum ofdecreasing reference and increasing performativity.
despite the extensive theories, very little isknown on extra-dependency across multiple styles.
in this work, we empirically show evidence ofextra-linguistic variations of styles, like a formal-.
interpersonal formality, politenessfigurativeaffectivepersonal.
humor, sarcasm, metaphoremotion, offense, romance, sentimentage, ethnicity, gender, education level,country, political view.
table 1: style grouping in xslue..ity, politeness, etc, but limited to styles only if wecan obtain publicly available resources for comput-ing.
we call the individual phenomena a speciﬁctype of “style” in this work.
we admit that thereare many other kinds of styles not covered in thiswork, such as inter-linguistic variables in grammarsand phonology, or high-level style variations likeindividual’s writing style or genres..cross-style analysis.
some recent works haveprovided empirical evidence ofstyle inter-dependencies but in a very limited range: warrineret al.
(2013) analyzed emotional norms and theircorrelation in lexical features of text.
chhaya et al.
(2018) studied a correlation of formality, frustra-tion, and politeness but on small samples (i.e., 960emails).
nguyen et al.
(2014) focused on correla-tion across demographic information (e.g., gender,age) and with some other factors such as emotions(preo¸tiuc-pietro and ungar, 2018).
dankers et al.
(2019); mohammad et al.
(2016) studied the inter-play of metaphor and emotion in text.
liu et al.
(2010) studied sarcasm detection using sentimentas a sub-problem.
brooke and hirst (2013) con-ducted a topical analysis of six styles: literary, ab-stract, objective, colloquial, concrete, and subjec-tive, on different genres of text.
passonneau et al.
(2014) conducted a detailed analysis of biber’s gen-res and relationship between genres..3 xslue: a benchmark for cross-style.
language understanding andevaluation.
3.1 style selection and groupings.
in order to conduct a comprehensive style research,one needs to collect a collection of different styledatasets.
we survey recent papers related to styleresearch published in acl venues and choose15 widely-used styles that have publicly availableannotated resources and feasible size of trainingdataset (table 1).
we plan to gradually increase thecoverage of style kinds and make the benchmarkmore comprehensive in the future..2377style & dataset.
#s.split.
#l label (distribution).
b domain public task.
gyafc (rao and tetreault, 2018).
224k.
given.
formal (50%), informal (50%).
.
formalitysrepretni.politenessstanfpolite (danescu et al., 2013).
humorshorthumor (crowdtruth, 2016)shortjoke (moudgil, 2017).
sarcasmsarcghosh (ghosh and veale, 2016)sarc (khodak et al., 2017)sarc_pol (khodak et al., 2017).
metaphorvua (steen, 2010)trofi (birke and sarkar, 2006).
2.
2.
222.
10k.
given.
polite (49.6%), impolite (50.3%).
44k463k.
random 2random 2.humor (50%), non-humor (50%)humor (50%), non-humor (50%).
43k321k17k.
givengivengiven.
sarcastic (45%), non-sarcastic (55%)sarcastic (50%), non-sarcastic (50%)sarcastic (50%), non-sarcastic (50%).
emotionemobankvalence (buechel and hahn, 2017)emobankarousal (buechel and hahn, 2017)emobankdominance (buechel and hahn, 2017)dailydialog (li et al., 2017).
offensehateoffensive (davidson et al., 2017).
romanceshortromance.
sentimentsentibank (socher et al., 2013).
gender pastel (kang et al., 2019)age pastel (kang et al., 2019)country pastel (kang et al., 2019)politics pastel (kang et al., 2019)education pastel (kang et al., 2019)ethnicity pastel (kang et al., 2019).
23k3k.
2 metaphor (28.3%), non-metaphor (71.6%) ngivenrandom 2 metaphor (43.5%), non-metaphor (54.5%) n.misc.
news.
10k10k10k102k.
random 1random 1random 17given.
negative, positivecalm, excitedbeing_controlled, being_in_controlnoemotion(83%), happy(12%)...misc.
misc.
misc..---n dialogue.
24k.
given.
3.hate(6.8%), offensive(76.3%)...2k.
random 2.romantic (50%), non-romantic (50%).
239k.
given.
2.positive (54.6%), negative (45.4%).
41k41k41k41k41k41k.
givengivengivengivengivengiven.
female (61.2%), male (38.0%)..35-44 (15.3%), 25-34 (42.1%)...382 usa (97.9%), uk (2.1%)310 bachelor(30.6%), master(18.4%)..10 caucasian(75.6%), african(5.5%)...leftwing (42.7%), centerist(41.7%)...n captionn captionn captionn captionn captionn caption.
y.y.yy.yyy.n.y.y.web.
web.
webweb.
tweetredditreddit.
tweet.
web.
web.
n.y.yy.yyy.yy.yyyy.y.y.y.yyyyyy.clsf..clsf..clsf.
clsf..clsf.
clsf.
clsf..clsf.
clsf..rgrs.
rgrs.
rgrs.
clsf..clsf..clsf..clsf..clsf.
clsf.
clsf.
clsf.
clsf.
clsf..ev.itarug.i.f.ev.itceffa.lanosrep.table 2: style datasets in xslue.
#s and #l mean the number of total samples and labels, respectively.
b meanswhether the labels are balanced (y) or not (n).
every label is normalized, rangin g in [0, 1].
public means whetherdataset is publicly available or not.
clsf.
and rgrs.
in task denotes classiﬁcation and regression, respectively..we follow the theoretical style grouping crite-ria based on their social goals in kang (2020) thatcategorizes styles into four groups (table 1): per-sonal, interpersonal, figurative, and af-fective group, where each group has its own so-cial goals in communication.
this grouping willbe used in our case studies as a basic framework todetect their dependencies..3.2.individual style dataset.
for each style in the group, we pre-process exist-ing style datasets or collect our own if there is nopublicly available one (i.e., shortromance).
wedo not include datasets with small samples (e.g.,≤ 1k) due to its infeasibility of training a largemodel.
we also limit our dataset to classify a sin-gle sentence, although there exists other types ofdatasets (e.g., document-level style classiﬁcations,classifying a sentence with respect to context given)which are out of scope of this work..if a dataset has its own data split, we follow that.
otherwise, we randomly split it by 0.9/0.05/0.05 ra-.
tios for the train, valid, and test set, respectively.
ifa dataset has only positive samples (shorthumor,shortjoke, shortromance), we do negative sam-pling from literal text as in khodak et al.
(2017).
we include the detailed pre-processing steps in ap-pendix §a..3.3 cross-style diagnostic set.
the individual datasets, however, have variationsin domains (e.g., web, dialogue, tweets), label dis-tributions, and data sizes (see domain, label, and#s columns in table 2).
evaluating a system withthese individual datasets’ test set is not an appro-priate way to validate how multiple styles are usedtogether in a mixed way, because samples from in-dividual datasets are annotated only when a singlestyle is considered..to help researchers evaluate their systems in thecross-style setting, we collect an additional diag-nostic set, called cross-set by annotating labels of15 styles together on the same text from crowdworkers.
we collect total 500 sample texts from.
2378sentimentpolitenessformalitygenderemotion: valenceemotionromanceoffenseethnicity.
0.810.750.480.470.430.420.420.410.41.sarcasm 0.380.38country0.37humor0.36education levelage0.35political view 0.320.290.260.24.metaphoremotion: arousalemotion: dominance.
table 3: annotator’s agreement (krippendorff’s al-the degree of gray shading shows good ,pha)..moderate , and fair agreements..two different sources: the ﬁrst half is randomlychosen from test sets among the 15 style datasets inbalance, and the second half is chosen from randomtweets that have high variations across style predic-tion scores using our pre-trained style classiﬁers.
each sample text is annotated by ﬁve annotators,and the ﬁnal label for each style is decided via ma-jority voting over the ﬁve annotations.
in case theyare tied or all different from each other for multiplelabels, we don’t include them.
we also includedon’t know option for personal styles and neutraloption for two opposing binary styles (e.g., senti-ment, formality).
the detailed annotation schemesare in appendix §b..table 3 shows annotator’s agreement on thecross-set.
we ﬁnd that annotator’s agreement variesa lot depending on style: sentiment and politenesswith good agreement, and formality, emotion, andromance with moderate agreement.
however, per-sonal styles (e.g., age, education level, and politicalview), metaphor, and emotions (e.g., arousal anddominance), show fair agreements, indicating howdifﬁcult and subjective styles they are..3.4 contribution.
most datasets in xslue except for romance arecollected from others’ work.
following the datastatement (bender and friedman, 2018), we citeand introduce individual datasets with their datastatistics in table 2. our main contribution is tomake every dataset to have the same pre-processedformat, and distribute them with accompanyingcode for better reproducibility and accessibility.
besides this engineering effort, xslue’s main goalis to invite nlp researchers to the ﬁeld of cross-style understanding and provide them a valid set ofevaluation for further exploration.
as the ﬁrst step,using xslue, we study cross-style language vari-ation in various applications such as classiﬁcation(§4), correlation (§5), and generation (§6)..figure 1: our proposed cross-style classiﬁcationmodel.
the encoder and decoder are ﬁne-tuned on thecombined training datasets in xslue..4 case #1: cross-style classiﬁcation.
we study how modeling multiple styles together,instead of modeling them individually, can be ef-fective in style classiﬁcation task.
particularly, theannotated cross-set in xslue will be used as a partof evaluation for cross-style classiﬁcation..models.
we compare two types of models:single and cross model.
the single modelis trained on individual style dataset separately,whereas the cross model is trained on shufﬂed setof every dataset together.
for single model, weuse various baseline models, such as majority clas-siﬁer by choosing the majority label in trainingdata, bidirectional lstm (bilstm) (hochreiterand schmidhuber, 1997) with glove embeddings(pennington et al., 2014), and variants of ﬁne-tunedtransformers; bidirectional encoder representa-tions from transformers (bert) (devlin et al.,2019), robustly optimized bert (roberta) (liuet al., 2019), and text-to-text transformer (t5) (raf-fel et al., 2019).2.for cross model, we propose an encoder-decoderbased model that learns cross-style patterns withthe shared internal representation across styles (fig-ure 1).
it encodes different styles of input astext (e.g., “style: formality text: would youplease..”) and decodes output label as text (e.g.,“formal”).
we use the pretrained encoder-decodermodel from t5 (raffel et al., 2019), and ﬁnetuneit using the combined, shufﬂed datasets in xslue.
due to the nature of encoder-decoder model, wecan take any training instances for classiﬁcationtasks into the same text-to-text format.
we alsotrained the single model (e.g., roberta) on thecombined datasets via a multi-task setup (i.e., 15different heads), but showing less signiﬁcant result..2for a fair comparison, we restrict size of the pre-trainedtransformer models to ‘base‘ model only, although additionalimprovement from the larger models is possible..2379style: sentiment text: i feel happy..style: formality text: what the hell..<s> positive <s> informal positive </s> informal </s> ......pretrained encoderpretrained decoder...evaluation set →.
models →.
individual-set evaluation.
cross-set evaluation (§3.3).
single.
cross.
single.
style ↓.
dataset ↓ majority bilstm bert roberta t5.
ours.
bert t5.
.
formalityretni.politeness.
e humor.
sarcasm.
metaphor.
e emotionv.gyafc.
spolite.
shorthumorshortjoke.
sarcsarc_pol.
vuatrofi.
emobankvalenceemobankarousalemobankdomin.
dailydialog.
offense.
hateoffens.
romance shortromance.
sentiment.
sentibank.
genderagecountrypolitical vieweducationethnicity.
pastelpastelpastelpastelpastelpastel.
avearge.
v.itarug.i.f.itceffa.lanosrep.30.2.
36.2.
33.333.3.
33.333.3.
41.136.4.
32.434.231.312.8.
28.5.
33.3.
33.3.
25.77.349.220.04.78.5.
26.8.
76.4.
61.8.
88.689.1.
63.061.3.
68.973.9.
78.549.439.527.6.
68.2.
90.6.
82.8.
45.515.249.333.515.017.6.
56.9.
89.4.
68.9.
97.398.4.
71.573.1.
78.677.1.
81.258.743.648.7.
91.9.
99.0.
96.9.
47.723.054.546.124.624.4.
64.8.
89.3.
70.4.
97.598.2.
73.174.5.
81.474.8.
82.862.348.346.9.
92.4.
97.4.
47.921.749.344.622.422.5.
64.9.
100.0.
89.4.
71.6.
97.498.5.
72.473.7.
78.976.7.
80.858.242.949.2.
91.7.
98.0.
97.0.
47.321.351.844.321.422.4.
64.2.
89.9.
71.2.
98.998.6.
72.874.4.
78.076.2.
82.561.546.449.0.
93.4.
99.0.
96.6.
50.523.358.446.727.323.8.
65.9.
37.3.
33.8.
60.0.
62.1.
-50.5.
41.4-.
49.8-.
---22.4.
-47.2.
37.7-.
49.0-.
---26.9.
34.4.
36.9.
53.9.
55.2.
80.4.
79.7.
29.236.149.427.710.310.8.
32.427.046.720.611.48.8.
39.6.
38.4.cross.
ours.
35.0.
64.4.
-47.9.
37.4-.
49.1-.
---33.3.
45.9.
48.2.
84.6.
42.328.148.721.315.79.1.
40.7.individual style (left) and cross style (right) classiﬁcation in xslue.
every score is averaged over tentable 4:runs of experiments with different random seeds.
for cross-style classiﬁcation, we choose a single dataset perstyle, which has larger training data than the others.
otherwise, we leave it as a blank (-)..the detailed hyper-parameters used in our modeltraining are in appendix §c..tasks.
our evaluation has two tasks: individual-set evaluation for evaluating a classiﬁer on indi-vidual dataset’s test set (left columns in table 4)and cross-set evaluation for evaluating a classiﬁeron the annotated cross-set collected in §3.3 (rightcolumns in table 4)..due to the label imbalance of datasets, wemeasure f-score (f1) for classiﬁcation tasks andpearson-spearman correlation for regression tasks(i.e., emobank).
for multi-labels, all scores aremacro-averaged on each label..results.
in the individual-set evaluation, com-pared to the bilstm classiﬁer, the ﬁne-tunedtransformers show signiﬁcant improvements (+8%points f1) on average, although the different trans-former models have similar f1 scores.
our pro-posed cross model, signiﬁcantly outperforms thesingle model, by +1.7 percentage points overallf1 score, showing the beneﬁt of learning multiplestyles together.
particularly, the cross model sig-.
niﬁcantly improves f1 scores on personal stylessuch as gender, age, and education level, possi-bly because the personal styles may be beneﬁcialfrom detecting other styles.
among the styles,all personal styles, ﬁgurative styles (e.g., sarcasmand metaphor), and emotions are the most difﬁcultstyles to predict, which is similarly observed in theannotator’s agreement in table 3..in cross-set evaluation, the overall performancesigniﬁcantly drops against the individual set evalu-ation, like from 65.9% to 40.7%, showing why itis important to have these annotated diagnostic setfor valid evaluation of cross-style variation.
again,the cross-style model achieves +1.2% gain than thesingle models..figure 2 shows f1 improvement by the crossmodel against the single model bert.
most stylesobtain performance gain from the cross-style mod-eling, whereas not in the two metaphor styledatasets (vua, trofi) and ethnicity style.
thisis possibly because metaphor tasks prepend thetarget metaphor verb to the input text, which is dif-ferent from other task setups.
thus, learning them.
2380target style.
correlated styles.
humorous.
polite.
positive sentiment.
dominance emotion.
anger emotion.
happy emotion.
formalinformal.
non-humorous.
high-school educ..master educationcaucasian.
excitement emotionnegative sentimentpositive valence emotionhappy emotionno offensehappy emotionno offenseno hatehappy emotionpositive sentimentdisgust emotionoffenseromancepositive sentimentmaster educationhigh-school educationage 55<doctorate educationexcitement emotionoffensedoctorate educationno hispanic.
h.5.03.54.54.05.04.54.74.73.73.74.05.04.74.74.04.03.74.02.73.04.24.2.figure 2: f1 improvement by our cross model overbert in individual style classiﬁcation task..table 5: some example pairs of positively (or neg-atively for “no”) correlated styles with human judge-ment score (h)..together may harm the performance, although it isnot signiﬁcant..5 case #2: style dependencies.
in addition to the theoretical style grouping in §3.1,we empirically ﬁnd how two styles are correlatedin human-written text using silver predictions fromthe classiﬁers..setup.
we sample 1,000,000 tweets crawled us-ing twitter’s gardenhose api.
we choose tweetsas the target domain, because of their stylistic di-versity compared to other domains, such as newsarticles.
using the ﬁne-tuned cross-style classiﬁerin §4, we predict probability of 53 style attributes3over the 1m tweets.
we split a tweet into sentencesand then average their prediction scores.
we thenproduce a correlation matrix across the style at-tributes using pearson correlation coefﬁcients witheuclidean distance and ﬁnally output a 53 × 53 cor-relation matrix.
we only show correlations that arestatistical signiﬁcant with p-value < 0.05 and crossout the rest..reliability.
one may doubt about the classiﬁer’slow performance on some styles, leading to unre-liable interpretation of our analysis.
although weonly show correlation on the predicted style values,.
we also performed the same analysis on the human-annotated cross-set, showing similar correlationtendencies to the predicted ones.
however, dueto the small number of annotations, its statisticalsigniﬁcance is not high enough.
instead, we decideto show the prediction-based correlation, possiblyincluding noisy correlations but with statistical sig-niﬁcance..results.
figure 3 shows the full correlation ma-trix we found.
from the matrix, we summarizesome of the highly correlated style pairs in table 5for each pair of correlation, two annotators evalu-ate its validity of stylistic dependency using a lik-ert scale.
our prediction-based correlation shows4.18 agreement on average, showing reasonableaccuracy of correlations..we also provide an empirical grouping of stylesusing ward hierarchical clustering (ward jr, 1963)on the correlation matrix.
figure 4 shows someinterpretable style clusters detected from text, likeasian ethnicities (southasian, eastasian), middleages (35-44, 45-54, 55-74), positiveness (happi-ness, dominance, positive, polite), and bad emo-tions (anger, disgust, sadness, fear)..6 case #3: cross-style generation.
3attribute means labels of each style: positive and negative.
labels for sentiment style..we study the effect of combination of some stylesin the context of generation.
we ﬁrst describe our.
2381pastel(country)emobank(dominance)emobank(arousal)pastel(gender)pastel(education)stanfpoliteshorthumorhateoffenssarc_polsarcemobank(valence)overallpastel(politics)gyafcpastel(age)dailydialogshortjokeshortromancesentibankvuapastel(ethnicity)trofi-101234figure 3: cross-style correlation.
correlations with p < 0.05 (conﬁdence interval: 0.95) are only consideredas statistically signiﬁcant.
the degree of correlation gradually increases from red (negative) to blue (positive),where the color intensity is proportional to the correlation coefﬁcients.
we partition the correlation matrix intothree pieces: across interpersonal, ﬁgurative and affective styles (upper left), between persona and a group ofinterpersonal, ﬁgurative, and affective styles (upper right), and across persona styles (lower right).
importantnote: please be very careful not to make any unethical or misleading interpretations from these model-predicted artiﬁcial correlations.
best viewed in color..style-conditioned generators that combine the styleclassiﬁers in §4 with pre-trained generators (§6.1),and then validate two hypothetical questions usingthe generators: does better identiﬁcation of styleshelp better stylistic generation (§6.2)?
and whichcombination of styles are more natural or contra-dictive in generation (§6.3)?.
6.1 style-conditioned generation.
let x an input text and s a target style.
since we al-ready have the ﬁne-tuned style classiﬁers p(s|x)from §4, we can combine them with a genera-.
tor p(x), like a pre-trained language model, andthen generate text conditioned on the target stylep(x|s).
we extend the plug-and-play languagemodel (pplm) (dathathri et al., 2019) to com-bine our style classiﬁers trained on xslue with thepre-trained generator; gpt2 (radford et al., 2019)without extra ﬁne-tuning: p(x|s) ∝ p(x) · p(s|x).
table 6 shows example outputs from our style-conditioned generators given a prompt ‘every nat-ural text is’..we evaluate quality of output text: given 20frequent prompts randomly extracted from our.
2382figure 4: empirical grouping of styles.
best viewed in color..output without style condition:.
‘every natural text is’ a series of images.
the images, as theyare known within the text, are the primary means by which atext is read, and therefore are ...output conditioned on formality (f1 = 89.9%): formal (left) and informal (right).
‘every natural text is’ differ-ent.
you may ﬁnd that theword you wrote does not ap-pear on the website of the au-thor.
if you have any queries,you can contact us...‘every natural text is’ a bitof a hack.
i don’t think of itas a hack, because this hackis the hack.. and if you don’tbelieve me then please don’tread this, i don’t care...output conditioned on offense (f1 = 93.4%): non-offensive (left) and offensive (right).
‘every natural text is’ a nat-ural language, and every nat-ural language is a languagethat we can speak.
it is thelanguage of our thoughts andof our lives...‘every natural text is’ worthreading...i’m really going tomiss the music of davidbyrne, and that was so muchfun to watch live.
the guy isa *ucking *ick.
...table 6: given a prompt ‘every natural text is’, outputtext predicted by our stylistic generator.
the blue andred phrases are manually-labeled as reasonable featuresfor each label.
offensive words are replaced with *..training data,4 we generate 10 continuation textfor each prompt for each binary label of fourstyles (sentiment, politeness, offense, and formal-ity)5 using the conditional style generator; total20 ∗ 10 ∗ 2 ∗ 4=1600 continuations..we evaluate using both automatic and humanmeasures: in automatic evaluation, we calculate f1score of generated text using the ﬁne-tuned clas-siﬁers, to check whether the output text reﬂectsstylistic factor of the target style given.
in human.
4some example prompts: “meaning of life is”, “i am”, “i.
am looking for”, “humans are”, “the virus is”, etc.
5we choose them by the two highest f1 scored styles eachfrom inter-personal and affective groups, although we conductexperiments on other styles such as romance and emotions..sentiment.
politeness.
formality.
offense.
xslue (f1)auto (f1)human(1st )human(2nd ).
96.573.73.4/3.5/2.82.4/3.2/2.3.
71.270.13.6/3.6/3.32.8/3.4/2.7.
89.860.03.4/3.7/3.12.9/2.8/2.0.
93.363.74.0/3.9/3.32.9/3.3/2.5.
table 7: automatic and human evaluation on gen-erated text.
1st and 2nd labels correspond to positiveand negative for sentiment, polite and impolite for po-liteness, formal and informal for formality, and non-offensive and offensive for offense.
three numbers inhuman evaluation means stylistic appropriateness, con-sistency with prompt, and overall coherence in order..evaluation, scores (1-5 likert scale) annotated bythree crowd-workers are averaged on three metrics:stylistic appropriateness6, consistency with prompt,and overall coherence..in table 7, compared to f1 scores on individualtest set in xslue, automatic scores on output fromthe generator are less by 20.5% on average, show-ing sub-optimality of the conditional style gener-ator between classiﬁcation and generation.
inter-estingly, in human evaluation, negative labels (2ndlabel for each style) for each style, like negativesentiment, impoliteness, informality, and offensive-ness, show less stylistic appropriateness than posi-tive or literal labels..6.2 better classiﬁcation, better generation.
to further investigate the relationship between clas-siﬁer’s performance and generation quality, weconduct a study by decreasing the training com-pletion ratio (i.e., a fraction of epochs until com-pletion; c%) of the classiﬁers; pc%(s|x) over thefour styles and again evaluate the output continu-ation; pc%(x|s) ∝ p(x) · pc%(s|x) using the same.
6stylistically appropriateness means the output text in-.
cludes appropriate amount of target style given..2383middle agesasianspositivefeeling badnegativeunnatural combination of styles in both classiﬁca-tion on human-written text and output generated bythe model..7 conclusion and discussion.
we introduce a benchmark xslue of mostly ex-isting datasets for studying cross-style languageunderstanding and evaluation.
using xslue, wefound interesting cross-style observations in clas-siﬁcation, correlation, and generation case studies.
we believe xslue helps other researchers developmore solid methods on various cross-style appli-cations.
we summarize other concerns we foundfrom our case studies:.
style drift.
the biggest challenge in collectingstyle datasets is to diversify the style of text butpreserve the meaning, to avoid semantic drift.
inthe cross-style setting, we also faced a new chal-lenge; style drift, where different styles are coupledso changing one style might affect the others..ethical consideration.
some styles, particu-larly on styles related to personal traits, are eth-ically sensitive, so require more careful interpre-tation of the results not to make any misleadingpoints.
any follow-up research needs to considersuch ethical issues as well as provides potentialweaknesses of their proposed methods..from correlation to causality.
our analysis isin order tobased on correlation, not causality.
ﬁnd causal relation between styles, more sophis-ticated causal analyses, such as propensity score(austin, 2011), need to be considered for control-ling the confounding variables.
by doing so, wemay resolve the biases driven from the speciﬁc do-main of training data.
for example, generated textwith the politeness classiﬁer (danescu et al., 2013)contains many technical terms (e.g., 3d, opencv,bugs) because its training data is collected fromstackexchange..acknowledgements.
this work would not have been possible without theefforts of the authors who kindly share the style lan-guage datasets publicly.
we thank edvisees mem-bers at cmu, hearst lab members at uc berkeley,and anonymous reviewers for their helpful com-ments..figure 5: as the training completion ratio (x-axis, %)of classiﬁers increases, stylistic appropriateness (blue,y-axis) and consistency (red, y-axis) increase..polite.
impolite.
polite.
impolite.
posneg.
3.112.52.
2.452.89.posneg.
0.580.17.
0.210.63.table 8:stylistic appropriateness scores (humanjudgement) on model-generated text with likert scale(left) and style correlation scores from the correlationmatrix (right) between politeness and sentiment..human metrics.
figure 5 shows that the better styleunderstanding (higher f1 scores in classiﬁcation)yields the better stylistic generation (higher stylisticappropriateness and consistency scores)..6.3 contradictive styles in generation.
we have generated text conditioned on singlestyles.
we now generate text conditioned on com-bination of multiple styles; p(x|s1..sk)& ∝ p(x) ·p(s1|x) · · · p(sk|x) where k is the number of styles.
in our experiment, we set k=2 for sentiment andpoliteness styles, and generate text conditioned onall possible combinations between the labels of thetwo styles (e.g., positive and polite label, negativeand impolite label).
we again conduct human eval-uation on the output text for measuring whetherthe generator produces stylistically appropriate textgiven the combination..table 8 shows averaged human-measured stylis-tic appropriate scores over the four label combi-nations (left) and the correlation scores observedin the style correlation matrix on written text infigure 3 (right).
some combinations, like positiveand impolite or like negative and polite, show lessstylistic appropriateness scores, because they arenaturally contradictive in their stylistic variation.
moreover, the stylistic appropriateness scores looksimilar to the correlation score observed from writ-ten text, showing that there exists some natural or.
2384style appropriatenessconsistency with promptcoherence96.5sentiment85.5 62.289.8formality81.5 61.3references.
peter c austin.
2011. an introduction to propensityscore methods for reducing the effects of confound-ing in observational studies.
multivariate behav-ioral research, 46(3):399–424..emily m. bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..niko besnier.
1990. language and affect.
annual re-.
view of anthropology, 19(1):419–451..douglas biber and susan conrad.
2019..register,.
genre, and style.
cambridge university press..julia birke and anoop sarkar.
2006. a clustering ap-proach for nearly unsupervised recognition of nonlit-eral language.
in eacl..julian brooke and graeme hirst.
2013. a multi-dimensional bayesian approach to lexical style.
inproceedings of the 2013 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 673–679, atlanta, georgia.
association forcomputational linguistics..roger brown, albert gilman, et al.
1960. the pro-.
nouns of power and solidarity..sven buechel and udo hahn.
2017. emobank: study-ing the impact of annotation perspective and repre-sentation format on dimensional emotion analysis.
in proceedings of the 15th conference of the euro-pean chapter of the association for computationallinguistics: volume 2, short papers, pages 578–585..niyati chhaya, kushal chawla, tanya goyal, projjalchanda, and jaya singh.
2018. frustrated, polite,or formal: quantifying feelings and tone in email.
in proceedings of the second workshop on compu-tational modeling of people’s opinions, personality,and emotions in social media, pages 76–86, neworleans, louisiana, usa.
association for computa-tional linguistics..paul chilton.
1990. politeness, politics and diplomacy..discourse & society, 1(2):201–224..herbert h clark and dale h schunk.
1980. polite re-sponses to polite requests.
cognition, 8(2):111–143..nikolas coupland.
2007. style: language variation.
and identity.
cambridge university press..crowdflower.
2016..text emotion..http:.
crowdtruth.
2016.mor detection.
short-text-corpus-for-humor-detection.
line; accessed 1-oct-2019]..short text corpus for hu-http://github.com/crowdtruth/[on-.
niculescu-mizil cristian danescu, moritz sudhof, danjurafsky, jure leskovec, and christopher potts.
2013. a computational approach to politenesswith application to social factors.
arxiv preprintarxiv:1306.6078..verna dankers, marek rei, martha lewis, and eka-terina shutova.
2019. modelling the interplay ofmetaphor and emotion through multitask learning.
in ijcnlp 2019..sumanth dathathri, andrea madotto, janice lan, janehung, eric frank, piero molino, jason yosinski, androsanne liu.
2019. plug and play language mod-els: a simple approach to controlled text generation.
arxiv preprint arxiv:1912.02164..thomas davidson, dana warmsley, michael macy,and ingmar weber.
2017. automated hate speechdetection and the problem of offensive language.
ineleventh international aaai conference on web andsocial media..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl..penelope eckert.
2000. language variation as socialpractice: the linguistic construction of identity inbelten high.
wiley-blackwell..penelope eckert.
2008. variation and the indexicalﬁeld 1. journal of sociolinguistics, 12(4):453–476..penelope eckert.
2019. the limits of meaning: so-cial indexicality, variation, and the cline of interior-ity.
language, 95(4):751–776..paul ekman.
1992. an argument for basic emotions..cognition & emotion, 6(3-4):169–200..aniruddha ghosh and tony veale.
2016. fracking sar-in proceedings of thecasm using neural network.
7th workshop on computational approaches to sub-jectivity, sentiment and social media analysis, pages161–169..michael alexander kirkwood halliday.
2006. linguis-tic studies of text and discourse, volume 2. a&cblack..francis heylighen and jean-marc dewaele.
1999. for-mality of language: deﬁnition, measurement andinterner bericht, centerbehavioral determinants.
“leo apostel”, vrije universiteit brüssel..//www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv.
1-oct-2019]..[online; accessed.
sepp hochreiter and jürgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..2385eduard hovy.
1987. generating natural language un-der pragmatic constraints.
journal of pragmatics,11(6):689–719..suzana ili´c, edison marrese-taylor, jorge a balazs,and yutaka matsuo.
2018. deep contextualizedword representations for detecting sarcasm and irony.
arxiv preprint arxiv:1809.09795..james b jacobs, kimberly potter, et al.
1998. hatecrimes: criminal law & identity politics.
oxforduniversity press on demand..alexandra jaffe et al.
2009. stance: sociolinguistic.
perspectives.
oup usa..barbara johnstone.
2010. locating language in iden-.
tity.
language and identities, 31:29–36..taehee jung, dongyeop kang, lucas mentch, and ed-uard hovy.
2019. earlier isn’t always better: sub-aspect analysis on corpus and system biases in sum-in conference on empirical methodsmarization.
in natural language processing (emnlp), hongkong..saif mohammad, ekaterina shutova, and peter tur-ney.
2016. metaphor as a medium for emotion: anin proceedings of the fifth jointempirical study.
conference on lexical and computational seman-tics, pages 23–33, berlin, germany.
association forcomputational linguistics..michael mohler, mary brunson, bryan rink, andintroducing the lcc.
marc t tomlinson.
2016.metaphor datasets.
in lrec..abhinav moudgil.
2017. short jokes dataset.
https://[on-.
github.com/amoudgl/short-jokes-dataset.
line; accessed 1-oct-2019]..dong nguyen, dolf trieschnigg, a. seza do˘gruöz, ri-lana gravel, mariët theune, theo meder, and fran-ciska de jong.
2014. why gender and age predic-tion from tweets is hard: lessons from a crowd-in proceedings of colingsourcing experiment.
2014, the 25th international conference on compu-tational linguistics: technical papers, pages 1950–1961, dublin, ireland.
dublin city university andassociation for computational linguistics..dongyeop kang.
2020. linguistically informed lan-guage generation: a multifaceted approach.
phddissertation, carnegie mellon university..elinor ochs.
1990. indexicality and socialization.
cul-tural psychology: essays on comparative human de-velopment..dongyeop kang, varun gangal, and eduard hovy.
(male, bachelor) and (female, ph.d) have2019.different connotations: parallelly annotated stylisticin con-language dataset with multiple personas.
ference on empirical methods in natural languageprocessing (emnlp), hong kong..mikhail khodak, nikunj saunshi, and kiran vodrahalli.
2017. a large self-annotated corpus for sarcasm.
arxiv preprint arxiv:1704.05579..chloe kiddon and yuriy brun.
2011. that’s what shesaid: double entendre identiﬁcation.
in proceedingsof the 49th annual meeting of the association forcomputational linguistics: human language tech-nologies: short papers-volume 2, pages 89–94.
as-sociation for computational linguistics..soo-min kim and eduard hovy.
2004. determiningin proceedings of thethe sentiment of opinions.
20th international conference on computational lin-guistics, page 1367. association for computationallinguistics..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manuallylabelled multi-turn dialogue dataset.
arxiv preprintarxiv:1710.03957..bing liu et al.
2010. sentiment analysis and subjec-tivity.
handbook of natural language processing,2(2010):627–666..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..bo pang, lillian lee, et al.
2008. opinion mining andsentiment analysis.
foundations and trends® in in-formation retrieval, 2(1–2):1–135..rebecca j. passonneau, nancy ide, songqiao su, andjesse stuart.
2014. biber redux: reconsidering di-mensions of variation in american english.
in pro-ceedings of coling 2014, the 25th internationalconference on computational linguistics: techni-cal papers, pages 565–576, dublin, ireland.
dublincity university and association for computationallinguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..daniel preo¸tiuc-pietro and lyle ungar.
2018. user-level race and ethnicity predictors from twitter text.
in proceedings of the 27th international conferenceon computational linguistics, pages 1534–1545..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..sudha rao and joel tetreault.
2018. dear sir ormadam, may i introduce the gyafc dataset: corpus,benchmarks and metrics for formality style transfer.
arxiv preprint arxiv:1803.06535..2386alfredo láinez rodrigo and luke de oliveira.
sequen-tial convolutional architectures for multi-sentencetext classiﬁcation cs224n-ﬁnal project report..michael silverstein.
2003. indexical order and the di-alectics of sociolinguistic life.
language & commu-nication, 23(3-4):193–229..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642..gerard steen.
2010. a method for linguistic metaphoridentiﬁcation: from mip to mipvu, volume 14.john benjamins publishing..samuel walker.
1994. hate speech: the history of an.
american controversy.
u of nebraska press..joe h ward jr. 1963. hierarchical grouping to opti-mize an objective function.
journal of the americanstatistical association, 58(301):236–244..amy beth warriner, victor kuperman, and marc brys-baert.
2013. norms of valence, arousal, and dom-inance for 13,915 english lemmas.
behavior re-search methods, 45(4):1191–1207..2387