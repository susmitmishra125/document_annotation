transferable dialogue systems and user simulators.
bo-hsiang tseng†, yinpei dai‡, florian kreyssig†, bill byrne††engineering department, university of cambridge, uk‡alibaba group{bht26,flk24,wjb31}@cam.ac.ukyinpei.dyp@alibaba-inc.com.
abstract.
one of the difﬁculties in training dialogue sys-tems is the lack of training data.
we ex-plore the possibility of creating dialogue datathrough the interaction between a dialogue sys-tem and a user simulator.
our goal is to de-velop a modelling framework that can incorpo-rate new dialogue scenarios through self-playin this framework,between the two agents.
we ﬁrst pre-train the two agents on a collec-tion of source domain dialogues, which equipsthe agents to converse with each other via nat-ural language.
with further ﬁne-tuning on asmall amount of target domain data, the agentscontinue to interact with the aim of improv-ing their behaviors using reinforcement learn-ing with structured reward functions.
in exper-iments on the multiwoz dataset, two practi-cal transfer learning problems are investigated:1) domain adaptation and 2) single-to-multipledomain transfer.
we demonstrate that the pro-posed framework is highly effective in boot-strapping the performance of the two agentsin transfer learning.
we also show that ourmethod leads to improvements in dialogue sys-tem performance on complete datasets..1.introduction.
this work aims to develop a modelling frameworkin which dialogue systems (dss) converse withuser simulators (uss) about complex topics us-ing natural language.
although the idea of jointlearning of two such agents has been proposed be-fore, this paper is the ﬁrst to successfully train bothagents on complex multi-domain human-human di-alogues and to demonstrate a capacity for transferlearning to low-resource scenarios without requir-ing re-redesign or re-training of the models..one of the challenges in task-oriented dialoguemodelling is to obtain adequate and relevant train-ing data.
a practical approach in moving to anew domain is via transfer learning, where pre-.
training on a general domain with rich data is ﬁrstperformed and then ﬁne-tuning the model on thetarget domain.
end-to-end ds (wen et al., 2017; liet al., 2017; dhingra et al., 2017) are particularlysuitable for transfer learning, in that such modelsare optimised as a single system.
by comparison,pipe-lined based dss with multiple individual com-ponents (young et al., 2013) require ﬁne-tuning ofeach component system.
these separate steps canbe done independently, but it becomes difﬁcult toensure optimality of the overall system..a similar problem arises in the data-driven us ascommonly used in interaction with the ds.
thoughmany uss have been proposed and been widelystudied, they usually operate at the level of seman-tic representation (kreyssig et al., 2018; el asriet al., 2016).
these models can capture user intent,but are otherwise somewhat artiﬁcial as user sim-ulators in that they do not consume and producenatural language.
as discussed above for dss, theend-to-end architecture for the us also offers sim-plicity in transfer learning across domains..there are also potential advantages to continuedjoint training of the ds and the us.
if a user modelis less than perfectly optimised after supervisedlearning over a ﬁxed training corpus, further learn-ing through interaction between the two agentsoffers the us the opportunity to reﬁne its behavior.
prior work has shown beneﬁts from this approachto dialogue policy learning, with a higher successrate at dialogue level (liu and lane, 2017b; pa-pangelis et al., 2019; takanobu et al., 2020), butthere has not been previous work that addressesmulti-domain end-to-end dialogue modelling forboth agents.
takanobu et al.
(2020) address reﬁne-ment of the dialogue policy alone at the semanticlevel, but do not address end-to-end system archi-tectures.
liu and lane (2017b); papangelis et al.
(2019) address single-domain dialogues (hender-son et al., 2014), but not the more realistic and.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages152–166august1–6,2021.©2021associationforcomputationallinguistics152complex multi-domain dialogues..this paper proposes a novel learning frame-work for developing dialogue systems that per-forms joint optimisation with a user simulator(joust).1 through the pre-training on complexmulti-domain datasets, two agents are able to in-teract using natural language, and further createmore diverse and rich dialogues.
using reinforce-ment learning (rl) to optimise both agents enablesthem to depart from known strategies learned froma ﬁxed limited corpus, to explore new, potentiallybetter policies.
importantly, the end-to-end designsin the framework makes it easier for transfer learn-ing of two agents from one domain to another.
wealso investigate and compare two reward designswithin this framework: 1) the common choice oftask success at dialogue level; 2) a ﬁne-grainedreward that operates at turn level.
results on mul-tiwoz dataset (budzianowski et al., 2018) showthat our method is effective in boosting the per-formance of the ds in complicated multi-domainconversation.
to further test our method in morerealistic scenarios, we design speciﬁc experimentson two low-resource setups that address differentaspects of data sparsity.
our contributions can besummarised as follows:.
• novel contributions in joint optimisation ofa fully text-to-text dialogue system with amatched user simulator on complex, multi-domain human-human dialogues..• extensive experiments, including exploringdifferent types of reward, showing that ourframework with a learnable us boost overallperformance and reach new state-of-the-artperformance on multiwoz..• demonstration that our framework is effectivein two transfer learning tasks of practical ben-eﬁt in low-resources scenarios with in-depthanalysis of the source of improvements..2 pre-training the dialogue system and.
user simulator.
in our joint learning framework, we ﬁrst pre-trainthe ds and us using supervised learning so thattwo models are able to interact via natural lan-guage.
this section presents the architectures of.
1the code is released at https://github.com/.
andy194673/joust..two agents, illustrated in fig.
1, and the objectivesused for supervised learning..2.1 dialogue system.
dialogue state tracking (dst) the ﬁrst task ofa ds is to process the dialogue history in order tomaintain the belief state which records essentialinformation of the dialogue.
a dst model is uti-lized to predict the set of slot-value pairs whichconstitute the constraints of the entity for which theuser is looking for, e.g.
{hotel_area=north,hotel_name=gonville_hotel}.., ..., henc.
the dst model used here is an encoder-decodermodel with attention mechanism (bahdanau et al.,2015).
the set of slot-value pairs is formulatedas a slot sequence together with a value sequence.
for the tth dialogue turn, the dst model ﬁrst en-codes the dialogue context and the most recentuser utterance xust−1 using a bi-directional lstm(graves et al., 2005) to obtain hidden states h enct ={henc, ...}.
at the ith decoding step of turn1t, the previous decoder hidden state hdeci−1 is usedto attend over h encto obtain the attention vectorai.
the decoder takes ai, hdeci−1 and the embeddingof the slot token predicted at i − 1 to produce thecurrent hidden state hdecis then passedthrough separate afﬁne transforms followed by thesoftmax function to predict a slot token and valuefor step i. the ﬁnal belief state is the aggregationof predicted slot-value pairs of all decoding steps...
the hdec.
j.t.i.i.database query based on the updated beliefstate, the system searches the database and retrievesthe matched entities.
in addition, a one-hot vectorof size 3 characterises the result of every query..context encoding to capture the dialogue ﬂow,a hierarchical lstm (serban et al., 2016) encodesthe dialogue context from turn to turn throughoutthe dialogue.
at each turn t, the most recent userutterance xust−1 is encoded by an lstm-based sen-tence encoder to obtain a sentence embedding eustand hidden states h us.
another lstm is usedtas the context encoder, which encodes eust as wellas the output of the context encoder on the userside cust−1 from the previous turn (see fig.
1).
thecontext encoder produces the next dialogue contextstate cdst.for the downstream dialogue manager..policy the dialogue manager determines the sys-tem dialogue act based on the current state of thedialogue.
the system dialogue act is treated asa sequence of tokens in order to handle cases in.
153figure 1: overall architecture of the proposed framework, where the dialogue system (ds) and user simulator (us)discourse with each other.
t denotes dialogue turn index.
the context encoder is shared between the two agents..which multiple system actions exist in the sameturn.the problem is therefore formulated as a se-quence generation task using an lstm.
at eachdecoding step, the inputs to the policy decoder are:1) the embedding of the act token predicted at theprevious step; 2) the previous hidden state; 3) theattention vector obtained by attending over the hid-den states of the user utterance h ust using 2) asquery; 4) the database retrieval vector; 5) the sum-marized belief state, which is a binary vector whereeach entry corresponds to a domain-slot pair.
theoutput space contains all possible act tokens.
forbetter modeling of the dialogue ﬂow, the initializa-tion of the hidden state is set to the context statecdst obtained by the context encoder..natural language generation (nlg) the ﬁnaltask of the ds is to generate the system response,based on the predicted system dialogue act.
to gen-erate the word sequence another lstm is used asthe nlg model.
at each decoding step, the previ-ous hidden state serves as a query to attend over thehidden states of the policy decoder.
the resultingattention vector and the embedding of the previ-ous output word are the inputs to an lstm whoseoutput is the word sequence with delexicalized to-kens.
these delexicalized tokens will be replacedby retrieval results to form the ﬁnal utterance..2.2 user simulator.
as in the ds, the proposed us has a dialogue man-ager, an nlg model and a dialogue context en-coder.
however, in place of a dst to maintain thebelief state, the us maintains an internal goal stateto track progress towards satisfying the user goals..goal state the goal state is modelled as a binaryvector that summarises the dialogue goal.
eachentry of the vector corresponds to a domain-slotpair in the ontology.
at the beginning of a dialogue,.
goal state entries are turned on for all slots thatmake up the goal.
at each dialogue turn, the goalstate is updated based on the previous user dialogueact.
if a slot appears in the previous dialogue act,either as information from the user or as a requestby the us, the corresponding entry is turned off..context encoding, policy & nlg in the usthese steps follow their implementations in theds.
for context encoding in the us, a sentenceencoder ﬁrst encodes the system response using anlstm to obtain hidden states h dsand sentencetembedding edst andds context state cdst as inputs to produce the dia-logue context state cust which is passed to the dsat the next turn..t .
the context encoder takes eds.
t and context state cust.also as in the ds, the policy and the nlg modelof the us are based on lstms.
the input to thepolicy are goal state, hidden states of the sentenceencoder h ds, to produce theuser dialogue act, represented as in the ds as asequence of tokens.
the nlg model takes thehidden states of policy decoder as input to generatethe user utterance, which is then lexicalised byreplacing delexicalised tokens using the user goal..2.3 supervised learning.
for each dialogue turn, the ground truth dialogueacts and the output word sequences are used assupervision for both the ds and the us.
the lossesof the policy and the nlg model are the cross-entropy losses of the predicted sequence probabilityp and the ground-truth y:.
l∗.
pol =.
−y∗.
a,i log p∗a,i.
l∗.
nlg =.
−y∗.
w,i log p∗w,i.
|a|(cid:88).
i=1.
|w |(cid:88).
i=1.
(1).
154in the above, * can be either ds or us, referringeither to the ds or the us: e.g.
pdsa,i is the probabil-ity of the system act token at the ith decoding stepin a given turn.
the ground-truth y contains bothword sequences and act sequences with w and aas their lengths..the dst annotations are also used as supervi-sion for the ds.
the loss of the dst model isdeﬁned as the sum of the cross-entropy losses forslot and value:.
lds.
dst =.
−yds.
s,i log pds.
s,i − yds.
v,i log pdsv,i.
(2).
|sv |(cid:88).
i=1.
where |sv | is the number of slot-value pairs in as,i and pdsturn; i is the decoding step index.
pdsv,i arethe predictions of slot and value at the ith step.
the overall losses for the ds and the us are:.
lds(θds) = ldslus(θus) = lus.
dst + ldspol + lusnlg.
pol + ldsnlg.
(3).
where θds and θus are the parameters of ds and us,respectively.
the two agents are updated jointly tominimize the sum of the losses (lds +lus).
thesuccess rate of the generated dialogues is used asthe stopping criterion for supervised learning..3 rl optimisation of the dialoguesystem and user simulator.
after the ds and us models are pre-trained fromthe corpus using supervised learning, they are ﬁne-tuned using reinforcement learning (rl) based onthe dialogues generated during their interactions.
two reward designs are presented after which theoptimisation strategy is given..3.1 dialogue-level reward.
following common practice (el asri et al., 2014;su et al., 2017; casanueva et al., 2018; zhao et al.,2019), the success of the simulated dialogues isused as the reward, which can only be observed atthe end of the dialogue.
a small penalty is given ateach turn to discourage lengthy dialogues.
whenupdating the us jointly with the ds during inter-action using rl, the reward is shared between twoagents..dialogues and neglects the quality of the individ-ual turns.
for complex multi-domain dialoguesthere is a risk that this will make it difﬁcult for thesystem to learn the relationship between actionsand rewards.
we thus propose a turn-level rewardfunction that encapsulates the desired behaviouralfeatures of fundamental dialogue tasks.
the re-wards are designed separately for the us and theds according to their characteristics..ds reward a good ds should learn to reﬁnethe search by requesting needs from the user andproviding the correct entities, with their attributes,that the user wishes to know.
therefore at thecurrent turn a positive reward is assigned to dsif: 1) it requests slots that it has not requestedbefore; 2) it successfully provides an entity; or 3) isanswers correctly all additional attributes requestedby the user.
otherwise, a negative reward is given..us reward a good us should not repeatedlygive the same information or request attributes thathave already been provided by the ds.
therefore,a positive reward is assigned to the us if: 1) itprovides new information about slots; 2) it asksnew attributes about a certain entity, or 3) it repliescorrectly to a request from the ds.
otherwise apenalty is given..3.3 optimization.
we apply the policy gradient theorem (suttonet al., 2000) to the space of (user/system) dialogueacts.
in the tth dialogue turn, the reward rdst or rustis assigned to the two agents at ﬁnal last step oftheir generated act sequence.
the return for theaction at the ith step is r∗t , where ∗denotes ds or us, and |a∗| is the length of the actsequence of each agent.
γ ∈ [0, 1] is a discountingfactor.
the policy gradient of each turn can then bewritten as:.
i = γ|a∗|−ir∗.
∇θ∗j ∗(θ∗) =.
r∗.
i ∇θ∗ log p∗a,i.
(4).
|a∗|(cid:88).
i.where p∗a,i is the probability of the act token at theith step in the predicted dialogue act sequence.
thetwo agents are updated using eqn.
(4) at each turnwithin the entire simulated dialogue..3.2 turn-level reward.
4 experiments.
while the dialogue-level reward is straight-forward,it only considers the ﬁnal task success rate of the.
dataset themultiwoz(budzianowski et al., 2018).
2.0.datasetis used for all.
155experiments.
it contains 10.4k dialogues with anaverage of 13.6 turns.
each dialogue can span up tothree domains.
compared to previous benchmarkcorpora such as dstc2 (williams et al., 2016) orwoz2.0 (wen et al., 2017), multiwoz is morechallenging because 1) its rich ontology contains39 slots across 7 domains; 2) the ds can takemultiple actions in a single turn; 3) the complexdialogue ﬂow makes it difﬁcult to hand-craft arule-based ds or an agenda-based us.
lee et al.
(2019) provided the user act labels..training details the positive and negative rlrewards of sec.
3 are tuned in the range [-5, 5]based on the dev set.
the user goals employedfor interaction during rl are taken from the train-ing data without synthesizing new goals.
furthertraining details can be found in appendix a.1..evaluation metrics the proposed model is eval-uated in terms of the inform rate (info), the successrate (succ), and bleu.2 the inform rate measureswhether the ds provides the correct entity match-ing the user goal, while the success rate furtherrequires the system to answer all user questionscorrectly.
following (mehri et al., 2019), the com-bined performance (comb) is also reported, calcu-lated as 0.5 ∗ (info + succ) + bleu..4.1.interaction quality.
first, it is examined whether the proposed learn-ing framework improves the discourse between di-alogue system and user simulator.
several vari-ants of our model are examined: 1) two agents arepre-trained using supervised learning, serving asbaseline; 2) rl is used to ﬁne-tune only the ds(rl-ds) or both agents (rl-joint).
in each rlcase, we can either use rewards at the dialoguelevel (dial-r, sec.
3.1) or rewards at the turn-level(turn-r, sec.
3.2).
the two trained agents interactbased on 1k user goals from the test corpus, withthe generated dialogues being evaluated using themetrics above..from table 1, we can see that the applicationof rl in our framework improves the success rateby more than 10% (b-e vs. a).
this indicates thatthe ds learns through interaction with the learnedus, and the designed rewards, to be better at com-pleting the task successfully.
moreover, the joint.
2for a fair comparison to previously proposed models, thesame evaluation script provided by the multiwoz organizershttps://github.com/budzianowski/multiwozis used and the ofﬁcial data split for train/dev/test is followed..model(a) supervised learning(b) rl-ds w/ dial-r(c) rl-joint w/ dial-r(d) rl-ds w/ turn-r(e) rl-joint w/ turn-r.info69.7781.3882.8385.6286.49.succ58.0270.6771.5770.3473.04.table 1: quality for dialogues generated by two agentsin joust using the test corpus user goals.
bleu isnot reported since no reference sentences are availablefor these interactions..model.
hred-ts (peng et al., 2019)damd (zhang et al., 2019)simpletod∗(hosseini-asl et al., 2020)soloist∗ (peng et al., 2020)mintl-bart∗ (lin et al., 2020a)joust supervised learningjoust rl-joint w/ dial-rjoust rl-joint w/ turn-r.info70.076.384.485.584.977.480.683.2.succ bleu comb81.517.558.085.016.660.492.315.070.195.716.572.997.817.974.989.517.466.792.517.569.496.017.673.5.table 2: empirical comparison with state-of-the-art di-alogue systems using the predicted belief state.
∗ indi-cates leveraging of pre-trained transfomer-based mod-els..optimisation of both the us and the ds providesdialogues with higher success rate than only opti-mising the ds (c&e vs. b&d).
it shows that thebehaviour of the us is realistic enough and diverseenough to interact with the ds, and its behaviorcan be improved together during rl optimisation.
finally, by comparing two reward designs, the ﬁne-grained rewards at the turn level seem to be moreeffective towards guiding two agents’ interaction(b&c vs. d&e), which is reasonable since they re-ﬂect more than simple success rate in terms of thenature of the tasks.
some real, generated dialoguesthrough the interactions are provided in appendixa.6; we note that after rl, both agents respond torequests more correctly and also learn not to repeatthe same information, leading to a more successfuland smooth interaction without loops in the dia-logue.
the corresponding error analysis of each ofthe agents is provided later in sec.
4.4.1..4.2 benchmark results.
we conduct experiments on the ofﬁcial test setfor comparison to existing end-to-end dss.
thetrained ds is used to interact with the ﬁxed testcorpus following the same setup of budzianowskiet al.
(2018).
results are reported using a predictedbelief state (table 2) and using an oracle belief state(table 3).
in general, we can observe similar perfor-mance trends as in sec.
4.1 with rl optimization.
156modelsimpletod∗(hosseini-asl et al., 2020)mognet (pei et al., 2020)ardm∗ (wu et al., 2019)damd (zhang et al., 2019)soloist∗ (peng et al., 2020)parg (gao et al., 2020)marco∗ (wang et al., 2020)joust supervised learningjoust rl-joint w/ dial-rjoust rl-joint w/ turn-r.info88.985.387.489.289.691.192.388.593.994.7.succ bleu comb94.916.967.199.420.173.3100.720.672.8102.218.677.979.3102.518.3103.818.878.9105.520.078.6102.318.379.4106.716.985.7109.418.786.7.table 3: empirical comparison with state-of-the-art di-alogue systems using oracle belief state.
∗ indicatesleveraging of pre-trained transfomer-based models..of our model.
joint learning of two agents usingrl with the ﬁne-grained rewards reaches the bestcombined score and success rate.
this implies thatthe exploration of more dialogue states and actionsin the simulated interactions reinforces the behav-iors that lead to higher success rate, and that thesegeneralise well to unfamiliar states encountered inthe test corpus..our best rl model produces competitive resultsin table 2 when using predicted belief state, andcan further outperform the previous work in table 3when using oracle belief state.
note that we donot leverage the powerful pre-trained transformer-based models like soloist or mintl-bartmodel.
we found that with rl optimisation, ourlstm-based models can still perform competi-tively.
in terms of ds model structure, the mostsimilar work would be the damd model.
theperformance gain found in comparing "joust su-pervised learning" to damd is partially due tothe better performance of our dst model.3.
we also conduct experiments using only 50% ofthe training data for supervised learning to verifythe efﬁcacy of the proposed method under differ-ent amounts of data.
as shown in table 4, it isobserved that our method also improves the modelupon supervised learning when trained with lessdata and the improvements are consistent with thecomplete data scenario..4.3 transfer learning.
in this section, we demonstrate the capability oftransfer learning of the proposed framework un-der two low-resource setups: domain adaptationand single-to-multiple domain transfer.
two ﬁne-tuning methods are adopted: the straightforwardﬁne-tuning without any constraints (naive) and.
model.
info..succ.
bleu comb..supervised learningrl-joint w/ turn-r.belief state = predicted.
70.3774.83.
55.4360.60belief state = oracle.
17.2917.41.
80.1985.12.supervised learningrl-joint w/ turn-r.89.6794.27.
74.581.47.
16.9617.20.
99.04105.06.table 4: results of joust using 50% training data insupervised learning..elastic weight consolidation (ewc) (kirkpatricket al., 2017).
we show that the proposed rl canbe further applied to both methods and producessigniﬁcantly improved results.
here we experimentthe best rl variants using turn-level rewards (sameas (e) in table 1)..domain adaptation in these experiments, eachof ﬁve domains is selected as the target domain.
taking the hotel domain for example, 300 dia-logues4 involving the hotel domain are sampledfrom the training corpus as adaptation data.
therest of the dialogues, not involving the hotel do-main, form the source data.
both the ds and theus are ﬁrst trained on the source data (source),and then ﬁne-tuned on the limited data of the tar-get domain (naive, ewc).
afterwards, the pair ofagents is trained in interaction using the proposedrl training regime (+rl)..results in the form of the combined score aregiven in table 5 (corresponding success rates areprovided in appendix a.5).
as expected, mod-els pre-trained on source domains obtain low com-bined scores on target domains.
fine-tuning usingnaive or ewc method signiﬁcantly bootstraps thesystems, where the regularization in ewc beneﬁtsmore for the low-resource training.
by applyingour proposed framework to the two sets of ﬁne-tuned models, the performance can be further im-proved by 7-10% in averaged numbers, with bothpredicted and oracle belief states.
this indicatesthat through the interaction with the us, the ds isnot constrained by having seen only a very limitedamount of target domain data, and that it can learneffectively from the simulated dialogues using thesimple reward structure (the rl learning curve ispresented in sec.
4.4.3).
with a better initializa-tion points such as ewc models, the models canlearn from a higher quality interaction and producebetter results (ewc+rl vs naive+rl).
on aver-.
3in correspondence, the damd authors report a dst.
model with joint accuracy of ca.
35%, while ours is 45%..4for each domain, 300 dialogues accounts for 10% of alltarget-domain data.
refer to appendix a.2 for data statistics..157model.
restaurant hotel attraction train taxi.
avg..model.
h+t r+t a+t a+h+x h+r+x a+r+x avg..belief state = predicted.
belief state = predicted.
sourcenaiveewcnaive+rlewc+rl.
sourcenaiveewcnaive+rlewc+rl.
21.146.756.757.064.6.
33.285.684.197.697.5.belief state = oracle.
28.656.258.266.867.8.
40.184.285.199.2100.7.
25.266.171.672.575.8.
34.377.989.888.596.0.
59.668.569.372.371.6.
70.796.7101.7104.0104.9.
48.766.378.775.487.6.
55.493.497.5103.4106.3.
36.660.866.968.873.5.
46.787.591.698.5101.1.sourcenaiveewcnaive+rlewc+rl.
46.057.257.463.264.7.
55.469.272.174.477.6.
82.3source88.8naive95.5ewcnaive+rl99.7ewc+rl 100.2.
93.398.496.9104.3103.0.
34.365.066.168.467.6.
76.285.989.692.093.9.
22.040.343.747.446.6.
36.872.270.080.682.6.belief state = oracle.
26.636.039.042.743.2.
55.479.881.597.295.0.
19.942.845.048.748.5.
42.476.779.689.389.2.
34.051.753.957.558.0.
64.483.685.593.994.0.table 5: combined scores in domain adaptation.
300dialogues are used for each target domain adaptation..age, the ﬁnal performance obtained by ewc+rlmodel doubles that of source model, which demon-strates the efﬁcacy of the proposed method in do-main adaptation..single-to-multiple domain transfer anothertransfer learning scenario is investigated whereonly limited multi-domain data is accessible butsufﬁcient single-domain dialogues are available.
this setup is based on a practical fact that single-domain dialogues are often easier to collect thanmulti-domain ones.
all single-domain dialogues inthe training set form the source data.
for each tar-get multi-domain combination, 100 dialogues5 aresampled as adaptation data.
as before, the ds andthe us are ﬁrst pre-trained on the source data andthen ﬁne-tuned on the adaptation data.
afterwards,two agents improve themselves through interac-tion.
the models are tested using the multi-domaindialogues of the test corpus..results in the form of the combined score aregiven in table 6 (refer to appendix a.5 for successrates).
although the source models capture indi-vidual domains, they cannot manage the complexﬂow of multi-domain dialogues and hence producepoor combined scores, with worst results on com-binations of three domains.
fine-tuning improvesperformance signiﬁcantly, as the systems learn totransition between domains in the multi-domaindialogue ﬂow.
finally, applying our rl optimiza-tion further increases the performance by 6-9% onaverage.
this indicates that the dialogue agents canlearn more complicated policies through exploringmore dialogue states and actions while interactingwith user simulator.
we analyse the sources ofimprovements in the following section..5there are 6 types of domain combinations in multiwoz,as shown in table 6. for each multi-domain combination, 100dialogues accounts for 11% of its multi-domain data..table 6: combined scores in single-to-multiple domaintransfer where 100 dialogues on each target scenarioare used for adaptation.
r, h, a, t, x represent restau-rant, hotel, attraction, train, taxi domain..model.
dialogue systemmiss ent.
wrong ans..user simulatorrep. att.
miss ans..naivenaive+rl.
17.592.73.
36.999.54.
10.121.47.
47.2732.60.table 7: error analysis (%) of the us and the ds agentsaveraged over 5 adaptation domains.
lower is better..4.4 analysis.
4.4.1 error analysis.
we ﬁrst examine the behavior of the us and the dsto understand the improved success rate in trans-fer learning.
the models are those of table 5and are examined after ﬁne-tuning using naivemethod (naive) and then after reinforcement learn-ing (naive+rl).
for the ds, the rates of missingentities (miss ent.)
and of wrong answers (wrongans.)
are reported.
for the us, rates of repeti-tions of attributes (rep.
att.)
and of missing an-swers (miss ans.)
are reported.
the results shownin table 7 are averaged over the ﬁve adaptationdomains6.
we see that with rl optimisation theerrors made by the two agents are reduced signiﬁ-cantly.
notably, the user model learns not to repeatthe information already provided and attempts toanswer more of the questions from the dialogueagent.
these are the behaviors the reward structureof sec.
3.2 are intended to encourage, and they leadto more successful interactions in policy learning..4.4.2 exploration of states and actions.
we now investigate whether our framework encour-ages exploration through increased interaction intransfer learning.
we report the number of uniquebelief states in the training corpus and in the di-alogues generated during rl interaction, as wellas the unique action sequences per state that each.
6results for each domain can be found in appendix a.3..158domain adaptationstates6141425.actions3.346.22.single-to-multiplestates223399.actions3.6115.33.corpusinteract..win ratio (%)ds successus human-likedialogue flow.
sl26.029.521.0.rl74.070.579.0.table 8: number of unique dialogue states and averagedialogue actions per state in the training corpus and inthe rl interactions in two transfer learning setups..table 9: human assessment of the system quality undersupervised learning and reinforcement learning..the models using supervised learning (sl) and an-other is generated by the models after rl optimiza-tion.
note that here we are evaluating the perfor-mance gain during interactions between two agents(sec.
4.1), instead of the gain in benchmark resultsby interacting with the static corpus (sec.
4.2).
thisis why the baseline is our sl model instead of theexisting state-of-the-art systems..the assessor offers judgement regarding:• which dialogue system completes the task.
more successfully (ds success)?.
• which user simulator behaves more like a real.
human user (us human-like)?.
• which dialogue is more natural, ﬂuent and.
efﬁcient (dialogue flow)?.
the results with relative win ratio, shown in ta-ble 9, are consistent with the automatic evaluation.
with the proposed rl optimisation, the ds is moresuccessful in dialogue completion.
more impor-tantly, joint optimisation of the us is found to pro-duce more human-like behavior.
the improvementunder the two agents leads to a more natural andefﬁcient dialogue ﬂow..5 related work.
in the emerging ﬁeld of end-to-end dss, in whichall components of a system are trained jointly (liuand lane, 2017a; wen et al., 2017; lei et al., 2018).
rl methods have been used effectively to optimizeend-to-end dss in (dhingra et al., 2017; liu et al.,2017; zhao et al., 2019), although using rule-baseduss or a ﬁxed corpus for interaction.
recent worksutilise powerful transformers such as gpt-2 (penget al., 2020; hosseini-asl et al., 2020) or t5 (linet al., 2020b) for dialogue modeling and reach state-of-the-art performance; however, the area of havinga user simulator involved during training is unex-plored.
by comparison, this work uses a learnedus as the environment for rl.
the two agents wepropose are able to generate abundant high-qualitydialog examples and they can be extended easily tounseen domains.
by utilizing an interactive envi-.
figure 2: learning curves observed on the dev set dur-ing rl optimization.
two domain adaptation cases arepresented, with restaurant (left) and hotel (right) as tar-get domain respectively..agent predicts..as shown in table 8, the ds encounters morestates in interaction with the us and also takesmore unique actions in reinforcement learning rela-tive to what it sees in supervised learning.
in thisway the ds considers additional strategies duringthe simulated training dialogues, with the oppor-tunity to reach better performance even with onlylimited supervised data.
detailed results for eachadaptation case are provided in appendix a.4..4.4.3 rl learning curve.
here we show that the designed reward structureis indeed a useful objective for training.
figure 2shows learning curves of the model performanceand the received (turn-level) rewards during rltraining.
the two examples are from the domainadaptation experiments in sec.
4.3, where restau-rant (left) and hotel (right) are the target domain.
we can see that both the reward value and modelperformance are consistently improved during rl,and their high correlation veriﬁes the efﬁcacy of theproposed reward design for training task-orienteddialogue systems..4.5 human evaluation.
the human assessment of dialogue quality is per-formed to conﬁrm the improvements of the pro-posed methods.
400 dialogues, generated by thetwo trained agents, are evaluated by 14 human as-sessors.
each assessor is shown a comparison oftwo dialogues where one dialogue is generated by.
159we found in our experiments very important forlearning complex behaviors of user simulators.
rel-ative to these three publications, this paper focuseson joint training of two fully end-to-end agents thatare able to participate in complex multi-domaindialogues.
more importantly, it is shown that theproposed framework is highly effective for transferlearning, which is a novel contribution relative toprevious work..6 conclusion and future work.
we propose a novel joint learning framework oftraining both the ds and the us for complex multi-domain dialogues.
under the low-resource sce-narios, the two agents can generate more dialoguedata through interacting with each other and theirbehaviors can be signiﬁcantly improved using rlthrough this self-play strategy.
two types of rewardare investigated and the turn-level reward beneﬁtsmore due to its ﬁne-grained structure.
experimentsshows that our framework outperforms previouslypublished results on the multiwoz dataset.
in twotransfer learning setups, our method can further im-proves the well-performed ewc models and boot-straps the ﬁnal performance largely.
future workwill focus on improving the two agents’ underly-ing capability with the powerful transformer-basedmodels..acknowledgements.
bo-hsiang tseng is supported by cambridgetrust and the ministry of education, taiwan.
florian kreyssig is funded by an epsrc doc-toral training partnership award.
this work hasbeen performed using resources provided by thecambridge tier-2 system operated by the uni-versity of cambridge research computing ser-vice (http://www.hpc.cam.ac.uk) funded by epsrctier-2 capital grant ep/p020259/1..ronment instead of a ﬁxed corpus, more dialoguestrategies are explored and more dialogue states arevisited..there have been various approaches to buildinguss.
in the research literature of uss, one lineof research is rule-based simulation such as theagenda-based user simulator (abus) (schatzmannand young, 2009; li et al., 2016).
the abus’sstructure is such that it has to be re-designed for dif-ferent tasks, which presents challenges in shiftingto new scenarios.
another line of work is data-driven modelling.
el asri et al.
(2016) modelleduser simulation as a seq2seq task, where the out-put is a sequence of user dialogue acts the levelof semantics.
gur et al.
(2018) proposed a vari-ational hierarchical seq2seq framework to intro-duce more diversity in generating the user dia-logue act.
kreyssig et al.
(2018) introduced theneural user simulator (nus), a seq2seq modelthat learns the user behaviour entirely from a cor-pus, generates natural language instead of dialogueacts and possesses an explicit goal representation.
the nus outperformed the abus on several met-rics.
kreyssig (2018) also compared the nus andabus to a combination of the abus with an nlgcomponent.
however, none of these prior worksare suitable for modelling complex, multi-domaindialogues in an end-to-end fashion.
by contrast,the user model proposed here consumes and gener-ates text and so can be directly employed to interactwith the ds, communicating via natural language..the literature on joint optimization of the ds andthe us is line of research most relevant to our work.
takanobu et al.
(2020) proposed a hybrid value net-work using marl (lowe et al., 2017) with role-aware reward decomposition used in optimising thedialogue manager.
however, their model requiresseparate nlu/nlg models to interact via naturallanguage, which hinders its application in the trans-fer learning to new domains.
liu and lane (2017b);papangelis et al.
(2019) learn both the ds and theus in a (partially) end-to-end manner.
however,their systems are designed for the single-domaindataset (dstc2) and cannot handle the complexityof multi-domain dialogues: 1) their models canonly predict one dialogue act per turn, which isnot sophisticated enough for modelling multipleconcurrent dialogue acts; 2) the simple dst com-ponents cannot achieve satisfactory performance inthe multi-domain setup; 3) the user goal change isnot modelled along the dialogue proceeds, which.
160references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in 3rd internationalconference on learning representations, iclr..paweł budzianowski, tsung-hsien wen, bo-hsiangtseng, iñigo casanueva, stefan ultes, osman ra-madan, and milica gasic.
2018. multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 5016–5026..iñigo casanueva, paweł budzianowski, stefan ultes,florian kreyssig, bo-hsiang tseng, yen-chen wu,and milica gaši´c.
2018. feudal dialogue manage-ment with jointly learned feature extractors.
in pro-ceedings of the 19th annual sigdial meeting on dis-course and dialogue..bhuwan dhingra, lihong li, xiujun li, jianfeng gao,yun-nung chen, faisal ahmed, and li deng.
2017.towards end-to-end reinforcement learning of dia-logue agents for information access.
in proceedingsof the 55th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 484–495, vancouver, canada.
association forcomputational linguistics..layla el asri, jing he, and kaheer suleman.
2016. asequence-to-sequence model for user simulation inspoken dialogue systems.
in proceedings of the 17thannual conference of the international speech com-munication association, san francisco..layla el asri, romain laroche, and olivier pietquin.
2014. task completion transfer learning for rewardinference.
in workshops at the twenty-eighth aaaiconference on artiﬁcial intelligence..silin gao, yichi zhang, zhijian ou, and zhou yu.
2020.paraphrase augmented task-oriented dialog genera-tion.
acl..alex graves, santiago fernández, and jürgen schmid-huber.
2005. bidirectional lstm networks for im-proved phoneme classiﬁcation and recognition.
ininternational conference on artiﬁcial neural net-works, pages 799–804.
springer..izzeddin gur, dilek z. hakkani-tür, gökhan tür, andpararth shah.
2018. user modeling for task orienteddialogues.
2018 ieee spoken language technologyworkshop (slt), pages 900–906..matthew henderson, blaise thomson, and jason d.williams.
2014. the second dialog state trackingchallenge.
in proceedings of the 15th annual meet-ing of the special interest group on discourse anddialogue (sigdial), pages 263–272, philadelphia,pa, u.s.a. association for computational linguis-tics..ehsan hosseini-asl, bryan mccann, chien-sheng wu,semih yavuz, and richard socher.
2020. a simplelanguage model for task-oriented dialogue.
arxivpreprint arxiv:2005.00796..james kirkpatrick, razvan pascanu, neil rabinowitz,joel veness, guillaume desjardins, andrei a rusu,kieran milan, john quan, tiago ramalho, ag-nieszka grabska-barwinska, et al.
2017. over-coming catastrophic forgetting in neural networks.
proceedings of the national academy of sciences,114(13):3521–3526..florian kreyssig.
2018. deep learning for user simu-lation in a dialogue system.
master’s thesis, univer-sity of cambridge, june..iñigo.
florian kreyssig,.
casanueva,.
pawełbudzianowski, and milica gaši´c.
2018. neural usersimulation for corpus-based policy optimisationin proc.
sigdial,of spoken dialogue systems.
melbourne..sungjin lee, qi zhu, ryuichi takanobu, zheng zhang,yaoqin zhang, xiang li, jinchao li, baolin peng,xiujun li, minlie huang, and jianfeng gao.
2019.convlab: multi-domain end-to-end dialog systemplatform.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics: system demonstrations, pages 64–69, flo-rence, italy.
association for computational linguis-tics..wenqiang lei, xisen jin, min-yen kan, zhaochunren, xiangnan he, and dawei yin.
2018. sequicity:simplifying task-oriented dialogue systems with sin-gle sequence-to-sequence architectures.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1437–1447, melbourne, australia.
as-sociation for computational linguistics..xiujun li, yun-nung chen, lihong li, jianfeng gao,end-to-end task-and asli celikyilmaz.
2017.completion neural dialogue systems.
in proceedingsof the eighth international joint conference on nat-ural language processing (volume 1: long papers),pages 733–743, taipei, taiwan.
asian federation ofnatural language processing..xiujun li, zachary chase lipton, bhuwan dhingra,lihong li, jianfeng gao, and yun-nung chen.
2016. a user simulator for task-completion dia-logues.
arxiv, abs/1612.05688..zhaojiang lin, andrea madotto, genta indra winata,and pascale fung.
2020a.
mintl: minimalist trans-fer learning for task-oriented dialogue systems.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3391–3405, online.
association for computa-tional linguistics..zhaojiang lin, andrea madotto, genta indra winata,and pascale fung.
2020b.
mintl: minimalist trans-fer learning for task-oriented dialogue systems.
in.
161proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3391–3405..bing liu and ian lane.
2017a.
an end-to-end trainableneural network model with belief tracking for task-oriented dialog.
in interspeech 2017..bing liu and ian lane.
2017b.
iterative policy learningin end-to-end trainable task-oriented neural dialogmodels.
2017 ieee automatic speech recognitionand understanding workshop (asru), pages 482–489..bing liu, gokhan tur, dilek hakkani-tur, pararthshah, and larry heck.
2017. end-to-end optimiza-tion of task-oriented dialogue model with deep rein-forcement learning.
in nips workshop on conver-sational ai..ryan lowe, yi wu, aviv tamar, jean harb, ope-nai pieter abbeel, and igor mordatch.
2017. multi-agent actor-critic for mixed cooperative-competitiveenvironments.
in i. guyon, u. v. luxburg, s. ben-gio, h. wallach, r. fergus, s. vishwanathan, andr. garnett, editors, advances in neural informationprocessing systems 30, pages 6379–6390.
curranassociates, inc..shikib mehri, tejas srinivasan, and maxine eskenazi.
2019. structured fusion networks for dialog.
in pro-ceedings of the 20th annual sigdial meeting on dis-course and dialogue, pages 165–177, stockholm,sweden.
association for computational linguistics..alexandros papangelis, yi-chia wang, piero molino,and gokhan tur.
2019. collaborative multi-agent di-alogue model training via reinforcement learning.
inproceedings of the 20th annual sigdial meeting ondiscourse and dialogue, pages 92–102, stockholm,sweden.
association for computational linguistics..end-to-end dialogue systems using generative hierar-chical neural network models.
in proceedings of thethirtieth aaai conference on artiﬁcial intelligence,pages 3776–3783..pei-hao su, paweł budzianowski, stefan ultes, mil-ica gaši´c, and steve young.
2017. sample-efﬁcientactor-critic reinforcement learning with supervisedin proceedings ofdata for dialogue management.
the 18th annual sigdial meeting on discourse anddialogue, pages 147–157, saarbrücken, germany.
association for computational linguistics..richard s sutton, david a. mcallester, satinder p.singh, and yishay mansour.
2000. policy gradientmethods for reinforcement learning with function ap-proximation.
in nips..ryuichi takanobu, runze liang, and minlie huang.
2020. multi-agent task-oriented dialog policy learn-ing with role-aware reward decomposition.
in acl..kai wang, junfeng tian, rui wang, xiaojun quan,and jianxing yu.
2020. multi-domain dialoguearxiv preprintacts and response co-generation.
arxiv:2004.12363..tsung-hsien wen, david vandyke, nikola mrkši´c,milica gasic, lina m. rojas barahona, pei-hao su,stefan ultes, and steve young.
2017. a network-based end-to-end trainable task-oriented dialoguesystem.
in eacl, pages 438–449, valencia, spain.
association for computational linguistics..jason d. williams, antoine raux, and matthew hen-derson.
2016. the dialog state tracking challengeseries: a review.
dialogue discourse, 7:4–33..qingyang wu, yichi zhang, yu li, and zhou yu.
2019. alternating recurrent dialog model with large-scale pre-trained language models.
arxiv preprintarxiv:1910.03756..jiahuan pei, pengjie ren, christof monz, and maartenretrospective and prospectivede rijke.
2020.mixture-of-generators for task-oriented dialogue re-sponse generation.
ecai..s. young, m. gaši´c, b. thomson, and j. d. williams.
2013. pomdp-based statistical spoken dialog sys-the ieee,tems: a review.
101(5):1160–1179..proceedings of.
baolin peng, chunyuan li, jinchao li, shahin shayan-deh, lars liden, and jianfeng gao.
2020. soloist:task-oriented dialog with a single pre-few-shotarxiv preprinttrained auto-regressive model.
arxiv:2005.05298..shuke peng, xinjing huang, zehao lin, feng ji,haiqing chen, and yin zhang.
2019.teacher-student framework enhanced multi-domain dialoguegeneration.
arxiv preprint arxiv:1908.07137..jost schatzmann and steve j. young.
2009. the hid-den agenda user simulation model.
ieee transac-tions on audio, speech, and language processing,17:733–747..iulian v serban, alessandro sordoni, yoshua bengio,aaron courville, and joelle pineau.
2016. building.
yichi zhang, zhijian ou, and zhou yu.
2019. task-oriented dialog systems that consider multiple appro-priate responses under the same context.
aaai..tiancheng zhao, kaige xie, and maxine eskenazi.
2019. rethinking action spaces for reinforcementlearning in end-to-end dialog agents with latent vari-in proceedings of the 2019 confer-able models.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1208–1218..162slrl.
slrl.
slrl.
slrl.
a.3 error analysis.
the error analysis of each domain adaptation casesare provided in tables 13 and 14..model restaurant hotel attraction train taxi.
average.
14.841.45.
44.395.64.
29.087.83.missing provision rate (%)24.742.33missing answer rate (%)61.5220.04.
32.3310.35.
8.610.41.
11.952.58.
10.681.62.
34.789.09.
17.592.73.
36.999.54.table 13: error analysis on dialogue system on eachdomain in terms of two behaviors.
lower the better..model restaurant hotel attraction train taxi.
average.
8.451.84.
27.8424.44.
16.810.70.repeat inform rate (%)9.151.28missing answer rate (%)56.4140.74.
64.6822.95.
9.691.47.
6.482.05.
35.4126.44.
52.0048.42.
10.121.47.
47.2732.60.table 14: error analysis on user simulator on each do-main in terms of two behaviors.
lower the better..a.4 exploration.
the detailed numbers of explored dialogue statesand the average of unique dialogue actions per statein each case of two transfer learning scenarios areprovided in tables 15 and 16..model restaurant hotel attraction train taxi averagenumber of dialogue states.
slrl.
slrl.
5141458.
3.514.92.average of dialogue actions per state.
7261666.
3.105.49.
5451087.
3.747.61.
513601.
3.707.98.
7742313.
2.655.10.
6141425.
3.346.22.table 15: number of dialogue states and average ofdialogue actions per state in each domain adaptationcase..slrl.
model h+t r+t a+t a+h+x h+r+x a+r+x averagenumber of dialogue states352263636348average of dialogue actions2.433.0011.3021.03.
5.4017.90.
3.9513.56.
4.0521.20.
3.6115.33.
2.846.98.
250523.
184294.
223399.
172383.
118208.slrl.
table 16: number of dialogue states and average of di-alogue actions per state in each single-to-multi domaincase..a appendices.
a.1 training details.
both the ds and the us are trained in an end-to-endfashion using the adam optimizer.
the sizes of theembedding and of the hidden layers are set to 300.during supervised training, the batch size is 100and the learning rate is 0.001, while during rl, 10is used as the batch size and 0.0001 as the learningrate for stability.
we set the discounting factor γ to1. the computing infrastructure used is linux 4.4.0-138-generic x86_64 with the nvidia gpu gtx-1080. average run time per model using 100%training data is around 6 hours.
model parametersis around 11m in total..the turn-level rewards used for the best modelsin benchmark results are reported in table 10 below.
all rewards are tuned based on the combined scoreof the validation performance averaged over threeseeds.
as for dialogue-level rewards, a positivereward 1.0 will be given if a dialogue is successful..model.
rl-dsrl-joint.
rewards on dsransrpro2.5, -50, -52.5, -50, -5.rreq0, -10, -1.rewards on usransrinfrreq0, 00, 00, 01, -11, -10, -1.table 10: the conﬁguration of turn-level rewards foreach best model in the reported benchmark results.
each reward has positive and negative values..a.2 details of dataset.
as noted in the paper, we follow the original splitof the multiwoz dataset and the number of dia-logues for train/dev/test split is 8420/1000/1000.
data statistics of the number of dialogues in thetwo transfer learning scenarios are provided in ta-bles 11 and 12..data restaurant hotel attraction train taxi300train206dev195test.
300400396.
300415394.
300438437.
300484495.table 11: number of dialogues of the splits in eachdomain adaptation.
data h+t r+t a+t a+h+x h+r+x a+r+x100train131dev129test.
10011092.
100148163.
100157155.
10010091.
100149144.table 12: number of dialogues of the splits in eachscenario in single-to-multi domain transfer learning..163a.5 transfer learning.
here we provide the results in success rate in twotransfer learning setups..model.
restaurant hotel attraction train taxi avg..belief state = predicted.
sourcenaiveewcnaive+rlewc+rl.
sourcenaiveewcnaive+rlewc+rl.
5.026.435.936.842.3.
11.860.759.373.173.3.belief state = oracle.
10.935.837.846.047.7.
18.662.162.476.279.3.
5.441.047.646.251.9.
9.146.864.558.570.5.
36.248.047.749.848.5.
45.373.979.582.682.8.
0.035.055.241.463.9.
0.067.274.581.484.6.
11.537.244.944.150.8.
17.062.168.074.478.1.table 17: success rate in domain adaptation.
300 dia-logues are used for each target domain adaptation..model.
h+t r+t a+t a+h+x h+r+x a+r+x avg..belief state = predicted.
sourcenaiveewcnaive+rlewc+rl.
sourcenaiveewcnaive+rlewc+rl.
30.840.741.047.245.8.
60.467.174.579.679.2.
40.748.850.553.857.9.
74.676.678.183.484.5.
15.137.842.342.944.6.
41.354.460.360.366.1.
5.116.319.618.120.7.
13.448.243.857.355.4.belief state = oracle.
8.815.817.222.020.2.
28.253.957.968.170.0.
4.919.920.424.024.6.
19.946.350.460.261.5.
17.629.931.834.735.6.
39.657.760.868.269.4.table 18: success rate in single-to-multiple domaintransfer where 100 dialogues on each target scenarioare used for adaptation.
r, h, a, t, x represent restau-rant, hotel, attraction, train, taxi domain..a.6 generated dialogue examples.
here we provides two dialogue pairs examples thatare generated by the two agents’ interaction usingsupervised learning (sl) and using rl respectivelyin the following pages.
as seen in the table 19 ﬁrstdialogue, since the sl user model answers wrongto the system’s request, the system keeps askingthe unsolved question, leading to an unsuccessfuldialogue with loop (highlighted in color).
on theother hand, the interaction between rl models(second dialogue) is much smoother and efﬁcientsince both agents answer correctly to each other.
this indicates the designed rewards are useful toimprove the agents’ behaviors.
similar trend can beobserved in table 20, where the user model keepsstating the same information about restaurant..164user goaltrain: destination: cambridge; day: wednesday; arriveby: 18:15;departure: kings lynn; request=[trainid, price, leaveat]attraction: name: club salsa; request=[address]dialogue generated by two agents trained using sl(dialogue has a loop and fails as the provided train is incorrect due to the wrong departure).
00-usr: i am looking for a train that leaves on wednesday and arrives at cambridge .
01-sys: there are few trains that meet your criteria .
what time would you like to leave ?
02-usr: i would like to arrive by 18:15 .
03-sys: there are few trains leaving london kings cross to cambridge on wednesday ..what time would you like to leave ?.
04-usr: i would like to arrive by 18:15 .
05-sys: there are few trains leaving london kings cross to cambridge ..is there a particular time you would like to leave ?.
06-usr: i would like to arrive by 18:15 .
07-sys: there are few trains leaving london kings cross to cambridge ..is there a particular time you would like to leave ?.
08-usr: i would like to arrive by 18:15 .
09-sys: there are few trains leaving london kings cross to cambridge ..is there a particular time you would like to leave ?.
10-usr: i would like to arrive by 18:15 .
11-sys: there are few trains leaving london kings cross to cambridge .
what time would you like to leave ?
12-usr: i would like to arrive by 18:15 .
13-sys: there are few trains leaving london kings cross to cambridge .
what time would you like to leave ?
14-usr: i would like to arrive by 18:15 .
15-sys: there are few trains leaving london kings cross to cambridge .
what time would you like to leave ?
16-usr: i would like to arrive by 18:15 .
17-sys: there are few trains leaving london kings cross to cambridge .
what time would you like to leave ?
18-usr: i would like to arrive by 18:15 .
19-sys: tr0269 arrives at 18:15 .
would you like me to book it for you ?
20-usr: yes , please .
i need the price and travel time .
21-sys: the train id is tr0269 , the price is 10 and the travel time is 50 .
22-usr: i am also looking for a particular attraction called club salsa .
23-sys: club salsa is a nightclub in the centre .
the address is 1 station road .
entrance is free ..is there anything else i can help you with ?
24-usr: that is all i need .
thank you for your help .
25-sys: thank you for using our system !.
dialogue generated by two agents trained using rl(dialogue is smooth and successful).
00-usr: i am looking for a train from kings lynn on wednesday .
01-sys: what is your destination ?
02-usr: i need to arrive in cambridge by 18:15 .
03-sys: i have train tr0415 that leaves at 17:11 and arrives at 17:58 .
would you like me to book it for you ?
04-usr: yes , please book 2 tickets .
05-sys: booking was successful , the total fee is 9.80 pounds payable at the station .
reference number is fztwszhh .
is there anything else i can help you with ?.
06-usr: yes , i am also looking for a particular attraction called club salsa .
07-sys: club salsa is located at 1 station road , postcode cb12jb .
is there anything else i can help you with ?
08-usr: no , that s all i need today .
thank you .
09-sys: thank you for using our service .
have a great day !.
table 19: example 1: two dialogues generated by sl system and rl system respectively.
the dialogue loop withrepeated information is highlighted.
output utterances are lexicalised with corresponding slot values (belief statefor system; user goal for user).
if the system generates attributes that has not existing in the belief state, the valuesare randomly sampled from the ontology, e.g., the train departure "london kings cross" in the ﬁrst dialogue..165user goalrestaurant: pricerange=expensive; area=west; request=[phone, postcode]train: destination: cambridge; day: friday; arriveby: 21:30;departure: norwich; request=[price, leaveat]dialogue generated by two agents trained using sl(dialogue has a loop and fails as the involved restaurant is cheap instead of expensive).
00-usr: i am looking for a expensive restaurant in the west .
01-sys: there are few restaurant -s that meet your criteria .
what type of food would you like ?
02-usr: i would like to book a table for 1 at 1 .
can i get the phone number for thanh binh ?
03-sys: thanh binh phone number is 01223362456 .
04-usr: i am also looking for a restaurant called thanh binh .
05-sys: thanh binh is located at 17 magdalene street city centre , phone number is 01223362456 .
06-usr: i am also looking for a restaurant called thanh binh .
07-sys: thanh binh is located at 17 magdalene street city centre , phone number is 01223362456 .
08-usr: i am also looking for a restaurant called thanh binh .
09-sys: thanh binh is located at 17 magdalene street city centre , phone number is 01223362456 .
10-usr: i am also looking for a restaurant called thanh binh .
11-sys: thanh binh is located at 17 magdalene street city centre , phone number is 01223362456 .
12-usr: thank you .
i am also looking for a restaurant called thanh binh .
13-sys: thanh binh is located at 17 magdalene street city centre , phone number is 01223362456 .
14-usr: can you give me the postcode for thanh binh ?
15-sys: the postcode is cb21ab .
is there anything else i can help you with ?
16-usr: i am also looking for a train from norwich to cambridge on friday .
17-sys: there are 1 trains that ﬁt your criteria .
what time would you like to leave or arrive by ?
18-usr: i need to arrive by 21:30 .
19-sys: none arrives at 21:30 .
would you like me to book it for you ?
20-usr: no , but can you give me the departure time and departure time ?
21-sys: the departure time is 05:16 .
22-usr: thank you so much .
that s all i need today .
23-sys: thank you for using our service .
have a great day !.
dialogue generated by two agents trained using rl(dialogue is smooth and successful).
00-usr: i am looking for a expensive restaurant in the west .
01-sys: tandoori palace is a expensive indian restaurant in the west .
02-usr: what is the phone number and postcode ?
03-sys: the phone number is 01223506055 and the postcode is cb43le .
04-usr: thank you .
i also need a train from norwich to cambridge on friday .
05-sys: i can help you with that .
what time would you like to leave or arrive by ?
06-usr: i need to arrive by 21:30 .
07-sys: tr8009 arrives at 21:30 .
08-usr: what is the travel time and price ?
09-sys: the travel time is 79 minutes and the price is 17.60 pounds .
10-usr: what is the departure time ?
11-sys: the train leaves at 18:16 .
12-usr: what is the train id ?
13-sys: the train id is tr8009 .
14-usr: thank you very much .
15-sys: thank you , goodbye ..table 20: example 2: two dialogues generated by sl system and rl system respectively.
the dialogue loop withrepeated information is highlighted.
output utterances are lexicalised with corresponding slot values (belief statefor system; user goal for user).
if the user generates attributes beyond the given user goal, the values are randomlysampled from the ontology, e.g., the restaurant name "thanh binh" in the ﬁrst dialogue..166