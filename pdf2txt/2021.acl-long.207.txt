multi-view cross-lingual structured prediction withminimum supervisionzechuan hu(cid:5)‡, yong jiang†∗, nguyen bach†, tao wang†, zhongqiang huang†,fei huang†, kewei tu(cid:5)∗(cid:5)school of information science and technology, shanghaitech university(cid:5)shanghai engineering research center of intelligent vision and imaging(cid:5)shanghai institute of microsystem and information technology, chinese academy of sciences(cid:5)university of chinese academy of sciences†damo academy, alibaba group.
@shanghaitech.edu.cn, yongjiang.jy@alibaba-inc.com.
huzch,tukw}{abstract.
in structured prediction problems,cross-lingual transfer learning is an efﬁcient wayto train quality models for low-resource lan-guages, and further improvement can be ob-tained by learning from multiple source lan-guages.
however, not all source models arecreated equal and some may hurt performanceon the target language.
previous work has ex-plored the similarity between source and tar-get sentences as an approximate measure ofstrength for different source models.
in thispaper, we propose a multi-view framework,by leveraging a small number of labeled tar-get sentences, to effectively combine multi-ple source models into an aggregated sourceview at different granularity levels (language,sentence, or sub-structure), and transfer it toa target view based on a task-speciﬁc model.
by encouraging the two views to interact witheach other, our framework can dynamically ad-just the conﬁdence level of each source modeland improve the performance of both viewsduring training.
experiments for three struc-tured prediction tasks on sixteen data sets showthat our framework achieves signiﬁcant im-provement over all existing approaches,in-cluding these with access to additional sourcelanguage data..1.introduction.
structured prediction is the task of mapping inputsentences to structured outputs.
it is a fundamentaltask in natural language processing and has manyapplications, i.e., sequence labeling (derose, 1988;lample et al., 2016; ma and hovy, 2016; hu et al.,2020b), dependency parsing (chen and manning,2014; dozat and manning, 2016; ahmad et al.,2019) and semantic role labeling (van der plas et al.,2011; strubell et al., 2018; cai and lapata, 2020)..∗ corresponding authors.
‡work was done when zechuan.
hu was interning at alibaba damo academy..to achieve strong performance, structured predic-tion models mostly require manually labeled datathat are costly to obtain in general..cross-lingual transfer learning (yarowsky andngai, 2001; wang and manning, 2014; guo et al.,2018; lin et al., 2019; hu et al., 2021) recentlyattracted attention for tackling that problem, bytransferring the knowledge from high-resource lan-guages to low-resource ones.
existing works can becategorized into two types: single-source transferand multi-source transfer.
the former is limited totransferring knowledge from one source languageand generally results in inferior performance thanthe latter (mcdonald et al., 2011; rahimi et al.,2019), especially when the target language is simi-lar to multiple source language over various char-acteristics, i.e., domain, word order, capitalization,and script style.
however, in practice, we are morelikely to encounter the situation where some sourcelanguages are not as similar to the target languageand may lead to worse performance (rosensteinet al., 2005; rahimi et al., 2019) (we provide anexample in the appendix a).
to tackle this chal-lenging problem, most of the previous works domajority voting (plank and agi´c, 2018) and truthinference on hard predictions of multiple sources(rahimi et al., 2019).
to better incorporate tar-get language information, some recent works traina new model on the target unlabeled data withhard/soft predictions from multiple source mod-els, such as mixture-of-experts model (chen et al.,2019) and knowledge distillation (kd) (wu et al.,2020), and assign weights to multiple sources basedon language similarity.
however, these similarity-based approaches are heuristic-based, and cannotwell learn the conﬁdence level of multiple sourcemodels..in this paper, we propose to leverage a smallnumber of labeled target data to selectively trans-fer the knowledge from multiple source models..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2661–2674august1–6,2021.©2021associationforcomputationallinguistics2661in many real applications, we are generally easyto obtain a small number of target labeled data.
these small amounts of data can reﬂect the diversestrength and weakness of different source models.
concretely, the (small-size) labeled data can be uti-lized to learn the aggregation strategy of multiplesource models or train a new task-speciﬁc modelin the target language.
both the aggregation modeland target task-speciﬁc model can map the inputsto the structured outputs but there exists a trade-off.
the aggregation model generally has strongcross-lingual ability since source models are ﬁrstlywell trained1, but has lower ﬂexibility since sourcemodels are usually frozen.
instead, the target task-speciﬁc model tends to be more ﬂexible and hasstrong capacity but has poor performance since themodel is easily over-ﬁtted on the small trainingsample..inspired by previous work on multi/cross-viewlearning (clark et al., 2018; jiang et al., 2019;fei and li, 2020), we regard the aggregationmodel (aggregated source view) and the target task-speciﬁc model (target view) as two views sincethey both can map the input sentence to structuredoutputs.
we propose a novel multi-view frame-work to achieve a good trade-off between the twoviews.
to capture the diverse strength and weak-ness of multiple source models, we propose threeapproaches to obtain the aggregated source viewfrom language/sentence/sub-structure level in acoarse-to-ﬁne manner.
by encouraging two viewsto inﬂuence each other, the proposed frameworkcan dynamically learn the conﬁdence level of mul-tiple source models in three coarse-to-ﬁne granu-larity and make the best use of the small number oflabeled data, and make both views improved duringtraining.
beneﬁted from the multi-view framework,our proposed approaches can leverage plenty oftarget unlabeled data to capture the useful targetlanguage information (wu et al., 2020).
the contributions of this work are:.
1. we propose to leverage a small number oftarget labeled data to better aggregate multiplesource models..2. our approach contains three novel coarse-to-ﬁne approaches to aggregate multiple sourcemodels (section 2.2)..1following wu et al.
(2020), source models are previouslytrained on their corresponding labeled training set and frozenduring training..3. we propose a novel multi-view learning.
framework (section 2.3)..4. by utilizing both the label & unlabeled dataset,our approach improves two views simultane-ously (section 2.4)..we extensively experiment on three structuredprediction tasks, which are named entity recog-nition (ner), part-of-speech tagging (pos), anddependency parsing.
our proposed approaches out-perform several state-of-the-art approaches..2 methodology.
the left part of figure 1 depicts the proposedgeneral framework.
our framework contains twoviews, a target view which is a target structuredpredictor, and an aggregated source view basedon multiple pre-trained source models.
both viewscan map the input sentences to the structured out-puts and have diverse statistical properties, and thuscan provide complementary information to eachother (learned by the consensus component)..2.1 the target view.
in the general framework, the target view is a task-speciﬁc model.
we leverage the multilingual bert(mbert) (devlin et al., 2019) as the sentence en-coder.
we feed the input sentence x to the mbertand obtain the contextual internal states h, whichare utilized by a task-speciﬁc module to produce astructured output y. speciﬁcally, we use a softmaxlayer for sequence labeling tasks and a biafﬁne at-tention mechanism (dozat and manning, 2016) fol-lowed by (wu and dredze, 2019a) for graph-basedtasks like dependency parsing.
the conditionalprobability of the structured output given the inputsequence is computed by,.
p(y.x) =.
(cid:80).
|.
exp((cid:80)y(cid:48) exp((cid:80).
u∈y s(h, u)).
u∈y(cid:48) s(h, u)).
where y(cid:48) is the candidate structured outputs, y isthe structured outputs and u is the sub-structureof y. sub-structure is the label of each token forsequence labeling and dependency head for depen-dency parsing.
during training with gold labels, thesequence labeling objective function is the crossentropy between the gold labels and the model’s.
2662figure 1: the proposed multi-view framework with k source models (k = 4 in this case).
the parts with graybackground are the aggregation modules of three levels.
left: the multi-view framework with substructure-levelaggregation as described in section 2.2 and 2.2.3. an input sentence is passed through k source models (s) andone target model (t).
k output probabilities from source models are aggregated by a trainable weighting factorsce is the cross-entropy loss term on bothα (vector for model/sentence level and matrix for sub-structure level).
two views only for labeled data, andkl is the kl-divergence loss between two views for both labeled dataand unlabeled data, as described in section 2.1, 2.3, and 2.4. right: the sentence-level aggregation (above) asdescribed in section 2.2.3 and the language-level aggregation as described in section 2.2.1 ..l.l.soft predictions 2,.ce =.
l.−.
log p(y∗.
x) =|.
−.
log p(y∗i |.
x).
n(cid:88).
i=1.
ce(label)..l.where y∗ is the gold label sequence.
in dependencyparsing, we use the biafﬁne parser (dozat and man-ning, 2016) which is one of the state-of-the-artparsers.
following wu and dredze (2019a), wereplace the bilstm encoder with mbert.
similarto sequence labeling, the biafﬁne parser models thedependency head separately for each token.
fol-lowing anderson and g´omez-rodr´ıguez (2020),it has two independent distributions, one for headprediction and one for label prediction.
the cross-entropy loss for dependency head is,.
ce(head) =.
log p(t∗.
l.−.
x) =|.
−.
log p(h∗i |.
x).
n(cid:88).
i=1.
where h∗i is the gold head for i-th word of the goldtree t∗.
together with the similar cross-entropy.
loss of predicted edge labels, the dependency pars-ce(head) +ing objective function is.
ce =.
l.l.2.2 the aggregated source view.
in this section, we take the sequence labelingtasks as an example to introduce our aggregatedsource view.
the source models have the samemodel structure as the task-speciﬁc model of thetarget view in section 2.1. as presented in ﬁg-ure 1, for a k-source setup, we have k pre-trained source models sk, kandthe target structured model t. given a sentencex =, where x0 represents the [cls]token, we feed it to these models and get the in-h(1), .
.
.
, h(k)and the probabilityternal states{(k)(1)distributionsover the structureds , .
.
.
, pps{output of k source models sk, and h(t) and pt ofthe target model.
to aggregate all source models,we propose three novel coarse-to-ﬁne approaches..x0, .
.
.
, xn.
1, .
.
.
, k.∈ {.}
}.
}.
}.
{.
2.2.1 language-level aggregation.
2this is a common way in the bert-ﬁntuning setup (wu.
and dredze, 2019a; wu et al., 2020)..we simply introduce a trainable probability vectorαlang, which is depicted on the bottom right part of.
2663<latexit sha1_base64="rlrvjq5zwcsnxvi/wxbome+ien0=">aaacz3icjvhltsjafd3uf+ildemmkzi4isvgdel04xkigamgacsae/pko9uqgnhrd7jvvzl+gf6fd8yhuynradqeofeem3pvdskpj8kyxjpg3pzc4lj2obeyura+kd/cairhgrus7ozegf86dsi8hrc64mjjl1hmbn/xwnmznsp484bfcq+dczgkwme3+whvcdcwrlxbvi0gtm98prkqx+clvtfsy5wfjq0k0ksa5l/qrhchxktwwrbaepzgi6gnhrisrmr1mcyujsrvnggchgltymkuyrm7pg+fdi3nbrsxnolsu3skr29mshn7pakplyystznvpfxokv3ne6w85d1g9he0l0+swidyv3ttzp/qzc0cpryrgjjvfclgvudql1r1rd7c/fkviieioim7fi8ju0o57bopnimqxfbwvve3lslzuxd1bop3eusaconnogdb46byoixatxkhcqjhncuodrfp8zxcbweook7eer7xhgejztwad8b9z6qr0zptffvgwwcgjpqi</latexit>s4<latexit sha1_base64="pnwjobnppgce/rrcs1lzrx9ckg8=">aaac0hicjvhlssnafd2nr1pfvzdugkvwvrif6lloxmv99aftkuk6bupzmpmiprrx6w+41a8s/0d/wjvjfnqioihjmxpvotp3xjvy3iqbxmtgm5mdm1/iluawlldw1/lrg9uktgohvzzqc+o6bsxmcwnw4s73wd2kmexbhqvzg1mrr92wohhd4iopi9byrv7gdl3h4ks1mr7f+3z3ddlu7+fa+yjrnotsp4gpqafqlcp8c5roiisdfd4yandchiwk9drgwkbexasj4mjcrowzjjejbupzjdisygf07dguodia9sizkwqhtvhojumpy4c0iexfhmvpuoyn0lmwv3mppke425d+tvlyiexoe/uxbpl5x52ohaoly1mdszvfkhhvocollv0rn9e/vmxjisjo4a7fy8kove76retnimsxvbvk/e1mclbshzwb4l3ckgzs/hznnkjufc3donf+ucidqffnsyvt7ni8j1dcgcqokpc1hvgez+1cu9xutpvpvc2jnjv4trshd2lqldy=</latexit>s3mbert<latexit sha1_base64="qlqi4xlct23vet6kfflwq5kyg1q=">aaac13icjvhlssnafd2nr1pfss7dbitqnyxxgs6lblxwsa9pa0nsarvmi2qilllcivt/wk3+kfgh+hfegvnqi+iejgfovefm3hut0hviruuvgwvmdm5+ibuyw1pewv1t1/o1oegim1xtwa2ihmxgzhv8vuuod1kjjjjpws6rw1cnil6/zlhsbp45h4as7zl93+k5tsmj6qj5lmfygdubdcadvctrke+mo2pbl+lyadpaseeb6aoe6gta6ckajqqeghxwwi5mxpq0yubhsfwbi+iiqo6mm4yri21cwywytgkv6nunxtnlfdolz1iqbtrfptcipyzt0gsufxewp2kynkhnwf7mpzke4m5d+lupl0csx4dyv3stzp/qrc0cprzjghyqkzsmqm5oxrlzfxfz7utvnbxc4gtuujwibevlpm+a1msydtfbu8bfzkzgxd5ocxo8i1vsgi2f45wgtd2scvdsz/yl5en01flsygtfmuchyjhfbvxyvsejnvcsxci3yp1y/5mqzflnbr4t5eed6suwvw==</latexit>h(t)3<latexit sha1_base64="iwxhyizx2iangtfqjdtpqcqaosk=">aaac13icjvhlssnafd2nr1pfss7dbitqnyurrzdfny4r2ie0tstpta3ni2qilllcivt/wk3+kfgh+hfegvnqi+iejgfovefm3hut0hviruuvgwvufmfxkbucw1ldw99qn/o1oegim1xtwa2ihmxgzhv8vuuod1kjjjjpws6rw8ntea9fsyh2av+cj0lw9sy+7/qc2+reddr8yzp5woqnb5ooctuu8r1jry3ojv0ubryyksggxzvafuelxqswkcadgw9o2iwjmj4mdogiiwtjtfxeyjfxhglype0oi1ggseyqvn3anvpwp73wjkxaplnceinsatgltub5ewfxmibjixqw7g/ey+kp7jaiv5v6ecrydij9szfn/k9o1mlrw7gswagaqsmi6uzujzfdetfxvltfysektuauxspctlro+6xjtsxrf701zfxnzgpw7o00n8g7ucun2pg5zllq2y8zhyx9/kbqpklhncu2dlcker6hjdnuucxvgzzicc/kpxkr3cn3n6lkjtvs4dtshj4a5f+wvq==</latexit>h(t)1<latexit sha1_base64="luqoqwqcadxkr56qgkizh+9wun4=">aaac13icjvhlssnafd2nr/qodekmwis6kulrdfl047kcfuhbs5jo29c8sczikcwdupuh3oofix+gf+gdmqw1ie5icubce87mvdckxsfmuv6auebmfxaxsssrq2vrg5vqvq4wb0lks6oduehusmyyuy7pqtzhlmueetm9y2v1a3gq4vvrfsvo4f/wucjantn3nz5jm5yojppresyfwl3xynipxy0lfh/sufn6uzdlmwvgcvjivyvqx9bcfwfsjpda4imtdmeipqcjazpc4toyexcrcmscyyiv0iauxsjdjhzi3z7tminr0154xljt0ykuvrepneyrjqc8ila4tzpxrdol9jfvsfqudxvr30q9pgi5bst+pztm/lcnauho4vjw4fbnowredxbqksiuijtrx6ri5basj3cx4hfhwyqnfdakjpa1i96amv4mmwur9naam+bd3jigbpwc5yyolyrgyve/p8ixt9jrz7gdxrronkco4wwvvmn7bo94wrnyqdwqd8r9z6qsstxb+lauhw/ozza+</latexit>h(t)2<latexit sha1_base64="phpkyg8hjw3r9fgpq82ooetu3nu=">aaac13icjvhlssnafd2nr1pfss7dbitqnyxxgs6lblxwsa9pa0nsarvmi2qilllcivt/wk3+kfgh+hfegvnqi+iejgfovefm3hut0hviruuvgwvmdm5+ibuyw1pewv1t1/o1oegim1xtwa2ihmxgzhv8vuuod1kjjjjpws6rw1cnil6/zlhsbp45h4as7zl93+k5tsmj6qj5lmfygdubdcadvctr0dgzd9scxtll0qabkyic0luj1be00euagwk8mpjghf2yiolpwocoklg2rsrfhbwzzxgjr9qeshhlmmre0bdpu2bk+rqxnrfu23sks29esg3bpakolyisttnkpjhogv3neyq9xd2g9ldsl49yjggxf+kmmf/vivo4ejisnthuuygzuz2duisyk+lm2peqodmexancpxhe2jbksz81qyll7ak3poy/yuzbir2d5iz4f7ekars/xzknarsl46ckn+0xysfpqlpyxbaknm9dlhgkcqrkfynhpofzuvbultvl/jnvyasadxxbysmhsywwfa==</latexit>h(1)3<latexit sha1_base64="1a8461c+6ajqu/pdwavswgeiclu=">aaac13icjvhlssnafd2nr/qodekmwis6kulrdfl047kcfuhbs5jo29c8sczikcwdupuh3oofix+gf+gdmqw1ie5icubce87mvdckxsfmuv6auebmfxaxsssrq2vrg5vqvq4wb0lks6oduehusmyyuy7pqtzhlmueetm9y2v1a3gq4vvrfsvo4f/wucjantn3nz5jm5yojppresyfwl3xynipxy0lxv6ko+b1oi6xngumfosrrkqgvqcflglysocbwqcn7mjete8tbnsexluxji4i5mg4wwqrpe0oi1ggseyqvn3anvpwp73wjkxaplnceinsatgjtub5ewfxmibjixqw7g/ey+kp7jaiv5v6ecrydij9szfn/k9o1mlrw7gswagaqsmi6uzujzfdetfxvltfysektuauxspctlro+6xjtsxrf701zfxnzgpw7o00n8g7ucun2pg5zllqkxwnw6j+fpavn6sjzmihuyjqpi9qxhkqqjl3dr7xhgflurlv7pt7z1qlk2q28w0pdx9iv5z7</latexit>h(1)2<latexit sha1_base64="rp/jjgeonz+zhtejk025rlivmme=">aaac13icjvhlssnafd2nr1pfss7dbitqnyurrzdfny4r2ie0tstpta3ni2qilllcivt/wk3+kfgh+hfegvnqi+iejgfovefm3hut0hviruuvgwvufmfxkbucw1ldw99qn/o1oegim1xtwa2ihmxgzhv8vuuod1kjjjjpws6rw8ntea9fsyh2av+cj0lw9sy+7/qc2+reddr8yzp5woqnb5ooctuugnutjlrqs7pc2iwwulbauiqb+oiwughgi4ehbh+csastmt1ngnaretfgmliikcpjdbpksjtqfqmmk9ghffu0a6ast3vhguu1tae49eak1lblmodyisline3ge+ks2n+8x9jt3g1efyv18ojlgbd7l26a+v+dqiwjh2nzg0m1hzir1dmpsyk7im6ufamkk0ninmbdikeebamc9lmtmljwlnpryvibzbss2ntpboj3cusaspfznlogtl8ydkv6+ughfjkooott7kbi8zxcgweooeren3jee56vs+vwuvpup1ovtkrzwrelphwarlmweg==</latexit>h(1)1<latexit sha1_base64="fzklwfevrb7jzjefhn/ds4cqwca=">aaac0hicjvhltsjafd3uf+ildemmkzi4ii3r6jloxiu+ecrasfsgaojldmokhbi3/obb/srjh+hfegccepuynabtmxpvotp3xjvy3iqbxmtgm5tfwfzkludwvtfwn/kbw7uktgohvz3qc+ogbsxmcwnw5s73wcokmexbhqvbw1mrr9+wohhd4iqpitb2rx7g9lzh4ks1w77fb3zvfdnplhkdfmeognlps8buoac1kmh+bs10ecjbch8mathhdxysepowysairo0xctehv8yzjsirnqusrhkwsup69mnxvgxae+gzslvdp3j0xqtusueakpjiwui0xczt6szy37zh0lpcbur/w3n5xhimip1ln838r07uwthdsazbpzoiyyjqhowsyq6im+tfqulkebencjfimwfhkqd91qumkbwl3loy/iyzbsv2jspn8s5usqm2f45zftrkrfowajwffmonatrz7gax+ztpi5rxhgqq5h2nrzzhwbvqbru77f4zvcsozta+le3ha2bvldu=</latexit>s2source<latexit sha1_base64="e0eykfhfezba81yesyb38g1irsg=">aaac2nicjvhlssnafd2nr1pfvxhljlgevyuvrzdfn4iuktghteum09gg5kuyeuvoxp249qfc6gejf6b/4z0xbr+itkhy5tx7zsy91w5dj5aw9zizjianpmfys4w5+yxfpelysimokoilog/cigrzlbau44u6dkqrwmekmge7omkpdlw8eswi2an8mzkmrddjl75z4xamitovrnu8jvucuenj6lwjxbvmjwkvs1bz0sv8csozkcfbtad4ja56cmcrwioad0nybunmtxsvwaij6yillilk6ljacaxsjpqlkimro6dvje3agevtxnngws3pfjfeijqmnkktuf5ewj1m6niinrx7m3eqpdxdhvs3my+pwik+sx/pxpn/1alajc6wr2twqkzqm6o6nrkkuivq5uanqiq5hmqp3kn4rjhr5bjpptbeunbvw6bjrzptswrps9web+qwnodk93h+bi3tcmw3bj3ulkoh2ajzwmcgtmiee6jicdxuytvfax7xzhsmg+pwuptinxkzzhvflnh/dnlamis=</latexit>lklsource models<latexit sha1_base64="n0m5zfppvv8vuticzpftxdqbnho=">aaacynicjvhlssnafd2nr1pfvzdugkuqfyurrzdfny5cvlapqluk02kntzmwmqghdocpunupe/9a/8i7ywpqez2q5my559yze68b+v4sleu1ymznlywufzdlk6tr6xvlza1mhcac8qyl/vc0xsfmvhfwhvskz9ur4m7y9xnlhz2reouei9glg2uzrrw7doabn/cyi4lqpt37njuy9movq2rpzc4cowcv5ksell9wgz5cmcqygyoajozdquxpbzysrmr1krenchk6zjfbibwjqtgphgjh9b3srpozae1vzli7gz3i0yviawkppchpbgf1mqnjic6s2n9yzzqnultkfzfpnszw4o7yv3xt5x99qhajau51dr7vfglgvcfylinuirq5+auqsrki4htuu1wqzto57boppbguxfxw0fe3rvss2rncm+bd3zigbp8c5yxohlbt46p1dvspnewjlmihu9inez6ghgvu0dbvpuijz8aliyzuyd6lrih3bopbmh4+ad91kc4=</latexit>y⇤1<latexit sha1_base64="ufwmgmnfjh22xdaw7oyqq2xgg1s=">aaacynicjvhlssnafd2nr1pfvzdugkuqfyuvrzdfny5cvlapqluk6bqozyvjraiho3/arx6y+af6f94zu1cl6iqkz849587ce53i47g0rnecmte/slhuxc6trk6tb5q3t1pxmaixnd3qc0xhswpm8ya1jzce60sc2b7jsbyzplfx9j0tmq+da5lgrofbo4apuwtlotpp//a2o5j0yxwraullzojadirivymsv+aga4rwkcahqwbj2ionmj4uaraqedddrpwgxhwcyyisernsmvlyxi7po6jdn2cd2qucsxa7diphrycnit3yhkqthnvppo4nornif8ud6zzqbin9ntyxt6zehbf/+abk//pulrjdnooaonuuauzv5+zzet0vdxpzs1wsmktektygucdsaue0z6b2xlp21vtbx9+0urfq7+babo/qljtg2s9xzolwybv2xlwujir1s3zurexgf/s0zxpucyegmrrkrzzh2bg0hjea2afukosebxxbxsmhqdmrzw==</latexit>y⇤2<latexit sha1_base64="agslqjjqeoizp/koc2aqfvma/dg=">aaacynicjvhlssnafd2nr1pfvzdugkuqfyx1gs6lbly4qgbbodastkc1nc8meyge7vwbt/ph4h/ox3hnteetohosndn3ndtz73uiz42lzb0wjjnzufmf4mjpaxllda28vtgkw0qw3mshf4prx4655wa8kv3p8eticnt3pn52rmcq3r7ninbd4eqmee/69jbwby6zjvhtthdwm+2ne+wkvbx0mqdblqcv5ksrll9wgz5cmctwwrfaevzgi6angxosrmr1krenclk6zjfgibwjqtgpbgjh9b3srpozae1vzli7gz3i0sviawkhpchpbgf1mqnjic6s2n9yzzqnultkfyfp5rmrcufsx76j8r8+vyveace6bpdqijsjqmn5lkr3rd3c/fkvpawrcqr3ks4im+2c9nnunljxrnpr6/ibvipw7vmutfcubkkdrv0c5zro7vdrr1xr8rbsp81hxcqwtrfl8zxghedookmrfmqtno0lqxipkx1kjulu2cs3ztx8aeq9kda=</latexit>y⇤3attention<latexit sha1_base64="pfkd67krf2+niklga1mbnbbchim=">aaac0xicjvhlssnafd2nr1pfvzdugkxqtule0axoxmvf2wpwzrjhgzp5mjkipqji1h9wqz8l/oh+hxfgkfhadeksm+fec2buvuemolx53kvjgrkdg58ot1ampmdm56rzc608lwtim2equnkcsjylkofnfsnbjzpjwrwi3g56ezrevuyyj9lkspuzfhqzqys6jekmidrrmjf12bl/nlj1127oqzwv7pnl/gs+btxy1uirz+jgailcfijbkuarfmdi6tmbdw8zcacyeccjrsbocymkaqvk4ptbio3r94p2j5znak89c6mo6rrbryslixxspjqncevtxbmvjlnmf/mege99tz79a+sve6vqjfyv3tdzvzpdi8iltk0nedwuguzxf1qxwnrf39z9vjuih4w4js8olgmhrjnss2s0uald95az+kvj1kzehza3wju+jq3y/z7on6c1xvc3697brm1n1466jcusy5xmuyud7kobjnllpoart86h03dunbupvkdknyv4spz7d7yulfu=</latexit>↵(1)1<latexit sha1_base64="5fmyb+dvaudcstaxwnggfml4qui=">aaac0xicjvhlssnafd2n7/qqunqtleldlkqouhtdufs0wqhtmyxjg5oxk4lqiibu/qg3+lpih+hfegdmqs2ie5kcofeem3pv9zlat5xjvbasicmp6znzuel8wulscmll9tynm8lfncdblbses0xgr6kufbwiriifc71axhj9qx2/ubey9epota0s0qpzn/kvfc4uue1lfiq91qm1hxv367ztkjtvxyx7hlg5kcnfx3hpbze4qgyodceeiijcarhseppw4sahrouhczkqb+ictyisnqmsqrmm2d59u7rr5mxee+2zgjwnuwj6jsltbjimpjxjwj9mm3hmndx7m/fqeoq7dejv5v4hsqo9yv/sjtl/q9o1kfxjz9tgu02jyxr1phfjtff0ze0vvslysijt+irikja3ylgfbanjte26t8ze30ymzvwe57kz3vutacduz3gog/na1d2poifb5f2dfnszwmcgkjtpxezjcmeok7fei57wbj1aa+vouv9mtqq5zg3flvxwab8ulfy=</latexit>↵(1)2<latexit sha1_base64="phbluh3lpg5nuxkxxcxp3lfb0de=">aaac0xicjvhlssnafd2nr1pfvzdugkwom5l4qjdfny4r2lqodyzx1me0czojuepb3podbvwnxd/qv/dooaufie5icubce87mvtdii5epz3spocojy+mtxcns1ptm7fx5fqgvjbkmetnmoks2a5bxsms8qyskeduvnhwdib8f17s6fntdzsas+fd1un7szzexubahu0sdhrmovwjn66f9qr86octxvjpnlvst+bzuyfcjkt/jgodiecjhfxwxfoeidbk9hfjwkbj3gj5xkpawcy4bsqtnkyttbip2mr6xtotynqa99symoqrtinolkv2skcahpelyn+aaeg6cnfubd9946rv16b9yry6xclfe/qubzv5xp2trumc2qufqtalhdhwhdclnv/tn3u9vkxjiidp4nokscgiuwz67rpoz2nvvmym/mkzn6n1oc3o86vvsgp3v4/wjwms1f7pm7w9u6jt21eusyrlvmucw6thda03ylnjai56ca6fn3dp3h6lowwow8wu59+/beprx</latexit>↵(1)3<latexit sha1_base64="pfkd67krf2+niklga1mbnbbchim=">aaac0xicjvhlssnafd2nr1pfvzdugkxqtule0axoxmvf2wpwzrjhgzp5mjkipqji1h9wqz8l/oh+hxfgkfhadeksm+fec2buvuemolx53kvjgrkdg58ot1ampmdm56rzc608lwtim2equnkcsjylkofnfsnbjzpjwrwi3g56ezrevuyyj9lkspuzfhqzqys6jekmidrrmjf12bl/nlj1127oqzwv7pnl/gs+btxy1uirz+jgailcfijbkuarfmdi6tmbdw8zcacyeccjrsbocymkaqvk4ptbio3r94p2j5znak89c6mo6rrbryslixxspjqncevtxbmvjlnmf/mege99tz79a+sve6vqjfyv3tdzvzpdi8iltk0nedwuguzxf1qxwnrf39z9vjuih4w4js8olgmhrjnss2s0uald95az+kvj1kzehza3wju+jq3y/z7on6c1xvc3697brm1n1466jcusy5xmuyud7kobjnllpoart86h03dunbupvkdknyv4spz7d7yulfu=</latexit>↵(1)1<latexit sha1_base64="5fmyb+dvaudcstaxwnggfml4qui=">aaac0xicjvhlssnafd2n7/qqunqtleldlkqouhtdufs0wqhtmyxjg5oxk4lqiibu/qg3+lpih+hfegdmqs2ie5kcofeem3pv9zlat5xjvbasicmp6znzuel8wulscmll9tynm8lfncdblbses0xgr6kufbwiriifc71axhj9qx2/ubey9epota0s0qpzn/kvfc4uue1lfiq91qm1hxv367ztkjtvxyx7hlg5kcnfx3hpbze4qgyodceeiijcarhseppw4sahrouhczkqb+ictyisnqmsqrmm2d59u7rr5mxee+2zgjwnuwj6jsltbjimpjxjwj9mm3hmndx7m/fqeoq7dejv5v4hsqo9yv/sjtl/q9o1kfxjz9tgu02jyxr1phfjtff0ze0vvslysijt+irikja3ylgfbanjte26t8ze30ymzvwe57kz3vutacduz3gog/na1d2poifb5f2dfnszwmcgkjtpxezjcmeok7fei57wbj1aa+vouv9mtqq5zg3flvxwab8ulfy=</latexit>↵(1)2<latexit sha1_base64="pfkd67krf2+niklga1mbnbbchim=">aaac0xicjvhlssnafd2nr1pfvzdugkxqtule0axoxmvf2wpwzrjhgzp5mjkipqji1h9wqz8l/oh+hxfgkfhadeksm+fec2buvuemolx53kvjgrkdg58ot1ampmdm56rzc608lwtim2equnkcsjylkofnfsnbjzpjwrwi3g56ezrevuyyj9lkspuzfhqzqys6jekmidrrmjf12bl/nlj1127oqzwv7pnl/gs+btxy1uirz+jgailcfijbkuarfmdi6tmbdw8zcacyeccjrsbocymkaqvk4ptbio3r94p2j5znak89c6mo6rrbryslixxspjqncevtxbmvjlnmf/mege99tz79a+sve6vqjfyv3tdzvzpdi8iltk0nedwuguzxf1qxwnrf39z9vjuih4w4js8olgmhrjnss2s0uald95az+kvj1kzehza3wju+jq3y/z7on6c1xvc3697brm1n1466jcusy5xmuyud7kobjnllpoart86h03dunbupvkdknyv4spz7d7yulfu=</latexit>↵(1)1<latexit sha1_base64="5fmyb+dvaudcstaxwnggfml4qui=">aaac0xicjvhlssnafd2n7/qqunqtleldlkqouhtdufs0wqhtmyxjg5oxk4lqiibu/qg3+lpih+hfegdmqs2ie5kcofeem3pv9zlat5xjvbasicmp6znzuel8wulscmll9tynm8lfncdblbses0xgr6kufbwiriifc71axhj9qx2/ubey9epota0s0qpzn/kvfc4uue1lfiq91qm1hxv367ztkjtvxyx7hlg5kcnfx3hpbze4qgyodceeiijcarhseppw4sahrouhczkqb+ictyisnqmsqrmm2d59u7rr5mxee+2zgjwnuwj6jsltbjimpjxjwj9mm3hmndx7m/fqeoq7dejv5v4hsqo9yv/sjtl/q9o1kfxjz9tgu02jyxr1phfjtff0ze0vvslysijt+irikja3ylgfbanjte26t8ze30ymzvwe57kz3vutacduz3gog/na1d2poifb5f2dfnszwmcgkjtpxezjcmeok7fei57wbj1aa+vouv9mtqq5zg3flvxwab8ulfy=</latexit>↵(1)2<latexit sha1_base64="phbluh3lpg5nuxkxxcxp3lfb0de=">aaac0xicjvhlssnafd2nr1pfvzdugkwom5l4qjdfny4r2lqodyzx1me0czojuepb3podbvwnxd/qv/dooaufie5icubce87mvtdii5epz3spocojy+mtxcns1ptm7fx5fqgvjbkmetnmoks2a5bxsms8qyskeduvnhwdib8f17s6fntdzsas+fd1un7szzexubahu0sdhrmovwjn66f9qr86octxvjpnlvst+bzuyfcjkt/jgodiecjhfxwxfoeidbk9hfjwkbj3gj5xkpawcy4bsqtnkyttbip2mr6xtotynqa99symoqrtinolkv2skcahpelyn+aaeg6cnfubd9946rv16b9yry6xclfe/qubzv5xp2trumc2qufqtalhdhwhdclnv/tn3u9vkxjiidp4nokscgiuwz67rpoz2nvvmym/mkzn6n1oc3o86vvsgp3v4/wjwms1f7pm7w9u6jt21eusyrlvmucw6thda03ylnjai56ca6fn3dp3h6lowwow8wu59+/beprx</latexit>↵(1)3<latexit sha1_base64="shobpzkpqbqpnmb86oa/khmtqsy=">aaaczhicjvhlssnafd2nr1pfvzdugkwom5kiosuig1dswt6k1pkk0xqaf5mjueq3/obb/s7xd/qvvdnoqs2ie5kcofeem3pvdzpa58kyxnpg3pzc4lj+ubcyura+udzcava4sz1w9+igtluuw1ngr6wufbgwvpiyj3qd1nshzzlevgcp9+posows1gmdqet3fc8rrf0nxx47ltv7k26xzfustcxzygtqgl61upicg/qqw0ogeawrboeadjg9bdiwkbdxwzi4ljcv4gwtfeiburajdifyix0htgtrnqk99ork7depab0pku3sksamvjswpm1u8uw5s/y377hylhcb0d/vxigxanfe/qwbzv5xj2sr6one1ebttylizhwedsluv+tnzs9vcxjiijo4r/guskeu0z6bssnv7bk3joq/quzjyr2nczo8y1vsgo2f45wfjyokfvsxlg9l1vm96jx2sisyzfmyvzyjhjp5h3jee56nc0myy2pymwrktgyb35bx8ahpx5jz</latexit>p(1)s<latexit sha1_base64="cmkpwkj0jpyvyj80mku96oavzdk=">aaaczxicjvhlssnafd2nr1pfvzdugkvwvrjrdfl0484kfaetkqttdmhetczcqxxrd7jv3xl/qp/co2mkahgdkotmufecmxuvg/s8kzb1mjpm5hcwl/llhzxvtfwn4uzwi4ls4bg6f/mrallownwesrrk0metwdanch3wdidnkt68zslhuvito5h1aqcf8h73henuvttw5mdtjwutm2ljklt6mbpazkaj2apgxre00uuedykcmisqhh04soi5hg0lmxedjikthliom0xqig1kwywyhgkh9o3t7jpjq9orz0srptrfp1eq0sqeaslke4tvaaaop9pzsb95j7wnutui/m7mfrarmsd2l9008786vytedye6bk41xzpr1xmzs6q7om5ufqlkkknmnmjdigvcnlzo+2xqtajrv711dpxnzypw7b0sn8w7uiun2p45zlnqocjbr2xr8rbuoc1gnccodrfp8zxgbeeook7eir7xhgfjwkino+p+m9xizzptffvgwwdz25n9</latexit>t<latexit sha1_base64="9g3fcppstkziioi+nh2hgf2v/g8=">aaac0hicjvhlssnafd2nr1pfvzdugkvwvrjrdfl047i+2gq1lek6bupzcjirsyni1h9wq18l/oh+hxfgfhwgoihjmxpvotp3xif2vura1kvomjqemz3lzxcwfpewv4qra/uksoxla27kr+lcyqn3vzdxpcd9fhelzglh5w1ncktijwsuei8kz+uw5q2a9ukv67lmetw6djjso93r2bhtf9rfklw29dj/ajsdjwsrghwfcykoirhieyajhctsgyghpwkbfmliwhgrjwh5os4xrog0kwvxymdedujbo10zy0pak89eq106xadxknlefmkiyhoe1wmmjqfawbg/ey+0p7rbkp5o5huqk9en9i/djpo/olwlrbchugapaoo1o6pzm5dud0xd3pxulsshmdifoxqxhf2tnptz1jpe1656y3t8vwcqvu3dldffm7olddj+ps6fol5ttvfk1sluqxkyjtqpdwxim+a5jwqouuwnvk/wgec8gafgjxfr3h2kgrlms44vy7h/b14oldq=</latexit>s1goldlabelstarget model<latexit sha1_base64="pfkd67krf2+niklga1mbnbbchim=">aaac0xicjvhlssnafd2nr1pfvzdugkxqtule0axoxmvf2wpwzrjhgzp5mjkipqji1h9wqz8l/oh+hxfgkfhadeksm+fec2buvuemolx53kvjgrkdg58ot1ampmdm56rzc608lwtim2equnkcsjylkofnfsnbjzpjwrwi3g56ezrevuyyj9lkspuzfhqzqys6jekmidrrmjf12bl/nlj1127oqzwv7pnl/gs+btxy1uirz+jgailcfijbkuarfmdi6tmbdw8zcacyeccjrsbocymkaqvk4ptbio3r94p2j5znak89c6mo6rrbryslixxspjqncevtxbmvjlnmf/mege99tz79a+sve6vqjfyv3tdzvzpdi8iltk0nedwuguzxf1qxwnrf39z9vjuih4w4js8olgmhrjnss2s0uald95az+kvj1kzehza3wju+jq3y/z7on6c1xvc3697brm1n1466jcusy5xmuyud7kobjnllpoart86h03dunbupvkdknyv4spz7d7yulfu=</latexit>↵(1)1<latexit sha1_base64="xxax7b5j53ub6cbgk+vza+p/rgo=">aaac0xicjvhlssnafd2nr1pfvzdugkxqtule0axoxmvf2wpwzrjhgzp5mjkipqji1h9wqz8l/oh+hxfgkfhadeksm+fec2buvuemolx53kvjgrkdg58ot1ampmdm56rzc608lwtim2equnkcsjylkofnfsnbjzpjwrwi3g56ezrevuyyj9lkspuzfhqzqys6jekmidrrmjf12bl3nlj1127oqzwv7pnl/gs+btxy1uirz+jgailcfijbkuarfmdi6tmbdw8zcacyeccjrsbocymkaqvk4ptbio3r94p2j5znak89c6mo6rrbryslixxspjqncevtxbmvjlnmf/mege99tz79a+sve6vqjfyv3tdzvzpdi8iltk0nedwuguzxf1qxwnrf39z9vjuih4w4js8olgmhrjnss2s0uald95az+kvj1kzehza3wju+jq3y/z7on6c1xvc3697brm1n1466jcusy5xmuyud7kobjnllpoart86h03dunbupvkdknyv4spz7d7pilfq=</latexit>↵(1)0<latexit sha1_base64="lyrdxauqlje+cxmds6pl7ukpxv8=">aaac13icjvhlssnafd2nr1pfss7dbitqnyuvrzdfny4r2ie0tstpta3ni2qilldcivt/wk3+kfgh+hfegvnqi+iejgfovefm3hvnwlejruuvgwvufmfxkbucw1ldw99qn/p1yi9di9us3/hdpmlezle9vum2d1gzcjnhmg5rmkntew9cszcyfe+cjwpwcy2bz/dty+beddv82zx40ownw0lxv0qkfg/svqt6szdlmwxlfbsqrqqvvqcnhnxyiogcwqmn7mbare8lzegiiosgis4kzms4wwq50sauxsjdihze3whtwinr0v54rljt0skovsepneysxqe8kla4tzpxwdol9jfvrhqku43pb6zelrecq2l/0k0z/6sttxd0csxrskmmqdkioit1iwvxxm21l1vxcgiie7hh8zcwjzxtpmtse8narw8ngx+tmyiveyvnjfeubkkdlv8c5yyo75fkhyx9/kbqoulhncu2dlcker6hgjnuuspvgzzicc/kpxkr3cn3n6lkjtvs4dtshj4a45mwva==</latexit>h(t)0<latexit sha1_base64="i/arkfw4xovvdgqbqoe4q/gbhgo=">aaac13icjvhlssnafd2nr1pfss7dbitqnyurrzdfny4r2ie0tstpta3ni2qilllcivt/wk3+kfgh+hfegvnqi+iejgfovefm3hut0hviruuvgwvufmfxkbucw1ldw99qn/o1oegim1xtwa2ihmxgzhv8vuuod1kjjjjpws6rw8ntea9fsyh2av+cj0lw9sy+7/qc2+reddr8yzp5woqnb5oofjuugnutjlrqs7pc2iwwulbauiqb+oiwughgi4ehbh+csastmt1ngnaretfgmliikcpjdbpksjtqfqmmk9ghffu0a6ast3vhguu1tae49eak1lblmodyisline3ge+ks2n+8x9jt3g1efyv18ojlgbd7l26a+v+dqiwjh2nzg0m1hzir1dmpsyk7im6ufamkk0ninmbdikeebamc9lmtmljwlnpryvibzbss2ntpboj3cusaspfznlogtl8ydkv6+ughfjkooott7kbi8zxcgweooeren3jee56vs+vwuvpup1ovtkrzwrelphwaq/oweq==</latexit>h(1)0<latexit sha1_base64="pfkd67krf2+niklga1mbnbbchim=">aaac0xicjvhlssnafd2nr1pfvzdugkxqtule0axoxmvf2wpwzrjhgzp5mjkipqji1h9wqz8l/oh+hxfgkfhadeksm+fec2buvuemolx53kvjgrkdg58ot1ampmdm56rzc608lwtim2equnkcsjylkofnfsnbjzpjwrwi3g56ezrevuyyj9lkspuzfhqzqys6jekmidrrmjf12bl/nlj1127oqzwv7pnl/gs+btxy1uirz+jgailcfijbkuarfmdi6tmbdw8zcacyeccjrsbocymkaqvk4ptbio3r94p2j5znak89c6mo6rrbryslixxspjqncevtxbmvjlnmf/mege99tz79a+sve6vqjfyv3tdzvzpdi8iltk0nedwuguzxf1qxwnrf39z9vjuih4w4js8olgmhrjnss2s0uald95az+kvj1kzehza3wju+jq3y/z7on6c1xvc3697brm1n1466jcusy5xmuyud7kobjnllpoart86h03dunbupvkdknyv4spz7d7yulfu=</latexit>↵(1)1<latexit sha1_base64="slmepvtze8enhuz/xihz1vqcqxu=">aaacz3icjvhltsjafd3uf+ildemmkzjghrrgo0uig5eqcjgammkzokg0zxsqiqtj1h9wq39l/ap9c++mjvgj0wnanjn3njnz73ui34ulzb1mjln5hcwl7hjuzxvtfso/uvwpw0s4voagfiguhrzz3wt4txrs55er4gzo+lzhdm5uvhhdreyfwyucrbw9zl3a63ouk0s1wsyp+uxqxlt3j9f5glwy9djngz2catjvcfmvakgdec4sdmerqbl2wrdt04qncxfxbyyje4q8heeyiefahli4ztbib/tt0a6zsghtlwes1s6d4tmrsglijzqh5qnc6jrtxxptrnjfvmfau91trh8n9rosk9en9i/dnpo/olwlrbcnugapaoo0o6pzu5ded0xd3pxslsshidifoxqxhf2tnpbz1jpy1656y3t8twcqvu3dndfbu7olddj+oc5zud8o2uclq3pykj+mo85ib7so0jypucy5kqird4rhpohzqbq3xp1x/5lqzflnnr4t4+edem6tsq==</latexit>↵(1)attention<latexit sha1_base64="shobpzkpqbqpnmb86oa/khmtqsy=">aaaczhicjvhlssnafd2nr1pfvzdugkwom5kiosuig1dswt6k1pkk0xqaf5mjueq3/obb/s7xd/qvvdnoqs2ie5kcofeem3pvdzpa58kyxnpg3pzc4lj+ubcyura+udzcava4sz1w9+igtluuw1ngr6wufbgwvpiyj3qd1nshzzlevgcp9+posows1gmdqet3fc8rrf0nxx47ltv7k26xzfustcxzygtqgl61upicg/qqw0ogeawrboeadjg9bdiwkbdxwzi4ljcv4gwtfeiburajdifyix0htgtrnqk99ork7depab0pku3sksamvjswpm1u8uw5s/y377hylhcb0d/vxigxanfe/qwbzv5xj2sr6one1ebttylizhwedsluv+tnzs9vcxjiijo4r/guskeu0z6bssnv7bk3joq/quzjyr2nczo8y1vsgo2f45wfjyokfvsxlg9l1vm96jx2sisyzfmyvzyjhjp5h3jee56nc0myy2pymwrktgyb35bx8ahpx5jz</latexit>p(1)s<latexit sha1_base64="shobpzkpqbqpnmb86oa/khmtqsy=">aaaczhicjvhlssnafd2nr1pfvzdugkwom5kiosuig1dswt6k1pkk0xqaf5mjueq3/obb/s7xd/qvvdnoqs2ie5kcofeem3pvdzpa58kyxnpg3pzc4lj+ubcyura+udzcava4sz1w9+igtluuw1ngr6wufbgwvpiyj3qd1nshzzlevgcp9+posows1gmdqet3fc8rrf0nxx47ltv7k26xzfustcxzygtqgl61upicg/qqw0ogeawrboeadjg9bdiwkbdxwzi4ljcv4gwtfeiburajdifyix0htgtrnqk99ork7depab0pku3sksamvjswpm1u8uw5s/y377hylhcb0d/vxigxanfe/qwbzv5xj2sr6one1ebttylizhwedsluv+tnzs9vcxjiijo4r/guskeu0z6bssnv7bk3joq/quzjyr2nczo8y1vsgo2f45wfjyokfvsxlg9l1vm96jx2sisyzfmyvzyjhjp5h3jee56nc0myy2pymwrktgyb35bx8ahpx5jz</latexit>p(1)ssentence-level aggregationsubstructure-level aggregationlanguage-level aggregation<latexit sha1_base64="z8cxx/hd3mvswf+lw6gllmavrce=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrjrdfl002vf+4basjkd1ta0czojworgd7jvtxp/qp/co+mu1ci6icmzc+85m/depwmdvdroa86am19yxmovf1zw19y3iptbjttobon1foexapleysmg4nuzyjc3esg9kr/ypj88u/hmdrdpeeexcpzwzsgbree/yj4k6uku63aljafs6gxpateaesyqxcuxxkghgawzruciiamh8jds04ylbwlxhuyie4qchee4r4g0gwvxyvcihdj3qlu2yspak89uqxmdetirsgljjzqx5qnc6jrbxzptrnjfvcfau91tth/fei2ilbgm9i/dnpo/olwlrb8nuoaaako0o6pjxixtxve3t79ujckhiu7hhsufyaav0z7bwppq2lvvpr1/05mkvxtmcjo8q1vsgn2f45wfjyoye1r2zg9llvmz6jx2sit9mucxkqiihjp5d/cijzxbvsuymuv2m9xkgc02vi3r4qmpxzal</latexit>x1<latexit sha1_base64="gx72pyhmzwwxmhsmqkjiyrgqdis=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrjrdfl002vf+4basjkd1qfpeiyttrtbh3crnyb+gf6fd8yu1ci6icmzc+85m/depw5eohznnwfnzs8sluwxcyura+sbxc2trhklkve6i4jitnwv4yeiev0jffbwllk38gpe9idnot684tiruxipxjhvjlxbkpqceyqoi7vuqbdycsqowfyscdnqqrzqufefv+ghakokethckmibpct0tohcquxcbxpijcfh4hz3kja2psxogr6xq/ooanfo2jd22jmxakanbprkutryi01eezkwps028dq4a/y374nx1hcb09/pvebeklwt+5dumvlfna5foy8tu4ogmmld6opy5pkaruib21+quuqqe6dxj+ksmdpkaz9to0lm7bq3nom/muzn6j3lclo861vsgn2f45wfjyoye1r2zg9lldns1hnsybf7nm9jvfbfdxxyhuart3i2qlzopdbtz6qvyztb+lashw8rvzam</latexit>x2<latexit sha1_base64="i2fs8c4pyflomsco7jcnmoclnp8=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrif6llopsuk9gg1lgq6rynpeiyttrtbh3crnyb+gf6fd8yu1ci6icmzc+85m/depw5eohznnwfnzm7nl+qxc0vlk6trxfwnrhklkve6i4jitnwv4yeiev0jffbwllk39ape9k9pdbx5w2uiovbcjwlegxqdupqf8xrr53fd/w6x5jqds+xp4gaghgzvouilltfdbiyuq3ceuiqdeejoacofg5i4dsbesulcxdnuusbtslmcmjxir+k7of07y0paa8/eqbmdetarswljhzqr5unc+jtbxfpjrnnfvmfgu99trh8/8xosq3bf7f+6sez/dbowht6otq2caoono6tjmutquqjvbn+pspfdtjzgpyplwswoj322jsyxteveeib+zji1q/csy03xrm9ja3z/jnmanpbk7mhzotsovu6yueexhw3s0jypueevndtje4bhpohzqlqhlvq3n6lwltns4tuyhj4afb2qjw==</latexit>x3the aggregated source viewthe target view<latexit sha1_base64="/uqtlzsr96coa4cn7qeestem1dy=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrjrdfl002wl9gg1lgq6ruptjewmsimcp+bwp038a/0l74wpqev0qpiz595zzu69fhyirdnoa85awfxaxsmvftbwnza3its7zsrkjemnfgwrbptewgmr8oyskudtwhjv7ae85y8udlx1y2uiovbktwlehxvduawe8xrr9bhx7xvlttkxy54hbgzkyfytkr7ggn1eyegxbkcirtiah4sedlw4iinrykqcjcrmnomebdkmlmupwyn2rn8h7tozg9jeeyzgzeiugf5jshshpikotxlwp9kmnhpnzf7mptwe+m4t+vuz15hyhrti/9lnmv+r07uodhbmahbuu2wyxr3lxfltfx1z+0tvihxi4jtuu1wszky567ntnimpxffwm/e3k6lzvwdzbop3fusasptznpogevr2t8ro5xgpcp6noo897ooq5nmkcqqoouheqzzicc9w1qqt1lr7tlvymwyx35b18afndza/</latexit>ps<latexit sha1_base64="/uqtlzsr96coa4cn7qeestem1dy=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrjrdfl002wl9gg1lgq6ruptjewmsimcp+bwp038a/0l74wpqev0qpiz595zzu69fhyirdnoa85awfxaxsmvftbwnza3its7zsrkjemnfgwrbptewgmr8oyskudtwhjv7ae85y8udlx1y2uiovbktwlehxvduawe8xrr9bhx7xvlttkxy54hbgzkyfytkr7ggn1eyegxbkcirtiah4sedlw4iinrykqcjcrmnomebdkmlmupwyn2rn8h7tozg9jeeyzgzeiugf5jshshpikotxlwp9kmnhpnzf7mptwe+m4t+vuz15hyhrti/9lnmv+r07uodhbmahbuu2wyxr3lxfltfx1z+0tvihxi4jtuu1wszky567ntnimpxffwm/e3k6lzvwdzbop3fusasptznpogevr2t8ro5xgpcp6noo897ooq5nmkcqqoouheqzzicc9w1qqt1lr7tlvymwyx35b18afndza/</latexit>ps<latexit sha1_base64="/uqtlzsr96coa4cn7qeestem1dy=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrjrdfl002wl9gg1lgq6ruptjewmsimcp+bwp038a/0l74wpqev0qpiz595zzu69fhyirdnoa85awfxaxsmvftbwnza3its7zsrkjemnfgwrbptewgmr8oyskudtwhjv7ae85y8udlx1y2uiovbktwlehxvduawe8xrr9bhx7xvlttkxy54hbgzkyfytkr7ggn1eyegxbkcirtiah4sedlw4iinrykqcjcrmnomebdkmlmupwyn2rn8h7tozg9jeeyzgzeiugf5jshshpikotxlwp9kmnhpnzf7mptwe+m4t+vuz15hyhrti/9lnmv+r07uodhbmahbuu2wyxr3lxfltfx1z+0tvihxi4jtuu1wszky567ntnimpxffwm/e3k6lzvwdzbop3fusasptznpogevr2t8ro5xgpcp6noo897ooq5nmkcqqoouheqzzicc9w1qqt1lr7tlvymwyx35b18afndza/</latexit>ps<latexit sha1_base64="pxi6dll8jsu3+my6augwyav8ht0=">aaacxnicjvhlssnafd2nr1pfvzdugkvwvrjrdfl002xfvqcwkkyndwiahmlekuxwb9zqp4l/oh/hnteftyhoshlm3hvozl3xjworkmd5zvkli0vlk/nvwtr6xuzwcxunmuspzlzboicsbd9lecbc3lbcbbwds+6n/yc3/ngfjrduuuxefnbvjobdstcmxuawtxf1fffqvwljkttm2fpazuaj2apfxrdco48idcng4aihcafwkndtgqshmxfdtimthisjc9yjqnqusjhlemso6dukxsdjq9prz8sogz0s0ctjaeoanbhlscl6nnveu+os2d+8p8zt321cfz/zghorcepsx7pz5n91uhafac5mdyjqig2jq2ozs2q6om9uf6lkkunmnmz9ikvczchnfbanjjg16956jv5mmjwr9yzltfgub0kddn+ocx40j8rusdm5pc5vzrnr57ghfrzspe9rqru1nmh7iec84dmqwqgvwnefqvyu0+zi27iepgbpbzba</latexit>pt<latexit sha1_base64="aeswhmbf3n0nb7qrhp1xfzzhlds=">aaac3hicjvhlssnafd2nr1pfurcu3asl4kqkouiykiilfxx6glblzbzbyf4ke1fkd+7ert/gvr9h/ap9c++medqioihjmxpvoxpvxcfy3eta9kvogbufmjzktxdmzufmf8zfpuyspjexdr56ydxywci8nxb16upptkjymn/xrno52ffx5qwiezcmavi6eic+6wxuucuzjkprrnr8jvuceyojybcjxzuc7b8mt2tds2ixbl2suvdoqbhzqobmmzo4qwiofd4eakjchhgsetoow0ze3akgxmwexb0xgkja2psybguwyi/o26ndo2md2ivprks5nelrg5pswjppqsqlcavtlb1ptbnif/meae9v2zx9nczlj1ait+xfus/m/+pulxln2nu9unrtpbnvhc9cun0rqnlrs1eshclifd6jeeyya+xnpvtak+je1d0yhx/vmypve57lpnhtvdkayz/hoqoam6xydsk+3ipw9rjr57gknwzqphdqwsgqqov6h/cij+puudfujbupvcoxazbxbrn371njmui=</latexit>ltce<latexit sha1_base64="jw3iadz0msmkoaz5ksxtbvck60a=">aaac3hicjvhlssnafd2nr1pfurcu3asl4kqkouhsliilfxxta6ywytjayf4ke7gu7tyjw3/arx6p+af6f94zu1cl6iqkz86958y9c53icxnp2685y2r0bhwip1mymp6zntpnf+pjmmzc1hjohxhtyynw3edupcs90yxiwxzhew3nqqlijwsrj24yhmtuje59dhm4fy5nkqi2udtymexw5vuo+u2wfdeyv9nrnx21zajdsvwyhke5a0vkqxqal2jhhce4uvgqccaje2bi6dlbgtyi4k7riy4m5oq4qb8f0qaujsidextf30vanwrsqhvlmwg1p1m8emnswlgltuh5mwf1mqxjqxzw7g/epe2pauvs38m8fgilost+prtk/lenepg4wlbuwaweis2o7njmkupbuzvbx7qs5barp/a5xwpcxcsh92xptaj7v3fldpxnzypw7xmwm+jdvukdlv8c5zcor5fkmyx7cko4s5unoo9lrgcn5rmfheyjipqu/xfpedbojfvjzrj/tdvymwyr35bx8afq6zlb</latexit>lscethe figure 1. the ﬁnal output distribution of theaggregated source view can be computed as,.
model.
we compute the probabilities αsub(xi) fori-th sub-structure as follows,.
ps(y.x) =.
|.
α.
(k)lang ·.
p(k)s (y.x)|.
k(cid:88).
k=1.
ki = [h.(1)i.
(k); .
.
.
; h]i(t)i wkti ).
αsub(xi) = softmax(h.we use superscript to represent the index of vectorαlang.
note that we use lowercase s, uppercase s,and uppercase t to differentiate the ﬁnal outputsof the source model, aggregated source view, andtarget view respectively.
in this approach, the k-th(k)lang over allsource model has the same weight αsentences..2.2.2 sentence-level aggregation.
in this section, we leverage an attention mecha-nism (luong et al., 2015; vaswani et al., 2017) tolearn the weight of each source model on an inputsentence, as shown on the top right part of figure1. firstly, we use the internal states of the [cls](t)token as sentence representation.
secondly, h0from the target model t is used as a query to attendfrom the k-th source model sk to produce thehk.probabilities αsent(x).
(k)0.
∈ r.k0 = [h.(k)(1)0 ; .
.
.
; h]0(t)0 wkt0 )αsent(x) = softmax(h.where k0 is the concatenation of sentence repre-d×dsentations from k source models, and wis the bilinear weight matrix.
then the probabilitiesare utilized to compute the aggregation distributionps(y.x) as follows,.
∈ r.|.
ps(y.x) =|.
(k)sent(x).
α.p(k)s (y.x).
·.
|.
k(cid:88).
k=1.
in sentence-level aggregation approach, k-th source(k)sent(x) over each sub-model has the same weight αstructure of a sentence, but different weights overdifferent sentences and thus can capture the diversestrengths of each source on different sentences..2.2.3 sub-structure-level aggregation.
we further propose a ﬁne-grained aggregation ap-proach on sub-structure level, which is also basedon the attention mechanism.
as shown in the leftpart of figure 1, for token xi in a given sentence x,(t)we use its representation has the query to attendithe corresponding representation from each source.
then the aggregation distribution becomes,.
ps(y.x) =.
|.
n(cid:89).
k(cid:88).
i=1.
k=1.
α.
(k)sub(xi).
p(k)s (yi.
·.
x)|.
in this approach, our target model acts as a selectorto dynamically assess the multiple source modelson sub-structure level..2.3 consensus between two views.
to achieve a good trade-off between the target viewand the aggregation view during training, inspiredby clark et al.
(2018), we utilize the kl divergence3 as the metric to encourage the similarity betweenthe two views.
for sequence labeling, the objectiveis,.
kl(x) = kl(ps(y.l.pt (y.x)|.
||.
x))|.
2.4 overall training objective.
in the model training, for the unlabeled sentences,u =we only calculate the kl-divergence losskl.
for the labeled sentences, we train the modellwith two supervised cross-entropy loss in additionto the kl-divergence loss,.
l.l = λ1.
l.sce + λ2l.tce + λ3l.l.kl.
where λ1, λ2 and λ3 are the interpolation factors.
finally, we introduce an interpolation µ to balancethe labeled and unlabeled sentences and the overall= µlearning objective is.
l + (1.µ).
u..l.l.−.
l.connections to kd there are mainly four dif-ferences between kd (wu et al., 2020) and ourapproach:.
1. unlike our approach, kd only utilizes thetarget unlabeled data, from which it cannotwell learn the strength and weakness of differ-ent source models (see sec.1 for more discus-sion.)..
3we also try many metrics of measuring the similarity be-tween two probability distributions, e.g., mean squared error(mse) (wu et al., 2020), cosine, and jensen-shannon diver-gence (js) (ruder and plank, 2017), and we ﬁnd kl performbest..26642. kd assigns equal importance to multiplesource models, which can be seen as a ﬁxeduniform vector in our language-level aggrega-tion approach..3. besides language-level aggregation, we pro-pose two ﬁne-grained aggregation strategiesto dynamically balance the information fromsource models..4. to achieve the previously described goal,our approach has trainable parameters in theaggregation component and our multi-viewlearning framework can jointly learn the pa-rameters of two views..2.5 training and inference strategies.
following previous work on cross-lingual trans-fer (rahimi et al., 2019; wu et al., 2020), thesource models are previously trained on their cor-responding labeled training data.
during training,we freeze the parameters of the pre-trained sourcemodels and only update the parameters of calculat-ing weights α in the aggregated source view, andupdate all parameters of the target view.
in everyiteration, we randomly sample a batch of data fromthe labeled dataset and unlabeled dataset accord-ing to the interpolation µ. in the experiments, ourmodel can signiﬁcantly beneﬁt from this trainingstrategy by controlling the ratio of labeled dataand unlabeled data.
during the inference phase, wehave two options to obtain the predictions: utilizingthe aggregated source view or the target view.
inour experiments, we use the second one as the mainresult for its simplicity and better performance..3 experiments.
we experiment on three structured prediction tasks:ner, pos tagging, and dependency parsing.
fol-lowing previous work (rahimi et al., 2019; wuet al., 2020), we conduct the experiments in a leave-one-out setting in which we hold out one languageas the target language and the others as the sourcelanguages.
to simulate the low-resources scenario,for each training set in a speciﬁc target language,we randomly select ﬁfty sentences 4 with the goldannotations and discard the annotations of the re-maining sentences to construct the training set.
we.
4we explore the effects of randomness on labeled data inthe appendix c.1 and the results show that our approach isrobust to randomness in the selection of labeled data..randomly select six languages from universal de-pendencies treebanks (v2.2)5 for dependency pars-ing and pos tagging tasks.
we use the datasetsfrom conll 2002 and conll 2003 shared tasks(tjong kim sang, 2002; tjong kim sang andde meulder, 2003) for ner tasks.
we utilize thebase cased multilingual bert (devlin et al., 2019)as base model for all approaches.
we run each ap-proach ﬁve times and report the averaged accuracyfor pos tagging, f1-score for ner, and unlabelledattachment score (uas) and labeled attachmentscore (las) for dependency parsing.
more detailscan be found in the appendix b.1..3.1 compared baselines.
we compare the results of the target view of ourlanguage/sentence/sub-structure-level approacheswhich are denoted as ours-lang/sent/sub respec-tively, with a large amount of previous state-of-the-art cross-lingual baselines: direct ﬁne-tuning (dt-ﬁnetuning), direct transfer (dt), hard knowledgedistillation (hard-kd) (liu et al., 2017), soft knowl-edge distillation (soft-kd) (hinton et al., 2015; wuet al., 2020), uniﬁed multilingual model (umm)which is similar to (yasunaga et al., 2018; ak-bik et al., 2019), and bootstrapping approaches(yarowsky, 1995; zhou and li, 2005; mccloskyet al., 2006; ruder and plank, 2018) based onumm..dt-ﬁnetuning we directly ﬁne-tune the task-speciﬁc view on ﬁfty labeled data..dt in dt, there is only test data in the targetlanguage.
therefore, we evaluate this approach inthree ways: 1) using the mean probability distribu-tion of source models (dt-mean); 2) using the max-imal probability distribution of source models overthe sub-structure level (dt-max).
3) evaluatingeach source model and voting on the sub-structurelevel (dt-vote).
we also provide the maximal re-sults of dt on language level (dt-max(lang)) 6..hard-kd the hard knowledge distillation ap-proaches ﬁrst predict the pseudo labels on targetunlabeled training set by using pre-trained sourcemodels and then train a new model on the pseudolabeled data (liu et al., 2017; rahimi et al., 2019)..5https://universaldependencies.org/6we separately evaluate the source language models onthe target test data and choose the best score.
since we don’tknow which source model is the best for dt in practice, thedt-max(lang) results are only for reference..2665with 50labeled data.
conll02/03 neresnlde.
en.
avg..en.
ca.
pos taggingfihiid.
ru.
avg..(cid:55)(cid:55).
(cid:51).
(cid:55)(cid:55)(cid:55).
(cid:51)(cid:51)(cid:51)(cid:51).
(cid:51)(cid:51)(cid:51).
(cid:51)(cid:51).
(cid:51)(cid:51)(cid:51).
dt-golddt-max(lang).
90.13 84.6080.85 74.27.
89.0981.00.
84.30 87.0378.42 78.64.
95.71 96.80 94.67 94.39 92.18 97.39 95.1987.38 94.26 89.33 87.96 82.47 91.71 88.85.dt-finetuning 72.71 54.49.
57.07.
70.82 63.77.
85.58 92.90 86.73 86.17 72.57 87.65 85.27.dt-votedt-maxdt-mean.
81.81 74.5282.21 74.9882.57 75.33.
81.6682.1982.19.
78.51 79.1378.74 79.5378.93 79.76.
89.73 94.26 90.29 89.09 82.82 92.51 89.7889.71 94.49 90.13 89.13 83.97 92.78 90.0490.04 94.38 90.40 89.26 83.72 92.86 90.11.hard-kd-cat79.07 80.17hard-kd-vote79.18 80.2379.40 80.05hard-kd-maxhard-kd-mean 83.42 75.67 82.306 79.29 80.17.
83.73 75.5683.45 75.8083.14 75.39.
82.3082.4882.27.
90.22 94.41 90.60 89.52 84.26 92.80 90.3090.06 94.38 90.52 89.53 83.77 92.65 90.1590.16 94.56 90.45 89.41 84.89 92.99 90.4190.32 94.46 90.67 89.61 84.48 92.96 90.41.ummself-training1tri-training2.
soft-kd-avg3soft-kd-sim4.
ours-langours-sentours-sub.
78.99 75.2680.76 75.9680.63 76.62.
83.52 75.8483.58 75.99.
83.48 75.8883.83 76.1384.78 76.56.
82.4882.9183.14.
82.4682.94.
83.0282.9284.12.
78.26 78.7579.63 79.8179.10 79.87.
88.14 93.88 89.65 88.42 83.03 93.26 89.4089.68 94.46 90.13 89.16 83.72 94.02 90.1989.83 94.40 90.04 89.69 83.94 94.05 90.32.
79.24 80.2679.63 80.54.
90.31 94.62 90.75 89.69 84.55 93.22 90.5289.79 94.80 90.79 89.70 84.55 93.54 90.53.
79.79 80.5480.07 80.7480.34 81.45.
90.27 94.73 90.81 89.62 84.78 93.44 90.6190.31 94.80 90.91 89.71 84.93 93.51 90.7091.12 95.30 91.15 90.11 85.68 93.57 91.16.
1 yarowsky (1995); mcclosky et al.
(2006).
2 ruder and plank (2018).
3,4 wu et al.
(2020).
table 1: results on conll02/03 ner and pos tagging tasks.
the approaches provided for reference is markedas italic.
we compare the best score of our approaches and the best score of the baselines by leveraging almoststochastic dominance (asd) test (dror et al., 2019).
we mark the the highest score as bold if its superiority issigniﬁcant (p < 0.05) and underline otherwise..we obtain the pseudo labels in four ways: 1) us-ing dt-mean (hard-kd-mean); 2) using dt-max(hard-kd-max); 3) using dt-vote (hard-kd-vote);4) concatenating all predictions of source modelsinstead of voting (hard-kd-concat).
for fairly com-parison, we also concatenate the ﬁfty target labeleddata into the pseudo labeled data..soft-kd instead of leveraging hard predictionsof source models in hard-kd, the soft-kd lever-ages soft probability distribution of source models.
the original soft-kd (wu et al., 2020) only focuseson zero-shot ner tasks.
instead, we modify theirtraining objective to leverage ﬁfty target labeleddata and adapt it to pos tagging and dependencyparsing tasks.
(refer to section 2.4 for details.)
we re-implement their two proposed approaches:1) uniformly aggregating multiple source models(kd-avg); 2) aggregating source models by ﬁxedweights pre-trained on source unlabeled data basedon language similarity (kd-sim)7..umm the umm is trained on the concatena-tion of all source languages labeled data and ﬁftylabeled data of target language..bootstrapping bootstrapping approaches ﬁrstlytrain a umm and then add the most conﬁdent sen-.
7for more details of the two approaches, please refer to.
the original paper..tences of target unlabeled data into the training setevery iteration during training.
we compare ourapproaches to self-training (yarowsky, 1995; mc-closky et al., 2006) and tri-training (ruder andplank, 2018)..we provide the upper bound results of dt (dt-gold).
we construct the upper bound using thegold label set in test data by selecting the gold la-bel if any prediction of source models appears inthe gold set.
besides, unlike umm, self-training,tri-training, and kd-sim, our approaches do not re-quire extra resources like source language trainingdata..3.2 results.
we report the results in table 1 for ner and postagging, and 2 for dependency parsing..common results on all tasks as shown in ta-ble 1 and 2, our three proposed approaches out-perform most of the baselines on all tasks, whichdemonstrates the effectiveness of the proposedmulti-view learning framework.
when trainedon only ﬁfty labeled data, the task-speciﬁc modelshows signiﬁcantly poor results especially on de-pendency parsing which veriﬁes our intuition thatthe task-speciﬁc model is easily over-ﬁtted andonly training the task-speciﬁc model is not sufﬁ-cient.
notably, umm, self-training, and tri-training.
2666with 50labeled data.
en.
hiuas las uas las uas las uas las uas las uas las uas las.
avg..ca.
ru.
id.
fi.
(cid:55)(cid:55).
(cid:51).
(cid:55)(cid:55)(cid:55).
(cid:51)(cid:51)(cid:51)(cid:51).
(cid:51)(cid:51)(cid:51).
(cid:51)(cid:51).
(cid:51)(cid:51)(cid:51).
dt-gold.
93.30 87.67 92.80 88.61 89.10 81.80 88.54 80.41 84.65 74.67 92.20 86.20 90.10 83.23dt-max(lang) 77.71 67.90 84.39 76.17 76.86 68.37 70.49 52.64 76.62 56.98 72.31 64.45 76.40 64.42.dt-finetuning 49.75 41.87 53.59 48.38 47.19 38.39 50.32 41.58 32.88 22.22 35.87 28.78 44.93 36.87.dt-votedt-maxdt-mean.
80.94 71.65 83.82 75.81 77.79 66.76 75.98 63.18 68.23 52.82 79.80 69.62 77.76 66.6481.07 71.56 84.29 75.87 77.46 65.78 76.54 63.42 69.13 52.79 79.42 69.22 77.99 66.4481.79 72.96 84.52 76.68 78.45 67.56 76.80 64.31 68.83 54.11 80.54 70.77 78.49 67.73.hard-kd-cat82.16 74.29 84.41 77.13 78.28 68.26 77.26 65.56 69.61 55.80 80.28 70.90 78.67 68.66hard-kd-vote 82.46 74.09 84.47 77.02 78.05 67.99 77.83 65.79 69.39 55.31 80.78 71.44 78.83 68.61hard-kd-max 82.35 74.16 85.13 77.73 77.62 67.45 78.19 66.42 69.49 54.68 80.79 71.52 78.93 68.66hard-kd-mean 82.69 74.61 84.85 77.41 78.11 68.45 78.23 66.45 69.88 56.04 81.15 72.08 79.15 69.17.umm.
82.89 73.44 83.02 73.24 78.28 63.21 75.36 61.38 66.85 49.13 80.40 70.84 77.80 65.21self-training1 83.89 74.64 83.76 74.10 79.01 63.31 77.56 63.31 67.95 50.39 80.78 72.20 78.82 66.33tri-training 2 83.97 74.64 83.80 75.34 79.17 63.49 77.94 63.89 68.35 51.07 80.51 71.84 78.96 66.71.soft-kd-avg3 82.07 74.64 84.80 77.82 78.18 68.73 78.27 67.46 68.90 54.84 80.83 72.12 78.84 69.27soft-kd-sim4 81.49 72.46 85.49 78.39 77.59 67.90 78.28 67.38 68.63 54.58 80.93 72.19 78.74 68.82.ours-langours-sentours-sub.
82.07 74.67 84.94 78.03 78.26 68.76 78.62 67.78 68.66 54.49 81.10 72.62 78.94 69.3982.33 74.89 85.25 78.10 78.62 69.03 78.74 67.91 69.06 56.13 81.19 72.54 79.20 69.7783.95 76.67 86.00 79.25 79.41 70.13 79.40 68.58 72.36 60.21 82.15 73.70 80.54 71.42.
1 yarowsky (1995); mcclosky et al.
(2006).
2 ruder and plank (2018).
3,4 wu et al.
(2020).
table 2: results on the dependency parsing task.
(refer to the caption of table 1 for the format detail.).
do not yield improvements compared to hard-kd-*, soft-kd-*, and ours-*, verifying our motivationthat simply concatenating all training data is notsufﬁcient to model the difference between multiplesources.
we also observe that our three approachesoutperform the two kd approaches consistently, in-dicating that their simple or heuristic-based aggre-gation strategies are difﬁcult to assess the diversequality of source models.
it is also worth noticingthat with a more ﬁne-grained aggregated sourceview, the target view has stronger performance,especially for ours-sub 8. even though umm,self-training, tri-training, and soft-kd-sim all uti-lize source language training data during training,ours-sub achieves remarkable advantage over thesebaselines without the extra resources, especially fordependency parsing..other results although tri-training achieves thehighest score and uas on de of ner and en ofparsing respectively, it is not statistically signif-icant compared to ours-sub and the gap is verymarginal (< 0.1%).
for ner task, it is probablydue to the difference of the capitalization style be-tween de and other languages on conll ner(chen et al., 2019), which may lead to the negativetransfer problem 9. besides, the gaps between the.
8this is mainly due to the stronger cross-lingual ability ofthe aggregated source view.
we further analyze this in section4.1..9we speculate that kd-based approaches also suffer fromthis problem and lead to low results.
our sub-structure-level.
dt-gold and the best transfer approaches suggestthe large potential space on multi-source transfertasks..4 analysis.
4.1 why the multi-view framework works?.
in this section, we study the reason why the pro-posed framework works.
we show the performanceof the aggregated source view in figure 2. it can beseen that with a more ﬁne-grained strategy, the per-formance of the aggregated source view becomesstronger.
it demonstrates the effectiveness of moreﬁne-grained aggregation strategies in the multi-source transfer.
the only counter case is languageand sentence level on nl, and the performanceof the target view drops accordingly.
connectingto table 1, the target view has the same trends.
the reason is probably that the stronger aggregatedsource view can lead to a stronger target view andvice versa, and the framework achieves a goodtrade-off to make them both improved..4.2 ablation study.
to further understand the proposed framework weinvestigate the component contributions.
we gradu-ally remove some components of our sub-structure-tkl, and evaluatece andlevel model, i.e.,l.sce,l.l.approach is the second-best system in this case, indicatingthat it can alleviate this problem by better leveraging labeleddata to access the conﬁdence level of source models on moreﬁne-grained-level property..2667unlabeled data or labeled data increases (the aggre-gation view generally shows comparable or evensuperior results to the target view with fewer data).
this veriﬁes our motivation that there exists a trade-off between two views.
with #0 unlabeled data,the task-speciﬁc model is over-ﬁtted when onlytrained on #200 or less labeled data..target labeled data:#50.
#10.
#200.
#1000.a #0.
1.22 — 49.13 — 68.53 — 77.01 —.
#1000 68.95 70.38 73.42 75.59 75.75 76.11 77.47 77.18.
#2000 70.94 71.09 75.18 76.15 76.54 76.37 78.48 77.66.
#4000 71.78 71.89 76.49 76.41 77.52 76.59 78.77 77.69.all 74.61 74.66 76.56 76.44 78.26 77.07 79.18 77.76.taddelebalnutegrat.table 4: results on different sizes of target unlabeleddata and labeled data on de of ner tasks.
in each cell,the right (underlined) and left part denote the results ofthe aggregated source view and target view respectively..5 related work.
cross-lingual structured prediction compar-ing to single-source transfer,the multi-sourcetransfer shows superior performance by leverag-ing multi-source language knowledge (mcdonaldet al., 2011; rahimi et al., 2019; hu et al., 2021).
however, the diverse quality of source modelssorely hurt the target model.
to tackle this chal-lenging problem, ammar et al.
(2016) leveragelanguage embeddings to model language topolog-ical similarities.
rahimi et al.
(2019) utilize truthinference to obtain the best labeling over multi-ple unreliable predictors.
hu et al.
(2021) mod-els the relations between the predicted labels fromthe source models and the true labels.approachesbased on the similarity of source and target data arewidely studied (chen et al., 2019; wu et al., 2020)..multi/cross-view learning multi-view learn-ing learns multiple representations for the targetdata.
tri-training approaches (zhou and li, 2005;ruder and plank, 2018) leverage voting on threeseparate models to select conﬁdent sentences.
jianget al.
(2019); cai and lapata (2020) utilize simi-larity metrics to regularize source-target languagepairs.
multi-view learning can also be utilized intraining ner models with different kinds of in-put components (wang et al., 2021).
cross-viewlearning (clark et al., 2018) is a semi-supervised ap-proach that aims to boost the monolingual model’sperformance.
it learns only one model with sev-eral auxiliary prediction modules which are treated.
figure 2: performance of the aggregated source viewon conll02/03 ner tasks..full modelw/o lscew/o ltcece & lt.w/o lkl.
w/o ls.
en de nl.
es avg..84.95 76.16 83.85 79.79 81.19.
84.92 75.89 83.38 79.49 80.9284.88 75.59 83.33 79.32 80.78ce 84.34 74.07 83.07 79.19 80.1772.59 54.60 57.19 71.06 63.86.l.table 3: ablation study of ours-sub model onconll02/03 ner task.
w/o denotes ‘without’.
on the ner task.
we report the average results oftwenty-ﬁve runs 10 in table 3. withoutkl theapproach degenerates into supervised training withonly ﬁfty labeled data and it leads to the largestdrop in performance.
it is because the model iseasily over-ﬁtted.
though the performance dropsstce, it still outperformwithout one ofce andllsce leads to lesskd-* baselines of table 1. w/oltdrops than w/oce, which suggest that the labeledldata inﬂuence more in the target model.
besides,without both cross-entropy loss of labeled data, theapproach degenerates into a zero-shot manner andresults in inferior performance..4.3 different sizes of unlabeled data or.
labeled data.
{.
10, 50, 200, 1000}.
in this section, we study the impact of the sizesof labeled data and unlabeled data on the tar-get language for the ours-sub model.
we ran-domly selectlabeled data and1000, 2000, 4000, allunlabeled data.
we repeat{}each experiment ﬁve times and report the aver-age results of both two views 11. it can be seenthat with more labeled data or unlabeled data, theresults both become higher and the labeled datashows higher inﬂuence than the unlabeled data.
unlike the aggregated source view, the target viewgains signiﬁcantly larger boosts when the size of.
10we randomly select ﬁve different copies of labeled data.
and run ﬁve times for each copy..11we only show the de results due to the space limitation.
the results of the other three languages can be found in theappendix c.2..2668757779818385ennldeesf1 scorelangsentsubas different views.
in contrast to it, we focus onthe cross-lingual scenario and our two views are atarget task-speciﬁc model and the aggregation ofmultiple pre-trained source models..contextual multilingual language modeltrained on massive unlabeled data of hundreds ofmonolingual corpus, the contextual multilingualmodels (devlin et al., 2019; conneau et al.,2020) learn common representations for multiplelanguages.
though cross-lingual transfer learningsigniﬁcantly beneﬁts from these models (pireset al., 2019; wu and dredze, 2019b), large gapsstill remain between low and high-resources setups(hu et al., 2020a; wu and dredze, 2020)..6 conclusion.
we propose a novel multi-view framework to selec-tively transfer knowledge from multiple sourcesby utilizing a small amount of labeled dataset.
experimental results show that our approachesachieve state-of-the-art performances on all tasks.
moreover, even compared to approaches with ex-tra resources like source language data, our sub-structure-level approach still shows signiﬁcant im-provements..acknowledgement.
this work was supported by the national natu-ral science foundation of china (61976139) andby alibaba group through alibaba innovative re-search program.
we thank yuting zhen for hersupport in processing datasets and conducting sig-niﬁcance tests..references.
wasi ahmad, zhisong zhang, xuezhe ma, eduardhovy, kai-wei chang, and nanyun peng.
2019. ondifﬁculties of cross-lingual transfer with order differ-ences: a case study on dependency parsing.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 2440–2452,minneapolis, minnesota.
association for computa-tional linguistics..a. akbik, t. bergmann, and roland vollgraf.
2019.multilingual sequence labeling with one model.
innldl 2019, northern lights deep learning work-shop..guages, one parser.
transactions of the associationfor computational linguistics, 4:431–444..mark anderson and carlos g´omez-rodr´ıguez.
2020.distilling neural networks for greener and faster de-in proceedings of the 16th in-pendency parsing.
ternational conference on parsing technologies andthe iwpt 2020 shared task on parsing into en-hanced universal dependencies, pages 2–13, on-line.
association for computational linguistics..mikel artetxe, sebastian ruder, dani yogatama,gorka labaka, and eneko agirre.
2020. a callfor more rigor in unsupervised cross-lingual learn-in proceedings of the 58th annual meetinging.
of the association for computational linguistics,pages 7375–7388, online.
association for compu-tational linguistics..rui cai and mirella lapata.
2020. alignment-freein proceed-cross-lingual semantic role labeling.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages3883–3894, online.
association for computationallinguistics..danqi chen and christopher manning.
2014. a fastand accurate dependency parser using neural net-in proceedings of the 2014 conference onworks.
empirical methods in natural language processing(emnlp), pages 740–750, doha, qatar.
associationfor computational linguistics..xilun chen, ahmed hassan awadallah, hany has-san, wei wang, and claire cardie.
2019. multi-source cross-lingual model transfer: learning whatto share.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 3098–3112, florence, italy.
associationfor computational linguistics..kevin clark, minh-thang luong, christopher d. man-ning, and quoc le.
2018.semi-supervised se-quence modeling with cross-view training.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1914–1925, brussels, belgium.
association for computa-tional linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..steven j. derose.
1988. grammatical category disam-biguation by statistical optimization.
computationallinguistics, 14(1):31–39..waleed ammar, george mulcaire, miguel ballesteros,chris dyer, and noah a. smith.
2016. many lan-.
jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
2669deep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d manning.
2016.deep biafﬁne attention for neural dependency pars-ing.
in international conference on learning rep-resentations..rotem dror, segev shlomov, and roi reichart.
2019.deep dominance - how to properly compare deepneural models.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2773–2785, florence, italy.
associa-tion for computational linguistics..hongliang fei and ping li.
2020. cross-lingual un-supervised sentiment classiﬁcation with multi-viewtransfer learning.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 5759–5771, online.
association forcomputational linguistics..jiang guo, darsh shah, and regina barzilay.
2018.multi-source domain adaptation with mixture of ex-in proceedings of the 2018 conference onperts.
empirical methods in natural language processing,pages 4694–4703, brussels, belgium.
associationfor computational linguistics..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020a.
xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in proceedings of the 37th internationalconference on machine learning, volume 119 ofproceedings of machine learning research, pages4411–4421.
pmlr..zechuan hu, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2020b.
an investigation of potential function de-signs for neural crf.
in findings of the associationfor computational linguistics: emnlp 2020, pages2600–2609, online.
association for computationallinguistics..zechuan hu, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021.risk minimization for zero-shot sequence labeling.
in the joint conference of the 59th annual meet-ing of the association for computational linguisticsand the 11th international joint conference on natu-ral language processing (acl-ijcnlp 2021).
as-sociation for computational linguistics..yong jiang, wenjuan han, and kewei tu.
2019. aregularization-based framework for bilingual gram-in proceedings of the 2019 con-mar induction.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1423–1428, hong kong, china.
as-sociation for computational linguistics..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..yu-hsiang lin, chian-yu chen, jean lee, zirui li,yuyan zhang, mengzhou xia, shruti rijhwani,junxian he, zhisong zhang, xuezhe ma, antoniosanastasopoulos, patrick littell, and graham neubig.
2019. choosing transfer languages for cross-linguallearning.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 3125–3135, florence, italy.
associationfor computational linguistics..liyuan liu, xiang ren, qi zhu, shi zhi, huan gui,heng ji, and jiawei han.
2017. heterogeneous su-pervision for relation extraction: a representationlearning approach.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 46–56..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..david mcclosky, eugene charniak, and mark johnson.
in pro-2006. effective self-training for parsing.
ceedings of the human language technology con-ference of the naacl, main conference, pages 152–159..ryan mcdonald, slav petrov, and keith hall.
2011.multi-source transfer of delexicalized dependencyparsers.
in proceedings of the 2011 conference onempirical methods in natural language processing,pages 62–72, edinburgh, scotland, uk.
associationfor computational linguistics..telmo pires, eva schlinger, and dan garrette.
2019.in pro-.
how multilingual is multilingual bert?.
2670ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..barbara plank and ˇzeljko agi´c.
2018. distant super-vision from disparate sources for low-resource part-of-speech tagging.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, pages 614–620, brussels, belgium.
as-sociation for computational linguistics..lonneke van der plas, paola merlo, and james hen-derson.
2011. scaling up automatic cross-lingualin proceedings of thesemantic role annotation.
49th annual meeting of the association for com-putational linguistics: human language technolo-gies, pages 299–304, portland, oregon, usa.
asso-ciation for computational linguistics..afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..michael t rosenstein, zvika marx, leslie pack kael-bling, and thomas g dietterich.
2005. to transferor not to transfer.
in in nips’05 workshop, induc-tive transfer: 10 years later.
citeseer..sebastian ruder and barbara plank.
2017. learning toselect data for transfer learning with bayesian opti-mization.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 372–382, copenhagen, denmark.
associationfor computational linguistics..sebastian ruder and barbara plank.
2018. strong base-lines for neural semi-supervised learning under do-main shift.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1044–1054, mel-bourne, australia.
association for computationallinguistics..patrick verga, daniel andor,emma strubell,david weiss,and andrew mccallum.
2018.linguistically-informed self-attention for semanticin proceedings of the 2018 confer-role labeling.
ence on empirical methods in natural languageprocessing, pages 5027–5038, brussels, belgium.
association for computational linguistics..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
in.
proceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..mengqiu wang and christopher d. manning.
2014.cross-lingual projected expectation regularizationfor weakly supervised learning.
transactions of theassociation for computational linguistics, 2:55–66..xinyu wang, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021.improving named entity recognition by externalcontext retrieving and cooperative learning.
inthe joint conference of the 59th annual meeting ofthe association for computational linguistics andthe 11th international joint conference on naturallanguage processing (acl-ijcnlp 2021).
associ-ation for computational linguistics..qianhui wu, zijia lin, b¨orje karlsson, jian-guanglou, and biqing huang.
2020. single-/multi-sourcecross-lingual ner via teacher-student learning onin proceedingsunlabeled data in target language.
of the 58th annual meeting of the association forcomputational linguistics, pages 6505–6514, on-line.
association for computational linguistics..shijie wu and mark dredze.
2019a.
beto, bentz, be-cas: the surprising cross-lingual effectiveness ofin proceedings of the 2019 conference onbert.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844..shijie wu and mark dredze.
2019b.
beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..shijie wu and mark dredze.
2020. are all languagescreated equal in multilingual bert?
in proceedingsof the 5th workshop on representation learning fornlp, pages 120–130, online.
association for com-putational linguistics..david yarowsky.
1995. unsupervised word sense dis-in 33rdambiguation rivaling supervised methods.
annual meeting of the association for computationallinguistics, pages 189–196..david yarowsky and grace ngai.
2001. inducing mul-tilingual pos taggers and np bracketers via robustprojection across aligned corpora.
in second meet-ing of the north american chapter of the associa-tion for computational linguistics..2671michihiro yasunaga,.
jungo kasai, and dragomirradev.
2018. robust multilingual part-of-speechtagging via adversarial training.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 976–986, new orleans, louisiana.
as-sociation for computational linguistics..lockerbie -.
juicio chavez pide ayuda a ....en.
de.
nl.
boobo.
o b-per.
b-loc.
o b-per.
boobo.
mean.
boobo.
o b-per.
i-per.
o.o.o.o.o.o.best.
gold.
boobo.
b-loc.
i-per.
i-per.
b-per.
b-per.
b-per.
o.o.o.o.o.o.b-loc o ....o.o.o.o.o.o ....o ....o ....o ....o ....zhi-hua zhou and ming li.
2005. tri-training: ex-ploiting unlabeled data using three classiﬁers.
ieeetransactions on knowledge and data engineering,17(11):1529–1541..a examples mentioned in the.
introduction.
example #1in practice, we are more likely to en-counter the situation where some source languagesare not as similar to the target language and maylead to worse performance (rosenstein et al., 2005;rahimi et al., 2019).
we show the example in table5. the results show that for a target language, thegap between the score of different source modelscan be large (> 10%)..source \ target.
en.
de.
nl.
es.
endenles.
— 72.77 79.47 75.1375.96 — 78.47 70.7469.38 72.35 — 74.1668.55 63.37 69.12 —.
transfer results on thetable 5: direct bilingualconll02/03 ner task measured in f1 scores (%).
we use the multilingual bert (mbert) (devlin et al.,2019) stacked by a softmax layer to train a sourcemodel.
each source model is pre-trained on the labeledtraining data of the source language and directly evalu-ated on the target language test data.
for a target lan-guage, the gap between the highest and lowest scoresranges from 4.4%-10.4%..example #2 the model/language level weightscan not well capture the diverse strength and weak-ness of multiple source models.
for example intable 6 12, none of the three source models predictcorrectly on the whole sequence, but selecting pre-dictions based on the sub-structure level can obtainthe correct label sequence..table 6: a negative transfer example on spanish tar-get language.
the three pre-trained source models areobtained in the same way in table 5. except the sen-tence of the ﬁrst row, each row represent predictionsfrom english (en), german (de), dutch (nl) sourcemodels and gold labels respectively.
mean and bestdenote the predictions from the uniform and the bestweights of three sources’ distributions on sentence levelrespectively.
labels with red background denote wrongpredictions.
each source has its advantages on sub-structure level..b experimental details.
b.1 tasks.
dependency parsing we randomly select ﬁvelanguages together with the english dataset fromuniversal dependencies treebanks (v2.2) for de-pendency tasks.
the whole datasets are english(en), catalan (ca), finnish (fi), indonesian (id),hindi (hi), and russian (ru).
we do not use syntac-tic information like gold pos tags as many super-vised dependency parsers do since we can’t assumethey are accessible in practice especially for low-resource languages.
even though we can obtainpseudo tags by pre-trained pos taggers of high-resource language, it may introduce unexpectednoises and disturb the experiments..named entity recognition we use the datasetsfrom conll 2002 and conll 2003 shared tasks(tjong kim sang, 2002; tjong kim sang andde meulder, 2003), which consist of four lan-guages: en, german (de), dutch (nl), and spanish(es).
each dataset contains four named entity types:organization, person, location, and miscellaneous.
we use the standard splits with the bio annotationscheme..12in this example, three pseudo predictions are from threesource models pre-trained on en, de, and nl training setrespectively.
the three pre-trained source models are obtainedin the same way in table 5..pos tagging for the pos tagging task, we usethe same six datasets as the dependency parsingtask..2672english german dutch spanish avg..multilingual77.13self-training1 80.57tri-training280.99kd-avg3kd-sim4ours-sub.
83.6983.70.
84.95.
75.0875.7775.99.
75.9175.92.
76.16.
81.9582.4482.54.
82.5982.76.
83.85.
77.6078.4978.28.
79.2079.39.
77.9479.3279.45.
80.3580.44.
79.79.
81.19.
1 yarowsky (1995); mcclosky et al.
(2006)3,4 wu et al.
(2020).
2 ruder and plank (2018).
randomness.
table 7: the average results ( twenty-ﬁve runs)oftest of ﬁfty labeled data onconll02/03 ner task.
we randomly select ﬁvedifferent copies of ﬁfty labeled data (ﬁve runs foreach copy)..parsing task over the languages that are drasticallydifferent from each other.
the sources are english,mandarin, arabic, and vietnamese, and the targetis turkish.
in this setting, the source languagesare drastically different from the target language.
our results show that ours-sub (uas 59.11, las45.02) still outperforms the strongest baseline (kd,uas 58.69, las 44.36)..c.1 effects of random seeds on labeled.
data.
we further explore the effects of randomness onlabeled data as mentioned in section 3 of the mainpaper.
we randomly select ﬁve different copies ofﬁfty labeled data to validate its inﬂuence.
we com-pare our sub-structure-level model to kd-* andumm based approaches on conll02/03 nertask.
the results are shown in table 7. ours-substill consistently outperforms the second-best base-line, which demonstrates that our approach is ro-bust to randomness in the selection of labeled data..c.2 different sizes of unlabeled data or.
labeled data.
in table 8, we provide the whole analysis resultsmentioned in section 4.3 of the main paper in thissection..b.2 model conﬁgurationwe utilize the base cased multilingual bert 13(devlin et al., 2019) which has 12 transformerblocks, 12 attention heads, and 768 hidden units.
before model training, the k source models arepre-trained with the corresponding source languagetraining sets 14..evaluation we select the best hyper-parametersbased on the score of the development set on high-resources language, which is english in practice,and adopt the hyper-parameters to other languages.
this may lead to sub-optimal results for other lan-guages but is more realistic (artetxe et al., 2020)..b.3 hyper-parameters.
we select hyper-parameters based on the perfor-mance on the english development set and applythem to other target languages.
we search the bestlearning rate for the mbert model of all the ap-2e5, 3eproaches in the range of, and{set it to 2e5 for its best performance.
we list theimportant hyper-parameters as follows..5−.
5, 5e.
−.
−.
−.
}.
learning rate for the top layer the toplayer’s learning rate is generally larger than thatof mbert.
we search the best learning rate in the4, 2erange of.
3, 2e.
2e{.
−.
−.
5−.
.
}.
interpolations there are three interpolationhyper-parameters in our framework: λ1, λ2, andλ3 in section 2.4 of the main paper.
we tune it inthe range of.
0.5, 1, 3, 10}{sample ratio there is a hyper-parameter µfor controlling the ratio of the labeled data andunlabeled data.
we tune itin the range of0.05, 0.1, 0.3, 0.5, 0.7.
....{c additional analysis.
}.
linguistic diversity when all the source lan-guages are different from the target language, thesource models generally have poor quality and thetarget model cannot beneﬁt much from the sourcemodels.
in this case, the cross-lingual transfer ismore difﬁcult.
intuitively, our approaches can dy-namically learn the conﬁdence level of multiplesource models and still facilitate cross-lingual trans-fer in this case.
we experiment on the dependency.
13https://huggingface.co/bert-base-multilingual-cased.
14in practice, we can obtain the released pre-trained sourcemodels on the open-source community, and thus there is noneed to use source language data..2673target labeled data:.
en.
#10.
#50.
#200.
#1000.
#10.
#50.
#200.
#1000.a #0.
0.84 — 71.13 — 81.85 — 87.07 — 1.22 — 49.13 — 68.53 — 77.01 —.
#1000 79.45 79.38 82.71 82.56 85.06 84.37 87.01 85.73 68.95 70.38 73.42 75.59 75.75 76.11 77.47 77.18.
#2000 82.59 82.13 84.23 84.12 85.43 84.87 87.40 85.77 70.94 71.09 75.18 76.15 76.54 76.37 78.48 77.66.
#4000 83.92 83.99 84.27 84.22 85.64 85.00 87.45 85.80 71.78 71.89 76.49 76.41 77.52 76.59 78.77 77.69.all.
84.73 84.71 84.78 84.72 86.34 85.4 87.46 85.78 74.61 74.66 76.56 76.44 78.26 77.07 79.18 77.76.nl.
#10.
#50.
#200.
#1000.
#10.
#50.
#200.
#1000.a #0.
1.91 — 35.97 — 73.53 — 85.21 — 0.31 — 63.45 — 78.36 — 82.55 —.
#1000 78.30 80.66 79.68 81.61 81.50 83.05 84.95 83.64 77.85 78.34 77.86 78.72 79.42 79.4 82.12 80.26.
#2000 81.51 82.23 81.64 82.60 82.70 83.41 85.13 84.00 78.70 78.67 79.00 79.13 80.01 79.49 82.24 80.57.
#4000 83.15 82.87 82.67 83.11 83.83 83.75 85.43 84.27 79.15 79.13 79.71 79.27 80.14 79.42 82.57 80.61.all.
83.32 83.31 84.14 83.56 85.01 84.06 86.59 84.35 79.71 79.22 80.20 79.35 80.15 79.65 82.56 80.57.taddelebal.nutegrat.taddelebal.nutegrat.de.
es.
table 8: results on different sizes of target unlabeled data and labeled data.
the numbers of vertical and horizontalaxis denote the unlabeled data sizes and labeled data sizes respectively.
in each cell, the right (underlined) and leftpart denote the results of the aggregated source view and target view respectively..2674