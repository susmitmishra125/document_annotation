tan-ntm: topic attention networks for neural topic modeling.
madhur panwar2∗†, shashank shailabh3∗†, milan aggarwal1∗, balaji krishnamurthy1media and data science research labs, adobe1birla institute of technology and science, pilani (bits pilani), india2indian institute of technology kanpur (iit kanpur), india3mdrpanwar@gmail.com, shailabhshashank@gmail.com.
abstract.
topic models have been widely used to learntext representations and gain insight into doc-ument corpora.
to perform topic discovery,most existing neural models either take docu-ment bag-of-words (bow) or sequence of to-kens as input followed by variational inferenceand bow reconstruction to learn topic-worddistribution.
however, leveraging topic-worddistribution for learning better features dur-ing document encoding has not been exploredmuch.
to this end, we develop a frameworktan-ntm, which processes document as a se-quence of tokens through a lstm whose con-textual outputs are attended in a topic-awaremanner.
we propose a novel attention mech-anism which factors in topic-word distributionto enable the model to attend on relevant wordsthat convey topic related cues.
the output oftopic attention module is then used to carryout variational inference.
we perform exten-sive ablations and experiments resulting in ∼9 - 15 percentage improvement over score ofexisting sota topic models in npmi coher-ence on several benchmark datasets - 20news-groups, yelp review polarity and agnews.
further, we show that our method learns bet-ter latent document-topic features comparedto existing topic models through improvementon two downstream tasks: document classiﬁ-cation and topic guided keyphrase generation..1.introduction.
topic models (steyvers and grifﬁths, 2007) havebeen popularly used to extract abstract topics whichoccur commonly across documents in a corpus.
each topic is interpreted as a group of semanticallycoherent words that represent a common concept.
in addition to gaining insights from unstructuredtexts, topic models have been used in several tasks.
∗equal contribution† work done during summer internship at adobe.
of practical importance such as learning text repre-sentations for document classiﬁcation (nan et al.,2019), keyphrase extraction (wang et al., 2019b),understanding reviews for e-commerce recommen-dations (jin et al., 2018), semantic similarity detec-tion between texts (peinelt et al., 2020) etc..early works on topic discovery include statis-tical methods such as latent semantic analysis(deerwester et al., 1990), latent dirichlet alloca-tion (lda) (blei et al., 2003) which approximateseach topic as a probability distribution over wordvocabulary (known as topic-word distribution) andperforms approximate inference over document-topic and topic-word distributions through varia-tional bayes.
this was followed by markov chainmonte carlo (mcmc) (andrieu et al., 2003) basedinference algorithm - collapsed gibbs sampling(grifﬁths and steyvers, 2004).
these methods re-quire an expensive iterative inference step whichhas to be performed for each document.
this wascircumvented through introduction of deep neu-ral networks and variational autoencoders (vae)(kingma and welling, 2013), where variational in-ference can be performed in single forward pass..neural variational inference topic models (miaoet al., 2017; ding et al., 2018; srivastava and sut-ton, 2017) commonly convert a document to bag-of-words (bow) determined on the basis of fre-quency count of each vocabulary token in the doc-ument.
the bow input is processed through anmlp followed by variational inference which sam-ples a latent document-topic vector.
a decoder net-work then reconstructs original bow using latentdocument-topic vector through topic-word distribu-tion (twd).
vae based neural topic models canbe categorised on the basis of prior enforced onlatent document-topic distribution.
methods suchas nvdm (miao et al., 2016), ntm-r (ding et al.,2018), nvdm-gsm (miao et al., 2017) use thegaussian prior.
nvlda and prodlda (srivastava.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3865–3880august1–6,2021.©2021associationforcomputationallinguistics3865and sutton, 2017) use approximation to the dirich-let prior which enables model to capture the factthat a document stems from a sparse set of topics..ation achieving sota performance on stackex-change and weibo datasets.
our contributions canbe summarised as:.
however, improving document encoding in topicmodels in order to capture document distributionand semantics better has not been explored much.
in this work, we build upon vae based topicmodel and propose a novel framework tan-ntm:topic attention networks for neural topic mod-eling which process the sequence of tokens in in-put document through an lstm (hochreiter andschmidhuber, 1997) whose contextual outputs areattended using topic-word distribution (twd).
we hypothesise that twd (being learned by themodel) can be factored in the attention mechanism(bahdanau et al., 2014) to enable the model to at-tend on the tokens which convey topic related in-formation and cues.
we perform separate attentionfor each topic using its corresponding word proba-bility distribution and obtain the topic-wise contextvectors.
the learned word embeddings and twdare used to devise a mechanism to determine topicweights representing the proportion of each topicin the document.
the topic weights are used to ag-gregate topic-wise context vectors.
the composedcontext vector is then used to perform variationalinference followed by the bow decoding.
we per-form extensive ablations to compare tan-ntmvariants and different ways of composing the topic-wise context vectors..for evaluation, we compute commonly usednpmi coherence (aletras and stevenson, 2013)which measures the extent to which most probablewords in a topic are semantically related to eachother.
we compare our tan-ntm model with sev-eral state-of-the-art topic models (statistical (bleiet al., 2003; grifﬁths and steyvers, 2004), neuralvae (srivastava and sutton, 2017; wu et al., 2020)and non-variational inference based neural model(nan et al., 2019)) outperforming them on threebenchmark datasets of varying scale and complex-ity: 20newsgroups (20ng) (lang, 1995), yelpreview polarity and agnews (zhang et al., 2015).
we verify that our model learns better documentfeature representations and latent document-topicvectors by achieving a higher document classiﬁca-tion accuracy over the baseline topic models.
fur-ther, topic models have previously been used toimprove supervised keyphrase generation (wanget al., 2019b).
we show that tan-ntm can beadapted to modify topic assisted keyphrase gener-.
• we propose a document encoding frameworkfor topic modeling which leverages the topic-word distribution to perform attention effec-tively in a topic aware manner..• our proposed model achieves better npmicoherence (∼9-15 percentage improvementover the scores of existing best topic models)on various benchmark datasets..• we show that the topic guided attention re-sults in better latent document-topic featuresachieving a higher document classiﬁcation ac-curacy than the baseline topic models..• we show that our topic model encoder canbe adapted to improve the topic guided su-pervised keyphrase generation achieving im-proved performance on this task..2 related work.
development of neural networks has paved pathfor variational autoencoders (vae) (kingma andwelling, 2013) which enables performing varia-tional inference (vi) efﬁciently.
the vae-basedtopic models use a prior distribution to approxi-mate the posterior for latent document-topic spaceand compute the evidence lower bound (elbo)using the reparametrization trick.
since our workis based on variational inference, we use prodldaand nvlda (srivastava and sutton, 2017) as base-lines for comparison.
the dirichlet distribution hasbeen commonly considered as a suitable prior onthe latent document-topic space since it capturesthe property that a document belongs to a sparsesubset of topics.
however, in order to enforce thedirichlet prior, vae methods have to resort to ap-proximations of the dirichlet distribution..several works have proposed solutions to imposethe dirichlet prior effectively.
rezaee and ferraro(2020) enforces dirichlet prior using vi withoutreparametrization trick through word-level topicassignments.
some works address the sparsity-smoothness trade-off in dirichlet distribution byfactoring dirichlet parameter vector as a product oftwo vectors (burkhardt and kramer, 2019).
wasser-stein autoencoders (wae) (tolstikhin et al., 2017)have led to the development of non-variational in-ference based topic model: wasserstein-lda (w-lda) which minimizes the wasserstein distance, a.
3866type of optimal transport (ot) distance, by lever-aging distribution matching to the dirichlet prior.
we compare our work with w-lda as a baseline.
zhao et al.
(2021) proposed an ot based topicmodel which directly calculates topic-word distri-bution without a decoder..adversarial topic model (atm) (wang et al.,2019a) was proposed based on gan (generativeadversarial network) (goodfellow et al., 2014)but it cannot infer document-topic distribution.
amajor advantage of w-lda over atm is distribu-tion matching in document-topic space.
bidirec-tional adversarial topic model (bat) (wang et al.,2020) employs a bilateral transformation betweendocument-word and document-topic distribution,while hu et al.
(2020) uses cyclegan (zhu et al.,2017) for unsupervised transfer between document-word and document-topic distribution..hierarchical topic models (viegas et al., 2020)utilize relationships among the latent topics.
su-pervised topic models have been explored previ-ously where the topic model is trained throughhuman feedback (kumar et al., 2019) or with atask speciﬁc network simultaneously such thattopic extraction is guided through task labels (per-gola et al., 2019; wang and yang, 2020).
cardet al.
(2018) leverages document metadata but with-out metadata their method is same as prodldawhich is our baseline.
topic modeling on doc-ument networks has been done leveraging rela-tional links between documents (zhang and lauw,2020; zhou et al., 2020).
however our problem set-ting is completely different, we extract topics fromdocuments in unsupervised way where documentlinks/metadata/labels either don’t exist or are notused to extract the topics..some very recent works use pre-trained bert(devlin et al., 2019) either to leverage improvedtext representations (bianchi et al., 2020; sia et al.,2020) or to augment topic model through knowl-edge distillation (hoyle et al., 2020a).
zhu et al.
(2020) and dieng et al.
(2020) jointly train wordsand topics in a shared embedding space.
how-ever, we train topic-word distribution as part ofour model, embed it using word embeddings beinglearned and use resultant topic embeddings to per-form attention over sequentially processed tokens.
idocnade (gupta et al., 2019) is an autoregressivetopic model for short texts utilizing pre-trained em-beddings as distributional prior.
however, it attainspoorer topic coherence than prodlda and gnb-.
ntm as shown in wu et al.
(2020)..some works have attempted to use other priordistributions such as zhang et al.
(2018) uses theweibull prior, thibaux and jordan (2007) usesthe beta distribution.
gamma negative binomial-neural topic model (gnb-ntm) (wu et al., 2020)is one of the recent neural variational topic modelswhich attempt to combine vi with mixed count-ing models.
mixed counting models can bettermodel hierarchically dependent and over-dispersedrandom variables while implicitly introducing non-negative constraints in topic modeling.
gnb-ntmuses reparameterization of gamma distribution andgaussian approximation of poisson distribution.
we use their model as a baseline for our work..topic models have been used with sequence en-coders such as lstm in applications like user ac-tivity modeling (zaheer et al., 2017).
dieng et al.
(2016) employs an rnn to detect stop words andmerges its output with document-topic vector fornext word prediction.
gururangan et al.
(2019) usesa vae pre-trained through topic modeling to per-form text classiﬁcation.
we perform document clas-siﬁcation and compare our model’s accuracy withthe accuracy of vae based and other topic mod-els.
ltmf (jin et al., 2018) combines text featuresprocessed through an lstm with a topic modelfor review based recommendations.
fundamentallydifferent from these, we use topic-word distribu-tion to attend on sequentially processed tokens vianovel topic guided attention for performing vari-ational inference, learning better document-topicfeatures and improving topic modeling..a key application of topic models is super-vised keyphrase generation.
some of the exist-ing neural keyphrase generation methods includeseq-tag (zhang et al., 2016) based on sequencetagging, seq2seq-corr (chen et al., 2018)based on seq2seq model without copy mecha-nism and seq2seq-copy (meng et al., 2017)which additionally uses copy mechanism.
topic-aware keyphrase generation (takg) (wang et al.,2019b) is a seq2seq based neural keyphrase gener-ation framework for social media language.
takguses a neural topic model in miao et al.
(2017) anda keyphrase generation (kg) module which is con-ditioned on latent document-topic vector from thetopic model.
we adapt our proposed topic modelto takg to improve keyphrase generation and dis-cuss it in detail later in the experiments section..3867figure 1: a-e: architecture of tan-ntm showing ﬂow of document processing through it.
document, beingembedded using embedding layer, is processed by lstm, yielding hidden states on which tan attends in a topicaware manner.
the resultant context vector is used to perform variational inference and processed through a bowdecoder as in vaes.
attention module e (zoomed in view of c) computes the blocks in the mentioned order 1-6..3 background.
lda is a generative statistical model and assumesthat each document is a distribution over a ﬁxednumber of topics (say k) and that each topic isa distribution of words over the entire vocabulary.
lda proposes an iterative process of documentgeneration where for each document d, we drawa topic distribution θ from dirichlet(α) distribu-tion.
for each word in d at index i, we samplea topic ti from m ultinomial(θ) distribution.
wiis sampled from p(wi|ti, β) distribution which isa multinomial probability conditioned on topic ti.
given the document corpus and the parameters αand β, we need the joint probability distributionof a topic mixture θ, a set of k topics t, and a setof n words w. this is given analytically by an in-tractable integral.
the solution is to use variationalinference wherein this problem is converted intoan optimization problem for ﬁnding various param-eters that minimize the kl divergence between theprior and the posterior distribution..this idea is leveraged at scale by the use of vari-ational autoencoders.
the encoder processes bowvector of the document xbow by an mlp (multilayer perceptron) which then forks into two inde-pendently trainable layers to yield zµ & zlog σ2.
then a re-parametrization trick is employed tosample the latent vector z from a logistic-normaldistribution (resulting from an approximation of.
dirichlet distribution).
this is essential since back-propagation through a sampling node is infeasible.
z is then used by decoder’s single dense layer dto yield the reconstructed bow xrec.
the objec-tive function has two terms: (a) kullback–leibler(kl) divergence term - to match the variationalposterior over latent variables with the prior and (b)reconstruction term - categorical cross entropyloss between xbow & xrec..ln t m = dkl(p(z) || q(z|x)) − eq(z|x)[p(x|z)].
our methodology improves upon the documentencoder and introduces a topic guided attentionwhose output is used to sample z. we use the sameformulation of decoder as used in prodlda..4 methodology.
in this section, we describe the details of our frame-work where we leverage the topic-word distribu-tion to perform topic guided attention over tokensin a document.
given a collection c with |c| doc-uments {x1, x2, .., x|c|}, we process each docu-ment x into bow vector xbow ∈ r|v | and as atoken sequence xseq, where v represents the vo-cabulary.
as shown in step a in ﬁgure 1, eachword wj ∈ xseq is embedded as ej ∈ re throughan embedding layer e ∈ r|v |×e (e = embed-ding dimension) initialised with glove (penning-ton et al., 2014).
the embedded sequence {ej}|x|j=1,.
3868where |x| is the number of tokens in x, is processedthrough a sequence encoder lstm (hochreiter andschmidhuber, 1997) to obtain the correspondinghidden states hj ∈ rh and cell states sj ∈ rh(step b in ﬁgure 1):.
document-topic representation z as the ﬁnal stepd in figure 1. the topic embeddings are thenused to determine the attention alignment matrixa ∈ r|x|×k between each topic k ∈ {1, 2, ..., k}and words in the document such that:.
hj, sj = flst m (ej, (hj−1, sj−1)).
where h is lstm’s hidden size.
we constructa memory bank m = (cid:104)h1, h2, ..., h|x|(cid:105) which isthen used to perform topic-guided attention (step cin ﬁgure 1).
the output vector of the attention mod-ule is used to derive prior distribution parameterszµ & zlog σ2 (as in vae) through two linear layers.
using the re-parameterisation trick, we sample thelatent document-topic vector z, which is then givenas input to bow decoder linear layer d that out-puts the reconstructed bow xrec (step d in ﬁgure1).
objective function is same as in vae setting, in-volving a reconstruction loss term between xrec &xbow and kl divergence between the prior (laplaceapproximation to dirichlet prior as in prodlda)and posterior.
we now discuss the details of ourtopic attention network..4.1 tan: topic attention network.
we intend the model to attend on document wordsin a manner such that the resultant attention is dis-tributed according to the semantics of the topicsrelevant to the document.
we hypothesize thatthis can enable the model to encode better docu-ment features while capturing the underlying latentdocument-topic representations.
the topic-worddistribution tw represents the afﬁnity of each topictowards words in the vocabulary (which is used tointerpret the semantics of each topic).
therefore,we factor tw ∈ rk×|v | into the attention mecha-nism, where k denotes the number of topics.
thetopic-aware attention encoder and topic-word dis-tribution inﬂuence each other during training whichconsequently results in convergence to better topicsas discussed in detail in experiments section..speciﬁcally, we perform attention on documentsequence of tokens for each topic using the embed-ded representation of the topics te ∈ rk×e:.
te = twe,.
[topic embeddings].
tw = softmax(d),.
[topic-word distribution].
where d ∈ rk×v is the decoder layer which isused to reconstruct xbow from the sampled latent.
ajk =.
exp(score((te)k, hj))j(cid:48)=1 exp(score((te)k, hj(cid:48))).
,.
(cid:80)|x|.
score((te)k, hj) = va.(cid:62)tanh(wa[(te)k;hj])where va ∈ rp , wa ∈ rp ×(e+h), (te)k ∈re is the embedded representation of the kth topicand ; is the concatenation operation.
we then de-termine topic-wise context vector corresponding toeach topic as:.
ct =.
aj ⊗ hj,.
[topic-wise context matrix].
|x|(cid:88).
j=1.
where ⊗ denotes outer product.
note that aj∈ rk (jth row of matrix a) is a k - dimensionalvector and hj is a h - dimensional vector, thereforeaj ⊗ hj for each j yields a matrix of order k ×h, hence ct ∈ rk×h .
the ﬁnal aggregatedcontext vector c is computed as a weighted averageover all rows of ct (each row representing eachtopic speciﬁc context vector) with document-topicproportion vector td as weights:.
c =.
(td)i(ct)k.k(cid:88).
k=1.
where, (td)k is a scalar, (ct)k ∈ rh denotesthe kth row of matrix ct & td is the document-topic distribution which signiﬁes the topic propor-tions in a document.
to compute it, we ﬁrst nor-malize the document bow vector xbow and embedit using the embedding matrix e, followed by mul-tiplication with topic embedding te ∈ rk×e:.
xnorm =.
,.
[normalized bow].
xbowi=1(xbow)i.
(cid:80)|v |.
xemb = x(cid:62).
norme,.
[document embedding].
td = softmax(te xemb), [document-topic dist.]
where xnorm ∈ r|v |, xemb ∈ re & td ∈ rk.
the context vector c is the output of our topicguided attention module which is then used for sam-pling the latent documents-topic vector followedby the bow decoding as done in traditional vaebased topic models..3869we call this framework as weighted-tan or w-tan where the context vector c is a weighted sumof topic-wise context vectors.
we also propose an-other model called top-tan or t-tan where weuse context vector of the topic with largest propor-tion in td as c. it has been experimentally observedthat doing so yields a model which generates morecoherent topics.
first, we ﬁnd the index m of mostprobable topic in td.
the context vector c is thenthe row corresponding to index m in matrix ct..5 experiments.
5.1 datasets.
1. topic quality: we evaluate and compare qual-ity of our proposed topic model on three benchmarkdatasets - 20newsgroups (20ng)1 (lang, 1995),agnews (zhang et al., 2015) and yelp reviewpolarity (yrp)2 - which are of varying complex-ity and scale in terms of number of documents,vocabulary size and average length of text after pre-processing3.
table 1 summarises statistics relatedto these datasets used for evaluating topics quality..dataset20ngagnewsyrp.
# train1125996000447873.
# test7488760038000.vocab19952788120001.avg.doc.len.
88.0622.7254.46.table 1: datasets used for evaluating topic quality.
2. keyphrase generation: neural topic model(ntm) has been used to improve the task ofsupervised keyphrase generation (wang et al.,2019b).
to further highlight the efﬁcacy of ourproposed encoding framework in providing betterdocument-topic vectors, we modify encoder mod-ule of ntm with our proposed tan-ntm andcompare the performance on stackexchange andweibo datasets4..5.2.implementation and training details.
documents in agnews are padded upto a maxi-mum length of 50, while those in 20ng and yrpare padded upto 200 tokens.
documents withlonger lengths are truncated.
these values werechosen such that ∼ 80 − 99% of all documents ineach dataset were included without truncation.
we.
1data link for 20ng dataset2data link for agnews and yrp datasets3we provide our detailed preprocessing steps in appendix.
a.1 and release processed data to standardise it..4the dataset details can be found in the baseline paper.
use batch size of 100, adam optimizer (kingmaand ba, 2015) with β1 = 0.99, β2 = 0.999 and(cid:15) = 10−8 and train each model for 200 epochs.
forall models except t-tan, learning rate was ﬁxedat 0.002 ([0.001, 0.003], 5)5. t-tan convergesrelatively faster than other models, therefore forsmooth training, we decay its learning rate everyepoch using exponential staircase scheduler withinitial learning rate = 0.002 and decay rate = 0.96.the number of topics k = 50, a value widely usedin literature.
we perform hyper-parameter tuningmanually to determine the hidden dimension valueof various layers: e = 200 ([100, 300], 5), h =450 ([300, 900], 10) and p = 350 ([10, 400], 10).
the weight matrices of all dense layers are xavierinitialized, while bias terms are initialized with ze-ros.
all our proposed models and baselines aretrained on a machine with 32 virtual cpus, singlenvidia tesla v 100 gpu and 240 gb ram..5.3 comparison with baselines.
we compare our tan-ntm with various baselinesin table 2 that can be enumerated as (please referto introduction and related work for their details):1) lda (c.g.
): statistical method (mccallum,2002) which performs lda using collapsed gibbs6sampling.
2) prodlda and 3) nvlda (srivastava and sut-ton, 2017): neural variational inference methodswhich use approximation to dirichlet prior7.
4) w-lda (nan et al., 2019) which is a non varia-tional inference based neural model using wasses-tein autoencoder8.
5) nb-ntm and 6) gnb-ntm: methods usingnegative binomial and gamma negative binomialdistribution as priors for topic discovery9(wu et al.,2020) respectively..we could not compare with other methods whoseofﬁcial error-free source code is not publicly avail-able yet.
we train and evaluate the baseline meth-ods on same data as used for our method usingnpmi coherence10 (aletras and stevenson, 2013).
it computes the semantic relatedness between top lwords in a given topic through determining similar-ity between their word embeddings trained over the.
5v ([a, b], t) means t values from [a, b] range tried for thishyper-parameter, of which v yielded best npmi coherence..6https://pypi.org/project/lda/7code for prodlda and nvlda8https://github.com/awslabs/w-lda9we thank authors for providing code and parameter info.
10repo used to calculate npmi.
please refer to appendix.
b for a detailed discussion on choice of evaluation metric..3870methodlda(c.g)nvldaprodldaw-ldanb-ntmgnb-ntmw-tan (ours)t-tan (ours).
20ng agnews yrp0.1140.2020.1390.1650.2160.20.1650.3220.2680.250.2620.2270.2240.310.1650.2410.3120.2060.2320.3270.2610.2720.3690.296.table 2: npmi coherence (determined using top 10words of each topic) comparison on 50 topics betweenbaselines and our proposed w-tan and t-tan on dif-ferent datasets.
it can be seen that t-tan achieves sig-niﬁcantly better scores on all the datasets..corpus used for topic modeling and reports averageover topics.
for w-lda, we refer to their originalpaper to select dataset speciﬁc hyper-parameter val-ues while training the model.
as can be seen in ta-ble 2, our proposed t-tan model performs signiﬁ-cantly better than previous topic models uniformlyon all datasets achieving a better npmi (measuredon a scale of -1 to 1) by a margin of 0.028 (10.44%)on 20ng, 0.047 (14.59%) on agnews and 0.022(8.8%) on yrp, where percentage improvementsare determined over the best baseline score.
eventhough w-tan does not uniformly performs betterthan all baselines on all datasets, it achieves betterscore than all baselines on agnews and performscomparably on remaining two datasets..for a more exhaustive comparison, we also eval-uate our model’s performance on 20ng dataset(which is the common dataset with gnb-ntm(wu et al., 2020)) using the npmi metric fromgnb-ntm’s code.
the npmi coherence of ourmodel using their criteria is 0.395 which is betterthan gnb-ntm’s score of 0.375 (as reported intheir paper).
however, we would like to highlightthat gnb-ntm’s computation of npmi metricuses relaxed window size, whereas the metric usedby us (lau et al., 2014) uses much stricter windowsize while determining word co-occurrence countswithin a document.
lau et al.
(2014) is a muchmore common and widely used way of computingthe npmi coherence and evaluating topic models..5.3.1 document classiﬁcation.
in addition to evaluating our framework in terms oftopic coherence, we also compare it with the base-lines on the downstream task of document clas-siﬁcation.
topic models have been used as text.
feature extractors to perform classiﬁcation (nanet al., 2019).
we analyse the quality of encodeddocument representations and predictive capacityof latent document-topic features generated by ourmodel and compare it with existing topic models11.
we train the topic model setting number of top-ics to 50 and freeze its weights.
the trained topicmodel is then used to infer latent document-topicfeatures.
we then separately train a single layerlinear classiﬁer through cross entropy loss on thetraining split using the document-topic vectors asinput and adam optimizer at a learning rate of 0.01..methodlda(c.g.)
prodldantm-rw-ldanb-ntmgnb-ntmt-tan (ours)t-tan (ours)(context vector).
20ng agnews yrp86.8584.7851.2977.7382.6521.3386.1685.6743.3485.6385.2943.0887.5186.6757.3884.5585.3457.1688.160.4487.3888.989.7864.36.table 3: comparison of accuracy between differenttopic models on document classiﬁcation.
we performtwo experiments with t-tan: using document-topicvector (2nd to last row) and context vector (last row)..we report classiﬁcation accuracy on the test splitof 20ng, agnews and yrp datasets (compris-ing of 20, 4 and 2 classes respectively) in table3. the document-topic features provided by t-tan achieve best accuracy on agnews (1.43%improvement over most performant baseline) withmost signiﬁcant improvement of 3.06% on 20ngwhich shows our model learns better document fea-tures.
t-tan performs almost the same as the bestbaseline on yrp.
further, to analyse the predic-tive performance of top topic attention based con-text vector, we use it instead of latent document-topic vector to perform classiﬁcation which fur-ther boosts accuracy leading to an improvement of∼6.9% on 20ng, ∼3.1% on agnews and ∼1.3%on yrp datasets over the baselines..5.3.2 running time analysiswe compare the running time of our method withbaselines in terms of average time taken (in sec-onds) for performing a forward pass through the.
11our aim is to analyse document-topic features amongtopic models only and not to compare with other non-topicmodel based generic text classiﬁers..3871model, where the average is taken over 10000passes.
our tan-ntm (implemented in tensor-ﬂow) takes 0.087s, 0.027s and 0.093s on 20ng,agnews and yrp datasets respectively.
sincetan-ntm processes the input documents as a se-quence of tokens through an lstm, its runningtime is proportional to the document lengths whichvary according to the dataset.
the running timefor baseline methods are: prodlda - 0.012s (im-plemented in tensorﬂow), w-lda - 0.003s (imple-mented in mxnet) and gnb-ntm - 0.003s (im-plemented in pytorch).
for baseline methods, wehave used their original code implementations.
wefound that the running time of baseline models isindependent of the dataset.
this is because they usethe bag-of-words (bow) representation of the doc-uments.
the sequential processing in tan-ntmis the reason for increased running time of ourmodels compared to the baselines.
in the case ofagnews, since the documents are of lesser lengthsthan 20ng and yrp, the running time of our tan-ntm is relatively less for agnews.
further, therunning time of other ablation variants (introducedin section 5.4) of our method on 20ng, agnewsand yrp datasets respectively are: 1) only lstm -0.083s, 0.033s and 0.091s ; 2) vanilla attn - 0.088s,0.037s and 0.095s..5.4 ablation studies.
in this section, we compare the performance ofdifferent variants of our model namely, 1) onlylstm: ﬁnal hidden state is used to derive sam-pling parameters zµ & zlog σ2, 2) vanilla attn: ﬁ-nal hidden state (w/o topic-word distribution) isused as query to perform attention (bahdanau et al.,2014) on lstm outputs such that context vectorz is used for vi, 3) w-tan: weighted topic at-tention network, 4) t-tan: top topic attentionnetwork and 5) t-tan w/o (without) glove: em-bedding layer in t-tan is randomly initialised..table 4 compares the topic coherence scores ofthese different ablation methods on 20ng, ag-news and yrp.
as can be seen, applying attentionperforms better than simple lstm model.
theweighted tan performs better than vanilla atten-tion model, however, t-tan uniformly providesthe best coherence scores across all the datasetscompared to all other methods.
this shows thatperforming attention corresponding to the mostprominent topic in a document results in more co-herent topics.
further, we perform an ablation to.
study the effect of using pre-trained embeddings fort-tan where it can be seen using glove for initial-ising word embeddings results in improved npmias compared to training t-tan initialised with ran-dom uniform embeddings (t-tan w/o glove)12..methodonly lstmvanilla attnw-tant-tant-tan w/o glove.
20ng agnews yrp0.0920.2020.2470.180.2440.2890.2320.3270.2610.2720.3690.2960.2480.3440.274.table 4: comparison of npmi coherence between ab-lation variants of our method for k=50 topics..5.5 qualitative analysis.
to verify performance of t-tan qualitatively, wedisplay few topics generated by prodlda and t-tan on agnews in figure 2. prodlda achievesbest score among baselines on agnews.
considercomparison 1 in figure 2: prodlda produces fourtopics corresponding to space, mixing them withnuclear weapons, while t-tan produces two sep-arate topics for both of these concepts.
in secondcomparison, we see that prodlda has problemsdistinguishing between closely related topics (foot-ball, olympics, cricket) and mixes them while t-tan produces three coherent topics..figure 2: two comparisons of corresponding topics(one topic per line) from prodlda and t-tan.
wordshaving similar meaning are highlighted in same colour.
the topics of prodlda are inter-mixed and incoherentwhile those of t-tan are unmixed and coherent..5.6 takg: topic aware keyphrase.
generation.
we further analyse the impact of our proposedframework on another downstream task where the.
12we also trained embeddings from scratch for other vari-.
ants but coherence score remained unaffected..3872stackexchange.
methodtakg (baseline)takg with w-tan (ours)takg with t-tan (ours).
f1@332.93133.52133.15.f1@5 map34.92528.73135.92929.80235.2629.118.f1@134.58435.61634.813.weibof1@3 map40.99424.30942.6825.65141.26124.65.table 5: f1@k and map (mean average precision) comparison between baseline (takg) and our proposed topicmodel based encoder for topic guided supervised keyphrase generation.
the metrics measure overlap betweenground truth and top k generated keyphrases factoring in rank of keyphrases generated through beam search..task speciﬁc model is assisted by the topic modeland both can be trained in an end-to-end manner.
for this, we discuss takg (wang et al., 2019b)and how our proposed topic model encoder canbe adapted to achieve better performance on su-pervised keyphrase generation from textual posts.
takg13 comprises of two sub-modules: (1) a topicmodel based on nvdm-gsm (as discussed in in-troduction) using bow as input to the encoder and(2) a seq2seq based model for keyphrase genera-tion.
both modules have an encoder and a decoderof their own.
keyphrase generation module usessequence input which is processed by bidirectionalgru (cho et al., 2014) to encode input sequence.
the keyphrase generation decoder uses unidirec-tional gru which attends on encoder outputs andtakes the latent document-topic vector from thetopic model as input in a differentiable manner.
since topic model trains slower than keyphrasegeneration module, the topic model is warmed upfor some epochs separately and then jointly trainedwith keyphrase generation.
please refer to originalpaper (wang et al., 2019b) for more details..we adapted our proposed topic model frame-work by changing the architecture of encoder in thetopic model of takg, replacing it with w-tanand t-tan.
the change subsequently results in bet-ter latent document-topic representation depictedby better performance on keyphrase generation asshown in table 5 where the improved topic modelencoding framework results in ∼1-2% improve-ment in f1 and map (mean average precision) onstackexchange and weibo datasets compared totakg.
here, even though takg with t-tan per-forms marginally better than the baseline, takgwith w-tan uniformly performs much better..6 conclusion.
in this work, we propose topic attention networkbased neural topic modeling framework: tan-.
13we use their code and data (link) to conduct experiments..ntm to discover topics in a document corpus byperforming attention on sequentially processed to-kens in a topic guided manner.
attention is per-formed effectively by factoring topic-word dis-tribution (twd) into attention mechanism.
wecompare different variants of our method throughablations and conclude that processing tokens se-quentially without attention or applying attentionwithout twd gives inferior performance.
ourtan-ntm model generates more coherent top-ics compared to state-of-the-art topic models onseveral benchmark datasets.
our model encodesbetter latent document-topic features as validatedthrough better performance on document classiﬁca-tion and supervised keyphrase generation tasks.
asfuture work, we would like to explore our frame-work with other sequence encoders such as trans-formers, bert etc.
for topic modeling..references.
nikolaos aletras and mark stevenson.
2013. evalu-ating topic coherence using distributional semantics.
in proceedings of the 10th international conferenceon computational semantics (iwcs 2013)–long pa-pers, pages 13–22..christophe andrieu, nando de freitas, arnaud doucet,and michael i jordan.
2003. an introduction tomcmc for machine learning.
machine learning,50(1-2):5–43..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..federico bianchi, silvia terragni, and dirk hovy.
2020.topic: contextual-ized document embeddings improve topic coher-ence.
arxiv, abs/2004.03974..pre-training is a hot.
steven bird, ewan klein,.
and edward loper.
2009. natural language processing with python.
o’reilly media..3873david m blei, andrew y ng, and michael i jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(jan):993–1022..s. burkhardt and s. kramer.
2019. decoupling sparsityand smoothness in the dirichlet variational autoen-coder topic model.
j. mach.
learn.
res., 20:131:1–131:27..dallas card, chenhao tan, and noah a. smith.
2018.inneural models for documents with metadata.
proceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2031–2040, melbourne, aus-tralia.
association for computational linguistics..jonathan chang, sean gerrish, chong wang, jordanboyd-graber, and david blei.
2009. reading tealeaves: how humans interpret topic models.
in ad-vances in neural information processing systems,volume 22, pages 288–296.
curran associates, inc..jun chen, xiaoming zhang, yu wu, zhao yan, andzhoujun li.
2018. keyphrase generation with corre-lation constraints.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, pages 4057–4066, brussels, belgium.
association for computational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..scott deerwester, susan t. dumais, george w. fur-nas, thomas k. landauer, and richard harshman.
indexing by latent semantic analysis.
jour-1990.nal of the american society for information science,41(6):391–407..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..adji b. dieng, francisco j. r. ruiz, and david m.blei.
2020. topic modeling in embedding spaces.
transactions of the association for computationallinguistics, 8:439–453..adji b dieng, chong wang, jianfeng gao, and johnpaisley.
2016. topicrnn: a recurrent neural net-work with long-range semantic dependency.
arxivpreprint arxiv:1611.01702..ran ding, ramesh nallapati, and bing xiang.
2018.in pro-coherence-aware neural topic modeling.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 830–836, brussels, belgium.
association for computa-tional linguistics..ian goodfellow, jean pouget-abadie, mehdi mirza,bing xu, david warde-farley, sherjil ozair, aaroncourville, and yoshua bengio.
2014. generativein z. ghahramani, m. welling,adversarial nets.
c. cortes, n. d. lawrence, and k. q. weinberger,editors, advances in neural information processingsystems 27, pages 2672–2680.
curran associates,inc..thomas l grifﬁths and mark steyvers.
2004. find-ing scientiﬁc topics.
proceedings of the nationalacademy of sciences, 101(suppl 1):5228–5235..pankaj gupta, yatin chaudhary, florian buettner, andhinrich sch¨utze.
2019. document informed neuralautoregressive topic models with distributional prior.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 6505–6512..suchin gururangan, t. dang, d. card, and noah a.variational pretraining for semi-.
smith.
2019.supervised text classiﬁcation.
in acl..sepp hochreiter and j¨urgen schmidhuber.
1997.neural comput.,.
long short-term memory.
9(8):1735–1780..alexander miserlis hoyle, pranav goel, and p. resnik.
2020a.
improving neural topic models using knowl-edge distillation.
arxiv, abs/2010.02377..alexander miserlis hoyle, pranav goel, and philipresnik.
2020b.
improving neural topic models us-in proceedings of theing knowledge distillation.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 1752–1771,online.
association for computational linguistics..xuemeng hu, rui wang, deyu zhou, and yuxuanxiong.
2020. neural topic modeling with cycle-consistent adversarial training.
in emnlp..s. ioffe and christian szegedy.
2015. batch normaliza-tion: accelerating deep network training by reduc-ing internal covariate shift.
arxiv, abs/1502.03167..mingmin jin, xin luo, huiling zhu, and hankz han-kui zhuo.
2018. combining deep learning and topicmodeling for review understanding in context-awarerecommendation.
in proceedings of the 2018 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages1605–1614, new orleans, louisiana.
associationfor computational linguistics..diederik p. kingma and jimmy ba.
2015. adam:corr,.
a method for stochastic optimization.
abs/1412.6980..3874diederik p kingma and max welling.
2013. auto-arxiv preprint.
encoding variational bayes.
arxiv:1312.6114..gabriele pergola, lin gui, and yulan he.
2019. tdam:a topic-dependent attention model for sentimentanalysis.
inf.
process.
manag., 56..varun kumar, alison smith-renner, leah findlater,k. seppi, and jordan l. boyd-graber.
2019. whydidn’t you listen to me?
comparing user control ofhuman-in-the-loop topic models.
in acl..ken lang.
1995. newsweeder: learning to ﬁlterin machine learning proceedings 1995,.netnews.
pages 331–339.
elsevier..jey han lau, david newman, and timothy baldwin.
2014. machine reading tea leaves: automaticallyevaluating topic coherence and topic model quality.
in proceedings of the 14th conference of the euro-pean chapter of the association for computationallinguistics, pages 530–539, gothenburg, sweden.
association for computational linguistics..andrew kachites mccallum.
2002. mallet: a ma-chine learning for language toolkit.
http://mallet.
cs.
umass.
edu..rui meng, sanqiang zhao, shuguang han, daqinghe, peter brusilovsky, and yu chi.
2017. deepin proceedings of the 55thkeyphrase generation.
annual meeting of the association for computa-tional linguistics (volume 1: long papers), pages582–592, vancouver, canada.
association for com-putational linguistics..yishu miao, edward grefenstette, and phil blunsom.
2017. discovering discrete latent topics with neu-ral variational inference.
volume 70 of proceedingsof machine learning research, pages 2410–2419,international convention centre, sydney, australia.
pmlr..yishu miao, lei yu, and phil blunsom.
2016. neu-ral variational inference for text processing.
vol-ume 48 of proceedings of machine learning re-search, pages 1727–1736, new york, new york,usa.
pmlr..feng nan, ran ding, ramesh nallapati, and bing xi-ang.
2019. topic modeling with wasserstein autoen-in proceedings of the 57th annual meet-coders.
ing of the association for computational linguis-tics, pages 6345–6381, florence, italy.
associationfor computational linguistics..nicole peinelt, dong nguyen, and maria liakata.
2020.tbert: topic models and bert joining forces forsemantic similarity detection.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 7047–7055, online.
as-sociation for computational linguistics..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..radim ˇreh˚uˇrek and petr sojka.
2010. software frame-work for topic modelling with large corpora.
inproceedings of the lrec 2010 workshop on newchallenges for nlp frameworks, pages 45–50, val-letta, malta.
elra..mehdi rezaee and f. ferraro.
2020..a dis-crete variational recurrent topic model without thereparametrization trick.
arxiv, abs/2010.12055..suzanna sia, ayush dalmia, and sabrina j. mielke.
2020. tired of topic models?
clusters of pretrainedword embeddings make for fast and good topics too!
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 1728–1736, online.
association for computa-tional linguistics..akash srivastava and charles sutton.
2017. autoen-coding variational inference for topic models.
arxivpreprint arxiv:1703.01488..nitish srivastava, geoffrey e. hinton, a. krizhevsky,and r. salakhutdinov.
2014.a simple way to prevent neural net-j. mach.
learn.
res.,.
ilya sutskever,dropout:works from overﬁtting.
15:1929–1958..mark steyvers and tom grifﬁths.
2007. probabilistictopic models.
handbook of latent semantic analysis,427(7):424–440..romain thibaux and michael i. jordan.
2007. hier-archical beta processes and the indian buffet pro-cess.
volume 2 of proceedings of machine learn-ing research, pages 564–571, san juan, puerto rico.
pmlr..ilya tolstikhin, olivier bousquet, sylvain gelly, andbernhard schoelkopf.
2017. wasserstein auto-encoders.
arxiv preprint arxiv:1711.01558..felipe viegas, washington cunha, christian gomes,antˆonio pereira, leonardo rocha, and marcosgoncalves.
2020. cluhtm - semantic hierarchicaltopic modeling based on cluwords.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 8138–8150, on-line.
association for computational linguistics..rui wang, xuemeng hu, deyu zhou, yulan he, yux-uan xiong, chenchen ye, and haiyang xu.
2020.neural topic modeling with bidirectional adversarialin proceedings of the 58th annual meet-training.
ing of the association for computational linguis-tics, pages 340–350, online.
association for com-putational linguistics..rui wang, deyu zhou, and yulan he.
2019a.
atm:information pro-.
adversarial-neural topic model.
cessing & management, 56(6):102098..3875x. wang and y. yang.
2020. neural topic model with.
attention for supervised learning.
in aistats..yue wang, jing li, hou pong chan, irwin king,michael r. lyu, and shuming shi.
2019b.
topic-aware neural keyphrase generation for social medialanguage.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 2516–2526, florence, italy.
association forcomputational linguistics..jiemin wu, yanghui rao, zusheng zhang, haoran xie,qing li, fu lee wang, and ziye chen.
2020. neu-ral mixed counting models for dispersed topic dis-in proceedings of the 58th annual meet-covery.
ing of the association for computational linguistics,pages 6159–6169, online.
association for computa-tional linguistics..m. zaheer, amr ahmed, and alex smola.
2017. la-tent lstm allocation: joint clustering and non-lineardynamic modeling of sequence data.
in icml..ce zhang and hady w. lauw.
2020. topic modelingon document networks with adjacent-encoder.
inaaai..hao zhang, b. chen, d. guo, and m. zhou.
2018.whai: weibull hybrid autoencoding inference fordeep topic modeling.
arxiv: machine learning..qi zhang, yang wang, yeyun gong, and xuanjinghuang.
2016. keyphrase extraction using deep re-in proceed-current neural networks on twitter.
ings of the 2016 conference on empirical methodsin natural language processing, pages 836–845,austin, texas.
association for computational lin-guistics..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-in advances in neural information pro-siﬁcation.
cessing systems, pages 649–657..he zhao, dinh phung, viet huynh, trung le, andwray buntine.
2021. neural topic model via op-in international conference ontimal transport.
learning representations..deyu zhou, xuemeng hu, and rui wang.
2020. neu-ral topic modeling by incorporating document rela-in proceedings of the 2020 con-tionship graph.
ference on empirical methods in natural languageprocessing (emnlp), pages 3790–3796, online.
as-sociation for computational linguistics..jun-yan zhu, t. park, phillip isola, and alexei a.efros.
2017. unpaired image-to-image translationusing cycle-consistent adversarial networks.
2017ieee international conference on computer vision(iccv), pages 2242–2251..li-xing zhu, yulan he, and deyu zhou.
2020. a neu-ral generative model for joint learning topics andtopic-speciﬁc word embeddings.
transactions of theassociation for computational linguistics, 8:471–485..3876nltk’s (bird et al., 2009) wordnetlemma-tizer.
finally, we remove punctuations17 andtokens containing any non-ascii character..• in steps 9 through 15, we construct the vocab-ulary vocab, which is a mapping of each to-ken to its occurrence count among the prunedtraining documents tr pruned.
we onlycount a token if it is not an english stopword18and its length is between 3 and 15 (inclusive)..• steps 16 through 19 ﬁlter the vocab by re-moving tokens whose total occurrence countis less than num below or whose occurrencecount per training document is greater thanfr abv, where the values of num belowand fr abv are taken from table 6. foryrp, we follow the w-lda paper (nan et al.,2019) and restrict its vocab to only containtop 20, 000 most occurring tokens..• steps 20 through 24 construct the token-to-index map w2idx by mapping each token invocab to an index starting from 1. next, wemap the padding token to index 0 (step 25)..• the ﬁnal step in the preprocessing is to en-code the train and test documents by mappingeach of their tokens to corresponding indicesaccording to w2idx.
this is done by the en-code function of algorithm 2 which is in-voked in steps 26 and 27..datasetagnewsyrp.
tr size num below fr abv320.
96000448000.
0.70.7.table 6: parameters used for preprocessing the ag-news and yrp datasets..appendices.
a further implementation details.
a.1 preprocessing.
for 20ng dataset, we used its preprocessedversion downloaded from prodlda’s (srivastavaand sutton, 2017) repository14, whereas agnewsand yrp datasets were downloaded from this15link.
these two datasets contain train.csv andtest.csv ﬁles.
the csv ﬁles of yrp contain adocument body only, whereas the csv ﬁles foragnews contain a document title as well as adocument body.
for uniformity, we concatenatethe title and body in the csv ﬁles of agnews andkeep it as a single ﬁeld.
the documents fromtrain.csv and test.csv are then read into trainand test lists which are passed to preprocessfunction of algorithm 1 for preprocessing..stepwise working of algorithm 1 is expained inthe following points:.
• before invoking the preprocess function,we initialize the data sampler by a ﬁxed seedso that preprocessing yields the same resultwhen run multiple times..• for each dataset, we randomly sampletr size documents (as mentioned in table6) from the train list in step 2. these val-ues of tr size are taken from table 1 ofw-lda paper (nan et al., 2019).
note that# train in table 1 represents the number oftraining documents after preprocessing.
ofthe tr size documents, some documentsmay be removed during preprocessing, there-fore # train may be less than tr size..• in steps 3 through 8, we prune the trainand test documents by invoking theprune doc function from algorithm 2. first,we remove the control characters from the doc-uments viz.
‘\n’, ‘\t’, and ‘\r’ (for yrp, weadditionally remove ‘\\t’, ‘\\n’, and ‘\\r’).
next, we remove the numeric tokens16 fromthe documents, convert them to lowercaseand lemmatize each of their tokens using the.
14data link for 20ng dataset15data link for agnews and yrp datasets16fully numeric tokens e.g..‘1487’, ‘1947’, etc..removed, whereas partially numeric tokens e.g.
‘de1080’, etc.
are retained..are‘g47’,.
17any of the following 32 characters is regarded as a punc-.
tuation !”#$%&’()*+,-./:;<=>?
@[\]ˆ `{|}∼.
18gensim’s ( ˇreh˚uˇrek and sojka, 2010) list of english stop-.
words is used..3877algorithm 1 pseudocode for preprocessing ag-news and yrp datasets..algorithm 2 pseudocode for pruning the documentand encoding it given a token-to-index mapping..1: function preprocess(train, test)2:.
train ← train.sample(tr size)tr pruned ← []te pruned ← [].
(cid:46) empty list(cid:46) empty list.
3:.
4:.
5:.
6:.
7:.
8:.
9:.
10:.
11:.
12:.
13:.
14:.
15:.
16:.
17:.
18:.
19:.
20:.
21:.
22:.
23:.
24:.
25:.
26:.
27:.
28:.
for document d in train do.
tr pruned.append(prune doc(d)).
for document d in test do.
te pruned.append(prune doc(d)).
vocab ← mapping of each token to 0num doc ← len(tr pruned).
for document d in tr pruned do.
for token t in d do.
if t /∈ stopwords andlen(t) ∈ [3, 15] then.
vocab[t]← vocab[t] +1.
for token t in vocab do.
if vocab[t] < num below orvocab[t]/num doc > fr abv then.
vocab[t].remove(t).
i ← 1w2idx ← empty mapfor token t in vocab do.
w2idx[t]= ii ← i + 1w2idx[0]← pad.
trd ← encode(tr pruned, w2idx)ted ← encode(te pruned, w2idx)return trd, ted, w2idx.
a.2 learning rate scheduler.
as mentioned in section 5.2, we use a learning ratescheduler while training t-tan.
the rate decayfollows the following equation:.
lrate = init rate ∗ decay rate.
(cid:106) train stepdecay steps.
(cid:107).
this is an exponential staircase function whichenables decrease in learning rate every epoch dur-ing training..we initialize the learning rate by init rate =0.002 and use decay rate = 0.96. train step is a.
1: function prune doc(doc)2:.
doc ← rm control(doc)doc ← rm numeric(doc)doc ← lowercase(doc)doc ← lemmatize(doc)doc ← rm punctuations(doc)doc ← rm non ascii(doc)return doc.
9: function encode(doc list, w2idx)10:.
encdoclist ← []for document d in doc list do.
11:.
ecdoc ← []for token t in d do.
ecdoc.append(w2idx[t])encdoclist.append(ecdoc).
return encdoclist.
3:.
4:.
5:.
6:.
7:.
8:.
12:.
13:.
14:.
15:.
16:.
global counter of training steps and decay steps =#train docsis the number of training steps takenbatch sizeper epoch.
therefore, effectively, the rate remainsconstant for all training steps in an epoch and de-creases exponentially as per the above equationonce the epoch completes..a.3 regularization.
we employ two types of regularization during train-ing:.
• dropout: we apply dropout (srivastava et al.,2014) to z with the rate of pdrop = 0.6 beforeit is processed by the decoder for reconstruc-tion..• batch normalization (bn): we apply a bn(ioffe and szegedy, 2015) to the inputs of de-coder layer and to the inputs of layers beingtrained for zµ & zlog σ2, with (cid:15) = 0.001 anddecay = 0.999..b evaluation metrics.
topic models have been evaluated using variousmetrics namely perplexity, topic coherence, topicuniqueness etc.
however, due to the absence ofa gold standard for the unsupervised task of topicmodeling, all of that metrics have received criti-cism by the community.
therefore, a consensus onthe best metric has not been reached so far.
perplex-ity has been found to be negatively correlated to.
3878topic quality and human judgements (chang et al.,2009).
this work presents experimental resultswhich show that in some cases models with higherperplexity were preferred by human subjects..would a pilot know that one of their crewis armed?
the federal flight deck ofﬁcer page onwikipedia says this:.
topic uniqueness (nan et al., 2019) quantiﬁesthe intersection among topic words globally.
how-ever, it also suffers from drawbacks and often pe-nalizes a model incorrectly (hoyle et al., 2020b).
firstly, it does not account for ranking of inter-sected words in the topics.
secondly, it fails todistinguish between the following two scenarios:1) when the intersected words in one topic are allpresent in a second topic (signifying strong simi-larity i.e.
these two topics are essentially identical)and, 2) when the intersected words of one topic arespread across all the other topics (signifying weaksimilarity i.e.
the topics are diffused).
the ﬁrst is aproblem related to uniqueness among topics whilesecond is a problem related to word intrusion intopics.
(chang et al., 2009) conducted experimentswith human subjects on two tasks: word intrusionand topic intrusion.
word intrusion measures thepresence of those words (called intruder words)which disagree with the semantics of the topic.
topic intrusion measures the presence of thosetopics (called intruder topics) which do not rep-resent the document corpus appropriately.
theseare better estimates of human judgement of topicmodels in comparison to perplexity and unique-ness.
however, since these metrics rely on humanfeedback, they cannot be widely used for unsuper-vised evaluation.
further, topic uniqueness unfairlypenalizes cases when some words are common be-tween topics, however other uncommon words inthose topics change the context as well as topicsemantics as also discussed in (hoyle et al., 2020b).
according to the work of (lau et al., 2014), measur-ing the normalized pointwise mutual information(npmi) between all the word pairs in a set of topicsagrees with human judgements most closely.
thisis called the npmi topic coherence in the litera-ture and is widely used for the evaluation of topicmodels.
we therefore adopt this metric in our work.
since the effectiveness of a topic model actuallydepends on the topic representations that it extractsfrom the documents, we report the performanceof our model on two downstream tasks: documentclassiﬁcation and keyphrase generation (which usethese topic representations) for a better and holisticevaluation and comparison..under the ffdo program, ﬂight crew mem-bers are authorized to use ﬁrearms.
a ﬂightcrew member may be a pilot, ﬂight engineeror navigator assigned to the ﬂight..to me, it seems like this would be crucial in-formation for the pic to know, if their ﬂightengineer (for example) was armed; but on theﬂip-side of this, the engineer might want tokeep that to himself if he’s with a crew hehasn’t ﬂown with before..is there a guideline on whether an ffdoshould inform the crew that he’s armed?.
gt: security, crew, ffdotakg: faa regulations, ffdo, ﬂight training,ﬁrearms, fartakg + w-tan: ffdo, crew, ﬂight controls,crewed spaceﬂight, securitydo the poisons in “ode on melancholy”have deeper meaning?
in ”ode on melancholy”, keats uses the im-ages of three poisons in the ﬁrst stanza: wolf’sbane, nightshade, and yew-berries.
are thesepoisons simply meant to connote death/suicide,or might they have a deeper purpose?.
gt: poetry, meaning, john keatstakg: the keats, meaning, poetry, ode,melancholy keatstakg + w-tan: poetry, meaning, the keats,john keats, greek literature.
table 7: two randomly selected posts (title in bold)from stackexchange dataset with ground truth (gt)and top 5 keyphrases predicted by takg with andwithout w-tan, denoted as takg + w-tan &takg respectively.
keyphrases generated with w-tan are closer to the ground truth in terms of bothprediction and ranking..c qualitative analysis.
c.1 key phrase predictions.
we saw the quantitative improvement in results intable 5 when we used w-tan as the topic model.
3879with takg.
in table 7, we display some postsfrom stackexchange dataset with ground truthkeyphrases and top 5 predictions by takg withand without w-tan.
we observe that using w-tan improves keyphrase generation qualitatively.
the ﬁrst post in table 7 inquires if a ﬂight of-ﬁcer should inform the pilot in command (pic)about him being armed or not.
for this post, takgalone only predicts one ground truth keyphrase cor-rectly and misses ‘security’ and ‘crew’.
however,when takg is used with w-tan, it gets all threeground truth keyphrases, two of which are its top 2predictions as well..the second post is inquiring about a possibledeeper meaning of three poisons in a poem by johnkeats.
takg alone predicts two of the groundtruth keyphrases correctly but assigns them largerranks and it misses ‘john keats’.
when takg isused with w-tan, it gets all three ground truthkeyphrases and its top 2 keyphrases are assignedthe exact same rank as they have in the ground truth.
this hints that using w-tan with takg improvesthe prediction as well as ranking of the generatedkeyphrases compared to using takg alone..3880