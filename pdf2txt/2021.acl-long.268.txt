prevent the language model from being overconﬁdent inneural machine translationmengqi miao1∗ , fandong meng2∗, yijin liu2, xiao-hua zhou3† , and jie zhou21peking university, china2pattern recognition center, wechat ai, tencent inc, china3beijing international center for mathematical research,national engineering lab for big data analysis and applications,department of biostatistics, peking university, beijing, chinamiaomq@pku.edu.cn, {fandongmeng, yijinliu}@tencent.comazhou@math.pku.edu.cn, withtomzhou@tencent.com.
abstract.
the neural machine translation (nmt)model is essentially a joint language modelconditioned on both the source sentence andpartial translation.
therefore, the nmt modelnaturally involves the mechanism of the lan-guage model (lm) that predicts the next to-ken only based on partial translation.
de-spite its success, nmt still suffers from thehallucination problem, generating ﬂuent butinadequate translations.
the main reason isthat nmt pays excessive attention to the par-translation while neglecting the sourcetialsentence to some extent, namely overconﬁ-dence of the lm.
accordingly, we deﬁnethe margin between the nmt and the lm,calculated by subtracting the predicted prob-ability of the lm from that of the nmtmodel for each token.
the margin is neg-atively correlated to the overconﬁdence de-gree of the lm.
based on the property, wepropose a margin-based token-level objec-tive (mto) and a margin-based sentence-level objective (mso) to maximize the mar-gin for preventing the lm from being over-conﬁdent.
experiments on wmt14 english-to-german, wmt19 chinese-to-english, andwmt14 english-to-french translation tasksdemonstrate the effectiveness of our approach,with 1.36, 1.50, and 0.63 bleu improve-ments, respectively, compared to the trans-former baseline.
the human evaluation furtherveriﬁes that our approaches improve transla-tion adequacy as well as ﬂuency.
1.
1.introduction.
neural machine translation (nmt) has achievedgreat success in recent years (sutskever et al., 2014;.
∗equal contribution.
this work was done when mengqimiao was interning at pattern recognition center, wechat ai,tencent inc, china..† corresponding author.
1code is available at https://github.com/mlair.
77/nmt adequacy.
cho et al., 2014; bahdanau et al., 2014; luonget al., 2015; vaswani et al., 2017; meng and zhang,2019; zhang et al., 2019a; yan et al., 2020b), whichgenerates accurate and ﬂuent translation throughmodeling the next word conditioned on both thesource sentence and partial translation.
however,nmt faces the hallucination problem, i.e., trans-lations are ﬂuent but inadequate to the source sen-tences.
one important reason is that the nmtmodel pays excessive attention to the partial trans-lation to ensure ﬂuency while failing to translatesome segments of the source sentence (weng et al.,2020b), which is actually the overconﬁdence of thelanguage model (lm).
in the rest of this paper,the lm mentioned refers to the lm mechanisminvolved in nmt..many recent studies attempt to deal with theinadequacy problem of nmt from two main as-pects.
one is to improve the architecture of nmt,such as adding a coverage vector to track the atten-tion history (tu et al., 2016), enhancing the cross-attention module (meng et al., 2016, 2018; wenget al., 2020b), and dividing the source sentence intopast and future parts (zheng et al., 2019).
the otheraims to propose a heuristic adequacy metric or ob-jective based on the output of nmt.
tu et al.
(2017)and kong et al.
(2019) enhance the model’s recon-struction ability and increase the coverage ratio ofthe source sentences by translations, respectively.
although some researches (tu et al., 2017; konget al., 2019; weng et al., 2020b) point out that thelack of adequacy is due to the overconﬁdence of thelm, unfortunately, they do not propose effectivesolutions to the overconﬁdence problem..from the perspective of preventing the overcon-ﬁdence of the lm, we ﬁrst deﬁne an indicator ofthe overconﬁdence degree of the lm, called themargin between the nmt and the lm, by subtract-ing the predicted probability of the lm from thatof the nmt model for each token.
a small mar-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3456–3468august1–6,2021.©2021associationforcomputationallinguistics3456gin implies that the nmt might concentrate on thepartial translation and degrade into the lm, i.e.,the lm is overconﬁdent.
accordingly, we proposea margin-based token-level objective (mto) tomaximize the margin.
furthermore, we observea phenomenon that if target sentences in the train-ing data contain many words with negative mar-gin, they always do not correspond to the sourcesentences.
these data are harmful to model perfor-mance.
therefore, based on the mto, we furtherpropose a margin-based sentence-level objective(mso) by adding a dynamic weight function toalleviate the negative effect of these “dirty data”..we validate the effectiveness and superiority ofour approaches on the transformer (vaswani et al.,2017), and conduct experiments on large-scalewmt14 english-to-german, wmt19 chinese-to-english, and wmt14 english-to-french transla-tion tasks.
our contributions are:.
• we explore the connection between inade-quacy translation and the overconﬁdence ofthe lm in nmt, and thus propose an indicatorof the overconﬁdence degree, i.e., the marginbetween the nmt and the lm..• furthermore, to prevent the lm from beingoverconﬁdent, we propose two effective opti-mization objectives to maximize the margin,i.e., the margin-based token-level objective(mto) and the margin-based sentence-levelobjective (mso)..• experiments on wmt14 english-to-german,wmt19 chinese-to-english, and wmt14english-to-french show that our approachesbring in signiﬁcant improvements by +1.36,+1.50, +0.63 bleu points, respectively.
ad-ditionally, the human evaluation veriﬁes thatour approaches can improve both translationadequacy and ﬂuency..2 background.
given a source sentence x = {x1, x2, ..., xn }, thenmt model predicts the probability of a targetsentence y = {y1, y2, ..., yt } word by word:t(cid:89).
p (y|x) =.
p(yt|y<t, x),.
(1).
t=1.
where y<t = {y1, y2, ..., yt−1} is the partial trans-lation before yt.
from eq.
1, the source sentencex and partial translation y<t are considered in themeantime, suggesting that the nmt model is es-.
t(cid:88).
t=1.
t(cid:89).
t(cid:88).
t=1.
sentially a joint language model and the lm isinstinctively involved in nmt..based on the encoder-decoder architecture, theencoder of nmt maps the input sentence x to hid-den states.
at time step t, the decoder of nmt em-ploys the output of the encoder and y<t to predictyt.
the training objective of nmt is to minimizethe negative log-likelihood, which is also known asthe cross entropy loss function:.
ln m tce.
= −.
log p(yt|y<t, x)..(2).
the lm measures the probability of a targetsentence similar to nmt but without knowledge ofthe source sentence x:.
p (y) =.
p(yt|y<t)..(3).
t=1the lm can be regarded as the part of nmt de-coder that is responsible for ﬂuency, only takes y<tas input.
the training objective of the lm is almostthe same as nmt except for the source sentence x:.
llm.
ce = −.
log p(yt|y<t)..(4).
the nmt model predicts the next word yt ac-cording to the source sentence x and meanwhileensures that yt is ﬂuent with the partial translationy<t.
however, when nmt pays excessive atten-tion to translation ﬂuency, some source segmentsmay be neglected, leading to inadequacy problem.
this is exactly what we aim to address in this paper..3 the approach.
in this section, we ﬁrstly deﬁne the margin betweenthe nmt and the lm (section 3.1), which reﬂectsthe overconﬁdence degree of the lm.
then we putforward the token-level (section 3.2) and sentence-level (section 3.3) optimization objectives to max-imize the margin.
finally, we elaborate our two-stage training strategy (section 3.4)..3.1 margin between the nmt and the lm.
when the nmt model excessively focuses on par-tial translation, i.e., the lm is overconﬁdent, thenmt model degrades into the lm, resulting inhallucinated translations.
to prevent the overcon-ﬁdence problem, we expect that the nmt modeloutperforms the lm as much as possible in pre-dicting golden tokens.
consequently, we deﬁnethe margin between the nmt and the lm at the.
3457t-th time step by the difference of the predictedprobabilities of them:.
∆(t) = pnm t (yt|y<t, x) − plm (yt|y<t),.
(5).
where pnm t denotes the predicted probability ofthe nmt model, i.e., p(yt|y<t, x), and plm de-notes that of the lm, i.e., p(yt|y<t)..the margin ∆(t) is negatively correlated to theoverconﬁdence degree of the lm, and differentvalues of the margin indicate different cases:.
• if ∆(t) is big, the nmt model is apparentlybetter than the lm, and yt is strongly relatedto the source sentence x. hence the lm is notoverconﬁdent..• if ∆(t) is medium, the lm may be slightlyoverconﬁdent and the nmt model has thepotential to be enhanced..• if ∆(t) is small, the nmt model might de-grade to the lm and not correctly translatethe source sentence, i.e., the lm is overconﬁ-dent.2.
note that sometimes, the model needs to focusmore on the partial translation such as the word tobe predicted is a determiner in the target language.
in this case, although small ∆(t) does not indicatethe lm is overconﬁdent, enlarging the ∆(t) canstill enhance the nmt model..3.2 margin-based token-level objective.
based on the margin, we ﬁrstly deﬁne the marginloss lm and then fuse it into the cross entropyloss function to obtain the margin-based token-evel optimization objective (mto).
formally, wedeﬁne the margin loss lm to maximize the marginas follow:.
lm =.
(1 − pnm t (t))m(∆(t)),.
(6).
t(cid:88).
t=1.
where we abbreviate pnm t (yt|y<t, x) as pnm t (t).
m(∆(t)) is a function of ∆(t), namely marginfunction, which is monotonically decreasing (e.g.,1 − ∆(t)).
moreover, when some words have thesame ∆(t) but different pnm t (t), their meaningsare quite different: (1) if pnm t (t) is big, the nmtmodel learns the token well and does not need tofocus on the margin too much; (2) if pnm t (t) is.
2in addition, if pnm t (yt|y<t, x) is large, less attentionwill be paid to this data because yt has been learned well,which will be described in detail in section 3.2..figure 1: the four margin functions m(∆).
all ofthem are monotonically decreasing, yet with differentslopes.
compared with linear, the three non-linearfunctions are more stable around |∆| = 0 and steeperaround |∆| = 1. we set α in log to 10 in this ﬁgure..small, the nmt model is urgently to be optimizedon the token thus the weight of m(∆(t)) shouldbe enlarged.
therefore, as the weight of m(∆(t)),1 − pnm t (t) enables the model treat tokens wisely..variations of m(∆).
we abbreviate marginfunction m(∆(t)) as m(∆) hereafter.
a sim-ple and intuitive deﬁnition is the linear function:m(∆) = 1 − ∆, which has the same gradient fordifferent ∆.
however, as illustrated in section 3.1,different ∆ has completely various meaning andneeds to be treated differently.
therefore, we pro-pose three non-linear margin functions m(∆) asfollows:.
• cube: (1 − ∆3)/2.
• quintic (ﬁfth power): (1 − ∆5)/2.
• log: 11+∆ ) + 0.5..α log( 1−∆.
where α is a hyperparamater for log..as shown in figure 1, the four variations3 havequite different slopes.
speciﬁcally, the three non-linear functions are more stable around ∆ = 0 (e.g.,∆ ∈ [−0.5, 0.5]) than linear, especially quintic.
we will report the performance of the four m(∆)concretely and analyze why the three non-linearm(∆) perform better than linear in section 5.4.finally, based on lm , we propose the margin-.
based token-level objective (mto):.
lt = ln m t.ce.
+ λm lm ,.
(7).
ce.
where ln m tis the cross-entropy loss of the nmtmodel deﬁned in eq.
2 and λm is the hyperparam-eter for the margin loss lm ..3in order to keep the range of m(∆) roughly [0,1], we set.
linear function to (1 − ∆)/2..34581.000.750.500.250.000.250.500.751.000.00.20.40.60.81.0m()linearcubequiticlogfigure 2: the parallel sentences, i.e., the source and tar-get sentences, are sampled from the wmt19 chinese-to-english training dataset.
we also list an expert trans-lation of the source sentence.
the words in bold redhave negative margin.
this target sentence has morethan 50% tokens with negative margin, and these to-kens are almost irrelevant to the source sentence.
ap-parently, the target sentence is a hallucination and willharm the model performance..3.3 margin-based sentence-level objective.
furthermore, through analyzing the margin distri-bution of target sentences, we observe that the tar-get sentences in the training data which have manytokens with negative margin are almost “halluci-nations” of the source sentences (i.e., dirty data),thus will harm the model performance.
therefore,based on mto, we further propose the margin-based sentence-level objective (mso) to addressthis issue..compared with the lm, the nmt model pre-dicts the next word with more prior knowledge(i.e., the source sentence).
therefore, it is intuitivethat when predicting yt, the nmt model shouldpredict more accurately than the lm, as follow:.
pnm t (yt|y<t, x) > plm (yt|y<t)..(8).
the above equation is equivalent.
actually,to∆(t) > 0. the larger ∆(t) is, the more the nmtmodel exceeds the lm.
however, there are manytokens with negative margin through analyzing themargin distribution.
we conjecture the reason isthat the target sentence is not corresponding to thesource sentence in the training corpus, i.e., the tar-get sentence is a hallucination.
actually, we alsoobserve that if a large proportion of tokens in atarget sentence have negative margin (e.g., 50%),the sentence is probably not corresponding to thesource sentence, such as the case in figure 2. these“dirty” data will harm the performance of the nmtmodel..to measure the “dirty” degree of data, we de-ﬁne the sentence-level negative margin ratio ofparallel sentences (x, y) as follow:.
r(x, y) =.
#{yt ∈ y : ∆(t) < 0}#{yt : yt ∈ y}.
,.
(9).
where #{yt ∈ y : ∆(t) < 0} denotes the numberof tokens with negative ∆(t) in y, and #{yt : yt ∈y} is the length of the target sentence y..when r(x, y) is larger than a threshold k (e.g.,k=50%), the target sentence may be desperatelyinadequate, or even completely unrelated to thesource sentence, as shown in figure 2. in order toeliminate the impact of these seriously inadequatesentences, we ignore their loss during training bythe margin-based sentence-level objective (mso):.
ls = ir(x,y)<k · lt ,where ir(x,y)<k is a dynamic weight function insentence level.
the indicative function ir(x,y)<kequals to 1 if r(x, y) < k, else 0, where k is ahyperparameter.
lt is mto deﬁned in eq.
7..(10).
ir(x,y)<k is dynamic at the training stage.
dur-ing training, as the model gets better, its ability todistinguish hallucinations improves thus ir(x,y)<kbecomes more accurate.
we will analyze thechanges of ir(x,y)<k in section 5.4..3.4 two-stage training.
we elaborate our two-stage training in this section,1) jointly pretraining an nmt model and an auxil-iary lm, and 2) ﬁnetuning the nmt model..jointly pretraining.
the language model mech-anism in nmt cannot be directly evaluated, thuswe train an auxiliary lm to represent it.
we pre-train them together using a fusion loss function:.
ce.
ce.
(11).
lpre = ln m tand llmce.
+ λlm llmce ,where ln m tare the cross entropy lossfunctions of the nmt model and the lm deﬁnedin eq.
2 and eq.
4, respectively.
λlm is a hyperpa-rameter.
speciﬁcally, we jointly train them throughsharing their decoders’ embedding layers and theirpre-softmax linear transformation layers (vaswaniet al., 2017).
there are two reasons for joint train-ing: (1) making the auxiliary lm as consistent aspossible with the language model mechanism innmt; (2) avoiding abundant extra parameters..finetuning.
we ﬁnetune the nmt model by min-imizing the mto (lt in eq.
7) and mso (ls ineq.
10).4 note that the lm is not involved at theinference stage..4the lm can be ﬁxed or trained along with the nmt afterpretraining.
our experimental results show that continuoustraining the lm and ﬁxing the lm have analogous perfor-mance during the ﬁnetuning stage.
therefore, we only reportthe results of keeping the lm ﬁxed in this paper..3459source尽管他们是孪生儿, 但性格却截然不同.targethow didyour mother succeed in keeping the peace between these two very different men?expert translationalthough they are twins, they are quite different in character.
4 experimental settings.
we conduct experiments on three large-scale nmttasks, i.e., wmt14 english-to-german (en→de),wmt14 english-to-french (en→fr), and wmt19chinese-to-english (zh→en)..datasets.
for en→de, we use 4.5m trainingdata.
following the same setting in (vaswani et al.,2017), we use newstest2013 as validation set andnewstest2014 as test set, which contain 3000 and3003 sentences, respectively.
for en→fr, the train-ing dataset contains about 36m sentence pairs, andwe use newstest2013 with 3000 sentences as valida-tion set and newstest2014 with 3003 sentences astest set.
for zh→en, we use 20.5m training dataand use newstest2018 as validation set and new-stest2019 as test set, which contain 3981 and 2000sentences, respectively.
for zh→en, the numberof merge operations in byte pair encoding (bpe)(sennrich et al., 2016a) is set to 32k for both sourceand target languages.
for en→de and en→fr, weuse a shared vocabulary generated by 32k bpes..evaluation.
we measure the case-sensitivebleu scores using multi-bleu.perl 5 for en→deand en→fr.
for zh→en, case-sensitive bleuscores are calculated by moses mteval-v13a.plscript6.
moreover, we use the paired bootstrapresampling (koehn, 2004) for signiﬁcance test.
weselect the model which performs the best on thevalidation sets and report its performance on thetest sets for evaluation..model and hyperparameters.
we conduct ex-periments based on the transformer (vaswani et al.,2017) and implement our approaches with the open-source tooklit opennmt-py (klein et al., 2017).
fol-lowing the transformer-base setting in (vaswaniet al., 2017), we set the hidden size to 512 andthe encoder/decoder layers to 6. all three tasksare trained with 8 nvidia v100 gpus, and thebatch size for each gpu is 4096 tokens.
the beamsize is 5 and the length penalty is 0.6. adam op-timizer (kingma and ba, 2014) is used in all themodels.
the lm architecture is the decoder of thetransformer excluding the cross-attention layers,sharing the embedding layer and the pre-softmax.
linear transformation with the nmt model.
foren→de, zh→en, and en→fr, the number of train-ing steps is 150k for jointly pretraining stage and150k for ﬁnetuning7.
during pretraining, we setλlm to 0.01 for all three tasks8.
experimental re-sults shown in appendix a indicate that the lm hasconverged after pretraining for all the three tasks.
during ﬁnetuning, the margin function m(∆) insection 3.2 is set to quintic, and we will analyzethe four m(∆) in section 5.4. λm in eq.
7 is setto 5, 8, and 8 on en→de, en→fr and zh→en,respectively.
for mso, the threshold k in eq.
10is set to 30% for en→de and zh→en, 40% foren→fr.
the two hyperparameters (i.e., λm andk) are searched on validation sets, and the selec-tion details are shown in appendix b. the baselinemodel (i.e., vanilla transformer) is trained for 300ksteps for en→de, en→fr and zh→en.
moreover,we use a joint training model as our secondarybaseline, namely nmt+lm, by jointly training thenmt model and the lm throughout the trainingstage with 300k steps.
the training steps of all themodels are consistent, thus the experiment resultsare strictly comparable..5 results and analysis.
we ﬁrst evaluate the main performance of our ap-proaches (section 5.1 and 5.2).
then, the humanevaluation further conﬁrms the improvements oftranslation adequacy and ﬂuency (section 5.3).
fi-nally, we analyze the positive impact of our modelson the distribution of margin and explore how eachfragment of our method works (section 5.4)..5.1 results on en→de.
the results on wmt14 english-to-german(en→de) are summarized in table 1. we list theresults from (vaswani et al., 2017) and several re-lated competitive nmt systems by various meth-ods, such as minimum risk training (mrt) ob-jective (shen et al., 2016), simple fusion of nmtand lm (stahlberg et al., 2018), optimizing ade-quacy metrics (kong et al., 2019; feng et al., 2019)and improving the transformer architecture (yanget al., 2018; zheng et al., 2019; yang et al., 2019;weng et al., 2020b; yan et al., 2020a).
we re-.
5https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl.
6https://github.com/moses-smt/mosesdecoder/blob/mast-er/scripts/generic/mteval-v13a.pl.
7the lm does not need to be state-of-the-art.
the previousstudy of (baziotis et al., 2020) has shown that a more powerfullm does not lead to further improvements to nmt..8the experimental results show that the model is insensi-tive to λlm .
therefore we make λlm consistent for all thethree tasks..3460en→de.
↑.
system.
system.
existing nmt systems.
transformer (vaswani et al., 2017)mrt* (shen et al., 2016)simple fusion** (stahlberg et al., 2018)localness (yang et al., 2018)context-aware (yang et al., 2019)aol (kong et al., 2019)eval.
module (feng et al., 2019)past&future (zheng et al., 2019)dual (yan et al., 2020a)multi-task (weng et al., 2020b).
our nmt systems.
nmt (transformer).
+ lm.
+ mto+ mso.
27.327.7127.8828.1128.2628.0127.5528.1027.8628.25.
----------.
ref.
27.2227.97 +0.7528.47†‡ +1.2528.58†‡ +1.36.
table 1: case-sensitive bleu scores (%) on the test setof wmt14 en→de.
↑ denotes the improvement com-pared with the nmt baseline (i.e., transformer).
“†”:signiﬁcantly better than nmt (p<0.01).
“‡”: signiﬁ-cantly better than the joint model nmt+lm (p<0.01).
(mrt* in (shen et al., 2016) is rnn-based, and theresult reported here is implemented on transformer byweng et al.
(2020b).
**: we re-implement simple fu-sion on upon of transformer.).
implement the transformer model (vaswani et al.,2017) as our baseline.
similarly, we re-implementthe simple fusion (stahlberg et al., 2018) model.
9finally, the results of the joint training modelnmt+lm, and models with our mto and msoobjectives are reported..compared with the baseline, nmt+lm yields+0.75 bleu improvement.
based on nmt+lm,our mto achieves further improvement with +0.50bleu scores, indicating that preventing the lmfrom being overconﬁdent could signiﬁcantly en-hance model performance.
moreover, mso per-forms better than mto by +0.11 bleu scores,which implies that the “dirty data” in the train-ing dataset indeed harm the model performance,and the dynamic weight function ir(x,y)<k ineq.
10 could reduce the negative impact.
in conclu-sion, our approaches improve up to +1.36 bleuscores on en→de compared with the transformerbaseline and substantially outperforms the exist-ing nmt systems.
the results demonstrate theeffectiveness and superiority of our approaches..vaswani et al.
(2017)*nmt (transformer).
+ lm.
+ mto+ mso.
en→fr↑-ref.
zh→en↑-ref.
bleu-25.75.bleu38.141.0741.14 +0.07 25.90 +0.1541.56†‡ +0.49 26.94†‡ +1.1941.70†‡ +0.63 27.25†‡ +1.50.
table 2: case-sensitive bleu scores (%) on the testset of wmt14 en→fr and wmt19 zh→en.
↑ de-notes the improvement compared with the nmt base-line (i.e., transformer).
“†”: signiﬁcantly better thannmt (p<0.01).
“‡”: signiﬁcantly better than the jointmodel nmt+lm (p<0.01).
* denotes the results comefrom the cited paper..5.2 results on en→fr and zh→en.
results on wmt14 english-to-frenchthe(en→fr)and wmt19 chinese-to-english(zh→en) are shown in table 2. we also listthe results of (vaswani et al., 2017) and ourreimplemented transformer as the baselines..on en→fr, our reimplemented result is higherthan the result of (vaswani et al., 2017), since weupdate 300k steps while vaswani et al.
(2017)only update 100k steps.
many studies obtainsimilar results to ours (e.g., 41.1 bleu scoresfrom (ott et al., 2019)).
compared with the base-line, nmt+lm yields +0.07 and +0.15 bleu im-provements on en→fr and zh→en, respectively.
the improvement of nmt+lm on en→de in ta-ble 1 (i.e., +0.75) is greater than these two datasets.
we conjecture the reason is that the amount of train-ing data of en→de is much smaller than that ofen→fr and zh→en, thus nmt+lm is more likelyto improve the model performance on en→de..compared with nmt+lm, our mto achievesfurther improvements with +0.42 and +1.04 bleuscores on en→fr and zh→en, respectively, whichdemonstrates the performance improvement ismainly due to our margin-based objective ratherthan joint training.
moreover, based on mto, ourmso further yields +0.14 and +0.31 bleu im-provements.
in summary, our approaches improveup to +0.63 and +1.50 bleu scores on en→frand zh→en compared with the baselines, respec-tively, which demonstrates the effectiveness andgeneralizability of our approaches..5.3 human evaluation.
9the architectures of the lm and nmt model in simple.
fusion are consistent with our mto and mso..we conduct the human evaluation for translationsin terms of adequacy and ﬂuency.
firstly, we ran-.
3461modelnmt (transformer).
+ lm.
+ mto+ mso.
adequacy fluency ave.4.354.494.574.66.
4.044.124.264.41.
4.664.864.874.91.table 3: human evaluation on adequacy and ﬂuency..figure 3: the distribution of ∆ of nmt+lm and mso.
we randomly sample 100k sentence pairs from thetraining dataset of zh→en and compute the marginof their tokens.
the purple area is the overlap of thetwo models’ ∆ distributions.
the two distributions arequite different.
compared with nmt+lm, mso re-duces the distribution around ∆ = 0 and meanwhileincreases the distribution around ∆ = 1..domly sample 100 sentences from the test set ofwmt19 zh→en.
then we invite three annotatorsto evaluate the translation adequacy and ﬂuency.
five scales have been set up, i.e., 1, 2, 3, 4, 5. foradequacy, “1” means totally irrelevant to the sourcesentence, and “5” means equal to the source sen-tence semantically.
for ﬂuency, “1” represents notﬂuent and incomprehensible; “5” represents very“native”.
finally, we take the average of the scoresfrom the three annotators as the ﬁnal score..the results of the baseline and our approachesare shown in table 3. compared with the nmtbaseline, nmt+lm, mto and mso improve ad-equacy with 0.08, 0.22, and 0.37 scores, respec-tively.
most improvements come from our mar-gin-based methods mto and mso, and mso per-forms the best.
for ﬂuency, nmt+lm achieves0.2 improvement compared with nmt.
based onnmt+lm, mto and mso yield further improve-ments with 0.01 and 0.05 scores, respectively.
hu-man evaluation indicates that our mto and msoapproaches remarkably improve translation ade-quacy and slightly enhance translation ﬂuency..modelnmt + lm.
percent of ∆ < 0 (↓) average ∆ (↑)12.45% (ref).
+ mto 10.17% (-2.28%)+ mso 10.89% (-1.56%).
0.33 (ref)0.44 (+0.11)0.44 (+0.11).
table 4: the percent of ∆ < 0 and average ∆ of mod-els computed from the 100k sentence pairs introducedin figure 3. compared with nmt+lm, both mto andmso effectively reduce the percent of ∆ < 0 and im-prove the average ∆..5.4 analysis.
margin between the nmt and the lm.
firstly,we analyze the distribution of the margin betweenthe nmt and the lm (i.e., ∆ in eq.
5).
as shownin figure 3, for the joint training model nmt+lm,although most of the margins are positive, thereare still many tokens with negative margin and alarge amount of margins around 0. this indicatesthat the lm is probably overconﬁdent for many to-kens, and addressing the overconﬁdence problem ismeaningful for nmt.
by comparison, the margindistribution of mso is dramatically different withnmt+lm: the tokens with margin around 0 aresigniﬁcantly reduced, and the tokens with marginin [0.75, 1.0] are increased apparently..more precisely, we list the percentage of tokenswith negative margin and the average margin foreach model in table 4. compared with nmt+lm,mto and mso reduce the percentage of negativemargin by 2.28 and 1.56 points, respectively.
wenotice mso performs slightly worse than mto,because mso neglects the hallucinations duringtraining.
as there are many tokens with negativemargin in hallucinations, the ability of mso toreduce the proportion of ∆ < 0 is weakened.
wefurther analyze effects of mto and mso on theaverage of margin.
both mto and mso improvethe average of the margin by 33% (from 0.33 to0.44).
in conclusion, mto and mso both indeedincrease the margin between the nmt and the lm..variations of m(∆).
we compare the perfor-mance of the four margin functions m(∆) deﬁnedin section 3.2. we list the bleu scores of thetransformer baseline, nmt+lm and our mto ap-proach with the four m(∆) in table 5. all thefour variations bring improvements over nmt andnmt+lm.
the results of log with different α aresimilar to linear, while far lower than cube andquintic.
and quintic performs the best among allthe four variations.
we speculate the reason is that.
34621.000.750.500.250.000.250.500.751.00050k100k150k200k250k300kfrequencynmt+lmmsofunctionnmt (transformer).
+ lm.
+ linear+ cube+ quintic+ log (α = 5)+ log (α = 10)+ log (α = 20).
bleu25.7525.9026.1326.4526.9426.1226.0726.24.
↑ref+0.15+0.38+0.60+1.19+0.37+0.32+0.49.
table 5: case-sensitive bleu scores (%) on zh→entest set of mto with several variations of m(∆).
α is the hyperparameter of log.
all four m(∆)achieve bleu improvements compared with nmt andnmt+lm, and quintic performs the best..modelsnmt (transformer).
+ lm.
+ mto w/ weight+ mto w/o weight.
valid23.6723.6124.0923.36.test25.7525.9026.9425.85.table 6: case-sensitive bleu scores (%) on zh→envalidation set and test set of mto with (w/) and without(w/o) the weight 1 − pnm t (t)..figure 4: changes of the proportion of ir(x,y)<30% =0 on zh→en during ﬁnetuning for mso, and bleuscores (%) on the validation set of zh→en for mtoand mso.
the orange line corresponds to the left y-axis, and the green and blue lines correspond to theright y-axis.
we sample 100k sentence pairs in thetraining data and compute ir(x,y)<30%..ﬂattens out, which is consistent with the trend ofbleu of mso.
moreover, by adding the dynamicweight function, mso outperforms mto at moststeps..case study.
to better illustrate the translationquality of our approach, we show several transla-tion examples in appendix c. our approach graspsmore segments of the source sentences, which aremistranslated or neglected by the transformer..∆ ∈ [−0.5, 0.5] is the main range for improvement,and quintic updates more careful on this range (i.e.,with smaller slopes) as shown in figure 1..6 related work.
effects of the weight of m(∆).
in mto, wepropose the weight 1−pnm t (t) of the margin func-tion m(∆) in eq.
6. to validate the importanceof it, we remove the weight and the margin lossdegrades to lm = (cid:80)tt=1 m(∆(t)).
the resultsare listed in table 6. compared with nmt+lm,mto without weight performs worse with 0.25and 0.05 bleu decreases on the validation set andtest set, respectively.
compared with mto withweight, it decreases 0.73 and 1.09 bleu scores onthe validation set and test set, respectively.
thisdemonstrates that the weight 1 − pnm t (t) is indis-pensable for our approach..changes of ir(x,y)<k during training.
inmso, we propose a dynamic weight functionir(x,y)<k in eq.
10. figure 4 shows the changes ofir(x,y)<k in mso and the bleu scores of msoand mto during ﬁnetuning.
as the training con-tinues, our model gets more competent, and theproportion of sentences judged to be “dirty data”by our model increases rapidly at ﬁrst and then.
translation adequacy of nmt.
nmt suffersfrom the hallucination and inadequacy problemfor a long time (tu et al., 2016; m¨uller et al.,2020; wang and sennrich, 2020; lee et al., 2019).
many studies improve the architecture of nmtto alleviate the inadequacy issue, including track-ing translation adequacy by coverage vectors (tuet al., 2016; mi et al., 2016), modeling a globalrepresentation of source side (weng et al., 2020a),dividing the source sentence into past and futureparts (zheng et al., 2019), and multi-task learn-ing to improve encoder and cross-attention mod-ules in decoder (meng et al., 2016, 2018; wenget al., 2020b).
they inductively increase the trans-lation adequacy, while our approaches directly max-imize the margin between the nmt and the lm toprevent the lm from being overconﬁdent.
otherstudies enhance the translation adequacy by ade-quacy metrics or additional optimization objectives.
tu et al.
(2017) minimize the difference betweenthe original source sentence and the reconstructionsource sentence of nmt.
kong et al.
(2019) pro-.
346323.023.223.423.623.824.024.224.424.624.825.00.0560.0580.0600.0620.0640.0660.0680.0700.072155k175k195k215k235k255k275k295kbleuproportiontraining stepspropotion of i=0mtomsopose a coverage ratio of the source sentence by themodel translation.
feng et al.
(2019) evaluate theﬂuency and adequacy of translations with an evalu-ation module.
however, the metrics or objectivesin the above approaches may not wholly representadequacy.
on the contrary, our approaches are de-rived from the criteria of the nmt model and thelm, thus credible..language model augmented nmt.
languagemodels are always used to provide more infor-mation to improve nmt.
for low-resource tasks,the lm trained on extra monolingual data can re-rank the translations by fusion (g¨ulc¸ehre et al.,2015; sriram et al., 2017; stahlberg et al., 2018),enhance nmt’s representations (clinchant et al.,2019; zhu et al., 2020), and provide prior knowl-edge for nmt (baziotis et al., 2020).
for dataaugmentation, lms are used to replace words insentences (kobayashi, 2018; wu et al., 2018; gaoet al., 2019).
differently, we mainly focus on themargin between the nmt and the lm, and no ad-ditional data is required.
stahlberg et al.
(2018)propose the simple fusion approach to model thedifference between nmt and lm.
differently, itis trained to optimize the residual probability, pos-itively correlated to pnm t /plm which is hard tooptimize and the lm is still required in inference,slowing down the inference speed largely..data selection in nmt.
data selection and dataﬁlter methods have been widely used in nmt.
to balance data domains or enhance the dataquality generated by back-translation (sennrichet al., 2016b), many approaches have been pro-posed, such as utilizing language models (mooreand lewis, 2010; van der wees et al., 2017;zhang et al., 2020), translation models (junczys-dowmunt, 2018; wang et al., 2019a), and curricu-lum learning (zhang et al., 2019b; wang et al.,2019b).
different from the above methods, ourmso dynamically combines language models withtranslation models for data selection during train-ing, making full use of the models..7 conclusion.
level and sentence-level objectives to maximize themargin.
experimental results on three large-scaletranslation tasks demonstrate the effectiveness andsuperiority of our approaches.
the human evalua-tion further veriﬁes that our methods can improvetranslation adequacy and ﬂuency..acknowledgments.
the research work descried in this paper has beensupported by the national nature science founda-tion of china (no.
12026606).
the authors wouldlike to thank the anonymous reviewers for theirvaluable comments and suggestions to improve thispaper..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..christos baziotis, barry haddow, and alexandra birch.
2020. language model prior for low-resource neu-ral machine translation.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 7622–7634, on-line.
association for computational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..stephane clinchant, kweon woo jung, and vassilinanikoulina.
2019. on the use of bert for neu-in proceedings of the 3rdral machine translation.
workshop on neural generation and translation,pages 108–117, hong kong.
association for com-putational linguistics..yang feng, wanying xie, shuhao gu, chenzeshao, wen zhang, zhengxin yang, and dong yu.
2019. modeling ﬂuency and faithfulness for di-arxiv preprintverse neural machine translation.
arxiv:1912.00178..we alleviate the problem of inadequacy translationfrom the perspective of preventing the lm frombeing overconﬁdent.
speciﬁcally, we ﬁrstly pro-pose an indicator of the overconﬁdence degree ofthe lm in nmt, i.e., margin between the nmtand the lm.
then we propose margin-based token-.
fei gao, jinhua zhu, lijun wu, yingce xia, taoqin, xueqi cheng, wengang zhou, and tie-yan liu.
2019. soft contextual data augmentation for neuralmachine translation.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 5539–5544, florence, italy.
asso-ciation for computational linguistics..3464c¸ aglar g¨ulc¸ehre, orhan firat, kelvin xu, kyunghyuncho, lo¨ıc barrault, huei-chi lin, fethi bougares,holger schwenk, and yoshua bengio.
2015. on us-ing monolingual corpora in neural machine transla-tion.
corr, abs/1503.03535..marcin junczys-dowmunt.
2018. dual conditionalcross-entropy ﬁltering of noisy parallel corpora.
inproceedings of the third conference on machinetranslation: shared task papers, pages 888–895,belgium, brussels.
association for computationallinguistics..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander rush.
2017. opennmt: open-insource toolkit for neural machine translation.
proceedings of acl 2017, system demonstrations,pages 67–72, vancouver, canada.
association forcomputational linguistics..sosuke kobayashi.
2018. contextual augmentation:data augmentation by words with paradigmatic re-in proceedings of the 2018 conference oflations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 452–457,new orleans, louisiana.
association for computa-tional linguistics..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..xiang kong, zhaopeng tu, shuming shi, eduardhovy, and tong zhang.
2019. neural machine trans-lation with adequacy-oriented learning.
proceed-ings of the aaai conference on artiﬁcial intelli-gence, 33(01):6618–6625..katherine lee, orhan firat, ashish agarwal, clarafannjiang, and david sussillo.
2019. hallucinationsin neural machine translation..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..fandong meng, zhengdong lu, hang li, and qun liu.
2016. interactive attention for neural machine trans-in proceedings of coling 2016, the 26thlation.
international conference on computational linguis-tics: technical papers, pages 2174–2185, osaka,japan..fandong meng, zhaopeng tu, yong cheng, haiyangwu, junjie zhai, yuekui yang, and di wang.
2018.neural machine translation with key-value memory-augmented attention.
in proceedings of ijcai..fandong meng and jinchao zhang.
2019. dtmt: anovel deep transition architecture for neural machinetranslation.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 33, pages 224–231..haitao mi, baskaran sankaran, zhiguo wang, and abeittycheriah.
2016. coverage embedding models forin proceedings of theneural machine translation.
2016 conference on empirical methods in natu-ral language processing, pages 955–960, austin,texas.
association for computational linguistics..robert c. moore and william lewis.
2010. intelligentin pro-selection of language model training data.
ceedings of the acl 2010 conference short papers,pages 220–224, uppsala, sweden.
association forcomputational linguistics..mathias m¨uller, annette rios, and rico sennrich.
2020. domain robustness in neural machine trans-lation.
in proceedings of the 14th conference of theassociation for machine translation in the americas(volume 1: research track), pages 151–164, virtual.
association for machine translation in the ameri-cas..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016a.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..shiqi shen, yong cheng, zhongjun he, wei he, huawu, maosong sun, and yang liu.
2016. minimumrisk training for neural machine translation.
in pro-ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1683–1692, berlin, germany.
asso-ciation for computational linguistics..3465anuroop sriram, heewoo jun, sanjeev satheesh, andadam coates.
2017. cold fusion: training seq2seqcorr,models together with language models.
abs/1708.06426..rongxiang weng, haoran wei, shujian huang, hengyu, lidong bing, weihua luo, and jiajun chen.
2020a.
gret: global representation enhanced trans-former.
aaai..felix stahlberg, james cross, and veselin stoyanov.
2018. simple fusion: return of the language model.
in proceedings of the third conference on machinetranslation: research papers, pages 204–211, brus-sels, belgium.
association for computational lin-guistics..rongxiang weng, heng yu, xiangpeng wei, and wei-hua luo.
2020b.
towards enhancing faithfulness forin proceedings of theneural machine translation.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 2675–2684,online.
association for computational linguistics..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
advances in neural information processing systems,27:3104–3112..zhaopeng tu, yang liu, lifeng shang, xiaohua liu,and hang li.
2017. neural machine translation within proceedings of the 31st aaaireconstruction.
conference on artiﬁcial intelligence..zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,and hang li.
2016. modeling coverage for neuralmachine translation.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 76–85..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..chaojun wang and rico sennrich.
2020. on exposurebias, hallucination and domain shift in neural ma-chine translation.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 3544–3552, online.
association forcomputational linguistics..shuo wang, yang liu, chao wang, huanbo luan, andimproving back-translationmaosong sun.
2019a.
with uncertainty-based conﬁdence estimation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 791–802, hong kong, china.
association for computa-tional linguistics..wei wang, isaac caswell, and ciprian chelba.
2019b.
dynamically composing domain-data selection withclean-data selection by “co-curricular learning” forin proceedings of theneural machine translation.
57th annual meeting of the association for compu-tational linguistics, pages 1282–1292..marlies van der wees, arianna bisazza, and christofmonz.
2017. dynamic data selection for neural ma-chine translation.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 1400–1410, copenhagen, den-mark.
association for computational linguistics..xing wu, shangwen lv, liangjun zang, jizhong han,and songlin hu.
2018. conditional bert contextualaugmentation..jianhao yan, fandong meng, and jie zhou.
2020a.
dual past and future for neural machine translation..jianhao yan, fandong meng, and jie zhou.
2020b.
multi-unit transformers for neural machine transla-in proceedings of the 2020 conference ontion.
empirical methods in natural language processing(emnlp), pages 1047–1059, online..baosong yang, jian li, derek f wong, lidia s chao,xing wang, and zhaopeng tu.
2019. context-awareself-attention networks.
in proceedings of the aaaiconference on artiﬁcial intelligence, pages 387–394..baosong yang, zhaopeng tu, derek f. wong, fandongmeng, lidia s. chao, and tong zhang.
2018. mod-eling localness for self-attention networks.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4449–4458, brussels, belgium.
association for computa-tional linguistics..boliang zhang, ajay nagesh, and kevin knight.
2020.parallel corpus ﬁltering via pre-trained languagein proceedings of the 58th annual meet-models.
ing of the association for computational linguistics,pages 8545–8554, online.
association for computa-tional linguistics..wen zhang, yang feng, fandong meng, di you, andqun liu.
2019a.
bridging the gap between train-ing and inference for neural machine translation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4334–4343, florence, italy..xuan zhang, pamela shapiro, gaurav kumar, paulmcnamee, marine carpuat, and kevin duh.
2019b.
curriculum learning for domain adaptation in neu-ral machine translation.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1903–1915, minneapolis, minnesota.
association for computational linguistics..zaixiang zheng, shujian huang, zhaopeng tu, xin-yu dai, and jiajun chen.
2019. dynamic past and.
3466in proceed-future for neural machine translation.
ings of the 2019 conference on empirical methodsin natural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 931–941, hongkong, china.
association for computational lin-guistics..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tieyan liu.
2020.incorporating bert into neural machine translation.
in international conference on learning represen-tations..a loss of the language model.
to validate whether the lm is converged or not af-ter pretraining, we plot the loss of the lm as shownin figure 5. the loss of the lm remains stable aftertraining about 80k steps for en→de, zh→en anden→fr, indicating that the lm is converged duringpretraining stage..(a) λm (en→de).
(b) k (en→de).
(c) λm (en→fr).
(d) k (en→fr).
(e) λm (zh→en).
(f) k (zh→en).
figure 6: case-sensitive bleu scores (%) on valida-tion sets of wmt14 en→de, wmt14 en→fr andwmt19 zh→en with different hyperparameters, re-spectively.
λm is deﬁned in eq.
7, and the search re-sults are shown in figure (a), (c) and (e).
the thresholdk for mso is deﬁned in eq.
10 and the results of it areshown in figure (b), (d), and (f)..with negative margin in a target sentence is greaterthan 30% or 40%, the sentence is most likely to bea hallucination..as shown in figure 7, our approach outperformsthe base model (i.e., the transformer) in translationadequacy.
in case 1, the base model generates “ontuesday”, which is unrelated to the source sentence,i.e., hallucination, and under-translates “novem-ber 5” and “the website of the chinese embassyin mongolia” information in the source sentence.
however, our approach translates the above twosegments well.
in case 2, the base model reversesthe chronological order of the source sentence, thusgenerates a mis-translation, while our model trans-lates perfectly.
in case 3, the base model neglectstwo main segments of the source sentence (the textin bold blue font) and leads to the inadequacy prob-lem.
however, our model takes them into account.
according to the three examples, we conclude thatour approach alleviates the inadequacy problemwhich is extremely harmful to nmt..figure 5: the loss of the lm on the validation set dur-ing pretraining for en→de, zh→en and en→fr.
thelm converges after training nearly 80k steps for all thethree tasks..c case study.
b hyperparameters selection.
the results of our approaches with different λm(deﬁned in eq.
7) and k (deﬁned in eq.
10) onthe validation sets of wmt14 en→de, wmt14en→fr and wmt19 zh→en are shown in fig-ure 6. we ﬁrstly search the best λm based on mto.
all the three datasets achieve better performancefor λm ∈ [5, 10].
the model reaches the peakwhen λm =5, 8, and 8 for the three tasks, respec-tively.
then, ﬁxing the best λm for each dataset,we search the best threshold k. as shown in theright of figure 6, the best k is 30% for en→de andzh→en, 40% for en→fr.
this is consistent withour observations.
when the proportion of tokens.
3467020k40k60k80k100k120k140ktraining steps5.05.56.06.57.07.58.0lossen>dezh>enen>fr135810m27.027.127.227.327.4bleu20%30%40%50%threshold  in s27.127.227.327.427.5bleu135810m41.241.341.441.541.6bleu20%30%40%50%threshold  in s41.7541.8541.9542.05bleu135810m23.723.823.924.024.124.2bleu20%30%40%50%threshold  in s24.024.124.224.324.424.5bleufigure 7: several example sentence pairs (src, ref) from wmt19 zh→en test set.
we list the translation of thetransformer baseline (base) and our mso method (ours).
the text in bold red font is mistranslated by the basemodel.
the text in bold blue font is mistranslated or under-translated by the base model but translated correctly byour model..3468case 1src 1中新网11月5日电据中国驻蒙古国大使馆网站4日消息,近日,中国公民郭玉芹和毛润新在蒙旅游期间失联。ref 1report on november 5of china news: the website of the chinese embassy in mongolia reported on november 5 that chinese citizens guo yuqin and mao runxinhad been missing when traveling in mongolia.base 1chinese citizens guo yu-qin and mao yunxinlost their ties during a trip to mongolia, china said on tuesday.ours 1chinese citizens guo yuqin and mao runxinlost their ties during a trip to mongolia, according to the website of the chinese embassy in mongolia on november 5.case 2src 2对此央视发表快评:这是我国英雄烈士保护法施行后第一个烈士纪念日。ref 2for this, cctv issued a quick comment: this was the first memorial day after the implementation of the law for the protection of heroes and martyrs in china.base 2cctv released a quick comment on this: this is our heroic martyrs protection law after the implementation of the first martyr anniversary.ours 2cctv issued a quick comment on this: this is the first martyr memorial day after the implementation of our country's heroic martyr protection law.case 3src 3据外媒报道,南非首都比勒陀利亚郊区的一处保育中心里,两只小狮子一起嬉闹玩耍,很难看出有任何异常之处,不过它们其实绝无仅有。ref 3according to foreign media reports, it was hard for people to find anything unusual in two little lions playing in a conservation center located in the suburb in pretoria, the capital of south africa, but they were absolutely unique.base 3it's hard to see anything unusual in a nursing home in a suburb of pretoria, south africa's capital, where two lions play together.ours 3according to foreign media reports, in a care center on the outskirts of pretoria, south africa, two lions play together, it is difficult to see any abnormalities, but they are unique.