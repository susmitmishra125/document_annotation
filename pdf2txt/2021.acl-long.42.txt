e2e-vlp: end-to-end vision-language pre-training enhanced by visuallearning.
haiyang xu∗, ming yan, chenliang li, bin bi,songfang huang, wenming xiao, fei huangalibaba group{shuofeng.xhy, ym119608, lcl193798, b.bi}@alibaba-inc.com{songfang.hsf, wenming.xiaowm, f.huang}@alibaba-inc.com.
abstract.
vision-language pre-training (vlp) on large-scale image-text pairs has achieved huge suc-cess for the cross-modal downstream tasks.
the most existing pre-training methods mainlyadopt a two-step training procedure, whichﬁrstly employs a pre-trained object detectorto extract region-based visual features, thenconcatenates the image representation and textembedding as the input of transformer totrain.
however, these methods face problemsof using task-speciﬁc visual representation ofthe speciﬁc object detector for generic cross-modal understanding, and the computation in-efﬁciency of two-stage pipeline..in this paper, we propose the ﬁrst end-to-endvision-language pre-trained model for bothv+l understanding and generation, namelye2e-vlp, where we build a uniﬁed trans-former framework to jointly learn visual rep-resentation, and semantic alignments betweenimage and text.
we incorporate the tasks ofobject detection and image captioning into pre-training with a uniﬁed transformer encoder-decoder architecture for enhancing visuallearning.
an extensive set of experiments havebeen conducted on well-established vision-language downstream tasks to demonstrate theeffectiveness of this novel vlp paradigm..1.introduction.
self-supervised pre-training has achieved great suc-cess in a wide range of natural language under-standing (devlin et al., 2018; liu et al., 2019;wang et al., 2019; lan et al., 2019) and genera-tion tasks (song et al., 2019; lewis et al., 2019; biet al., 2020).
recent studies (li et al., 2019; luet al., 2019; chen et al., 2019; tan and bansal,2019; li et al., 2020b; yu et al., 2020) havealso witnessed the progress of self-supervised pre-training on vision-and-language tasks, which learns.
∗corresponding author.
general cross-modal representations from massiveimage-text pairs, and ﬁne-tunes vision-languagepre-training (vlp) models on task-speciﬁc dataachieving state-of-the-art results on various down-stream v+l tasks..most existing mainstream vlp models adopta two-step training method, which ﬁrstly extractssemantic visual features using a pre-trained objectdetection model, and then combines the derivedobject-centric representation of the image and textembedding as the input of transformer (vaswaniet al., 2017) for cross-modal pre-training.
despitethe superior performance brought by the large-scaleimage-text pairs, the two-stage solution suffersfrom the following weaknesses: 1) the object de-tection model in the ﬁrst step is trained on speciﬁcvisual dataset such as visual genome dataset (kr-ishna et al., 2017), and the visual representation isnot optimized towards a more generic cross-modalunderstanding in the second step.
it may sufferfrom an error propagation problem when the objectdetection model fails to recognize certain importantinformation.
2) extracting region features with anobject detection model is so time-consuming thatmost state-of-the-art models are directly trainedand evaluated on cached visual features.
this prac-tice not only imposes unnecessary constraints onmodel designs, but also confronts the run-time in-ference inefﬁciency in the prediction phase..recently, several studies such as (jiang et al.,2020) have begun to revisit the grid features forcross-modal understanding and found the grid fea-tures can also work surprisingly well, while makingthe model design and training process much sim-pler.
one pioneering work pixel-bert (huanget al., 2020) explores to pre-train with grid featuresin an end-to-end fashion directly from pixels.
it re-moves all the ﬁne-grained visual pre-training tasks,which proves to be important for v+l pre-training.
(zhang et al., 2021) also demonstrates that visual.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages503–513august1–6,2021.©2021associationforcomputationallinguistics503features provided by the object detection modelmatter signiﬁcantly in vlp models..to address the limitations, we propose a new end-to-end paradigm for pixel-level vision-languagepre-training, namely e2e-vlp, by enhancing withﬁne-grained visual learning.
during pre-training,e2e-vlp jointly learns the visual region featuresand the cross-modal representation in a uniﬁedtransformer encoder-decoder architecture directlyfrom image pixels.
in addition to the typicalpre-training tasks of masked language modelingand image-text matching, we enhance the vision-language pre-training with ﬁne-grained visual se-mantic learning.
speciﬁcally, two end-to-end pre-training tasks are further incorporated: 1) objectdetection:inspired from detr (carion et al.,2020), we view the object detection as a directset prediction problem.
the cross-modal trans-former encoder and image encoder are joint learntto fuse the cross-modal data from pixels, while thedecoder is used to capture ﬁne-grained visual infor-mation via bipartite matching between predictedand ground-truth objects; 2) image-text genera-tion: to better understand the semantics within theimage, we also use the paired text to guide thelearning of image features.
we use the encodernetwork to represent the image and a left-to-rightdecoder to generate the caption text.
the stan-dard auto-regressive language model objective isused to maximize the data probability.
these twotasks can help learn high-quality visual representa-tions (zhang et al., 2021; desai and johnson, 2020).
detection task can learn object-level visual seman-tics, while the image caption task can capture text-aligned visual semantics.
these two kinds of visualsemantics matter signiﬁcantly in vlp cross-modalfusion.
during ﬁne-tuning, e2e-vlp can be ﬂexi-bly applied to vision-language understanding taskswith the encoder module, and vision-language gen-eration tasks with the encoder-decoder module..we evaluate e2e-vlp on a variety of represen-tative vision-language tasks, including visual ques-tion answering, natural language visual reasoning,cross-modal retrieval and image captioning.
withthe new end-to-end pre-training paradigm, we canobtain surprising good performance across differ-ent v+l tasks and greatly decrease the online in-ference time with the new one-stage solution..we make the following major contributions in.
this paper:• we propose the ﬁrst end-to-end vision-language.
pre-trained model for both v+l understandingand generation, namely e2e-vlp, which canachieve comparable or superior performancewith faster online inference speedup..• e2e-vlp is the ﬁrst model that incorporatesﬁne-grained visual pre-training in an encoder-decoder architecture, which paves a new wayfor designing advanced vision and language pre-training tasks..• we enhance cross-modal feature fusion by vi-sual learning of object detection and image cap-tion, which has empirically shown to be effec-tive for vision-language pre-training..2 related work.
self-supervised pre-training has substantially ad-vanced the performance across a variety of naturallanguage understanding (devlin et al., 2018; liuet al., 2019; wang et al., 2019; lan et al., 2019) andtext generation tasks (song et al., 2019; lewis et al.,2019; bi et al., 2020).
inspired by language modelpre-training, several researchers propose vision-language pre-training(vlp) models on large-scaleimage-text pairs, which has proved effective fora wide range of vision-language (vl) tasks, suchas vqa (antol et al., 2015), nlvr (young et al.,2014), cross-modal retrieval (suhr et al., 2018).
the current vlp models mainly take two-steptraining pipeline, which consists of extracting se-mantic visual features by object detector and train-ing the cross-modal pre-training model to align textand visual features.
in this kind of method, thereare mainly two broad directions to conduct vision-language pre-training.
the ﬁrst line uses a single-stream transformer architecture (vaswani et al.,2017) to model both image and text representationsin a uniﬁed semantic space such as vlbert (suet al., 2019), uniter (chen et al., 2019) andoscar (li et al., 2020b).
in contrast, the otherline adopts a two-stream transformer architecturethat ﬁrst encodes the image and text modalitiesseparately, and then fuses the cross-modal repre-sentations with another transformer network, suchas lxmert (tan and bansal, 2019) and ernie-vil (yu et al., 2020).
besides, semvlp (li et al.,2021) is pre-trained iteratively with two prevalentfashions.
these methods are directly trained andevaluated on cached visual features, which im-poses unnecessary constraints on model designsand makes it hard to enable an end-to-end vision-language pre-training.
furthermore, pixel-bert.
504figure 1: the overall framework of e2e-vlp.
our model employs a uniﬁed encoder-decoder transformer frame-work to learn visual representation, and semantic alignment between image and text jointly..(huang et al., 2020) represents the ﬁrst and onlywork to pre-train with grid features in an end-to-end fashion.
however, due to the characteristics oflearnt grid features, the end-to-end pre-training isconducted without object-level visual tasks, whichis important in aligning the semantics betweencross-modal representations..in this paper, we focus on enhancing the end-to-end vision-language pre-training with more ﬁne-grained visual semantic learning.
the object detec-tion task and image caption task are incorporatedinto the pre-training stage for further improvingthe ﬁne-grained visual-language understanding andgeneration abilities..3 e2e-vlp pre-training.
3.1 model architecture.
the architecture of e2e-vlp is shown in fig-ure 1. inspired by the recent breakthrough of us-ing transformer on computer vision tasks suchas detr (carion et al., 2020) and vit trans-former (dosovitskiy et al., 2020), we proposeto use a transformer encoder-decoder frame-work (vaswani et al., 2017) for cross-modal learn-ing, and a simple cnn backbone module is usedas the image encoder for extracting visual represen-tations from pixels so as to allow for more ﬂexiblenetwork design.
we jointly train the whole frame-work in an end-to-end fashion, so as to learn the.
generic visual representations and high-level cross-modal alignment simultaneously.
different v+lpre-training tasks are designed to further enhancethe cross-modal understanding and generation abil-ities.
next, we describe each component of thismodel in detail..3.1.1.input representations.
the input to e2e-vlp is an image and its relatedtext (e.g.
caption text).
we ﬁrst introduce the wayto represent the text sequence and raw image pixelsas input to the transformer..sentence embeddings each sentence is ﬁrstsplit into a sequence of sub-words {w1, ..., wm} bywordpiece tokenizer.
then, similar to bert (de-vlin et al., 2018), each token wi is assigned threekinds of embeddings: token, segment and positionembeddings.
the three embeddings are summedand layer-normalized to represent input sentencerepresentations as a sequence of embedding vectorseemb = {ecls, e1, ..., em, esep }, where [cls]and [sep ] are special tokens in bert..image representations for image feature rep-resentation, the most existing vlp models followbottom-up and top-down attention (andersonet al., 2018) to extract region features by faster r-cnn (ren et al., 2015) trained on visual genomedataset.
the detector extracts region features byﬁrst detecting regions under pre-deﬁned categories,.
505and then uses the features before the ﬁnal classi-ﬁer as the output.
these methods are limited tothe task-speciﬁc visual representation of the spe-ciﬁc object detector, which may hinder the genericcross-modal understanding..32 , w = w0.
to improve the generalization of the image rep-resentation, we learn from pixels to represent an im-age instead of using bounding boxes.
the pixel fea-tures are learned by a cnn visual backbone such asresnet (he et al., 2016).
starting from the initialimage vimg ∈ r3×h0×w0 (with 3 color channels),a conventional cnn backbone generates a lower-resolution activation map fimg ∈ rc×h×w usingthe typical values as in detr (carion et al., 2020):c = 2048 and h = h032 .
then, we takea 1 × 1 convolution to reduce the channel dimen-sion of the high-level activation map f from c toa smaller dimension d, creating a new feature mapzimg ∈ rd×h×w .
the encoder expects a sequenceas input, hence we collapse the spatial dimensionsof zimg into one dimension, resulting in a hw × dfeature map zimg.
since the transformer architec-ture is permutation-invariant, we supplement thefeature maps with ﬁxed positional encodings (par-mar et al., 2018) that are added to the input of eachattention layer.
finally, the sequential image repre-sentation zimg = {o1, ..., ohw } can be seen as ahw length of d-dimensional vector..3.1.2 cross-modal encoder pre-training.
given the embeddings of the tokens for the sen-tence {ei}mi=1 and the sequential image represen-tations {oj}nj=1, we adopt the transformer en-coder to learn cross-modal attention between im-age grid features and language tokens.
the en-coder is a stacked model with l standard blocks,where the l-th block consists of a multi-headself-attention module and a feed forward network(ffn).
to allow a ﬁne-grained feature-level se-mantic fusion, we directly concatenate the de-rived image features and text embeddings to con-struct the input sequence, which is formulated as:{ecls, e1, ..., em, esep , o1, ..., ohw }..the cnn backbone for visual representationlearning and the transformer for cross-modal se-mantic fusion is combined into a single model,which is end-to-end trainable.
in this way, thelearnt visual feature representation can be moresuitable for the pre-training tasks of generic cross-modal understanding.
to facilitate cross-modalunderstanding, we follow (tan and bansal, 2019;chen et al., 2019; huang et al., 2020) and conduct.
two popular pre-training tasks in encoder side, in-cluding masked language modeling (mlm) andimage-text matching (itm)..masked language modeling the task setup isbasically the same as in bert (devlin et al., 2018),we randomly mask 15% tokens in the text and themodel is asked to predict these masked words withthe output text and visual representations.
differentfrom mlm task in bert that only relies on thesurrounding text of textual modality for prediction,the masked words will be predicted with the helpof image feature map from visual modality so as toresolve ambiguity..image-text matching we randomly sample50% mismatched image-text pairs and 50%matched pairs, and train an classiﬁer to predictwhether an image and a sentence match each otheron the representation of token [cls] in the lastencoder layer hl.
cls..3.1.3 visual-enhanced decoder.
due to that the cnn feature map has no object-level semantics, it is difﬁcult to directly align thecross-modal semantics between cnn feature mapand the language embeddings.
therefore, we fur-ther add a transformer decoder to help capture theﬁne-grained semantics of the visual features, wheretwo speciﬁc pre-training tasks of object detectionand image-caption generation are incorporated..the decoder adopts the standard architecture ofthe transformer with multi-headed self-attentionfollowed by cross-attention and a feed forward net-work (ffn).
both tasks share the same attentionparameters of decoder, while using different linearhead for the two tasks.
the object detection taskfocuses more on understanding the ﬁne-grained ob-ject information within image, while image caption-ing task helps guide the learning of visual featuresregarding the textual semantics..enhanced by object detection following theone-stage detection model detr (carion et al.,2020), we deﬁne object detection task as the directset prediction problem, and use a set-based globalloss that forces unique predictions via bipartitematching with the transformer encoder-decoderarchitecture..let us denote by y the ground truth set of objectsand ˆy = {ˆyi}ni=1.
the set-based loss of bipartitematching is to search for a permutation of n ele-.
506ments σ ∈ ln with the lowest cost:.
3.2.joint training.
ˆσ = arg min.
σ∈ϕn.
n(cid:88).
i.lmatch(yi, ˆyσ(i)).
(1).
where lmatch(yi, ˆyσ(i)) is a pair-wise matchingcost between ground truth yi and a prediction withindex σ(i)..the hungarian algorithm (stewart et al., 2016) isused to efﬁciently compute the optimal assignment.
different from the original detr for single-modallearning, our cross-modal pre-training with objectdetection differs in two aspects..in encoder side, we combine both the visual rep-resentation and language embedding as input andreuse the transformer encoder for cross-modal fu-sion.
in decoder side, we take the learned positionalembeddings as the input to multiple l transformerdecoder layers, and detects the n objects in parallelat each decoder layer.
in addition to the tasks of boxcoordinate regression and class category prediction,we also incorporate an object attribute predictiontask for visual genome dataset so as to enhancethe learning of ﬁne-grained semantics.
the modelis trained with a negative log-likelihood loss forattribute, class prediction and a box regression lossdeﬁned as follows:.
n(cid:88).
lv(y, ˆy) =.
[−logˆpˆσ(i)(ai) − logˆpˆσ(i)(ci) +.
i=1+ lbox(bi, ˆbˆσ(i)(i))].
where ˆpˆσ(i)(ai), ˆpˆσ(i)(ci) is the attribute andclass probability, lbox(bi, ˆbˆσ(i)(i)) is a normalizedbounding boxes regression loss as in (carion et al.,2020)..enhanced by image captioning to guide thelearning of visual features in regards to the tex-tual semantics, we use semantically dense captionsto learn vision representations with sequence-to-sequence (seq2seq) image-to-text generation task.
the decoder is pre-trained to auto-regressively gen-erate the target text based on the contextual rep-resentations from the image encoder.
the pre-training loss for the decoder is deﬁned as:.
ldec = −.
log.
p (yt|y<t, x).
(2).
(cid:88).
(x,y)∈(x ,y).
n(cid:89).
t=1.
where x represents the sequence of vision context,y represents the set of text to be generated and nis the length of tokens in output text y..we pre-train e2e-vlp with all the encoder anddecoder pre-training tasks (i.e., masked languagemodeling, image-text matching, object detection,image-to-text generation) jointly by minimizingthe four loss functions as:.
l = lmlm + litm + lv + ldec.
(3).
4 experiments.
4.1 pre-training dataset.
we pre-train our e2e-vlp on two in-domainimage-text datasets: ms-coco (lin et al., 2014)and visual genome (krishna et al., 2017).
weutilize the object detection and image caption anno-tations in ms-coco, and object detection, regiondescription annotations in visual genome.
thetotal amount of the dataset is 6.01m image-and-sentence pairs on 180k distinct images..4.2.implementation details.
the maximum sequence length for the sentence isset as 40. we use scale augmentation, and resizethe input images so that the shortest side is at least480 and at most 800 pixels while the longest is atmost 1333 (carion et al., 2020).
for the modelarchitecture, we pre-train e2e-vlp with 6 and 12layers of transformer encoder respectively, whilethe decoder is ﬁxed as 6 layers.
each layer blockhas 256 hidden units and 12 self-attention heads,the intermediate layer size is 1,024. the visualbackbone is selected as resnet with different sizes(he et al., 2016) from torchvision with frozen batch-norm layers.
we pre-train e2e-vlp model with atotal batch size of 32 for 200 epoches on 8 v100gpus.
we use the adamw optimizor (loshchilovand hutter, 2018) for both the transformer andresnet.
the initial learning rate is set as 10−4for transformer and 10−5 for resnet.
the weightdecay is set as 10−4..5 experiments.
5.1 downstream tasks.
we compare e2e-vlp model against other com-petitive vlp models of the comparable model sizeon the following downstream v+l tasks.
• vqa v2.0 (antol et al., 2015): the vqatask requires the model to answer natural lan-guage questions given an image.
we conductexperiments on the widely-used vqa v2.0.
507models.
params.
vqa.
test-dev.
test-std dev.
nlvr2.
coco captiontest-p bleu4 cider.
single-stream.
two-stream.
visualbertvlpvlbertunicoder-vluniteroscar.
vilbert12-in-1lxmerternie-vil.
110m 70.80110m 70.5110m 71.16110m -110m 72.70110m 73.16.
221m 70.55221m 73.15183m 72.42210m 72.62.
----77.1478.07.
67.40-74.90-.
----77.8778.36.
67.00-74.50-.
-36.5---36.5.
----.
-.
-116.9---123.7.
----.
-.
71.0070.7--72.9173.61.
70.92-72.5472.85.
71.42.
73.67.end2end.
pixelbert.
142m 71.35.
71.7.
72.4.our model.
e2e-vlp.
94m.
73.25.
77.25.
77.96.
36.2.
117.3.table 1: evaluation results on vqa, nlvr2 and image caption..models.
params.
ir-flickr30kr@5.r@1.r@10 r@1.tr-flickr30kr@5.r@10.single-stream.
two-stream.
visualbertvlbertunicoder-vluniteroscar.
vilbert12-in-1lxmerternie-vil.
110m -110m -110m 71.50110m 72.52110m -.
221m 58.20221m 67.90183m -210m 74.44.
--90.9092.36-.
84.90--92.72.
--94.9096.08-.
91.52--95.94.
--86.2085.90-.
---86.70.
--96.3097.10-.
---97.80.
--99.0098.80-.
---99.00.end2end.
pixelbert.
142m 59.8.
85.5.
91.6.
75.7.
94.7.
97.1.our model.
e2e-vlp.
94m.
73.58.
92.42.
96.03.
86.24.
97.50.
98.92.table 2: evaluation results on flickr30k..dataset (antol et al., 2015), which contains204k images and 1.1m questions about theseimages.
following (anderson et al., 2018), wetreat vqa as a multi-label classiﬁcation task bypicking an answer from a shared set consistingof 3,129 answers.
to ﬁne-tune vqa task, weuse a binary cross-entropy loss to train a multi-label classiﬁer, we train with a batch size of 32for 12 epochs.
we set an initial learning rate of1e-4 which decays by 0.1 at the end of epoch 6and epoch 9..• nlvr2 (suhr et al., 2018): nlvr2 (suhr et al.,2018) is a challenging task for visual reason-ing.
the goal is to determine whether a natu-ral language statement is true about a pair ofimages.
it consists of 86k/7k data for train-ing/development.
since each data example innlvr2 has two natural images img0, img1 andone language statement s, we concatenate thegiven sentence and each image to build two se-quences, and then train a binary classiﬁer basedon the concatenation of the two outputs.
we.
ﬁne-tune nlvr model with a batch size of 32for 12 epochs, and set an initial learning rate of1e-4 which decays by 0.1 at the end of epoch 6and epoch 9..• image caption: a visual generation task thatrequires the model to generate the content of animage.
to ﬁne-tune image caption task, we usethe seq2seq loss with label smoothing(szegedyet al., 2016).
during inference, we use beamsearch (i.e., beam size=4), and set α = 0.9 forthe length penalty (wu et al., 2016).
we setinitial learning rate of 1e-4 which decays by0.1 at the end of epoch 6 and epoch 9. we re-port our results on the coco image captioningdataset (chen et al., 2015)..• image-text retrieval: the image-text re-trieval task consists of two sub-tasks: imageretrieval and text retrieval, depending on whichmodality is used as the retrieval target.
we con-duct experiments on flickr30k dataset (younget al., 2014), which contains 31,000 images col-.
508lected from flickr website and each image has 5captions.
we follow the same split in (lee et al.,2018) for training and evaluation.
during ﬁne-tuning, we follow the method in uniter (chenet al., 2019) and formulate it as a ranking prob-lem.
we use the hidden state of hlcls to com-pute the similarity scores for the sampled posi-tive and negative pairs, and maximize the mar-gin between them through circle loss (sun et al.,2020) as ernie-vil (yu et al., 2020).
we ﬁne-tune our model with a batch size of 64 and alearning rate of 5e-5 for 4 epochs..5.2 baseline methods.
we compare our e2e-vlp model with all thei.e., single-three prevalent vlp architectures:stream and two-stream architectures of two-steppipeline framework and end-to-end one-step so-lution.
single-stream architecture uses a uniﬁedtransformer to encode the vision-language inputs,including the state-of-the-art methods such as os-car(li et al., 2020b), uniter(chen et al., 2019),unicoder-vl (li et al., 2020a), vlbert (su et al.,2019) and vlp (zhou et al., 2020).
image andtext are separately encoded ﬁrstly and then fusedtogether in two-stream architecture, including thestate-of-the-art methods such as ernie-vil(yuet al., 2020), lxmert (tan and bansal, 2019),vilbert (lu et al., 2019, 2020).
these two ar-chitectures both adopt the region-based visual fea-tures, where a object detector is ﬁrst used to obtainthe object-level feature representations.
we alsocompare with the only end-to-end solution pixel-bert (huang et al., 2020).
pixelbert adoptsa random pixel sampling strategy to conduct thecross-modal pre-training, while it has no visual se-mantic understanding tasks for pre-training whichis very important in v+l tasks..5.3 main results.
the results on the downstream v+l tasks areshown in table 1. it can be observed that: 1) withless parameters and only in-domain pre-trainingdata (ms-coco and visual genome), e2e-vlpcan consistently achieve comparable performanceagainst two-step region feature-based methods suchas oscar and ernie-vil.
it shows the effective-ness of our end-to-end grid feature-based method,which can offer new perspectives to address thecross-modal pre-training and conduct fusion at amore ﬁne-grained level.
it has the potential of re-moving the complex procedure of region feature ex-.
model.
vqa nlvr2.
e2e-vlp-image-to-text generation-attribute prediction-object detection.
70.7670.2069.9268.85.
72.1271.5970.9270.38.table 3: ablation tests for different visual pre-trainingtasks of e2e-vlp (6 layer encoder, and resnet50backbone) on development set..traction, and facilitate deeper interaction betweenvisual feature and text data in an end-to-end fash-ion.
2) our e2e-vlp method can signiﬁcantlyimprove upon the end-to-end method pixelbert,which demonstrates the advantages of our methodfor enhancing the ﬁne-grained visual learning withobject detection and image captioning,.
5.4.importance of visual learning.
to further investigate the importance of each com-ponent in our method, we conduct ablation studiesto assess the impact of different visual learningtasks on the vqa and nlvr2 development set.
table 3 shows the result.
we can see that: 1) all thethree visual pre-training tasks contribute to the ﬁnalperformance gain, and removing each of them candecrease the performance on both tasks.
the objectdetection and attribute prediction tasks can helpcapture ﬁne-grained object-level semantics withinthe image, which is consistent with the previoustwo-step solutions that using region features fromthe detection can help improve the performancefor cross-modal understanding.
the image-to-textgeneration task can help guide the learning of vi-sual features in regards to the textual semantics,which has the same conclusion as virtex (desaiand johnson, 2020).
2) among the different vi-sual pre-training tasks, the object detection andattribute prediction tasks are more important thanthe image-to-text generation task, this may be dueto the fact that the typical cross-modal downstreamtasks such as vqa and nlvr2 focus more on theﬁne-grained semantics of the objects within image..5.5.inference efﬁciency.
one of the biggest advantages of end-to-end vlpmethod is the inference efﬁciency with one singlestage.
therefore, we further examine the onlineinference efﬁciency of e2e-vlp, compared withthe two-step region-based models (uniter andlxmert) and the existing end-to-end vlp model(pixelbert).
we examine the average inference.
509model.
parameters.
vqa nlvr2.
avg time(ms).
lxmertuniterpixel-bert.
e2e-vlp.
183m110m142m.
94m.
496501201.
192.
72.4272.7071.35.
73.25.
72.5477.1471.7.
77.25.table 4: results of the inference comparison of dif-ferent pre-trained model architectures on the vqa andnlvr2 dataset..layers backbone.
params vqa nlvr2.
666121212.r50r101r152r50r101r152.
49m68m84m59m78m94m.
70.5671.4272.2371.3472.4373.25.
72.1274.3476.2173.0475.2377.25.table 5: results of different pre-trained model architec-tures on development set..time (per query) of different models on the vqadataset.
the result is shown in table 4. we can seethat: 1) the end-to-end methods can be much moreefﬁcient in online inference (2-3 times speedup)than the two-step model.
we further analyze theinference time of different components of two-stepmodels and ﬁnd that among the total cost of 500msper image-text pair, about 80% of the total time isused to extract region-based features using fasterr-cnn (ren et al., 2015).
it takes much time forregion selection and this will happen twice whenextracting the ﬁnal regions, and it contains manycomplicated post-processing procedures.
2) oure2e-vlp model can achieve comparable resultson both the vqa and nlvr2 datasets by savingabout 3.5 times running time.
besides, we canalso use a smaller image size to further improv-ing the inference speed.
compared with pixel-bert, e2e-vlp can also obtain some speed-upsdue to the reason that the transformer hidden sizeof e2e-vlp is only 256, which makes e2e-vlpmore light-weight and ﬂexible.
our end-to-end so-lution can signiﬁcantly improve the performanceupon pixelbert, because there are no visual pre-training tasks for pixelbert and we enhance thepre-training of e2e-vlp with both the ﬁne-grainedobject detection and image captioning tasks..5.6 architecture selection.
architectures by changing the number of trans-former encoder layers and the different resnet vi-sual backbone layers.
we expect to further examinewhether the visual backbone or transformer net-work is more important for the cross-modal under-standing and fusion.
from table 5, we can see thatboth adding more transformer encoder layers andusing more complicated visual backbones can con-tribute to the ﬁnal performance gain, which provesthe importance of both modules for cross-modalunderstanding.
learning better visual features andconducting more deeply interacted visual-languagefusion are both important for v+l tasks.
besides,we can see that using a more strong visual back-bone (such as resnet 152) can give more beneﬁtto the ﬁnal performance than just increasing thenumber of transformer encoder layers from 6 to12. this may be due to the fact that visual seman-tic understanding is rather important in v+l tasksand that is also why we design more ﬁne-grainedvisual pre-training tasks for further enhancing thelearning of e2e-vlp..5.7.impact of input image size.
as mentioned in section 3.1.1, the sequence lengthof the visual features is determined by the imagesize hw .
therefore, the ﬁnal sequence length ofthe input to the transformer also largely depends onthe image size, which can in turn inﬂuence the in-ference speed of our whole framework.
we furtheranalyze the impact of input image size to the efﬁ-ciency and effectiveness of e2e-vlp.
the resultsof e2e-vlp with different image sizes as inputare shown in table 6. from the results, we cansee that e2e-vlp beneﬁts from larger images asinput, and for larger images, the sequence lengthof the visual representation is longer and more in-formation is embedded in the visual representation.
the cross-modal transformer is capable of learn-ing more ﬁne-grained vision-language fusion forbetter performance.
moreover, down-sampling theimage to a smaller size can signiﬁcantly improvethe inference speed of e2e-vlp model, while themodel accuracy only decreases a little.
for exam-ple, when changing the input size from (800, 1333)to (448, 448), the inference can be about 5 timesfaster while the performance only decreases about2%-3%..since our whole framework contains both the vi-sual backbone and transformer network as a whole,we further study the importance of different model.
5.8 object detection with paired text.
finally, we expect to further examine whether thecross-modal fusion is stable and e2e-vlp capture.
510input size.
shorter side.
longer side.
speedup vqa nlvr2.
448448600800.
44874610001333.
5x3x1.5x-.
71.1472.0473.0873.25.
75.4375.7976.8777.25.table 6: impact of input image size on the vqa andnlvr2 set..model.
ap.
ap50 aps apm apl.
detre2e-vlp.
40.641.9.
61.662.6.
19.920.3.
44.345.6.
60.261.1.table 7: results of object detection on mscoco de-velopment dataset.
ﬁne-grained semantics by visual learning.
there-fore, we encode both the image content and cap-tion text with e2e-vlp, and directly ﬁne-tune iton mscoco object detection benchmark datasetwith the decoder as in detr(carion et al., 2020).
table 7 shows the detection result.
we can see thatour e2e-vlp model can also support the objectdetection task based on text-image pairs and per-form surprising well compared with the originaldetr model.
this phenomenon may also demon-strate that e2e-vlp well captures the ﬁne-grainedsemantics within image and can appropriately fusethe multi-modal information for conducting visual-only task..6 conclusion.
in this paper, we propose a new end-to-endparadigm for pixel-level vision-language pre-training, to jointly learn visual representation, andsemantic alignments between image and text.
dif-ferent from the previous methods using the re-gion features in a two-stage pipeline, we pro-pose to use the more ﬂexible and efﬁcient imagegrid features for vision-language pre-training.
wefurther incorporate the tasks of object detectionand image captioning into pre-training with a uni-ﬁed transformer encoder-decoder architecture forenhancing visual learning.
the experiments onwell-established vision-language downstream tasksdemonstrate the effectiveness and efﬁciency of oure2e-vlp model.
we hope that this study can po-tentially offer new perspectives and guide for end-to-end vision-language pre-training..in the future, we will explore more deeply in-teracted ways for image-text fusion from a bottom.
layer, and incorporate more advanced vision andlanguage pre-training tasks for further improvingthe performance..references.
peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in proceedings of the ieee internationalconference on computer vision, pages 2425–2433..bin bi, chenliang li, chen wu, ming yan, wei wang,songfang huang, fei huang, and luo si.
2020.palm: pre-training an autoencoding&autoregressivelanguage model for context-conditioned generation.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8681–8691..nicolas carion, francisco massa, gabriel synnaeve,nicolas usunier, alexander kirillov, and sergeyzagoruyko.
2020. end-to-end object detection withtransformers.
in european conference on computervision, pages 213–229.
springer..xinlei chen, hao fang, tsung-yi lin, ramakr-ishna vedantam, saurabh gupta, piotr doll´ar, andc lawrence zitnick.
2015. microsoft coco captions:data collection and evaluation server.
arxiv preprintarxiv:1504.00325..yen-chun chen, linjie li, licheng yu, ahmedel kholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2019. uniter: universal image-textrepresentation learning..karan desai and justin johnson.
2020. virtex: learn-ing visual representations from textual annotations.
arxiv preprint arxiv:2006.06666..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..alexey dosovitskiy,.
lucas beyer, alexanderkolesnikov, dirk weissenborn, xiaohua zhai,thomas unterthiner, mostafa dehghani, matthiasminderer, georg heigold, sylvain gelly, et al.
2020.an image is worth 16x16 words: transformersarxiv preprintfor image recognition at scale.
arxiv:2010.11929..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition..511computer vision and pattern recognition, pages 770–778..zhicheng huang, zhaoyang zeng, bei liu, dongmeifu, and jianlong fu.
2020. pixel-bert: aligning im-age pixels with text by deep multi-modal transform-ers.
arxiv preprint arxiv:2004.00849..huaizu jiang, ishan misra, marcus rohrbach, eriklearned-miller, and xinlei chen.
2020. in defenseinof grid features for visual question answering.
proceedings of the ieee/cvf conference on com-puter vision and pattern recognition, pages 10267–10276..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a shamma, et al.
2017. visual genome: connecting language and vi-sion using crowdsourced dense image annotations.
international journal of computer vision, 123(1):32–73..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learningin international con-of language representations.
ference on learning representations..kuang-huei lee, xi chen, gang hua, houdong hu,and xiaodong he.
2018. stacked cross attentionin proceedings of thefor image-text matching.
european conference on computer vision (eccv),pages 201–216..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..chenliang li, ming yan, haiyang xu, fuli luo, weiwang, bin bi, and songfang huang.
2021. semvlp:vision-language pre-training by aligning semanticsat multiple levels.
arxiv preprint arxiv:2103.07829..gen li, nan duan, yuejian fang, ming gong, anddaxin jiang.
2020a.
unicoder-vl: a universal en-coder for vision and language by cross-modal pre-in proceedings of the aaai conferencetraining.
on artiﬁcial intelligence, volume 34, pages 11336–11344..liunian harold li, mark yatskar, da yin, cho-juihsieh, and kai-wei chang.
2019. visualbert: asimple and performant baseline for vision and lan-guage.
arxiv preprint arxiv:1908.03557..xiujun li, xi yin, chunyuan li, xiaowei hu,pengchuan zhang, lei zhang, lijuan wang,houdong hu, li dong, furu wei, et al.
2020b.
aligned pre-trainingoscar:arxiv preprintfor vision-languagearxiv:2004.06165..object-semanticstasks..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2018. decoupledin international con-.
weight decay regularization.
ference on learning representations..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems, pages 13–23..jiasen lu, vedanuj goswami, marcus rohrbach, deviparikh, and stefan lee.
2020.
12-in-1: multi-taskvision and language representation learning.
inproceedings of the ieee/cvf conference on com-puter vision and pattern recognition, pages 10437–10446..niki parmar, ashish vaswani, jakob uszkoreit, lukaszkaiser, noam shazeer, alexander ku, and dustinin internationaltran.
2018.conference on machine learning, pages 4055–4064.
pmlr..image transformer..shaoqing ren, kaiming he, ross girshick, and jiansun.
2015. faster r-cnn: towards real-time ob-ject detection with region proposal networks.
inadvances in neural information processing systems,pages 91–99..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in in-ternational conference on machine learning, pages5926–5936.
pmlr..russell stewart, mykhaylo andriluka, and andrew yng.
2016. end-to-end people detection in crowdedin proceedings of the ieee conferencescenes.
on computer vision and pattern recognition, pages2325–2333..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2019. vl-bert: pre-training of generic visual-linguistic representations.
arxiv preprint arxiv:1908.08530..alane suhr, stephanie zhou, ally zhang, iris zhang,huajun bai, and yoav artzi.
2018. a corpus forreasoning about natural language grounded in pho-tographs.
arxiv preprint arxiv:1811.00491..yifan sun, changmao cheng, yuhan zhang, chizhang, liang zheng, zhongdao wang, and yichenwei.
2020. circle loss: a uniﬁed perspective of.
512pair similarity optimization.
in proceedings of theieee/cvf conference on computer vision and pat-tern recognition, pages 6398–6407..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkinginthe inception architecture for computer vision.
proceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-formers.
arxiv preprint arxiv:1908.07490..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..wei wang, bin bi, ming yan, chen wu, zuyi bao,jiangnan xia, liwei peng, and luo si.
2019. struct-incorporating language structures into pre-bert:arxivtraining for deep language understanding.
preprint arxiv:1908.04577..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap between hu-arxiv preprintman and machine translation.
arxiv:1609.08144..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
transactions of theassociation for computational linguistics, 2:67–78..fei yu, jiji tang, weichong yin, yu sun, haotian, hua wu, and haifeng wang.
2020. ernie-vil: knowledge enhanced vision-language repre-arxiv preprintsentations through scene graph.
arxiv:2006.16934..pengchuan zhang, xiujun li, xiaowei hu, jianweiyang, lei zhang, lijuan wang, yejin choi, andjianfeng gao.
2021. vinvl: making visual repre-sentations matter in vision-language models.
arxivpreprint arxiv:2101.00529..luowei zhou, hamid palangi, lei zhang, houdonghu, jason corso, and jianfeng gao.
2020. uni-ﬁed vision-language pre-training for image caption-ing and vqa.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 34, pages 13041–13049..513