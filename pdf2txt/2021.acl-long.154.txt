uxla: a robust unsupervised data augmentation framework forzero-resource cross-lingual nlp.
m saiful bari ∗ ¶, tasnim mohiuddin ∗¶, and shaﬁq joty¶§¶ nanyang technological university, singapore§ salesforce research asia, singapore{bari0001@e., mohi0004@e., srjoty@}ntu.edu.sg.
abstract.
transfer learning has yielded state-of-the-art(sota) results in many supervised nlp tasks.
however, annotated data for every target taskin every target language is rare, especially forlow-resource languages.
we propose uxla anovel unsupervised data augmentation frame-work for zero-resource transfer learning sce-narios.
in particular, uxla aims to solve cross-lingual adaptation problems from a sourcelanguage task distribution to an unknowntargetlanguage task distribution, assumingno training label in the target language.
atits core, uxla performs simultaneous self-training with data augmentation and unsuper-vised sample selection.
to show its effective-ness, we conduct extensive experiments onthree diverse zero-resource cross-lingual trans-fer tasks.
uxla achieves sota results in allthe tasks, outperforming the baselines by agood margin.
with an in-depth framework dis-section, we demonstrate the cumulative contri-butions of different components to its success..1.introduction.
self-supervised learning in the form of pretrainedlanguage models (lm) has been the driving forcein developing state-of-the-art nlp systems in re-cent years.
these methods typically follow twobasic steps, where a supervised task-speciﬁc ﬁne-tuning follows a large-scale lm pretraining (rad-ford et al., 2019).
however, getting labeled datafor every target task in every target language isdifﬁcult, especially for low-resource languages..recently, the pretrain-ﬁnetune paradigm hasalso been extended to multi-lingual setups to traineffective multi-lingual models that can be usedfor zero-shot cross-lingual transfer.
jointly traineddeep multi-lingual lms like mbert (devlin et al.,2019) and xlm-r (conneau et al., 2020) coupled.
∗equal contribution.
with supervised ﬁne-tuning in the source languagehave been quite successful in transferring linguisticand task knowledge from one language to anotherwithout using any task label in the target language.
the joint pretraining with multiple languages al-lows these models to generalize across languages.
despite their effectiveness, recent studies (pireset al., 2019; k et al., 2020) have also highlightedone crucial limiting factor for successful cross-lingual transfer.
they all agree that the cross-lingual generalization ability of the model is limitedby the (lack of) structural similarity between thesource and target languages.
for example, for trans-ferring mbert from english, k et al.
(2020) reportabout 23.6% accuracy drop in hindi (structurallydissimilar) compared to 9% drop in spanish (struc-turally similar) in cross-lingual natural languageinference (xnli).
the difﬁculty level of transferis further exacerbated if the (dissimilar) target lan-guage is low-resourced, as the joint pretraining stepmay not have seen many instances from this lan-guage in the ﬁrst place.
in our experiments (§3.2),in cross-lingual ner (xner), we report f1 reduc-tions of 28.3% in urdu and 30.4% in burmese forxlm-r, which is trained on a much larger multi-lingual dataset than mbert..one attractive way to improve cross-lingualgeneralization is to perform data augmentation(simard et al., 1998), and train the model on exam-ples that are similar but different from the labeleddata in the source language.
formalized by the vic-inal risk minimization (vrm) principle (chapelleet al., 2001), such data augmentation methods haveshown impressive results in vision (zhang et al.,2018; berthelot et al., 2019).
these methods en-large the support of the training distribution bygenerating new data points from a vicinity distri-bution around each training example.
for images,the vicinity of a training image can be deﬁned bya set of operations like rotation and scaling, or by.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1978–1992august1–6,2021.©2021associationforcomputationallinguistics1978linear mixtures of features and labels (zhang et al.,2018).
however, when it comes to text, such unsu-pervised augmentation methods have rarely beensuccessful.
the main reason is that unlike images,linguistic units are discrete and a smooth changein their embeddings may not result in a plausiblelinguistic unit that has similar meanings..in nlp, to the best of our knowledge, the mostsuccessful augmentation method has so far beenback-translation (sennrich et al., 2016) which para-phrases an input sentence through round-trip trans-lation.
however, it requires parallel data to traineffective machine translation systems, acquiringwhich can be more expensive for low-resource lan-guages than annotating the target language data.
furthermore, back-translation is only applicable ina supervised setup and to tasks where it is possibleto ﬁnd the alignments between the original labeledentities and the back-translated entities, such as inquestion answering (yu et al., 2018).
other relatedwork includes contextual augmentation (kobayashi,2018), conditional bert (wu et al., 2018) andaug-bert (shi et al., 2019).
these methods usea constrained augmentation that alters a pretrainedlm to a label-conditional lm for a speciﬁc task.
since they rely on labels, their application is lim-ited by the availability of enough task labels..in this work, we propose uxla, a robustunsupervised cross-lingual augmentation frame-work for improving cross-lingual generalization ofmultilingual lms.
uxla augments data from theunlabeled training examples in the target languageas well as from the virtual input samples gener-ated from the vicinity distribution of the sourceand target language sentences.
with the augmenteddata, it performs simultaneous self-learning withan effective distillation strategy to learn a stronglyadapted cross-lingual model from noisy (pseudo)labels for the target language task.
we proposenovel ways to generate virtual sentences using amultilingual masked lm (conneau et al., 2020),and get reliable task labels by simultaneous multi-lingual co-training.
this co-training employs a two-stage co-distillation process to ensure robust trans-fer to dissimilar and/or low-resource languages..we validate the effectiveness and robustness ofuxla by performing extensive experiments onthree diverse zero-resource cross-lingual transfertasks–xner, xnli, and paws-x, which positdifferent sets of challenges, and across many (14in total) language pairs comprising languages that.
are similar/dissimilar/low-resourced.
uxla yieldsimpressive results on xner, setting sota in alltested languages outperforming the baselines by agood margin.
the relative gains for uxla are par-ticularly higher for structurally dissimilar and/orlow-resource languages: 28.54%, 16.05%, and9.25% absolute improvements for urdu, burmese,and arabic, respectively.
for xnli, with only5% labeled data in the source, it gets compara-ble results to the baseline that uses all the la-beled data, and surpasses the standard baselineby 2.55% on average when it uses all the labeleddata in the source.
we also have similar ﬁnd-ings in paws-x.
we provide a comprehensiveanalysis of the factors that contribute to uxla’sperformance.
we open-source our framework athttps://ntunlpsg.github.io/project/uxla/ ..2 uxla framework.
while recent cross-lingual transfer learning effortshave relied almost exclusively on multi-lingualpretraining and zero-shot transfer of a ﬁne-tunedsource model, we believe there is a great poten-tial for more elaborate methods that can leveragethe unlabeled data better.
motivated by this, wepresent uxla, our unsupervised data augmenta-tion framework for zero-resource cross-lingual taskadaptation.
figure 1 gives an overview of uxla.
let ds = (xs, ys) and dt = (xt) denote thetraining data for a source language s and a tar-get language t, respectively.
uxla augments datafrom various origins at different stages of train-ing.
in the initial stage (epoch 1), it uses the aug-mented training samples from the target language(d(cid:48)t) along with the original source (ds).
in laterstages (epoch 2-3), it uses vicinal sentences gen-erated from the vicinity distribution of source andn|xstarget examples: ϑ(˜xsn), wherexsn ∼ xs and xtn ∼ xt.
it performs self-trainingon the augmented data to acquire the correspond-ing pseudo labels.
to avoid conﬁrmation bias withself-training where the model accumulates its ownerrors, it simultaneously trains three task modelsto generate virtual training data through data aug-mentation and ﬁltering of potential label noises viamulti-epoch co-teaching (zhou and li, 2005)..n) and ϑ(˜xt.
n|xt.
in each epoch, the co-teaching process ﬁrst per-forms co-distillation, where two peer task modelsare used to select “reliable” training examples totrain the third model.
the selected samples withpseudo labels are then added to the target task.
1979figure 1: training ﬂow of uxla.
after training the base task models θ(1), θ(2), and θ(3) on source labeled data ds (warmup),we use two of them (θ(j), θ(k)) to pseudo-label and co-distill the unlabeled target language data (d(cid:48)t).
a pretrained lm(gen-lm) is used to generate new vicinal samples for both source and target languages, which are also pseudo-labeled andco-distilled using the two task models (θ(j), θ(k)) to generate ˜ds and ˜dt.
the third model θ(l) is then progressively trained onthese datasets: {ds, d(cid:48).
t} in epoch 1, ˜dt in epoch 2, and all in epoch 3..model’s training data by taking the agreement fromthe other two models, a process we refer to as co-guessing.
the co-distillation and co-guessing mech-anism ensure robustness of uxla to out-of-domaindistributions that can occur in a multilingual setup,e.g., due to a structurally dissimilar and/or low-resource target language.
algorithm 1 gives a pseu-docode of the overall training method.
each of thetask models in uxla is an instance of xlm-r ﬁne-tuned on the source language task (e.g., englishner), whereas the pretrained masked lm param-eterized by θmlm (i.e., before ﬁne-tuning) is usedto deﬁne the vicinity distribution ϑ(˜xn|xn, θmlm)around each selected example xn.
in the following,we describe the steps in algorithm 1..2.1 warm-up: training task models.
we ﬁrst train three instances of the xlm-r model(θ(1), θ(2), θ(3)) with an additional task-speciﬁc lin-ear layer on the source language (english) labeleddata.
each model has the same architecture (xlm-r large) but is initialized with different randomseeds.
for token-level prediction tasks (e.g., ner),the token-level representations are fed into the clas-siﬁcation layer, whereas for sentence-level tasks(e.g., xnli), the [cls] representation is used asinput to the classiﬁcation layer..training with conﬁdence penalty our goal isto train the task models so that they can be usedreliably for self-training on a target language thatis potentially dissimilar and low-resourced.
in suchsituations, an overly conﬁdent (overﬁtted) modelmay produce more noisy pseudo labels, and thenoise will then accumulate as the training pro-gresses.
overly conﬁdent predictions may also im-.
pose difﬁculties on our distillation methods (§2.3)in isolating good samples from noisy ones.
how-ever, training with the standard cross-entropy (ce)loss may result in overﬁtted models that produceoverly conﬁdent predictions (low entropy), espe-cially when the class distribution is not balanced.
we address this by adding a negative entropy term−h to the ce loss as follows..l(θ) =.
c(cid:88).
(cid:104).
c=1.
− yc log pc(cid:123)(cid:122)ce.
(cid:124).
θ(x)(cid:125).
+ pc(cid:124).
θ(x) log pc(cid:123)(cid:122)−h.
θ(x)(cid:125).
(cid:105).
(1).
where x is the representation that goes to the outputlayer, and yc and pcθ(x) are respectively the groundtruth label and model predictions with respect toclass c. such regularizer of output distribution hasbeen shown to be effective for training large models(pereyra et al., 2017).
we also report signiﬁcantgains with conﬁdence penalty in §3.
appendix bshows visualizations on why conﬁdence penalty ishelpful for distillation..2.2 sentence augmentation.
n|xt.
our augmentated sentences come from two dif-ferent sources: the original target language sam-ples xt, and the virtual samples generated fromthe vicinity distribution of the source and targetn, θmlm) and ϑ(˜xtsamples: ϑ(˜xsn|xsn, θmlm) withxsn ∼ xs and xtn ∼ xt.
it has been shown thatcontextual lms pretrained on large-scale datasetscapture useful linguistic features and can be used togenerate ﬂuent grammatical texts (hewitt and man-ning, 2019).
we use xlm-r masked lm (conneauet al., 2020) as our vicinity model θmlm, which istrained on massive multilingual corpora (2.5 tbof common-crawl data in 100 languages).
the.
1980, y (k).
for k ∈ {1, 2, 3} do.
t = distil(xt, ηe, θ(k)).
x (k)tfor j ∈ {1, 2, 3} do.
(cid:46) infer and select tgt training data for augmentation..(cid:46) warm up with conf.
penalty.
(cid:46) e denotes epoch..algorithm 1 uxla: a robust unsupervised data augmentation framework for cross-lingual nlpinput: source (s) and target (t) language datasets: ds = (xs, ys), dt = (xt); task models: θ(1), θ(2), θ(3), pre-trained maskedlm θmlm, mask ratio p , diversiﬁcation factor δ, sampling factor α, and distillation factor ηoutput: models trained on augmented data1: θ(1), θ(2), θ(3) = warmup(ds, θ(1), θ(2), θ(3))2: for e ∈ [1 : 3] do3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23: return {θ(1), θ(2), θ(3)}.
if k == j then continue/* source language data augmentation */˜xs = gen-lm(xs, θmlm, p, δ)x (k)s˜ds = agreement(cid:0)d(k), y (k)s/* target language data augmentation (no vicinity) */x (j)t = distil(xt, ηe, θ(j))tt = (x (k)t = (x (j)d(cid:48)/* target language data augmentation */˜xt = gen-lm(xt, θmlm, p, δ)t = distil( ˜xt, ηe, θ(k));x (k)t˜dt = agreement(cid:0)d(k)t = (x (k)/* train new models on augmented data */for l ∈ {1, 2, 3} do.
with sampling factor α, train θ(l) on d,where d = {ds1(e ∈ {1, 3}) ∪ d(cid:48).
t = distil( ˜xt, ηe, θ(j)), y (j)t.s = distil( ˜xs, ηe, θ(k));s = (x (k).
s = distil( ˜xs, ηe, θ(j)), y (j).
t1(e ∈ {1, 3}) ∪ ˜ds1(e = 3) ∪ ˜dt1(e ∈ {2, 3})}.
t = agreement(cid:0)d(k).
, y (j)t = (x (j).
, y (j)s = (x (j).
(cid:46) vicinal example generation..(cid:46) vicinal example generation..if l (cid:54)= j and l (cid:54)= k then.
x (j)s), d(j).
x (j)t), d(j).
(cid:46) train progressively.
(cid:46) see line 4.
), d(j).
, y (k)t., y (k)t., y (j)t.s )(cid:1).
, y (k).
, y (k).
, y (j).
)(cid:1).
)(cid:1).
s.s.t.t.t.t.vicinity model is a disjoint pretrained entity whoseparameters are not trained on any task objective..in order to generate samples around each se-lected example, we ﬁrst randomly choose p % ofthe input tokens.
then we successively (one at atime) mask one of the chosen tokens and ask xlm-r masked lm to predict a token in that maskedposition, i.e., compute ϑ(˜xm|x, θmlm) with m be-ing the index of the masked token.
for a speciﬁcmask, we sample s candidate words from the out-put distribution, and generate novel sentences byfollowing one of the two alternative approaches..(i) successive max in this approach, we takethe most probable output token (s = 1) at each pre-diction step, o∗m = arg maxo ϑ(˜xm = o|x, θmlm).
a new sentence is constructed by p % newly gener-ated tokens.
we generate δ (diversiﬁcation factor)virtual samples for each original example x, byrandomly masking p % tokens each time..(ii) successive cross.
in this approach, we di-vide each original (multi-sentence) sample x intotwo parts and use successive max to create two setsof augmented samples of size δ1 and δ2, respec-tively.
we then take the cross of these two sets togenerate δ1 × δ2 augmented samples..augmentation of sentences through successivemax or cross is carried out within the gen-lm.
(generate via lm) module in algorithm 1. fortasks involving a single sequence (e.g., xner),we directly use successive max.
pairwise tasks likexnli and paws-x have pairwise dependencies:dependencies between a premise and a hypothe-sis in xnli or dependencies between a sentenceand its possible paraphrase in paws-x.
to modelsuch dependencies, we use successive cross, whichuses cross-product of two successive max appliedindependently to each component..2.3 co-labeling through co-distillation.
due to discrete nature of texts, vrm based aug-mentation methods that are successful for imagessuch as mixmatch (berthelot et al., 2019) that gen-erates new samples and their labels as simple linearinterpolation, have not been successful in nlp.
themeaning of a sentence can change entirely evenwith minor variations in the original sentence.
forexample, consider the following example generatedby our vicinity model..original: eu rejects german call to boycott british lamb..masked: <mask> rejects german call to boycott british lamb.
xlm-r: trump rejects german call to boycott british lamb..here, eu is an organization whereas the newlypredicted word trump is a person (different nametype).
therefore, we need to relabel the augmented.
1981sentences no matter whether the original sentencehas labels (source) or not (target).
however, therelabeling process can induce noise, especially fordissimilar/low-resource languages, since the basetask model may not be adapted fully in the earlytraining stages.
we propose a 2-stage sample distil-lation process to ﬁlter out noisy augmented data..stage 1: distillation by single-model the ﬁrststage of distillation involves predictions from a sin-gle model for which we propose two alternatives:(i) distillation by model conﬁdence: in this ap-proach, we select samples based on the model’sprediction conﬁdence.
this method is similarin spirit to the selection method proposed byruder and plank (2018a).
for sentence-leveltasks (e.g., xnli), the model produces a sin-gle class distribution for each training exam-the model’s conﬁdence isple.
in this case,computed by p∗ = maxc∈{1...c} pcθ(x).
fortoken-level sequence labeling tasks (e.g., ner),the model’s conﬁdence is computed by: p∗ =θ(xt)(cid:9), where t is the1tlength of the sequence.
the distillation is then doneby selecting the top η% samples with the highestconﬁdence scores..(cid:8) maxc∈{1...c} pc.
(cid:80)t.t=1.
(ii) sample distillation by clustering: we pro-pose this method based on the ﬁnding that largeneural models tend to learn good samples fasterthan noisy ones, leading to a lower loss for goodsamples and higher loss for noisy ones (han et al.,2018; arazo et al., 2019).
we use a 1d two-component gaussian mixture model (gmm) tomodel per-sample loss distribution and cluster thesamples based on their goodness.
gmms provideﬂexibility in modeling the sharpness of a distri-bution and can be easily ﬁt using expectation-maximization (em) (see more on appendix c).
the loss is computed based on the pseudo labelspredicted by the model.
for each sample x, itsgoodness probability is the posterior probabilityp(z = g|x, θgmm), where g is the component withsmaller mean loss.
here, distillation hyperparame-ter η is the posterior probability threshold based onwhich samples are selected..stage 2: distillation by model agreementinthe second stage of distillation, we select sam-ples by taking the agreement (co-guess) of twodifferent peer models θ(j) and θ(k) to train thethird θ(l).
formally, agreement(cid:0)d(k), d(j)) ={(x (k), y (k)) : y (k) = y (j)} s.t.
k (cid:54)= j.
2.4 data samples manipulation.
uxla uses multi-epoch co-teaching.
it uses dst in the ﬁrst epoch.
in epoch 2, it uses ˜dt (tar-and d(cid:48)get virtual), and ﬁnally it uses all the four datasets -t, ˜dt, and ˜ds (line 22 in algorithm 1).
theds, d(cid:48)datasets used at different stages can be of differ-ent sizes.
for example, the number of augmentedsamples in ˜ds and ˜dt grow polynomially withthe successive cross masking method.
also, theco-distillation produces sample sets of variablesizes.
to ensure that our model does not overﬁt onone particular dataset, we employ a balanced sam-pling strategy.
for n number of datasets {di}nwith probabilities, {pi}nmultinomial distribution to sample from:.
i=1i=1, we deﬁne the following.
pi =.
f αij=1 f αj.
(cid:80)n., where fi =.
(2).
ni(cid:80)nj=1 nj.
where α is the sampling factor and ni is the totalnumber of samples in the ith dataset.
by tweakingα, we can control how many samples a dataset canprovide in the mix..3 experiments.
we consider three tasks in the zero-resource cross-lingual transfer setting.
we assume labeled trainingdata only in english, and transfer the trained modelto a target language.
for all experiments, we re-port the mean score of the three models that usedifferent seeds..3.1 tasks & settings.
xner: we use the standard conll datasets(sang, 2002; sang and meulder, 2003) for english(en), german (de), spanish (es) and dutch (nl).
we also evaluate on finnish (ﬁ) and arabic (ar)datasets collected from bari et al.
(2020).
notethat arabic is structurally different from english,and finnish is from a different language family.
toshow how the models perform on extremely low-resource languages, we experiment with three struc-turally different languages from wikiann (panet al., 2017) of different (unlabeled) training datasizes: urdu (ur-20k training samples), bengali (bn-10k samples), and burmese (my-100 samples)..xnli we use the standard dataset (conneauet al., 2018).
for a given pair of sentences, the taskis to predict the entailment relationship betweenthe two sentences, i.e., whether the second sentence(hypothesis) is an entailment, contradiction, or.
1982model.
en.
es.
nl.
de.
ar.
ﬁ.supervised results.
lstm-crf (bari et al., 2020)xlm-r (conneau et al., 2020)xlm-r (our imp.).
89.7792.9292.9.
84.7189.7289.2.
85.1692.5392.9.
78.1485.8186.2.
75.49–86.8.
84.21–92.4.zero-resource baseline.
mbertcased (our imp.)
xlm-r (our imp.)
xlm-r (ensemble).
91.1392.2392.76.
74.7679.2980.62.
79.5880.8781.46.
70.9973.4075.40.
45.4849.0452.30.
65.9575.5776.85.our method.
mbertcased +con-penaltyxlm-r+con-penaltyuxlauxla (ensemble).
90.8192.49––.
75.0680.4583.0583.24.
79.2681.0785.2185.32.
72.3173.7680.3380.99.
47.0349.9457.3558.29.
66.7276.0579.7579.87.table 1: f1 scores in xner on the datasets from conll and (bari et al., 2020).
"–" represents no results were reported..neutral with respect to the ﬁrst one (premise).
weexperiment with spanish, german, arabic, swahili(sw), hindi (hi) and urdu..paws-x the paraphrase adversariesfromword scrambling cross-lingual task (yang et al.,2019) requires the models to determine whethertwo sentences are paraphrases.
we evaluate on allthe six (typologically distinct) languages: fr, es, de,chinese (zh), japanese (ja), and korean (ko)..evaluation setup our goal is to adapt a taskmodel from a source language distribution to anunknown target language distribution assuming nolabeled data in the target.
in this scenario, theremight be two different distributional gaps: (i) thegeneralization gap for the source distribution, and(ii) the gap between the source and target languagedistribution.
we wish to investigate our method intasks that exhibit such properties.
we use the stan-dard task setting for xner, where we take 100%samples from the datasets as they come from vari-ous domains and sizes without any speciﬁc bias..however, both xnli and paws-x training datacome with machine-translated texts in target lan-guages.
thus, the data is parallel and lacks enoughdiversity (source and target come from the samedomain).
cross-lingual models trained in this setupmay pick up distributional bias (in the label space)from the source.
artetxe et al.
(2020) also arguethat the translation process can induce subtle arti-facts that may have a notable impact on models..therefore, for xnli and paws-x, we exper-iment with two different setups.
first, to ensuredistributional differences and non-parallelism, weuse 5% of the training data from the source lan-guage and augment a different (nonparallel) 5%.
model.
ur.
bn.
my.
supervised results.
xlm-r (our-impl).
97.1.
97.8.
76.8.zero-resource results.
xlm-r (xtreme)xlm-r (our imp.)
uxla.
56.456.4584.99.
78.878.1782.68.
54.354.5670.61.table 2: xner results on wikiann..data for the target language.
we used a differentseed each time to retrieve this 5% data.
second, tocompare with previous methods, we also evaluateon the standard 100% setup.
the evaluation is doneon the entire test set in both setups.
we will refer tothese two settings as 5% and 100%.
more detailsabout model settings are in appendix d..3.2 resultsxner table 1 reports the xner results on thedatasets from conll and (bari et al., 2020), wherewe also evaluate an ensemble by averaging the prob-abilities from the three models.
we observe that af-ter performing warm-up with conf-penalty (§2.1),xlm-r performs better than mbert on averageby ∼3.8% for all the languages.
uxla gives abso-lute improvements of 3.76%, 4.34%, 6.94%, 8.31%,and 4.18% for es, nl, de, ar, and ﬁ, respectively.
in-terestingly, it surpasses supervised lstm-crf fornl and de without using any target language labeleddata.
it also produces comparable results for es..in table 2, we report the results on the three low-resource langauges from wikiann.
from theseresults and the results of ar and ﬁ in table 1, wesee that uxla is particularly effective for lan-guages that are structurally dissimilar and/or low-resourced, especially when the base model is weak:.
1983model.
en.
es.
de.
ar.
sw.hi.
ur.
supervised results (translate-train-all).
xlm-r.89.1.
86.6.
85.7.
83.1.
78.0.
81.6.
78.1.zero-resource baseline for full (100%) english labeled training set.
xlm-r (xtreme)xlm-r (our imp.)
xlm-r (ensemble).
xlm-r+con-penaltyuxlauxla (ensemble).
xlm-r (our imp.)
xlm-r (ensemble).
xlm-r+con-penaltyuxlauxla (ensemble).
88.788.8789.24.
88.83––.
83.0884.65.
84.24––.
83.784.3484.73.
84.3085.6586.12.
78.4879.56.
79.2381.5382.35.
82.582.7883.27.
82.8684.1584.61.
77.5478.38.
78.4780.8881.93.
77.278.4479.06.
78.2080.5080.89.
72.0472.22.
72.4377.4278.56.
71.272.0873.17.
71.8374.7074.89.
67.366.93.
67.7272.3173.53.
75.676.4077.23.
76.2478.7478.98.
70.4171.00.
71.0874.7075.20.
71.772.1073.07.
71.6273.3573.45.
66.7266.79.
67.6370.8471.15.zero-resource baseline for 5% english labeled training set.
table 3: results in accuracy for xnli..28.54%, 16.05%, and 9.25% absolute improve-ments for ur, my and ar, respectively..xnli-5% from table 3, we see that the perfor-mance of xlm-r trained on 5% data is surpris-ingly good compared to the model trained on fulldata (see xlm-r (our imp.
)), lagging by only 5.6%on average.
in our single gpu implementation ofxnli, we could not reproduce the reported resultsof conneau et al.
(2020).
however, our results re-semble the reported xlm-r results of xtreme(hu et al., 2020).
we consider xtreme as ourstandard baseline for xnli-100%..we observe that with only 5% labeled data inthe source, uxla gets comparable results to thextreme baseline that uses 100% labeled data(lagging behind by only ∼0.7% on avg.
); even forar and sw, we get 0.22% and 1.11% improvements,respectively.
it surpasses the standard 5% baselineby 4.2% on average.
speciﬁcally, uxla gets abso-lute improvements of 3.05%, 3.34%, 5.38%, 5.01%,4.29%, and 4.12% for es, de, ar, sw, hi, and ur, re-spectively.
again, the gains are relatively higher forlow-resource and/or dissimilar languages despitethe base model being weak in such cases..xnli-100% now, considering uxla’s perfor-mance on the full (100%) labeled source data intable 3, we see that it achieves sota results forall of the languages with an absolute improvementof 2.55% on average from the xtreme baseline.
speciﬁcally, uxla gets absolute improvements of1.95%, 1.68%, 4.30%, 3.50%, 3.24%, and 1.65%for es, de, ar, sw, hi, and ur, respectively..paws-x similar to xnli, we observe sizableimprovements for uxla over the baselines onpaws-x for both 5% and 100% settings (table 4).
speciﬁcally, in 5% setting, uxla gets absolutegains of 5.33%, 5.94%, 5.04%, 6.85%, 7.00%, and5.45% for de, es, fr, ja, ko, and zh, respectively,while in 100% setting, it gets 2.21%, 2.36%, 2.00%,3.99%, 4.53%, and 4.41% improvements respec-tively.
in general, we get an average improvementsof 5.94% and 3.25% in paws-x-5% and paws-x-100% settings respectively.
moreover, our 5%setting outperforms 100% xlm-r baselines fores, ja, and zh.
interestingly, in the 100% setup, ouruxla (ensemble) achieves almost similar accura-cies compared to supervised ﬁnetuning of xlm-ron all target language training dataset..4 analysis.
in this section, we analyze uxla by dissecting itand measuring the contribution of its each of thecomponents.
for this, we use the xner task andanalyze the model based on the results in table 1..4.1 analysis of distillation methods.
model conﬁdence vs. clustering we ﬁrst ana-lyze the performance of our single-model distilla-tion methods (§2.3) to see which of the two alter-natives works better.
from table 5, we see thatboth perform similarly with model conﬁdence be-ing slightly better.
in our main experiments (tables1-4) and subsequent analysis, we use model conﬁ-dence for distillation.
however, we should not ruleout the clustering method as it gives a more general.
1984model.
en.
de.
es.
fr.
ja.
ko.
zh.
supervised results (translate-train-all).
xlm-r (our impl.).
95.8.
92.5.
92.8.
93.5.
85.5.
86.6.
87.6.zero-resource baseline for full (100%) english labeled training set.
xlm-r (xtreme)xlm-r (our imp.)
xlm-r (ensemble).
xlm-r+con-penaltyuxlauxla (ensemble).
xlm-r (our imp.)
xlm-r (ensemble).
xlm-r+con-penaltyuxlauxla (ensemble).
94.795.4696.10.
95.38––.
91.1592.05.
91.85––.
89.790.0690.75.
90.7592.2792.55.
83.7284.05.
86.1589.0589.25.
90.189.9290.55.
90.7292.2892.35.
84.3284.65.
86.3890.2790.85.
90.490.8591.80.
91.7192.8593.35.
85.0885.75.
85.9890.1290.25.
78.779.8980.55.
81.7783.8884.30.
73.6574.30.
76.0380.5081.15.
79.079.7480.70.
82.0784.2784.35.
72.6071.95.
75.4379.6080.15.
82.382.4983.45.
84.2586.9086.95.
77.2277.50.
79.1582.6582.90.zero-resource baseline for 5% english labeled training set.
table 4: results in accuracy for paws-x..solution to consider other distillation features (e.g.,sequence length, language) than model predictionscores, which we did not explore in this paper..distillation factor η we next show the resultsfor different distillation factor (η) in table 5. here100% refers to the case when no single-model dis-tillation is done based on model conﬁdence.
wenotice that the best results for each of the languagesare obtained for values other than 100%, which in-dicates that distillation is indeed an effective stepin uxla.
see appendix b for more analysis on η..two-stage distillation we now validate whetherthe second-stage distillation (distillation by modelagreement) is needed.
in table 5, we also comparethe results with the model agreement (shown as∩) to the results without using any agreement (φ).
we observe better performance with model agree-ment in all the cases on top of the single-modeldistillation which validates its utility.
results withη = 100, agreement = ∩ can be considered asthe tri-training (ruder and plank, 2018b) baseline..4.2 augmentation in stages.
figure 2 presents the effect of different types ofaugmented data used by different epochs in ourmulti-epoch co-teaching framework.
we observethat in every epoch, there is a signiﬁcant boost inf1 scores for each of the languages.
arabic, beingstructural dissimilar to english, has a lower basescore, but the relative improvements brought byuxla are higher for arabic, especially in epoch 2.η.agreement.
es.
nl.
de.
ar.
ﬁ.
0.7.
0.5.
50%.
80%.
90%.
100%.
distillation by clustering.
82.28.
83.25.
78.86.
52.64.
78.47.
82.35.
83.11.
78.16.
54.20.
78.28.distillation by model conﬁdence.
82.5281.66.
82.3381.61.
81.9081.21.
82.5081.89.
82.4682.26.
83.5383.03.
82.8082.77.
82.3582.15.
75.9577.19.
78.5077.08.
79.0377.28.
77.0676.97.
52.0052.97.
54.4853.31.
52.4152.20.
52.5852.68.
77.5177.77.
78.4378.34.
78.6677.93.
77.5178.01.
∩.
∩.
∩φ.
∩φ.
∩φ.
∩φ.table 5: analysis of distillation on xner.
results afterepoch-1 training that uses {ds, d(cid:48).
t}..figure 2: validation f1 results in xner for multi-epochco-teaching training of uxla..1985tgtlang.
zero shot +con-penalty.
es.
nl.
ar.
ﬁ.enesnldearﬁ.
92.8881.4281.2775.2050.8876.97.
92.9283.2481.2273.6352.6677.02.
92.8782.0185.3275.0353.0877.06.uxlade.
92.9177.7180.5480.0352.5276.69.
92.8080.2982.3676.9758.2977.13.
92.6881.9784.2073.7753.8080.11.table 6: f1 scores on xner.
each column (e.g., es) underuxla represents results in all target languages for a uxlatrained with the augmented data in a speciﬁc language (e.g.,es).
the zero shot+con-penalty column represents the zero-shot results for the model after warmup..when it gets exposed to the target language virtualdata ( ˜dt) generated by the vicinity distribution..4.3 effect of conﬁdence penalty & ensemble.
for all the three tasks, we get reasonable improve-ments over the baselines by training with conﬁ-dence penalty (§2.1).
speciﬁcally, we get 0.56%,0.74%, 1.89%, and 1.18% improvements in xner,xnli-5%, paws-x-5%, and paws-x-100% re-spectively (table 1,3,4).
the improvements inxnli-100% are marginal and inconsistent, whichwe suspect due to the balanced class distribution.
from the results of ensemble models, we see thatthe ensemble boosts the baseline xlm-r. however,our regular uxla still outperforms the ensemblebaselines by a sizeable margin.
moreover, ensem-bling the trained models from uxla further im-proves the performance.
these comparisons ensurethat the capability of uxla through co-teachingand co-distillation is beyond the ensemble effect..4.4 robustness & efﬁciency.
table 6 shows the robustness of the ﬁne-tuneduxla model on xner task.
after ﬁne-tuning in aspeciﬁc target language, the f1 scores in englishremain almost similar (see ﬁrst row).
for somelanguages, uxla adaptation on a different lan-guage also improves the performance.
for example,arabic gets improvements for all uxla-adaptedmodels (compare 50.88 with others in row 5).
thisindicates that augmentation of uxla does not over-ﬁt on a target language.
more baselines, analysisand visualizations are added in appendix..5 related work.
recent years have witnessed signiﬁcant progress inlearning multilingual pretrained models.
notably,mbert (devlin et al., 2019) extends (english)bert by jointly training on 102 languages.
xlm.
(lample and conneau, 2019) extends mbert witha conditional lm and a translation lm (using paral-lel data) objectives.
conneau et al.
(2020) train thelargest multilingual language model xlm-r withroberta (liu et al., 2019).
wu and dredze (2019),keung et al.
(2019), and pires et al.
(2019) evaluatezero-shot cross-lingual transferability of mberton several tasks and attribute its generalization ca-pability to shared subword units.
pires et al.
(2019)also found structural similarity (e.g., word order)to be another important factor for successful cross-lingual transfer.
k et al.
(2020), however, show thatthe shared subword has a minimal contribution; in-stead, the structural similarity between languagesis more crucial for effective transfer..older data augmentation approaches relied ondistributional clusters (täckström et al., 2012).
anumber of recent methods have been proposedusing contextualized lms (kobayashi, 2018; wuet al., 2018; shi et al., 2019; ding et al., 2020;liu et al., 2021).
these methods rely on labelsto perform label-constrained augmentation, thusnot directly comparable with ours.
also, there arefundamental differences in the way we use the pre-trained lm.
unlike them our lm augmentation ispurely unsupervised and we do not perform anyﬁne-tuning of the pretrained vicinity model.
thisdisjoint characteristic gives our framework the ﬂex-ibility to replace θlm even with a better monolin-gual lm for a speciﬁc target language, which inturn makes uxla extendable to utilize strongerlms that may come in the future.
in a concurrentwork (mohiuddin et al., 2021), we propose a con-textualized lm based data augmentation for neuralmachine translation and show its advantages overtraditional back-translation gaining improved per-formance in low-resource scenarios..6 conclusion.
we propose a novel data augmentation framework,uxla, for zero-resource cross-lingual task adap-tation.
it performs simultaneous self-training withdata augmentation and unsupervised sample selec-tion.
with extensive experiments on three differentcross-lingual tasks spanning many language pairs,we have demonstrated the effectiveness of uxla.
for the zero-resource xner task, uxla sets a newsota for all the tested languages.
for both xnliand paws-x tasks, with only 5% labeled data inthe source, uxla gets comparable results to thebaseline that uses 100% labeled data..1986references.
eric arazo, diego ortego, paul albert, noel eo’connor, and kevin mcguinness.
2019. unsu-pervised label noise modeling and loss correction.
in international conference on machine learning(icml)..mikel artetxe, gorka labaka, and eneko agirre.
2020.translation artifacts in cross-lingual transfer learn-ing..m saiful bari, shaﬁq joty, and prathyusha jwalapuram.
2020. zero-resource cross-lingual named entityrecognition.
in proceedings of the 34th aaai con-ference on artiﬁcal intelligence, aaai ’20, pagesxx–xx, new york, usa.
aaai..david berthelot, nicholas carlini, ian goodfellow,nicolas papernot, avital oliver, and colin a raf-fel.
2019. mixmatch: a holistic approach to semi-supervised learning.
in h. wallach, h. larochelle,a. beygelzimer, f. d alché-buc, e. fox, and r. gar-nett, editors, advances in neural information pro-cessing systems 32, pages 5050–5060.
curran asso-ciates, inc..olivier chapelle, jason weston, léon bottou, andvladimir vapnik.
2001. vicinal risk minimization.
in t. k. leen, t. g. dietterich, and v. tresp, editors,advances in neural information processing systems13, pages 416–422.
mit press..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau, guillaume lample, ruty rinott, ad-ina williams, samuel r. bowman, holger schwenk,and veselin stoyanov.
2018.xnli: evaluat-ing cross-lingual sentence representations.
corr,abs/1809.05053..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..bosheng ding, linlin liu, lidong bing, canasai kru-engkrai, thien hai nguyen, shaﬁq joty, luo si, andchunyan miao.
2020. daga: data augmentationwith a generation approach for low-resource tag-ging tasks.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing, emnlp’20, pages 6045—-6057, punta cana,dominican republic.
acl..bo han, quanming yao, xingrui yu, gang niu, miaoxu, weihua hu, ivor tsang, and masashi sugiyama.
2018. co-teaching: robust training of deep neuralnetworks with extremely noisy labels.
in s. bengio,h. wallach, h. larochelle, k. grauman, n. cesa-bianchi, and r. garnett, editors, advances in neu-ral information processing systems 31, pages 8527–8537. curran associates, inc..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-taskbenchmark for evaluating cross-lingual generaliza-tion.
corr, abs/2003.11080..karthikeyan k, zihan wang, stephen mayhew, anddan roth.
2020. cross-lingual ability of multilin-in internationalgual {bert}: an empirical study.
conference on learning representations..phillip keung, yichao lu, and vikas bhardwaj.
2019.adversarial learning with contextual embeddingsfor zero-resource cross-lingual classiﬁcation and ner.
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp)..sosuke kobayashi.
2018. contextual augmentation:data augmentation by words with paradigmatic re-in proceedings of the 2018 conference oflations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 452–457,new orleans, louisiana.
association for computa-tional linguistics..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
advances inneural information processing systems (neurips)..junnan li, richard socher, and steven c.h.
hoi.
2020.dividemix: learning with noisy labels as semi-supervised learning.
in international conference onlearning representations..linlin liu, bosheng ding, lidong bing, shaﬁq joty,luo si, and chunyan miao.
2021. mulda: a mul-tilingual data augmentation framework for low-in proceedings ofresource cross-lingual ner.
the 59th annual meeting of the association for com-putational linguistics, acl’21, bangkok, thailand.
acl..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,.
1987patrice y. simard, yann a. lecun, john s. denker, andbernard victorri.
1998. transformation invariancein pattern recognition — tangent distance and tan-gent propagation, pages 239–274.
springer berlinheidelberg, berlin, heidelberg..christian szegedy, wojciech zaremba, ilya sutskever,joan bruna, dumitru erhan, ian goodfellow, androb fergus.
2014.intriguing properties of neuralnetworks.
in international conference on learningrepresentations..oscar täckström, ryan mcdonald, and jakob uszko-reit.
2012. cross-lingual word clusters for directin the 2012 con-transfer of linguistic structure.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies (naacl-hlt 2012)..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..xing wu, shangwen lv, liangjun zang, jizhong han,and songlin hu.
2018. conditional bert contex-tual augmentation.
corr, abs/1812.06705..yinfei yang, yuan zhang, chris tar, and jasonbaldridge.
2019.paws-x: a cross-lingual ad-inversarial dataset for paraphrase identiﬁcation.
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3687–3692, hong kong, china.
association for computa-tional linguistics..adams wei yu, david dohan, quoc le, thang luong,rui zhao, and kai chen.
2018. fast and accuratereading comprehension by combining self-attentionin international conference onand convolution.
learning representations..hongyi zhang, moustapha cisse, yann n. dauphin,and david lopez-paz.
2018. mixup: beyond empir-ical risk minimization.
in international conferenceon learning representations..zhi-hua zhou and ming li.
2005. tri-training: ex-ploiting unlabeled data using three classiﬁers.
ieeetransactions on knowledge and data engineering,17:1529–1541..luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..tasnim mohiuddin, m saiful bari, and shaﬁq joty.
2021. augvic: exploiting bitext vicinity for low-in findings of the association forresource nmt.
computational linguistics: acl-ijcnlp 2021, on-line.
association for computational linguistics..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings ofthe 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..gabriel pereyra, george tucker,.
jan chorowski,lukasz kaiser, and geoffrey e. hinton.
2017. regu-larizing neural networks by penalizing conﬁdent out-put distributions.
corr, abs/1701.06548..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computationallinguistics..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..sebastian ruder and barbara plank.
2018a.
strongbaselines for neural semi-supervised learning un-in proceedings of the 56th an-der domain shift.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1044–1054, melbourne, australia.
association for compu-tational linguistics..sebastian ruder and barbara plank.
2018b.
strongbaselines for neural semi-supervised learning underdomain shift.
corr, abs/1804.09530..erik tjong kim sang.
2002. introduction to the conll-2002 shared task: language-independent named en-tity recognition.
corr, cs.cl/0209010..erik tjong kim sang and fien de meulder.
2003. in-troduction to the conll-2003 shared task: language-independent named entity recognition.
in conll..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96, berlin, germany.
association for computa-tional linguistics..linqing shi, danyang liu, gongshen liu, and kuimeng.
2019. aug-bert: an efﬁcient data augmen-tation algorithm for text classiﬁcation.
in csps..1988appendix.
a faq: justiﬁcations for design.
methodology of uxla.
here are our justiﬁcations for various design prin-ciples of the uxla framework..is masked language model pre-training withcross-lingual training data from task datasetuseful?
in table 7, we perform language modelﬁnetuning on xlm-r large model with multilin-gual sentences of ner dataset and perform adap-tation with only english language.
with the lm-ﬁnetuned xlm-r model, we didn’t see any signif-icant increase in cross-lingual transfer.
for span-ish, arabic language, the score even got decreased,which indicates possible over-ﬁtting.
however, ro-bustness experiment in table 6 (see in the mainpaper, sec 4.4) indicates that our proposed methoddoesn’t overﬁt on target language rather than aug-ment the new knowledge base..model.
es.
nl.
de.
ar.
ﬁ.xlm-rxlm-r + ensuxlauxla + ens.
80.4581.4283.0583.24.
81.0781.2785.2185.32.
73.7675.2080.3380.99.
49.9450.9357.3558.29.
76.0576.9779.7579.87.finetuned xlm-r 78.11.
81.61.
76.33.
48.04.
76.63.table 7: some additional baseline results on xnertask.
here, ens reefers to emsemble..is using three models with different initializa-tion necessary?
yes, different initialization en-sures different convergence paths, which resultsin diversity during inference.
co-labeling (section3.3) utilizes this property.
there could be someother ways to achieve the same thing.
our initialattempt with three different heads (sharing a back-bone network) didn’t work well..is using three epochs necessary?
we utilize dif-ferent types of datasets in different epochs.
whilepseudo-labeling may induce noise, the model’s pre-dictions for in-domain cross-lingual samples areusually better.
because of this, for a smooth tran-sition, we apply the vicinal samples in the secondepoch.
finally, inspired by the joint training of thecross-lingual language model, in the third epochwe use all four datasets.
we also include the labeledsource data which ensures that our model does notoverﬁt on target distribution as well as persists thegeneralization capability of the source distribution..need for the combination of co-teaching, co-distillation and co-guessing?
the combinationof these helps to distill out the noisy samples better..efﬁciency of the method and expensive extracosts for large-scale pretrained modelsit is acommon practice in model selection to train 3-5disjoint lm-based task models (e.g., xlm-r onner) with different random seeds and report theensemble score or score of the best (validation set)model.
in contrast, uxla uses 3 different modelsand jointly trains them where the models assist eachother through distillation and co-labeling.
in thatsense, the extra cost comes from distillation andco-labeling, which is not signiﬁcant and is compen-sated by the signiﬁcant improvements that uxlaoffers..b visualization of conﬁdence penalty.
b.1 effect of conﬁdence penalty in.
classiﬁcation.
in figure 3 (a-b), we present the effect of the conﬁ-dence penalty (eq.
1 in the main paper) in the targetlanguage (spanish) classiﬁcation on the xner dev.
data (i.e., after training on english ner).
we showthe class distribution from the ﬁnal logits (on thetarget language) using t-sne plots.
from the ﬁg-ure, it is evident that the use of conﬁdence penaltyin the warm-up step makes the model more robustto unseen out-of-distribution target language datayielding better predictions, which in turn also pro-vides a better prior for self-training with pseudolabels..b.2 effect of conﬁdence penalty in loss.
distribution.
figures 3(c) and 3(d) present the per-sample loss(i.e., mean loss per sentence w.r.t.
the pseudo la-bels) distribution in histogram without and withconﬁdence penalty, respectively.
here, accurate-2 refers to the sentences which have at most twowrong ner labels, and sentences containing morethan two errors are referred to as noisy samples.
itshows that without conﬁdence penalty, there aremany noisy samples with a small loss which is notdesired.
in addition to that, the ﬁgures also sug-gest that the conﬁdence penalty helps to separatethe clean samples from the noisy ones either byclustering or by model conﬁdence..figures 4(a) and 4(b) present the loss distributionin a scatter plot by sorting the sentences based.
1989(a) withoutpenalty..conﬁdence.
(b) withpenalty..conﬁdence.
(c) withoutpenalty..conﬁdence.
(d) withpenalty..conﬁdence.
figure 3: (a-b) effect of training with conﬁdence penalty in the warm-up step on target (spanish) language xnerclassiﬁcation using t-sne plots.
from the visualization, it can be seen that the model trained with conﬁdencepenalty shows better inter-class separation which exhibits robustness of the multilingual model.
(c-d) histogramof loss distribution on target (spanish) language xner classiﬁcation..on their length in the x-axis; y-axis represents theloss.
as we can see, the losses are indeed morescattered when we train the model with conﬁdencepenalty, which indicates higher per-sample entropy,as expected.
also, we can see that as the sentencelength increases, there are more wrong predictions.
our distillation method should be able to distill outthese noisy pseudo samples..finally, figures 4(c) and 4(d) show the lengthdistribution of all vs. the selected sentences (bydistillation by model conﬁdence) without and withconﬁdence penalty.
bari et al.
(2020) shows thatcross-lingual ner inference is heavily dependenton the length distribution of the samples.
in gen-eral, the performance of the lower length samplesis more accurate.
however, if we only select thelower length samples we will easily overﬁt.
fromthese plots, we observe that the conﬁdence penaltyalso helps to perform a better distillation as moresentences are selected (by the distillation proce-dure) from the lower length distribution, while stillcovering the entire lengths.
this shows that usingthe conﬁdence penalty in training, model becomesmore robust..in summary, comparing the figures 3(c-d) - 4(c-d), we can conclude that training without conﬁ-dence penalty can make the model more prone toover-ﬁtting, resulting in more noisy pseudo labels.
training with conﬁdence penalty not only improvespseudo labeling accuracy but also helps the distilla-tion methods to perform better noise ﬁltering..c details on distillation by clustering.
one limitation of the conﬁdence-based (single-model) distillation is that it does not consider task-.
speciﬁc information.
apart from classiﬁer conﬁ-dence, there could be other important features thatcan distinguish a good sample from a noisy one.
forexample, for sequence labeling, sequence lengthcan be an important feature as the models tend tomake more mistakes (hence noisy) for longer se-quences bari et al.
(2020).
one might also want toconsider other features like ﬂuency, which can beestimated by a pre-trained conditional lm like gptradford et al.
(2020).
in the following, we intro-duce a clustering-based method that can considerthese additional features to separate good samplesfrom bad ones..here our goal is to cluster the samples basedon their goodness.
it has been shown in computervision that deep models tend to learn good sam-ples faster than noisy ones, leading to a lower lossfor good samples and higher loss for noisy oneshan et al.
(2018), arpit et al.
(2017).
we proposeto model per-sample loss distribution (along withother task-speciﬁc features) with a mixture model,which we ﬁt using an expectation-maximization(em) algorithm.
however, contrary to those ap-proaches which use actual (supervised) labels, weuse the model predicted pseudo labels to computethe loss for the samples..we use a two-component gaussian mixturemodel (gmm) due to its ﬂexibility in modelingthe sharpness of a distribution li et al.
(2020a).
inthe following, we describe the em training of thegmm for one feature, i.e., per-sample loss, but it istrivial to extend it to consider other indicative task-speciﬁc features like sequence length or ﬂuencyscore (see any textbook on machine learning)..1990(a) withoutpenalty..conﬁdence.
(b) withpenalty..conﬁdence.
(a) withoutpenalty..conﬁdence.
(b) withpenalty..conﬁdence.
figure 4: (a-b)scatter plot of loss distribution on target (spanish) language xner classiﬁcation.
(c-d) distributionof selected sentence lengths on target (spanish) language xner classiﬁcation..em training for two-component gmm letxi ∈ ir denote the loss for sample xi and zi ∈{0, 1} denote its cluster id.
we can write the 1dgmm model as:.
p(xi|θ, π) =.
n (xi|µk, σk)πk.
(3).
1(cid:88).
k=0.
where θk = {µk, σ2k} are the parameters of the k-th mixture component and πk = p(zi = k) is theprobability (weight) of the k-th component withthe condition 0 ≤ πk ≤ 1 and (cid:80).
k πk = 1..in em, we optimize the expected complete data.
log likelihood q(θ, θt−1) deﬁned as:.
log[p(xi, zi|θ)]).
i(zi = k) log[p(xi|θk)πk]).
i(cid:88).
(cid:88).
ke(i(zi = k)) log[p(xi|θk)πk].
q(θ, θt−1)(cid:88)= e(.
i(cid:88).
(cid:88).
= e(.
=.
=.
=.
i(cid:88).
k(cid:88).
i(cid:88).
k(cid:88).
i.k.p(zi = k|xi, θt−1) log[p(xi|θk)πk].
ri,k(θt−1) log p(xi|θk) + ri,k(θt−1) log πk.
(4).
where ri,k(θt−1) is the responsibility that clusterk takes for sample xi, which is computed in the e-step so that we can optimize q(θ, θt−1) (eq.
4) inthe m-step.
the e-step and m-step for a 1d gmmcan be written as:.
compute.
e-step:n (xi|θt−1k n (xi|θt−1m-step: optimize q(θ, θt−1) w.r.t.
θ and π.
)πt−1k)πt−1k.ri,k(θt−1).
k.k.(cid:80).
=.
• πk =.
• µk =.
(cid:80).
i ri,k(cid:80).
k ri,k.
(cid:80).
i.
(cid:80)i ri,kxi(cid:80)i ri,k.
;.
= 1n.(cid:80).
i ri,k.
σ2k =.
(cid:80).
i ri,k(xi−µk)2i ri,k.
(cid:80).
inference for a sample x, its goodness proba-bility is the posterior probability p(z = g|x, θ),where g ∈ {0, 1} is the component with smallermean loss.
here, distillation hyperparameter η isthe posterior probability threshold based on whichsamples are selected..relation with distillation by model conﬁdenceastute readers might have already noticed thatper-sample loss has a direct deterministic relationwith the model conﬁdence.
even though they aredifferent, these two distillation methods considerthe same source of information.
however, as men-tioned, the clustering-based method allows us toincorporate other indicative features like length,ﬂuency, etc.
for a fair comparison between the twomethods, we use only the per-sample loss in ourprimary (single-model) distillation methods..d hyperparameters.
we present the hyperparameter settings for xnerand xnli tasks for the xla framework in ta-ble 8. in the warm-up step, we train and validatethe task models with english data.
however, forcross-lingual adaptation, we validate (for modelselection) our model with the target language devel-opment set.
we train our model with respect to thenumber of steps instead of the number of epochs.
in the case of a given number of epochs, we convertit to a total number of steps.
we observe that learning rate is a crucial hyperpa-rameter.
in table 8, lr-warm-up-steps refer to thewarmup-step from triangular learning rate schedul-ing.
this hyperparameter is not to be confused with.
1991hyperparameter.
xner.
xnli.
paws-x.
warm-up step x-lingual adaptation warm-up step.
x-lingual adaptation warm-up step.
x-lingual adaptation.
model-typesampling-factor αdrop-outmax-seq-lengthper-gpu-train-batch-sizegrad-accumulation-stepslogging-steplearning-rate (lr)lr-warm-up-stepsweight-decayadam-epsilonmax-grad-normnum-of-train-epochsuxla-epochsmax-stepstrain-data-percentageconf-penalty.
xlm-r l–0.128045503e−52000.011e−81.0––3000100true.
#mixture-componentposterior-thresholdcovariance-typedistilation-factor ηdistillation-type.
–––––.
training-hyperparameters.
warm-up-ckpt0.70.128044505e−610% of train0.011e−81.013–100false.
.
20.5full80, 100, 100confidence.
xlm-r l–0.1128162501e−610% of train–1e−81.0–6–5true.
–––––.
warm-up-ckpt0.70.1128162251e−610% of train–1e−81.013–5false.
–––50, 80, 100confidence.
distillation-hyperparameters.
augmentation-hyperparameters.
warm-up-ckpt0.70.1128162251e−610% of train–1e−81.016.
5false.
–––80, 90, 80confidence.
xlm-r l–0.1128162501e−610% of train–1e−81.0–10–5true.
–––––.
-–––.
do-lower-caseaug-typeaug-percentage pdiversiﬁcation-factor δ.false–––.
falsesuccessive-max303.false–––.
falsesuccessive-cross302×2.
falsesuccessive-cross402 × 2.table 8: hyperparameter settings for xner, xnli, and paws-x task.
total number of parameter for each of themodel is 550m.
we used v100 gpus to do the experiments.
average run-time for each of the languages may differbased on total number of augmented samples.
in an average, for per million augmentation requires .5-2 days basedof various settings of training mechanism (ie., fp16 training, gradient accumulation etc)..in vrm, we minimize the empirical vicinal risk.
deﬁned as:.
n(cid:88).
1n.lv(θ) =.
l(fθ(˜xn), ˜yn).
(5).
n=1where fθ denotes the model parameterized byθ, and daug = {(˜xn, ˜yn)}nn=1 is an augmenteddataset constructed by sampling the vicinal dis-tribution ϑ(˜x, ˜y|xi, yi) around the original trainingsample (xi, yi).
deﬁning vicinity is however chal-lenging as it requires to extract samples from adistribution without hurting the labels.
earlier meth-ods apply simple rules like rotation and scaling ofimages (simard et al., 1998).
recently, zhang et al.
(2018); berthelot et al.
(2019) and li et al.
(2020)show impressive results in image classiﬁcation withsimple linear interpolation of data.
however, to ourknowledge, none of these methods has so far beensuccessful in nlp due to the discrete nature oftexts..warm-up step of the uxla framework.
in our ex-periments, effective batch-size is another crucialhyperparameter that can be obtained by gradientaccumulation steps.
we ﬁx the maximum sequencelength to 280 for xner and 128 tokens for xnli.
for each of the experiments, we report the averagescore of three task models, θ(1), θ(2), θ(3), whichare initialized with different seeds.
we performeach of the experiments in a single gpu setup withﬂoat32 precision..e additional related work.
vicinal risk minimization.
one of the funda-mental challenges in deep learning is to train mod-els that generalize well to examples outside thetraining distribution.
the widely used empiricalrisk minimization (erm) principle where modelsare trained to minimize the average training errorhas been shown to be insufﬁcient to achieve gener-alization on distributions that differ slightly fromthe training data (szegedy et al., 2014; zhang et al.,2018).
data augmentation supported by the vici-nal risk minimization (vrm) principle (chapelleet al., 2001) can be an effective choice for achievingbetter out-of-training generalization..1992