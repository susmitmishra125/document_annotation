learning relation alignment for calibrated cross-modal retrieval.
shuhuai ren1, junyang lin3, guangxiang zhao1, rui men3, an yang3,jingren zhou3, xu sun1,2∗, hongxia yang31moe key lab of computational linguistics, school of eecs, peking university2center for data science, peking university3alibaba group, chinashuhuai ren@stu.pku.edu.cn, {zhaoguangxiang,xusun}@pku.edu.cn{junyang.ljy,menrui.mr,ya235025}@alibaba-inc.com{jingren.zhou,yang.yhx}@alibaba-inc.com.
abstract.
despite the achievements of large-scale mul-timodal pre-training approaches, cross-modalretrieval, e.g., image-text retrieval, remains achallenging task.
to bridge the semantic gapbetween the two modalities, previous studiesmainly focus on word-region alignment at theobject level, lacking the matching between thelinguistic relation among the words and the vi-sual relation among the regions.
the neglectof such relation consistency impairs the con-textualized representation of image-text pairsand hinders the model performance and theinterpretability.
in this paper, we ﬁrst pro-pose a novel metric, intra-modal self-attentiondistance (isd), to quantify the relation con-sistency by measuring the semantic distancebetween linguistic and visual relations.
in re-sponse, we present inter-modal alignment onintra-modal self-attentions (iais), a regular-ized training method to optimize the isd andcalibrate intra-modal self-attentions from thetwo modalities mutually via inter-modal align-ment.
the iais regularizer boosts the perfor-mance of prevailing models on flickr30k andms coco datasets by a considerable margin,which demonstrates the superiority of our ap-proach.1.
1.introduction.
cross-modal retrieval, including image-text re-trieval, video-text retrieval, etc., has long beenan important downstream task in cross-modal rep-resentation learning.
image-text retrieval (itr)aims at modeling the similarity of image-text pairsand recalling the most relevant one.
it remains quitechallenging due to the heterogeneity of the data andthe semantic gap between two different modalities.
to bridge this gap, neural networks are responsi-ble for learning global representations of images.
∗corresponding author1our code is available at https://github.com/.
lancopku/iais.
figure 1: the upper part shows a comparison of previ-ous object-level alignment and our relation-level align-ment.
the symbol ↔ denotes alignment and (cid:57)(cid:57)(cid:75) de-notes the self-attention stems from a query.
the lowerpanel gives a bad case of inconsistent textual and vi-sual relations.
the region of “a red shirt” pays consid-erable attention to the region of the dog, which doesnot beneﬁt the matching and is inconsistent with theself-attention of the corresponding phrase..and texts in a joint semantic space and aligning theimages and texts with the same semantics (faghriet al., 2018; kiros et al., 2014).
a straightforwardway to enhance the alignment is to enforce the localmatching between the object-oriented words andthe corresponding image regions, and then lever-age the object co-occurrence statistics (liu et al.,2020; zhang et al., 2020a) in the pairs for inference.
previous studies incorporate auxiliary knowledgesource like scene graphs (yu et al., 2020) or objecttags (li et al., 2020) to explicitly indicate the cross-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages514–524august1–6,2021.©2021associationforcomputationallinguistics514aguywitharedshirtwalking.withaguyaredshirtwalking.previousalignment:objectlevelouralignment:relationlevelaredshirtwithaguyaredshirtwalking.aredshirt0.120.360.52aredshirtaguy(therestregions)with,walking.
(theresttokens)0.470.230.30intra-modalself-attentiondisagreement☹abadcaseofintra-modalself-attentiondisagreementself-attentionattentiondistributionmodal mapping.
other researches try to establishﬁne-grained interaction on cross-modal attentionto reinforce the focus from words to their most rel-evant regions, and vice versa (chen et al., 2020;wang et al., 2019; messina et al., 2020; lee et al.,2018; zhang et al., 2020b; yang et al., 2020)..however, such word-region alignment at objectlevel serves only as the basis because it mainly fo-cuses on the local semantics but lacks the matchingof global features like the intra-modal relation.
the intra-modal relation refers to the correlationof items within a textual or visual sequence.
morespeciﬁcally, given a sentence and an image thatdescribe the same scene and are highly matched,the correlation of the items in the textual sequenceshould also agree with the correlation of the corre-sponding items in the visual sequence.
but suchconstraint of relation consistency is neglected inprevious works, which hinders performance andinterpretability of the models.
to corroborate this,we conduct a case study on flickr30k entitiesdataset (plummer et al., 2015) to probe the agree-ment of relation-level semantics in pre-trained mod-els like uniter (chen et al., 2020).
we utilizethe self-attention distribution as a representation ofthe intra-modal relations (clark et al., 2019; htutet al., 2019; kovaleva et al., 2019)..as shown in figure 1, the attention distributionsgrouped by the annotated object of the given textand image are in disagreement with each other.
speciﬁcally, the attention distribution in the linguis-tic modality is reasonable.
however, in the visualmodality, the region “a red shirt” pays inappropri-ate attention to the region of the dog that doesn’tappear in the text, which impairs the representationof this visual item, i.e., “a red shirt” under the con-dition of the corresponding text.
such mismatchedattention distributions suggest that the model repre-sents the same concept with inconsistent semantics,which misleads the model to reduce the estimatedsimilarity of the positive pairs and further leadsto the wrong predictions that they are unmatched.
what’s even worse is that in practice, the input re-gions of the existing methods are extracted by apre-trained object detector like faster r-cnn (renet al., 2015).
the visual features are much noisierdue to over-sampling (li et al., 2020; andersonet al., 2018), which necessitates a stronger regu-larizer to guide the alignment of the intra-modalrelations..motivated by the above observations, we pro-.
mote the semantic alignment from object level torelation level.
we leverage self-attention matrix tocharacterize the relation of items within one modal-ity, and design intra-modal self-attention distance(isd), a novel metric to measure the consistencybetween textual and visual relations.
our empiri-cal analysis illustrates that the isd and the modelperformance on image-text retrieval are highly cor-related, which veriﬁes our hypothesis and inspiresus to minimize the semantic distance between intra-modal self-attentions in training.
accordingly,we propose a new regularized training methodcalled inter-modal alignment on intra-modal self-attentions (iais) to calibrate two intra-modal at-tention distributions mutually via inter-modal align-ment, which helps learn better contextualized repre-sentations for image-text pairs.
the model perfor-mance of image-text retrieval on flickr30k and mscoco datasets is improved by a considerable mar-gin with iais, which demonstrates the superiorityof our proposal..2 measuring semantic distance between.
intra-modal relations.
in this section, we present a formal deﬁnition ofintra-modal relation alignment (section 2.1).
suchalignment requires extracting the visual and linguis-tic items corresponding to all objects and sortingthem in the same order to make their self-attentiondistributions comparable.
we ﬁrst introduce themechanism for multimodal attention calculation,and then present the method of attention weightextraction for constructing comparable intra-modalself-attentions (section 2.2).
finally, we propose ametric named intra-modal self-attention distance(isd) to quantify the relation consistency.
we con-duct an empirical analysis on prevailing modelsto verify the correlation of the model performanceand our metric (section 2.3)..2.1 from intra-modal relation to.
self-attention.
given a sequence o = [o1, · · · , on ] of n objectsappeared in an image-text pair, the linguistic and vi-sual representation of such object sequence can bewritten as l = [l1, · · · , ln ] and v = [v1, · · · , vn ],respectively.
each item li, vi with the same indexrefers to the same object oi.2 for every object, its.
2an object oi may require one or more tokens in the textand one or more regions in the image to describe, such thatthe linguistic item li and the visual item vi may refer to acollection of tokens and regions, respectively..515relation to the others is depicted in both the lin-guistic and the visual modality.
from a linguisticview, we regard the following textual self-attentiondistribution as the relation rli stems from li:.
rli = [ali→l1, · · · , ali→li, · · · , ali→ln ],.
(1).
where ali→lj is the attention weight from li to lj.
similarly, the relation rvi from the view of thevisual modality can be written as.
rvi = [avi→v1, · · · , avi→vi, · · · , avi→vn ]..(2).
consequently, we can achieve relation-level align-ment by narrowing the semantic distance, e.g.,kullback-leibler divergence, between the linguis-tic and visual self-attention distribution for all ob-jects from i = 1 to n :.
(cid:88)n.min.
i=1.
distance (rli, rvi) ..(3).
in the original self-attention matrix, however, the at-tention weights of speciﬁc objects are scattered anddisordered.
we need to extract the target weightsand reorder them to construct comparable attentiondistributions rli and rvi..2.2.intra-modal self-attentionreconstruction.
in this subsection, we ﬁrst introduce the vanillamultimodal attention mechanism and then presenta speciﬁc way of attention weight extraction..consider models of single-stream transformer-based architecture like uniter (chen et al., 2020).
the model consists of a stack of transformer layerswith attention mechanism (vaswani et al., 2017)and is responsible for encoding image-text pairsinto feature representations.
given q, k, v ∈rn ×d, the matrix of n query, key and value vec-tors with dimension d, respectively, the attentionfunction att(q, k, v) is deﬁned as:.
att(q, k, v) = σ.v = σ (s) v..(4).
(cid:16).
qk(cid:62)(cid:17).
here, σ is a row-wise, scaled softmax and s is a ma-trix of attention scores that measure the similaritybetween every pair of query and key vectors.
let land v denote the linguistic and the visual modality,respectively.
given a textual sequence xl of nltokens and a visual sequence xv of nv regions,the input x = [xl(cid:107)xv ] in the single-stream ar-chitecture is a concatenation of two sequences withlength n = nl + nv .
accordingly, the query and.
figure 2: an example of calculating intra-modal self-attention distance (isda) for a matched image-textpair.
two inputs in the pair both contain the objectof “two surfers” and “the waves”.
for self-attentionmatrix sll and svv from each modality, we extractobject-orientated patches according to the annotationsand summarize it with the cps operation (eq.
(7)) tosynthesize new matrices s(a)vv .
finally, we useour isda metric to measure their semantic distance..ll and s(a).
key matrix3 can be written as.
q = xwq =.
wq =.
(cid:19).
(cid:19).
(cid:18) xlxv(cid:18) xlxv.
(cid:18) qlqv(cid:18) klkv.
(cid:19).
(cid:19).
,.
(5).
k = xwk =.
wk =.
furthermore,.
where wq and wk are learnable parame-ters.
the attention score matrixs ∈ rn ×n can be organized into four sub-matrices (bugliarello et al., 2020):.
s = qk(cid:62) =.
(cid:17).
k(cid:62).
l k(cid:62)v.(cid:19).
l qlk(cid:62)vl qv k(cid:62)v.(cid:19) (cid:16).
(cid:18) qlqv(cid:18) qlk(cid:62)qv k(cid:62)(cid:18) sll slvsvl svv.
=.
=.
(cid:19).
..(6).
the matrices sll and svv on the diagonal rep-resent the linguistic and the visual intra-modalself-attention, respectively.
slv and svl onback-diagonal represent the inter-modal attentionscores from text to image, and the opposite.
weregard the self-attention σ (sll) and σ (svv ) asdepictions of the intra-modal relations.
each rowof the matrix represents the relation stemming fromone linguistic or visual item to the others withinthe same modality..to construct the comparable intra-modal self-attention matrices, we leverage the object annota-tions in the flickr30k entities dataset (plummer.
3the value matrix v is omitted for brevity..516two surfersenjoyingthe waves.twosurfersenjoyingthewavescpscpslinguisticobjectsequence:[two surfers,the waves]visualobjectsequence:[,]linguisticself-attentionafterreconstructionsℒℒavisualself-attentionafterreconstructions𝒱𝒱𝒱𝒱alinguisticself-attentionsℒℒvisualself-attentions𝒱𝒱𝒱𝒱cpscpsisd𝐋𝐋𝐕𝐕et al., 2015) to extract the tokens, regions, and at-tention weights with respect to the target objects.
as shown in figure 2, the text and the image bothcontain annotated objects of “two surfers” and “thewaves”.
the linguistic object sequence can be writ-ten as l = [l1, l2] = [“two surfers”, “the waves”].
these two objects derive four intrinsic relationsand can be described by four patches in the origi-nal linguistic self-attention matrix sll.
for clarity,we deﬁne an operation ext(s, oi, oj) that extractsthe patch of attention scores in matrix s from theobject oi to oj.
accordingly, the relation from“two surfers” to “the waves” can be denoted asext (sll, l1, l2).
to describe the relation with asingle value instead of a sub-matrix, we further con-struct an operation cps(·) to summarize the atten-tion patch s ∈ rm ×n to a scalar via column-wisesum and row-wise average:.
cps(s) =.
(cid:16)(cid:88)mi.
(cid:88)nj.
(cid:17).
sij.
/m..(7).
after the above processing, we complete the ex-traction of the linguistic self-attention sll throughgrouping the items by annotated object.
the ex-traction of visual self-attention svv is similar andthe ﬁnal results are denoted as s(a)vv .
asour processing for two intra-modal self-attentionsfollows the same order of object annotations, thematrices s(a)ll and s(a)vv from two modalities are ofthe same dimension and comparable..ll and s(a).
2.3.intra-modal self-attention distance withannotation (isda).
ll and s(a).
given two comparable matrices s(a)vv , wepropose a metric called intra-modal self-attentiondistance with annotation (isda) to quantify theirsemantic gap at the relation level.
we deﬁne the fol-lowing symmetric matrix-based kullback-leiblerdivergence (m-kl) for measuring the distance be-tween two matrices a and b:.
m-kl(a, b) =.
kl (ai(cid:107)bi) + kl (bi(cid:107)ai) , (8).
(cid:88)ni.where (·)i stands for the ith row-vector in the ma-trix and kl denotes the kullback-leibler diver-gence.
accordingly, the ﬁnal isda metric for s(a)lland s(a).
vv is deﬁned as:.
isda = m-kl.
(cid:16).
ll, s(a)s(a).
vv.
(cid:17).
..(9).
we present our algorithm for the calculation ofisda in algorithm 1..algorithm 1: intra-modal self-attentiondistance with annotation (isda).
input: intra-modal self-attention matrices sll, svvinput: linguistic object sequence linput: visual object sequence vfor linguistic object li in l do.
for linguistic object lj in l do.
sli→lj ← ext (sll, li, lj)(cid:1)ll[i, j] ← cps (cid:0)sli→ljs(a).
for visual object vi in v do.
for visual object vj in v do.
svi→vj ← ext (svv , vi, vj)vv [i, j] ← cps (cid:0)svi→vjs(a).
(cid:1).
isda = m-klreturn isda.
(cid:16)s(a)ll, s(a).
vv.
(cid:17).
// eq.9.
figure 3: the isda (blue ×) and model performance(meta-sum of recall, orange •) with respect to thetraining steps.
they are highly correlated with a pear-son’s correlation coefﬁcient of -0.60..to study the correlation between the isda metricand the model performance,4 we conduct an empir-ical analysis on uniter (chen et al., 2020).
asshown in figure 3, the isda decreases during thetraining phase while the model performance contin-ues to increase.
they are strongly correlated witha pearson’s correlation coefﬁcient of -0.60. afterthe middle stage of training, the curve of the modelperformance and isda tends to be ﬂat, suggest-ing that merely optimizing the task-oriented lossfunction while neglecting the constraint of relationconsistency hinders the model from achieving bet-ter performance.
to eliminate the bottleneck, wecan minimize the isd in the training phase as aregularization to induce further improvement forthe itr task and better the model interpretability..4we use the meta-sum (chen et al., 2020), sum of re-call@1, recall@5, recall@10 across the image and text re-trieval as a metric for model performance..5170.1880.1980.2080.2180.2280.2380.2480.2580.2680.2783753853954054154254350500100015002000isdameta-sumofrecalltrainingstepsmeta-sum of recallisdafigure 4: singular alignment and distributed alignment.
the image-text pair here is the same as that in figure 2..3.inter-modal alignment on intra-modalself-attentions (iais).
in this section, we propose a new regularized train-ing method, inter-modal alignment on intra-modalself-attentions (iais), for image-text retrieval.
ourgoal is to enhance the semantic alignment of re-lations by minimizing the distance between twointra-modal self-attentions (isd)..in practice, given the original visual and lin-guistic input sequence v = [v1, · · · , vnv ], l =[l1, · · · , lnl] with the scattered items,5 there are noobject annotations and the region features extractedby faster r-cnn are much noisier (li et al., 2020;anderson et al., 2018), which results in difﬁcultyin grouping the attention weights by ground-truthobject.
the isda thus cannot be used directly asthe objective function to minimize..to tackle this problem, we regard the inputsequence from one modality (e.g., the visual se-quence v) as an anchor.
for every item in theanchor sequence, we extract its corresponding rep-resentation from the other modality (e.g., one itemor a collection of items in the linguistic sequence l)to reconstruct a mirrored sequence.
after that, theitems and their relations within the anchor sequencehave a one-to-one correspondence with the itemsand relations within the mirrored sequence, whichmakes the intra-modal self-attentions derived fromthe two sequences comparable.
in the next two sub-sections, we propose two methods, singular align-ment and distributed alignment, to accomplishthe attention extraction and reconstruction.
theformer establishes a one-to-one mapping between.
5as there are no object annotations in practice, each visualitem now refers to only one region.
each linguistic item alsorefers to only one token, even if it is a sub-word..linguistic and visual attention weight, while the lat-ter establishes a distributed mapping.
besides, wedesign two losses l(s)iais and l(d)iais as a surrogate ofthe isda to measure the semantic distance betweenintra-modal self-attention matrices.
finally, we in-corporate the surrogate loss minimization as a reg-ularization to calibrate intra-modal self-attentionsmutually and achieve the relation-level alignment..3.1 singular alignment.
for every item in the anchor sequence, singularalignment utilizes the inter-modal attention to ﬁndits most relevant item from the opposite modality.
as the inter-modal attention score quantiﬁes thesimilarity between the items from two modalities,the visual and the linguistic item with the highestscore can be aligned with each other.
for example,given the ith visual item vi and the inter-modal at-tention matrix svl, the similarities between vi andall the linguistic items are depicted in svl[i, :], i.e.,the ith row of the matrix.
hence the most relevantlinguistic item for vi can be denoted as li∗, wherei∗ = arg max svl[i, :].
accordingly, for everyweight avi→vj in the original visual self-attentionmatrix svv , its corresponding weight ali∗ →lj∗ inthe linguistic self-attention matrix sll can be ex-tracted by the following operation:6.ali∗ →lj∗ = ext (sll, li∗ , lj∗ ) ,.
i∗ = arg max svl[i, :],j∗ = arg max svl[j, :],.
(10).
as a singular alignment.
after all the extractions,we reconstruct a mirrored matrix s(s)vv such thats(s)vv [i, j] = ali∗ →lj∗ , which can be regarded as a.
6compared with section 2.2, the ext operation here ex-.
tracts a singular attention weight instead of a patch..518visualself-attentionvisualself-attentionfromcross-modalviewvisualself-attentionfromlinguisticviewproductself-attentions𝒱𝒱𝒱𝒱dextractedself-attentionsurfersvisualself-attentions𝒱𝒱𝒱𝒱v2lcross-attentions𝒱𝒱ℒl2vcross-attentionsℒ𝒱𝒱=twosurfersenjoyingthewavestextualself-attentionsℒℒsurfersxtwosurfersenjoyingthewavesxtwosurfersenjoyingthewaveswavessurfersextractsurfers=singularalignmentdistributedalignmentsingularalignmentdistributedalignmentitemrelationmatrixrelationsurferswavessurferssurferssingularmapping:s𝒱𝒱𝒱𝒱sdistributedmapping:algorithm 2: singular alignment.
input: intra-modal self-attention matrices sll, svvfor i = 1 to nv do.
i∗ ← arg max svl[i, :]for j = 1 to nv do.
j∗ ← arg max svl[j, :]s(s).
vv [i, j] ← ext (sll, li∗ , lj∗ ).
for i = 1 to nl do.
i∗ ← arg max slv [i, :]for j = 1 to nl do.
j∗ ← arg max slv [j, :]s(s).
ll[i, j] ← ext (svv , vi∗ , vj∗ ).
(cid:16)σ(svv ) , σ(s(s)vv ).
(cid:17).
+.
σ(sll) , σ(s(s)ll).
(cid:17).
l(s).
iais = m-kl(cid:16).
m-klreturn l(s)iais.
vv is denoted as l(s).
representation of the original visual self-attentionsvv from the linguistic view.
the surrogate loss ofisda between svv and s(s)iais-vwhen taking vision as the anchor modality.
thesimilar processing can also be performed when thelinguistic sequence is the anchor.
we can gener-ate the matrix s(s)ll as a visual representation ofthe linguistic self-attention sll and deﬁne a corre-sponding loss l(s).
iais-l..the detailed processing of singular alignmentis illustrated in algorithm 2 and figure 4. thesingular version of iais loss is deﬁned as:.
iais =l(s)l(s).
iais-v + l(s)(cid:16).
iais-l.= m-kl.
(cid:17).
+.
σ(svv ) , σ(s(s)vv )(cid:17).
σ(sll) , σ(s(s)ll).
..(cid:16).
m-kl.
(11).
3.2 distributed alignment.
as singular items from different modalities may notbe able to give a full representation for each other,we further propose distributed alignment, whichutilizes a collection of linguistic items as a repre-sentation of a visual item, and vice versa.
speciﬁ-cally, given two visual items vi and vj, we regardthe inter-modal attentions σ(svl[i, :])7 from vi toall linguistic items and σ(slv [:, j])8 from all lin-guistic items to vj as a kind of features.
hence theoriginal similarity svv [i, j] = avi→vj between viand vj can also be modeled as a dot-product of theirdistributed attention features from the cross-modalview: σ(svl[i, :]) · σ(slv [:, j]).
such distributed.
7the ith row of svl.
8the jth column of slv ..alignment leverages the language as a bridge todraw implicit connections within the visual modal-ity, which can be intuitively regarded as the back-translation (sennrich et al., 2016) for multimodal.
as shown in figure 4, the distributed version of mir-rored self-attention matrix can be constructed by amatrix multiplication of two inter-modal attentionmatrices:.
s(d)s(d).
vv = σ(svl) σ(slv ),ll = σ(slv ) σ(svl)..similar to the version of singular alignment, thedistributed iais loss can be written as:.
iais =l(d)l(d).
iais-v + l(d)(cid:16).
iais-l.= m-kl.
σ(svv ) , s(d)vv.
(cid:16).
m-kl.
σ(sll) , s(d)ll.
(cid:17).
+.
(cid:17)..(12).
(13).
3.3 relation alignment as regularizer.
with the iais loss, the surrogate of semantic dis-tance between two intra-modal self-attentions, wepresent a new regularized training method to en-hance the relation alignment for image-text re-trieval.
our ﬁnal loss is two-fold.
the ﬁrst is thetask-orientated margin loss:.
lmargin =.
(cid:88)npi=1.
(cid:104)(cid:88)nnj=1.
sj − si + α.
(14).
(cid:105).
,.
+.
where [x]+ = max(0, x) and α is a preset margin.
np and nn denote the number of positive and neg-ative pairs.
si and sj are the similarity scores of apositive and negative image-text pair, respectively.
the second is the iais loss for all positive pairsthat quantiﬁes their relation distance.
the iais lossis computed based on the attentions from the lasttransformer-layer, and it can be either the singu-lar alignment version (eq.
(11)) or the distributedalignment version (eq.
(13)).
to summarize, ourﬁnal ﬁnal loss can be formalized as:.
l = lmargin + λtliais,.
(15).
where λt is a hyper-parameter w.r.t training steps tto balance two loss items.
since our relation-levelalignment is based on mappings between linguisticand visual items, it is beneﬁcial to focus on theitem-level alignment at the previous training stagevia the task-orientated loss.
accordingly, we utilizetraining signal annealing (xie et al., 2020) togradually incorporate the signal of the iais lossand design the following exponential schedule:.
λt = exp ((t/t − 1) × 5) ..(16).
519flickr30k.
ms coco.
modeluniter-base∗uniter-base†.
+ iais-singular+ iais-distributeduniter-large∗uniter-large†.
+ iais-singular+ iais-distributed.
image retrieval.
text retrieval.
overall.
image retrieval.
text retrieval.
overall.
r@1 r@5 r@10 r@1 r@5 r@10 meta-sum r@1 r@5 r@10 r@1 r@5 r@10 meta-sum.
72.5272.70.
73.5473.66.
73.5675.98.
76.8676.28.
92.3692.60.
93.1492.88.
94.0893.40.
93.3093.32.
96.0896.14.
96.3296.28.
96.7696.68.
95.7295.58.
85.9085.50.
86.1087.10.
87.3085.80.
88.3088.30.
97.1097.30.
98.1097.90.
98.0097.80.
98.4098.60.
98.8098.60.
99.1099.20.
99.2098.80.
99.4099.30.
542.76542.84.
546.30547.02.
548.90548.46.
551.98551.38.
50.3350.41.
50.9951.10.
52.9352.57.
53.1753.18.
78.5278.33.
78.8578.70.
79.9379.76.
80.0779.99.
87.1686.94.
87.4187.09.
87.9588.00.
87.9488.18.
64.4065.16.
66.9866.88.
65.6864.24.
67.7867.68.
87.4087.60.
89.1088.90.
88.5688.00.
89.7089.34.
93.0893.14.
94.0294.10.
93.7693.62.
94.4894.02.
460.89461.58.
467.35466.77.
468.81466.19.
473.14472.39.table 1: results of image and text retrieval on flickr30k and ms coco.
r@k corresponds to whether the groundtruth is recalled among top k results.
∗ denotes the results of uniter taken from chen et al.
(2020) and † denotesour reproduction.
iais-singular and isa-distributed denote the singular and distributed version of the proposedrelation-leve alignment, respectively..here t is the total training steps during ﬁne-tuningphase and t is the current step.
as a pluggableregularizer, our iais method does not incorporateany extra parameters and additional data collectionyet empowers the models to capture the higher-level semantics of relation consistency efﬁciently..positive instances, the batch size is 512. the learn-ing rate is 5e-5 and the training steps are 5000 forboth base and large models.
all experiments arerun on 8 nvidia v100 gpus..5 results and analysis.
4 experimental settings.
5.1 main results.
4.1 benchmark datasets.
we conduct experiments on the flickr30k (younget al., 2014) and ms coco (lin et al., 2014)datasets.
flickr30k contains 31k images col-lected from the flickr website, with ﬁve textualdescriptions per image.
we follow karpathy andli (2015) to split the data into 30k/1k/1k train-ing/validation/test splits.
ms coco consists of123k images, each accompanied with ﬁve human-written captions.
following karpathy and li(2015), the data is divided into 82k/5k/5k train-ing/validation/test images..4.2 fine-tuning settings.
due to the limitation of computing resource, weonly incorporate iais regularization in the phaseof ﬁne-tuning instead of pre-training.
we use thebase (12 layers) and the large (24 layers) versionof uniter (chen et al., 2020), one of the mostprevailing large-scale pre-trained models, as ourbaseline and backbone for iais.
we follow theﬁne-tuning setting and hyper-parameter conﬁgura-tion of the original paper.9 the margin in eq.
(14)is 0.2. for each positive instance, 31 hard negativeinstances are sampled on the text and image side,respectively, and as each batch contains 8 different.
9https://github.com/chenrocks/uniter.
the main results of the uniter performance withand without our iais regularization are reportedin table 1. our methods of both singular and dis-tributed version surpass the baseline by a consid-erable margin.
the average improvement over alldatasets and models is 4.49..there are also some interesting ﬁndings: (1)compared with image retrieval, the model perfor-mance on text retrieval is boosted by iais moreremarkably with an average improvement of 3.50.note that each image in both datasets is pairedwith ﬁve ground-truth sentences, and our iais reg-ularizer helps the model capture the common re-lations for the image and the corresponding textsso that more ground-truth texts can be successfullyretrieved.
(2) the improvement on uniter-baseis 17.2% higher than that on uniter-large.
aconsistent result can be found in table 2, whichdemonstrates various relation distance metrics ofﬁne-tuned models.
the isda of uniter-largeis smaller than that of uniter-base, indicatinguniter-large learns more about the relation con-sistency due to its large capability while there isstill room to improve the relation alignment withour iais method.
(3) the relative improvementbrought by the singular version of iais is 7.0%,higher than that of the distributed version.
theisda and l(s)iais are correlated with a pearson’s cor-.
520model.
uniter-base+ iais-singular+ iais-distributed.
uniter-large+ iais-singular+ iais-distributed.
isda.
0.260.180.17.
0.230.180.18.l(s)iais0.591.31e-32.80e-3.
0.402.27e-33.15e-3.
l(d)iais0.362.58e-32.72e-3.
0.163.22e-33.70e-3.
modeluniter-base∗uniter-base†.
+ iais-singular-l+ iais-singular-v.+ iais-distributed-l 72.48+ iais-distributed-v 73.14.image retrieval.
text retrieval.
r@1 r@5 r@10 r@1 r@5 r@10.
72.5272.70.
72.7473.44.
92.3692.60.
92.7492.76.
92.9692.44.
96.0896.14.
96.1295.96.
96.2696.06.
85.9085.50.
86.9087.00.
86.9087.10.
97.1097.30.
96.7097.50.
97.1097.40.
98.8098.60.
99.0099.10.
99.1099.20.table 2: different relation distance metrics of eachmodel after ﬁne-tuning.
lower is better..table 3: ablation study on the flickr30k dataset.
“-l”denotes that only liais-l is incorporated, which regardslanguage as the anchor modality.
similar for “-v”..figure 5: the singular and distributed version of iaisloss with respect to the training steps..relation coefﬁcient of 0.779, which is also highercompared to l(d)iais with 0.774. besides, our em-pirical analysis in figure 5 shows that it is slightlyeasier to optimize the l(s)iais, indicating it is a bettersurrogate of isda..5.2 effect of anchor modality.
in section 3.3, we leverage both the linguistic andthe visual input as the anchor sequence to recon-struct the mirrored sequence from the oppositemodalities.
to study the impact of the anchormodality, we conduct an ablation study and theresults are listed in table 3. compared to using lan-guage as the anchor modality, i.e., only liais-l isincorporated, the overall model performance is 2.1higher when vision is taken as the anchor.
an ex-planation is that the description capability of visualregions is more concrete and powerful.
however,introducing both liais-v + liais-l to the ﬁnal losscan achieve a further improvement of 2.22, whichindicates the necessity of such combination..5.3 effect of annealing schedule.
besides the exp schedule in eq.
(16) for trainingsignal annealing, we also try other schedules:.
• log schedule: λt = 1 − exp (−t/t × γ);.
• linear schedule: λt = t/t ;.
figure 6: schedules for iais signal annealing..we compare the results of ﬁve schedules foriais signal annealing.
the results in figure 8 showthat the exp schedule with scale γ = 5 achieves thebest performance..5.4 effect of layer to apply iais.
we also apply iais on different layers of uniter-base.
as illustrated in figure 9, the optimal way isto apply iais on the last layer.
we speculate that itis more important to learn relation alignment in thedeeper layers because the attention in the deeperlayers has a bigger impact on the ﬁnal output, whilethe effect of the attention in shallow layers mightfade away due to the normalization..5.5 case study.
we further discuss the advantage of our proposedrelation-level alignment.
figure 7 shows twovisualization examples of the intra-modal self-attentions from the flickr30k entities dataset.
withiais regularization, the model is instructed to con-centrate on the common relations within the linguis-tic and visual sequence, yielding more calibratedand consistent self-attention distributions..• exp schedule: λt = exp ((t/t − 1) × γ),.
6 related work.
where γ is chosen from {5, 10}.
all the schedulesare shown in figure 6..in this section, we introduce the task of image-textretrieval and review the representative studies of.
52105001k1.5k2k2.5k3k3.5k4k4.5k5k6-e36-e85-e45-e94-e53-e13-e602.0losstraining steps: loss of singular alignment             : loss of distributed alignment0.00.20.40.60.81.0tt0.00.20.40.60.81.0tlinearexp_5exp_10log_5log_10figure 7: visualization of intra-modal self-attentions with and without our iais method..comparison of.
figure 8:the different hyper-parameters in training signal annealing.
the exp sched-ule with scale γ = 5 achieves the best performance..figure 9: comparison of the different layers to applyiais on the flickr30k dataset.
the iais applied on thelast layer achieves the best performance..large-scale multimodal pre-trained models..image-text retrievalimage-text retrieval(itr, barnard et al., 2003; barnard and forsyth,2001), also known as image-text matching, isone of the popular and challenging language-and-vision (v+l) tasks.
given image-text pairs,the prevailing approaches project them into ajoint representation space, on which cosine ordot-product similarities are deﬁned, and recall themost relevant one according to the similarity..multimodal pre-trained models the develop-ment of the transformer-based large-scale pre-training paradigm sweeps across the area of multi-modal learning and achieves many state-of-the-artresults on v+l tasks like image captioning, vi-sual question answering, visual commonsensereasoning, etc.
recent prevailing multimodalpre-trained models can be categorized into single-stream (chen et al., 2020; gan et al., 2020; linet al., 2020; li et al., 2020; su et al., 2020; linet al., 2021) and two-stream (yu et al., 2020; tanand bansal, 2019; lu et al., 2019) models.
given apiece of text and an image, the former architectureconcatenates the features of tokens and regions andlearns their joint representations with one trans-former model, while the latter embeds the textualand the visual input separately with two indepen-dent intra-modal transformers and then utilizes an.
inter-modal transformer to reinforce cross-modalinteractions via cross-modal attention modules..7 conclusion.
in this paper, we promote the semantic alignmentfor cross-modal retrieval from the object level tothe relation level.
we propose a surrogate metricto quantify the relation consistency by measuringthe semantic distance between linguistic and visualrelations.
furthermore, we present a regularizedtraining method iais to calibrate intra-modal self-attentions mutually by minimizing the isd metric.
our method improves both the performance and theinterpretability of large-scale pre-trained models.
note that, without object annotation in practice, thesingular and distributed version of the iais lossonly provides a coarse-grained attention distribu-tion alignment.
we leave the elaborate design ofisda proxy function for future work..acknowledgments.
this work is partly supported by beijing academyof artiﬁcial intelligence (baai).
we thank all theanonymous reviewers for their constructive com-ments, and xuancheng ren and lei li for theirhelpful suggestions in preparing the manuscript..5220.290.140.56(therestregions)(theresttokens)0.240.360.40intra-modalself-attentionsinconsistent☹visualself-attentiondistribution:a gray-haired man is sitting down with his arm stretched out.a gray-haired manhis armis sitting down with,stretched out.his arm0.270.290.44(therestregions)(theresttokens)0.250.310.44intra-modalself-attentionscalibrateda gray-haired manhis armis sitting down with,stretched out.his arm"linguisticself-attentiondistribution:uniter-large+iais-singularuniter-large0.400.160.44(therestregions)0.550.300.15intra-modalself-attentionsinconsistent☹visualself-attentiondistribution:a boat captain steering a large green wheel.a boatcaptain(theresttokens)steering.a largegreen wheel0.480.230.29intra-modalself-attentionscalibrated"linguisticself-attentiondistribution:(therestregions)a largegreen wheel0.500.250.25a boatcaptain(theresttokens)steering.a largegreen wheela largegreen wheeluniter-large+iais-singularuniter-largew/o iaislog-10log-5linearexp-10exp-5hyper-parameters of training signal annealing543.0543.5544.0544.5545.0545.5546.0546.5547.0meta-sum of recall542.76544.64544.8544.32545.8547.02w/o iaislayer-1layer-6layer-12layer to apply iais543544545546547meta-sum of recall542.76543.6545.68547.02references.
peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
in2018 ieee conference on computer vision and pat-tern recognition, cvpr 2018, salt lake city, ut,usa, june 18-22, 2018, pages 6077–6086.
ieeecomputer society..kobus barnard, pinar duygulu, david a. forsyth,nando de freitas, david m. blei, and michael i. jor-dan.
2003. matching words and pictures.
j. mach.
learn.
res., 3:1107–1135..kobus barnard and david a. forsyth.
2001. learn-in pro-ing the semantics of words and pictures.
ceedings ofthe eighth international conferenceon computer vision (iccv-01), vancouver, britishcolumbia, canada, july 7-14, 2001 - volume 2,pages 408–415.
ieee computer society..emanuele bugliarello, ryan cotterell, naoakiokazaki, and desmond elliott.
2020. multimodalpretraining unmasked: unifying the vision andlanguage berts.
corr, abs/2011.15124..yen-chun chen, linjie li, licheng yu, ahmed elkholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2020. uniter: universal image-textrepresentation learning.
in computer vision - eccv2020 - 16th european conference, glasgow, uk, au-gust 23-28, 2020, proceedings, part xxx, volume12375 of lecture notes in computer science, pages104–120.
springer..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertan analysis of bert’s attention.
corr,look at?
abs/1906.04341..fartash faghri, david j. fleet, jamie ryan kiros, andsanja fidler.
2018. vse++:improving visual-semantic embeddings with hard negatives.
in britishmachine vision conference 2018, bmvc 2018, new-castle, uk, september 3-6, 2018, page 12. bmvapress..zhe gan, yen-chun chen, linjie li, chen zhu,yu cheng, and jingjing liu.
2020. large-scale ad-versarial training for vision-and-language represen-tation learning.
in advances in neural informationprocessing systems 33: annual conference on neu-ral information processing systems 2020, neurips2020, december 6-12, 2020, virtual..phu mon htut, jason phang, shikha bordia, andsamuel r. bowman.
2019. do attention headscorr,in bert track syntactic dependencies?
abs/1911.12246..andrej karpathy and fei-fei li.
2015. deep visual-semantic alignments for generating image descrip-tions.
in ieee conference on computer vision andpattern recognition, cvpr 2015, boston, ma, usa,.
june 7-12, 2015, pages 3128–3137.
ieee computersociety..ryan kiros, ruslan salakhutdinov, and richard s.zemel.
2014. unifying visual-semantic embeddingswith multimodal neural language models.
corr,abs/1411.2539..olga kovaleva, alexey romanov, anna rogers, andanna rumshisky.
2019. revealing the dark secretsof bert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on nat-ural language processing, emnlp-ijcnlp 2019,hong kong, china, november 3-7, 2019, pages4364–4373.
association for computational linguis-tics..kuang-huei lee, xi chen, gang hua, houdong hu,and xiaodong he.
2018. stacked cross attention forin computer vision - eccvimage-text matching.
2018 - 15th european conference, munich, ger-many, september 8-14, 2018, proceedings, part iv,volume 11208 of lecture notes in computer sci-ence, pages 212–228.
springer..xiujun li, xi yin, chunyuan li, pengchuan zhang, xi-aowei hu, lei zhang, lijuan wang, houdong hu,li dong, furu wei, yejin choi, and jianfeng gao.
2020. oscar: object-semantics aligned pre-trainingin computer vision -for vision-language tasks.
eccv 2020 - 16th european conference, glasgow,uk, august 23-28, 2020, proceedings, part xxx,volume 12375 of lecture notes in computer sci-ence, pages 121–137.
springer..junyang lin, rui men, an yang, chang zhou, mingding, yichang zhang, peng wang, ang wang,le jiang, xianyan jia, jie zhang, jianwei zhang,xu zou, zhikang li, xiaodong deng, jie liu, jin-bao xue, huiling zhou, jianxin ma, jin yu, yongli, wei lin, jingren zhou, jie tang, and hongxiayang.
2021. m6: a chinese multimodal pretrainer.
corr, abs/2103.00823..junyang lin, an yang, yichang zhang, jie liu, jingrenzhou, and hongxia yang.
2020. interbert: vision-and-language interaction for multi-modal pretrain-ing.
corr, abs/2003.13198..tsung-yi lin, michael maire, serge j. belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c. lawrence zitnick.
2014. microsoft coco:in computer visioncommon objects in context.
- eccv 2014 - 13th european conference, zurich,switzerland, september 6-12, 2014, proceedings,part v, volume 8693 of lecture notes in computerscience, pages 740–755.
springer..chunxiao liu, zhendong mao, tianzhu zhang, hong-tao xie, bin wang, and yongdong zhang.
2020.graph structured network for image-text matching.
in 2020 ieee/cvf conference on computer visionand pattern recognition, cvpr 2020, seattle, wa,usa, june 13-19, 2020, pages 10918–10927.
ieee..523zihao wang, xihui liu, hongsheng li, lu sheng,junjie yan, xiaogang wang, and jing shao.
2019.camp: cross-modal adaptive message passing forin 2019 ieee/cvf interna-text-image retrieval.
tional conference on computer vision, iccv 2019,seoul, korea (south), october 27 - november 2,2019, pages 5763–5772.
ieee..qizhe xie, zihang dai, eduard h. hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in advances in neuralinformation processing systems 33: annual con-ference on neural information processing systems2020, neurips 2020, december 6-12, 2020, virtual..pengcheng yang, boxing chen, pei zhang, andxu sun.
2020. visual agreement regularized train-in theing for multi-modal machine translation.
thirty-fourth aaai conference on artiﬁcial intelli-gence, aaai 2020, the thirty-second innovative ap-plications of artiﬁcial intelligence conference, iaai2020, the tenth aaai symposium on educationaladvances in artiﬁcial intelligence, eaai 2020, newyork, ny, usa, february 7-12, 2020, pages 9418–9425. aaai press..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
trans.
assoc.
com-put.
linguistics, 2:67–78..fei yu, jiji tang, weichong yin, yu sun, hao tian,hua wu, and haifeng wang.
2020.ernie-vil:knowledge enhanced vision-language representa-tions through scene graph.
corr, abs/2006.16934..bowen zhang, hexiang hu, vihan jain, eugene ie,and fei sha.
2020a.
learning to represent imagein proceedings ofand text with denotation graph.
the 2020 conference on empirical methods in nat-ural language processing, emnlp 2020, online,november 16-20, 2020, pages 823–839.
associationfor computational linguistics..zhihan zhang, zhiyi yin, shuhuai ren, xinhang li,and shicheng li.
2020b.
dca: diversiﬁed co-attention towards informative live video comment-in natural language processing and chi-ing.
nese computing - 9th ccf international confer-ence, nlpcc 2020, zhengzhou, china, october14-18, 2020, proceedings, part ii, volume 12431of lecture notes in computer science, pages 3–15.
springer..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems 32: annual conference on neural infor-mation processing systems 2019, neurips 2019, de-cember 8-14, 2019, vancouver, bc, canada, pages13–23..nicola messina, giuseppe amato, andrea esuli,fabrizio falchi, claudio gennaro, and st´ephanemarchand-maillet.
2020. fine-grained visual tex-tual alignment for cross-modal retrieval using trans-former encoders.
corr, abs/2008.05231..bryan a. plummer, liwei wang, chris m. cervantes,juan c. caicedo, julia hockenmaier, and svetlanalazebnik.
2015.flickr30k entities: collectingregion-to-phrase correspondences for richer image-in 2015 ieee internationalto-sentence models.
conference on computer vision, iccv 2015, santi-ago, chile, december 7-13, 2015, pages 2641–2649.
ieee computer society..shaoqing ren, kaiming he, ross b. girshick, andjian sun.
2015. faster r-cnn: towards real-timeobject detection with region proposal networks.
inadvances in neural information processing systems28: annual conference on neural information pro-cessing systems 2015, december 7-12, 2015, mon-treal, quebec, canada, pages 91–99..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics, acl 2016, august 7-12, 2016,berlin, germany, volume 1: long papers.
the asso-ciation for computer linguistics..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2020. vl-bert: pre-training of generic visual-linguistic representations.
in 8th international conference on learning repre-sentations, iclr 2020, addis ababa, ethiopia, april26-30, 2020. openreview.net..hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-formers.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on nat-ural language processing, emnlp-ijcnlp 2019,hong kong, china, november 3-7, 2019, pages5099–5110.
association for computational linguis-tics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..524