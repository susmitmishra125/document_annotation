unire: a uniﬁed label space for entity relation extraction.
yijun wang∗1, 2, changzhi sun∗4, yuanbin wu3, hao zhou4, lei li4, and junchi yan†1, 2.
1department of computer science and engineering, shanghai jiao tong university2moe key lab of artiﬁcial intelligence, ai institute, shanghai jiao tong university3school of computer science and technology, east china normal university4bytedance ai lab{yjwang.cs, yanjunchi}@sjtu.edu.cn ybwu@cs.ecnu.edu.cn{sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com.
abstract.
many joint entity relation extraction modelssetup two separated label spaces for the twosub-tasks (i.e., entity detection and relationclassiﬁcation).
we argue that this setting mayhinder the information interaction between en-tities and relations.
in this work, we proposeto eliminate the different treatment on the twosub-tasks’ label spaces.
the input of our modelis a table containing all word pairs from a sen-tence.
entities and relations are representedby squares and rectangles in the table.
we ap-ply a uniﬁed classiﬁer to predict each cell’s la-bel, which uniﬁes the learning of two sub-tasks.
for testing, an effective (yet fast) approximatedecoder is proposed for ﬁnding squares andrectangles from tables.
experiments on threebenchmarks (ace04, ace05, scierc) showthat, using only half the number of parameters,our model achieves competitive accuracy withthe best extractor, and is faster..1.introduction.
extracting structured information from plain textsis a long-lasting research topic in nlp.
typically, itaims to recognize speciﬁc entities and relations forproﬁling the semantic of sentences.
an example isshown in figure 1, where a person entity “davidperkins” and a geography entity “california” havea physical location relation phys..methods for detecting entities and relations canbe categorized into pipeline models or joint models.
in the pipeline setting, entity models and relationmodels are independent with disentangled featurespaces and output label spaces.
in the joint setting,on the other hand, some parameter sharing of fea-ture spaces (miwa and bansal, 2016; katiyar and.
∗equal contribution.
†corresponding author..figure 1: example of a table for joint entity relationextraction.
each cell corresponds to a word pair.
en-tities are squares on diagonal, relations are rectanglesoff diagonal.
note that per-soc is a undirected (sym-metrical) relation type, while phys and org-aff aredirected (asymmetrical) relation types.
the table exactlyexpresses overlapped relations, e.g., the person entity“david perkins” participates in two relations, (“davidperkins”, “wife”, per-soc) and (“david perkins”,“california”, phys).
for every cell, a same biafﬁnemodel predicts its label.
the joint decoder is set to ﬁndthe best squares and rectangles..cardie, 2017) or decoding interactions (yang andcardie, 2013; sun et al., 2019) are imposed to ex-plore the common structure of the two tasks.
it wasbelieved that joint models could be better since theycan alleviate error propagations among sub-models,have more compact parameter sets, and uniformlyencode prior knowledge (e.g., constraints) on bothtasks..however, zhong and chen (2020) recently show.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages220–231august1–6,2021.©2021associationforcomputationallinguistics220perper⊥⊥per-soc⊥⊥⊥⊥physperper⊥⊥per-soc⊥⊥⊥⊥phys⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥perper-soc⊥⊥⊥⊥⊥per-socper-soc⊥per-socper⊥⊥⊥⊥phys⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥gpe⊥⊥⊥⊥⊥⊥⊥⊥⊥org-affper⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥⊥gpedavidperkinsandhiswifearevillagedoctorscaliforniadavidperkinsandhiswifearecaliforniaininvillagedoctorsperperperpergpegpeper-socper-socorg-affphysphysthat with the help of modern pre-training tools (e.g.,bert), separating the entity and relation model(with independent encoders and pipeline decoding)could surpass existing joint models.
they arguethat, since the output label spaces of entity and re-lation models are different, comparing with sharedencoders, separate encoders could better capturedistinct contextual information, avoid potential con-ﬂicts among them, and help decoders making amore accurate prediction, that is, separate labelspaces deserve separate encoders..in this paper, we pursue a better joint model forentity relation extraction.
after revisiting existingmethods, we ﬁnd that though entity models andrelation models share encoders, usually their la-bel spaces are still separate (even in models withjoint decoders).
therefore, parallel to (zhong andchen, 2020), we would ask whether joint encoders(decoders) deserve joint label spaces?.
the challenge of developing a uniﬁed entity-relation label space is that the two sub-tasks areusually formulated into different learning prob-lems (e.g., entity detection as sequence labeling,relation classiﬁcation as multi-class classiﬁcation),and their labels are placed on different things (e.g.,words v.s.
words pairs).
one prior attempt (zhenget al., 2017) is to handle both sub-tasks with onesequence labeling model.
a compound label setwas devised to encode both entities and relations.
however, the model’s expressiveness is sacriﬁced:it can detect neither overlapping relations (i.e., en-tities participating in multiple relation) nor isolatedentities (i.e., entities not appearing in any relation)..our key idea of deﬁning a new uniﬁed labelspace is that, if we think zheng et al.
(2017)’s so-lution is to perform relation classiﬁcation duringentity labeling, we could also consider the reversedirection by seeing entity detection as a specialcase of relation classiﬁcation.
our new input spaceis a two-dimensional table with each entry corre-sponding to a word pair in sentences (figure 1).
the joint model assign labels to each cell from auniﬁed label space (union of entity type set andrelation type set).
graphically, entities are squareson the diagonal, and relations are rectangles offthe diagonal.
this formulation retains full modelexpressiveness regarding existing entity-relationextraction scenarios (e.g., overlapped relations, di-rected relations, undirected relations).
it is alsodifferent from the current table ﬁlling settings forentity relation extraction (miwa and sasaki, 2014;.
gupta et al., 2016; zhang et al., 2017; wang andlu, 2020), which still have separate label spacefor entities and relations, and treat on/off-diagonalentries differently..based on the tabular formulation, our joint en-tity relation extractor performs two actions, ﬁllingand decoding.
first, ﬁlling the table is to predicteach word pair’s label, which is similar to arc pre-diction task in dependency parsing.
we adopt thebiafﬁne attention mechanism (dozat and manning,2016) to learn interactions between word pairs.
wealso impose two structural constraints on the ta-ble through structural regularizations.
next, giventhe table ﬁlling with label logits, we devise an ap-proximate joint decoding algorithm to output theﬁnal extracted entities and relations.
basically, itefﬁciently ﬁnds split points in the table to iden-tify squares and rectangles (which is also differentwith existing table ﬁlling models which still applycertain sequential decoding and ﬁll tables incre-mentally)..experimental.
results on three benchmarks(ace04, ace05, scierc) show that the proposedjoint method achieves competitive performancescomparing with the current state-of-the-art extrac-tors (zhong and chen, 2020): it is better on ace04and scierc, and competitive on ace05.1 mean-while, our new joint model is fast on decoding(10x faster than the exact pipeline implementation,and comparable to an approximate pipeline, whichattains lower performance).
it also has a more com-pact parameter set: the shared encoder uses onlyhalf the number of parameters comparing with theseparate encoder (zhong and chen, 2020)..2 task deﬁnition.
given an input sentence s = x1, x2, .
.
.
, x|s| (xi isa word), this task is to extract a set of entities e anda set of relations r. an entity e is a span (e.span)with a pre-deﬁned type e.type ∈ ye (e.g., per,gpe).
the span is a continuous sequence of words.
a relation r is a triplet (e1, e2, l), where e1, e2 aretwo entities and l ∈ yr is a pre-deﬁned relationtype describing the semantic relation among twoentities (e.g., the phys relation between per andgpe mentioned before).
here ye, yr denote theset of possible entity types and relation types re-spectively..we formulate the joint entity relation extraction.
1source code and models are available at https://github..com/receiling/unire..221as a table ﬁlling task (multi-class classiﬁcation be-tween each word pair in sentence s), as shown infigure 1. for the sentence s, we maintain a tablet |s|×|s|.
for each cell (i, j) in table t , we assigna label yi,j ∈ y, where y = ye ∪ yr ∪ {⊥} (⊥ denotes no relation).
for each entity e, the la-bel of corresponding cells yi,j(xi ∈ e.span, xj ∈e.span) should be ﬁlled in e.type.
for each re-lation r = (e1, e2, l), the label of correspondingcells yi,j(xi ∈ e1.span, xj ∈ e2.span) shouldbe ﬁlled in l.2 while others should be ﬁlled in ⊥.
in the test phase, decoding entities and relationsbecomes a rectangle ﬁnding problem.
note thatsolving this problem is not trivial, and we proposea simple but effective joint decoding algorithm totackle this challenge..3 approach.
in this section, we ﬁrst introduce our biafﬁne modelfor table ﬁlling task based on pre-trained languagemodels (section 3.1).
then we detail the mainobjective function of the table ﬁlling task (section3.2) and some constraints which are imposed onthe table in training stage (section 3.3).
finallywe present the joint decoding algorithm to extractentities and relations (section 3.4).
figure 2 showsan overview of our model architecture.3.
3.1 biafﬁne model.
given an input sentence s, to obtain the contex-tual representation hi for each word, we use apre-trained language model (plm) as our sentenceencoder (e.g., bert).
the output of the encoder is.
{h1, .
.
.
, h|s|} = plm({x1, .
.
.
, x|s|}),.
where xi is the input representation of each wordxi.
taking bert as an example, xi sums the corre-sponding token, segment and position embeddings.
to capture long-range dependencies, we also em-ploy cross-sentence context following (zhong andchen, 2020), which extends the sentence to a ﬁxedwindow size w (w = 200 in our default settings).
to better encode direction information of wordsin table t , we use the deep biafﬁne attention mech-anism (dozat and manning, 2016), which achievesimpressive results in the dependency parsing task.
speciﬁcally, we employ two dimension-reducing.
mlps (multi-layer perceptron), i.e., a head mlpand a tail mlp, on each hi as.
i.hheadi.
∈ rd and htail.
= mlphead(hi), htail.
i = mlptail(hi),i ∈ rd are projectionwhere hheadrepresentations, allowing the model to identify thehead or tail role of each word.
next, we calculatethe scoring vector gi,j ∈ r|y| of each word pairwith biafﬁne model,.
gi,j = biaff(hhead.
, htailj1 u1h2 + u2(h1 ⊕ h2) + b,.
),.
i.biaff(h1, h2) = ht.
where u1 ∈ r|y|×d×d and u2 ∈ r|y|×2d areweight parameters, b ∈ r|y| is the bias, ⊕ denotesconcatenation..3.2 table filling.
after obtaining the scoring vector gi,j, we feed gi,jinto the softmax function to predict correspondinglabel, yielding a categorical probability distributionover the label space y as.
p (yi,j|s) = softmax(dropout(gi,j))..in our experiments, we observe that apply-ing dropout in gi,j, similar to de-noising auto-encoding, can further improve the performance.
4.we refer this trick to logit dropout and the trainingobjective is to minimize.
lentry=−.
log p (yi,j = yi,j|s), (1).
1|s|2.
|s|(cid:88).
|s|(cid:88).
i=1.
j=1.
where the gold label yi,j can be read from annota-tions, as shown in figure 1..3.3 constraints.
in fact, equation 1 is based on the assumption thateach label is independent.
this assumption sim-pliﬁes the training procedure, but ignores somestructural constraints.
for example, entities andrelations correspond to squares and rectangles inthe table.
equation 1 does not encode this con-straint explicitly.
to enhance our model, we pro-pose two intuitive constraints, symmetry and im-plication, which are detailed in this section.
herewe introduce a new notation p ∈ r|s|×|s|×|y|, de-noting the stack of p (yi,j|s) for all word pairs insentence s.5.
2assuming no overlapping entities in one sentence.
3we only show three labels of y in figure 2 for simplicity.
4we set dropout rate p = 0.2 by default.
5p without logit dropout mentioned in section 3.2 to pre-.
and clarity..serve learned structure..222figure 2: overview of our model architecture.
one main objective (lentry) and two additional objectives(lsym, limp) are imposed on probability tensor p and optimized jointly..dataset.
ace04/ace05.
scierc.
ysym.
ent.
per,org,loc,fac,wea,veh,gpe.
rel.
per-soc.
task,method,metric,material,generic,otherscientificterm.
comparep,conjunction.
table 1: symmetrical label set ysym for used datasets..symmetry we have several observations fromthe table in the tag level.
firstly, the squares corre-sponding to entities must be symmetrical about thediagonal.
secondly, for symmetrical relations, therelation triples (e1, e2, l) and (e2, e1, l) are equiva-lent, thus the rectangles corresponding to two coun-terpart relation triples are also symmetrical aboutthe diagonal.
as shown in figure 1, the rectanglescorresponding to (“his”, “wife”, per-soc) and(“wife”, “his”, per-soc) are symmetrical aboutthe diagonal.
we divide the set of labels y into asymmetrical label set ysym and an asymmetricallabel set yasym.
the matrix p:,:,t should be sym-metrical about the diagonal for each label t ∈ ysym.
we formulate this tag-level constraint as symmetri-cal loss,.
lsym =.
1|s|2.
|s|(cid:88).
|s|(cid:88).
(cid:88).
i=1.
j=1.
t∈ysym.
|pi,j,t − pj,i,t|..we list all ysym in table 1 for our adopted datasets..implication a key intuition is that if a relationexists, then its two argument entities must also ex-ist.
in other words, it is impossible for a relation toexist without two corresponding entities.
from the.
perspective of probability, it implies that the proba-bility of relation is not greater than the probabilityof each argument entity.
since we model entityand relation labels in a uniﬁed probability space,this idea can be easily used in our model as theimplication constraint.
we impose this constrainton p: for each word in the diagonal, its maximumpossibility over the entity type space ye must notbe lower than the maximum possibility for otherwords in the same row or column over the rela-tion type space yr. we formulate this table-levelconstraint as implication loss,.
limp=.
1|s|.
|s|(cid:88).
(cid:20).
i=1.
maxl∈yr.
{pi,:,l, p:,i,l} − maxt∈ye.
(cid:21){pi,i,t}.
∗.
where [u]∗ = max(u, 0) is the hinge loss.
it isworth noting that we do not add margin in this lossfunction.
since the value of each item is a probabil-ity and might be relatively small, it is meaninglessto set a large margin..finally, we jointly optimize the three objectives.
in the training stage as lentry + lsym + limp.6.
3.4 decoding.
in the testing stage, given the probability tensorp ∈ r|s|×|s|×|y| of the sentence s, 7 how to decodeall rectangles (including squares) corresponding toentities or relations remains a non-trivial problem.
since brute force enumeration of all rectangles is in-tractable, a new joint decoding algorithm is needed.
we expect our decoder to have,.
6we directly sum the three losses to avoid introducing.
more hyper-parameters..pj,i,t = (pi,j,t + pj,i,t)/2..7for the symmetrical label t ∈ ysym, we set pi,j,t =.
223plmsoftmaxdecoder𝑥!𝑥"𝑥#𝑥$biaffine modelprobability tensor  <latexit sha1_base64="j7hu8hi5rx5anpweenxyesuffv0=">aaacjxicbvdlssnafj3uv62vqes3g0vwvrjrdogi6mzlffuqjpbjdnionuzczeqoax7gjb/ixovfbff+itm0fg09mhdmnhu59x4vylqqy/oyckvlk6trxfxsxubw9o65u9eqyswwqeoqhalliuky5asuqgkkfqmcao+rpje4nvjnjyikdfm9gkbedvcpu59ipltums+dakk+riyppdchhgz/z0vu0sfkfdqkbktcgrnnyh/sudoxy1bfygaxiz2tmshr65hjpxviocbcyyakbntwpnwecuuxi2njiswjeb6ghmlrypge6sbzlsk80kox+qhqjyuyqb87ehrioqw8xtlzus57e/e/rx0r/8jnki9irtiedvjjbluij5hblhuekzbubgfb9a4q95fawolgszoee/7krdi4qdhnfev2tfy9yumoggnwci6bdc5bfdyagqgddj7bk3ghy+pfedm+jm9pachie/bbhxjfp01hpcu=</latexit>p2r4⇥4⇥|y|decoding<latexit sha1_base64="e9vvmniirvp9hexza+c7uajgkiq=">aaab83icbvbns8nafhypx7v+vt16wsycp5kioseif48vbc00pwy2l+3szsbsbsqs+je8efdeq3/gm//gtzudtg4sddpv8wynsatxxnw/ndlk6tr6rnmzsrw9s7tx3t9o6zhvdfssfrhqbfsj4bjbhhubnuqhjqkbd8h4jvcfhlfphst7m0mwf9gh5cfn1fjj9ynqrkgypu37xr9ac+vudgszeawpqyfmv/rld2kwrigne1trrucmppdrztgtok34qcaesjedytdssspuvwyweupordigyazsk4bm1n8bgy20nksbncwz6kuvf//zuqkjr3ozl0lqull5otavxmqkl4amuejmxmqsyhs3wqkbuuwzstvvbane4pexsfus7l3u3bvzwuo6qkmmr3amp+dbjttgfprqagyjpmmrvdmp8+k8ox/z0zjt7bzchzifpyuykcq=</latexit>x1<latexit sha1_base64="4khuv1ypzzx7f2uoxigidugo7vm=">aaab83icbvdlsgmxfl2pr1pfvzdugkvwvwakosuig5cv7am6q8mkmty0kxmsjfig/oybf4q49wfc+tdm2llo64ha4zx7uscnsatxxng+uwltfwnzq7xd2dnd2z+ohh51djwqyto0frhqbuqzwsvrg24e6ywkksgqrbtmbno/+8iu5rf8mnoe+rezsr5ysoyvpc8izhye2dns0bhua07dmqovercgnsjqgls/vgfm04hjqwxruu86ifezogyngs0qxqpzquiejfjfukkipv1snnmgz6wyxggs7jmgz9xfgxmjtj5ggz3mm+pllxf/8/qpca/9jmsknuzsxaewfdjeoc8ad7li1iipjyqqbrnioiakugnrqtgs3ouvr5joo+5e1p37i1rzpqijdcdwcufgwhu04q5a0aykctzdk7yhfl2gd/sxgc2hyucy/gb9/gatnphf</latexit>x2<latexit sha1_base64="qzjesn/adnfjxnnrn2q3su/rzlu=">aaab83icbvdlsgmxfl1tx7w+qi7dbivgqsz4qjdfny4r2ad0hpjjm21ojhmsjfig/oybf4q49wfc+tdm2llo64ha4zx7uscntdjtxnw/ndlk6tr6rnmzsrw9s7tx3t9oa5kqqltecqm6idaum0fbhhlou4mioa457ytj29zvpfklmrqpzplqimzdwsjgslgs78fyjmioe5r2z/vvmlt3z0dlxctidqo0+9uvfybjglnhcmda9zw3mugglwge02nftzvnmbnjie1zknbmdzdnmk/rivugkjlkpmhqtp29kefy60kc2sk8o170cve/r5ea6drimehsqwwzh4psjoxeeqfowbqlhk8swuqxmxwrevaygfttxzbglx55mbtp6t5l3b2/qdvuijrkcathcaoexeed7qajlscqwdo8wputoi/ou/mxhy05xc4h/ihz+qmuuphg</latexit>x3<latexit sha1_base64="ewa9l9c3rgqnrqurz0k2678y1rq=">aaab83icbvdlsgmxfl2pr1pfvzdugkvwvwakosuig5cv7am6q8mkmty0kxmsjfig/oybf4q49wfc+tdm2llo64ha4zx7uscnsatxxng+uwltfwnzq7xd2dnd2z+ohh51djwqyto0frhqbuqzwsvrg24e6ywkksgqrbtmbno/+8iu5rf8mnoe+rezsr5ysoyvpc8izhye2dns0bhua07dmqovercgnsjqgls/vgfm04hjqwxruu86ifezogyngs0qxqpzquiejfjfukkipv1snnmgz6wyxggs7jmgz9xfgxmjtj5ggz3mm+pllxf/8/qpca/9jmsknuzsxaewfdjeoc8ad7li1iipjyqqbrnioiakugnrqtgs3ouvr5lord29rdv3jvrzpqijdcdwcufgwhu04q5a0aykctzdk7yhfl2gd/sxgc2hyucy/gb9/gawpphh</latexit>x4<latexit sha1_base64="ph5aslq5ggjac7xxdkitqenfvsg=">aaab83icbvdlssnafl2pr1pfvzdubovgqisi6lloxmuf+4amlml00g6dtmlmjvbcf8onc0xc+jpu/bunbrbaemdgcm693dmntkuw6lrftmltfwnzq7xd2dnd2z+ohh61tzjpxlsskynuhtrwkrrvoudju6nmna4l74tju5nfeelaieq94itlquyhskscubss78cur2guj6z9r1+tuxv3drjkviluoeczx/3ybwnlyq6qswpmz3ntdhkqutdjpxu/mzylbeyhvgepoje3qt7ppcvnvhmqknh2ksrz9fdgtmnjjnfoj2czzbi3e//zehlgn0euvjohv2xxkmokwytmciadotldobgemi1svsjgvfogtqaklcfb/viqav/uvau6+3bza9wwdzthbe7hhdy4hgbcqxnawccfz3ifnydzxpx352mxwnkknwp4a+fzbxnckbq=</latexit>h1<latexit sha1_base64="6givn2of65h4537xzaky9802sss=">aaab83icbvdlssnafl3xweur6tlnybfclaqouiy6cvnbpqapztkdtemnkzbzi5tq33djqhg3/ow7/8zjm4w2hhg4nhmv98wjeikmuu63s7a+sbm1xdop7+7thxxwjo7bjk414y0wy1h3a2q4fiq3ukdk3urzggwsd4ljxe53nrg2ilapoe14p6ijjulbkfrj9yok4ydmxrnbfvcpujv3drjkvijuoubzupnyhzfli66qswpmz3mt7gduo2csz8p+anhc2ysoem9srsnu+tk884ycw2viwljbp5dm1d8bgy2mmuabncwzmmuvf//zeimgn/1mqcrfrtjiujhkgjhjcybdotldobwemi1svslgvfogtqaylcfb/viqaddr3lxnfbisnm6lokpwcmdwar5cqwpuoqktyjdam7zcm5m6l86787eyxxoknrp4a+fzbxtgkbu=</latexit>h2<latexit sha1_base64="hw8sbjnj6+qg13c7kxvmkv6is38=">aaab83icbvdlssnafl2pr1pfvzdubovgqiq+0gxrjcsk1haauibtm3bozbjmjkij/q03lhrx68+482+ctflo64gbwzn3cs+cibfcg9f9dkorq2vrg+xnytb2zu5edf/gucepythisyhvj6aabzfymtwi7cqkarqibafj29xvp6hspjypzpjgl6jdyupoqlgs70fujiiwg0375/1qza27m5bl4hwkbgwa/eqxp4hzgqe0tfctu56bmf5glefm4ltipxotysz0if1ljy1q97jz5ik5scqahlgytxoyu39vzdtsehifdjlpqbe9xpzp66ymvo5lxcapqcnmh8juebotvaay4aqzernlkfpczivsrbvlxtzussv4i19ejo9nde+y7t5f1bo3rr1loijjoauprqabd9cefjbi4ble4c1jnrfn3fmyj5acyucq/sd5/aewspg2</latexit>h3<latexit sha1_base64="fd/cw9in/zggwy1f+v3kkfci0k4=">aaab83icbvdlssnafl2pr1pfvzdubovgqisi6lloxmuf+4cmlml00g6dtmlmjvbcf8onc0xc+jpu/bsnbrbaemdgcm693dmnskqw6lrftmltfwnzq7xd2dnd2z+ohh61tzxqxlsslrhubtrwkrrvoudju4nmnaok7wstu9zvphftrkweczrwfkrhsoscubss70cux0gyjwedy0g15tbdocgq8qpsgwlnqfxlh8ysjbhcjqkxpc9nsj9rjyjjpqv4qeejzrm64j1lfy246wfzzdnyzpuhcwntn0iyv39vzdqyzhofdjlpaja9xpzp66uy3vqzoziuuwklq2eqccykl4amheym5dqsyrswwqkbu00z2poqtgrv+curph1r967q7snlrxfb1fggezifc/dgghpwd01oaymenuev3pzuexheny/famkpdo7hd5zphxfokbc=</latexit>h4<latexit sha1_base64="y3vvkfy2hca6c4a/a1x7ideqrrg=">aaaca3icbvdlssnafj3uv62vqdvddbbbvule0wxrjcsk9gftdjpjpbk6myszivbcwi2/4safim79cxf+jzm0c209mhdmnhu59x4vyvqqy/o2akvlk6tr9fxgxubw9o65u9etcsow6ekyxwlgiuky5asrqgjkkaicio+rvje5lvz+axgsxvxotrpirgjmauaxulpyzynrhftobvmyu/s+/igocwnyc9dswi2rbfwkdkwaoelhnb9gfoztihcfgzjyafujcjikfmwm5i1rkkmc8asnyvbtjiiinay8iyfhwvfheav9uikl+rsjq5gu08jtlcwsct4rxp+8yaqcsyejpekv4xg2kegzvdesaoe+fqqrntueyuh1rhchsccsdgwnhyi9f/ii6z227powdxvwbf9vcdtbitgcj8agf6anbkahdaegj+azvii348l4md6nj1lpzah69sefgj8/hvwydg==</latexit>hheadi<latexit sha1_base64="ffhxmuujkdplnpus5pywejzl+1g=">aaacanicbvdlssnafj3uv62vqctxm1gevyurrzdfny4r2ae0muwmk3bozctmtiqsght/xy0lrdz6fe78gydpf9p6yodmofdy7z1bwqjsjvntvzawv1bxquu1jc2t7r17d6+j4lri0syxi2uvqiowkkhbu81il5ee8ycrbjc+lvzua5gkxujotxlictqunkiyasp59sgaiz0komyu+/s+/eieerhmvl13gk4juejcgamdgvq+/tuiy5xyijrmskm+6ytay5dufdos1wapigncyzqkfumf4kr5wxlcdo+nesioluyjduv1d0egufithpjkykc17xxif14/1dgll1grpjoipb0upqzqgbz5wjbkgjwbgikwpgzxiediiqxnajutgjt/8ilpndbc84zze1zvxs3iqijdcarogasuqbpcgbzoawwewtn4bw/wk/vivvsf09kknevzb39gff4ay+2yeq==</latexit>hendi<latexit sha1_base64="1fxtmfaam7+iwxcip43an4jn3ae=">aaacnxicbvdlssnafj3uv62vqes3wsiiqkle0wxrjysuktghtcfmppn26mwkzeyeepjtbvwpv7pwoyhbf8fjmow2vxdhcm693hoph1eilw2/gzwv1bx1jepmbwt7z3fp3d/oyjawchdqsepr96helhdcuurr3i8ehsynuodpb3o994ifjcf/uemexqbhnaqeqaupz2wngvqtbgnayrwcc5zirkssns2tzmkwc4rfmwfw7yzdlluinbluqvltz3wzjkium30qusjlwlej5azqkiiozmrdwoiioikc44gghdis3bt4ornondoygldo5soq2l8bkwrs2/x1zo5rzms5uuwbxcq4dlpco1hhjmahgphakrtyck0rergpmmgaksdaq4umueckdna1hyiz//ii6j43nmugfx9rb96ucvtbetggp8abv6aj7kabdaact+avfibp49l4n76m79loxsh3dsg/mn5+aze1rva=</latexit>lentry+lsym+limppergpephys、、、dataset.
#sents.
#ents(#types).
#rels(#types).
ace04ace05scierc.
8,68314,5252,687.
22,519(7)38,287(7)8,094(6).
4,417(6)7,691(6)5,463(7).
table 2: the statistics of the adopted datasets..speciﬁcally, we ﬂatten p ∈ r|s|×|s|×|y| as a ma-trix p row ∈ r|s|×(|s|·|y|) from row perspective, andthen calculate the euclidean distances (l2 distances)of adjacent rows.
similarly, we calculate the othereuclidean distances of adjacent columns accord-ing to a matrix p col ∈ r(|s|·|y|)×|s| from columnperspective, and then average the two distances asthe ﬁnal distance.
if the distance is larger than thethreshold α (α = 1.4 in our default settings), thisposition is a split position.
in this way, we candecode all the spans in o(|s|) time complexity..entity type decoding given a span (i, j) byspan decoding,8 we decode the entity type ˆt accord-ing to the corresponding square symmetric aboutthe diagonal: ˆt = arg maxt∈ye∪{⊥}avg(pi:j,i:j,t).
if ˆt ∈ ye, we decode an entity.
if ˆt = ⊥, the span(i, j) is not an entity..relation type decoding after entity type de-coding, given an entity e1 with the span (i, j)and another entity e2 with the span (m, n), wedecode the relation type ˆl between e1 and e2 ac-cording to the corresponding rectangle.
formally,ˆl = arg maxl∈yr∪{⊥}avg(pi:j,m:n,l).
if ˆl ∈ yr,we decode a relation (e1, e2, ˆl).
if ˆl = ⊥, e1 ande2 have no relation..4 experiments.
datasets we conduct experiments on three entityrelation extraction benchmarks: ace04 (dodding-ton et al., 2004),9 ace05 (walker et al., 2006),10and scierc (luan et al., 2018).11 table 2 showsthe dataset statistics.
besides, we provide detaileddataset speciﬁcations in the appendix b..evaluation following suggestions in (taill´eet al., 2020), we evaluate precision (p), recall (r),and f1 scores with micro-averaging and adopt thestrict evaluation criterion.
speciﬁcally, a pre-dicted entity is correct if its type and boundariesare correct, and a predicted relation is correct if its.
8i and j denote start and end indices of the span.
9https://catalog.ldc.upenn.edu/ldc2005t0910https://catalog.ldc.upenn.edu/ldc2006t0611http://nlp.cs.washington.edu/sciie/.
figure 3: overview of our joint decoding algorithm.
it consists of three steps: span decoding, entity typedecoding, and relation type decoding..• simple implementation and fast decoding.
we permit slight decoding accuracy drops forscalability..• strong interactions between entities and re-lations.
when decoding entities, it should takethe relation information into account, and viceversa..inspired by the procedures of (sun et al., 2019),we propose a three-steps decoding algorithm: de-code span ﬁrst (entity spans or spans between enti-ties), and then decode entity type of each span, andat last decode relation type of each entity pair (fig-ure 3).
we consider each cell’s probability scoreson all labels (including entity labels and relationlabels) and predict spans according to a threshold.
then, we predict entities and relations with thehighest score.
our heuristic decoding algorithmcould be very efﬁcient.
next we will detail the en-tire decoding process, and give a formal descriptionin the appendix a..span decoding one crucial observation of aground-truth table is that, for an arbitrary entity,its corresponding rows (or columns) are exactlythe same in the table (e.g., row 1 and row 2 of fig-ure 1 are identical), not only for the diagonal entries(entities are squares), but also for the off-diagonalentries (if it participates in a relation with anotherentity, all its rows (columns) will spot that relationlabel in the same way).
in other words, if the adja-cent rows/columns are different, there must be anentity boundary (i.e., one belonging to the entityand the other not belonging to the entity).
there-fore, if our biafﬁne model is reasonably trained,given a model predicted table, we could use thisproperty to ﬁnd split positions of entity boundary.
as expected, experiments (figure 4) verify our as-sumption.
we adapt this idea to the 3-dimensionalprobability tensor p..224、、、、、、、、、<latexit sha1_base64="j7hu8hi5rx5anpweenxyesuffv0=">aaacjxicbvdlssnafj3uv62vqes3g0vwvrjrdogi6mzlffuqjpbjdnionuzczeqoax7gjb/ixovfbff+itm0fg09mhdmnhu59x4vylqqy/oyckvlk6trxfxsxubw9o65u9eqyswwqeoqhalliuky5asuqgkkfqmcao+rpje4nvjnjyikdfm9gkbedvcpu59ipltums+dakk+riyppdchhgz/z0vu0sfkfdqkbktcgrnnyh/sudoxy1bfygaxiz2tmshr65hjpxviocbcyyakbntwpnwecuuxi2njiswjeb6ghmlrypge6sbzlsk80kox+qhqjyuyqb87ehrioqw8xtlzus57e/e/rx0r/8jnki9irtiedvjjbluij5hblhuekzbubgfb9a4q95fawolgszoee/7krdi4qdhnfev2tfy9yumoggnwci6bdc5bfdyagqgddj7bk3ghy+pfedm+jm9pachie/bbhxjfp01hpcu=</latexit>p2r4⇥4⇥|y|avgpergpe⊥pergpe⊥phys⊥⊥⊥⊥⊥123<latexit sha1_base64="k1rvh/9iaglnlha/fbfy7fwhwam=">aaaclnicbvdlsgmxfm3uv62vqks3wslutzmrii6liris0pd0piwtztrqtgzimkkzzhe58vd0iaiiwz/ddfpeww8edufcm3vvcungptlnvyoztlyyupzdz21sbm3v5hf3gjkibcz1hlbatfwkcaoc1bvvjlrcqzdvmtj0h5ctv3lphkqbr6lrsbwf9tn1kezks938vdh2krpgxojq0km58gp9b3lcqugbcphqrhvfjp24dg1ffsjhefztdpemk26+yjbmfhcrwdnsadnuu/lnuxfgycdcyyakbftmqjwycuuxi0nojiqjer6ipmlrypee6stpuqk80kopeohqjyuyqr87yurlofjdxtlzus57e/e/rx0p79yjkq8jrtiedviibluaj9nbhhuekzbsbgfb9a4qd5bawomeczoea/7krdi4kvmnjfomxkhczoliggnwciraamegaq5bfdqbbg/gcbybd+prede+jm9pacay9eydpzc+vgeyhknd</latexit>(pcol)t2r4⇥4|y|<latexit sha1_base64="s6kppofd66ell18l1won2w0oyfy=">aaacknicbvdltsjafj3ic/fvdelmijfxrvqd0sxqxiuaerhayhqyymj02sxmnat0e9z4k25yaihbp8shnebbk9zk5jx7c+89xsiovjy1nxirq2vrg/nnwtb2zu6eux9ql0ekmknhgawi6sfjgowkpqhipbkkgnypkyy3vjn5jscija34gxqfxpvrn9mexuhpqwneot5sa4xyxe3akrd+lilnbdquw1twvpg+acdl6cjqewnl45+zx2scdmyivbjswgviz6qimlq75stpbjjycveyislbthuqn0zcucxiunaisukeh6hpwppypje6cfpqak+00ow9qojicqbq74ky+vkofe93zo6ui95m/m9rrap36cauh5eihm8x9sigvqbnuceufqqrntieyuh1rrapkeby6xqlogr78evluj8r2ecl665crfxncetbetggp8agf6acbkev1aagl+anvimp49wygfpjc96am7kzq/ahxtc38s2oza==</latexit>prow2r4⇥4|y|𝑙!-dist𝑙!-dist𝑥!𝑥"𝑥#𝑥$𝑥!𝑥"𝑥#𝑥$𝑥!𝑥"𝑥#𝑥$dataset model.
encoder.
entityr.relationr.ace04.
ace05.
li and ji (2014)miwa and bansal (2016)katiyar and cardie (2017)li et al.
(2019)wang and lu (2020)zhong and chen (2020)(cid:5)zhong and chen (2020)(cid:5)unire(cid:5)unire(cid:5).
li and ji (2014)miwa and bansal (2016)katiyar and cardie (2017)sun et al.
(2019)li et al.
(2019)wang et al.
(2020)wang and lu (2020)zhong and chen (2020)(cid:5)zhong and chen (2020)(cid:5)unire(cid:5)unire(cid:5).
-lstmlstmbertlargealbertxxlargebertbasealbertxxlarge.
bertbasealbertxxlarge.
-lstmlstmlstmbertlargebertbasealbertxxlargebertbasealbertxxlarge.
bertbasealbertxxlarge.
scierc.
wang et al.
(2020)zhong and chen (2020)(cid:5)unire(cid:5).
scibertscibert.
scibert.
p.83.580.881.284.4---.
87.488.9.
85.282.984.086.184.7----.
88.889.9.
--.
76.282.978.182.9---.
88.090.0.
76.983.981.382.484.9----.
88.990.5.
--.
p.60.848.746.450.1---.
62.167.3.
65.457.255.568.164.8----.
67.172.3.
--.
36.148.145.348.7---.
58.059.3.
39.854.051.852.356.2----.
61.860.7.
--.
f1.
79.781.879.683.688.689.290.3.
87.789.5.
80.883.482.684.284.887.289.590.290.9.
88.890.2.
68.068.2.
68.4.f1.
45.348.445.749.459.660.162.2.
60.063.0.
49.555.653.659.160.263.264.364.667.8.
64.366.0.
34.636.7.
36.9.
65.8.
71.1.
37.3.
36.6.table 3: overall evaluation.
(cid:5) means that the model leverages cross-sentence context information..relation type is correct, as well as the boundariesand types of two argument entities are correct..implementation details we tune all hyper-parameters based on the averaged entity f1and relation f1 on ace05 development set,then keep the same settings on ace04 andscierc.
forfair comparison with previousworks, we use three pre-trained language mod-bert-base-uncased (devlin et al.,els:2019), albert-xxlarge-v1 (lan et al., 2019)and scibert-scivocab-uncased (beltagyet al., 2019) as the sentence encoder and ﬁne-tunethem in training stage.12.
for the mlp layer, we set the hidden size asd = 150 and use gelu as the activation function.
we use adamw optimizer (loshchilov and hutter,2017) with β1 = 0.9 and β2 = 0.9, and observe aphenomenon similar to (dozat and manning, 2016)in that setting β2 from 0.9 to 0.999 causes a sig-niﬁcant drop on ﬁnal performance.
the batch sizeis 32, and the learning rate is 5e-5 with weight de-cay 1e-5.
we apply a linear warm-up learning ratescheduler with a warm-up ratio of 0.2. we train ourmodel with a maximum of 200 epochs (300 epochsfor scierc) and employ an early stop strategy.
we.
12the ﬁrst two are for ace04 and ace05, and the last one.
is for scierc..perform all experiments on an intel(r) xeon(r)w-3175x cpu and a nvidia quadro rtx 8000gpu..4.1 performance comparison.
table 3 summarizes previous works and ourunire on three datasets.13 in general, unireachieves the best performance on ace04 and sci-erc and a comparable result on ace05.
com-paring with the previous best joint model (wangand lu, 2020), our model signiﬁcantly advancesboth entity and relation performances, i.e., an abso-lute f1 of +0.9 and +0.7 for entity as well as +3.4and +1.7 for relation, on ace04 and ace05 re-spectively.
for the best pipeline model (zhong andchen, 2020) (current sota), our model achievessuperior performance on ace04 and scierc andcomparable performance on ace05.
comparingwith ace04/ace05, scierc is much smaller, soentity performance on scierc drops sharply.
since(zhong and chen, 2020) is a pipeline method,its relation performance is severely inﬂuencedby the poor entity performance.
nevertheless,our model is less inﬂuenced in this case and.
13since (luan et al., 2019a; wadden et al., 2019) neglect theargument entity type in relation evaluation and underperformour baseline (zhang et al., 2020), we do not compare theirresults here..225ace05.
scierc.
model.
parameters w.settings.
default.
w/o symmetry lossw/o implication lossw/o logit dropoutw/o cross-sentence context.
hard decoding.
ent.
88.8.
88.989.088.887.9.
74.0.rel.
64.3.
64.063.361.862.7.
34.6.ent.
68.4.
67.368.066.965.3.
46.1.rel.
36.9.
35.537.134.732.1.
17.8.table 4: results (f1 score) with different settingson ace05 and scierc test sets.
note that we usebertbase on ace05..achieves better performance.
besides, our modelcan achieve better relation performance even withworse entity results on ace04.
actually, our basemodel (bertbase) has achieved competitive rela-tion performance, which even exceeds prior mod-els based on bertlarge (li et al., 2019) andalbertxxlarge (wang and lu, 2020).
theseresults conﬁrm the proposed uniﬁed label spaceis effective for exploring the interaction betweenentities and relations.
note that all subsequent ex-periment results on ace04 and ace05 are basedon bertbase for efﬁciency..4.2 ablation study.
in this section, we analyze the effects of compo-nents in unire with different settings (table 4).
particularly, we implement a naive decoding al-gorithm for comparison, namely “hard decoding”,which takes the “intermediate table” as input.
the“intermediate table” is the hard form of probabilitytensor p output by the biafﬁne model, i.e., choos-ing the class with the highest probability as thelabel of each cell.
to ﬁnd entity squares on thediagonal, it ﬁrst tries to judge whether the largestsquare (|s| × |s|) is an entity.
the criterion is sim-ply counting the number of different entity labelsappearing in the square and choosing the most fre-if the most frequent label is ⊥, wequent one.
shrink the size of square by 1 and do the samework on two (|s| − 1) × (|s| − 1) squares and soon.
to avoid entity overlapping, an entity will bediscarded if it overlaps with identiﬁed entities.
toﬁnd relations, each entity pair is labeled by themost frequent relation label in the correspondingrectangle..from the ablation study, we get the following.
observations..• when one of the additional losses is removed,the performance will decline with varying de-.
z&c(2020)z&c(2020)†.
unireunire.
hard decoding.
219m219m.
110m110m.
110m.
100100.
100200.
200.ace05.
scierc.
rel(f1).
64.6-.
63.664.3.
34.6.speed(sent/s).
14.7237.6.
340.6194.2.
139.1.rel(f1).
36.7-.
34.036.9.
17.8.speed(sent/s).
19.9194.7.
314.8200.1.
113.0.table 5: comparison of accuracy and efﬁciency onace05 and scierc test sets with different context win-dow sizes.
† denotes the approximation version with afaster speed and a worse performance..grees (line 2-3).
speciﬁcally, the symmetricalloss has a signiﬁcant impact on scierc (de-crease 1.1 points and 1.4 points for entity andrelation performance).
while removing the im-plication loss will obviously harm the relationperformance on ace05 (1.0 point).
it demon-strates that the structural information incorpo-rated by both losses is useful for this task..• comparing with the “default”, the perfor-mance of “w/o logit dropout” and “w/o cross-sentence context” drop more sharply (line 4-5).
logit dropout prevents the model from overﬁt-ting, and cross-sentence context provides morecontextual information for this task, especiallyfor small datasets like scierc..• the “hard decoding” has the worst perfor-mance (its relation performance is almost halfof the “default”) (line 6).
the major reason isthat “hard decoding” separately decodes entitiesand relations.
it shows the proposed decodingalgorithm jointly considers entities and relations,which is important for decoding..4.3.inference speed.
following (zhong and chen, 2020), we evalu-ate the inference speed of our model (table 5)on ace05 and scierc with the same batch sizeand pre-trained encoders (bertbase for ace05and scibert for scierc).
comparing with thepipeline method (zhong and chen, 2020), we ob-tain a more than 10× speedup and achieve a com-parable or even better relation performance withw = 200. as for their approximate version, ourinference speed is still competitive but with betterperformance.
if the context window size is set thesame as (zhong and chen, 2020) (w = 100), wecan further accelerate model inference with slightperformance drops.
besides, “hard decoding” is.
226figure 4: distributions of adjacent rows’ distances fortwo categories with respect to the threshold α on ace05dev set..value.
ace05.
scierc.
w.p.100200300.
0.10.20.30.4.ent.
87.487.987.2.
87.487.987.287.4.rel.
62.462.160.8.
61.862.162.162.0.ent.
69.070.669.4.
71.170.667.870.6.rel.
36.738.335.4.
37.838.333.535.8.table 6: results (f1 scores) with respect to the contextwindow size and the logit dropout rate on ace05 andscierc dev sets..4.5 context window and logit dropout rate.
in table 4, both cross-sentence context and logitdropout can improve the entity and relation perfor-mance.
table 6 shows the effect of different con-text window size w and logit dropout rate p. theentity and relation performances are signiﬁcantlyimproved from w = 100 to w = 200, and dropsharply from w = 200 to w = 300. similarly, weachieve the best entity and relation performanceswhen p = 0.2. so we use w = 200 and p = 0.2in our ﬁnal model..we further analyze the remaining errors for relationextraction and present the distribution of ﬁve errors:span splitting error (sse), entity not found (enf),entity type error (ete), relation not found (rnf),and relation type error (rte) in figure 6. the pro-portion of “sse” is relatively small, which provesthe effectiveness of our span decoding method.
moreover, the proportion of “not found error” issigniﬁcantly larger than that of “type error” forboth entity and relation.
the primary reason is thatthe table ﬁlling suffers from the class imbalanceissue, i.e., the number of ⊥ is much larger thanthat of other classes.
we reserve this imbalancedclassiﬁcation problem in the future..finally, we give some concrete examples in fig-ure 7 to verify the robustness of our decoding algo-rithm.
there are some errors in the biafﬁne model’sprediction, such as cells in the upper left corner(ﬁrst example) and upper right corner (second ex-ample) in the intermediate table.
however, theseerrors are corrected after decoding, which demon-strates that our decoding algorithm not only recoverall entities and relations but also corrects errorsleveraging table structure and neighbor cells’ infor-mation..figure 5: performances with respect to the threshold αon ace05 dev set..4.6 error analysis.
much slower than unire, which demonstrates theefﬁciency of the proposed decoding algorithm..4.4.impact of different threshold α.in figure 4, the distance between adjacent rowsnot at entity boundary (“non-ent-bound”) mainlyconcentrates at 0, while that at entity boundary(“ent-bound”) is usually greater than 1. this phe-nomenon veriﬁes the correctness of our span decod-ing method.
then we evaluate the performances,with regard to the threshold α in figure 5.14 bothspan and entity performances sharply decreasewhen α increases from 1.4 to 1.5, while the re-lation performance starts to decline slowly fromα = 1.5. the major reason is that relations are sosparse that many entities do not participate in anyrelation, so the threshold of relation is much higherthan that of entity.
moreover, we observe a similarphenomenon on ace04 and scierc, and α = 1.4is a general best setting on three datasets.
it showsthe stability and generalization of our model..14we use an additional metric to evaluate span performance,.
“span f1”, is micro-f1 of predicted split positions..2270.00.51.01.52.02.53.0euclidean distance02468densitytypesent-boundnon-ent-bound0.80.91.01.11.21.31.41.51.61.7thresholdα485358636873788388f1scorespanf1entityf1relationf1figure 6: distribution of ﬁve relation extraction errorson ace05 and scierc test data..5 related work.
entity relation extraction has been extensively stud-ied over the decades.
existing methods can beroughly divided into two categories according tothe adopted label space..separate label spaces this category study thistask as two separate sub-tasks: entity recognitionand relation classiﬁcation, which are deﬁned in twoseparate label spaces.
one early paradigm is thepipeline method (zelenko et al., 2003; miwa et al.,2009) that uses two independent models for twosub-tasks respectively.
then joint method handlesthis task with an end-to-end model to explore moreinteraction between entities and relations.
the mostbasic joint paradigm, parameter sharing (miwa andbansal, 2016; katiyar and cardie, 2017), adoptstwo independent decoders based on a shared en-coder.
recent span-based models (luan et al.,2019b; wadden et al., 2019) also use this paradigm.
to enhance the connection of two decoders, manyjoint decoding algorithms are proposed, such asilp-based joint decoder (yang and cardie, 2013),joint mrt (sun et al., 2018), gcn-based joint in-ference (sun et al., 2019).
actually, table ﬁllingmethod (miwa and sasaki, 2014; gupta et al., 2016;zhang et al., 2017; wang et al., 2020) is a specialcase of parameter sharing in table structure.
thesejoint models all focus on various joint algorithmsbut ignore the fact that they are essentially basedon separate label spaces..uniﬁed label space this family of methodsaims to unify two sub-tasks and tackle this taskin a uniﬁed label space.
entity relation extractionhas been converted into a tagging problem (zhenget al., 2017), a transition-based parsing problem(wang et al., 2018), and a generation problem with.
figure 7: examples showing the robustness of our de-coding algorithm.
“gold table” presents the gold label.
“intermediate table” presents the biafﬁne model’s pre-diction (choosing the label with the highest probabilityfor each cell).
“decoded table” presents the ﬁnal resultsafter decoding..seq2seq framework (zeng et al., 2018; nayak andng, 2020).
we follow this trend and propose a newuniﬁed label space.
we introduce a 2d table totackle the overlapping relation problem in (zhenget al., 2017).
also, our model is more versatile asnot relying on complex expertise like (wang et al.,2018), which requires external expert knowledgeto design a complex transition system..6 conclusion.
in this work, we extract entities and relations in auniﬁed label space to better mine the interactionbetween both sub-tasks.
we propose a novel ta-ble that presents entities and relations as squaresand rectangles.
then this task can be performedin two simple steps: ﬁlling the table with our bi-afﬁne model and decoding entities and relationswith our joint decoding algorithm.
experimentson three benchmarks show the proposed methodachieves not only state-of-the-art performance butalso promising efﬁciency..acknowledgement.
the authors wish to thank the reviewers fortheir helpful comments and suggestions.
thiswork was (partially) supported by nationalkey research and development program ofchina (2018aaa0100704), nsfc (61972250,62076097), stcsm (18zr1411500), shanghaimunicipal science and technology major project(2021shzdzx0102), and the fundamental re-search funds for the central universities..228sseenfeternfrte010203040500.0%20.9%17.5%54.1%7.5%4.9%34.5%28.1%28.8%3.7%ace05sciercgold tableintermediate tabledecoded table、、wingswereoffofit903americanairlinesflightvehorgvehart、、、、references.
iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
arxivpreprint arxiv:1903.10676..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186, minneapolis, minnesota.
association forcomputational linguistics..george doddington, alexis mitchell, mark przybocki,lance ramshaw, stephanie strassel, and ralphweischedel.
2004. the automatic content extrac-tion (ace) program – tasks, data, and evaluation.
inproceedings of the fourth international conferenceon language resources and evaluation (lrec’04),lisbon, portugal.
european language resources as-sociation (elra)..timothy dozat and christopher d manning.
2016.deep biafﬁne attention for neural dependency pars-ing.
arxiv preprint arxiv:1611.01734..pankaj gupta, hinrich sch¨utze, and bernt andrassy.
2016. table ﬁlling multi-task recurrent neural net-work for joint entity and relation extraction.
in pro-ceedings of coling 2016, the 26th internationalconference on computational linguistics: technicalpapers, pages 2537–2547, osaka, japan.
the col-ing 2016 organizing committee..arzoo katiyar and claire cardie.
2017. going out on alimb: joint extraction of entity mentions and relationswithout dependency trees.
in proceedings of the 55thannual meeting of the association for computationallinguistics (volume 1: long papers), pages 917–928,vancouver, canada.
association for computationallinguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..qi li and heng ji.
2014. incremental joint extractionin proceedingsof entity mentions and relations.
of the 52nd annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 402–412, baltimore, maryland.
associationfor computational linguistics..xiaoya li, fan yin, zijun sun, xiayu li, arianna yuan,duo chai, mingxin zhou, and jiwei li.
2019. entity-relation extraction as multi-turn question answering.
in proceedings of the 57th annual meeting of the as-sociation for computational linguistics, pages 1340–1350, florence, italy.
association for computationallinguistics..ilya loshchilov and frank hutter.
2017. decou-pled weight decay regularization.
arxiv preprintarxiv:1711.05101..yi luan, luheng he, mari ostendorf, and hannanehhajishirzi.
2018. multi-task identiﬁcation of entities,relations, and coreference for scientiﬁc knowledgegraph construction.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, pages 3219–3232, brussels, belgium.
association for computational linguistics..yi luan, dave wadden, luheng he, amy shah,mari ostendorf, and hannaneh hajishirzi.
2019a.
a general framework for information extractionarxiv preprintusing dynamic span graphs.
arxiv:1904.03296..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019b.
a gen-eral framework for information extraction using dy-in proceedings of the 2019namic span graphs.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3036–3046, minneapolis, minnesota.
association for computational linguistics..makoto miwa and mohit bansal.
2016. end-to-end re-lation extraction using lstms on sequences and treestructures.
in proceedings of the 54th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1105–1116, berlin,germany.
association for computational linguistics..makoto miwa, rune sætre, yusuke miyao, and jun’ichitsujii.
2009. a rich feature vector for protein-proteininteraction extraction from multiple corpora.
in pro-ceedings of the 2009 conference on empirical meth-ods in natural language processing, pages 121–130,singapore.
association for computational linguis-tics..makoto miwa and yutaka sasaki.
2014. modeling jointentity and relation extraction with table represen-tation.
in proceedings of the 2014 conference onempirical methods in natural language processing(emnlp), pages 1858–1869..tapas nayak and hwee tou ng.
2020. effective mod-eling of encoder-decoder architecture for joint entityand relation extraction.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 34,pages 8528–8535..changzhi sun, yeyun gong, yuanbin wu, ming gong,daxin jiang, man lan, shiliang sun, and nan duan.
2019. joint type inference on entities and relationsvia graph convolutional networks.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 1361–1370, florence,italy.
association for computational linguistics..changzhi sun, yuanbin wu, man lan, shiliang sun,wenting wang, kuang-chih lee, and kewen wu..2292018. extracting entities and relations with jointminimum risk training.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 2256–2265, brussels, bel-gium.
association for computational linguistics..haoran zhang, qianying liu, aysa xuemo fan, hengji, daojian zeng, fei cheng, daisuke kawahara, andsadao kurohashi.
2020. minimize exposure bias ofseq2seq models in joint entity and relation extraction.
arxiv preprint arxiv:2009.07503..meishan zhang, yue zhang, and guohong fu.
2017.end-to-end neural relation extraction with global op-timization.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 1730–1740, copenhagen, denmark.
associa-tion for computational linguistics..suncong zheng, feng wang, hongyun bao, yuexinghao, peng zhou, and bo xu.
2017. joint extractionof entities and relations based on a novel taggingscheme.
arxiv preprint arxiv:1706.05075..zexuan zhong and danqi chen.
2020. a frustratinglyeasy approach for joint entity and relation extraction.
arxiv preprint arxiv:2010.12812..bruno taill´e, vincent guigue, geoffrey scoutheeten,and patrick gallinari.
2020. let’s stop incorrectcomparisons in end-to-end relation extraction!
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3689–3701, online.
association for computa-tional linguistics..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5784–5789, hong kong, china.
association for computa-tional linguistics..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilin-gual training corpus.
linguistic data consortium,philadelphia, 57:45..jue wang and wei lu.
2020. two are better thanone: joint entity and relation extraction with table-sequence encoders.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1706–1721, online.
as-sociation for computational linguistics..shaolei wang, yue zhang, wanxiang che, and tingliu.
2018. joint extraction of entities and relationsin ijcai, pagesbased on a novel graph scheme.
4461–4467..yijun wang, changzhi sun, yuanbin wu, junchi yan,peng gao, and guotong xie.
2020. pre-training en-tity relation encoder with intra-span and inter-spaninformation.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 1692–1705, online.
associationfor computational linguistics..bishan yang and claire cardie.
2013. joint inferencefor ﬁne-grained opinion extraction.
in proceedingsof the 51st annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1640–1649..dmitry zelenko, chinatsu aone,.
and anthonyrichardella.
2003. kernel methods for relationextraction.
journal of machine learning research,3(feb):1083–1106..xiangrong zeng, daojian zeng, shizhu he, kang liu,and jun zhao.
2018. extracting relational facts byan end-to-end neural model with copy mechanism.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 506–514..230moreover, we correct the annotations of undi-rected relations for three datasets, regarding eachundirected relation as two directed relation in-stances, e.g., for the undirected relation per-soc,only one relation triplet (“his”, wife”, per-soc)is annotated in the original dataset, we will addanother relation triplet (“wife”, “his”, per-soc)in our corrected datasets for symmetry.
in this case,each undirected relation corresponds to two rectan-gles, which are symmetrical about the diagonal..algorithm 1 decoding algorithm.
input: probability tensor p ∈ r|s|×|s|×|y| of sentence soutput: a set of entities e and a set of relations r1: esplit = [], e = set(), r = set()2: p row ← p.view(n, n ∗ |y|)3: p col ← p.transpose(0, 1).view(n, n ∗ |y|)4: i ← 15: while i<|s| do||p row.
i −p row.
i+1 ||2.
i −p col.i+1||22.
2+||p col2.esplit.append(i).
end ifi ← i + 1.d ←if d>α then.
ˆt = arg maxt∈ye∪{⊥}avg(pi:j,i:j,t)if ˆt (cid:54)= ⊥ then.
6:7:8:9:10:11: end while12: esplit.append(|s|)13: i ← 114: for j ∈ esplit do15:16:17:18:19:20:21: end for22: for e1, e2 ∈ e, e1 (cid:54)= e2 do(i, j) = e1.span23:(m, n) = e2.span24:ˆl = arg maxl∈yr ∪{⊥}avg(pi:j,m:n,l)25:if ˆl (cid:54)= ⊥ then26:27:28:29: end for.
r.add((e1, e2, ˆl)).
end ifi ← j + 1.end if.
new e: e.span = (i, j) and e.type = ˆte.add(e).
a decoding algorithm.
a formal description are shown in algorithm 1..b datasets.
the ace04 and ace05 corpora are collected fromvarious domains, such as newswire and online fo-rums.
both corpora annotate 7 entity types and 6relation types.
we use the same data splits and pre-processing as (li and ji, 2014; miwa and bansal,2016), i.e., 5-fold cross-validation for ace04, and351 training, 80 validating, and 80 testing forace05.15 besides, we randomly sample 10% oftraining set as the development set for ace04..the scierc corpus collects 500 scientiﬁc ab-stracts taken from ai conference/workshop pro-ceedings.
this dataset annotates 6 entity typesand 7 relation types.
we adopt the same data splitprotocol as in (luan et al., 2019b) (350 training,50 validating, and 100 testing).
detailed datasetspeciﬁcations are shown in table 2..15we use the pre-processing scripts provided by(wang and lu, 2020) at https://github.com/lorrinwww/two-are-better-than-one/tree/master/datasets..231