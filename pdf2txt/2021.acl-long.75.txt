compositional generalization and natural language variation:can a semantic parsing approach handle both?.
peter shaw ming-wei chang.
panupong pasupat kristina toutanova.
google research{petershaw,mingweichang,ppasupat,kristout}@google.com.
abstract.
predominant approaches.
sequence-to-sequence models excel at han-dling natural language variation, but have beenshown to struggle with out-of-distributioncompositional generalization.
this has mo-tivated new specialized architectures withstronger compositional biases, but most ofthese approaches have only been evaluated onsynthetically-generated datasets, which are notrepresentative of natural language variation.
inthis work we ask: can we develop a semanticparsing approach that handles both natural lan-guage variation and compositional generaliza-tion?
to better assess this capability, we pro-pose new train and test splits of non-syntheticdatasets.
we demonstrate that strong exist-ing approaches do not perform well across abroad set of evaluations.
we also proposenqg-t5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model.
it outper-forms existing approaches across several com-positional generalization challenges on non-synthetic data, while also being competitivewith the state-of-the-art on standard evalua-tions.
while still far from solving this problem,our study highlights the importance of diverseevaluations and the open challenge of handlingboth compositional generalization and naturallanguage variation in semantic parsing..1.introduction.
sequence-to-sequence (seq2seq) models have beenwidely used in semantic parsing (dong and lap-ata, 2016; jia and liang, 2016) and excel at han-dling the natural language variation1 of human-generated queries.
however, evaluations on syn-thetic2 tasks such as scan (lake and baroni,.
1we use the term natural language variation in a broadsense to refer to the many different ways humans can expressthe same meaning in natural language, including differencesin word choice and syntactic constructions..2we make a coarse distinction between synthetic datasets,where natural language utterances are generated by a program,.
specializedarchitectureswith strongcompositional bias.
lano.itisopmoc.no.itazilareneg.under-explored.
general-purposepre-trained models(e.g.
seq2seq).
synthetic.
non-synthetic.
natural language variation.
figure 1: we study whether a semantic parsing ap-proach can handle both out-of-distribution composi-tional generalization and natural language variation.
existing approaches are commonly evaluated acrossonly one dimension..2018) have shown that seq2seq models often gener-alize poorly to out-of-distribution compositional ut-terances, such as “jump twice” when only “jump”,“walk”, and “walk twice” are seen during training.
this ability to generalize to novel combinations ofthe elements observed during training is referred toas compositional generalization..this has motivated many specialized architec-tures that improve peformance on scan (li et al.,2019; russin et al., 2019; gordon et al., 2019; lake,2019; liu et al., 2020; nye et al., 2020; chen et al.,2020).
however, most approaches have only beenevaluated on synthetic datasets.
while syntheticdatasets enable precise, interpretable evaluation ofspeciﬁc phenomena, they are less representativeof the natural language variation that a real-worldsemantic parsing system must handle..in this paper, we ask: can we develop a semanticparsing approach that handles both natural lan-guage variation and compositional generalization?.
and non-synthetic datasets, where natural language utterancesare collected from humans..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages922–938august1–6,2021.©2021associationforcomputationallinguistics922surprisingly, this question is understudied.
as visu-alized in figure 1, most prior work evaluates eitherout-of-distribution compositional generalization onsynthetic datasets, or in-distribution performanceon non-synthetic datasets.
notably, designing ap-proaches that can handle both compositional gen-eralization and the natural language variation ofnon-synthetic datasets is difﬁcult.
for example,large pre-trained seq2seq models that perform wellon in-distribution evaluations do not address mostof the compositional generalization challenges pro-posed in scan (furrer et al., 2020)..our research question has two important motiva-tions.
first, humans have been shown to be adeptcompositional learners (lake et al., 2019).
severalauthors have argued that a greater focus on com-positional generalization is an important path tomore human-like generalization and nlu (lakeet al., 2017; battaglia et al., 2018).
second, itis practically important to assess performance onnon-synthetic data and out-of-distribution exam-ples, as random train and test splits can overesti-mate real-world performance and miss importanterror cases (ribeiro et al., 2020).
therefore, we areinterested in approaches that do well not only oncontrolled synthetic challenges of compositionalityor in-distribution natural utterances, but across allof the diverse set of evaluations shown in figure 2..our contributions are two-fold.
first, on theevaluation front, we show that performance onscan is not well-correlated with performanceon non-synthetic tasks.
in addition, strong existingapproaches do not perform well across all evalu-ations in figure 2. we also propose new targetmaximum compound divergence (tmcd) trainand test splits, extending the methodology of key-sers et al.
(2020) to create challenging evaluationsof compositional generalization for non-syntheticdatasets.
we show that tmcd splits complementexisting evaluations by focusing on different as-pects of the problem..second, on the modeling front, we propose nqg,a simple and general grammar-based approach thatsolves scan and also scales to natural utterances,obtaining high precision for non-synthetic data.
inaddition, we introduce and evaluate nqg-t5, ahybrid model that combines nqg with t5 (raf-fel et al., 2020), leading to improvements acrossseveral compositional generalization evaluationswhile also being competitive on the standard splitsof geoquery (zelle and mooney, 1996) and spi-.
lano.itisopmoc.no.itazilareneg.train and test splits.
mcd(keysers et al., 2020).
tmcd.
add primitive(lake and baroni, 2018).
template(finegan-dollak et al., 2018).
length.
length.
random.
synthetic.
non-synthetic.
natural language variation.
figure 2: we evaluate semantic parsing approachesacross a diverse set of evaluations focused on natu-ral language variation, compositional generalization, orboth.
we add tmcd splits to complement existingevaluations.
ordering within each cell is arbitrary..der (yu et al., 2018).
our results indicate thatnqg-t5 is a strong baseline for our challenge ofdeveloping approaches that perform well across adiverse set of evaluations focusing on either naturallanguage variation, compositional generalization,or both.
comparing ﬁve approaches across eightevaluations on scan and geoquery, its aver-age rank is 1, with the rank of the best previousapproach (t5) being 2.9; performance is also com-petitive across several evaluations on spider..while still far from afﬁrmatively answering ourresearch question, our study highlights the impor-tance of a diverse set of evaluations and the openchallenge of handling both compositional general-ization and natural language variation.3.
2 background and related work.
in this section, we survey recent work related tocompositional generalization in semantic parsing..evaluations to evaluate a model’s ability to gen-eralize to novel compositions, previous work hasproposed several methods for generating train andtest splits, as well as several synthetic datasets..a widely used synthetic dataset for assessingcompositional generalization is scan (lake andbaroni, 2018), which consists of natural languagecommands (e.g., “jump twice”) mapping to actionsequences (e.g., “i_jump i_jump”).
one split forscan is the length split, where examples are sepa-rated by length such that the test set contains longer.
3our.
code.
and.
data.
splits.
are.
available.
at.
https://github.com/google-research/language/tree/master/language/nqg..923examples than the training set.
another is the prim-itive split, where a given primitive (e.g., “jump”)is seen by itself during training, but the test setconsists of the primitive recombined with otherelements observed during training (e.g., “jumptwice”).
other synthetic datasets have been de-veloped to evaluate aspects of compositional gen-eralization beyond scan, including nacs (bast-ings et al., 2018), cfq (keysers et al., 2020), andcogs (kim and linzen, 2020)..in addition to introducing the cfq dataset, key-sers et al.
(2020) propose maximum compounddivergence (mcd) splits based on the notion of acompound distribution.
their algorithm generatestrain and test splits that maximize the divergenceof their respective compound distributions whilebounding the divergence of their respective atomdistributions.
we extend their methodology to cre-ate new tmcd splits for non-synthetic datasets..another method for generating train and testsplits is the template4 split (finegan-dollak et al.,2018).
unlike the aforementioned evaluations,template splits have been applied to non-syntheticdatasets, primarily for text-to-sql.
in templatesplits, any parse template (deﬁned as the targetsql query with entities anonymized) appearing inthe training set cannot appear in the test set.
weanalyze and discuss template splits in § 6.1..finally, herzig and berant (2019) studies biasesresulting from methods for efﬁciently collectinghuman-labeled data, providing further motivationfor out-of-distribution evaluations..approaches many specialized architectureshave been developed to address the compositionalgeneralization challenges of scan.
several ofthem have recently reached 100% accuracy acrossmultiple scan challenges (liu et al., 2020; nyeet al., 2020; chen et al., 2020).
similarly to thenqg-t5 approach we propose in § 4, all of thesemodels incorporate discrete structure.
however,unlike nqg-t5, they have only been evaluated onsynthetic parsing tasks..recently, herzig and berant (2020) also beginsto address our research question, proposing an ap-proach that not only solves several scan chal-lenges but also achieves strong performance on thestandard and template splits of the non-syntheticdataset geoquery.
however, their approach re-quires some manual task-speciﬁc engineering.
wecompare nqg-t5 with this approach and other.
scan-inspired architectures.
oren et al.
(2020)and zheng and lapata (2020) also explored compo-sitional generalization on non-synthetic datasetsby focusing on the template splits proposed byfinegan-dollak et al.
(2018), demonstrating im-provements over standard seq2seq models..the effect of large-scale pre-training on compo-sitional generalization ability has also been studied.
furrer et al.
(2020) ﬁnds that pre-training alone can-not solve several compositional generalization chal-lenges, despite its effectiveness across nlp taskssuch as question answering (raffel et al., 2020)..while our work focuses on modeling approaches,compositional data augmentation techniques havealso been proposed (jia and liang, 2016; andreas,2020).
nqg-t5 outperforms previously reportedresults for these methods, but more in-depth analy-sis is needed..3 target maximum compounddivergence (tmcd) splits.
the existing evaluations targeting compositionalgeneralization for non-synthetic tasks are templatesplits and length splits.
here we propose an addi-tional method which expands the set of availableevaluations by generating data splits that maximizecompound divergence over non-synthetic datasets,termed target maximum compound divergence(tmcd) splits.
as we show in § 6, it results in ageneralization problem with different characteris-tics that can be much more challenging than tem-plate splits, and contributes to the comprehensive-ness of evaluation..in standard mcd splits (keysers et al., 2020),the notion of compounds requires that both sourceand target are generated by a rule-based proce-dure, and therefore cannot be applied to existingnon-synthetic datasets where natural language ut-terances are collected from humans.
for tmcd,we propose a new notion of compounds based onlyon the target representations.
we leverage theirknown syntactic structure to deﬁne atoms and com-pounds.
for instance, example atoms in funql arelongest and river, and an example compoundis longest(river).
detailed deﬁnitions of atomsand compounds for each dataset we study can befound in appendix b.3..given this deﬁnition of compounds, our deﬁni-tion of compound divergence, dc, is the same asthat of keysers et al.
(2020).
speciﬁcally,.
4also referred to as a query split..dc = 1 − c0.1(ftrain (cid:107) ftest),.
924where ftrain and ftest are the weighted frequencydistributions of compounds in the training andtest sets, respectively.
the chernoff coefﬁcientcα(p (cid:107)q) = (cid:80)k pα(chung et al., 1989) isused with α = 0.1..k q1−αk.for tmcd, we constrain atom divergence by re-quiring that every atom appear at least once in thetraining set.
an atom constraint is desirable so thatthe model knows the possible target atoms to gener-ate.
a greedy algorithm similar to the one of key-sers et al.
(2020) is used to generate splits that ap-proximately maximize compound divergence.
first,we randomly split the dataset.
then, we swap ex-amples until the atom constraint is satisﬁed.
finally,we sequentially identify example pairs that can beswapped between the train and test sets to increasecompound divergence without violating the atomconstraint, breaking when a swap can no longer beidentiﬁed..4 proposed approach: nqg-t5.
we propose nqg-t5, a hybrid semantic parserthat combines a grammar-based approach with aseq2seq model.
the two components are motivatedby prior work focusing on compositional general-ization and natural language variation, respectively,and we show in § 5 that their combination sets astrong baseline for our challenge..the grammar-based component, nqg, consistsof a discriminative neural parsing model and aﬂexible quasi-synchronous grammar inductionalgorithm which can operate over arbitrary pairsof strings.
like other grammar-based approaches,nqg can fail to produce an output for certain in-puts.
as visualized in figure 3, in cases wherenqg fails to produce an output, we return the out-put from t5 (raffel et al., 2020), a pre-trainedseq2seq model.
this simple combination can workwell because nqg often has higher precision thant5 for cases where it produces an output, especiallyin out-of-distribution settings..we train nqg and t5 separately.
training datafor both components consists of pairs of source andtarget strings, referred to as x and y, respectively..4.1 nqg component.
nqg is inspired by more traditional approachesto semantic parsing based on grammar formalismssuch as ccg (zettlemoyer and collins, 2005, 2007;kwiatkowski et al., 2010, 2013) and scfg (wongand mooney, 2006, 2007; andreas et al., 2013; li.
start.
runnqg.
nqg hasoutput?.
no.
runt5.
yes.
return t5 output.
return nqg output.
figure 3: overview of how predictions are generatedby nqg-t5, a simple yet effective combination oft5 (raffel et al., 2020) with a high-precision grammar-based approach, nqg..et al., 2015).
nqg combines a qcfg induction al-gorithm with a neural parsing model.
training is atwo-stage process.
first, we employ a compression-based grammar induction technique to constructour grammar.
second, based on the induced gram-mar, we build the nqg semantic parsing modelvia a discriminative latent variable model, usinga powerful neural encoder to score grammar ruleapplications anchored in the source string x..4.1.1 nqg grammar inductiongrammar formalism synchronous context-free grammars (scfgs) synchronously generatestrings in both a source and target language.
com-pared to related work based on scfgs for machinetranslation (chiang, 2007) and semantic parsing,nqg uses a slightly more general grammar formal-ism that allows repetition of a non-terminal with thesame index on the target side.
therefore, we adoptthe terminology of quasi-synchronous context-freegrammars (smith and eisner, 2006), or qcfgs,to refer to our induced grammar g.5 our gram-mar g contains a single non-terminal symbol, n t .
we restrict source rules to ones containing at most2 non-terminal symbols, and do not allow unaryproductions as source rules.
this enables efﬁcientparsing using an algorithm similar to cky (cocke,1969; kasami, 1965; younger, 1967) that does notrequire binarization of the grammar..induction procedure to induce g from thetraining data, we propose a qcfg induction algo-rithm that does not rely on task-speciﬁc heuristicsor pre-computed word alignments.
notably, ourapproach makes no explicit assumptions about thesource or target languages, beyond those implicitin the qcfg formalism.
table 1 shows examplesof induced rules..our grammar induction algorithm is guided bythe principle of occam’s razor, which leads us to.
5see appendix a.1 for additional background on qcfgs..925scann t → (cid:104)turn right, i_turn_right(cid:105)n t → (cid:104)n t[1] after n t[2], n t[2] n t[1](cid:105)n t → (cid:104)n t[1] thrice, n t[1] n t[1] n t[1](cid:105).
geoqueryn t → (cid:104)names of n t[1], n t[1](cid:105)n t → (cid:104)towns, cities(cid:105)n t → (cid:104)n t[1] have n t[2] running through them,.
intersection ( n t[1] , traverse_1 ( n t[2] ) )(cid:105).
spider-sspn t → (cid:104)reviewer, reviewer(cid:105)n t → (cid:104)what is the id of the n t[1] named n t[2] ?,.
select rid from n t[1] where name = " n t[2] "(cid:105).
table 1: examples of induced qcfg rules.
the sub-script 1 in n t[1] indicates the correspondence betweensource and target non-terminals..seek the smallest, simplest grammar that explainsthe data well.
we follow the minimum descriptionlength (mdl) principle (rissanen, 1978; grun-wald, 2004) as a way to formalize this intuition.
speciﬁcally, we use standard two-part codes tocompute description length, where we are inter-ested in an encoding of targets y given the inputsx, across a dataset d consisting of these pairs.
atwo-part code encodes the model and the targetsencoded using the model; the two parts measurethe simplicity of the model and the extent to whichit can explain the data, respectively..for grammar induction, our model is simply ourgrammar, g. the codelength can therefore be ex-pressed as h(g) − (cid:80)x,y∈d log2 pg(y|x) whereh(g) corresponds to the codelength of some en-coding of g. we approximate h(g) by countingterminal (ct ) and non-terminal (cn ) symbols inthe grammar’s rules, r. for pg, we assume auniform distribution over the set of possible deriva-tions.6 as the only mutable aspect of the grammarduring induction is the set of rules r, we abusenotation slightly and write our approximate code-length objective as a function of r only:.
l(r) = ln cn (r) + lt ct (r)−.
(cid:88).
log2.
(x,y)∈d.
|zg|zg.
x,y|x,∗|.
,.
where zgyield the pair of strings x and y, while zg.
x,y is the set of all derivations in g that.
x,∗ ⊃ zgx,y.
6this can be viewed as a conservative choice, as in practicewe expect our neural parser to learn a better model for p (y|x)than a naive uniform distribution over derivations..is the set of derivations that yield source string xand any target string.
the constants ln and lt canbe interpreted as the average bitlength for encodingnon-terminal and terminal symbols, respectively.
in practice, these are treated as hyperparameters..we use a greedy search algorithm to ﬁnd agrammar that approximately minimizes this code-length objective.
we initialize g by creating a rulen t → (cid:104)x, y(cid:105) for every training example (x, y).
by construction, the initial grammar perfectly ﬁtsthe training data, but is also very large.
our algo-rithm iteratively identiﬁes a rule that can be addedto g that decreases our codelength objective byenabling ≥ 1 rule(s) to be removed, under the in-variant constraint that g can still derive all trainingexamples.
the search completes when no rule thatdecreases the objective can be identiﬁed.
in prac-tice, we use several approximations to efﬁcientlyselect a rule at each iteration.
additional detailsregarding the grammar induction algorithm are de-scribed in appendix a.2..4.1.2 nqg semantic parsing modelbased on the induced grammar g, we train a dis-criminative latent variable parsing model, using amethod similar to that of blunsom et al.
(2008).
we deﬁne p(y | x) as:.
p(y | x) =.
p(z | x),.
(cid:88).
z∈zg.
x,y.
where zgg. we deﬁne p(z | x) as:.
x,y is the set of derivations of x and y in.
p(z | x) =.
exp(s(z, x)).
exp(s(z(cid:48), x)).
,.
(cid:80)z(cid:48)∈zg.
x,∗.
where s(z, x) is a derivation score and the denom-inator is a global partition function.
similarly tothe neural crf model of durrett and klein (2015),the scores decompose over anchored rules.
unlikedurrett and klein (2015), we compute these scoresbased on contextualized representations from abert (devlin et al., 2019) encoder.
additional de-tails regarding the model architecture can be foundin appendix a.3..at training time, we use a maximum marginallikelihood (mml) objective.
we preprocess eachexample to produce parse forest representationsfor both zgx,∗, which correspond to thenumerator and denominator of our mml objective,respectively.
by using dynamic programming to.
x,y and zg.
926efﬁciently sum derivation scores inside the trainingloop, we can efﬁciently compute the exact mmlobjective without requiring approximations such asbeam search..at inference time, we select the highest scoringderivation using an algorithm similar to cky thatconsiders anchored rule scores generated by theneural parsing model.
we output the correspondingtarget if it can be derived by a cfg deﬁning validtarget constructions for the given task..4.1.3 nqg discussion.
we note that nqg is closely related to work thatuses synchronous grammars for hierarchical statis-tical machine translation, such as hiero (chiang,2007).
unlike hiero, nqg does not rely on anadditional word alignment component.
moreover,hiero simply uses relative frequency to learn ruleweights.
additionally, in contract with traditionalscfg models for machine translation applied tosemantic parsing (wong and mooney, 2006; an-dreas et al., 2013), our neural model conditions onglobal context from the source x via contextualword embeddings, and our grammar’s rules do notneed to carry source context to aid disambiguation..4.2 t5 component.
t5 (raffel et al., 2020) is a pre-trained sequence-to-sequence transformer model (vaswani et al., 2017).
we ﬁne-tune t5 for each task..5 experiments.
we evaluate existing approaches and the newlyproposed nqg-t5 across a diverse set of evalu-ations to assess compositional generalization andhandling of natural language variation.
we aim tounderstand how the approaches compare to eachother for each type of evaluation and in aggregate,and how the performance of a single approach mayvary across different evaluation types..5.1 experiments on scan and geoquery.
for our main experiments, we focus on evaluationacross multiple splits of two datasets with compo-sitional queries: scan (lake and baroni, 2018)and geoquery (zelle and mooney, 1996; tangand mooney, 2001).
the two datasets have beenwidely used to study compositional generalizationand robustness to natural language variation, re-spectively.
both datasets are closed-domain andhave outputs with straightforward syntax, enabling.
us to make clear comparisons between synthetic vs.non-synthetic setups..approaches for nqg-t5, to assess the effect ofmodel size, we compare two sizes of the underlyingt5 model: base (220 million parameters) and 3b (3billion parameters).
to evaluate nqg individually,we treat any example where no output is providedas incorrect when computing accuracy..we select strong approaches from prior workthat have performed well in at least one setting.
we group them into two families of approachesdescribed in figure 1. first, for general-purposemodels that have shown strong ability to handlenatural language variation, we consider t5, a pre-trained seq2seq model, in both base and 3b sizes.
second, for specialized methods with strongcompositional biases, we consider approaches thathave been developed for scan.
some previousapproaches for scan require task-speciﬁc infor-mation such as the mapping of atoms (lake, 2019;gordon et al., 2019) or a grammar mimicking thetraining data (nye et al., 2020), and as such are dif-ﬁcult to adapt to non-synthetic datasets.
among theapproaches that do not need task-speciﬁc resources,we evaluate two models with publicly availablecode: syntactic attention (russin et al., 2019) andcgps (li et al., 2019).
we report results on scanfrom the original papers as well as new results onour proposed data splits..datasets for the scan dataset, we evaluate us-ing the length split and two primitive splits, jumpand turn left, included in the original dataset (lakeand baroni, 2018).
we also evaluate using thescan mcd splits from keysers et al.
(2020)..geoquery (zelle and mooney, 1996) containsnatural language questions about us geography.
similarly to prior work (dong and lapata, 2016,2018), we replace entity mentions with placehold-ers.
we use a variant of functional query lan-guage (funql) as the target representation (kateet al., 2005).
in addition to the standard split ofzettlemoyer and collins (2005), we generate multi-ple splits focusing on compositional generalization:a new split based on query length and a tmcdsplit, each consisting of 440 train and 440 test ex-amples.
we also generate a new template splitconsisting of 441 train and 439 test examples.7.
7we generate a new template split rather than use the geo-query template split of finegan-dollak et al.
(2018) to avoidoverlapping templates between the train and test sets whenmapping from sql to funql..927scan.
geoquery.
system.
jump turn left len.
mcd standard template len.
tmcd.
lane (liu et al., 2020)nssm (chen et al., 2020)syntactic attn.
(russin et al., 2019)cgps (li et al., 2019)geca (andreas, 2020)sbsp (herzig and berant, 2020)sbsp −lexicon.
t5-base (raffel et al., 2020)t5-3b (raffel et al., 2020).
nqg-t5-basenqg-t5-3bnqg.
10010091.098.887.0100100.
99.599.0.
100100100.
——99.999.7—100100.
62.065.1.
100100100.
100100100 —2.915.22.020.3——100100100100.
14.43.3.
100100100.
15.411.6.
100100100.
——77.562.178.0†86.1†78.9†.
92.993.2.
92.993.776.8.
——70.632.8———.
87.083.1.
88.885.061.9.
——23.69.3———.
39.136.8.
52.251.437.4.
——0.032.3———.
54.351.6.
56.654.141.1.avg.
rank.
——3.94.4———.
2.9—.
1.0—2.3.table 2: main results.
existing approaches do not excel on a diverse set of evaluations across synthetic andnon-synthetic tasks, but nqg-t5 obtains signiﬁcant improvements.
for comparison, we report the average rankamong 5 approaches across all 8 evaluations.
gray cells are previously reported results.
† indicates differences ingeoquery settings (see discussion in § 5.1).
boldfaced results are within 1.0 points of the best result..we report exact-match accuracy for bothdatasets.8 hyperparameters and pre-processing de-tails can be found in appendix b..results the results are presented in table 2.the results for t5 on scan are from furreret al.
(2020).
additionally, we include resultsfor geca9 (andreas, 2020), a data augmentationmethod, as well as lane (liu et al., 2020) andnssm (chen et al., 2020)10. we also compare withspanbasedsp11 (herzig and berant, 2020)..from the results, we ﬁrst note that the rela-tive performance of approaches on compositionalsplits of scan is not very predictive of their rela-tive performance on compositional splits of geo-query.
for example, ggps is better than t5on the length split of scan but is signiﬁcantlyworse than t5 on the length split of geoquery.
similarly, the ranking of most methods is different.
8for geoquery we report the mean of 3 runs for nqg,.
with standard deviations reported in appendix b.5.
9geca reports geoquery results on a setting with pro-log logical forms and without anonymization of entities.
notethat the performance of geca depends on both the quality ofthe generated data and the underlying parser (jia and liang,2016), which can complicate the analysis..10these scan-motivated approaches both include aspectsof discrete search and curriculum learning, and have not beendemonstrated to scale effectively to non-synthetic parsingtasks.
moreover, the code is either not yet released (nssm)or specialized to scan (lane)..11spanbasedsp preprocesses scan to add program-levelsupervision.
for geoquery, they similarly use funql, butuses slightly different data preprocessing and report denotationaccuracy.
we computed nqg-t5’s denotation accuracy to be2.1 points higher than exact-match accuracy on the standardsplit of geoquery..on the (t)mcd splits of the two datasets.
sec-ond, the proposed nqg-t5 approach combines thestrengths of t5 and nqg to achieve superior re-sults across all evaluations.
it improves over t5 oncompositional generalization for both synthetic andnon-synthetic data while maintaining t5’s perfor-mance on handling in-distribution natural languagevariation, leading to an average rank of 1.0 com-pared to 2.9 for t5.
(to the best of our knowledge,both t5 and nqg-t5 achieve new state-of-the-artaccuracy on the standard split of geoquery.).
finally, we note that there is substantial roomfor improvement on handling both compositionalgeneralization and natural language variation..5.2 experiments on spider.
we now compare the approaches on spider (yuet al., 2018), a non-synthetic text-to-sql datasetthat includes the further challenges of schema link-ing and modeling complex sql syntax..spider contains 10,181 questions and 5,693unique sql queries across 138 domains.
theprimary evaluation is in the cross-database set-ting, where models are evaluated on examples fordatabases not seen during training.
the primarychallenge in this setting is generalization to newdatabase schemas, which is not our focus.
there-fore, we use a setting where the databases areshared between train and test examples.12 we gen-.
12this is similar to the “example split” discussed in yuet al.
(2018).
however, we only consider examples in theoriginal training set for databases with more than 50 examplesto ensure sufﬁcient coverage over table and column names in.
928spider-ssp.
system.
rand.
templ.
len.
tmcd.
t5-base −schemat5-baset5-3b.
nqg-t5-basenqg-t5-3bnqg.
76.582.085.6.
81.885.41.3.
45.359.364.8.
59.264.70.5.
42.549.056.7.
49.056.70.0.
42.360.969.6.
60.869.50.5.ryansql v2 (choi et al., 2020).
system.
t5-baset5-3b.
nqg-t5-basenqg-t5-3bnqg.
spider-xspdev.
70.6.
57.170.0.
57.170.00.0.table 3: results on spider-ssp.
while the text-to-sqltask is not modeled well by the nqg grammar due tosql’s complex syntax, nqg-t5 still performs well byrelying on t5..table 4: although spider-xsp is not our focus, t5 andnqg-t5 are competitive with the state-of-the-art..erate 3 new splits consisting of 3,282 train and1,094 test examples each: a random split, a splitbased on source length, and a tmcd split.
we alsogenerate a template split by anonymizing integersand quoted strings, consisting of 3,280 train and1,096 test examples.
we adopt the terminologyof suhr et al.
(2020) and use spider-ssp to re-fer to these same-database splits, and use spider-xsp to refer to the standard cross-database setting.
we prepend the name of the target database tothe source sequence.
for t5, we also serialize thedatabase schema as a string and append it to thesource sequence similarly to suhr et al.
(2020).
wereport exact set match without values, the standardspider evaluation metric (yu et al., 2018)..results table 3 shows the results of t5 andnqg-t5 on different splits of spider-ssp.
we also show t5-base performance without theschema string appended.
the text-to-sql map-ping is not well modeled by nqg.
nevertheless,the performance of nqg-t5 is competitive witht5, indicating a strength of the hybrid approach..table 4 shows the results on spider-xsp,which focuses on handling unseen schema ratherthan compositional generalization.
to our surprise,t5-3b proves to be competitive with the state-of-the-art (choi et al., 2020) for approaches withoutaccess to database contents beyond the table andcolumn names.
as nqg-t5 simply uses t5’s out-put when the induced grammar lacks coverage, ittoo is competitive..6 analysis.
6.1 comparison of data splits.
table 6 compares the compound divergence, thenumber of test examples with unseen atoms, and.
the training data.
this includes 51 databases..the accuracy of t5-base across various splits.
forgeoquery, the tmcd split is signiﬁcantly morechallenging than the template split.
however, forspider, the template and tmcd splits are simi-larly challenging.
notably, template splits do nothave an explicit atom constraint.
we ﬁnd that forthe spider template split, t5-base accuracy is53.9% for the 30.3% of test set examples that con-tain an atom not seen during training, and 61.6%on the remainder, indicating that generalization tounseen atoms can contribute to the difﬁculty oftemplate splits.13 length splits are also very chal-lenging, but they lead to a more predictable errorpattern for seq2seq models, as discussed next..6.2 t5 analysis.
we analyze nqg-t5’s components, starting witht5.
on length splits, there is a consistent pat-tern to the errors.
t5’s outputs on the test set arenot signiﬁcantly longer than the maximum lengthobserved during training, leading to poor perfor-mance.
this phenomenon was explored by new-man et al.
(2020)..diagnosing the large generalization gap on the(t)mcd splits is more challenging, but we noticedseveral error patterns.
for t5-base on the geo-query tmcd split, in 52 of the 201 incorrectpredictions (26%), the ﬁrst incorrectly predictedsymbol occurs when the gold symbol has 0 prob-ability under a trigram language model ﬁt to thetraining data.
this suggests that the decoder’s im-plicit target language model might have over-ﬁttedto the distribution of target sequences in the train-ing data, hampering its ability to generate novelcompositions.
non-exclusively with these errors,53% of the incorrect predictions occur when thegold target contains an atom that is seen in only 1.
13future work could explore different choices for construct-ing template and tmcd splits, such as alternative compounddeﬁnitions and atom constraints..929scan.
geoquery.
spider-ssp.
metric.
jump turn l. len.
mcd stand.
templ.
len.
tmcd rand.
templ.
len.
tmcd.
nqg coveragenqg precision.
100100.
100100.
100100.
100100.
80.295.7.
64.595.8.
43.386.4.
43.794.1.
1.587.5.
0.583.3.
0.0—.
0.685.7.table 5: nqg coverage and precision.
nqg-t5 outperforms t5 when nqg has higher precision than t5 over thesubset of examples it covers..dataset.
split.
%za dc.
t5-base.
geoquery.
standard.
geoquerygeoquerygeoquerygeoquery.
randomtemplatelengthtmcd.
spider-ssp randomspider-ssp templatelengthspider-ssptmcdspider-ssp.
0.3.
1.40.94.30.
6.230.327.40.
0.03.
0.030.070.170.19.
0.030.080.080.18.
92.9.
91.187.039.154.3.
82.059.249.060.9.table 6: percentage of test examples with atoms not in-cluded in the training set (%za), compound divergence(dc), and t5-base accuracy for various dataset splits..example during training, suggesting that t5 strug-gles with single-shot learning of new atoms.
inother cases, the errors appear to reﬂect over-ﬁttingto spurious correlations between inputs and outputs.
some error examples are shown in appendix b.6..6.3 nqg analysis.
to analyze nqg, we compute its coverage (frac-tion of examples where nqg produces an output)and precision (fraction of examples with a correctoutput among ones where an output is produced)on different data splits.
the results in table 5 showthat nqg has high precision but struggles at cover-age on some data splits..there is a signiﬁcant difference in the effective-ness of the grammar induction procedure amongthe three datasets.
induction is particularly unsuc-cessful for spider, as sql has complicated syn-tax and often requires complex coordination acrossdiscontinuous clauses.
most of the induced rulesare limited to simply replacing table and columnnames or value literals with non-terminals, such asthe rule shown in table 1, rather than representingnested sub-structures.
the degree of span-to-spancorrespondence between natural language and sqlis seemingly lower than for other formalisms suchas funql, which limits the effectiveness of gram-mar induction.
intermediate representations forsql such as semql (guo et al., 2019) may help.
increase the correspondence between source andtarget syntax..for both geoquery and spider, nqg is lim-ited by the expressiveness of qcfgs and the simplegreedy search procedure used for grammar induc-tion, which can lead to sub-optimal approxima-tions of the induction objective.
notably, qcfgscannot directly represent relations between sourcestrings, such as semantic similarity, or relationsbetween target strings, such as logical equivalence(e.g.
intersect(a,b) ⇔ intersect(b,a)), thatcould enable greater generalization.
however, suchextensions pose additional scalability challenges,requiring new research in more ﬂexible approachesfor both learning and inference..7 conclusions.
our experiments and analysis demonstrate thatnqg and t5 offer different strengths.
nqg gen-erally has higher precision for out-of-distributionexamples, but is limited by the syntactic constraintsof the grammar formalism and by requiring exactlexical overlap with induced rules in order to pro-vide a derivation at inference time.
t5’s coverageis not limited by such constraints, but precision canbe signiﬁcantly lower for out-of-distribution exam-ples.
with nqg-t5, we offer a simple combinationof these strengths.
while accuracy is still limitedfor out-of-distribution examples where nqg lackscoverage, we believe it sets a strong and simplebaseline for future work..more broadly, our work highlights that evaluat-ing on a diverse set of benchmarks is important,and that handling both out-of-distribution composi-tional generalization and natural language variationremains an open challenge for semantic parsing..acknowledgements.
we thank kenton lee, william cohen, jeremycole, and luheng he for helpful discussions.
thanks also to emily pitler, jonathan herzig, andthe anonymous reviewers for their comments andsuggestions..930ethical considerations.
this paper proposed to expand the set of bench-marks used to evaluate compositional generaliza-tion in semantic parsing.
while we hope that en-suring semantic parsing approaches perform wellacross a diverse set of evaluations, including onesthat test out-of-distribution compositional gener-alization, would lead to systems that generalizebetter to languages not well represented in smalltraining sets, we have only evaluated our methodson semantic parsing datasets in english..our nqg-t5 method uses a pre-trained t5model, which is computationally expensive in ﬁne-tuning and inference, especially for larger mod-els (see appendix b.1 for details on running timeand compute architecture).
our method does notrequire pre-training of large models, as it usespre-existing model releases.
nqg-t5-base out-performs or is comparable in accuracy to t5-3b onthe non-sql datasets, leading to relative savingsof computational resources..references.
alfred v aho and jeffrey d ullman.
1972. the the-ory of parsing, translation, and compiling, volume 1.prentice-hall englewood cliffs, nj..jacob andreas.
2020. good-enough compositionaldata augmentation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7556–7566, online.
associationfor computational linguistics..jacob andreas, andreas vlachos, and stephen clark.
2013. semantic parsing as machine translation.
inproceedings of the 51st annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 47–52, soﬁa, bulgaria.
associ-ation for computational linguistics..jasmijn bastings, marco baroni,.
jason weston,jumpkyunghyun cho, and douwe kiela.
2018.to better conclusions: scan both left and right.
inproceedings of the 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 47–55..peter w battaglia, jessica b hamrick, victor bapst,alvaro sanchez-gonzalez, vinicius zambaldi, ma-teusz malinowski, andrea tacchetti, david raposo,adam santoro, ryan faulkner, et al.
2018. rela-tional inductive biases, deep learning, and graph net-works.
arxiv preprint arxiv:1806.01261..phil blunsom, trevor cohn, and miles osborne.
2008.a discriminative latent variable model for statisti-cal machine translation.
in proceedings of acl-08:.
hlt, pages 200–208, columbus, ohio.
associationfor computational linguistics..xinyun chen, chen liang, adams wei yu, dawnsong, and denny zhou.
2020. compositional gen-eralization via neural-symbolic stack machines.
ad-vances in neural information processing systems,33..david chiang.
2007. hierarchical phrase-based trans-lation.
computational linguistics, 33(2):201–228..donghyun choi, myeongcheol shin, eunggyun kim,and dong ryeol shin.
2020. ryansql: recur-sively applying sketch-based slot ﬁllings for com-plex text-to-sql in cross-domain databases.
arxivpreprint arxiv:2004.03125..jk chung, pl kannappan, ct ng, and pk sahoo.
1989. measures of distance between probability dis-tributions.
journal of mathematical analysis and ap-plications, 138(1):280–292..john cocke.
1969. programming languages and theircompilers: preliminary notes.
new york univer-sity..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..li dong and mirella lapata.
2016. language to logi-cal form with neural attention.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages33–43..li dong and mirella lapata.
2018. coarse-to-ﬁne de-coding for neural semantic parsing.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 731–742, melbourne, australia.
associationfor computational linguistics..greg durrett and dan klein.
2015. neural crf parsing.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages302–312..catherine finegan-dollak, jonathan k. kummerfeld,li zhang, karthik ramanathan, sesh sadasivam,rui zhang, and dragomir radev.
2018. improvingin proceed-text-to-sql evaluation methodology.
ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 351–360, melbourne, australia.
asso-ciation for computational linguistics..931daniel furrer, marc van zee, nathan scales, andnathanael schärli.
2020. compositional generaliza-tion in semantic parsing: pre-training vs. specializedarchitectures.
arxiv preprint arxiv:2007.08970..jonathan gordon, david lopez-paz, marco baroni,and diane bouchacourt.
2019. permutation equiv-ariant models for compositional generalization inlanguage.
in international conference on learningrepresentations..peter grünwald.
1995. a minimum description lengthin internationalapproach to grammar inference.
joint conference on artiﬁcial intelligence, pages203–216.
springer..peter grunwald.
2004. a tutorial introduction tothe minimum description length principle.
arxivpreprint math/0406077..jiaqi guo, zecheng zhan, yan gao, yan xiao,jian-guang lou, ting liu, and dongmei zhang.
2019. towards complex text-to-sql in cross-domainin pro-database with intermediate representation.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4524–4535..jonathan herzig and jonathan berant.
2019. don’tparaphrase, detect!
rapid and effective data collec-in proceedings of thetion for semantic parsing.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3801–3811..jonathan herzig and jonathan berant.
2020. span-based semantic parsing for compositional general-ization.
arxiv preprint arxiv:2009.06040..theo mv janssen and barbara h partee.
1997. com-in handbook of logic and language,.
positionality.
pages 417–473.
elsevier..robin jia and percy liang.
2016. data recombinationfor neural semantic parsing.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages12–22..t. kasami.
1965. an efﬁcient recognition and syntaxanalysis algorithm for context-free languages.
tech-nical report afcrl-65-758, air force cambridgeresearch laboratory, bedford, ma..rohit j kate, yuk wah wong, and raymond j mooney.
2005. learning to transform natural to formal lan-guages.
in proceedings of the national conferenceon artiﬁcial intelligence, volume 20, page 1062.menlo park, ca; cambridge, ma; london; aaaipress; mit press; 1999..marc van zee, and olivier bousquet.
2020. measur-ing compositional generalization: a comprehensivemethod on realistic data.
in iclr..najoung kim and tal linzen.
2020. cogs: a com-positional generalization challenge based on seman-tic interpretation.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 9087–9105, online.
as-sociation for computational linguistics..tom kwiatkowski, eunsol choi, yoav artzi, and lukezettlemoyer.
2013. scaling semantic parsers withon-the-ﬂy ontology matching.
in proceedings of the2013 conference on empirical methods in naturallanguage processing, pages 1545–1556..tom kwiatkowski, luke zettlemoyer, sharon gold-water, and mark steedman.
2010. inducing proba-bilistic ccg grammars from logical form with higher-in proceedings of the 2010 con-order uniﬁcation.
ference on empirical methods in natural languageprocessing, pages 1223–1233.
association for com-putational linguistics..b. m. lake, t. linzen, and m. baroni.
2019. humanfew-shot learning of compositional instructions.
inproceedings of the 41st annual conference of thecognitive science society..brenden lake and marco baroni.
2018. generalizationwithout systematicity: on the compositional skillsof sequence-to-sequence recurrent networks.
in in-ternational conference on machine learning, pages2873–2882..brenden m lake.
2019. compositional generalizationthrough meta sequence-to-sequence learning.
in ad-vances in neural information processing systems,pages 9788–9798..brenden m lake, tomer d ullman, joshua b tenen-baum, and samuel j gershman.
2017. building ma-chines that learn and think like people.
behavioraland brain sciences, 40..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017. end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing,pages 188–197..junhui li, muhua zhu, wei lu, and guodong zhou.
improving semantic parsing with enriched2015.in proceed-synchronous context-free grammar.
ings of the 2015 conference on empirical meth-ods in natural language processing, pages 1455–1465, lisbon, portugal.
association for computa-tional linguistics..daniel keysers, nathanael schärli, nathan scales,hylke buisman, daniel furrer, sergii kashubin,nikola momchev, danila sinopalnikov, lukaszstaﬁniak, tibor tihon, dmitry tsarkov, xiao wang,.
yuanpeng li, liang zhao, jianyu wang, and joel hes-tness.
2019. compositional generalization for prim-itive substitutions.
in proceedings of the 2019 con-ference on empirical methods in natural language.
932processing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 4284–4293, hong kong, china.
as-sociation for computational linguistics..qian liu, shengnan an, jian-guang lou, bei chen,zeqi lin, yan gao, bin zhou, nanning zheng, anddongmei zhang.
2020. compositional generaliza-tion by learning analytical expressions.
advances inneural information processing systems, 33..richard montague.
1970. universal grammar.
theo-.
ria, 36(3):373–398..benjamin newman, john hewitt, percy liang, andchristopher d. manning.
2020. the eos decisionin proceedings of theand length extrapolation.
third blackboxnlp workshop on analyzing and in-terpreting neural networks for nlp, pages 276–291,online.
association for computational linguistics..maxwell i nye, armando solar-lezama, joshua btenenbaum, and brenden m lake.
2020. learn-ing compositional rules via neural program synthe-sis.
arxiv preprint arxiv:2003.05562..inbar oren, jonathan herzig, nitish gupta, matt gard-improving com-ner, and jonathan berant.
2020.inpositional generalization in semantic parsing.
proceedings of the 2020 conference on empiricalmethods in natural language processing: findings,pages 2482–2495..hoifung poon, colin cherry, and kristina toutanova.
2009. unsupervised morphological segmentationin proceedings of humanwith log-linear models.
language technologies: the 2009 annual confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 209–217,boulder, colorado.
association for computationallinguistics..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21:1–67..sujith ravi and kevin knight.
2009. minimized mod-els for unsupervised part-of-speech tagging.
in pro-ceedings of the joint conference of the 47th annualmeeting of the acl and the 4th international jointconference on natural language processing of theafnlp, pages 504–512, suntec, singapore.
associ-ation for computational linguistics..jorma rissanen.
1978. modeling by shortest data de-.
scription.
automatica, 14(5):465–471..jake russin, jason jo, randall c o’reilly, and yoshuabengio.
2019. compositional generalization in adeep seq2seq model by separating syntax and seman-tics.
arxiv preprint arxiv:1904.09708..markus saers, karteek addanki, and dekai wu.
2013.unsupervised transduction grammar induction viaminimum description length.
in proceedings of thesecond workshop on hybrid approaches to transla-tion, pages 67–73, soﬁa, bulgaria.
association forcomputational linguistics..david a smith and jason eisner.
2006..quasi-synchronous grammars: alignment by soft projec-in proceedings ontion of syntactic dependencies.
the workshop on statistical machine translation,pages 23–30..mitchell stern, jacob andreas, and dan klein.
2017. aminimal span-based neural constituency parser.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 818–827, vancouver, canada.
association for computational linguistics..alane suhr, ming-wei chang, peter shaw, and ken-ton lee.
2020. exploring unexplored generalizationchallenges for cross-database semantic parsing.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8372–8388..lappoon r tang and raymond j mooney.
2001. us-ing multiple clause constructors in inductive logicin europeanprogramming for semantic parsing.
conference on machine learning, pages 466–477.
springer..iulia turc, ming-wei chang, kenton lee, and kristinatoutanova.
2019. well-read students learn better:on the importance of pre-training compact models.
arxiv preprint arxiv:1908.08962..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..yuk wah wong and raymond mooney.
2007. learn-ing synchronous grammars for semantic parsingin proceedings of the 45thwith lambda calculus.
annual meeting of the association of computationallinguistics, pages 960–967..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4902–4912, online.
association for computational lin-guistics..yuk wah wong and raymond j mooney.
2006. learn-ing for semantic parsing with statistical machinetranslation.
in proceedings of the main conferenceon human language technology conference of thenorth american chapter of the association of com-putational linguistics, pages 439–446.
associationfor computational linguistics..933daniel h younger.
1967. recognition and parsing ofcontext-free languages in time n3.
information andcontrol, 10(2):189–208..tao yu, rui zhang, kai yang, michihiro yasunaga,dongxu wang, zifan li, james ma,irene li,qingning yao, shanelle roman, zilin zhang,and dragomir radev.
2018.spider: a large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages3911–3921, brussels, belgium.
association forcomputational linguistics..john m zelle and raymond j mooney.
1996. learn-ing to parse database queries using inductive logicprogramming.
in proceedings of the thirteenth na-tional conference on artiﬁcial intelligence-volume 2,pages 1050–1055..luke zettlemoyer and michael collins.
2007. onlinelearning of relaxed ccg grammars for parsing to log-in proceedings of the 2007 joint con-ical form.
ference on empirical methods in natural languageprocessing and computational natural languagelearning (emnlp-conll), pages 678–687..luke s zettlemoyer and michael collins.
2005. learn-ing to map sentences to logical form: structured clas-siﬁcation with probabilistic categorial grammars.
inproceedings of the twenty-first conference on un-certainty in artiﬁcial intelligence, pages 658–666.
auai press..hao zheng and mirella lapata.
2020. compositionalgeneralization via semantic tagging.
arxiv preprintarxiv:2010.11818..934appendix.
we organize the appendix into two sections:.
• additional details for nqg in appendix a..• additional experimental details and analysis.
in appendix b..a nqg details.
in this section we describe the nqg grammarinduction algorithm and parsing model in detail,starting with relevant background and notation forqcfgs..a.1 background: scfgs and qcfgs.
synchronous context-free grammars (scfgs)have been used to model the hierarchical mappingbetween pairs of strings in areas such as compilertheory (aho and ullman, 1972) and natural lan-guage processing..informally, scfgs can be viewed as an exten-sion of context-free grammars (cfgs) that syn-chronously generate strings in both a source andtarget language.
we write scfg rules as:.
s → (cid:104)α, β(cid:105).
where s is a non-terminal symbol, and α and βare strings of non-terminal and terminal symbols.
an scfg rule can be viewed as two cfg rules,s → α and s → β with a pairing between theoccurrences of non-terminal symbols in α and β.this pairing is indicated by assigning each non-terminal in α and β an index ∈ n. non-terminalssharing the same index are called linked.
followingconvention, we denote the index for a non-terminalusing a boxed subscript, e.g.
n t[1].
a completescfg derivation is a pair of parse trees, one for thesource language and one for the target language.
an example derivation is shown in figure 4..the ⇒r operator refers to a derives relation,such that (cid:104)α1, β1(cid:105) ⇒r (cid:104)α2, β2(cid:105) states that thestring pair (cid:104)α2, β2(cid:105) can be generated from (cid:104)α1, β1(cid:105)by applying the rule r. we write ⇒ to leave therule unspeciﬁed, assuming the set of possible rulesis clear from context.
we write ⇒⇒ to indicate achain of 2 rule applications, omitting the interme-∗⇒ to denote thediate string pair.
finally, we writereﬂexive transitive closure of ⇒..quasi-synchronous context-free grammars(qcfgs) qcfgs generalize scfgs in variousways, notably relaxing the restriction on a strict.
one-to-one alignment between source and targetnon-terminals (smith and eisner, 2006)..grammar.
compositionality notably,for-malisms such scfgs and qcfgs capture theformal notion of the principle of compositionalityas a homomorphism between source and targetstructures (montague, 1970; janssen and partee,1997)..a.2 nqg grammar induction details.
having deﬁned the codelength scoring functionthat we use to compare grammars in section 4.1.1,we describe our greedy search algorithm that ﬁndsa grammar that approximately minimizes this ob-jective.14.
initialization we initialize r to be {n t →(cid:104)x, y(cid:105) | x, y ∈ d}.
we also add identity rulesfor substrings that exactly match between sourceand target examples, e.g.
n t → (cid:104)k, k(cid:105) where k isa substring of both x and y for some x, y ∈ d.15.
optimization algorithm our algorithm was de-signed with simplicity in mind, and therefore usesa simple greedy search process that could likelybe signiﬁcantly improved upon by future work.
at a high level, our greedy algorithm iterativelyidentiﬁes a rule to be added to r that decreasesthe codelength by enabling ≥ 1 rules in r to beremoved while maintaining the invariant that gallows for deriving all of the training examples,i.e.
(cid:104)n t, n t (cid:105) ∗⇒ (cid:104)x, y(cid:105) for every x, y ∈ d. thesearch completes when no rule that decreases l(r)can be identiﬁed..to describe the implementation, ﬁrst let us deﬁneseveral operations over rules and sets of rules.
wedeﬁne the set of rules that can be derived from agiven set of rules, r:d(r) = {n t → (cid:104)α, β(cid:105) | (cid:104)n t, n t (cid:105) ∗⇒ (cid:104)α, β(cid:105)}.
we deﬁne an operation split that generates.
possible choices for splitting a rule into 2 rules:.
split(n t → (cid:104)α, β(cid:105)) = {g, h |(cid:104)n t, n t (cid:105) ⇒g⇒h (cid:104)α, β(cid:105) ∨(cid:104)n t, n t (cid:105) ⇒h⇒g (cid:104)α, β(cid:105)},.
14the induction objective contains hyperparameters repre-senting the bitlength of terminal and non-terminal symbols.
for all experiments we use ln = 1. for geoquery andspider we use lt = 8, and use lt = 32 for scan..15these initialization rules are used for geoquery andspider, but scan does not contain any exact token overlapbetween source and target languages..935n t → (cid:104)how many n t[1] pass through n t[2], answer ( count ( intersection ( n t[1] , loc_1 ( n t[2] ) ) ) )(cid:105)n t → (cid:104)rivers, river(cid:105)n t → (cid:104)the largest n t[1], largest ( n t[1] )(cid:105)n t → (cid:104)state, state(cid:105).
how.
many.
n t[1].
pass.
through.
n t[2].
rivers.
the.
largest.
n t[1].
n t.n t.answer.
(.
count.
(.
intersection.
(.
n t[1].
,.
loc_1.
(.
n t[2].
).
).
).
).
river.
largest.
(.
n t[1].
).
state.
state.
figure 4: an example qcfg derivation.
each non-terminal in the source derivation (blue) corresponds to a non-terminal in the target derivation (green).
the qcfg rules used in the derivation are shown above..where g and h is a pair of new rules that wouldmaintain the invariant that (cid:104)n t, n t (cid:105) ∗⇒ (cid:104)x, y(cid:105)for every x, y ∈ d, even if the provided rule iseliminated.16.
split can be implemented by consider-ing pairs of sub-strings in α and β to re-place with a new indexed non-terminal sym-the rule “n t →bol.
(cid:104)largest state, largest ( state )(cid:105)” can be split into therules “n t → (cid:104)largest n t[1], largest ( n t[1] )(cid:105)”and “n t → (cid:104)state, state(cid:105)”.
this step can requirere-indexing of non-terminals..for example,.
during our greedy search, we only split ruleswhen one of the two resulting rules can alreadybe derived given r. therefore, we deﬁne a func-tion new that returns a set of candidate rules toconsider:.
new(r) =.
{g | g, h ∈ split(f ) ∧ f ∈ r ∧ h ∈ d(r)}.
similarly, we can compute the set of rules thatare made redundant and can be eliminated by intro-ducing one these candidate rules, f :.
elim(r, f ) =.
{h | f, g ∈ split(h) ∧ g ∈ d(r) ∧ h ∈ r}.
16we optionally allow split to introduce repeated targetnon-terminals when the target string has repeated substrings.
otherwise, we do not allow split to replace a repeatedsubstring with a non-terminal, as this can lead to an ambiguouschoice.
we enable this option for scan and spider but notfor geoquery, as funql does not require such repetitions..we can then deﬁne the codelength reduction ofadding a particular rule, −∆l(r, f ) = l(r) −l(r(cid:48)) where r(cid:48) = (r ∪ f ) \ elim(r, f ).17 fi-nally, we can select the rule with the largest −∆l:.
max(r) = argmax.
− ∆l(r, f ).
f ∈new(r).
conceptually, after initialization, the algorithm.
then proceeds as:.
while |new(r)| > 0 do.
r ← max(r)if −∆l(r, r) < 0 then.
break.
end ifr ← (r ∪ r) \ elim(r, r).
end while.
for efﬁciency, we select the shortest n exam-ples from the training dataset, and only considerthese during the induction procedure.
avoidinglonger examples is helpful as the number of can-didates returned by split is polynomial with re-spect to source and target length.
once induction.
17the last term of the codelength objective described insection 4.1.1 is related to the increase in the proportion ofincorrect derivations due to introducing f .
rather than com-puting this exactly, we estimate this quantity by sampling upto k examples from d that contain all of the sub-strings ofsource terminal symbols in f such that f could be used in aderivation, and estimating the increase in incorrect derivationsover this sample only.
we sample k = 10 examples for allexperiments..936has completed, we then determine which of thelonger examples cannot be derived based on theset of induced rules, and add rules for these exam-ples.18.
our algorithm maintains a signiﬁcant amount ofstate between iterations to cache computations thatare not affected by particular rule changes, basedon overlap in terminal symbols.
we developed thealgorithm and selected some hyperparameters byassessing the size of the induced grammars overthe training sets of scan and geoquery..our grammar induction algorithm is similar tothe transduction grammar induction method formachine translation by saers et al.
(2013).
morebroadly, compression-based criteria have been suc-cessfully used by a variety of models for language(grünwald, 1995; tang and mooney, 2001; raviand knight, 2009; poon et al., 2009)..a.3 nqg parsing model details.
in this section we provide details on how we gener-ate derivation scores, s(z, x), using a neural model,as introduced in § 4.1. the derivation scores de-compose over anchored rules from our grammar:.
s(z, x) =.
φ(r, i, j, x),.
(cid:88).
(r,i,j)∈z.
where r is an index for a rule in g and i and j areindices deﬁning the anchoring in x. the anchoredrule scores, φ(r, i, j, x), are based on contextual-ized representations from a bert (devlin et al.,2019) encoder:.
for t5 and nqg based on random splits of thetraining sets for geoquery and spider.
weused the same hyperparameters for all splits of agiven dataset..for t5, we selected a learning rate of 1e−4 from[1e−3, 1e−4, 1e−5], which we used for all experi-ments.
otherwise, we used the default hyperparam-eters for ﬁne-tuning.
we ﬁne-tune for 3, 000 stepsfor geoquery and 10, 000 for spider.
t5-basetrained with a learning rate of 1e−4 reached 94.2%accuracy at 3, 000 steps on a random split of thestandard geoquery training set into 500 trainingand 100 validation examples..for the nqg neural model, we use the pre-trained bert tiny model of turc et al.
(2019)(4.4m parameters) for scan and spider, andbert base (110.1m parameters) for geoquery,where there is more headroom for improved scor-ing.
we do not freeze pre-trained bert parame-ters during training.
for all experiments, we used = 256 dimensions for computing anchored rulescores.
we ﬁne-tune for 256 steps and use a learn-ing rate of 1e−4.
we use a batch size of 256..we train nqg on 8 v100 gpus.
training nqgtakes < 5 minutes for scan and spider (berttiny), and up to 90 minutes for geoquery (bertbase).
we ﬁne-tune t5 on 32 cloud tpu v3cores.19 for geoquery, ﬁne-tuning t5 takesapproximately 5 and 37 hours for base and 3b,respectively.
for spider, ﬁne-tuning t5 takesapproximately 5 and 77 hours for base and 3b,respectively..φ(r, i, j, x) = fs([wi, wj]) + e(cid:124).
r fr([wi, wj]),.
b.2 dataset preprocessing.
where [wi, wj] is the concatenation of the bertrepresentations for the ﬁrst and last wordpiece inthe anchored span, fr is a feed-forward networkwith hidden size d that outputs a vector ∈ rd, fs isa feed-forward network with hidden size d that out-puts a scalar, and er is an embedding ∈ rd for therule index r. our formulation for encoding spansis similar to that used in other neural span-factoredmodels (stern et al., 2017; lee et al., 2017)..b experimental details.
b.1 model hyperparameters and runtime.
we selected reasonable hyperparameter values andperformed some minimal hyperparameter tuning.
18we use n = 500 for scan and n = 1000 for spi-der.
as the geoquery training set contains < 500 uniqueexamples, we use the entire training set..for geoquery, we use the version of thedataset withvariable-free funql logicalforms (kate et al., 2005), and expand certainlogical deﬁnitions,functions based on theirsuchstate(next_to_1(state(all)))becomesconventionaltheintersection(state, next_to_1(state)).
we replace entity mentions with placeholders (e.g.
“m0”, “m1”) in both the source and target..more.
that.
for spider, we prepend the name of the targetdatabase to the source sequence.
for t5, we also se-rialize the database schema as a string and appendit to the source sequence similarly to suhr et al.
(2020).
this schema string contains the names ofall tables in the database, and the names of thecolumns for each table.
as we use a maximum.
19https://cloud.google.com/tpu/.
937answer ( count ( intersection ( state , next_to_2 ( intersection ( major ,.
source: how many states are next to major riverstarget:river ) ) ) ) )prediction: answer ( count ( intersection ( state , next_to_2 ( intersection ( major ,intersection ( river , m0 ) ) ) ) ) )notes: the trigram “major , intersection” occurs 28 times during training, but “major , river”occurs 0 times.
in this case, t5 also hallucinates “m0” despite no entity placeholder occuring the source..source: which state has the highest peak in the countrytarget: answer ( intersection ( state , loc_1 ( highest ( place ) ) ) )prediction: answer ( highest ( intersection ( state , loc_2 ( highest ( intersection (mountain , loc_2 ( m0 ) ) ) ) ) )notes: the token “highest” occurs after “answer (” in 83% of instances in which “highest” occurs inthe training set.
note that t5 also hallucinates “m0” in this case..table 7: example prediction errors for t5-base for the geoquery tmcd split..dataset.
examples.
induced rules ratio.
b.4 grammar sizes.
scangeoqueryspider-ssp.
167276003282.
212344155.
796.52.60.79.table 8: sizes of induced grammars..induced grammar sizes for a selected split of eachdataset are shown in table 8. for spider, thenumber of induced rules is larger than the origi-nal dataset due to the identity rules added duringinitialization..std.
templ.
len.
tmcd.
b.5 geoquery variance.
in tables 2 and 5 we report the mean of 3 runs fornqg for geoquery.
the standard deviations forthese runs are reported in table 9. the reportedstandard deviations for nqg-t5 use the same ﬁne-tuned t5 checkpoint, so they do not reﬂect anyadditional variance from different ﬁne-tuned t5checkpoints..b.6 t5 geoquery errors.
we include several example t5-base errors on thegeoquery tmcd split in table 7..nqg-t5-3b acc.
nqg-t5-base acc.
nqg acc..nqg coveragenqg precision.
0.60.51.2.
0.70.7.
1.21.44.5.
3.41.9.
1.21.11.5.
1.81.7.
0.40.40.4.
0.11.2.table 9: standard deviation of nqg for geoquery..source sequence length of 512 for t5, this leadsto some schema strings being truncated (affectingabout 5% of training examples)..scan did not require any dataset-speciﬁc pre-.
processing..b.3 atom and compound deﬁnitions.
for geoquery, the tree structure of funql isgiven by explicit bracketing.
we deﬁne atomsas individual funql symbols, and compounds ascombinations between parent and child symbolsin the funql tree.
example atoms are longest,river, and exclude and example compounds arelongest(river) and exclude(longest(_), _).
for spider, we tokenize the sql string anddeﬁne atoms as individual tokens.
to deﬁne com-pounds, we parse the sql string using an unam-biguous cfg, and deﬁne compounds from the re-sulting parse tree.
we deﬁne compounds over bothﬁrst and second order edges in the resulting parsetree..938