weight distillation: transferring the knowledgein neural network parametersye lin1∗, yanyang li2∗, ziyang wang1, bei li1, quan du1, tong xiao1,3, jingbo zhu1,3†1nlp lab, school of computer science and engineering,northeastern university, shenyang, china2the chinese university of hong kong, hong kong, china3niutrans research, shenyang, china{linye2015,blamedrlee,libeineu,duquanneu}@outlook.com{wangziyang}@stumail.neu.edu.cn{xiaotong,zhujingbo}@mail.neu.edu.cn.
abstract.
knowledge distillation has proven to be effec-tive in model acceleration and compression.
ittransfers knowledge from a large neural net-work to a small one by using the large neuralnetwork predictions as targets of the small neu-ral network.
but this way ignores the knowl-edge inside the large neural networks, e.g., pa-rameters.
our preliminary study as well asthe recent success in pre-training suggests thattransferring parameters are more effective indistilling knowledge.
in this paper, we proposeweight distillation to transfer the knowledgein parameters of a large neural network to asmall neural network through a parameter gen-erator.
on the wmt16 en-ro, nist12 zh-en,and wmt14 en-de machine translation tasks,our experiments show that weight distillationlearns a small network that is 1.88∼2.94×faster than the large network but with competi-tive bleu performance.
when ﬁxing the sizeof the small networks, weight distillation out-performs knowledge distillation by 0.51∼1.82bleu points..1.introduction.
knowledge distillation (kd) is a popular modelacceleration and compression approach (hintonet al., 2015).
it assumes that a lightweight network(i.e., student network, or student for short) can learnto generalize in the same way as a large network(i.e., teacher network, or teacher for short).
to thisend, a simple method is to train the student networkwith predicted probabilities of the teacher networkas its targets..but kd has its limitation: the student networkcan only access the knowledge in the predictions ofthe teacher network.
it does not consider the knowl-edge in the teacher network parameters.
these pa-rameters contain billions of entries for the teacher.
∗authors contributed equally.
† corresponding author..network to make predictions.
yet in kd the stu-dent only learns from those predictions with atmost thousands of categories.
this way results inan inferior student network, since it learns from thelimited training signals.
our analysis in section 5.1shows that kd performs better if we simply cut offparts of parameters from the teacher to initializethe student.
this fact implies that the knowledge inparameters is complementary to kd but missed.
italso agrees with the recent success in pre-training(yang et al., 2019; liu et al., 2019; devlin et al.,2019), where parameters reusing plays the mainrole.
based on this observation, a superior studentis expected if all parameters in the teacher networkcould be exploited.
however, this imposes a greatchallenge as the student network is too small to ﬁtin the whole teacher network..to fully utilize the teacher network, we proposeweight distillation (wd) to transfer all the param-eters of the teacher network to the student network,even if they have different numbers of weight ma-trices and (or) these weight matrices are of dif-ferent shapes.
we ﬁrst use a parameter generatorto predict the student network parameters fromthe teacher network parameters.
after that, a ﬁne-tuning process is performed to improve the qualityof the transferred parameters.
see fig.
1 for acomparison of kd and wd..we test.
the wd method in a well-tunedtransformer-based machine translation system.
the experiments are run on three machine transla-tion benchmarks, including the wmt16 english-roman (en-ro), nist12 chinese-english (zh-en),and wmt14 english-german (en-de) tasks.
witha similar speedup, the student network trained bywd achieves bleu improvements of 0.51∼1.82points over kd.
with similar bleu performance,the student network trained by wd is 1.11∼1.39×faster than kd.
more interestingly, it is found thatwd is very effective in improving the student net-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2076–2088august1–6,2021.©2021associationforcomputationallinguistics2076prediction.
prediction.
groundtruth.
prediction.
sls...s1.
tlt.
....t2.
t1.
parametergenerator.
tlt.
....t2.
t1.
groundtruth.
prediction.
sls...s1.
teacher network.
student network.
teacher network.
student network.
(a) knowledge distillation.
(b) weight distillation.
figure 1: a comparison of knowledge distillation and weight distillation (solid red lines denote the knowledgetransfer.
t1 and s1 are the teacher and student weight matrices at the 1st layer and so on.
lt and ls are thenumbers of layers in the teacher and student networks.)..
work when its model size is close to the teachernetwork.
on the wmt14 en-de test data, our wd-based system achieves a strong result (a bleuscore of 30.77) but is 1.88× faster than the bigteacher network..2 background.
2.1 transformer.
in this work, we choose transformer (vaswaniet al., 2017) for study because it is one of the state-of-the-art neural models in natural language pro-cessing.
transformer is a seq2seq model, whichconsists of an encoder and a decoder.
the encodermaps an input sequence to a sequence of contin-uous representations and the decoder maps theserepresentations to an output sequence.
both theencoder and the decoder are composed of an em-bedding layer and multiple hidden layers.
thedecoder has an additional output layer at the end.
the hidden layer in the encoder consists of aself-attention sub-layer and a feed-forward network(ffn) sub-layer.
the decoder has an additionalencoder-decoder attention sub-layer between theself-attention and the ffn sub-layers.
for moredetails, we refer the reader to (vaswani et al., 2017)..2.2 knowledge distillation.
kd encourages the student network to produceoutputs close to the outputs of the teacher network..kd achieves this by:.
¯s = arg min.
l(yt , ys).
(1).
s.where l is the cross-entropy loss, yt is the teacherprediction, t is the teacher parameters, ys is thestudent prediction and s is the student parameters.
in practice, eq.
1 serves as a regularization term..a more effective kd variant for seq2seq mod-els is proposed by kim and rush (2016).
theyreplace the predicted distributions yt by the gener-ated sequences from the teacher network..3 weight distillation.
3.1 the parameter generator.
the proposed parameter generator transforms theteacher parameters t to the student parameters s.it is applied to the encoder and decoder separately.
the process is simple: it ﬁrst groups weight ma-trices in the teacher network into different subsets,and then each subset is used to generate a weightmatrix in the student network.
though using allteacher weights to predict student weights is possi-ble, its efﬁciency becomes an issue.
for instance,the number of parameters in a simple linear trans-formation will be the product of the numbers ofentries in its input and output, where in our casethese input and output contain billions of entries(from the teacher and student weights), making itintractable to keep this simple linear transforma-tion in the memory.
grouping is an effective way.
2077t.1.group.
6=.
tl.g.ro.u.p.2.
512.
=.
ot = 2048 i t.teachernetwork.
os = 2048 i s.os = 2048 i s.×.
w to.ot = 2048.
×.
1.wi.
s.=1024.is =256.
=512.i t.3=sl/tl.512.
3=sl/tl.512.
=.
=.
3=sl/tl.3=sl/tl.wl.
×.
e.q..6.s.o.o.wl.
×.
eq.6.
×.
w to.ot = 2048.
×.
1.wi.
s.=1024.is =256.
=512.i t.2=sl=256.i s.os =1024.studentnetwork.
figure 2: a running example of the parameter generator.
we take the transformation of w1 in eq.
2 from theteacher to the student as an example.
the teacher (stacked large cubes in the left) contains lt = 6 weights (w1)with each weight from different layers.
w1 (a single cube) in the teacher has an input dimension it of 512 and anoutput dimension ot of 2048. the student (stacked small cubes in the right) contains only ls = 2 weights (w1)with input dimension is = 256 and output dimension os = 1024..to reduce it to light-weighted transformation prob-lems.
here we take the encoder as an example forthe following discussion..3.1.1 weight groupingthe left of fig.
2 shows an example of weightgrouping for one group with two subsets..before the discussion, we deﬁne the weight classas a weight matrix from the network formulation,and the weight instance as the instantiation of aweight class.
take the ffn for an example.
itsformulation is deﬁned as:.
ffn(x) = max(xw1 + b1, 0)w2 + b2.
(2).
where w1, b1, w2 and b2 are learnable weight ma-trices.
in this case, w1 in eq.
2 deﬁnes a weightclass.
then all the corresponding weight matricesfrom ffns in different layers of the network arethe instantiations of this w1 weight class..from this sense, a weight class determines therole of its instantiations in design, e.g., extractingfeatures for w1 in eq.
2. this means that whentransferring parameters, different weight classeswill contribute little to each other as they have dif-ferent roles.
therefore, when predicting a studentweight matrix, it is sufﬁcient to consider the teacherweight matrices with the same weight class only,which makes the prediction efﬁcient.
so our pa-rameter generator groups the teacher weight ma-trices by the weight class they belong to, i.e., dif-.
ferent weight classes clusters all their instantia-tions to form their own groups.
in the previousexample, the w1 weight class will form a group[t1, t2, · · · , tlt], where each ti is the w1 weightinstance in the i-th ffn and lt is the number of lay-ers in the teacher network.
these weight matricesare then used to generate the w1 weight instancesin the student network..the parameter generator further divides eachgroup into smaller subsets with weight matricesfrom adjacent layers, because the adjacent layersfunction similarly (jawahar et al., 2019) and soas their weights.
this way additionally makesthe latertransformation more light-weighted.
namely, given a group of lt weight matrices,the parameter generator splits it into ls subsets,where ls is the number of layers in the studentnetwork.
for example, the i-th subset of the groupof w1 weight class in the previous example will be(cid:2)t(i−1)∗lt/ls+1, t(i−1)∗lt/ls+2, · · · , ti∗lt/lsthis subset is used to generate the weight matrixsi, which corresponds to w1 weight instance inthe i-th ffn of the student network..(cid:3)..3.1.2 weight transformation.
given a subset of teacher weight matrices, the pa-rameter generator then transforms them to the de-sired student weight matrix, as shown in the rightof fig.
2..let us see the process of generating the.
2078weight matrix s ∈ ris×os from the subset(cid:3) with each ti ∈ rit×ot,(cid:2)t1, t2, · · · , tlt/lswhere is and os are the input and output dimen-sions of the student weight matrix, it and ot are theinput and output dimensions of the teacher weightmatrix.
the parameter generator ﬁrst stacks allweight matrices in this subset into a tensor ˆt ∈rit×ot×lt/ls.
then it uses three learnable weightmatrices, wi ∈ rit×is, wo ∈ rot×os, wl ∈rlt/ls×1, to transform ˆt to the shape is × os × 1sequentially:.
ˆt·jk ← ˆt·jkwi , ∀j ∈ [1, ot], k ∈ [1, l(cid:48)] (3)ˆtj·k ← ˆtj·kwo, ∀j ∈ [1, is], k ∈ [1, l(cid:48)] (4)ˆtjk· ← ˆtjk·wl, ∀j ∈ [1, is], k ∈ [1, os](5).
t .
the objective of phase 1 is:.
¯π = arg min.
[(1 − α)l(yt , yπ) +.
αl(y, yπ)].
(7).
where l is the cross-entropy loss, yt is the teacherprediction, yπ is the prediction of the student net-work generated by the parameter generator π, y isthe ground truth, and α is a hyper-parameter thatbalances two losses and is set to 0.5 by default.
theﬁrst term of eq.
7 is the kd loss as in eq.
1, andthe second term is the standard loss..the objective of phase 2 has the same form aseq.
7, except that it optimizes s instead of π, likethis:.
where l(cid:48) = lt/ls..finally we transform ˆt (with 1 in its shape get.
eliminated) to produce s, as follows:.
¯s = arg min.
[(1 − α)l(yt , ys) +.
αl(y, ys)].
(8).
π.s.s = tanh( ˆt ) (cid:12) w + b.
(6).
where w and b are learnable weight matrices ofthe parameter generator and have the same shapeas ˆt .
(cid:12) denotes the hadamard product.
the tanhfunction provides non-linearity.
w and b are usedto scale and shift the tanh output to any desirablevalue.
note that we do not share wi , wo, wl,w and b when generating different s. if the en-coder is of the same size in both the teacher andstudent networks, only eq.
6 is needed to mapeach weight matrix from the teacher network to thestudent network..3.2 training.
there are two training phases in wd: in the ﬁrstphase (phase 1), we train the parameter generatorπ = {wi , wo, wl, w, b} to predict the studentnetwork s; in the second phase (phase 2), we ﬁne-tune the generated student network s to obtainbetter results.
phase 2 is necessary because theparameter generator is simply a feed-forward net-work with one hidden layer and thus has no enoughcapacity to produce a good enough student networkat once.
a more sophisticated parameter generatoris an alternative, but it is expensive due to its largeinput and output spaces..the task of phase 1 is to minimize the loss of thestudent network with parameters s predicted by theparameter generator π from the teacher parameters.
4 experiments.
4.1 datasets.
we evaluate our methods on the wmt16 english-roman (en-ro), nist12 chinese-english (zh-en),and wmt14 english-german (en-de) tasks..for the en-ro task, we use the wmt16 english-roman dataset (610k pairs).
we choose newsdev-2016 as the validation set and newstest-2016 as thetest set.
for the zh-en task, we use 1.8m sentencechinese-english bitext provided within nist12openmt1.
we choose the evaluation data of mt06as the validation set, and mt08 as the test set.
for the en-de task, we use the wmt14 english-german dataset (4.5m pairs).
we share the sourceand target vocabularies.
we choose newstest-2013as the validation set and newstest-2014 as the testset..for all datasets, we tokenize every sentence us-ing the script in the moses toolkit and segmentevery word into subword units using byte-pair en-coding (sennrich et al., 2016).
the number of thebpe merge operations is set to 32k.
we removesentences with more than 250 subword units (xiaoet al., 2012).
in addition, we evaluate the resultsusing multi-bleu.perl..1ldc2000t46,.
ldc2000t50,ldc2003e14, ldc2005t10, ldc2002e18, ldc2007t09,ldc2004t08.
ldc2000t47,.
2079+ kd+ wdsmall+ kd+ wdteachertiny.
system depth width5126teacher2561tiny2561256151225122512251262561256125615122512251225126256125612561512251225122.
+ kd+ wdsmall+ kd+ wdteachertiny.
+ kd+ wdsmall+ kd+ wd.
or-ne61tmw.ne-hz21tsin.ed-ne41tmw.test ∆bleu valid32.07-31.6429.73-29.6529.980.0030.0330.89+0.8630.8931.19-31.2230.770.0030.9731.27+0.6831.6551.91-45.1448.28-41.9049.710.0042.7851.56+1.8244.6050.83-44.3051.870.0044.8953.04+1.3146.2026.79-27.4724.88-24.6226.010.0026.5126.42+0.6127.1226.07-26.6826.540.0027.4726.97+0.7128.18.params.
speed.
83m 138.35 sent./s45m 323.26 sent./s45m 347.07 sent./s45m 359.53 sent./s66m 281.31 sent./s66m 289.11 sent./s66m 289.80 sent./s88.42 sent./s102m60m 225.46 sent./s60m 214.06 sent./s60m 247.90 sent./s85m 194.23 sent./s85m 199.74 sent./s85m 199.29 sent./s96m 158.29 sent./s55m 321.79 sent./s55m 412.91 sent./s55m 406.68 sent./s80m 281.97 sent./s80m 306.91 sent./s80m 309.11 sent./s.
speedup1.00×2.34×2.51×2.60×2.03×2.09×2.09×1.00×2.55×2.42×2.80×2.20×2.26×2.25×1.00×2.03×2.61×2.57×1.78×1.94×1.95×.
table 1: results of transformer-base on different tasks (sent./s: translated sentences per second)..4.2 model setup.
our baseline system is based on the open-sourceimplementation of the transformer model pre-sented in ott et al.
(2019)’s work.
for all ma-chine translation tasks, we experiment with thetransformer-base (base) setting.
we additionallyrun the transformer-big (big) (vaswani et al., 2017)and transformer-deep (deep) (wang et al., 2019;zhang et al., 2020) settings on the large en-dedataset.
all systems consist of a 6-layer encoderand a 6-layer decoder, except that the transformer-deep encoder has 48 layers (depth) (li et al.,2020).
the embedding size (width) is set to 512 fortransformer-base/deep and 1,024 for transformer-big.
the ffn hidden size equals to 4× embeddingsize in all settings.
we stop training until the modelstops improving on the validation set.
all exper-iments are done on 8 nvidia titian v gpuswith mixed-precision training (micikevicius et al.,2018).
at test time, the model is decoded with abeam of width 4/6/4, a length normalization weightof 1.0/1.0/0.6 and a batch size of 64 for the en-ro/zh-en/en-de tasks with half-precision..note that our method can also be seen as an ad-vanced version of tucker decomposition (tucker,1966).
so we also implement a baseline based on.
tucker decomposition.
unfortunately, this modeldoes not converge to a good optima and performsextremely poor..for the kd baseline, we adopt kim and rush(2016)’s method, which has proven to be the mosteffective for seq2seq models (kim et al., 2019).
it generates the pseudo data from the source sideof the bilingual corpus.
the choices of studentnetworks are based on the observation that the en-coder has a greater impact on performance and thedecoder dominates the decoding time (kasai et al.,2020).
therefore we vary the depth and width ofthe decoder.
we test two student network conﬁgu-rations: tiny halves the decoder width and usesa 1-layer decoder (the fastest wd student networkwith the performance close to the teacher network);small uses a 2-layer decoder whose width is thesame as the teacher network (the fastest kd studentnetwork with the performance close to the teachernetwork)..all hyper-parameters of wd are identical to thebaseline system, except that wd uses 1/4 warmupsteps in phase 2. for the parameter generator initial-ization, we use glorot and bengio (2010)’s methodto initialize wi , wo, wl in eqs.
3 - 5. w and bin eq.
6 are initialized to constants 1 and 0 respec-.
2080+ kd+ wdsmall+ kd+ wdteachertiny.
system depth width10246teacher5121tiny512151211024210242102425126256125612561512251225122.
+ kd+ wdsmall+ kd+ wd.
test ∆bleu valid27.66-29.1125.33-25.8326.520.0027.7026.83+0.9028.6026.78-27.6227.540.0029.0127.97+0.5129.5227.82-29.4326.05-26.3427.390.0029.3627.99+0.5629.9226.51-28.0628.020.0029.8328.33+0.9430.77.params.
speed.
281m 123.92 sent./s150m 353.42 sent./s150m 353.82 sent./s150m 364.67 sent./s214m 252.46 sent./s214m 261.78 sent./s214m 260.34 sent./s229m 134.26 sent./s187m 270.30 sent./s187m 308.57 sent./s187m 285.43 sent./s212m 245.82 sent./s212m 258.45 sent./s212m 252.69 sent./s.
speedup1.00×2.85×2.86×2.94×2.04×2.11×2.10×1.00×2.01×2.30×2.13×1.83×1.92×1.88×.
gib.peed.table 2: results of transformer-big/deep on wmt14 en-de (sent./s: translated sentences per second)..systemtiny (kd)+ init+ wd.
+ init+ wd.
test ∆bleu valid ∆bleu0.000.00 49.7142.7843.36 +0.58 50.32 +0.6144.60 +1.82 51.56 +1.850.000.00 51.8745.66 +0.77 52.57 +0.7046.20 +1.31 53.04 +1.17.
small (kd) 44.89.table 3: initialization study (init: initialize the studentnetwork with the teacher parameters)..tively.
all results are the average of three identicalruns with different random seeds..4.3 results.
table 1 shows the results of different approaches ondifferent student networks with transformer-baseas the teacher network.
in all three tasks and differ-ent sized student networks, wd outperforms kdby 0.77, 1.57, and 0.66 bleu points on en-ro, zh-en, and en-de on average.
our method (tiny) canobtain similar performance to the teacher networkwith only half of its parameters and is 2.57∼2.80×faster, while kd (small) uses more parametersand has only a 1.94∼2.26× speedup in the samecase.
we attribute the success of wd to that theparameter generator uses parameters of the teachernetwork to provide a good initialization for thestudent network, as phase 1 behaves like the ini-tialization, and the effectiveness of a good initial-ization has been widely proven (erhan et al., 2010;mishkin and matas, 2016).
interestingly, both kdand wd surpass the teacher network when the stu-.
dent network size is close to the teacher network(small).
this is due to that kd has a form similarto data augmentation (gordon and duh, 2019)..in table 1..table 2 shows the results of larger networks,i.e., transformer-big/deep.
the phenomenonthe ac-here is similar to thatceleration on transformer-big is more obviousthan on transformer-base (2.94× vs.2.57×for tiny and 2.10× vs. 1.95× for small inwd).
this is because the decoder in transformer-big occupies a larger portion of the decodingtime than in transformer-base.
but the acceler-ation on transformer-deep is less obvious than ontransformer-base (2.13× vs. 2.57× for tiny and1.88× vs. 1.95× for small in wd), as a deeperencoder consumes more inference time.
moreover,compared with such a strong transformer-deepteacher, wd (small) can still outperform it by1.34 bleu points with a 1.88× speedup, achievingthe state-of-the-art..5 analysis.
to better understand wd, we conduct a series ofexperiments on the nist12 zh-en validation setwith the transformer-base teacher..5.1.initialization study.
to test whether kd misses knowledge in param-eters, we initialize the student network with theteacher parameters.
if the teacher and student net-works have different depths, we initialize the stu-dent network with the bottom layers of the teachernetwork (sanh et al., 2019).
if they have different.
2081teacher.
student.
wd.
uelb.
53.
52.
51.
50.uelb.
50.
45.kd.
uelb.
53.
52.
51.
50.
120.
170.
220.speed (sentences/s).
1.
3.
5learning rate (×10−4).
7.
9.
1.
2.
3#warmup (×103).
4.
5.figure 3: sensitivity analysis on small..system.
test ∆bleu valid ∆bleu0.000.00 50.8344.30small44.89 +0.59 51.87 +1.04+ kd+ encoder45.40 +1.10 51.62 +0.7945.26 +0.96 51.34 +0.51+ decoder+ embed (enc) 44.67 +0.37 51.22 +0.39+ embed (dec) 45.06 +0.76 51.26 +0.43+ output45.10 +0.80 51.28 +0.45.
w.d.256.
512.bleukd/wd params bleukd/wd params65m38.46/40.3472m45.33/47.2180m47.30/49.0987m47.90/50.0848.87/50.7094m102m49.78/50.73.
30m 43.51/45.3932m 50.02/50.4534m 51.18/51.9936m 51.05/52.0538m 52.15/52.0040m 52.40/53.09.
123456.table 4: ablation study of using different weight matri-ces solely..table 5: compression study with various depth (d) andwidth (w) of both the encoder and decoder..widths, we slice the teacher weight matrices to ﬁtthe student network (wang et al., 2020).
table 3shows that initializing the student networks withthe teacher parameters improves kd, supportingour claim that knowledge in parameters is com-plementary to kd but missed.
we also see thatwd outperforms this simple initialization, whichimplies that using all teacher parameters helps toobtain a better student..5.2 sensitivity analysis.
the left part of fig.
3 studies how sensitive theperformance (bleu) of different methods are tovarious levels of inference speedup (obtained byvarying decoder depth and width).
it shows thatwd distributes on the upper right of the ﬁgure,which means that wd produces student networksthat are consistently faster and better..we also investigate how sensitive different meth-ods are to the training hyper-parameters, i.e., thelearning rate and warmup steps.
here we focuson phase 2 of wd, as it directly impacts the ﬁnalperformance.
the middle part of fig.
3 showsthat wd can endure learning rates in a wide range,because its performance does not vary much.
how-ever, a very large learning rate still negatively im-pacts the performance.
the right part of fig.
3is the opposite, where wd is more sensitive to.
the warmup steps than the learning rate.
this isbecause more warmup steps will run the networkwith a high learning rate in a longer period.
ahigh learning rate has been proven to be harmful asshown in the middle part of fig.
3..5.3 ablation study.
table 4 studies which weight matrices in theteacher network are the most effective.
it isachieved by training the parameter generator withonly the intended weight matrices and without thekd loss term in eq.
7. we see that using anyweight matrix brings a signiﬁcant improvementover the baseline.
this observation shows thatweight matrices in the teacher network do containabundant knowledge.
among these, the encoderweight matrices produce the most signiﬁcant result,which agrees with the previous study claiming thatthe encoder is more important than the decoder(wang et al., 2019; bapna et al., 2018)..5.4 compression study.
as the previous experiments focus on a lightweightdecoder for acceleration, the compression is lim-ited as the encoder remains large.
to examine theeffectiveness of wd on model compression, weshrink the depth and width of the encoder and de-coder simultaneously.
as shown in table 5, wdconsistently outperforms kd by about 1 bleu.
2082#epoch (phase 2).
bleu.
1 2 3 4 5 6 7 8 9 10.
6 related work.
6.1 knowledge distillation.)
1.esahp(.
hcope#.
12345678910.
52.0.
51.0.
50.0.figure 4: training efﬁciency of wd on small..point under various compression ratios (rangingfrom 1.00× to 3.40×).
note that decreasing thewidth brings more signiﬁcant compression.
this isbecause a large portion of the parameters is fromthe embedding matrices and the output projection.
the sizes of these matrices are determined by thewidth and a ﬁxed vocabulary size..5.5 training efﬁciency.
fig.
4 studies the training efﬁciency of wd by com-paring the ﬁnal bleu scores when two trainingphases end in different epochs.
as shown in fig.
4,phase 1 has little impact on phase 2, because phase2 converges to optimums with similar bleu scoresonce phase 1 runs for a few epochs (say, 3 epochs).
if we run phase 1 longer, then phase 2 convergesfaster.
this phenomenon suggests that phase 1 al-ready transfers the knowledge in the teacher param-eters within the ﬁrst few epochs, and the remainingepochs merely do the ﬁne-tuning (phase 2) job.
this implies that the training of wd is efﬁcient,since we can just train the parameter generator forseveral epochs ﬁrst, then ﬁne-tune the generatednetwork as in kd, and ﬁnally obtain a much betterresult than kd..though we could train the parameter generatorfor just a few epochs as suggested, phase 1 is stilltime-consuming.
the reasons are two folds: 1) theparameter generator consumes a lot of memory andwe have to resort to gradient accumulation; 2) theparameter generator involves many large matrixmultiplications.
for the experiments in table 1 andtable 2, it takes us 0.66 days for wd to ﬁnish train-ing on average, whereas 0.55 days for the teachernetwork baseline and 0.31 days for both the studentnetwork baseline and kd..knowledge distillation (hinton et al., 2015; freitaget al., 2017) is a widely used model accelerationand compression technique (jiao et al., 2019; sanhet al., 2019; liu et al., 2020).
it treats the networkpredictions as the knowledge learned by the teachernetwork, since these predicted distributions containthe ranking information on similarities among cat-egories.
it then transfers this knowledge to thestudent network by enforcing the student networkto have similar predictions.
the followed work ex-tends this idea by providing more knowledge fromdifferent sources to the student network.
fitnets(romero et al., 2015) uses not only the predictionsbut also the intermediate representations learnedby the teacher network to supervise the studentnetwork.
for the seq2seq model, kim and rush(2016) proposes to use the generated sequences asthe sequence-level knowledge to guide the studentnetwork training.
moreover, self-knowledge dis-tillation (hahn and choi, 2019) even shows thatknowledge (representations) from the student net-work itself can improve the performance..our weight distillation, on the other hand, ex-plores a new source of knowledge and a new way toleverage this knowledge.
it transfers the knowledgein parameters of the teacher network to the studentnetwork via a parameter generator.
therefore, it isorthogonal to other knowledge distillation variants..6.2 transfer learning.
transfer learning aims at transferring knowledgefrom a source domain to a target domain.
basedon what knowledge is transferred to the model inthe target domain, transfer learning methods canbe classiﬁed into three categories (pan and yang,2010): instance-based methods reuse certain partsof the data in the source domain (jiang and zhai,2007; dai et al., 2007); feature-based methods usethe representation from the model learned in thesource domain as the input (peters et al., 2018;gao et al., 2008); parameter-based methods directlyﬁne-tune the model learned in the source domainwith the target domain data (yang et al., 2019; liuet al., 2019; devlin et al., 2019)..perhaps the most related work is platanioset al.
(2018)’s work.
their method falls into theparameter-based category.
they use a universalparameter generator to share the knowledge amongtranslation tasks.
this parameter generator pro-.
2083duces a translation model from a given language-speciﬁc embedding.
though we similarly employthe idea of a parameter generator, our weight dis-tillation aims at transferring knowledge from onemodel to another rather than from one translationtask to another.
therefore our parameter genera-tor takes a model instead of a language-speciﬁcembedding as its input and is only used once..7 conclusion.
in this work, we propose weight distillation totransfer knowledge in the parameters of the teachernetwork to the student network.
it generates thestudent network from the teacher network via aparameter generator.
our experiments on three ma-chine translation tasks show that weight distillationconsistently outperforms knowledge distillation byproducing a faster and better student network..acknowledgments.
this work was supported in part by the nationalscience foundation of china (nos.
61876035and 61732005), the national key r&d programof china (no.
2019qy1801), and the ministryof science and technology of the prc (nos.
2019yff0303002 and 2020aaa0107900).
theauthors would like to thank anonymous reviewersfor their comments..references.
ankur bapna, mia chen, orhan firat, yuan cao,and yonghui wu.
2018. training deeper neuralmachine translation models with transparent atten-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 3028–3033, brussels, belgium.
associationfor computational linguistics..wenyuan dai, qiang yang, gui-rong xue, and yongin ma-yu.
2007. boosting for transfer learning.
chine learning, proceedings of the twenty-fourthinternational conference (icml 2007), corvallis,oregon, usa, june 20-24, 2007, volume 227 ofacm international conference proceeding series,pages 193–200.
acm..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..dumitru erhan, aaron c. courville, yoshua bengio,and pascal vincent.
2010. why does unsupervisedin proceedingspre-training help deep learning?
of the thirteenth international conference on artiﬁ-cial intelligence and statistics, aistats 2010, chialaguna resort, sardinia, italy, may 13-15, 2010,volume 9 of jmlr proceedings, pages 201–208.
jmlr.org..markus freitag, yaser al-onaizan, and baskaransankaran.
2017. ensemble distillation for neuralmachine translation.
corr, abs/1702.01802..jing gao, wei fan, jing jiang, and jiawei han.
2008.knowledge transfer via multiple model local struc-in proceedings of the 14th acmture mapping.
sigkdd international conference on knowledgediscovery and data mining, las vegas, nevada,usa, august 24-27, 2008, pages 283–291.
acm..xavier glorot and yoshua bengio.
2010. understand-ing the difﬁculty of training deep feedforward neu-in proceedings of the thirteenth in-ral networks.
ternational conference on artiﬁcial intelligence andstatistics, aistats 2010, chia laguna resort, sar-dinia, italy, may 13-15, 2010, volume 9 of jmlrproceedings, pages 249–256.
jmlr.org..mitchell a. gordon and kevin duh.
2019. explain-ing sequence-level knowledge distillation as data-augmentation for neural machine translation.
corr,abs/1912.03334..sangchul hahn and heeyoul choi.
2019..self-knowledge distillation in natural language process-ing.
in proceedings of the international conferenceon recent advances in natural language process-ing, ranlp 2019, varna, bulgaria, september 2-4,2019, pages 423–430.
incoma ltd..geoffrey e. hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
corr, abs/1503.02531..ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structure oflanguage?
in proceedings of the 57th conference ofthe association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 3651–3657.
associationfor computational linguistics..jing jiang and chengxiang zhai.
2007..instancein aclweighting for domain adaptation in nlp.
2007, proceedings of the 45th annual meeting of theassociation for computational linguistics, june 23-30, 2007, prague, czech republic.
the associationfor computational linguistics..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2019. tinybert: distilling bert for natural lan-guage understanding.
corr, abs/1909.10351..2084jungo kasai, nikolaos pappas, hao peng, james cross,and noah a. smith.
2020. deep encoder, shallowdecoder: reevaluating the speed-quality tradeoff inmachine translation.
corr, abs/2006.10369..yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in naturallanguage processing, emnlp 2016, austin, texas,usa, november 1-4, 2016, pages 1317–1327.
theassociation for computational linguistics..young jin kim, marcin junczys-dowmunt, hanyhassan, alham fikri aji, kenneth heaﬁeld, ro-man grundkiewicz, and nikolay bogoychev.
2019.from research to production and back: ludicrouslyin proceedingsfast neural machine translation.
of the 3rd workshop on neural generation andtranslation@emnlp-ijcnlp 2019, hong kong,november 4, 2019, pages 280–288.
association forcomputational linguistics..bei li, ziyang wang, hui liu, quan du, tong xiao,chunliang zhang, and jingbo zhu.
2020. learn-ing light-weight translation models from deep trans-former.
corr, abs/2012.13866..weijie liu, peng zhou, zhe zhao, zhiruo wang,haotang deng, and qi ju.
2020. fastbert: a self-distilling bert with adaptive inference time.
corr,abs/2004.02178..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..paulius micikevicius, sharan narang, jonah alben,gregory f. diamos, erich elsen, david garc´ıa,boris ginsburg, michael houston, oleksii kuchaiev,ganesh venkatesh, and hao wu.
2018. mixed pre-cision training.
in 6th international conference onlearning representations, iclr 2018, vancouver,bc, canada, april 30 - may 3, 2018, conferencetrack proceedings.
openreview.net..dmytro mishkin and jiri matas.
2016. all you needin 4th international conference onis a good init.
learning representations, iclr 2016, san juan,puerto rico, may 2-4, 2016, conference track pro-ceedings..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, naacl-hlt 2019,minneapolis, mn, usa, june 2-7, 2019, demonstra-tions, pages 48–53.
association for computationallinguistics..sinno jialin pan and qiang yang.
2010. a survey onieee trans.
knowl.
data eng.,.
transfer learning.
22(10):1345–1359..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, naacl-hlt 2018, new or-leans, louisiana, usa, june 1-6, 2018, volume 1(long papers), pages 2227–2237.
association forcomputational linguistics..emmanouil antonios platanios, mrinmaya sachan,graham neubig, and tom m. mitchell.
2018. con-textual parameter generation for universal neural ma-chine translation.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, brussels, belgium, october 31 - novem-ber 4, 2018, pages 425–435.
association for com-putational linguistics..adriana romero, nicolas ballas, samira ebrahimi ka-hou, antoine chassang, carlo gatta, and yoshuabengio.
2015. fitnets: hints for thin deep nets.
in3rd international conference on learning represen-tations, iclr 2015, san diego, ca, usa, may 7-9,2015, conference track proceedings..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled version ofbert: smaller, faster, cheaper and lighter.
corr..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics, acl 2016, august 7-12, 2016, berlin, ger-many, volume 1: long papers.
the association forcomputer linguistics..ledyard r tucker.
1966. some mathematical notespsychometrika,.
on three-mode factor analysis.
31(3):279–311..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998–6008..hanrui wang, zhanghao wu, zhijian liu, han cai,ligeng zhu, chuang gan, and song han.
2020. hat:hardware-aware transformers for efﬁcient naturallanguage processing..qiang wang, bei li, tong xiao,.
jingbo zhu,changliang li, derek f. wong, and lidia s. chao.
2019. learning deep transformer models for ma-chine translation.
in proceedings of the 57th confer-ence of the association for computational linguis-tics, acl 2019, florence, italy, july 28- august 2,.
20852019, volume 1: long papers, pages 1810–1822.
as-sociation for computational linguistics..tong xiao, jingbo zhu, hao zhang, and qiang li.
2012. niutrans: an open source toolkit for phrase-based and syntax-based machine translation.
in the50th annual meeting of the association for com-putational linguistics, proceedings of the systemdemonstrations, july 10, 2012, jeju island, korea,pages 19–24.
the association for computer lin-guistics..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, 8-14 december 2019, vancou-ver, bc, canada, pages 5754–5764..yuhao zhang, ziyang wang, runzhe cao, binghaowei, weiqiao shan, shuhan zhou, abudurexiti re-heman, tao zhou, xin zeng, laohu wang, yongyumu, jingnan zhang, xiaoqian liu, xuanjun zhou,yinqiao li, bei li, tong xiao, and jingbo zhu.
2020. the niutrans machine translation systems forwmt20.
in proceedings of the fifth conference onmachine translation, wmt@emnlp 2020, online,november 19-20, 2020, pages 338–345.
associationfor computational linguistics..a appendices.
hyper-parameters.
we tune the learning rate andwarmup steps in phase 2 of wd.
we use the gridsearch to select the learning rate in [1 × 10−4, 3 ×10−4, 5 × 10−4, 7 × 10−4, 9 × 10−4] and warmupsteps in [1000, 2000, 3000, 4000, 5000] that havethe best average bleu performance in all valida-tion sets.
datasets.
detailed data statistics as well asthe urls of three machine translation tasks weused, including wmt16 english-roman (en-ro),nist12 chinese-english (zh-en), and wmt14english-german (en-de), are shown in table 6..for en-ro, the training set consists of 0.6m bilin-gual sentence pairs.
the validation set newsdev-2016 contains 1999 pairs and the test set newstest-2016 contains 1999 pairs.
for zh-en, the trainingset consists of 1.8m bilingual sentence pairs.
thevalidation set mt06 contains 1,664 pairs and the testset mt08 contains 1,357 pairs.
for en-de, the train-ing set consists of 4.5m bilingual sentence pairs.
the validation set newstest-2013 contains 3,000pairs and the test set newstest-2014 contains 3,003pairs.
runtime.
to compare the average runtime foreach approach, table 7 shows the actual number of.
train.
lang..testsent.
word sent.
word sent.
worden-ro 0.6m 33m 1999 112k 1999 118kzh-en 1.8m 115m 1357 247k 1664 280ken-de 4.5m 262m 3003 164k 3000 156k.
valid.
table 6: date statistics..updates and runtime.
for the baseline models (i.e.,teacher, tiny and small) and kd, we recordtheir runtime in the phase 1 entry because theyonly need to be trained once..one can observe that in table 7, phase 2 of wdgenerally consumes similar or less time as wellas the number of updates than other approaches.
this is because the model is already close to theoptimum before the ﬁne-tuning (phase 2).
table 7also shows that the number of updates in phase 1of wd is much less than other approaches, yet itstraining time is much longer.
this phenomenon ismore obvious in transformer-deep models.
this isbecause one step in phase 1 of wd is 2.11× slowerthan in phase 2 of wd.
decoder.
we also investigate how wd’s perfor-mance (on the validation set) and speed changegiven different decoder depths and widths.
wechoose the speed of wd to compute the speedup ofdifferent decoder depths and widths.
although theactual speedup of kd will not be exactly the sameas the one of wd due to their different decodingresults, they are close..as shown in table 8, wd is robust to differentsized decoders, with both bleu and speed signif-icantly outperform kd.
wd consistently outper-forms kd by about 1 bleu point under various de-coder depths and widths.
interestingly, we ﬁnd thatpruning the layers degrades the performance morethan shrinking its width, but it provides a higherspeedup.
taking the student network with depth2 and width 512 as an example, if we shrink thedepth from 2 to 1, there is a decrease of 1.21 bleupoints in wd but with 1.12× speedup.
when weshrink the width from 512 to 256, it leads to a mod-erate decrease of 0.59 bleu points yet with only1.06× speedup.
this might be because layers arecomputed sequentially and wider matrices enjoythe parallel computation acceleration provided bymodern gpus.
loss.
in table 7, we observe that wd generatesstudent networks that are superior to kd.
we be-lieve that this is because wd converges to a better.
2086system.
depth width.
test.
valid.
or-ne61tmw.ne-hz21tsin.ed-ne41tmw.teacher (base)tiny.
+ kd+ wdsmall+ kd+ wd.
+ kd+ wdsmall+ kd+ wd.
teacher (base)tiny.
teacher (base)tiny.
+ kd+ wdsmall+ kd+ wdteacher (big)tiny.
teacher (deep)tiny.
+ kd+ wdsmall+ kd+ wd.
+ kd+ wdsmall+ kd+ wd.
61112226111222611122261112226111222.
5122562562565125125125122562562565125125125122562562565125125121024512512512102410241024512256256256512512512.
31.6429.6530.0330.8931.2230.9731.6545.1441.9042.7844.6044.3044.8946.2027.4724.6226.5127.1226.6827.4728.1829.1125.8327.7028.6027.6229.0129.5229.4326.3429.3629.9228.0629.8330.77.
32.0729.7329.9830.8931.1930.7731.2751.9148.2849.7151.5650.8351.8753.0426.7924.8826.0126.4226.0726.5426.9727.6625.3326.5226.8326.7827.5427.9727.8226.0527.3927.9926.5128.0228.33.phase 1#update time70k 0.0670k 0.0370k 0.0347k 0.0470k 0.0470k 0.0447k 0.0630k 0.0830k 0.0530k 0.0520k 0.0730k 0.0630k 0.0620k 0.09100k 0.24100k 0.14100k 0.1450k 0.18100k 0.19100k 0.1950k 0.25200k 1.71200k 0.58200k 0.5867k 0.57200k 0.79200k 0.7967k 0.5560k 0.6760k 0.5760k 0.5730k 1.5160k 0.6060k 0.6030k 1.53.phase 2#update time---.
---.
70k 0.03.
--.
---.
--.
---.
--.
---.
--.
---.
--.
--.
---.
--.
---.
--.
---.
--.
---.
--.
70k 0.04.
30k 0.05.
30k 0.06.
80k 0.11.
80k 0.15.
100k 0.29.
100k 0.40.
30k 0.29.
30k 0.30.table 7: results of transformer on different tasks (time is measured by gpu days)..w.d.123456.
256wd ∆bleu+1.8551.56+1.2052.45+0.9752.49+1.0152.42+1.4452.71+1.8652.65.kd49.7151.2551.5251.4151.2750.79.speedup2.80×2.12×1.78×1.62×1.33×1.18×.
kd50.8951.8752.4652.0752.0751.91.
512wd ∆bleu+0.9451.83+1.1753.04+0.3552.81+1.5953.66+0.6752.74+1.1853.09.speedup2.53×2.25×1.68×1.56×1.30×1.02×.
table 8: bleu and speed vs. decoder depth and width (transformer-base, nist12 zh-en)..2087ssol.5.
4.kdwd (phase 1)wd (phase 2).
2.
5.
10.
15.
20#update (×103).
25.
30.figure 5: train (solid)/valid (dash) loss of small..optimum.
to examine this hypothesis, we study itsloss in fig.
5. as can be seen, wd does obtainmuch lower train and valid losses than kd.
we alsosee that phase 1 already outperforms kd at the end.
given the fact that phase 1 does the initializationjob for phase 2 and phase 2 is kd exactly, the waywd works can be treated as providing a good start..2088