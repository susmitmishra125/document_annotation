annotating online misogyny.
philine zeinertit university of copenhagendenmarkphze@itu.dk.
nanna inieit university of copenhagendenmarknans@itu.dk.
leon derczynskiit university of copenhagendenmarkleod@itu.dk.
abstract.
online misogyny, a category of online abusivelanguage, has serious and harmful social con-sequences.
automatic detection of misogynis-tic language online, while imperative, posescomplicated challenges to both data gathering,data annotation, and bias mitigation, as thistype of data is linguistically complex and di-verse.
this paper makes three contributionsin this area: firstly, we describe the detaileddesign of our iterative annotation process andcodebook.
secondly, we present a comprehen-sive taxonomy of labels for annotating misog-yny in natural written language, and ﬁnally, weintroduce a high-quality dataset of annotatedposts sampled from social media posts..1.introduction.
abusive language is a phenomenon with seriousconsequences for its victims, and misogyny isno exception.
according to a 2017 report fromamnesty international, 23% of women from eightdifferent countries have experienced online abuseor harassment at least once, and 41% of these saidthat on at least one occasion, these online experi-ences made them feel that their physical safety wasthreatened (amnesty international, 2017)..automatic detection of abusive language canhelp identify and report harmful accounts and acts,and allows counter narratives (chung et al., 2019;garland et al., 2020; ziems et al., 2020).
due tothe volume of online text and the mental impacton humans who are employed to moderate onlineabusive language - moderators of abusive onlinecontent have been shown to develop serious ptsdand depressive symptoms (casey newton, 2020) -it is urgent to develop systems to automate the de-tection and moderation of online abusive language.
automatic detection, however, presents signiﬁcantchallenges (vidgen et al., 2019)..abusive language is linguistically diverse (vid-gen and derczynski, 2020), both explicitly, in theform of swear words or profanities; implicitly, inthe form of sarcasm or humor (waseem et al.,2017); and subtly, in the form of attitudes and opin-ions.
recognizing distinctions between variants ofmisogyny is challenging for humans, let alone com-puters.
systems for automatic detection are usuallycreated using labeled training data (kiritchenkoet al., 2020), hence, their performance dependson the quality and representativity of the availabledatasets and their labels.
we currently lack trans-parent methods for how to create diverse datasets.
when abusive language is annotated, classes are of-ten created based on each unique dataset (a purelyinductive approach), rather than taking advantageof general, established terminology from, for in-stance, social science or psychology (a deductiveapproach, building on existing research).
thismakes classiﬁcation scores difﬁcult to compare andapply across diverse training datasets..this paper investigates the research question:how might we design a comprehensive annotationprocess which results in high quality data for au-tomatically detecting misogyny?
we make threenovel contributions: 1. methodology: we describeour iterative approach to the annotation process ina transparent way which allows for a higher degreeof comparability with similar research.
2. model:we present a taxonomy and annotation codebookgrounded in previous research on automatic detec-tion of misogyny as well as social science termi-nology.
3. dataset: we present a new, annotatedcorpus of danish social media posts, bajer,1 an-notated for misogyny, including analysis of classbalance, word frequencies, inter-annotator agree-ment (iaa), annotation errors, and classiﬁcationbaseline..1https://github.com/phze22/online-misogyny-in-danish-bajer.
since research has indicated that misogynypresents differently across languages, and, likely,cultures (anzovino et al., 2018), an additional con-tribution of this work is that it presents a datasetof misogyny in danish, a north germanic lan-guage, spoken by only six million people, andindeed the ﬁrst work of its kind in any scandina-vian/nordic culture to our knowledge.
in denmarkan increasing proportion of people refrain from on-line discourse due to the harsh tone, with 68% ofsocial media users self-excluding in 2021 (anal-yse & tal, 2021; andersen and langberg, 2021),making this study contextually relevant.
further,the lack of language resources available for dan-ish (kirkedal et al., 2019) coupled with its lexicalcomplexity (bleses et al., 2008) make it an intricateresearch objective for natural language processing..2 background and related work.
abusive language is as ancient a phenomenon aswritten language itself.
written profanities and in-sults about others are found as old as grafﬁti on ru-ins from the roman empire (wallace, 2005).
auto-matic processing of abusive text is far more recent,early work including e.g.
davidson et al.
(2017)and waseem et al.
(2017).
research in this ﬁeldhas produced both data, taxonomies, and methodsfor detecting and deﬁning abuse, but there exists noobjective framing for what constitutes abuse andwhat does not.
in this work, we focus on a speciﬁccategory of online abuse, namely misogyny..2.1 online misogyny and existing datasets.
misogyny can be categorised as a subbranch of hatespeech and is described as hateful content targetingwomen (waseem, 2016).
the degree of toxicitydepends on complicated subjective measures, forinstance, the receiver’s perception of the dialect ofthe speaker (sap et al., 2019)..annotating misogyny typically requires morethan a binary present/absent label.
chiril et al.
(2020), for instance, use three categories to classifymisogyny in french: direct sexist content (directlyaddressed to a woman or a group of women), de-scriptive sexist content (describing a woman orwomen in general) or reporting sexist content (areport of a sexism experience or a denunciation ofa sexist behaviour).
this categorization does not,however, specify the type of misogyny..jha and mamidi (2017) distinguish betweenharsh and benevolvent sexism, building on the data.
from the work of waseem and hovy (2016).
whileharsh sexism (hateful or negative views of women)is the more recognized type of sexism, benevo-lent sexism (“a subjectively positive view towardsmen or women”), often exempliﬁed as a compli-ment using a positive stereotypical picture, is stilldiscriminating (glick and fiske, 1996).
other cat-egorisations of harassment towards women havedistinguished between physical, sexual and indirectoccurrences (shariﬁrad and jacovi, 2019)..anzovino et al.
(2018) classify misogyny moresegregated in ﬁve subcategories: discredit, harass-ment & threats of violence, derailing, stereotype& objectiﬁcation, and dominance.
they also dis-tinguish between if the abuse is active or passivetowards the target.
these labels appear to applywell to other languages, and quantitative represen-tation of labels differ by language.
for example,spanish shows a stronger presence of dominance,italian of stereotype & objectiﬁcation, and englishof discredit.
as we see variance across languages,building terminology for labeling misogyny cor-rectly is therefore a key challenge in being able todetect it automatically.
parikh et al.
(2019) takea multi-label approach to categorizing posts fromthe “everyday sexism project”, where as manyas 23 different categories are not mutually exclu-sive.
the types of sexism identiﬁed in their datasetinclude body shaming, gaslighting, and mansplain-ing.
while the categories of this work are extremelydetailed and socially useful, several studies havedemonstrated the challenge for human annotatorsto use labels that are intuitively unclear (chatzakouet al., 2017; vidgen et al., 2019) or closely relatedto each other (founta et al., 2018)..guest et al.
(2021) suggest a novel taxonomy formisogyny labeling applied to a corpus of primarilyenglish reddit posts.
based on previous research,including anzovino et al.
(2018), they present thefollowing four overarching categories of misog-yny: (i) misogynistic pejoratives, (ii) descriptionsof misogynistic treatment, (iii) acts of misogynis-tic derogation and (iv) gendered personal attacksagainst women..the current work combines previous categoriza-tions on misogyny into a taxonomy which is usefulfor annotation of misogyny in all languages, whilebeing transparent about the construction of thistaxonomy.
our work builds on the previous workpresented in this section, continuous discussionsamong the annotators, and the addition of social.
science terminology to create a single-label tax-onomy of misogyny as identiﬁed in danish socialmedia posts across various platforms..3 methodology and dataset creation.
the creation of quality datasets involves a chain ofmethodological decisions.
in this section, we willpresent the rationale of creating our dataset underthree headlines: dataset, annotation process, andmitigating biases..3.1 dataset: online misogyny in social media.
bender and friedman (2018) present a set of datastatements for nlp which help “alleviate issues re-lated to exclusion and bias in language technology,lead[ing] to better precision in claims about hownatural language processing research can general-ize and thus better engineering results”..data statements are a characterization of adataset which provides context to others to under-stand how experimental results might generalizeand what biases might be reﬂected in systems builton the software.
we present our data statements forthe dataset creation in the following:.
curation rationale: random sampling of textoften results in scarcity of examples of speciﬁcallymisogynistic content (e.g.
(wulczyn et al., 2017;founta et al., 2018)).
therefore, we used the com-mon alternative of collecting data by using pre-deﬁned keywords with a potentially high search hit(e.g.
waseem and hovy (2016)), and identifyingrelevant user-proﬁles (e.g.
(anzovino et al., 2018))and related topics (e.g.
(kumar et al., 2018))..we searched for keyword (speciﬁc slurs, hash-tags), that are known to occur in sexist posts.
thesewere deﬁned by previous work, a slur list fromreddit, and from interviews and surveys of onlinemisogyny among women.
we also searched forbroader terms like “sex” or “women”, which donot appear exclusively in a misogynistic context,for example in the topic search, where we gatheredrelevant posts and their comments from the socialmedia pages of public media.
a complete list ofkeywords can be found in the appendix..social media provides a potentially biased, butbroad snapshot of online human discourse, withplenty of language and behaviours represented.
fol-lowing best practice guidelines (vidgen and der-czynski, 2020), we sampled from a language forwhich there are no existing annotations of the targetphenomenon: danish..different social media platforms attract differ-ent user groups and can exhibit domain-speciﬁclanguage (karan and ˇsnajder, 2018).
rather thanchoosing one platform (existing misogyny datasetsare primarily based on twitter and reddit (guestet al., 2021)), we sampled from multiple platforms:statista (2020) shows that the platform where mostdanish users are present is facebook, followedby twitter, youtube, instagram and lastly, reddit.
the dataset was sampled from twitter, facebookand reddit posts as plain text..language variety: danish, bcp-47: da-dk..text characteristics: danish colloquial webspeech.
posts, comments, retweets: max.
length512, average length: 161 characters..speaker demographics: social media users,age/gender/race unknown/mixed..speech situation:cussions..interactive, social media dis-.
annotator demographics: we recruited anno-tators aiming speciﬁcally for diversity in gender,age, occupation/ background (linguistic and ethno-graphic knowledge), region (spoken dialects) aswell as an additional facilitator with a backgroundin ethnography to lead initial discussions (see table1).
annotators were appointed as full-time employ-ees with full standard beneﬁts..gender:age:ethnicity:.
6 female, 2 male (8 total)5 <30; 3 ≥305 danish: 1 persian, 1 arabic, 1polishlinguistics (2);.
study/occupation: health/software design;.
ethnography/digital design;communication/psychology;anthropology/broadcastmoderator;ethnography/climate change;film artist.
table 1: annotators/facilitator demographicsall annotators were involved during the whole project period..3.2 annotation process.
in annotating our dataset, we built on the matterframework (pustejovsky and stubbs, 2012) and usethe variation presented by finlayson and erjavec(2017) (the maler framework), where the train.
& test stages are replaced by leveraging of an-notations for one’s particular goal, in our case thecreation of a comprehensive taxonomy..users (wiegand et al., 2019), domain (wiegandet al., 2019), time (florio et al., 2020) and lack oflinguistic variety (vidgen and derczynski, 2020)..we created a set of guidelines for the annotators.
the annotators were ﬁrst asked to read the guide-lines and individually annotate about 150 differentposts, after which there was a shared discussion.
after this pilot round, the volume of samples per an-notator was increased and every sample labeled by2-3 annotators.
when instances were ‘ﬂagged’ orannotators disagreed on them, they were discussedduring weekly meetings, and misunderstandingswere resolved together with the external facilita-tor.
after round three, when reaching 7k annotatedposts (figure 2), we continued with independentannotations maintaining a 15% instance overlapbetween randomly picked annotator pairs..management of annotator disagreement is an im-portant part of the process design.
disagreementscan be solved by majority voting (davidson et al.,2017; wiegand et al., 2019), labeled as abuse if atleast one annotator has labeled it (golbeck et al.,2017) or by a third objective instance (gao andhuang, 2017).
most datasets use crowdsourcingplatforms or a few academic experts for annotation(vidgen and derczynski, 2020).
inter-annotator-agreement (iaa) and classiﬁcation performanceare established as two grounded evaluation mea-surements for annotation quality (vidgen and der-czynski, 2020).
comparing the performance of am-ateur annotators (while providing guidelines) withexpert annotators for sexism and racism annotation,waseem (2016) show that the quality of amateurannotators is competitive with expert annotationswhen several amateurs agree.
facing the trade-offbetween training annotators intensely and the num-ber of involved annotators, we continued with thetrained annotators and group discussions/ individ-ual revisions for ﬂagged content and disagreements(section 5.4)..3.3 mitigating biases.
prior work demonstrates that biases in datasetscan occur through the training and selection ofannotators or selection of posts to annotate (gevaet al., 2019; wiegand et al., 2019; sap et al., 2019;al kuwatly et al., 2020; ousidhoum et al., 2020)..selection biases: selection biases for abusivelanguage can be seen in the sampling of text, for in-stance when using keyword search (wiegand et al.,2019), topic dependency (ousidhoum et al., 2020),.
label biases: label biases can be caused by, forinstance, non-representative annotator selection,lack in training/domain expertise, preconceivednotions, or pre-held stereotypes.
these biases aretreated in relation to abusive language datasetsby several sources, e.g.
general sampling andannotators biases (waseem, 2016; al kuwatlyet al., 2020), biases towards minority identitymentions based for example on gender or race(davidson et al., 2017; dixon et al., 2018; parket al., 2018; davidson et al., 2019), and politicalannotator biases (wich et al., 2020).
other quali-tative biases comprise, for instance, demographicbias, over-generalization, topic exposure as socialbiases (hovy and spruit, 2016)..systematic measurement of biases in datasetsremains an open research problem.
friedman andnissenbaum (1996) discuss “freedom from biases”as an ideal for good computer systems, and statethat methods applied during data creation inﬂu-ence the quality of the resulting dataset qualitywith which systems are later trained.
shah et al.
(2020) showed that half of biases are caused bythe methodology design, and presented a ﬁrst ap-proach of classifying a broad range of predictivebiases under one umbrella in nlp..we applied several measures to mitigate biasesoccurring through the annotation design and execu-tion: first, we selected labels grounded in existing,peer-reviewed research from more than one ﬁeld.
second, we aimed for diversity in annotator proﬁlesin terms of age, gender, dialect, and background.
third, we recruited a facilitator with a backgroundin ethnographic studies and provided intense anno-tator training.
fourth, we engaged in weekly groupdiscussions, iteratively improving the codebookand integrating edge cases.
fifth, the selection ofplatforms from which we sampled data is based onlocal user representation in denmark, rather thanconvenience.
sixth, diverse sampling methods fordata collection reduced selection biases..4 a taxonomy and codebook for labeling.
online misogyny.
good language taxonomies systematically bringtogether deﬁnitions and describe general principlesof each deﬁnition.
the purpose is categorizing.
referencezampieri et al.
(2019).
abusivelanguage.
lang.
da,en,gr,ar,tu.
labelsoffensive (off)/not offensive (not)targeted insult (tin)/untargeted (unt)/individual (ind)/group (grp)/other (oth)sexism, racism.
hate speech waseem and hovy (2016)misogyny.
anzovino et al.
(2018).
enen,it,es discredit, stereotype, objectiﬁcation,.
jha and mamidi (2017).
en.
sexual harassm., dominance, derailingbenevolent extension.
table 2: established taxonomies and their use for the misogyny detection task.
and mapping entities in a way that demonstratestheir natural relationship, e.g.
schmidt and wie-gand (2017); anzovino et al.
(2018); zampieri et al.
(2019); banko et al.
(2020).
their application isespecially clear in shared tasks, as for multilingualsexism detection against women, semeval 2019(basile et al., 2019)..on one hand, it should be an aim of a taxon-omy that it is easily understandable and applicablefor annotators from various background and withdifferent expertise levels.
on the other hand, ataxonomy is only useful if it is also correct andcomprehensive, i.e.
a good representation of theworld.
therefore, we have aimed to integrate deﬁ-nitions from several sources of previous research(deductive approach) as well as categories result-ing from discussions of the concrete data (inductiveapproach)..our taxonomy for misogyny is the product of (a)existing research in online abusive language andmisogyny (speciﬁcally the work in table 2), (b) areview of misogyny in the context of online plat-forms and online platforms in a danish context (c)iterative adjustments during the process includingdiscussions between the authors and annotators..the labeling scheme (figure 1) is the mainstructure for guidelines for the annotators, while acodebook ensured common understanding of thelabel descriptions.
the codebook provided the an-notators with deﬁnitions from the combined tax-onomies.
the descriptions were adjusted to dis-tinguish edge-cases during the weekly discussionrounds..(2) target.
the taxonomy has four levels:.
(1) abu-sive (abusive/not abusive),(indi-vidual/group/others/untargeted), (3) group type(racism/misogyny/others),(4) misogyny type(harassment/discredit/stereotype & objectiﬁca-tion/dominance/neosexism/benevolent).
to demon-strate the relationship of misogyny to other in-.
stances of abusive language, our taxonomy embedsmisogyny as a subcategory of abusive language.
misogyny is distinguished from, for instance, per-sonal attacks, which is closer to the abusive lan-guage of cyberbullying.
for deﬁnitions and ex-amples from the dataset to the categories, see ap-pendix a.1.
we build on the taxonomy suggestedin zampieri et al.
(2019), which has been applied todatasets in several languages as well as in semeval(zampieri et al., 2020).
while parikh et al.
(2019)provide a rich collection of sexism categories, mul-tiple, overlapping labels do not fulﬁll the purpose ofbeing easily understandable and applicable for an-notators.
the taxonomies in anzovino et al.
(2018)and jha and mamidi (2017) have proved their ap-plication to english, italian and spanish, and of-fer more general labels.
some labels from previ-ous work were removed from the labeling schemeduring the weekly discussions among authors andannotators, (for instance derailing), because no in-stances of them were found in the data..4.1 misogyny: neosexism.
during our analysis of misogyny in the danishcontext (b), we became aware of the term “neosex-ism”.
neosexism is a concept deﬁned in tougaset al.
(1999), and presents as the belief that womenhave already achieved equality, and that discrimi-nation of women does not exist.
neosexism is basedon covert sexist beliefs, which can “go unnoticed,disappearing into the cultural norms.
those whoconsider themselves supporters of women’s rightsmay maintain non-traditional gender roles, but alsoexhibit subtle sexist beliefs” (martinez et al., 2010).
sexism in denmark appear to correlate with themodern sexism scale (skewes et al., 2019; tougaset al., 1995; swim et al., 1995; campbell et al.,1997).
neosexism was added to the taxonomy be-fore annotation began, and as we will see in theanalysis section, neosexism was the most common.
figure 1: a labeling scheme for online misogyny (blue) embedded within the taxonomy for labeling abusivelanguage occurrences (green).
deﬁnitions and examples can be found in the compressed codebook in a.1.
form of misogyny present in our dataset (figure 1).
here follow some examples of neosexism from ourdataset:.
• resenting complaints about discrimination:“i often feel that people have treated me betterand spoken nicer to me because i was a girl,so i have a hard time taking it seriously whenpeople think that women are so discriminatedagainst in the western world.”.
• questioning the existence of discrimination:“can you point to research showing that child-birth is the reason why mothers miss out onpromotions?”.
• presenting men as victims: “classic.
if it’s adisadvantage for women it’s the fault of soci-ety.
if men, then it must be their own.
sexismthrives on the feminist wing.”.
neosexism is an implicit form of misogyny, whichis reﬂected in annotation challenges summarisedin section 5.5. in prior taxonomies, instances ofneosexism would most likely have been assigned tothe implicit appearances of misogynistic treatment(ii) (guest et al., 2021) – or perhaps not classiﬁed asmisogyny at all.
neosexism is most closely relatedto the deﬁnition “disrespectful actions, suggestingor stating that women should be controlled in someway, especially by men”.
this deﬁnition, however,does not describe the direct denial that misogynyexists.
without a distinct and explicit neosexismcategory, however, these phenomena may be mixedup or even ignored..the taxonomy follows the suggestions of vid-gen et al.
(2019) for establishing unifying tax-.
onomies in abusive language while integratingcontext-related occurrences.
a similar idea isdemonstrated in mulki and ghanem (2021), addingdamning as an occurrence of misogyny in an ara-bic context.
while most of previous research isdone in english, these language-speciﬁc ﬁndingshighlight the need for taxonomies that are ﬂexibleto different contexts, i.e.
they are good represen-tations of the world.
lastly, from an nlp pointof view, languages with less resources for trainingdata can proﬁt further from transfer learning withsimilar labels, as demonstrated in pamungkas et al.
(2020) for misogyny detection..5 results and analysis.
5.1 class balance.
the ﬁnal dataset contains 27.9k comments, ofwhich 7.5k contain abusive language.
misogy-nistic posts comprise 7% of overall posts.
neosex-ism is by far the most frequently represented classwith 1.3k tagged posts, while discredit and stereo-type & objectiﬁcation are present in 0.3k and 0.2kposts.
benevolent, dominance, and harrassmentare tagged in between only 45 and 70 posts..5.2 domain/sampling representation.
most posts tagged as abusive and/or containingmisogyny are retrieved from searches on posts frompublic media proﬁles, see table 3. facebook andtwitter are equally represented, while reddit is inthe minority.
reddit posts were sampled from anavailable historical collection..notsexual harassmentmisogynyneosexismdiscreditstereotypes & objectificationbenevolent sexismdominancedisgrace/ humiliate womenwith no larger intentasking for sexual favours, unwanted sexualisationadvocating superioritypositive, gender-typical sentiment, often disguised as a complimentnormative held but fixed,oversimplified gender imagesdenial of discrimination/ resentment of complaintstowards women1.3k0.3k0.2k0.07k0.06k0.05kbecause of ethnicityabusiveindividualprofanity/ swearingfollowing 11-point-list after waseem & hovy (2016)person-targeted, e.g.
cyberbullyinggroup-targeted,i.e.
hate speechothersgroupe.g.
conceptual against the media, a political partyracismuntargetedotherse.g.
lgtb, towards men3k2k0.5k0.1k2.6k1k0.8k20.4k7.5ksamp.
domain.
time.
dis.
dom.
topic facebook 48% 07-.dis.
dis.
abs.
⊂⊂inkmisabus12,3 51% 63%.
keyw.
twitter.
45% 08-.
7,8.
32% 27%.
usertwitterkeyw.
reddit.
3,62,4.
8% 6%7% 2%.
11/20.
12/20.
7% 02-.
04/19.
popul.
facebook.
1.
2% 2%.
table 3: distribution sampling techniques and domainssampling techniques: topic = posts from public media sitesand comments to these posts; keyw.
=keyword/hashtag-search; popul.
= most interactions..5.3 word counts.
frequencies of the words; ‘kvinder’ (women) and‘mænd’ (men) were the highest, but these words didnot represent strong polarities towards abusive andmisogynistic content (table 4).
the word ‘user’represents de-identiﬁed references to discussionparticipants (“@user”)..dataset(kvinder,0.29)(user, 0.29)(metoo, 0.25)(mænd, 0.21)(bare, 0.16).
⊂ abus(kvinder,0.34)(user, 0.25)(mænd, 0.22)(bare, 0.17)(metoo, 0.16).
⊂ mis(kvinder,0.41)(mænd, 0.28)(user, 0.18)(˚ar, 0.16)(n˚ar, 0.15).
table 4: top-3 word frequenciestf-idf scores with prior removal of special character and stop-words, notion:(token, tf-idf).
5.4.inter-annotator agreement (iaa).
figure 2: inter-annotator-agreementy-axis: agreement by rel.
overlap of label-sequences persample; x-axis: annotated data samples in k..we measure iaa using the agreement between3 annotators for each instance until round 3 (7kposts), and then sub-sampled data overlaps between.
2 annotators.
iaa is calculated through averagelabel agreement at post level – for example if twoannotators label two posts [abusive, untargeted] and[abusive, group targeted] the agreement would be0.5. our iaa during iterations of dataset construc-tion ranged between 0.5 and 0.71. in the penulti-mate annotation round we saw a drop in agreement(figure 2); this is attributed to a change in underly-ing text genre, moving to longer reddit posts.
25%of disagreements about classiﬁcations were solvedduring discussions.
annotators had the opportu-nity to adjust their disagreed annotation in the ﬁrstrevision individually, which represents the remain-ing 75% (table 5).
the majority of disagreementswere on subtask a, deciding whether the post wasabusive or not..individual corr.
417.group solv.
169.discussion round69 (+125 pilot).
table 5: solved disagreements/ﬂagged content.
the ﬁnal overall fleiss’ kappa (fleiss (1971))for individual subtasks are: abusive/not: 0.58, tar-geted: 0.54, misogyny/not: 0.54. it is notable herethat the dataset is signiﬁcantly more skewed thanprior work which upsampled to 1:1 class balances.
chance-corrected measurements are sensitive toagreement on rare categories and higher agreementis needed to reach reliability, as shown in artsteinand poesio (2008)..5.5 annotator disagreement analysis.
based on the discussion rounds, the following typesof posts were the most challenging to annotate:.
1. interpretation of the author’s intention (irony,.
sarcasm, jokes, and questions)e.g.
haha!
virksomheder i danmark: vi ansætter(haha!
companies in den-.
aldrig en kvinde igen...mark: we will never hire a woman again ...).
sexisme og seksuelt frisind er da vist ikke det samme?
(i don’t believe sexism and sexual liberalism are the.
same?).
2. degree of abuse: misrepresenting the truth to.
harm the subject or facte.g.
han er en stor løgner (he is a big liar).
3. hashtags: meaning and usage of hashtags in.
relation to the contexte.g.
#nometoo.
4. world knowledge required:.
du siger at frank bruger sin magt forkert men du.
051015202530annotated data samples in k0.30.40.50.60.70.80.91.00.40.460.610.650.710.70.70.71per iterationaccumelatedbruger din til at brænde s˚a mange mænd p˚a b˚alet ...(you say that frank uses his power wrongly, but you use.
yours to throw so many men on the ﬁre ... - referring to.
a speciﬁc political topic.).
5. quotes: re-posting or re-tweeting a quotegives limited information about the support ordenial of the author.
6. jargon: receiver’s perception.
i skal alle have et klap i m˚asen herfra (you all get apat on the behind from me).
handling these was an iterative process of raisingcases for revision in the discussion rounds, formu-lating the issue, and providing documentation.
weadded the status and, where applicable, outcomefrom these cases to the guidelines.
we also addedexplanations of hashtags and deﬁnitions of unclearidentities, like “the media”, as a company.
forquotes without declaration of rejection or support,we agreed to label them as not abusive, since themotivation of re-posting is not clear..5.6 baseline experiments as an indicator.
lastly, we provide a classiﬁcation baseline: formisogyny and abusive language, the bert modelfrom devlin et al.
(2019) proved to be a robust ar-chitecture for cross-domain (swamy et al., 2019)and cross-lingual (pamungkas et al., 2020; mulkiand ghanem, 2021) transfer.
we use therefore mul-tilingual bert (’bert-base-multilingual-un cased’)for general language understanding in danish, ﬁne-tuned on our dataset..model: we follow the suggested parametersfrom mosbach et al.
(2020) for ﬁne-tuning (learn-ing rate 2e-5, weight decay 0.01, adamw opti-mizer without bias correction).
class imbalance ishandled by weighted sampling and data split fortrain/test 80/20.
experiments are conducted withbatch size 32 using tesla v100 gpu..preprocessing: our initial pre-processing of theunstrucutured posts included converting emojis totext, url replacement, limit @user and punctu-ation occurrences and adding special tokens forupper case letters adopted from ahn et al.
(2020).
classiﬁcation: since the effect of applying multi-task-learning might not conditionally improve per-formance (mulki and ghanem, 2021), the classi-ﬁcation is evaluated on a subset of the dataset foreach subtask (see table 6) including all posts of thetarget label (e.g.
misogyny) and stratiﬁed samplingof the non-target classes (e.g.
for non-misogynistic:abusive and non-abusive posts) with 10k posts for.
each experiment.
results are reported when themodel reached stabilized per class f1 scores forall classes on the test set (± 0.01/20).
the resultsindicate the expected challenge of accurately pre-dicting less-represented classes and generalizing tounseen data.
analysing false positives and falsenegatives on the misogyny detection task, we can-not recognise noticeable correlations with otherabusive forms and disagreements/ difﬁcult casesfrom the annotation task..epoch f1subtask200abus/nottarget120misog./not 200misog.
*misog.categ.
100.prec.
recall0.7650 76.43% 76.4%0.6502 64.45% 66.2%0.8549 85.27% 85.85%0.61910.7913 77.79% 81.26%.
table 6: baseline evaluation: f1-scores, precision, re-call (weighted, *except for misog., class f1-score) withmbert.
6 discussion and reﬂection.
reﬂections on sampling we sampled from dif-ferent platforms, and applied different samplingtechniques.
the goal was to ensure, ﬁrst, a suf-ﬁcient amount of misogynistic content and, sec-ondly, mitigation of biases stemming from a uni-form dataset..surprisingly, topic sampling unearthed a higherdensity of misogynistic content than targeted key-word search (table 3).
while researching plat-forms, we noticed the limited presence of dan-ish for publicly available men-dominated fora(e.g.
gaming forums such as dota2 and extrem-ist plaftorms such as gab (kennedy et al., 2018)).
this, as well as limitations of platform apis causeda narrow data selection.
often, non-privileged lan-guages can gain from cross-language transfer learn-ing.
we experimented with translating misogy-nistic posts from fersini et al.
(2018) to danish,using translation services, and thereby augment theminority class data.
translation services did notprovide a sampling alternative.
additionally, asdiscovered by anzovino et al.
(2018), misogynis-tic content seems to vary with culture.
this makes.
total960.text corrected877.label corrected224.out48.table 7: translating ibereval posts en to da.
language-speciﬁc investigations important, both forthe sake of quality of automatic detection systems,as well as for cultural discovery and investigation.
table 7 shows results of post-translation manualcorrection by annotators (all ﬂuent in english)..reﬂections on annotation process using justseven annotators has the disadvantage that one isunlikely to achieve as broad a range of annotatorproﬁles as, for instance, through crowdsourcing.
however, during annotation and weekly discus-sions, we saw clear beneﬁts from having a smallannotator group with different backgrounds andintense training.
while annotation quality cannotbe measured by iaa alone, the time for debate clar-iﬁed taxonomy items, gave thorough guidelines,and increased the likelihood of correct annotations.
the latter reﬂects the quality of the ﬁnal dataset,while the former two indicate that the taxonomyand codebook are likely useful for other researchersanalysing and processing online misogyny..6.1 a comprehensive taxonomy for misogyny.
the semi-open development of the taxonomy andfrequent discussions allowed the detection neo-sexism as an implicit form of misogyny.
futureresearch in taxonomies of misogyny could con-sider including distinctions between active/passivemisogyny, as suggested by anzovino et al.
(2018)as well as other sub-phenomena..in the resulting dataset, we saw a strong repre-sentation of neosexism.
whether this is a speciﬁccultural phenomenon for danish, or indicative ofgeneral online behaviour, is not clear..the use of uniﬁed taxonomies in research af-fords the possibility to test the codebook guide-lines iteratively.
we include a short version of theguidelines in the appendix; the original documentconsists of seventeen pages.
in a feedback surveyfollowing the annotation work, most of the anno-tators described that during the process, they usedthe guidelines primarily for revision in case theyfelt unsure how to label the post.
to make theannotation more intuitively clear for annotators,we suggest reconsidering documentation tools andtheir accessibility for annotators.
guidelines arecrucial for handling linguistic challenges, and well-documented decisions about them serve to createcomparable research on detecting online misogynyacross languages and dataset..7 conclusion and future work.
in this work, we have documented the constructionof a dataset for training systems for automatic de-tection of online misogyny.
we also present theresulting dataset of misogyny in danish social me-dia, bajer, including class balance, word counts,and baseline as an indicator.
this dataset is avail-able for research purposes upon request..the objective of this research was to explore thedesign of an annotation process which would resultin a high quality dataset, and which was transparentand useful for other researchers..our approach was to recruit and train a diversegroup of annotators and build a taxonomy and code-book through collaborative and iterative annotator-involved discussions.
the annotators reached goodagreement, indicating that the taxonomy and code-book were understandable and useful..however, to rigorously evaluate the quality ofthe dataset and the performance of models thatbuild on it, the models should be evaluated in prac-tice with different text types and languages, as wellas compared and combined with models trainedon different datasets, i.e.
guest et al.
(2021).
be-cause online misogyny is a sensitive and precarioussubject, we also propose that the performance ofautomatic detection models should be evaluatedwith use of qualitative methods (inie and derczyn-ski, 2021), bringing humans into the loop.
as wefound through our continuous discussions, onlineabuse can present in surprising forms, for instancethe denial that misogyny exists.
the necessary in-tegration of knowledge and concepts from relevantﬁelds, e.g.
social science, into nlp research is onlyreally possible through thorough human participa-tion and discussion..acknowledgement.
this research was supported by the it univer-sity of copenhagen, computer science for inter-nal funding on abusive language detection; andthe independent research fund denmark underproject 9131-00131b, verif-ai.
we thank our anno-tators nina schøler nørgaard, tamana saidi, jonasjoachim kofoed, freja birk, cecilia andersen, ul-rik dolzyk, im soﬁe skak and rania m. tawﬁk.
we are also grateful for discussions with deboranozza, elisabetta fersini and tracie farrell..impact statement: data anonymization.
usernames and discussion participant/authornames are replaced with a token @user value.
annotators were presented with the text of the postand no author information.
posts that could notbe interpreted by annotators because of missingbackground information were excluded.
we onlygathered public posts..annotators worked in a tool where they couldnot export or copy data.
annotators are instructedto ﬂag and skip pii-bearing posts..all further information about dataset creation is.
included in the main body of the paper above..references.
hwijeen ahn, jimin sun, chan young park, andjungyun seo.
2020. nlpdove at semeval-2020task 12: improving offensive language detectionwith cross-lingual transfer.
in proceedings of thefourteenth workshop on semantic evaluation, pages1576–1586..hala al kuwatly, maximilian wich, and georg groh.
identifying and measuring annotator bias2020.based on annotators’ demographic characteristics.
in proceedings of the fourth workshop on onlineabuse and harms, pages 184–190, online.
associa-tion for computational linguistics..amnesty international.
2017. amnesty reveals alarm-ing impact of online abuse against women.
https://www.amnesty.org/en/latest/news/2017/11/amnesty-reveals-alarmin-impact-of-/online-abuse-against-women/.
accessed:jan, 2021..analyse & tal.
2021. angreb i den offentlige debat p˚a.
facebook.
technical report, analyse & tal..astrid skov andersen and maja langberg.
2021.nogle personer tror, at de gør verden til et bedrested ved at sende hadbeskeder, siger ekspert.
tv2nyheder..maria anzovino, elisabetta fersini, and paolo rosso.
2018. automatic identiﬁcation and classiﬁcationof misogynistic language on twitter.
in max sil-berztein, faten atigui, elena kornyshova, elisabethm´etais, and farid meziane, editors, natural lan-guage processing and information systems, volume10859, pages 57–64.
springer international publish-ing, cham.
series title: lecture notes in computerscience..ron artstein and massimo poesio.
2008. inter-coderagreement for computational linguistics.
compu-tational linguistics, 34(4):555–596..michele banko, brendon mackeen, and laurie ray.
2020. a uniﬁed taxonomy of harmful content..in proceedings of the fourth workshop on onlineabuse and harms, pages 125–137, online.
associa-tion for computational linguistics..valerio basile, cristina bosco, elisabetta fersini,debora nozza, viviana patti, francisco manuelrangel pardo, paolo rosso, and manuela san-guinetti.
2019. semeval-2019 task 5: multilingualdetection of hate speech against immigrants andwomen in twitter.
in proceedings of the 13th inter-national workshop on semantic evaluation, pages54–63, minneapolis, minnesota, usa.
associationfor computational linguistics..emily m. bender and batya friedman.
2018. datastatements for natural language processing: to-ward mitigating system bias and enabling betterscience.
transactions of the association for com-putational linguistics, 6:587–604..dorthe bleses, werner vach, malene slott, sonja we-hberg, pia thomsen, thomas o madsen, and hansbasbøll.
2008. early vocabulary development indanish and other languages: a cdi-based compari-son.
journal of child language, 35(3):619..bernadette campbell, e. glenn schellenberg, andcharlene y. senn.
1997. evaluating measures ofcontemporary sexism.
psychology of women quar-terly, 21(1):89–102.
publisher: sage publicationsinc..casey newton.
2020. facebook will pay $52 millionin settlement with moderators who developed ptsdon the job.
the verge.
accessed: jan, 2021..despoina chatzakou, nicolas kourtellis,.
jeremyblackburn, emiliano de cristofaro, gianlucastringhini, and athena vakali.
2017. mean birds:detecting aggression and bullying on twitter.
inproceedings of the 2017 acm on web science con-ference, websci ’17, pages 13–22, new york, ny,usa.
association for computing machinery..patricia chiril, v´eronique moriceau, farah benamara,alda mari, gloria origgi, and marl`ene coulomb-gully.
2020. an annotated corpus for sexism de-in proceedings of thetection in french tweets.
12th language resources and evaluation confer-ence, pages 1397–1403, marseille, france.
euro-pean language resources association..yi-ling chung, elizaveta kuzmenko, serra sinemtekiroglu, and marco guerini.
2019. conan -counter narratives through nichesourcing: a mul-tilingual dataset of responses to fight online hatein proceedings of the 57th annual meet-speech.
ing of the association for computational linguis-tics, pages 2819–2829, florence, italy.
associationfor computational linguistics..danske kvindesamfund.
2020..sexisme og sex-chikane.
https://danskkvindesamfund.dk/dansk-kvindesamfunds-abc/sexisme/.
accessed 2021-01-17..thomas davidson, debasmita bhattacharya, and ing-mar weber.
2019. racial bias in hate speech andabusive language detection datasets.
in proceed-ings of the third workshop on abusive languageonline, pages 25–35, florence, italy.
association forcomputational linguistics..thomas davidson, dana warmsley, michael macy,and ingmar weber.
2017. automated hate speechdetection and the problem of offensive language.
in proceedings of the international aaai confer-ence on web and social media, 1..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..lucas dixon, john li, jeffrey sorensen, nithum thain,and lucy vasserman.
2018. measuring and mitigat-ing unintended bias in text classiﬁcation.
in pro-ceedings of the 2018 aaai/acm conference on ai,ethics, and society, aies ’18, pages 67–73, newyork, ny, usa.
association for computing machin-ery..bo ekehammar, nazar akrami, and tadesse araya.
2000. development and validation of swedishscandina-classical and modern sexism scales.
vian journal of psychology, 41(4):307–314.
eprint:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9450.00203..elisabetta fersini, paolo rosso, and maria anzovino.
2018. overview of the task on automatic misog-in ibereval@yny identiﬁcation at ibereval 2018.sepln, pages 214–228..mark a finlayson and tomaˇz erjavec.
2017. overviewof annotation creation: processes and tools.
inhandbook of linguistic annotation, pages 167–191.
springer..joseph l. fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378–382..komal florio, valerio basile, marco polignano, pier-paolo basile, and viviana patti.
2020. time ofyour hate: the challenge of time in hate speechapplied sciences,detection on social media.
10(12):4180. number: 12 publisher: multidisci-plinary digital publishing institute..antigoni maria founta, constantinos djouvas, de-spoina chatzakou, ilias leontiadis, jeremy black-burn, gianluca stringhini, athena vakali, michaelsirivianos, and nicolas kourtellis.
2018. largescale crowdsourcing and characterization of twit-ter abusive behavior.
in twelfth international aaaiconference on web and social media..batya friedman and helen nissenbaum.
1996. biasin computer systems.
acm transactions on infor-mation systems, 14(3):330–347.
publisher: associ-ation for computing machinery (acm)..lei gao and ruihong huang.
2017. detecting on-line hate speech using context aware models.
inproceedings of the international conference recentadvances in natural language processing, ranlp2017, pages 260–266, varna, bulgaria.
incomaltd..joshua garland, keyan ghazi-zahedi, jean-gabrielyoung, laurent h´ebert-dufresne, and mirta galesic.
2020. countering hate on social media: large scalein pro-classiﬁcation of hate and counter speech.
ceedings of the fourth workshop on online abuseand harms, pages 102–112, online.
association forcomputational linguistics..mor geva, yoav goldberg, and jonathan berant.
2019.are we modeling the task or the annotator?
an in-vestigation of annotator bias in natural languagein proceedings of theunderstanding datasets.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1161–1166, hong kong,china.
association for computational linguistics..peter glick and susan fiske.
1996. the ambiva-lent sexism inventory: differentiating hostile andbenevolent sexism.
journal of personality and so-cial psychology, 70:491–512..jennifer golbeck, zahra ashktorab, rashad o. banjo,alexandra berlinger, siddharth bhagwan, codybuntain, paul cheakalos, alicia a. geller, quintgergory, rajesh kumar gnanasekaran, raja ra-jan gunasekaran, kelly m. hoffman, jenny hot-tle, vichita jienjitlert, shivika khare, ryan lau,marianna j. martindale, shalmali naik, heather l.nixon, piyush ramachandran, kristine m. rogers,lisa rogers, meghna sardana sarin, gaurav sha-hane, jayanee thanki, priyanka vengataraman, zi-jian wan, and derek michael wu.
2017. a largelabeled corpus for online harassment research.
in proceedings of the 2017 acm on web scienceconference, pages 229–233, troy new york usa.
acm..ella guest, bertie vidgen, alexandros mittos, nis-hanth sastry, gareth tyson, and helen margetts.
2021. an expert annotated dataset for the detec-tion of online misogyny.
in proceedings of the 16thconference of the european chapter of the associ-ation for computational linguistics: main volume,pages 1336–1350, online.
association for computa-tional linguistics..dirk hovy and shannon l. spruit.
2016. the socialimpact of natural language processing.
in proceed-ings of the 54th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 591–598, berlin, germany.
associationfor computational linguistics..nanna inie and leon derczynski.
2021. an idrframework of opportunities and barriers betweenhci and nlp.
in proceedings of the first workshopon bridging human–computer interaction and nat-ural language processing, pages 101–108..akshita jha and radhika mamidi.
2017. when doesa compliment become sexist?
analysis and classiﬁ-cation of ambivalent sexism using twitter data.
inproceedings of the second workshop on nlp andcomputational social science, pages 7–16, vancou-ver, canada.
association for computational linguis-tics..mladen karan and jan ˇsnajder.
2018. cross-domaindetection of abusive language online.
in proceed-ings of the 2nd workshop on abusive language on-line (alw2), pages 132–137, brussels, belgium.
as-sociation for computational linguistics..brendan.
kennedy,.
mohammad.
atari,aida mostafazadeh davani, leigh yeh, aliomrani, yehsong kim, kris coombs, shreyahavaldar, gwenyth portillo-wightman, elainegonzalez, joseph hoover, aida azatian, alyzehhussain, austin lara, gabriel olmos, adam omary,christina park, clarisa wijaya, xin wang, yongzhang, and morteza dehghani.
2018. the gab hatecorpus: a collection of 27k posts annotated for hatespeech.
technical report, psyarxiv.
type: article..svetlana kiritchenko, isar nejadgholi, and kathleen c.fraser.
2020. confronting abusive language on-line: a survey from the ethical and humanrights perspective.
arxiv:2012.12305 [cs].
arxiv:2012.12305..andreas kirkedal, barbara plank, leon derczynski,and natalie schluter.
2019. the lacunae of dan-ish natural language processing.
in proceedings ofthe 22nd nordic conference on computational lin-guistics, pages 356–362..ritesh kumar, atul kr.
ojha, shervin malmasi, andmarcos zampieri.
2018. benchmarking aggressionidentiﬁcation in social media.
in proceedings of thefirst workshop on trolling, aggression and cyber-bullying (trac-2018), pages 1–11, santa fe, newmexico, usa.
association for computational lin-guistics..carmen martinez, consuelo paterna, patricia roux,and juan manuel falomir.
2010. predicting genderawareness: the relevance of neo-sexism.
journal ofgender studies, 19(1):1–12..barbara masser and dominic abrams.
1999. con-temporary sexism: the relationships among hos-tility, benevolence, and neosexism.
psychologyof women quarterly, 23(3):503–517.
publisher:sage publications inc..marius mosbach, maksym andriushchenko, and diet-rich klakow.
2020. on the stability of fine-tuningbert: misconceptions, explanations, and strong.
baselines.
2006.04884..arxiv:2006.04884 [cs, stat].
arxiv:.
hala mulki and bilal ghanem.
2021. let-mi: an ara-bic levantine twitter dataset for misogynistic lan-guage.
in proceedings of the sixth arabic naturallanguage processing workshop, pages 154–163..nedjma ousidhoum, yangqiu song, and dit-yan ye-ung.
2020.comparative evaluation of label-agnostic selection bias in multilingual hate speechin proceedings of the 2020 conferencedatasets.
on empirical methods in natural language process-ing (emnlp), pages 2532–2542, online.
associa-tion for computational linguistics..endang wahyu pamungkas, valerio basile, and vi-viana patti.
2020. misogyny detection in twitter: amultilingual and cross-domain study.
informationprocessing & management, 57(6):102360..pulkit parikh, harika abburi, pinkesh badjatiya, rad-hika krishnan, niyati chhaya, manish gupta, andvasudeva varma.
2019. multi-label categorizationof accounts of sexism using a neural framework.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1642–1652, hong kong, china.
association for computa-tional linguistics..ji ho park, jamin shin, and pascale fung.
2018. re-ducing gender bias in abusive language detec-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 2799–2804, brussels, belgium.
associationfor computational linguistics..james pustejovsky and amber stubbs.
2012. nat-ural language annotation for machine learn-ing: a guide to corpus-building for applica-tions.
”o’reilly media, inc.”.
google-books-id:a57ts7fs8muc..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial biasin proceedings of thein hate speech detection.
57th annual meeting of the association for com-putational linguistics, pages 1668–1678, florence,italy.
association for computational linguistics..anna schmidt and michael wiegand.
2017. a sur-vey on hate speech detection using natural lan-guage processing.
in proceedings of the fifth inter-national workshop on natural language processingfor social media, pages 1–10, valencia, spain.
as-sociation for computational linguistics..deven santosh shah, h. andrew schwartz, and dirkhovy.
2020. predictive biases in natural languageprocessing models: a conceptual framework andoverview.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5248–5264, online.
association for computa-tional linguistics..sima shariﬁrad and alon jacovi.
2019. learning andunderstanding different categories of sexism us-ing convolutional neural network’s filters.
in pro-ceedings of the 2019 workshop on widening nlp,pages 21–23..gudbjartur ingi sigurbergsson and leon derczynski.
2020. offensive language and hate speech detec-in proceedings of the 12th lan-tion for danish.
guage resources and evaluation conference, pages3498–3508..lea skewes, joshua skewes, and michelle ryan.
2019.attitudes to sexism and gender equity at a danishuniversity.
kvinder, køn & forskning, pages 71–85..statista.
2020. denmark: most popular social media.
sites 2020. accessed: jan, 2021..steve durairaj swamy, anupam jamatia, and bj¨orngamb¨ack.
2019. studying generalisability acrossabusive language detection datasets.
in proceed-ings of the 23rd conference on computational nat-ural language learning (conll), pages 940–950,hong kong, china.
association for computationallinguistics..janet swim, kathryn aikin, wayne hall, and barbarahunter.
1995. sexism and racism: old-fashionedand modern prejudices.
journal of personality andsocial psychology, 68:199–214..francine tougas, rupert brown, ann m. beaton, andst´ephane joly.
1995. neosexism: plus c¸ a change,plus c’est pareil.
personality and social psychol-ogy bulletin, 21(8):842–849.
publisher: sage pub-lications inc..francine tougas, rupert brown, ann m. beaton, andline st-pierre.
1999. neosexism among women:the role of personally experienced social mobilityattempts.
personality and social psychology bul-letin, 25(12):1487–1497.
publisher: sage publica-tions inc..bertie vidgen and leon derczynski.
2020. direc-tions in abusive language training data, a system-atic review: garbage in, garbage out.
plos one,15(12):e0243300.
publisher: public library of sci-ence..bertie vidgen, alex harris, dong nguyen, rebekahtromble, scott hale, and helen margetts.
2019.challenges and frontiers in abusive content detec-tion.
in proceedings of the third workshop on abu-sive language online, pages 80–93, florence, italy.
association for computational linguistics..rex e wallace.
2005. an introduction to wall inscrip-tions from pompeii and herculaneum.
bolchazy-carducci publishers..zeerak waseem.
2016. are you a racist or am i see-ing things?
annotator inﬂuence on hate speechin proceedings of the firstdetection on twitter..workshop on nlp and computational social sci-ence, pages 138–142, austin, texas.
association forcomputational linguistics..zeerak waseem, thomas davidson, dana warmsley,and ingmar weber.
2017. understanding abuse:a typology of abusive language detection sub-tasks.
in proceedings of the first workshop on abu-sive language online, pages 78–84, vancouver, bc,canada.
association for computational linguistics..zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatein proceedings ofspeech detection on twitter.
the naacl student research workshop, pages 88–93, san diego, california.
association for computa-tional linguistics..maximilian wich, jan bauer, and georg groh.
2020.impact of politically biased data on hate speechin proceedings of the fourth work-classiﬁcation.
shop on online abuse and harms, pages 54–64, on-line.
association for computational linguistics..michael wiegand, josef ruppenhofer, and thomaskleinbauer.
2019. detection of abusive language:the problem of biased datasets.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 602–608, minneapolis,minnesota.
association for computational linguis-tics..ellery wulczyn, nithum thain, and lucas dixon.
2017.ex machina: personal attacks seen at scale.
inproceedings of the 26th international conference onworld wide web, pages 1391–1399..marcos zampieri, shervin malmasi, preslav nakov,sara rosenthal, noura farra, and ritesh kumar.
2019. predicting the type and target of offensiveposts in social media.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1415–1420, minneapolis, minnesota.
association for computational linguistics..marcos zampieri, preslav nakov, sara rosenthal, pepaatanasova, georgi karadzhov, hamdy mubarak,leon derczynski, zeses pitenis, and c¸ a˘grı c¸ ¨oltekin.
2020. semeval-2020 task 12: multilingual offen-sive language identiﬁcation in social media (of-arxiv:2006.07235 [cs].
arxiv:fenseval 2020).
2006.07235..caleb ziems, bing he, sandeep soni, and srijan ku-mar.
2020. racism is a virus: anti-asian hateand counterhate in social media during the covid-arxiv:2005.12423 [physics].
arxiv:19 crisis.
2005.12423..a appendices.
a.1 annotation codebook.
general rules for annotators.
• the focus of the annotation task is on thewhole post.
some words and hashtags de-pend on their contextual use, if they are meantoffensive, not.
for example:.
(1) ”hykleriske”, ”ad helvede til”,”føj”,”sgu”, ”pisse”, ”fanden”, ”eddermame”,”hold kæft”, ”liderkarl”, ”bolle”, ”˚andssvag”eller ”løgner”in case these words appear without any con-text in a post, i.e.
”hold nu kæft”, the post isnot abusive.
quotes: quotes are considered as alwayscontext-dependent.
the author uses someoneelse words and agree with them, not.
border-case: if a quote and only if is used withoutany further comment, between two cases aredistinguished:1) quote contains profanity: labeled as:abus/unt, i.e..(2) copypasta textual memes: i.a.
navy seal.
copypasta2) quote contains vague abuse without anyprofanity/slurs: not abus, intention of theauthor is unclear why the quote is posted..• no observations on top of the post, just thetext of the post is relevant for the evaluation.
examples for being not abusive just by thepost itself:.
(3) ”it is best that they stay there and not.
come back.”.
danskernes penge”.
(4) ”jo og hendes der gambler med.
(5) ”hvorfor ikke sætte navn p˚a manden ??”.
annotation scheme.
precedence of labels: for each post a labelis chosen in sub-task a according to the anno-.
tation scheme.
depending on the chosen label,further labels (sub-tasks b and c) may need tobe selected following the hierarchically annotationscheme above (green lines).
the determining labeladdressed by the post should be selected..for example, the primary abuse of this postsadresses racism, where the chat participant is of-fended by the fact of being from a ”dansk/afghanskkultur”:.
(6) ”@user du ser sexistiske spøgelser allevegne, fordi du kommer fra en dansk/afghanskkultur, hvor overgreb mod kvinder er almindeligtaccepteret og derfor en del af selvforst˚aelsen.”.
subtasks and tags.
subtask a: abusive language detectiongenerally, posts containing abusive languageinclude insults, threats, any type of untargeted(abus/not) speciﬁcally, a post isprofanity.
abusive if it:.
• uses slurs, clear abusive expressions (in caseof censorship, i.e.
”p*s”,”fu..”, the actual slurhas to be clear)..(7) ”kælling”, ”lort”, ”klamme.
svin”,”sindssyge”, ”idiot”, ”fucked/fucking”, ”wtf/whatthe fuck”, ”luder”.
• attacks a person, minority to cause harm,repetitiveness,, an imbalance of power (ex-amples see subtask b)..• promotes, but does not directly use abuse lan-guage, violent crime, i.e.
agreeing with a abu-sive quote by ”#præcis”..(8) ”@user: hørt p˚a tribunen: jeg elsker alledansker men pigerne har en klam personlihed.
lud-ere #præcis.”.
• contains offensive criticism without a well.
founded argument/ backed-up fact..(9) ”ja.
pippi langstrømpe fx.
mega negativt.
portræt og meget undertrykt af patriarkatet.”.
not offensive criticism - not abusive:(10) ”det ville være dejligt hvis tvangsfjernelseromskæring gensidig forsørgelse barnebrude sygeborgere ﬁk lige s˚a meget hjælp af offentligheden ogmedierne.”.
• blatantly misrepresents truth, seeks to distortviews on a person, minority with unfoundedarguments/ claims..(11) ”mænd kan ikke blive medlem hos de.
lort du vil jo ikke selv.”.
- ”#s˚afuckdig”: used to express that women vic-.
sikkert p˚a forsiden med en lille pige.”.
radikale, det ender med noget rod!!!”.
(12) ”feminist partiet, vil blot have fjernet mænd.
fra parti toppen, bruger derfor beskidte kneb”.
(13) ”lad nu morten være i fred.
men et parsygemeldinger fra soﬁe carsten nielsen og lotterod mangler vi.
for soﬁe og lotte har sider p˚aspring for at ﬁnde noget p˚a morten, bare for atf˚a formands posten.
de skulle skamme sig.
m˚ada ikke h˚abe folk stemmer p˚a dem/ hende til næstevalg.”.
• shows support of problematic hash tags,.
screennames..(14) ”#kriminelleudlændinge”.
explanations various hashtags:- ”#sikkerhedssele”: disadvantage of women inrights/ opportunities because of male-dominateddevelopment&research.
timize themselves.
- ”#mændsligestilling”: used against feminismbut also just showing men inequality in some cases.
• negatively, positively stereotypes in an offen-sive way (examples, see subtask c: stereotype& objectiﬁcation)..• defends xenophobia, sexism (examples, see.
subtask c)..• seeks to silence a person, minority by con-sciously intending to limit the person’s free-dom of speech..(15) ”stop nu alt det fnidder !!”.
• is ambiguous (sarcastic/ ironic), and the postis on a topic that satisﬁes any of the abovecriteria (see various examples in the appendixbordercases)..subtask b: target identiﬁcation.
an abusive post can classiﬁed as untargeted(unt), targeted (ind/grp/oth).
untargetedposts (unt) contain nontargeted profanity andswearing.
posts with general profanity are nottargeted, but they contain non-acceptable language..(16) ”remoulade, kællinger!”(17) ”indtil lortet begynder at hvæse og g˚ar.
amok...”.
(18) ”er de af glas s˚a smid dem i ﬂaskekon-taneren du vil vel ikke have andre skal bruge det.
targeted posts can be towards a speciﬁc individ-ual person/ persons that is/ are part of the chat, aperson the conversation is about (ind).
the personcan be named (i.e.
@user), unnamed..example for a speciﬁc person, unnamed:(19) ”største luder p˚a dansk jord.”example for a speciﬁc person, named:.
(20) ”@user , det er ikke dig der dømmer.
du lovgiver.
hvis du mener noget s˚a m˚a du g˚a ifolketinget.
føj for satan, du er mega klam.”example for a speciﬁc person the chat is about:.
(21) ”@user prøv at se hende i øjnene.
hunkan lyve overfor den danske befolkning uden, atblinke med øjnene... imponerende.
men kikker durigtigt efter, s˚a vil du se satan selv, i hendesøjne!
ondskabsfulde kælling!”.
(22) ”havde gru eksisteret i dag, var joe biden.
posts can also contain any language thatdisparages a group of people united by race, color,ethnicity, gender, sexual orientation, nationality,religion,, other social identity (grp).
this cantarget the group in general, an individual groupmembers receiving abuse because of their groupmembership..(23) ”det er sgu ﬂot s˚a kan de komme og sprede.
smitten rundt gud fader bevar os.”if a post is still targeted but does not belongto one of the other categories, i.e.
targeting anorganisation, country (if it is not used to refer tothe nationality of persons), party, it should belabeled as oth..(24) ”jeg er sgu lidt forvirret... hvad var det.
danske bank lavede lort i, denne her gang?”this also includes undeﬁned groups outside thechat, i.e..(25) ”kære alle idioter.
samtykke og #metoo er.
ogs˚a frigørende for mænd.”additional clariﬁcations about certain groups:.
(26) ”medier” = company(27) ”immigrants” = group based on race(28) ”muslimer” = racism.
subtask c: hate speech categorization.
a post containing abusive language against agroup can be sexist (sex), racist (rac), of othernature (oth).
sexist content can be described asprejudice, stereotyping,, discrimination typically.
sexist posts are categorized in:.
desværre bare.”.
against women, while racism is on the base ofracial, ethnic characters of a group.
others can be: lgtb, sexism against men.
racist posts can be:(29) ”@user hvad med et forbudt kun for.
muslimer.!
:) dette vil være en løsning.”.
(30) ”@user migranter, der ikke har f˚aetasyl eller ﬂygtningestatus kan smides ud udenproblemer.
det er lykkesøgere s˚a man kan følgedem til den tyske grænse,og de kan s˚a forsøge derellers m˚a de rejse videre til næste land, og prøvelykken der er ingen krav om, at de skalforsørges af dk.”.
• stereotype & objectiﬁcation (nor), if anormative held but ﬁxed and oversimpliﬁedimage, idea (stereotype), description of per-son’s physical appeal and/or comparisons tonarrow standards (objectiﬁcation) is used inan offensive way..(31) ”kvindefoodbold , ???
hummmmm,.
gabbbbbb”.
(32) ”hysterike kællinger”i.
(33) ”du høre.
til.
vaskekælderen.”.
køkkenet.
eller.
i.
• benevolent (ambivalent), if the authoruses a strong subjective positive sentiment/stereotypical picture with men, women oftendisguised as a compliment (benevolent sex-ism), i.e.
”they’re probably surprised at how smartyou are, for a girl”, there is a reverence forthe stereotypical role of women as moth-ers, daughters and wives: ‘no man succeedswithout a good woman besides him.
wife,mother.’examples are:.
(34) ”spørg tina bilsbo.
hun havde nosser ogtog konsekvensen, da hun ikke ﬁk det samme somde mandlige.”.
(35) ”tag det som end mand.”(36) ”piger og kvinder er de meste udsatte borgerb˚ade i danmark og p˚a det globale plan.!
s˚a dinoffer rolle virker ikke.”.
highlight gender inequality without using astereotypical picture..(37) ”kvinder er mindre intelligente end mænd.”accusing somebody as ”lille pige” is categorizedunder dominance (not stereotyped) of the malegender over the female:.
(38) ”@user det var den gamle.
jeg harudviklet mig.
du ﬂæber stadig som en lille pige.”.
• discredit (discredit), if the author dis-grace, humiliate woman with no larger inten-tion..(39) ”kvinder er fucking korrupte, s˚adan er det.
(40) ”danes for trump er ikke kæmpe fan af bin-goværten fra jylland, men er nu alligevel lidt tristeover den m˚ade, som en ﬂok hysteriske kælliger harf˚aet manden sl˚aet tilbage til bingostudiet p˚a.”.
• sexual harassment & threats of violence(harrassment), if actions as sexual ad-vances, requests for sexual favours, harass-ment of sexual nature is used; intent to physi-cally assert power over women through threatsof violence..(41) ”hvorfor er særligt kvinder tolerant ift.
is-lam ?
mange kvinder m˚a tilsyneladende havedrømme om at blive behandlet som slave ellerluder.”.
• neosexism (neosex): egalitarian valuesabout women and the justiﬁcation of womanabuse.
authors:.
1. deny the existence of discrimination.
against women..2. resent complaints about discrimination.
3. resent ”special” favours for women..in general, neosexism measures more atti-tudes toward feminist changes in society notdirect attitudes towards women.
it follows theidea of: ”women are no longer discriminatedin our society.” people expressing neosexistbeliefs have an interest in opposing to so-cial policy changes that would beneﬁt womenand keeping the status quo although they maymaintain non-traditional gender roles..• dominance (dominance), if the authorassert the superiority of men over women to.
discrimination:.
example for (1.)
questioning the existence of.
(42) ”kan i pege p˚a forskning der viser atbarslen er grunden til at mødrene g˚ar glip afforfremmelser?.”.
example for (2.).
resent complaints about.
discrimination:.
(43) ”jeg føler ofte folk har behandlet mig bedre.
og talt pænere til migfordi jeg var en pige, s˚a jeg har ret svært ved attage det seriøst n˚ar folk mener at kvinder er s˚a˚a˚adiskriminerede imod i den vestlige verden.”.
including authors demonstrating that ”men are.
victims of the feminism movement”:.
(44) ”der er nu mange middelaldrende mænd,som er endt i en prækær situation som ’denpressede mand’ tæt p˚a bunden.
husk at skriveom mænd der ikke er i medieeliten.”.
(45) ”klassisk.
hvis det er en ulempe for kvinderer det samfundets skyld.
hvis mænd, s˚a m˚a det jovære deres egen.
sexisme trives godt p˚a den femi-nistiske ﬂøj.”but barely demonstrating men inequality is notneosexism.
it does not deny the existence of dis-crimination of women, i.e..(46) “hvad med alle de som er soldat og erfaldet i kamp?
der mange ﬂere mænd som er dødi kamp!
hvorfor hylder man ikke dem enkeltvis?
der fandme intet ligestilling der.
.
.
”example for (3.)
women:.
resent ”special” favours for.
(47) ”man kan altid ﬁnde en ting at pege p˚a,uanset kontekst, hvor kvinder er d˚arligere stillet.
fx whatabout: smerter!
ingen andre steder atkonkludere sig hen end partriarkat og systematiskkvindeundertrykkelse.”.
(48) ”det er ogs˚a kendt at det først er indenforde seneste f˚a ˚ar at kvinder er blevet nervøse for atvære alene med fremmede mænd langt fra andremennesker... *(face with rolling eyes)* aldrig væretet issue før i historien.”.
a.2 danish misogyny (empirical).
• the oldest women’s organization in denmarkdanske kvindesamfund (2020) deﬁnes sex-ism against women as ”sexisme er en fordomeller diskrimination p˚a baggrund af køn, isæri forhold til nedvurdering af kvinder.” (sexismis a prejudice, discrimination based on gender,especially in relation to the downgrading ofwomen.).
a.3 search keywords.
• skewes et al.
(2019) present a survey at adanish university and their ﬁndings exhibita correlation of modern sexism scale and theattitude “enough, too much was being donefor gender equity”..• ekehammar et al.
(2000) proves the existenceof modern sexism attitudes in sweden withstronger means for men..• the modern sexism scale is based on the mod-ern sexism theories studied in north americaand their application to the european contextalready proven by masser and abrams (1999)(study undertaken in great britain)..• ambigious keywords: voldtægt, synd, helvede,.
lækker, dødt, sæk.
• slurs from sigurbergsson and derczynski(2020): ﬁsefornem, hjemmefødning, kvin-deagtig, ludder, papmor, pigebarn, pigefnid-der, plasticmor, tyskertøs, pattebørn, kvinder-menneske, svabrefjams.
• from articles/interviews: luder, møgsæk, grup-pevoltægt, kælling, lille pige, dumt svin, klam.
• translated from previous work: ”s˚a god somen mand”, ”som en mand”, ”til en pige”,”smart til en pige”, ”kærlighed til en kvinder”,”intelligent til en pige”, #adaywithoutwomen,”en dag uden kvinder”, ”#womensday”, ”#ev-erydaysexism”, ”#weareequal”.
• by pattern recognition from posts: ”#metoo”,”#getbackinthekitchen”, ”som end mand”,”gør noget rent”, ”jeg er jo en mand”’, ’”kvin-der er”, ”til en pige”, ”en mand som”, ”lillepige”, ”dumt svin”, ”høre til i køkkenet”, ”vikvinder”, ”men kvinder”, ”mænd der siger”’,”#mændsligestilling”.
• for.
related.
’voldtaget’,.
topic-search:.
’kvinder’,’sexisme’,’skræm-mende’,’mediechefer’,’trussel’,’indvilligede’,kol-’mandligeleger’,’sexisitisk’,’mediebranchen’,’sex-beskeder’,’kvindelige’.
’sex’,.