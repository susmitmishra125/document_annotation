ghostbert: generate more features with cheap operations for bert.
zhiqi huang1, lu hou2, lifeng shang2, xin jiang2, xiao chen2, qun liu21peking university, 2huawei noahâ€™s ark labzhiqihuang@pku.edu.cn, {houlu3, shang.lifeng, jiang.xin, chen.xiao, qun.liu}@huawei.com.
abstract.
transformer-based pre-trained language mod-els like bert, though powerful in many tasks,are expensive in both memory and computa-tion, due to their large number of parameters.
previous works show that some parameters inthese models can be pruned away without se-vere accuracy drop.
however, these redun-dant features contribute to a comprehensiveunderstanding of the training data and remov-ing them weakens the modelâ€™s representationability.
in this paper, we propose ghostbert,which generates more features with very cheapoperations from the remaining features.
inthis way, ghostbert has similar memory andcomputational cost as the pruned model, butenjoys much larger representation power.
theproposed ghost module can also be applied tounpruned bert models to enhance their per-formance with negligible additional parame-ters and computation.
empirical results on theglue benchmark on three backbone models(i.e., bert, roberta and electra) verifythe efï¬cacy of our proposed method..1.introduction.
recently, there is a surge of research interests incompressing the transformer-based pre-trained lan-guage models like bert into smaller ones usingvarious compression methods, i.e., knowledge dis-tillation (sanh et al., 2019; sun et al., 2019; jiaoet al., 2020), pruning (michel et al., 2019; fanet al., 2019), low-rank approximation (lan et al.,2020), weight-sharing (lan et al., 2020), dynamicnetworks with adaptive depth and/or width (liuet al., 2020; hou et al., 2020; xin et al., 2020;zhou et al., 2020), and quantization (shen et al.,2020; fan et al., 2020; zhang et al., 2020; bai et al.,2021)..previous works show that there are some redun-dant features in the bert model, and unimportantattention heads or neurons can be pruned away.
figure 1: average glue development accuracy versus#params and flops with the (pruned) bert and ourghostbert.
m is the width multiplier of the model..without severe accuracy degradation (michel et al.,2019; hou et al., 2020).
however, for computervision (cv) tasks, it is shown in (han et al., 2020)that redundant features in convolutional neural net-works also contribute positively to the performance,and using cheap linear operations to generate moreghost feature maps enhances the performance withfew additional parameters.
on the other hand, it isshown in (voita et al., 2019; kovaleva et al., 2019;rogers et al., 2020) that many attention maps inpre-trained language models exhibit typical posi-tional patterns, e.g., diagonal or vertical, which canbe easily generated from other similar ones usingoperations like convolution..based on the above two aspects, in this paper,we propose to use cheap ghost modules on top ofthe remaining important attention heads and neu-rons to generate more features, so as to compensatefor the pruned ones.
considering that the convo-lution operation (1) encodes local context depen-dency, as a complement of the global self-attentionin transformer models (wu et al., 2020); and (2)can generate some bert features like positionalattention maps from similar others, in this work, wepropose to use the efï¬cient 1-dimensional depth-wise separable convolution (wu et al., 2019) asthe basic operation in the ghost module.
to ensurethe generated ghost features have similar scales.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6512â€“6523august1â€“6,2021.Â©2021associationforcomputationallinguistics6512ğ’=ğŸğŸğŸğ’=ğŸ‘ğŸğŸğ’=ğŸ”ğŸğŸğ’=ğŸ—ğŸğŸğ’=ğŸğŸğŸğ’=ğŸğŸğŸğŸğ’=ğŸ‘ğŸğŸğ’=ğŸ”ğŸğŸğ’=ğŸ—ğŸğŸğ’=ğŸğŸğŸğŸğ’=ğŸğŸğŸğ’=ğŸ‘ğŸğŸğ’=ğŸ”ğŸğŸğ’=ğŸ—ğŸğŸğ’=ğŸğŸğŸğ’=ğŸğŸğŸğŸğ’=ğŸ‘ğŸğŸğ’=ğŸ”ğŸğŸğ’=ğŸ—ğŸğŸğ’=ğŸğŸğŸğŸ(a) adding ghost modules {gf,h}f,m.
f =1,h=1 to mha and ffn..(b) ghost module gf,h..figure 2: using ghost modules to generate more features in bert.
g-mha/ffn stands for ghost-mha/ffn..as the original ones, we use a softmax function tonormalize the convolution kernel..afterwards, we ï¬ne-tune the parameters in boththe bert backbone model and the added ghostmodules.
note that the ghost modules are not nec-essarily applied to pruned models.
they can also bedirectly applied to pre-trained language models forbetter performance while with negligible additionalparameters and ï¬‚oating-point operations (flops).
figure 1 summarizes the average accuracy versusparameter size and flops on the glue bench-mark, where adding ghost modules to both theunpruned (m = 12/12) and pruned (m < 1)bert models perform better than the counterpartswithout ghost modules.
more experiments on theglue benchmark show that with only 0.4% moreparameters and 0.9% more flops, the proposedghost modules improve the average accuracy ofbert-base, roberta-base and electra-smallby 0.9, 0.6, 2.4 points, respectively.
when apply-ing ghost modules to small or pruned models, theresultant models outperform other bert compres-sion methods..2 approach.
in this section, we ï¬rst introduce where to addghost modules in a bert model (section 2.1), andthen discuss the components and optimization de-tails of the ghost module (section 2.2)..for attention heads of mha and neurons in theintermediate layer of ffn can be performed in par-allel.
thus the bert model can be compressedin a structured manner by pruning parameters as-sociated with these heads and neurons (hou et al.,2020).
in this paper, after pruning the unimpor-tant heads and neurons, we employ cheap ghostmodules upon the remaining ones to generate moreghost features to compensate for the pruned ones.
for simplicity of notation, we omit the bias termsin linear and convolution operations where applica-ble in the rest of this work..2.1.1 ghost module on mha.
following (hou et al., 2020), we divide the com-putation of mha into the computation of eachattention head.
speciï¬cally, suppose the sequencelength and hidden state size are n and d, respec-tively.
each transformer layer consists of nh atten-tion heads.
for input matrix x âˆˆ rnÃ—d, the h-thattention head computes its output as hh(x) =h wo(cid:62)h x(cid:62))xwvh wk(cid:62)softmax(1/h ,where wqh âˆˆ rdÃ—dh with dh =h , wod/nh are the projection matrices associated withit.
in multi-head attention, nh heads are computedin parallel to get the ï¬nal output:.
d Â· xwqh , wv.
h , wk.
âˆš.
mha(x) =.
hh(x)..(1).
2.1 adding ghost modules to bert.
the bert model is built with transformer lay-ers, each of which contains a multi-head attention(mha) layer and a feed-forward network (ffn),as well as skip connections and layer normaliza-tions.
hou et al.
(2020) show that the computations.
given a width multiplier m â‰¤ 1, we keep m =(cid:98)nhm(cid:99) heads and use them to generate f ghostfeatures.
the f th ghost feature is generated by.
gf (x) = nonlinear.
gf,h (hh(x)).
.
(2).
(cid:33).
nh(cid:88).
h=1.
(cid:32) m(cid:88).
h=1.
6513orghost featuresoriginal featuresdwconv[cls]thethemat[sep]catsaton[cls]thethemat[sep]catsatonnd(pruned)mha(pruned)ffninputg-mhasoftmaxg-ffndwconv[cls]thethemat[sep]catsaton[cls]thethemat[sep]catsatonndkwhere gf,h is the proposed cheap ghost mod-ule which generates features from the hth atten-tion headâ€™s representation to the f th ghost feature.
relu is used as the nonlinearity function.
thusthe computation of mha in the ghostbert is:.
ghost-mha(x) =.
hh(x)+.
gf (x).
(3).
m(cid:88).
h=1.
f(cid:88).
f =1.
besides being added to the output of mha, theghost modules can also be added to other positionsin mha.
detailed discussions are in section 4.2..2.1.2 ghost module on ffnsimilar to the attention heads in mha, the com-putation of ffn can also be divided into computa-tions for each neuron in the intermediate layer offfn (hou et al., 2020).
with a slight abuse of no-tation, we still use x âˆˆ rnÃ—d as the input to ffn.
denote the number of neurons in the intermediatelayer as df f , the computation of ffn can be writ-ten as: ffn(x) = (cid:80)df fi,:,where w1, w2 are the weights in ffn..i=1 gelu.
xw1:,i.w2.
(cid:16).
(cid:17).
for simplicity, we also use width multiplier mfor ffn as mha, and divide these neurons intonh folds, where each fold contains df = df f /nhneurons.
for the h-th fold, its output can be com-puted as hh(x) = gelu(cid:0)xw1h wherew1(hâˆ’1)df :hdf ,:are the parameters associated with it.
in ffn, nhfolds are computed in parallel to get the output:.
hh = w2.
h = w1.
and w2.
:,(hâˆ’1)df :hdf.
(cid:1) w2.
ffn(x) =.
hh(x)..(4).
nh(cid:88).
h=1.
for width multiplier m, we keep m folds ofneurons and use ghost modules to generate f ghostfeatures as in equation (2).
thus the computationof ffn in the ghostbert can be written as:.
ghost-ffn(x) =.
hh(x)+.
gf (x).
(5).
m(cid:88).
h=1.
f(cid:88).
f =1.
2.2 ghost module.
in the previous section, we discussed where weinsert the ghost modules in the transformer layer.
in this section, we elaborate on the componentsand normalization of the ghost modules..generally speaking, any function can be used asthe ghost module g in equation (2).
consideringthat (i) convolution operation can encode local con-text dependency, as a compensation for the global.
self-attention (wu et al., 2020; jiang et al., 2020);and (ii) features like diagonal or vertical attentionmaps (kovaleva et al., 2019; rogers et al., 2020)can be easily generated by convolving similar oth-ers, we consider using convolution as the basicoperation in the ghost module..2.2.1 convolution typewith a slight abuse of notation, here we still usex âˆˆ rnÃ—d as the input to the convolution, i.e.,the output hh of hth head in mha or hth fold ofneurons in ffn.
denote o âˆˆ rnÃ—d as the outputof the convolution in the ghost module..1-dimensional convolution (conv1d) over thesequence direction encodes local dependency overcontexts, and has shown remarkable performancefor nlp tasks (wu et al., 2019, 2020).
to utilize therepresentation power of conv1d without too muchadditional memory and computation, we choose1-dimensional depthwise separable convolution(dwconv) (wu et al., 2019) for the ghost mod-ule.
compared with conv1d, dwconv performs aconvolution independently over every channel, andreduces the number of parameters from d2k to dk(where k is the convolution kernel size).
denote theweight of the dwconv operation as w âˆˆ rdÃ—k.
after applying dwconv, the output for the ith to-ken and cth channel can be written as:.
oi,c = dwconv(x:,c, wc,:, i, c).
=.
k(cid:88).
m=1.
wc,m Â· xiâˆ’(cid:100) k+1.
2 (cid:101)+m,c..2.2.2 normalizationsince the parameters of the bert backbone modeland the ghost modules can have quite differ-ent scales and optimization behaviors, we use asoftmax function to normalize each convolutionkernel wc,: across the sequence dimension assoftmax(wc,:) before convolution as wu et al.
(2019).
by softmax normalization, the weights inone kernel are summed up to 1, ensuring that theconvolved output has a similar scale as the input.
thus after applying the ghost module, the outputfor the ith token and cth channel can be written as:.
Ë†oi,c = dwconv(x:,c, softmax(wc,:), i, c)..2.3 training details.
to turn a pre-trained bert model into a smaller-sized ghostbert, we do the following three steps:.
6514model-size.
flops(g) #params(m) mnli qnli qqp rte sst-2 mrpc cola sts-b avg..bert-base (devlin et al., 2019)ghostbert (m = 12/12)ghostbert (m = 9/12)ghostbert (m = 6/12)ghostbert (m = 3/12)ghostbert (m = 1/12).
roberta-base (liu et al., 2019)ghostroberta (m = 12/12)ghostroberta (m = 9/12)ghostroberta (m = 6/12)ghostroberta (m = 3/12)ghostroberta (m = 1/12).
electra-small (clark et al., 2020)ghostelectra-small (m = 4/4).
22.522.516.911.35.82.0.
22.522.516.911.35.82.0.
1.71.7.
11011088674632.
125125103826147.
1414.
84.5 92.0 90.9 71.1 92.984.7 92.3 91.1 71.8 93.084.8 92.1 91.2 72.6 92.684.7 92.2 91.2 72.2 92.984.3 91.6 91.4 72.9 94.682.8 90.0 90.5 66.1 92.8.
87.6 92.8 91.9 78.7 94.888.0 93.1 91.9 80.5 95.387.6 92.9 91.9 79.4 95.486.8 92.6 91.6 77.6 94.486.1 91.7 91.2 73.6 94.582.1 89.2 90.5 66.1 93.7.
78.9 87.9 88.3 68.5 88.382.5 89.3 90.7 71.5 92.0.
87.888.087.587.386.586.0.
90.290.789.089.788.083.3.
87.488.7.
58.163.661.158.153.946.1.
63.665.060.857.652.439.8.
56.859.6.
89.8 83.489.7 84.389.8 84.089.2 83.589.2 83.187.8 80.3.
91.2 86.491.3 87.090.7 86.090.3 85.189.2 83.387.4 79.0.
86.8 80.488.4 82.8.table 1: development set results of the baseline pre-trained language models and our proposed method on theglue benchmark.
both pruned and unpruned bert-base (resp.
roberta-base) are used as the backbone mod-els for ghostbert (resp.
ghostroberta).
the unpruned electra-small is used as the backbone model for theghostelectra-small.
m is the width multiplier written in the form of proportion, whose numerator and denom-inator represent the remaining attention heads/folds of neurons and the total number of heads/folds, respectively..pruning.
for a certain width multiplier m, weprune the attention heads in mha and neurons inthe intermediate layer of ffn from a pre-trainedbert-based model following (hou et al., 2020)..distillation.
then we add ghost modules to thepruned model as in section 2.1. suppose thereare l transformer layers.
we distill the knowl-edge from the embedding (i.e., the output of theembedding layer) e, hidden states ml after mhaand fl after ffn (where l = 1, 2, Â· Â· Â· , l) fromthe full-sized teacher model to em, mml ofthe student ghostbert.
following (jiao et al.,2020), we use the augmented data for distilla-tion.
denote mse as the mean squared error,the three loss terms are (cid:96)emb = mse(em, e),(cid:96)mha = (cid:80)ll=1 mse(mml , ml), and (cid:96)f f n =(cid:80)ll=1 mse(fml , fl), respectively.
thus, the distil-.
l , fm.
lation loss function is:.
ldistill = (cid:96)emb + (cid:96)mha + (cid:96)f f n..fine-tuning.
denote y as the predicted logits,we ï¬nally ï¬ne-tune the ghostbert with ground-truth labels Ë†y as:.
lf inetune = crossentropy(Ë†y, y)..note that instead of being applied to pruned mod-els, the cheap ghost modules can also be directlyapplied to a pre-trained model for better perfor-mance while with negligible additional parameters.
and flops.
in this case, the training procedurecontains only the distillation and ï¬ne-tuning steps.
empirically, to save memory and computation,we generate one ghost feature for each mha orffn (i.e., f = 1 in equations (3) and (5)), andlet all ghost modules gf,h share the same param-eters with each other.
as will be shown in sec-tion 3, adding these simpliï¬ed ghost modules al-ready achieve clear performance gain empirically..3 experiment.
in this section, we show the efï¬cacy of the pro-posed method with (pruned) bert (devlin et al.,2019), roberta (liu et al., 2019) and elec-tra (clark et al., 2020) as backbone models..3.1 setup.
experiments are performed on the glue bench-mark (wang et al., 2019), which consists of variousnatural language understanding tasks.
more statis-tics about the glue datasets are in appendix a.1.
following (clark et al., 2020), we report spear-man correlation for sts-b, matthews correlationfor cola and accuracy for the other tasks.
formnli, we report the results on the matched section.
the convolution kernel size in the ghost moduleis set as 3 unless otherwise stated.
the detailedhyperparameters for training the ghostbert arein appendix a.2.
the model with the best develop-ment set performance is used for testing.
for eachmethod, we also report the number of parameters.
6515model.
flops(g) #params(m) mnli qnli qqp rte sst-2 mrpc cola sts-b avg..bert-base (devlin et al., 2019)roberta-base (liu et al., 2019)electra-small (clark et al., 2020).
tinybert6 (jiao et al., 2020)tinybert4 (jiao et al., 2020)convbert-medium (jiang et al., 2020)convbert-small (jiang et al., 2020)mobilebert w/o opt (sun et al., 2020)mobilebert (sun et al., 2020)mobilebert-tiny (sun et al., 2020).
ghostbert (m = 12/12)ghostbert (m = 9/12)ghostbert (m = 6/12)ghostbert (m = 3/12)ghostbert (m = 1/12)ghostroberta (m = 12/12)ghostroberta (m = 9/12)ghostroberta (m = 6/12)ghostroberta (m = 3/12)ghostroberta (m = 1/12)ghostelectra-small (m = 4/4).
22.522.51.7.
11.31.24.72.05.75.73.1.
22.516.911.35.82.022.516.911.35.82.01.7.
11012514.
67151714252515.
1108867463212510382614714.
84.6 90.5 89.2 66.4 93.586.0 92.5 88.7 73.0 94.679.7 87.7 88.0 60.8 89.1.
84.6 90.4 89.1 70.0 93.182.5 87.7 89.2 66.6 92.682.1 88.7 88.4 65.3 89.281.5 88.5 88.0 62.2 89.284.3 91.6 88.3 70.4 92.666.2 92.8-83.3 90.665.1 91.7-81.5 89.5.
84.6 91.1 89.3 70.2 93.184.9 91.0 88.6 69.2 92.984.2 90.8 89.1 69.6 93.183.8 90.789 68.6 93.282.5 89.3 88.7 65.0 92.987.9 93.0 89.6 74.6 95.187.7 92.6 89.5 73.0 94.586.3 92.1 89.5 71.5 94.585.5 91.2 89.1 68.5 93.481.3 88.6 88.5 62.8 92.182.3 88.3 88.5 64.7 91.9.
84.886.583.7.
87.386.484.683.384.5--.
86.986.184.082.581.088.085.786.885.382.888.4.
52.150.554.6.
51.144.156.454.851.150.546.7.
54.653.753.451.341.352.451.951.248.939.755.8.
85.8 80.988.1 82.580.3 78.0.
83.7 81.280.4 78.782.9 79.783.4 78.984.8 81.084.480.1.
--.
83.8 81.784.0 81.383.1 80.982.5 80.280.0 77.688.3 83.687.1 82.887.0 82.484.7 80.881.8 77.283.5 80.4.table 2: test set results of the baseline pre-trained language models, bert compression methods and our proposedmethod on the glue benchmark..and flops at inference (details can be found inappendix a.3)..we compare our proposed method against thefollowing methods: (i) baseline pre-trained lan-guage models: bert-base (devlin et al., 2019),roberta-base (liu et al., 2019) and electra-small (clark et al., 2020); (ii) bert compres-sion methods: tinybert (jiao et al., 2020), con-vbert (jiang et al., 2020), and mobilebert (sunet al., 2020).
the development set results ofroberta-base are from hou et al.
(2020).
thetest set results of electra, bert-base and con-vbert are from jiang et al.
(2020).
the others arefrom their original papers or repositories..3.2 main results.
3.2.1 ghost modules on unpruned models.
table 1 shows the glue development set resultsof the baseline pre-trained language models andour proposed method.
when the cheap ghost mod-ules are directly applied to these unpruned pre-trained models, better performances are achievedwith only negligible additional parameters andflops.
speciï¬cally, adding ghost modules tobert-base, roberta-base and electra-smallincreases the average development accuracy by0.9, 0.6, 2.4 points with only 55.3k more parame-ters, and 14.2m more flops.
for the test set, the.
average performance gains are 0.8, 1.1, 2.4 points..3.2.2 ghost modules on pruned models.
comparison with baseline models.
from ta-ble 1, when the ghost modules are applied to thepruned bert (or roberta) model with m < 1,the proposed ghostbert or ghostroberta alsoachieves comparable performances as bert-baseor roberta-base with fewer flops.
speciï¬cally,ghostbert (m = 6/12) and ghostroberta (m =9/12) perform similarly or even better than bert-base and roberta-base with only 50% and 75%flops, respectively.
in particular, when the com-pression ratio increases (i.e., m = 3/12, 1/12), westill achieve 99.6% performance (resp.
96.3%) withonly 25% flops (resp.
8%) of bert-base model..comparison with other compression meth-ods.
table 2 shows the comparison betweenthe proposed method and other popular bertcompression methods.
under similar parametersizes or flops, the proposed ghostbert per-forms comparably as the other bert compressionmethods, while ghostroberta often outperformsthem.
in particular, ghostelectra-small hasover 1.5 points or higher accuracy gain than othersimilar-sized small models like electra-small,tinybert4 and convbert-small..in table 3 and figure 1, we also compare the.
6516pruned bert with and without ghost modules.
for fair comparison, for the pruned model withoutghost module, we use the same training procedureas section 2.3. as can be seen, adding the ghostmodules achieves considerable improvement withnegligible additional memory and computation..pruned bert.
ghostbert.
m.1/123/126/129/12.
flops(g).
#params(m).
avg.
acc.
flops(g).
#params(m).
avg.
acc.
2.05.811.316.9.
32466788.
78.281.882.283.2.
2.05.811.316.9.
32466788.
80.383.183.584.0.table 3: comparison of ghostbert and pruned bert.
m stands for the width multiplier..3.3 ablation study.
in this section, we perform ablation study in the (i)training procedure: including data augmentation(da) and knowledge distillation (kd); (ii) ghostmodule: including convolution kernel size, soft-max normalization over the convolution kernel andnonlinearity for each ghost feature in equation (2)..training procedure.
table 4 veriï¬es the ef-fectiveness of the data augmentation (da) andknowledge distillation (kd) upon the ghostbertmodel with width multiplier m âˆˆ {3/12, 1/12}.
the ghostbert incurs severe accuracy drop with-out da and kd.
with a drop of 3.5 and 6.4 pointson average, for m = 3/12 and 1/12, respectively..ghost module.
table 4 also shows the effective-ness of the softmax normalization over the convolu-tion kernel and relu nonlinearity in equation (2).
as can be seen, dropping the softmax normalizationor relu nonlinearity reduces the average accuracyby 0.8 and 1.6 points respectively for m = 3/12,and 0.9 and 2.2 points respectively for m = 1/12.
further, we explore whether the kernel size playsan important role in the dwconv in the ghost mod-ule.
figure 3 shows the results of ghostbert withwidth multipliers m âˆˆ {3/12, 1/12}, with variousconvolution kernel sizes in dwconv.
average ac-curacy over ï¬ve tasks is reported.
detailed resultsfor each task can be found in table 9 in appendixb.1.
as can be seen, the performance of ghost-bert increases ï¬rst and then decreases graduallyas the kernel size increases.
for both width multi-pliers, kernel size 3 performs best and is used asthe default kernel size in other experiments unlessotherwise stated..4 discussion.
in this section, we discuss about different choicesof which type of convolution to use in the ghostmodule (section 4.1), and where to posit the ghostmodules in a bert model (section 4.2)..4.1 ghost module types.
besides the dwconv in section 2.2, in this sec-tion, we discuss more options for the convolutionin the ghost module.
we follow the notation in sec-tion 2.2 and denote the input, output, kernel size ofthe convolution as x, w and k, respectively..1-dimensional convolution.
if the kernel con-volves input over the sequence direction (abbre-viated as conv1d s), the number of input andoutput channel is d, and the weight w has shapew âˆˆ rdÃ—dÃ—k.
after applying conv1d s, the out-put for the ith token and cth channel is:.
oi,c = conv1d s(x, wc,:,:, i, c).
d(cid:88).
k(cid:88).
=.
j=1.
m=1.
wc,j,k Â· xiâˆ’(cid:100) k+1.
2 (cid:101)+m,j..if the kernel convolves input over the feature di-rection (abbreviated as conv1d f), the number ofinput and output channel is n, and the weight hasshape w âˆˆ rnÃ—nÃ—k.
after applying conv1d f,the output for the ith token and cth channel is:.
oi,c = conv1d f(x, wi,:,:, i, c).
n(cid:88).
k(cid:88).
=.
j=1.
m=1.
wi,j,m Â· xj,câˆ’(cid:100) k+1.
2 (cid:101)+m..2-dimensional convolution (conv2d).
forconv2d, the number of input and output channelsare both 1, and thus the weight w has shapew âˆˆ r1Ã—1Ã—kÃ—k.
after applying conv2d, theoutput for the ith token and cth channel is:.
oi,c = conv2d(x, w, i, c).
=.
k(cid:88).
k(cid:88).
w=1.
h=1.
w:,:,h,w Â· xiâˆ’(cid:100) k+1.
2 (cid:101)+h,câˆ’(cid:100) k+1.
2 (cid:101)+w..4.1.1 comparison of different convolutionstable 5 shows the comparison of using differ-ent convolutions for the ghost module.
for 1-dimensional convolution, conv1d s performs bet-ter conv1d f. this may because that convolvingover the sequence urges the model to learn the de-pendencies among tokens..6517m model.
mnli qnli qqp rte sst-2 mrpc cola sts-b avg..3/12.
1/12.
ghostbert.
- da & kd- softmax- relu.
ghostbert.
- da & kd- softmax- relu.
84.380.284.384.0.
82.876.082.682.7.
91.688.591.591.7.
90.083.490.089.8.
91.490.090.991.0.
90.586.690.490.6.
72.963.271.870.8.
66.158.165.360.3.
94.691.692.392.3.
92.886.692.192.0.
86.583.885.585.8.
86.080.685.584.8.
53.952.552.647.6.
46.135.840.837.8.
89.286.789.188.6.
87.884.488.187.1.
83.179.682.381.5.
80.373.979.478.1.table 4: ablation study of data augmentation (da), knowledge distilla-tion (kd), softmax normalization over the convolution kernel, and non-linearity.
results on the glue development set are reported..figure 3: average score over ï¬ve taskswith various kernel sizes of dwconvin the ghost module..m convolution type flops(g).
#params(m) mnli qnli qqp rte sst-2 mrpc cola sts-b avg..3/12.
1/12.
conv1d sconv1d fconv2dours: dwconv.
conv1d sconv1d fconv2dours: dwconv.
16.67.65.85.8.
12.93.92.12.0.
88474646.
74333232.
83.684.083.784.3.
82.682.281.782.8.
91.391.591.791.6.
90.186.689.290.0.
91.191.091.091.4.
90.490.090.190.5.
70.061.768.272.9.
66.154.963.266.1.
92.592.292.594.6.
92.092.391.792.8.
86.587.086.586.5.
85.572.883.886.0.
52.348.950.453.9.
47.531.237.946.1.
89.389.089.389.2.
88.287.388.087.8.
82.180.781.783.1.
80.374.778.280.3.table 5: comparison of different convolutions in the ghost module.
the convolution kernel size is 3. the backbonemodel is bert-base.
results on the glue development set are reported..though 2-dimensional convolution (conv2d)is quite successful in cv tasks, it performs muchworse than conv1d s here.
this may because thetwo dimensions of feature maps in cv tasks encodesimilar information, while those of hidden statesin transformers encode quite different informa-tion (i.e., feature and sequence).
thus conv2d re-sults in worse performance than conv1d s, thoughmuch fewer parameters and flops are required..on the other hand, dwconv achieves compara-ble performance as conv1d s, while being muchmore efï¬cient in terms of number of parametersand flops, by performing the convolution inde-pendently over every feature dimension..4.2 ghost module positions.
in this section, we explore more possible positionsof adding the ghost module.
for mha, besidesadding ghost module after the projection layer(after o in figure 4(c)) as in section 2.1.1,we can also add it right after calculating the at-tention score (after qk in figure 4(a)), or af-ter multiplying the attention score and the valuelayer (after v in figure 4(b)).
for ffn, be-sides adding the ghost module after the secondlinear layer (after ffn2 in figure 4(e)) as insection 2.1.1, we can also add it after the intermedi-ate layer (after ffn1 in figure 4(d)).
note that.
we use conv2d as the ghost module for afterqk because the attention map encodes attentionprobabilities in both dimensions.
for after qkand after v, to match the dimension of other pa-rameters, the number of input and output channelsare m and nh âˆ’ m , respectively..table 6 shows the results of adding one ghostmodule to the same position for each transformerlayer.
as can be seen, adding ghost module uponthe attention maps (after qk) performs best.
however, since the parameters in the value andprojection layer of mha are left unpruned, afterqk has much more parameters and flops thanthe other positions.
adding ghost modules tothe other four positions has similar average accu-racy.
thus in this work, for mha, we choose themost memory- and computation-efï¬cient strategyafter o. similarly, for ffn, we also add ghostmodules to the ï¬nal output (after ffn2).
fromtable 6, our way of adding ghost modules has com-parable performance as after qk, while beingmuch more efï¬cient in parameter size and flops..5 related work.
5.1 network pruning in transformer.
pruning removes unimportant connections or neu-rons in the network.
compared with pruning con-nections (yu et al., 2019; gordon et al., 2020; sanh.
6518(a) after qk..(b) after v..(c) after o..(d) after ffn1..(e) after ffn2..figure 4: candidate positions to add the ghost module in the transformer layer..position.
after qkafter vafter oafter ffn1after ffn2.
convolutiontype.
flops(g).
#params(m).
avg.
acc.
conv2ddwconvdwconvdwconvdwconv.
5.43.62.02.02.0.
2.0.
4539323232.
32.
75.974.374.474.374.3.
75.8.ours: after o&ffn2 dwconv.
table 6: comparison of different positions to add theghost module.
average development set accuracy onï¬ve glue tasks (rte, sst-2, mrpc, cola and sts-b) are reported.
the pruned bert with width multi-plier 1/12 is used as backbone model ..et al., 2020), structured pruning prunes away agroup of parameters without changing the modeltopology and is more favored for hardware and realinference speedup..in the width direction, michel et al.
(2019); voitaet al.
(2019) retain the performance after pruning alarge percentage of attention heads in a structuredmanner.
besides attention heads, mccarley et al.
(2019) also prune the neurons and the embeddings.
in the depth direction, pruning transformer lay-ers is proposed in layerdrop (fan et al., 2019)via structured dropout.
efï¬cient choice of trans-former layers at inference via early exit are alsoproposed in (liu et al., 2020; xin et al., 2020; zhouet al., 2020).
hou et al.
(2020) perform structuredpruning in both width and depth directions.
theimportance of attention heads and neurons in theintermediate layer of feed-forward network is mea-sured by their impact on the loss, and the leastimportant heads and neurons are pruned away..5.2 enhanced representation in.
transformer-based models.
various methods have been proposed to use linearor convolution operations to enhance the represen-tation of the transformer layers..the ï¬rst group of research works replaces the.
self-attention mechanism or feed-forward networkswith simpler and more efï¬cient convolution oper-ations, while maintaining comparable results.
wuet al.
(2019) introduce the token-based dynamicdepth-wise convolution to compute the importanceof context elements, and achieve better results invarious nlp tasks.
iandola et al.
(2020) replaceall the feed-forward networks with grouped con-volution.
adabert (chen et al., 2020) uses dif-ferentiable neural architecture to search for moreefï¬cient convolution-based nlp models..the second group uses linear or convolutionalmodule along with the self-attention mechanismfor more powerful representation.
the new modulecan be incorporated though serial connection tothe original self-attention mechanism (mehta et al.,2020), or be used in parallel with the original self-attention mechanism (wu et al., 2020; jiang et al.,2020) to capture both local and global context de-pendency.
serial and parallel connections of theselinear or convolution operations to transformerlayers are also extended to multi-task (houlsbyet al., 2019; stickland and murray, 2019) and multi-lingual tasks (pfeiffer et al., 2020)..note that the proposed ghost modules are orthog-onal to the above methods in that these modules areused to generate more features for the transformermodels and can be easily integrated into existingmethods to boost their performance..6 conclusion.
in this paper, we propose ghostbert to gener-ate more features in pre-trained model with cheapoperations.
we use the softmax-normalized 1-dimensional convolutions as ghost modules andadd them to the output of the mha and ffn ofeach transformer layer.
empirical results on bert,roberta and electra demonstrate that addingthe proposed ghost modules enhances the repre-sentation power and boosts the performance of theoriginal model by supplying more features..6519linearlinear linearinputmat mulscale & softmaxghostmat mulnonlinearlinearinputghostnonlinearlinearinputghostlinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mulnonlinearlinearinputghostnonlinearlinearinputghostlinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mulnonlinearlinearinputghostnonlinearlinearinputghostlinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mulnonlinearlinearinputghostnonlinearlinearinputghostlinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mulnonlinearlinearinputghostnonlinearlinearinputghostlinearlinear linearinputmat mulscale & softmaxghostmat mullinearlinear linearinputmat mulscale & softmaxghostmat mulacknowledgement.
we thank mindspore for the partial supportof this work, which is a new deep learningcomputing framework.
given the superiorperformance of huawei ascend ai processorand mindspore computing framework, our codewill be released based on mindspore at (https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/nlp/ghostbert)..references.
h. bai, w. zhang, l. hou, l. shang, j. jin, x. jiang,q. liu, m. lyu, and i. king.
2021. binarybert: push-ing the limit of bert quantization.
in annual meetingof the association for computational linguistics..d. chen, y. li, m. qiu, z. wang, b. li, b. ding,h. deng, j. huang, w. lin, and j. zhou.
2020. ad-abert: task-adaptive bert compression with differ-entiable neural architecture search.
in internationaljoint conference on artiï¬cial intelligence..k. clark, m. luong, q. le, and c. manning.
2020.electra: pre-training text encoders as discrimina-tors rather than generators.
in international confer-ence on learning representations..j. devlin, m. chang, k. lee, and k. toutanova.
2019.bert: pre-training of deep bidirectional transform-ers for language understanding.
in north americanchapter of the association for computational lin-guistics..a. fan, e. grave, and a. joulin.
2019. reducing trans-former depth on demand with structured dropout.
ininternational conference on learning representa-tions..a. fan, p. stock, b. graham, e. grave, r. gribon-val, h. jegou, and a. joulin.
2020. training withquantization noise for extreme model compression.
preprint arxiv:2004.07320..m. a. gordon, k. duh, and n. andrews.
2020. com-pressing bert: studying the effects of weight pruningon transfer learning.
preprint arxiv:2002.08307..k. han, y. wang, q. tian, j. guo, c. xu, and c. xu.
2020. ghostnet: more features from cheap opera-tions.
in ieee/cvf conference on computer visionand pattern recognition, pages 1580â€“1589..l. hou, z. huang, l. shang, x. jiang, x. chen, andq. liu.
2020. dynabert: dynamic bert with adaptivewidth and depth.
in advances in neural informationprocessing systems..n. houlsby, a. giurgiu, s. jastrzebski, b. morrone,q. laroussilhe, a. gesmundo, m. attariyan, ands. gelly.
2019. parameter-efï¬cient transfer learningfor nlp.
in international conference on machinelearning..f. iandola, a. shaw, r. krishna, and k. keutzer.
2020.squeezebert: what can computer vision teach nlpabout efï¬cient neural networks?
in proceedings ofsustainlp: workshop on simple and efï¬cient natu-ral language processing..z. jiang, w. yu, d. zhou, y. chen, j. feng, and s. yan.
2020. convbert: improving bert with span-baseddynamic convolution.
in advances in neural infor-mation processing systems..x. jiao, y. yin, l. shang, x. jiang, x. chen, l. li,f. wang, and q. liu.
2020. tinybert: distillingbert for natural language understanding.
in find-ings of empirical methods in natural language pro-cessing..o. kovaleva, a. romanov, a. rogers,.
anda. rumshisky.
2019.revealing the dark se-crets of bert.
in conference on empirical methodsin natural language processing, pages 4356â€“4365..z. lan, m. chen, s. goodman, k. gimpel, p. sharma,and r. soricut.
2020. albert: a lite bert for self-supervised learning of language representations.
ininternational conference on learning representa-tions..w. liu, p. zhou, z. zhao, z. wang, h. deng, and q. ju.
2020. fastbert: a self-distilling bert with adaptive in-ference time.
in annual meeting of the associationfor computational linguistics..y. liu, m. ott, n. goyal, j. du, m. joshi, d. chen,o. levy, m. lewis, l. zettlemoyer, and v. stoyanov.
2019. roberta: a robustly optimized bert pretrain-ing approach.
preprint arxiv:1907.11692..j. s. mccarley, r. chakravarti, and a. sil.
2019. struc-tured pruning of a bert-based question answeringmodel.
preprint arxiv:1910.06360..s. mehta, m. ghazvininejad, s. iyer, l. zettlemoyer,and h. hajishirzi.
2020. delight: very deep andlight-weight transformer.
corr, abs/2008.00623..p. michel, o. levy, and g. neubig.
2019. are sixteenheads really better than one?
in advances in neuralinformation processing systems..mindspore.
https://www.mindspore.cn.
2021..j. pfeiffer, i. vuliÂ´c, i. gurevych, and s. ruder.
2020.mad-x: an adapter-based framework for multi-taskin conference on empiri-cross-lingual transfer.
cal methods in natural language processing, pages7654â€“7673..a. rogers, o. kovaleva, and a. rumshisky.
2020. aprimer in bertology: what we know about how bertworks.
preprint arxiv:2002.12327..v. sanh, l. debut, j. chaumond, and t. wolf.
2019.distilbert, a distilled version of bert: smaller, faster,cheaper and lighter.
preprint arxiv:1910.01108..6520v. sanh, t. wolf, and a. rush.
2020. movementin ad-pruning: adaptive sparsity by ï¬ne-tuning.
vances in neural information processing systems,volume 33..s. shen, z. dong, j. ye, l. ma, z. yao, a. gholami,m. w. mahoney, and k. keutzer.
2020. q-bert: hes-sian based ultra low precision quantization of bert.
in aaai conference on artiï¬cial intelligence..a. c. stickland and i. murray.
2019. bert and pals:projected attention layers for efï¬cient adaptation inmulti-task learning.
in international conference onmachine learning, pages 5986â€“5995.
pmlr..s. sun, y. cheng, z. gan, and j. liu.
2019. patientknowledge distillation for bert model compression.
in conference on empirical methods in natural lan-guage processing..z. sun, h. yu, x. song, r. liu, y. yang, and d. zhou.
2020. mobilebert: a compact task-agnostic bertfor resource-limited devices.
in annual meeting ofthe association for computational linguistics..e. voita, d. talbot, f. moiseev, r. sennrich, andi. titov.
2019. analyzing multi-head self-attention:specialized heads do the heavy lifting, the rest canbe pruned.
in annual meeting of the association forcomputational linguistics..a. wang, a. singh, j. michael, f. hill, o. levy, ands. bowman.
2019. glue: a multi-task benchmarkand analysis platform for natural language under-standing.
in international conference on learningrepresentations..f. wu, a. fan, a. baevski, y. dauphin, and m. auli.
2019. pay less attention with lightweight and dy-namic convolutions.
in international conference onlearning representations..z. wu, z. liu, j. lin, y. lin, and s. han.
2020. litetransformer with long-short range attention.
in inter-national conference on learning representations..j. xin, r. tang, j. lee, y. yu, and j. lin.
2020. dee-bert: dynamic early exiting for accelerating bert in-in annual meeting of the association forference.
computational linguistics..h. yu, s. edunov, y. tian, and a. s morcos.
2019.playing the lottery with rewards and multiple lan-guages: lottery tickets in rl and nlp.
in internationalconference on learning representations..w. zhang, l. hou, y. yin, l. shang, x. chen, x. jiang,and q. liu.
2020. ternarybert: distillation-awareultra-low bit bert.
in empirical methods in naturallanguage processing..w. zhou, c. xu, t. ge, j. mcauley, k. xu, and f. wei.
2020. bert loses patience: fast and robust inferencewith early exit.
in advances in neural informationprocessing systems..6521a experiment settings.
a.1 statistics of glue datasets.
the glue benchmark (wang et al., 2019) consistsof various sentence understanding tasks, includ-ing two single-sentence classiï¬cation tasks (colaand sst-2), three similarity and paraphrase tasks(mrpc, sts-b and qqp), and four inference tasks(mnli, qnli, rte and wmli).
for mnli task,we report the result on the matched section.
forwinograd schema (wnli), it is a small naturalinference dataset while even a majority baselineoutperforms many methods on it.
as is notedin the glue ofï¬cial website1, there are some is-sues with the construction of it.
like previouswork (hou et al., 2020; jiang et al., 2020), wedo not experiment on wnli.
we use the defaulttrain/development/test splits from the ofï¬cial web-site..corpus train.
test task.
metrics.
single-sentence tasks.
colasst-2.
8.5k67k.
1k1.8k.
acceptabilitysentiment.
matthews corr.
acc..similarity and paraphrase tasks.
mrpcsts-bqqp.
3.7k7k364k.
1.7k1.4k391k.
paraphrasesentence similarity spearman corr.
paraphrase.
acc..acc..inference tasks.
mnliqnlirtewnli.
393k105k2.5k634.
20k nli5.4k qa/nli3k nli146.coreference/nli.
matched acc.
acc.
acc.
acc..table 7: statistics of the glue datasets.
all tasks aresingle-sentence or sentence-pair classiï¬cation tasks, ex-cept sts-b, which is a regression task.
mnli has threeclasses while all other classiï¬cation tasks have two..a.2 hyperparameters.
we show the detailed hyperparameters for the dis-tillation and ï¬ne-tuning stages in section 2.3 ofthe proposed method on the glue benchmark intable 8..a.3 flops.
floating-point operations (flops) measures thenumber of ï¬‚oating-point operations that the modelperforms for a single process and can be used as ameasure of the computational complexity of deepneural network models.
to count the flops, we.
1https://gluebenchmark.com/faq.
distillation fine-tuning.
batch sizelearning rateadam Î²1adam Î²2warmup stepslearning rate decayweight decaygradient clippingdropoutattention dropoutdistillationÎ»1, Î»2, Î»3training epochs (mnli, qqp)training epochs (other datasets).
322e âˆ’ 50.90.9990linear010.10.1y0, 1, 113.
322e âˆ’ 50.90.9990linear010.10.1y1, 0, 033.table 8: hyperparameters for the distillation and ï¬ne-tuning stages in training ghostbert on the gluebenchmark..follow the setting in (hou et al., 2020) and in-fer flops with batch size 1 and sequence length128. since the operations in the embedding lookupare relatively cheap compared to those in trans-former layers, following (hou et al., 2020; sunet al., 2020), we do not count them.
note that thereported flops for electra (clark et al., 2020)and convbert(jiang et al., 2020) in their origi-nal papers include those for the embedding lookup,and are slightly different from the numbers in thispaper..b more experiment results.
b.1 full results of different convolution.
kernel sizes.
in table 9, we show the detailed results of differentconvolutions kernel sizes for each of the ï¬ve tasks(sst-2, mrpc, cola, sts-b and rte).
as canbe seen, for each task, dwconv with kernel size 3has the best performance..m kernel size sst-2 mrpc cola sts-b rte avg..3/12.
1/12.
135917.
135917.
92.594.692.792.692.4.
92.192.892.292.192.0.
85.386.586.585.584.8.
85.386.085.384.884.3.
54.853.953.553.453.3.
41.746.141.141.440.9.
89.189.289.089.088.9.
87.687.887.587.587.5.
70.8 78.572.9 79.470.4 78.469.3 78.068.2 77.5.
64.3 74.266.1 75.864.6 74.263.9 73.963.5 73.7.table 9: comparison of different kernel sizes on thedevelopment set on ï¬ve tasks of glue.
m stands forthe width multiplier..6522m model flops(g).
#params(m) mnli qnli qqp rte sst-2 mrpc cola sts-b avg..1/12.
3/12.
6/12.
9/12.
pruneghost.
pruneghost.
pruneghost.
pruneghost.
2.02.0.
5.85.8.
11.311.3.
16.916.9.
3232.
4646.
6767.
8888.
82.082.8.
83.684.3.
82.984.7.
84.284.8.
88.990.0.
91.191.6.
90.692.2.
91.592.1.
90.490.5.
90.991.4.
90.991.2.
91.091.2.
60.766.1.
68.672.9.
71.872.2.
72.272.6.
91.292.8.
92.694.6.
92.192.9.
92.492.6.
83.186.0.
85.186.5.
86.887.3.
86.887.5.
42.646.1.
53.753.9.
54.258.1.
58.461.1.
86.487.8.
88.589.2.
88.489.2.
89.489.8.
78.280.3.
81.883.1.
82.283.5.
83.284.0.table 10: development set results of the pruned bert and ghostbert.
m is the width multiplier of the model..position.
convolution type flops(g) #params(m) rte sst-2 mrpc cola sts-b avg..after qkafter vafter oafter ffn1after ffn2.
ours: after o&ffn2.
conv2ddwconvdwconvdwconvdwconv.
dwconv.
5.43.62.02.02.0.
2.0.
4539323232.
32.
65.362.163.264.662.5.
91.991.691.791.291.5.
66.1.
92.8.
85.886 785.584.684.8.
86.0.
48.544.343.843.344.9.
46.1.
87.987.787.887.787.7.
87.8.
75.974.374.474.374.3.
75.8.table 11: comparison of different ghost positions on the development set on ï¬ve tasks of glue.
bert-base is setas backbone model with the width multiplier 1/12..2.0g respectively, accounting for 7.2% and 9.1%of the backbone model..when f increases, the accuracy of ghostbertï¬rst increases slowly and soon begins to saturateor decrease.
e.g., for ghostbert (m = 1/12),the average development accuracy on glue onlyincreases from 80.3 to 80.6 when f increases from1 to 4, and then saturates when f > 4. for ghost-bert (m = 3/12), the highest accuracy 83.1 isachieved when f = 1 or 2, and then the accuracybegins to decrease..thus in the paper, we simply choose f=1 whichis cheap, but already achieves good performanceon most tasks..b.2 full results of pruned bert.
in table 10, we show the detailed results of thepruned bert and the ghostbert for each task.
we can see that under the same training procedure,the ghostbert outperforms the pruned bert overall compared sizes..b.3 full results of different positions.
table 11 shows the detailed results of adding ghostmodules to different positions of the model..b.4 generating more features.
as is mentioned at the end of section 2.3, we gen-erate only one ghost feature for each mha andffn, i.e., f = 1 to save computation and memory.
indeed, our framework has no limitation on f , andalso allows the model to generate more features(i.e., f > 1).
in this section, we discuss the rela-tionship between generating more ghost featuresand the computation/memory requirements..following the notation in section 2 and omittingthe cheap computation of relu and softmax, gen-erating f ghost features from m features for alll layers requires 2lm f dk additional parametersand 4lm f ndk additional flops.
both of themscale linearly as f , and can be large when f islarge.
for instance, for bert-base with d = 768,when n = 128, k = 3, m = 12 and f = 12,the additional #parameters and flops are 8m and.
6523