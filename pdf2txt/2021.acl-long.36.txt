introducing orthogonal constraint in structural probes.
tomasz limisiewicz and david mareˇcekinstitute of formal and applied linguistics, faculty of mathematics and physicscharles university, prague, czech republic{limisiewicz, marecek}@ufal.mff.cuni.cz.
(a) structural probe.
abstract.
with the recent success of pre-trained mod-els in nlp, a signiﬁcant focus was put on in-terpreting their representations.
one of themost prominent approaches is structural prob-ing (hewitt and manning, 2019), where alinear projection of word embeddings is per-formed in order to approximate the topology ofdependency structures.
in this work, we intro-duce a new type of structural probing, wherethe linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling thatidentiﬁes and scales the most relevant dimen-sions.
in addition to syntactic dependency, weevaluate our method on novel tasks (lexicalhypernymy and position in a sentence).
wejointly train the probes for multiple tasks andexperimentally show that lexical and syntacticinformation is separated in the representations.
moreover, the orthogonal constraint makes thestructural probes less vulnerable to memoriza-tion..(b) orthogonal structural probe.
figure 1: comparison of the structural probe of he-witt and manning (2019) and the orthogonal struc-tural probe proposed by us..1.introduction.
latent representations of neural networks encodespeciﬁc linguistic features.
recently, a lot of fo-cus was devoted to interpret these representationsand analyze structures captured by the deep mod-els.
one of the most popular analysis methodsis probing (belinkov et al., 2017; blevins et al.,2018; linzen et al., 2016; liu et al., 2019).
thepre-trained model’s 1 parameters are ﬁxed, and itslatent states or outputs are then fed into a simpleneural network optimized to solve an auxiliary task,e.g., semantic, syntactic parsing, anaphora resolu-tion, morphosyntactic tagging, etc.
the amountof language information stored in the representa-tions can be evaluated by measuring the speciﬁclanguage task’s performance..probing experiments usually involve classiﬁca-tion tasks.
lately, hewitt and manning (2019)proposed structural probes, which use regressionas an optimization objective.
they train a linearprojection layer to approximate: 1. dependencytree distances between words2 by the euclideandistance between transformed vectors; 2. the treedepth of a word by the norm of its vector..in figure 1, we visualize our orthogonal struc-tural probe.
a linear transformation is replaced byan orthogonal transformation (rotation of the em-bedding space), and product-wise multiplication ofrotated vectors by a scaling vector to get the ﬁnalprojections.
our motivation is to obtain an embed-ding space that is isomorphic with the original one,and the impact of each dimension can be evaluated.
1typically models for language modeling or machine trans-.
2tree distance is the length of the tree path between two.
lation are analyzed..tokens.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages428–442august1–6,2021.©2021associationforcomputationallinguistics428by analyzing scaling vector’s weights.
we elabo-rate on mathematical properties and training detailsin section 3..in addition to dependency trees used by hewittand manning (2019), we introduce new structuraltasks related to lexical hypernymy and word’s po-sition in the sentence.
we also employ a con-trol task, in which we evaluate the memorizationof randomly generated trees.
orthogonal struc-tural probes let us optimize for multiple objectivesjointly by keeping a shared orthogonal transfor-mation matrix and changing task-speciﬁc scalingvectors..we will answer the following questions:.
1. do our orthogonal structural probes achievecomparable or better performance to the struc-tural probes of hewitt and manning (2019)?.
2. can we ﬁnd other phenomena such as lexicalhypernymy and a word’s absolute position in asentence using orthogonal structural probe?
how vulnerable are the probes to memorizingrandom data?.
3. is it possible to effectively train orthogonalstructural probes jointly for multiple auxil-iary objectives, i.e., depth and distance, ormultiple types of structures mentioned in theprevious question?.
4. can we identify particular dimensions of theembedding space that encode particular lin-guistic structures?
are there any superﬂuousdimensions?.
5. if yes, what is the relationship between sub-.
spaces encoding distinct structures?.
2 related work.
basic linguistic features can be easily extractedfrom the contextual representations (liu et al.,2019).
probing was intensively used to investi-gate the representation of morphological informa-tion (mainly pos tags) in hidden states of ma-chine translation systems and language models (be-linkov et al., 2017; peters et al., 2018; tenney et al.,2019b).
besides the work of hewitt and manning(2019), probing for dependency syntax was per-formed by tenney et al.
(2019a) and blevins et al.
(2018).
they utilize a binary classiﬁer to predictdependency edges.
in work contemporary to ours,.
ravichander et al.
(2020) employ a softmax classi-ﬁer to show that bert can be successfully probedfor hypernymy..there is an ongoing debate on which probe ar-chitectures offer a good insight into underlying rep-resentations.
zhang and bowman (2018) showedthat a pos tagger on top of a frozen randomly ini-tialized lstm model achieves unexpectedly highresults.
in the work of hewitt and liang (2019),the multilayer perceptron probes display similaraccuracy for predicting pos tags as for randomlyassigned tags.
these symptoms underscore howcrucial it is to carefully consider the probe’s archi-tecture to avoid reaching spurious conclusions.
itis good practice to monitor additional aspects ofthe probe beyond performance on a linguistic task,such as selectivity (hewitt and liang, 2019), orcomplexity (pimentel et al., 2020).
the recent stateof knowledge is summarized in surveys on probing(belinkov and glass, 2019) and interpretation ofbert’s representations (rogers et al., 2020)..orthogonality has been applied broadly in theﬁeld of deep learning, especially to cope with ex-ploding/vanishing gradient problem in recurrentneural networks (arjovsky et al., 2016; jing et al.,2017a; wisdom et al., 2016).
in this work, weuse regularization to enforce the orthogonality of adense layer.
in literature, such an approach is called“soft constraint” (bansal et al., 2018; vorontsovet al., 2017).
alternatively, “hard constraint” as-sumes parameterization of a network such that thetransformation of latent states is orthogonal by def-inition (arjovsky et al., 2016; jing et al., 2017b).
there are a few examples of orthogonality applica-tions in nlp: in rnn language model (dangovskiet al., 2019); in performer (choromanski et al.,2020), which is a more efﬁcient counterpart oftransformer (vaswani et al., 2017).
best to ourknowledge, we are the ﬁrst to use orthogonal trans-formation in probing..3 method.
in this section, we ﬁrst review the structural prob-ing proposed by hewitt and manning (2019) andthen introduce our orthogonal structural probe..3.1 structural probes.
in the previous work, a linear transformation isoptimized to transform the contextual word repre-sentations produced by a pre-trained neural model(e.g.
bert devlin et al.
(2019), elmo peters et al..429(2018)).
the squared l2 norm of the differencesbetween transformed word vectors approximate thetree distance between them:.
db(hi, hj)2 = (b(hi − hj))t (b(hi − hj)), (1).
where b is the linear transformation matrix andhi, hj are the vector representations of words atpositions i and j..the probe is optimized to approximate the dis-tance between tokens in the dependency tree (dt )by gradient descent objective:.
minb.
1s2.
i,j.
(cid:88)(cid:12)dt (wi, wj) − db(hi, hj)2(cid:12)(cid:12)(cid:12),.
(2).
where s is the length of a sentence..moreover,.
the same work introduced depthprobes, where vectors were linearly transformedso that the squared l2 length of the mapping ap-proximate the token’s depth in a dependency tree:.
||hi||2.
b = (bhi)t (bhi).
gradient descent objective is analogical:.
minb.
1s.i.
(cid:88).
(cid:12)(cid:12)(cid:107)wi(cid:107)t − (cid:107)hi(cid:107)2b.
(cid:12)(cid:12).
3.2 orthogonal structural probes.
we introduce orthogonality to structural probes.
for that purpose, we perform the singular valuedecomposition of the matrix b.b = u · d · v t ,.
(5).
where the matrices u and v are orthogonal, and dis diagonal.
notably, when we substitute b withu · d · v t in eq.
(1), the matrix u cancels out.
itcan be easily shown by rearranging the variables inthe equation:3.db(hi, hj)2= (dv t (hi − hj))t (dv t (hi − hj)).
(6).
we can replace the diagonal matrix d with avector ¯d and use element-wise product (we will call¯d the scaling vector).
finally, we get the followingequation for orthogonal distance probe:.
d ¯dv t (hi, hj)2= ( ¯d (cid:12) v t (hi − hj))t ( ¯d (cid:12) v t (hi − hj)).
(7).
3a complete derivation can be found in the appendix..the same reasoning can be applied to eq.
(3) toobtain orthogonal depth probe:.
||hi||2.
¯dv t = ( ¯d (cid:12) v t hi)t ( ¯d (cid:12) v t hi)we showed that orthogonal structural probe ismathematically equivalent to standard structuralprobe..(8).
3.3 multitask training.
orthogonal structural probe can be easily adaptedto multitask probing for a set of objectives o. weuse one shared orthogonal transformation and dif-ferent scaling vectors for each task.
in one batch,we compute a loss for a speciﬁc objective.
foreach batch (with objective o ∈ o), a forward passconsists of multiplication by a shared orthogonalmatrix v t and product-wise multiplication by adesignated vector ¯do.
all the batches are shufﬂedtogether in a training epoch..3.4 orthogonality regularization.
(3).
(4).
we use double soft orthogonality regularization(dso) proposed by bansal et al.
(2018) to coerceorthogonality of the matrix v during training:.
λodso(v ) = λo(||v t v −i||2.
f +||v v t −i||2f )(9)|| · ||f stands for the frobenius norm of a matrix..3.5 sparsity regularization.
in further experiments, we investigate the effectsof sparsity in scaling vector.
for that purpose, wecompute the l1 norm and add it to the training loss..λs(cid:107) ¯d(cid:107)1.
(10).
3.6 training objective.
altogether, the loss equation in orthogonal dis-tance probe for objective o ∈ o is the following:.
lo,dist.
=.
(cid:88)(cid:12)dt (wi, wj) − d ¯dov t (hi, hj)2(cid:12)(cid:12)(cid:12)+.
1s2.
i,j.
+λodso(v ) + λs(cid:107) ¯do(cid:107)1(11).
and in orthogonal depth probe:.
lo,depth =.
1s.(cid:88)(cid:12)(cid:12)(cid:107)wi(cid:107)t − (cid:107)hi(cid:107)2.
(cid:12)(cid:12)+.
¯dov t.i+λodso(v ) + λs(cid:107) ¯do(cid:107)1.
(12).
the loss is normalized by the number of predictionsin a sentence and averaged across a batch..4304 experiments.
we train probes on top of each of 24 layers ofenglish bert large cased model (devlin et al.,2019) implemented by huggingface (wolf et al.,2020).
we optimize for the approximation of depthand distance in four types of structures: syntacticdependency, lexical hypernymy, absolute positionin a sentence, and randomly generated trees.
inthe following subsection, we expand upon thesestructures..4.1 data and objectives.
in our experiments, we use training, evaluation,and test sentences from universal dependenciesenglish web treebank (silveira et al., 2014).
de-pending on the objective, we reveal only partialrelevant annotation from the dataset..dependency syntax we probe for syntacticstructure in universal dependencies parse trees(nivre et al., 2020).
dependency trees are anno-tated in english web treebank.
we focus on dis-tances between words in dependency trees and theirdepth, i.e., distance from the syntactic root..lexical hypernymy we introduce probing forlexical information.
we optimize probes to approx-imate the distance between pairs of words in thehypernymy tree and the depth for each word.
forthat purpose, we use the tree from wordnet (miller,1995).
we consider lexical distances between pairsof nouns and pairs of verbs in sentences and lexicaldepth for each noun and verb.
we provide goldpos information and look up synset by a lemma-tized form of a word to avoid ambiguity..position in a sentence probing for the sentenceindex of a word and positional difference betweenpairs of words..random structures we probe for randomlygenerated trees.
when we jointly optimize fordepth and distance, we keep the same randomlygenerated tree.
this control task allows us to deter-mine the extent to which our probes memorize thestructures and thus over-ﬁt to the training data..4.2 training.
we use batches of size 12 and an initial trainingrate of 0.02. we use learning rate decay and early-stopping mechanism: if validation loss does notachieve a new minimum after an epoch, the learn-ing rate is divided by 10. after three consecutive.
learning rate updates not resulting in a new mini-mum, the training is stopped..orthogonality regularization in our experi-ments, we took λo equal to 0.05.4 the regular-ization converged early during the gradient opti-mization.
hence we can assume that matrix v isorthogonal..sparsity regularization by default λs = 0.only in the experiments described in section 5.1,we use sparsity regularization by setting λs to apositive value (0.005, 0.05, or 0.1) when dsodrops below 1.5 during the training.
this mecha-nism prevents weakening orthogonality constraintin early epochs..the training are de-additional details ofscribed in the appendix.
the code is avail-able at github: https://github.com/tom556/orthogonaltransformerprobing..4.3 evaluation.
we assess spearman’s rank correlation betweengold and predicted values.
we report the averagecorrelations for the sentences with lengths from5 to 50 in the same way as hewitt and manning(2019)..our orthogonal structural probes are trainedjointly for multiple objectives (section 3.3).
weevaluate the effect of multitasking testing differentconﬁgurations: a) separate probing for each ob-jective; b) joint probing for distance and depth inthe same structure type; c) joint probing for dis-tance in all structures; d) joint probing for depthsin all structures; e) probing for all objectives to-gether.
we compare the results with two baselines:i) optimizing only scaling vector; ii) structuralprobes..4.4 dimensionality of scaling vector.
we hypothesize that the orthogonality regulariza-tion allows us to ﬁnd embedding subspace capa-ble of representing a particular linguistic structure.
in section 5.1, we examine the performance oflower-rank projections and ask whether further re-strictions of dimensionality affect the results.
insection 5.2 we analyze interactions between sub-spaces related to a particular objective in a jointprobing setting..4we experimentally checked that ten times smaller and tentimes larger values of λo do not affect orthogonality of matrixv and lead to the same results..431dep depthlayer.
dep dist.
layer.
lex depthlayer.
lex dist.
layer.
pos depthlayer.
pos dist.
layer.
rand depthlayer.
rand dist.
layer.
i.ii.
a.scalingvectoronly.459 ±.00117.
.513 ±.00118.
.572 ±.00113.
.560 ±.00113.
.232 ±.0135.
.441 ±0.0011.
.008 ±.0026.
.149 ±.00117.structuralprobe.
.856 ±.00118.
.843 ±.00117.
.892 ±.0028.
.816 ±.0086.
.989 ±.0011.
.980 ±.0014.
.206 ±.01017.
.242 ±.00519.
.896.673.orthogonalstructuralprobe.858 ±.00117.
.842 ±.00117.
.882 ±.0028.
.803 ±.0056.
.983 ±.0016.
.979 ±.0014.
.136 ±.00718.
.220 ±.00618.
.891.713.bc / dmultitask orthogonal probing.
e.distance+ depth.
all distancesor all depths.
.855 ±.00116.
.838 ±.00117.
.869 ±.0058.
.789 ±.0047.
.986 ±.0011.
.977 ±.0014.
.129 ±.01018.
.206 ±.00417.
.886.718.
.850 ±.00216.
.833 ±.00117.
.885 ±.0046.
.792 ±.0106.
.976 ±.0042.
.978 ±.0015.
.163 ±.02318.
.209 ±.00519.
.886.699.all tasks.
.852 ±.00116.
.832 ±.00216.
.873 ±.0059.
.792 ±.0056.
.982 ±0.0013.
.976 ±0.0014.
.107 ±.01919.
.208 ±.00715.
.883.726.avg.
dep, lex, posabove - avg.
rand.
.463.385.table 1: the highest spearman’s correlations (across layers) between predicted values and gold annotations on aheld out test set (for random structures computed on a train set).
each column represents another variant of training.
standard deviation was calculated for six runs.
each row’s optimal result is underlined (except baseline i); resultswithin 95% conﬁdence interval based on student’s t-test (student, 1908) are marked in bold..1.0.
0.8.
0.6.
0.4.
0.2.
0.0.noitalerrocnamareps.500.
400.
300.
200.
100.snoisnsemd.i.fo.rebmun.0.
1.
1.
3.
5.
7.
9.
15.
17.
19.
21.
23.
11.
13layer.
dep depthdep distancelex depth.
lex distancepos depthpos distance.
rand depthrand distnace.
3.
5.
7.
9.
15.
17.
19.
21.
23.
11.
13layer.
figure 2: spearman correlations and number of non-zero scaling vector’s dimensions across layers for jointtraining..5 results.
we compare spearman’s correlations between pre-dicted values and gold tree depths and distancesin table 1. the correlations obtained from or-thogonal structural probes are high for linguisticstructures: from 0.803 for lexical distance to 0.882for lexical depth.
predicted positional depths anddistances nearly match gold values..correlation on training data for random struc-tures is very weak, hinting that the probes do notmemorize structures during training but extractthem from the model’s representations.
the cor-relation for distances is higher than for depth.
wehypothesize it is because the probes learn somebasic tree properties.5.
the results obtained by orthogonal structuralprobes are close to those of structural probes.
fordependency distance, the difference is not statisti-cally signiﬁcant.
notably, correlations on trainingset for randomly generated trees decreased.
it sug-gests that orthogonal structural probes are lessvulnerable to memorization.
in multitask probing,.
5for instance, when the distances between nodes x and y,and y and z are both 1, then the distance between x and zneeds to be 2.
432subspace.
share of droppeddimensions.
sparsity regularizationλs = 0.05.λs = 0.005.λs = 0.1.dims corr.
25% 33% 50% dims corr dims corr dims corr.
dep depthdep dist..lex depthlex dist..pos depthpos dist..rand depthrand dist..137189.
19263.
2098.
259399.
.858.842.
.884.805.
.983.979.
.128.222.
.783.800.
.841.768.
.760.890.
.108.215.
.758.781.
.822.755.
.686.859.
.101.213.
.700.741.
.784.722.
.526.627.
.091.208.
2676.
1992.
1138.
6116.
.856.835.
.875.792.
.982.978.
.037.208.
221.
1160.
614.
120.
.832.784.
.852.756.
.981.975.
.011.163.
114.
1052.
311.
113.
.822.746.
.836.737.
.981.970.
.010.155.table 2: the highest spearman’s correlations (across layers) between predicted values and gold annotations ona held-out test set (for random structures computed on a train set).
in columns 2-3, results, when only selecteddimensions are used.
in columns 4-6, a portion of the selected dimensions is masked.
in columns 7-12, sparsityregularization with different λs is applied.
probing for one objective..correlation evenly decreases across all tasks.
whileselectivity (the difference between average correla-tion for dependency, lexical, and positional objec-tives and random objectives) increases from 0.673to 0.726. optimizing only a scaling vector givesdistinctly lower correlations.
these results empha-size the necessity of changing the coordinate sys-tem to amplify the dimensions encoding linguisticinformation..in fig.
2 (upper), we observe that the perfor-mance varies throughout the layers, conﬁrming pre-vious observations by hewitt and manning (2019)and tenney et al.
(2019a).
the mid-upper layerstend to be more syntactic, and the mid-lower onesare more lexical.
predicting word position is moreaccurate in the lower layers, dropping signiﬁcantlytoward the last layers.
it is due to the fact thatin bert, positional embeddings are added beforethe ﬁrst layer.
random structure probes maintainsteady results across all the layers..5.1 dimensionality.
we observe that orthogonality constraint is quiteeffective in restricting the probe’s rank.
in mostof our experiments, the majority of scaling vec-tor parameters converged to zero.
it allows select-ing subspaces encoding particular linguistic fea-tures.
we want to answer whether such subspacehas enough capacity for each probing task.
forthat purpose, we zero out the dimensions with cor-responding scaling vector weights closer to zero.
than (cid:15) = 10−4.6 their elimination does not affectthe results; correlations in table 2 and table 1 col-umn a are practically equal.
the dimensionalityreduction is the strongest for lexical and positionaldepth probes, where subspaces with the rank of 19and 20 respectively encode the structures as well asthe whole embedding space with 1024 dimensions(fig.
2, lower).
the number of selected dimen-sions is the highest in probing for random struc-tures.
this is because a large capacity is requiredfor memorization..another question we pose is whether it would beadequate to shrink the subspace even further.
foreach objective, we choose and drop a random por-tion of parameters to examine how it would affectthe predictions.
we conduct a procedure similar tocross-validation, i.e., we repeatedly drop disjointand exhaustive sets of dimensions and average re-sults for each set at the end.7 table 2 shows thatdimension dropping had the largest impact on po-sitional probes: −0.458 for depth; the decreaseis low for lexical distance – only −0.083.
it sug-gests that the information necessary for the latterobjective is more dispersed than for the former one..sparsity regularization we use sparsity regu-larization of scaling vector to examine whetherdimensionality can be reduced more intelligently.
the strength of regularization is regulated by value.
6in the appendix, we show that dimension selection is not.
sensitive to the selection of low 10−30 < (cid:15) < 10−3..7when we drop 25% of dimensions, we randomly choose.
four sets.
each dimension is exactly in one set..433dep.
lex.
pos.
rand.
5.2 separation of information.
htped.62..
tsid.48.
126.htped.0.
0..
tsid.0.
0.
20.
18.
131.htped.10.
9.
0.
0.
14..
tsid.19.
23.
4.
7.
10.
70.htped.23.
25.
1.
5.
13.
33..
tsid.21.
30.
5.
19.
10.
50.
131.
95.
262.p depthed.dist..x depth.
el.dist..s depthop.dist..d depthnar.dist..table 3: the number of shared dimensions selected byscaling vector after the joint training of probe on topof the 16th layer..of λs ∈ {0.005, 0.05, 0.1}.
we observe that forsome objectives (dependency depth, positionaldepth, and positional distance), the relevant in-formation is captured in a small number of di-mensions.
remarkably, only one dimension ofembedding space can achieve 0.822 correlationwith dependency depths.
we conjecture that ifit is possible to achieve a high correlation withsparse subspaces, information on the phenomenonis focal in the model (concentrated in few dimen-sions).
for the objectives with focal information,results decrease sharply when random dimensionsare dropped because the probability of droppingimportant coordinates is high.
on the other endof the spectrum, we can identify the objective forwhich information is spread – lexical distance.
thedropping of random dimensions only moderatelydecreases correlation, as there are no especiallyessential coordinates.
probing with sparsity reg-ularization produces subspaces of relatively largesize..sparsity regularization also positively affectscontrol objectives, decreasing correlations with dis-tances and depths of randomly generated structures,indicating that regularized probes are less prone tomemorization..notably, torroba hennigen et al.
(2020) pro-posed a method for selecting embeddings’ dimen-sions relevant to particular linguistic phenomena.
in our setting, thanks to the orthogonal transfor-mation, we are not constrained to analyzing thedimensions of just one coordinate system..another outcome of joint training was the ability toexamine relationships between subspaces for eachof the objectives.
figure 3 shows histograms ofthe dimensions selected in lexical and dependencyprobes.
each bin of the histogram corresponds to10 coordinates.
the height of a bar (in one color)represents how many were selected for a speciﬁctask.
the dimensions on the x-axis are ordered bythe weighted absolute values of scaling vectors.8we found that in layers 6 and 16 (they achievethe highest correlation in lexical and dependency,respectively), the histograms are disjoint, indicat-ing that the layers’ representations of dependencysyntax and lexical hypernymy are orthogonal toeach other in the embedding space.
the orthogo-nality is less visible in the ﬁrst layer and disappearsalmost entirely in the top one.
in most layers, depthsubspace is included in distance subspace for thesame structural type.
this behavior was expectedas distance probing is more complex and thereforerequires more capacity..in fig.
4 we present histograms for additionaltasks at the model’s 16th layer.
the positional sub-space has a sizable intersection with the syntacticone, yet only a few common dimensions with thelexical subspace.
the connection can be attributedto the fact that dependency edges can often be in-ferred from words’ relative positions.
probing forrandom structures is interlinked with other objec-tives.
the sizes of shared subspaces for each paircan be found in table 3. histograms and tables forother sets of tasks are presented in the appendix..6 discussion.
the introduction of an orthogonal constraint is acore element of our analysis.
the constraint as-sures that no dimension is enhanced or diminishedin the transformation and allows interpreting themagnitude of values in the scaling vector as therelevance of each dimension for the objectives..in an orthogonal structural probe, the sufﬁ-cient rank of a transformation is learned duringthe optimization.
the rank regularization is a pre-requisite to disentangle the information encodedby the probe (section 5.2).
the natural question.
8we weight the values before sorting to keep togethernon-zero dimensions of each scaling vector, i.e., dependencydepth values are multiplied by 1000, dependency distance100, lexical depth by 10. the weighting is performed only forvisualization; the separation of linguistic information can beobserved independently in table 3..4340.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.lex depthlex distdep depthdep dist.
layer: 1.layer: 6.layer: 16.layer: 24.
25.
20.
15.
10.
5.
0.
20.
15.
10.
5.
0.
15.
10.
5.
0.
20.
15.
10.
5.
0.dep depthdep distpos depthpos dist.
dep depthdep distrand depthrand dist.
lex depthlex distpos depthpos dist.
lex depthlex distrand depthrand dist.
pos depthpos distrand depthrand dist.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.figure 3: histograms of dimensions selected by depen-dency and lexical scaling vector after joint training .
best in color..0.
200.
400.
600.
800.
1000.figure 4: histograms of dimensions selected by scal-ing vector after the joint training of probe on top of the16th layer.
best in color..17.5.
15.0.
12.5.
10.0.
7.5.
5.0.
2.5.
0.0.
20.
15.
10.
5.
0.
30.
25.
20.
15.
10.
5.
0.
20.
15.
10.
5.
0.
30.
25.
20.
15.
10.
5.
0.
435is whether such analysis can be performed by re-ducing the rank of structural probe with anotherregularizer and decomposing linear transformationafter the optimization.
we argue that it is not possi-ble both in joint and separate probing:.
• in joint probing for multiple tasks: one scal-ing vector is shared for all the tasks.
it isnot possible to attribute the dimensions to aspeciﬁc task..• in separate probing for each task: the decom-position leads to different orthogonal matrices.
hence, the dimensions of distinct scaling vec-tors do not correspond to each other..6.1 limitations.
we focus on syntax annotated in universal depen-dencies and lexical hypernymy encoded in word-net.
we do not claim that there is no correla-tion between syntactic and lexical information inbert, just that the topologies of those two struc-tures are encoded separately.
it is entirely possiblethat we could ﬁnd dimensions overlap when prob-ing for syntax and lexicon in differently annotateddatasets..conversely to structural probes, our reformu-lation of the loss (in eq.
(12) and eq.
(11)) is notconvex.
we thank one of the anonymous acl re-viewers for pointing it out.
nevertheless, we showthat despite non-convexity, our orthogonal struc-tural probes achieve similar results to structuralprobes and are more selective..7 conclusions.
we have expanded structural probing to new typesof auxiliary tasks and introduced a new setting,orthogonal structural probe, in which probes canbe optimized jointly.
we found out that:.
1. results of orthogonal structural probes areon par with standard structural probeson linguistic tasks.
orthogonal structuralprobes are less vulnerable to memorization..2. in addition to syntactic dependencies or-thogonal structural probes can be efﬁcientlytrained to approximate dependency and depthin wordnet hypernymy trees and positionalorder..3. orthogonal structural probes can be trainedjointly for multiple objectives.
in most cases,.
the performance moderately drops, and selec-tivity increases.
the number of parametersdecreases in comparison to training many sep-arate probes..4. usually, information necessary for each objec-tive is stored in a subspace of relatively lowrank (19 - 263).
we can further reduce dimen-sionality by applying sparsity regularization.
for a few objectives (e.g., positional depth,dependency depth), the information is hugelyfocal, and the performance can fall markedlywhen just 25% randomly selected dimensionsare dropped..5. we have found that in most of bert’s lay-ers, the subspace encoding linguistic hyper-nymy is separated from the subspace encod-ing dependency syntax and subspace encodingword’s position..7.1 further work.
our method can be adjusted for multitask and mul-tilingual settings.
following the observation thatthe orthogonal transformation can map distribu-tions of embeddings in typologically close lan-guages (mikolov et al., 2013; vuli´c et al., 2020).
we think that joint training for many languagesmay be possible by keeping the same scaling vec-tor and adding a separate orthogonal transforma-tion per language, fulﬁlling the role of orthogonalmappings.
another leg of research would be an-alyzing probes for other linguistic structures, forinstance, derivation trees..acknowledgments.
we thank ondˇrej duˇsek, greg durrett, and anony-mous reviewers of acl for valuable comments onprevious versions of this paper.
this work has beensupported by grant 18-02196s of the czech sciencefoundation and by grant 338521 of the charlesuniversity grant agency.
we have been using lan-guage resources and tools developed, stored anddistributed by the lindat/clariah-cz projectof the ministry of education, youth and sports ofthe czech republic (project lm2018101)..references.
mart´ın abadi, ashish agarwal, paul barham, eugenebrevdo, zhifeng chen, craig citro, greg s. cor-rado, andy davis, jeffrey dean, matthieu devin,sanjay ghemawat, ian goodfellow, andrew harp,.
436geoffrey irving, michael isard, yangqing jia, rafaljozefowicz, lukasz kaiser, manjunath kudlur, joshlevenberg, dan man´e, rajat monga, sherry moore,derek murray, chris olah, mike schuster, jonathonshlens, benoit steiner, ilya sutskever, kunal talwar,paul tucker, vincent vanhoucke, vijay vasudevan,fernanda vi´egas, oriol vinyals, pete warden, mar-tin wattenberg, martin wicke, yuan yu, and xiao-qiang zheng.
2015. tensorflow: large-scale ma-chine learning on heterogeneous systems.
softwareavailable from tensorﬂow.org..martin arjovsky, amar shah, and yoshua bengio.
2016. unitary evolution recurrent neural networks.
in international conference on machine learning,pages 1120–1128..nitin bansal, xiaohan chen, and zhangyang wang.
2018. can we gain more from orthogonalityregularizations in training deep networks?
ins. bengio, h. wallach, h. larochelle, k. grauman,n. cesa-bianchi, and r. garnett, editors, advancesin neural information processing systems 31, pages4261–4271.
curran associates, inc..yonatan belinkov, nadir durrani, fahim dalvi, has-san sajjad, and james glass.
2017. what do neu-ral machine translation models learn about morphol-ogy?
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 861–872, vancouver,canada.
association for computational linguistics..yonatan belinkov and james glass.
2019. analysismethods in neural language processing: a survey.
transactions of the association for computationallinguistics (tacl), 7:49–72..terra blevins, omer levy, and luke zettlemoyer.
2018.deep rnns encode soft hierarchical syntax.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 14–19, melbourne, australia.
asso-ciation for computational linguistics..krzysztof choromanski, valerii likhosherstov, daviddohan, xingyou song, andreea gane, tamas sar-los, peter hawkins, jared davis, afroz mohiuddin,lukasz kaiser, david belanger, lucy colwell, andadrian weller.
2020. rethinking attention with per-formers..rumen dangovski, li jing, preslav nakov, mi´co tat-alovi´c, and marin soljaˇci´c.
2019. rotational unitof memory: a novel representation unit for rnnswith scalable applications.
transaction of the asso-ciation of computational linguistics, 7:121–138..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt..john hewitt and percy liang.
2019. designing andinterpreting probes with control tasks.
in proceed-ings of the 2019 conference on empirical methods.
in natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2733–2743, hongkong, china.
association for computational lin-guistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word represen-tations.
in naacl-hlt..li jing, caglar gulcehre, john peurifoy, yichen shen,max tegmark, marin soljaˇci´c, and y. bengio.
2017a.
gated orthogonal recurrent units: on learn-ing to forget.
neural computation, 31..li jing, yichen shen, tena dubcek, john peurifoy,scott skirlo, yann lecun, max tegmark, and marinsoljaˇci´c.
2017b.
tunable efﬁcient unitary neural net-works (eunn) and their application to rnns.
inproceedings of the 34th international conferenceon machine learning, volume 70 of proceedingsof machine learning research, pages 1733–1741,international convention centre, sydney, australia.
pmlr..diederik kingma and jimmy ba.
2014. adam: ainternational.
method for stochastic optimization.
conference on learning representations..artur kulmizev, vinit ravishankar, mostafa abdou,and joakim nivre.
2020. do neural language mod-els show preferences for syntactic formalisms?
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4077–4091, online.
association for computational lin-guistics..tal linzen, emmanuel dupoux, and yoav goldberg.
2016. assessing the ability of lstms to learnsyntax-sensitive dependencies.
transactions of theassociation for computational linguistics, 4:521–535..nelson f. liu, matt gardner, yonatan belinkov,matthew e. peters, and noah a. smith.
2019. lin-guistic knowledge and transferability of contextualrepresentations.
in naacl-hlt..tomas mikolov, quoc v. le, and ilya sutskever.
2013.exploiting similarities among languages for ma-chine translation..george a. miller.
1995. wordnet: a lexical database.
for english.
commun.
acm, 38(11):39–41..joakim nivre, marie-catherine de marneffe, filip gin-ter, jan hajiˇc, christopher d. manning, sampopyysalo, sebastian schuster, francis tyers, anddaniel zeman.
2020. universal dependencies v2:an evergrowing multilingualtreebank collection.
in proceedings of the 12th language resourcesand evaluation conference, pages 4034–4043, mar-seille, france.
european language resources asso-ciation..437ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998–6008..eugene vorontsov, chiheb trabelsi, samuel kadoury,and chris pal.
2017. on orthogonality and learn-ing recurrent networks with long term dependencies.
in proceedings of the 34th international conferenceon machine learning, volume 70 of proceedingsof machine learning research, pages 3570–3578,international convention centre, sydney, australia.
pmlr..ivan vuli´c, sebastian ruder, and anders søgaard.
2020. are all good word vector spaces isomorphic?
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3178–3192, online.
association for computa-tional linguistics..scott wisdom, thomas powers,.
john hershey,full-jonathan le roux, and les atlas.
2016.in ad-capacity unitary recurrent neural networks.
vances in neural information processing systems,volume 29, pages 4880–4888.
curran associates,inc..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..kelly zhang and samuel bowman.
2018. languagemodeling teaches you more than translation does:lessons learned through auxiliary syntactic taskanalysis.
in proceedings of the 2018 emnlp work-shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 359–361, brussels, bel-gium.
association for computational linguistics..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), neworleans, louisiana.
association for computationallinguistics..tiago pimentel, naomi saphra, adina williams, andryan cotterell.
2020. pareto probing: trading offaccuracy for complexity.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 3138–3153, on-line.
association for computational linguistics..abhilasha ravichander, eduard hovy, kaheer sule-man, adam trischler, and jackie chi kit cheung.
2020. on the systematicity of probing contextual-ized word representations: the case of hypernymyin proceedings of the ninth joint con-in bert.
ference on lexical and computational semantics,pages 88–102, barcelona, spain (online).
associa-tion for computational linguistics..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we know abouthow bert works.
transactions of the association forcomputational linguistics, 8:842–866..natalia silveira, timothy dozat, marie-catherinede marneffe, samuel bowman, miriam connor,john bauer, and christopher d. manning.
2014. agold standard dependency corpus for english.
inproceedings of the ninth international conferenceon language resources and evaluation (lrec-2014)..student.
1908..the probable error of a mean..biometrika, pages 1–25..ian tenney, dipanjan das, and ellie pavlick.
2019a.
bert rediscovers the classical nlp pipeline.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r thomas mccoy, najoung kim,benjamin van durme, sam bowman, dipanjan das,and ellie pavlick.
2019b.
what do you learn fromcontext?
probing for sentence structure in contextu-in international con-alized word representations.
ference on learning representations..lucas torroba hennigen, adina williams, and ryancotterell.
2020.intrinsic probing through dimen-sion selection.
in proceedings of the 2020 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 197–216, online.
as-sociation for computational linguistics..438a technical details.
the orthogonal structural probe is trained to min-imize l1 loss between predicted and gold distancesand depths.
the loss is normalized by the numberof predictions in a sentence and averaged across abatch of size 12. optimization is conducted withadam (kingma and ba, 2014) with initial learn-ing rate 0.02 and meta parameters: β1 = 0.9,β2 = 0.999, and (cid:15) = 10−8.
we use learning ratedecay and early-stopping mechanism: if valida-tion loss does not achieve a new minimum after anepoch, learning rate is divided by 10. after threeconsecutive learning rate updates not resulting in anew minimum, the training is stopped..to alleviate sharp jumps in training loss that weobserved mainly in training of depth probes, weclip each gradient’s norm at c = 1.5..we implemented the network in tensorflow2 (abadi et al., 2015).
the code is avail-able at github: https://github.com/tom556/orthogonaltransformerprobing..a.1 orthogonal regularization.
in order to coerce orthogonality of matrix v weadd dso to the loss.
bansal et al.
(2018) showedthat for convolutional neural network applied toimage processing, a simpler regularization – so ismore powerful..λoso(v ) = λo||v t v − i||2f.(13).
in our experiments, dso led to faster conver-gence.
fig.
5 shows values of orthogonality penaltyduring the training.
taking into account the prop-erties of the frobenius norm, we observe that vmatrix is close to orthogonal already after initialepochs..a.2 sparsity regularization.
fig.
6 presents values of sparsity penalty during thetraining.
the regularization is applied only afterthe orthogonality penalty drops below 1.5..a.3 number of parameters.
the number of orthogonal structural probe’s pa-rameters is given by equation:.
figure 5: values of orthogonality penalty during jointtraining of orthogonal structural probe on top of lay-ers: 3 (green), 7 (yellow), 16 (gray), 24 (blue).
opti-mization steps on the x-axis..figure 6: values of sparsity penalty during separatetraining of orthogonal structural probes with λ =0.05. objectives from the highest to the lowest value:lexical distance (yellow), positional distance (green),dependency distance (gray), positional depth (violet),lexical depth (magenta), dependency depth (blue), ran-dom depth (orange).
optimization steps on the x-axis..bert large for all eight objectives have 10242 +1024 · 8 = 1, 056, 768 parameters.
it is morethan in structural probes of hewitt and manning(2019).
nevertheless, our probes have less degreesof freedom, because we use orthogonal transfor-mation instead of linear transformation..demb · (demb − 1)2.dofortho =.
+ demb · nobj(15)in the case of joint training for all objectives, thenumber of degrees of freedom equals to 523, 766..n p aramsortho = d2.
emb + demb · nobj,.
(14).
a.4 computation time.
where demb is dimensionality of the embeddingsand nobj is a number of jointly probed objec-tives.
therefore, our biggest probes on top of.
we have trained orthogonal structural probes ongpu a core geforce gtx 1080 ti.
approximaterun times of speciﬁc conﬁgurations:.
439• separate probing for depth ∼ 3 minutes.
training conﬁg..layer uuas uas.
• separate probing for distance ∼ 5 minutes.
• joint probing for distance and depth in the.
same structure type ∼ 7 minutes.
• joint probing for depths in all structures ∼ 13.
• joint probing for distance in all structures ∼.
• probing for all objectives together ∼ 35 min-.
minutes.
18 minutes.
utes.
b derivation of orthogonal structural.
probe equation.
eq.
(6) with intermediate steps:.
structural probeorthogonal probe.
82.2982.47.
––.
multitask orthogonal probing.
distance + depthall distancesall tasks.
80.8680.7279.03.
77.51–75.66.
1515.
161516.table 4: (undirected) unlabeled attachment score oftrees extracted from dependency probes..e scaling vector properties.
in this appendix, we elaborate on the properties ofscaling vectors parameters in the multi-task prob-ing..db(hi, hj)2= (u dv t (hi − hj))t (u dv t (hi − hj))= (hi − hj)t v dt u t u dv t (hi − hj)= (hi − hj)t v dt dv t (hi − hj)= (dv t (hi − hj))t (dv t (hi − hj)).
(16).
e.1 parameters distribution.
the distribution of values in scaling vector (fig.
7)shows that the majority of parameters converge tozero.
they are within 10−40 to 10−30 margin aftertraining.
therefore, the signiﬁcant dimensions areclearly identiﬁable..ycneuqerf.102.
101.
100.c dataset description.
universal dependencies english web treebank(silveira et al., 2014) is available at https://github.com/universaldependencies/ud_english-ewt.
it consist of: 12, 543 test, 2, 002dev, and 2, 077 test sentences..d application in dependency parsing.
we have computed the uas of dependency treespredicted based on dependency probes.
we employthe algorithm for extraction of directed dependencytrees proposed by kulmizev et al.
(2020).
our in-novation to the method is that we optimize distanceand depth probes jointly during one optimization.
in line with the previous studies, we show thatorthogonal structural probes can be employed forparsing.
table 4 presents unlabeled attachmentscores achieved by different multi-task conﬁgura-tions.
joint probing for dependency distance anddepth allows us to extract a directed dependencytree in just one optimization.
best to our knowl-edge, it has not been tried before.
analogicallyto spearman’s correlation, uas drops when moreobjectives are used in optimization.
however, evenjoint probing for all eight objectives is capable ofproducing trees with 75.66% uas..10−40.
10−30.
10−20parameter absolute value.
10−10.
100.figure 7: logarithmic histogram of scaling vector pa-rameters for dependency distance.
joint probing of16th layer’s representations..e.2 separation of information (continued).
on the following pages, we present dimension over-lap histograms and tables, as in section 5.2, for theremaining pairs of objectives..440layer: 1.layer: 1.dep depthdep distpos depthpos dist.
lex depthlex distpos depthpos dist.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.layer: 6.layer: 6.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.layer: 16.layer: 16.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.layer: 24.layer: 24.
20.
15.
10.
5.
0.
15.0.
12.5.
10.0.
7.5.
5.0.
2.5.
0.0.
15.
10.
5.
0.
20.
15.
10.
5.
0.
30.
25.
20.
15.
10.
5.
0.
20.
15.
10.
5.
0.
20.
15.
10.
5.
0.
30.
25.
20.
15.
10.
5.
0.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.dep.
lex.
pos.
rand.
dep.
lex.
pos.
rand.
htped.65..
tsid.54.
109.htped.1.
6..
tsid.18.
43.
46.
45.
551.htped.11.
11.
2.
2..
tsid.24.
39.
13.
42.
20.
11.
111.htped.48.
45.
7.
46.
20.
47..
tsid.44.
64.
8.
103.
14.
71.
265.
152.
112.p depthed.dist..x depth.
el.dist..s depthop.dist..d depthnar.dist..htped.50..
tsid.43.
81.htped.0.
1..
tsid.1.
2.
30.
28.
346.htped.11.
11.
0.
0.
14..
tsid.26.
38.
4.
19.
11.
99.htped.30.
35.
1.
14.
13.
41..
tsid.26.
39.
6.
45.
11.
71.
113.
70.
267.p depthed.dist..x depth.
el.dist..s depthop.dist..d depthnar.dist..table 5: number of shared dimensions selected byscaling vector after the joint training of probe on topof the 1st layer..table 6: number of shared dimensions selected byscaling vector after the joint training of probe on topof the 6th layer..441layer: 1.dep depthdep distrand depthrand dist.
0.
200.
400.
600.
800.
1000.layer: 6.layer: 1.lex depthlex distrand depthrand dist.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.layer: 16.layer: 6.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.layer: 24.layer: 16.
30.
25.
20.
15.
10.
5.
0.
30.
25.
20.
15.
10.
5.
0.
30.
25.
20.
15.
10.
5.
0.
30.
20.
10.
0.
0.
200.
400.
600.
800.
1000.
0.
200.
400.
600.
800.
1000.layer: 24.
15.
10.
5.
0.
20.
15.
10.
5.
0.
20.
15.
10.
5.
0.
20.
15.
10.
5.
0.
0.
200.
400.
600.
800.
1000.dep.
lex.
pos.
rand.
htped.189.
144..
tsid.463.htped.17.
16..
tsid.39.
82.htped..
tsid.146.
123.
141.
186.
275.htped.70.
81.
9.
30..
tsid.66.
10.
48.
124.
70.
190.
33.
22.
173.
18.
64.
107.
136.
16.
98.
97.
177.
410.
287.
198.p depthed.dist..x depth.
el.dist..s depthop.dist..d depthnar.dist..table 7: number of shared dimensions selected byscaling vector after the joint training of probe on topof the 24th layer..442