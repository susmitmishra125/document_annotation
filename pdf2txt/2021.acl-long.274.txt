document-level event extraction via heterogeneousgraph-based interaction model with a tracker.
runxin xu1, tianyu liu1, lei li3 and baobao chang1,2∗1key laboratory of computational linguistics, peking university, moe, china2peng cheng laboratory, shenzhen, china3bytedance ai labrunxinxu@gmail.com,lileilab@bytedance.com{tianyu0421,chbb}@pku.edu.cn.
abstract.
document-level event extraction aims to rec-ognize event information from a whole pieceof article.
existing methods are not effectivedue to two challenges of this task: a) the tar-get event arguments are scattered across sen-tences; b) the correlation among events in adocument is non-trivial to model.
in this pa-per, we propose heterogeneous graph-basedinteraction model with a tracker (git) tosolve the aforementioned two challenges.
forthe ﬁrst challenge, git constructs a hetero-geneous graph interaction network to captureglobal interactions among different sentencesand entity mentions.
for the second, git in-troduces a tracker module to track the ex-tracted events and hence capture the interde-pendency among the events.
experiments ona large-scale dataset (zheng et al., 2019) showgit outperforms the existing best methods by2.8 f1.
further analysis reveals git is ef-fective in extracting multiple correlated eventsand event arguments that scatter across thedocument.
our code is available at https://github.com/runxinxu/git..figure 1: an example document from a chinesedataset proposed by zheng et al.
(2019) in the ﬁnancialdomain, and we translate it into english for illustration.
entity mentions are colored.
due to space limitation,we only show four associated sentences and three argu-ment roles of each event type.
the complete originaldocument can be found in appendix c. eu: equity un-derweight, eo: equity overweight..1.introduction.
event extraction (ee) is one of the key and chal-lenging tasks in information extraction (ie), whichaims to detect events and extract their argumentsfrom the text.
most previous methods (chen et al.,2015; nguyen et al., 2016; liu et al., 2018; yanget al., 2019; du and cardie, 2020b) focus onsentence-level ee, extracting events from a sin-gle sentence.
the sentence-level model, however,fails to extract events whose arguments spread inmultiple sentences, which is much more commonin real-world scenarios.
hence, extracting events atthe document-level is critical.
it has attracted muchattention recently (yang et al., 2018; zheng et al.,2019; du and cardie, 2020a; du et al., 2020)..*corresponding author..though promising, document-level ee still facestwo critical challenges.
firstly, the argumentsof an event record may scatter across sentences,which requires a comprehensive understanding ofthe cross-sentence context.
figure 1 illustrates anexample that one equity underweight (eu) and oneequity overweight (eo) event records are extractedfrom a ﬁnancial document.
it is less challengingto extract the eu event because all the related ar-guments appear in the same sentence (sentence2).
however, for the arguments of eo record, nov6, 2014 appears in sentence 1 and 2 while xiaot-ing wu in sentence 3 and 4. it would be quitechallenging to identify such events without con-sidering global interactions among sentences andentity mentions.
secondly, a document may ex-press several correlated events simultaneously, andrecognizing the interdependency among them is.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3533–3546august1–6,2021.©2021associationforcomputationallinguistics3533[1] on nov 6, 2014, the company received a letterof share reduction frommingting wu, theshareholder of the company.
[2] mingting wu decreased his holding of 7.2 million shares of the company on the shenzhen stock exchange on nov 6, 2014.
[3]the 7.2 millionshares of the company mingting wureduced this time were transferred to xiaoting wu.
[4] xiaoting wu is the daughter of mingting wu, and they were identified as persons acting in concert according to relevant regulations.eventtypeequityholdertradedsharesstartdate7.2 millionnov 6, 2014xiaotingwueoeumingtingwu7.2 millionnov 6, 2014………fundamental to successful extraction.
as shown infigure 1, the two events are interdependent becausethey correspond to exactly the same transactionand therefore share the same startdate.
effectivemodeling on such interdependency among the cor-related events remains a key challenge in this task.
yang et al.
(2018) extracts events from a cen-tral sentence and query the neighboring sen-tences for missing arguments, which ignores thecross-sentence correspondence between augments.
though zheng et al.
(2019) takes a ﬁrst step to fusethe sentences and entities information via trans-former, they neglect the interdependency amongevents.
focusing on single event extraction, duand cardie (2020a) and du et al.
(2020) concate-nate multiple sentences and only consider a singleevent, which lacks the ability to model multipleevents scattered in a long document..to tackle the aforementioned two challenges, inthis paper, we propose a heterogeneous graph-based interaction model with a tracker (git)for document-level ee.
to deal with scatteredarguments across sentences, we focus on theglobal interactions among sentences and entitymentions.
speciﬁcally, we construct a hetero-geneous graph interaction network with mentionnodes and sentence nodes, and model the inter-actions among them by four types of edges (i.e.,sentence-sentence edge, sentence-mention edge,intra-mention-mention edge, and inter-mention-mention edge) in the graph neural network.
in thisway, git jointly models the entities and sentencesin the document from a global perspective..to facilitate the multi-event extraction, we targeton the global interdependency among correlatedevents.
concretely we propose a tracker module tocontinually tracks the extracted event records witha global memory.
in this way, the model is encour-aged to incorporate the interdependency with othercorrelated event records while predicting..we summarize our contributions as follows:.
• we construct a heterogeneous graph interac-tion network for document-level ee.
with dif-ferent heterogeneous edges, the model couldcapture the global context for the scatteredevent arguments across different sentences..• we introduce a novel tracker module to trackthe extracted event records.
the tracker easesthe difﬁculty of extracting correlated events,as interdependency among events would betaken into consideration..• experiments show git outperforms the pre-vious state-of-the-art model by 2.8 f1 on thelarge-scale public dataset (zheng et al., 2019)with 32, 040 documents, especially on cross-sentence events and multiple events scenarios(with 3.7 and 4.9 absolute increase on f1)..2 preliminaries.
we ﬁrst clarify some important notions.
a) entitymention: a text span within document that refersto an entity object; b) event argument: an entityplaying a speciﬁc event role.
event roles are pre-deﬁned for each event type; c) event record: anentry of a speciﬁc event type containing argumentsfor different roles in the event.
for simplicity, weuse record for short in the following sections..following zheng et al.
(2019), given a docu-ment composed of sentences d = {si}|d|i=1 anda sentence containing a sequence of words si ={wj}|si|j=1, the task aims to handle three sub-tasks: 1) entity extraction: extracting entities e ={ei}|e|i=1 from the document to serve as argumentcandidates.
an entity may have multiple mentionsacross the document.
2) event types detection: de-tecting speciﬁc event types that are expressed bythe document.
3) event records extraction: ﬁnd-ing appropriate arguments for the expressed eventsfrom entities, which is the most challenging andalso the focus of our paper.
the task does not re-quire to identify event triggers (zeng et al., 2018;liu et al., 2019b), which reduces manual effort ofannotation and the application scenarios becomesmore extensive..3 methodology.
as shows in figure 2, git ﬁrst extracts candi-date entities through sentence-level neural extrac-tor (sec 3.1).
then we construct a heterogeneousgraph to model the interactions among sentencesand entity mentions (sec 3.2), and detect eventtypes expressed by the document (sec 3.3).
fi-nally we introduce a tracker module to continu-ously track all the records with global memory, inwhich we utilize the global interdependency amongrecords for multi-event extraction (sec 3.4)..3.1 entity extractiongiven a sentence s = {wj}|s|s into a sequence of vectors {gj}|si|.
j=1 ∈ d, we encodej=1 using trans-.
3534figure 2: overview of our git.
firstly, sentences of the document are fed into the encoder to obtain contextualizedrepresentation, followed by a crf layer to extract entities.
then git constructs a heterogeneous graph interactionnetwork with mention nodes and sentence nodes, which captures the global interactions among them based ongcns.
after obtaining document-aware representations of entities and sentences, git detects event types andextracts records through the decoding module with a tracker.
the tracker tracks extracted records with globalmemory, based on which the decoding module incorporates global interdependency among correlated event records.
different entities are marked by different colors.
m: mingting wu.
x: xiaoting wu.
n: nov 6, 2014. s: 7.2 million..former (vaswani et al., 2017):.
{g1, .
.
.
, g|s|} = transformer({w1, .
.
.
, w|s|}).
the word representation of wj is a sum of the cor-responding token and position embeddings..we extract entities at the sentence level and for-mulate it as a sequence tagging task with bio (be-gin, inside, other) schema.
we leverage a condi-tional random ﬁeld (crf) layer to identify entities.
for training, we minimize the following loss:.
lner = −.
log p (ys|s).
(1).
(cid:88).
s∈d.
where ys is the golden label sequence of s. forinference, we use viterbi algorithm to decode thelabel sequence with the maximum probability..3.2 heterogeneous graph interaction.
network.
an event may span multiple sentences in the docu-ment, which means its corresponding entity men-tions may also scatter across different sentences.
identifying and modeling these entity mentions inthe cross-sentence context is fundamental in doc-ument ee.
thus we build a heterogeneous graphg which contains entity mention nodes and sen-tence nodes in the document d. in the graph g,interactions among multiple entity mentions and.
sentences can be explicitly modeled.
for each en-tity mention node e, we initialize node embed-ding h(0)e = mean({gj}j∈e) by averaging therepresentation of the contained words.
for eachsentence node s, we initialize node embeddingh(0)s = max({gj}j∈s) + sentpos(s) by max-pooling all the representation of words within thesentence plus sentence position embedding..to capture the interactions among sentences and.
mentions, we introduce four types of edges..sentence-sentence edge (s-s) sentence nodesare fully connected to each other with s-s edges.
in this way, we can easily capture the global prop-erties in the document with sentence-level interac-tions, e.g., the long range dependency between anytwo separate sentences in the document would bemodeled efﬁciently with s-s edges..sentence-mention edge (s-m) we model thelocal context of an entity mention in a speciﬁcsentence with s-m edge, speciﬁcally the edge con-necting the mention node and the sentence node itbelongs to..intra-mention-mention edge (m-mintra) weconnect distinct entity mentions in the same sen-tences with m-mintra edges.
the co-occurrence ofmentions in a sentence indicates those mentionsare likely to be involved in the same event.
we.
3535global memoryencodercrf layerclassifierentitypledgeentityfreezeentityoverweight……decodingmodule[1]mingtingwu decreased... 7.2 million shares ... on  nov 6, 2014.
[2]the 7.2 million shares ... mingtingwu … to xiaotingwu.
[3] xiaotingwu is the daughter of mingtingwu …123nmmmxxssmxns123record 1record 2representationsdocument sentencesheterogeneous interaction graph networktypes detection and records extractionmsxn123mention nodesentence nodesentence-sentence edgesentence-mention edgeintra-mention-mention edgeinter-mention-mention edgetracker𝓛𝐧𝐞𝐫𝓛𝐫𝐞𝐜𝐨𝐫𝐝𝓛𝐝𝐞𝐭𝐞𝐜𝐭explicitly model this indication by m-mintra edges..inter-mention-mention edge (m-minter) theentity mentions that corresponds to the same entityare fully connected with each other by m-minteredges.
as in document ee, an entity usually cor-responds to multiple mentions across sentences,we thus use m-minter edge to track all the appear-ances of a speciﬁc entity, which facilitates the longdistance event extraction from a global perspective.
in section.
4.5, experiments show that all ofthese four kinds of edges play an important rolein event detection, and the performance would de-crease without any of them..after heterogeneous graph construction *, we ap-ply multi-layer graph convolution network (kipfand welling, 2017) to model the global interactionsinspired by zeng et al.
(2020).
given node u atthe l-th layer, the graph convolutional operation isdeﬁned as follows:.
.
h(l+1)u.
= relu.
.
(cid:88).
(cid:88).
k∈k.
v∈nk(u) (cid:83){u}.
1cu,k.
w (l).
k h(l).
v..
where k represents differenttypes of edges,w (l)k ∈ rdm×dm is trainable parameters.
nk(u)denotes the neighbors for node u connected in k-thtype edge and cu,k is a normalization constant.
wethen derive the ﬁnal hidden state hu for node u,.
hu = wa[h(0).
u ; h(1).
u ; .
.
.
; h(l)u ].
where h(0)and l is the number of gcn layers..u is the initial node embedding of node u,.
1 h(cid:62).
2 .
.
.
h(cid:62).
finally, we obtain the sentence embedding ma-|d|] ∈ rdm×|d| and entitytrix s = [h(cid:62)embedding matrix e ∈ rdm×|e|.
the i-th entitymay have many mentions, where we simply usestring matching to detect entity coreference follow-ing zheng et al.
(2019) , and the entity embeddingei is computed by the average of its mention nodeembedding, ei = mean({hj}j∈mention(i)).
in thisway, the sentences and entities are interactivelyrepresented in a context-aware way..3.3 event types detection.
since a document can express events of differenttypes, we formulate the task as a multi-label classi-ﬁcation and leverage sentences feature matrix s to.
*traditional methods in sentence-level ee also utilizegraph to extract events (liu et al., 2018; yan et al., 2019),based on the dependency tree.
however, our interaction graphis heterogeneous and have no demands for dependency tree..figure 3: the decoding module of git.
three eq-uity freeze records have been extracted completely,and git is predicting the startdate role for the eq-uity pledge records (in the dashed frame), based onthe global memory where tracker tracks the recordson-the-ﬂy.
both entity e and f are predicted as thelegal startdate role while a is not.
pre-deﬁned argu-ment roles are shown in the blue box, and git extractsrecords in this order.
capital letters (a-k) refer to dif-ferent entities.
a path from root to leaf node representsone unique event record..detect event types:.
a = multihead(q, s, s) ∈ rdm×tr = sigmoid(a(cid:62)wt) ∈ rt.
where q ∈ rdm×t and wt ∈ rdm are train-able parameters, and t denotes the number ofpossible event types.
multihead refers to thestandard multi-head attention mechanism withquery/key/value.
therefore, we derive the eventtypes detection loss with golden label (cid:98)r ∈ rt :.
ldetect = −.
(cid:98)rt = 1.log p (rt|d).
t(cid:88).
(cid:16).
i.
(cid:17).
t=1(cid:16).
+ i.
(cid:17).
(cid:98)rt = 0.log (1 − p (rt|d)).
(2).
3.4 event records extraction.
since a document is likely to express multiple eventrecords and the number of records cannot be knownin advance, we decode records by expanding atree orderly as previous methods did (zheng et al.,2019).
however, they treat each record indepen-dently.
instead, to incorporate the interdependencyamong event records, we propose a tracker mod-ule, which improves the model performance..to be self-contained, we introduce the orderedin each step,.
tree expanding in this paragraph..3536equityfreezeequityholderfrozesharesstartdatelegalinstitutionacdefhiequitypledgepledgerabkpledgeestartdatea     d     f      iglobal memorya      bcompleteduncompleteda…b     c     g     ja      k…trackervirtual nodevirtual nodebjcga     c     e     hefwe extract event records of a speciﬁc event type.
the arguments extraction order is predeﬁned sothat the extraction is modeled as a constrained treeexpanding task†.
taking equity freeze records asan example, as shown in figure 3, we ﬁrstly extractequityholder, followed by frozeshares and others.
starting from a virtual root node, the tree expandsby predicting arguments in a sequential order.
asthere may exist multiple eligible entities for theevent argument role, the current node will expandseveral branches during extraction, with differententities assigned to the current role.
this branchingoperation is formulated as multi-label classiﬁcationtask.
in this way, each path from the root node tothe leaf node is identiﬁed as a unique event record..interdependency exists extensively among dif-ferent event records.
for example, as shown infigure 1, an equity underweight event recordis closely related to an equity overweight eventrecord, and they may share some key argumentsor provide useful reasoning information.
to takeadvantage of such interdependency, we propose anovel tracker module inspired by memory network(weston et al., 2015).
intuitively, the tracker con-tinually tracks the extracted records on-the-ﬂy andstore the information into a global memory.
whenpredicting arguments for current record, the modelwill query the global memory and therefore makeuse of useful interdependency information of otherrecords..in detail, for the i-th record path consisting ofa sequence of entities, the tracker encodes thecorresponding entity representation sequence ui =[ei1, ei2, ...] into an vector gi with an lstm (lasthidden state) and add event type embedding.
thenthe compressed record information is stored in theglobal memory g, which is shared across differentevent types as shown in figure 3. for extraction,given a record path ui ∈ rdm×(j−1) with the ﬁrstj − 1 arguments roles, we predict the j-th roleby injecting role-speciﬁc information into entityrepresentations, e = e + rolej , where rolejis the role embedding for the j-th role.
then weconcatenate e, sentences feature s, current entitiespath ui, and the global memory g, followed bya transformer to obtain new entity feature matrix(cid:101)e ∈ rdm×|e|, which contains global role-speciﬁc.
information for all entity candidates.‡.
[ (cid:101)e, (cid:101)s, (cid:101)ui, (cid:101)g] = transformer([e; s; ui; g]).
we treat the path expansion as a multi-label clas-siﬁcation problem with a binary classiﬁer over (cid:101)ei,i.e., predicts whether the i-th entity is the next ar-gument role for the current record and expand thepath accordingly as shown in figure 3..during training, we minimize the following loss:.
lrecord = −.
log p (yn.
t |n).
(3).
(cid:88).
|e|(cid:88).
n∈nd.
t=1.
where nd denotes the nodes set in the eventrecords tree, and ynif thett-th entity is validate for the next argument in noden, then yn.
is the golden label..t = 1, otherwise yn.
t = 0..3.5 training.
we sum the losses coming from three sub-taskswith different weight respectively in eq.
(1), (2)and (3) as follows:.
lall = λ1lner + λ2ldetect + λ3lrecord.
more training details are shown in appendix a..4 experiments.
4.1 dataset.
we evaluate our model on a public dataset proposedby zheng et al.
(2019)§, which is constructed fromchinese ﬁnancial documents.
it consists of up to32, 040 documents which is the largest document-level ee dataset by far.
it focuses on ﬁve eventtypes: equity freeze (ef), equity repurchase (er),equity underweight (eu), equity overweight (eo)and equity pledge (ep), with 35 different kinds ofargument roles in total.
we follow the standard splitof the dataset, 25, 632/3, 204/3, 204 documentsfor training/dev/test set.
the dataset is quite chal-lenging, as a document has 20 sentences and con-sists of 912 tokens on average.
besides, there areroughly 6 sentences involved for an event record,and 29% documents express multiple events..‡to distinguish different parts in the concatenated vector,we also add segment embedding, which is omitted in eq.
3.4..§https://github.com/dolphin-zs/.
†we simply adopt the order used by zheng et al.
(2019)..doc2edag/blob/master/data.zip.
3537model.
ef.
er.
eu eo ep overall.
model.
i.ii.
iii.
iv.
46.7dcfee-sdcfee-m 42.7greedy-dec57.7doc2edag 71.0.
80.073.379.488.4.
47.545.851.269.8.
46.744.650.073.5.
56.153.854.274.8.git (ours).
73.4.
90.8.
74.3.
76.3.
77.7.
60.356.661.077.5.
80.3.table 1: f1 scores on test set.
git achieves the bestperformance.
we also list the results reported in zhenget al.
(2019) in appendix b, and git consistently out-performs other baselines.
ef/er/eu/eo/ep refer tospeciﬁc event types, and overall denotes micro f1..4.2 experiments setting.
in our implementation of git, we use 8 and 4 lay-ers transformer (vaswani et al., 2017) in encodingand decoding module respectively.
the dimensionsin hidden layers and feed-forward layers are thesame as previous work (zheng et al., 2019), i.e.,768 and 1, 024. we also use l = 3 layers of gcn,and set dropout rate to 0.1, batch size to 64. git istrained using adam (kingma and ba, 2015) as opti-mizer with 1e − 4 learning rate for 100 epochs.
weset λ1 = 0.05, λ2 = λ3 = 1 for the loss function..4.3 baselines and metrics.
yang et al.
(2018) proposes dcfee that extractsarguments from the identiﬁed central sentence andqueries surrounding sentences for missing argu-ments.
the model has two variants, dcfee-s anddcfee-m. dcfee-s produces one record at atime, while dcfee-m produces multiple possi-ble argument combinations by the closest distancefrom the central sentence.
besides, doc2edag(zheng et al., 2019) uses transformer encoder to ob-tain sentence and entity embeddings, followed byanother transformer to fuse cross-sentence context.
then multiple events are extracted simultaneously.
greedy-dec is a variant of doc2edag, which pro-duces only one record greedily..three sub-tasks of the document-level ee areall evaluated by f1 score.
due to limited space,we leave the results of entity extraction and eventtypes detection in appendix b, which shows gitonly slightly outperform doc2edag, because wemainly focus on event record extraction and themethods are similar to doc2edag for these twosub-tasks.
in the following, we mainly report andanalyze the results of event record extraction..dcfee-s64.6dcfee-m 54.8greedy-dec67.4doc2edag 79.6.
70.054.168.082.4.
57.751.560.878.4.
52.347.150.272.0.git (ours).
81.9.
85.7.
80.0.
75.7.table 2: f1 scores on four sets with growing averagenumber of involved sentences for records (increasesfrom i to iv).
the highest improvement of git comesfrom event records involving the most sentences (setiv) by 3.7 f1 score compared with doc2edag..4.4 main results.
overall performance.
the results of the overallperformance on the document-level ee dataset isillustrated in table 1. as table 1 shows, our gitconsistently outperforms other baselines, thanksto better modelling of global interactions and in-terdependency.
speciﬁcally, git improves 2.8 mi-cro f1 compared with the previous state-of-the-art,doc2edag, especially 4.5 improvement in equityunderweight (eu) event type..cross-sentence records scenario.
there aremore than 99.5% records of the test set are cross-sentence event records, and the extraction becomesgradually more difﬁcult as the number of their in-volved sentences grows.
to veriﬁes the effective-ness of git to capture cross-sentence information,we ﬁrst calculate the average number of sentencesthat the records involve for each document, and sortthem in ascending order.
then we divide them intofour sets i/ii/iii/iv with equal size.
documents inset.
iv is considered to be the most challengingas it requires the most number of sentences to suc-cessfully extract records.
as table 2 shows, gitconsistently outperforms doc2edag, especiallyon the most challenging set.
iv that involves themost sentences, by 3.7 f1 score.
it suggests thatgit can well capture global context and mitigatethe arguments-scattering challenge, with the helpof the heterogeneous graph interaction network..multiple records scenario.
git introduces thetracker to make use of global interdependencyamong event records, which is important in mul-tiple records scenario.
to illustrate its effective-ness, we divide the test set into single-record set(s.) containing documents with one record, andmulti-record set (m.) containing those with multi-ple records.
as shown in table.
3, f1 score on m..3538model.
ef.
er.
eu.
eo.
ep.
overall.
s. m..s. m..s. m..s. m..s. m..s. m..dcfee-s55.7dcfee-m 45.3greedy-dec74.0doc2edag 79.7.
38.140.540.763.3.
83.076.182.290.4.
55.550.650.070.7.
52.348.361.574.7.
41.443.135.663.3.
49.245.763.476.1.
43.643.329.470.2.
62.458.178.684.3.
52.251.236.569.3.
69.063.277.881.0.
50.349.437.067.4.git (ours).
81.9.
65.9.
93.0.
71.7.
82.0.
64.1.
80.9.
70.6.
85.0.
73.5.
87.6.
72.3.table 3: f1 scores on single-record (s.) and multi-record (m.) sets..model.
git- s-s- s-m- m-mintra- m-minter- graph.
f1.
80.3-1.4-1.0-1.3-1.1-2.0.i.ii.
iii.
iv.
81.9-0.9-1.6-0.5-0.5-1.8.
85.7-0.1-1.7-1.4-1.6-1.5.
80.0-1.9-0.7-2.4-1.4-2.0.
75.7-2.3-0.7-1.5-1.7-2.5.table 4: the decrease of f1 scores on ablation studyfor git’s heterogeneous graph interaction network.
re-moving the heterogeneous graph leads to signiﬁcantdrop on f1, especially for records involving the mostsentences (i.e., −2.5 f1 on set iv)..model.
p.r.f1.
s. m..git.
82.3git-ot -0.6-1.0git-opgit-nt -2.8.
78.4-0.4-1.6+0.1.
80.3-0.5-1.2-1.3.
87.6-0.8-1.0-1.3.
72.3-0.7-1.5-1.5.table 5: performance of git on ablation study for thet racker module.
the removal of the tracker (git-nt) brings about higher f1 decrease on m. than thaton s.. s.: single-record set, m.: multi-record set..is much lower than that on s., indicating it is chal-lenging to extract multiple records.
however, gitstill surpasses other strong baselines by 4.9 ∼ 35.3on multi-record set (m.).
this is because git isaware of other records through the t racker mod-ule, and leverage the interdependency informationto improve the performance¶..¶nguyen et al.
(2016) maintain three binary matrices tomemorize entities and events states.
although they aimat sentence-level ee that contains fewer entities and eventrecords, it would be also interesting to compare with them andwe leave it as future work..figure 4: f1 scores on documents with different num-ber of event records.
the f1 gap between w/ (git) andw/o tracker (git-nt) becomes wider as the number ofevent records of documents increases..4.5 analysis.
we conduct further experiments to analyze the keymodules in git more deeply..on the effect of heterogeneous graph interac-tion network.
the heterogeneous graph we con-structed contains four types of edges.
to exploretheir functions, we remove one type of edges at atime, and remove the whole graph network ﬁnally.
results are shown in table 4, including micro f1and f1 on the four sets, which are divided by thenumber of involved sentences for records as we didbefore.
the micro f1 would decreases 1.0 ∼ 1.4without a certainty type of edge.
besides, removingthe whole graph causes an signiﬁcant drop by 2.0f1, especially for set iv by 2.5, which requiresthe most number of sentences to extract the eventrecord.
it demonstrates that the graph interactionnetwork helps improve the performance, especiallyon records involving many sentences, and all kindsof edges play an important role for extraction..on the effect of tracker module.
git canleverage interdependency among records based onthe information of other event records tracked bytracker.
to explore its effect, ﬁrstly, we removethe global interdependency information betweenrecords of different event types, by clearing theglobal memory whenever we extract events for an-.
35396365676971732 - 34 - 5>=6f1 scorethe number of records of documentsgitgit-otgit-ntdoc2edagfigure 5: the case study of our proposed git and doc2edag, with their key prediction difference colored inred.
related entities are colored in blue.
git successfully extract totalholdingshares and totalpledgedshares forrecord 2, while doc2edag fails.
the complete content are provided in appendix c..other new event type (git-own type).
next, weremove all the tracking information except the ownpath for a record, to explore whether the trackingof other records makes effect indeed (git-ownpath).
finally, we remove the whole tracker mod-ule (git-no tracker).
as table 5 shows, the f1in git-ot/git-op decreases by 0.5/1.2, suggest-ing the interdependency among records of both thesame and different event types do play an essentialrole.
besides, their f1 decrease in m. by 0.7/1.5 aremore than those in s. by 0.8/1.0, verifying the ef-fectiveness of the tracker in multi-event scenarios.
moreover, the performances are similar betweengit-op and git-nt, which also provides evidencethat other records do help.
we also reveal f1 ondocuments with different number of records in fig-ure 4. the gap between models with or withouttracker raises as the number of records increases,which validates the effectiveness of our tracker..4.6 case study.
figure 5 demonstrates a case of the predictions ofdoc2edag and git for equity pledge (ep) eventtypes.
the totalholdingshares and totalpledged-shares information lies in sentence 8, while thepledgedshares and pledgee information for record2 lies in sentence 5. though doc2edag fails toextract these arguments in record 2 (colored inred), git succeeds because it can capture interac-tions between long-distance sentences, and utilizethe information of record 1 (325.4 million and218.6 million) thanks to the tracker model..5 related work.
sentence-level event extraction.
previous ap-proaches mainly focus on sentence-level event.
extraction.
chen et al.
(2015) propose a neuralpipeline model that identiﬁes triggers ﬁrst and thenextracts argument roles.
nguyen et al.
(2016) use ajoint model to extract triggers and argument rolessimultaneously.
some studies also utilize depen-dency tree information (liu et al., 2018; yan et al.,2019).
to utilize more knowledge, some studiesleverage document context (chen et al., 2018; zhaoet al., 2018), pre-trained language model (yanget al., 2019), and explicit external knowledge (liuet al., 2019a; tong et al., 2020) such as wordnet(miller, 1995).
du and cardie (2020b) also try toextract events in a question-answer way.
thesestudies usually conduct experiments on sentence-level event extraction dataset, ace05 (walker et al.,2006).
however, it is hard for the sentence-levelmodels to extract multiple qualiﬁed events span-ning across sentences, which is more common inreal-world scenarios..document-level event extraction.
document-level ee has attracted more and more attention re-cently.
yang and mitchell (2016) use well-deﬁnedfeatures to handle the event-argument relationsacross sentences, which is, unfortunately, quitenontrivial.
yang et al.
(2018) extract events froma central sentence and ﬁnd other arguments fromneighboring sentences separately.
although zhenget al.
(2019) use transformer to fuse sentencesand entities, interdependency among events is ne-glected.
du and cardie (2020a) try to encode thesentences in a multi-granularity way and du et al.
(2020) leverage a seq2seq model.
they conductexperiments on muc-4 (sundheim, 1992) datasetwith 1, 700 documents and 5 kinds of entity-basedarguments, and it is formulated as a table-ﬁllingtask, coping with single event record of single event.
3540… [5] the shareholder of the company, quanliechen,pledged 52.4 millionto gdzq  co., ltd.in 2018, and supplemented the pledge recently because of the decline of the share price.
… [7]since the borrowings have been paid off, quanliechencompleted the pledge cancellation procedures of 35.5 millionthat were pledged to gtja co., ltd.on nov 7, 2018.
[8] as of today, quanliechenholds a total of 325.4 millionof the company, and there are still 218.6 millionin pledge status.
…quanliechenquanliechenpledger   pledgedsharespledgee   totalholdingsharestotalpledgedshares35.5 milliongtja  co., ltd.325.4 million218.6 million52.4 milliongdzq co., ltd.nullnull………doc2edagquanliechenquanliechenpledger   pledgedsharespledgee   totalholdingsharestotalpledgedshares35.5 milliongtja co., ltd.325.4 million218.6 million52.4 milliongdzq co., ltd.325.4 million218.6 million………gitno.12no.12type.
however, our work is different from thesestudies in that a) we utilize heterogeneous graph tomodel the global interactions among sentences andmentions to capture cross-sentence context, b) andwe leverage the global interdependency throughtracker to extract multiple event records of multi-ple event types..6 conclusion.
although promising in practical application,document-level ee still faces some challenges suchas arguments-scattering phenomenon and multi-ple correlated events expressed by a single docu-ment.
to tackle the challenges, we introduce het-erogeneous graph-based interaction model witha tracker (git).
git uses a heterogeneous graphinteraction network to model global interactionsamong sentences and entity mentions.
git alsouses a tracker to track the extracted records toconsider global interdependency during extraction.
experiments on large-scale public dataset (zhenget al., 2019) show git outperforms previous state-of-the-art by 2.8 f1.
further analysis veriﬁes theeffectiveness of git especially in cross-sentenceevents extraction and multi-event scenarios..acknowledgments.
the authors would like to thank changzhi sun,mingxuan wang, and the anonymous review-ers for their thoughtful and constructive com-ments.
this paper is supported in part by the na-tional key r&d program of china under grandno.2018aaa0102003, the national science foun-dation of china under grant no.61936012 and61876004..references.
samy bengio, oriol vinyals, navdeep jaitly, andnoam shazeer.
2015. scheduled sampling for se-quence prediction with recurrent neural networks.
in proceedings of the 28th international confer-ence on neural information processing systems(neurips)..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (acl-ijcnlp)..yubo chen, hang yang, kang liu, jun zhao, and yan-tao jia.
2018. collective event detection via a hier-.
archical and bias tagging networks with gated multi-in proceedings of thelevel attention mechanisms.
2018 conference on empirical methods in naturallanguage processing (emnlp)..xinya du and claire cardie.
2020a.
document-levelevent role ﬁller extraction using multi-granularitycontextualized encoding.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics (acl)..xinya du and claire cardie.
2020b.
event extractionin pro-by answering (almost) natural questions.
ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp)..xinya du, alexander m. rush, and claire cardie.
2020. document-level event-based extraction us-ing generative template-ﬁlling transformers.
arxivpreprint arxiv:2008.09249..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations(iclr)..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in 5th international conference on learn-ing representations (iclr)..jian liu, yubo chen, and kang liu.
2019a.
exploit-ing the ground-truth: an adversarial imitation basedknowledge distillation approach for event detection.
in proceedings of the 33rd aaai conference on ar-tiﬁcial intelligence (aaai)..shulin liu, yang li, feng zhang, tao yang, and xin-peng zhou.
2019b.
event detection without triggers.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..xiao liu, zhunchen luo, and heyan huang.
2018.jointly multiple events extraction via attention-in proceed-based graph information aggregation.
ings of the 2018 conference on empirical methodsin natural language processing (emnlp)..george a. miller.
1995. wordnet: a lexical database.
for english.
commun.
acm..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentneural networks.
in proceedings of the 2016 confer-ence of the north american chapter of the associa-tion for computational linguistics (naacl)..adam paszke, sam gross, soumith chintala, gregorychanan, edward yang, zachary devito, zeminglin, alban desmaison, luca antiga, and adamlerer.
2017. automatic differentiation in pytorch.
in nips-w..3541beth m. sundheim.
1992. overview of the fourth mes-sage understanding evaluation and conference.
infourth message uunderstanding conference (muc-4)..meihan tong, bin xu, shuai wang, yixin cao, leihou, juanzi li, and jun xie.
2020. improving eventdetection via open-domain trigger knowledge.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics (acl)..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems (neurips)..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilingualin philadelphia: linguistic datatraining corpus.
consortium..minjie wang, lingfan yu, da zheng, quan gan,yu gai, zihao ye, mufei li, jinjing zhou, qi huang,chao ma, ziyue huang, qipeng guo, hao zhang,haibin lin, junbo zhao, jinyang li, alexander jsmola, and zheng zhang.
2019. deep graph li-brary: towards efﬁcient and scalable deep learn-iclr workshop on representationing on graphs.
learning on graphs and manifolds..jason weston, sumit chopra, and antoine bordes.
2015. memory networks.
in 3rd international con-ference on learning representations (iclr)..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated atten-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp)..bishan yang and tom m. mitchell.
2016. joint extrac-tion of events and entities within a document context.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics (naacl)..hang yang, yubo chen, kang liu, yang xiao, and junzhao.
2018. dcfee: a document-level chinese ﬁ-nancial event extraction system based on automati-in proceedings of thecally labeled training data.
56th annual meeting of the association for compu-tational linguistics (acl)..sen yang, dawei feng, linbo qiao, zhigang kan,and dongsheng li.
2019. exploring pre-trained lan-guage models for event extraction and generation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics (acl)..conference on empirical methods in natural lan-guage processing (emnlp).
association for com-putational linguistics..ying zeng, yansong feng, rong ma, zheng wang, ruiyan, chongde shi, and dongyan zhao.
2018. scaleup event extraction learning via automatic train-in proceedings of the thirty-ing data generation.
second aaai conference on artiﬁcial intelligence(aaai)..yue zhao, xiaolong jin, yuanzhuo wang, and xueqicheng.
2018. document embedding enhanced eventdetection with hierarchical and supervised attention.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (acl)..shun zheng, wei cao, wei xu, and jiang bian.
2019.doc2edag: an end-to-end document-level frame-work for chinese ﬁnancial event extraction.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp)..a training details.
to mitigate the error propagation due to the gapbetween training and inference phrase (i.e., theextracted entities are ground truth during trainingbut predicted results during inference), we adoptscheduled sampling strategy (bengio et al., 2015)as zheng et al.
(2019) did.
we gradually switchthe entity extraction results from golden label towhat the model predicts on its own.
speciﬁcally,from epoch 10 to epoch 20, we linearly increase theproportion of predicted entity results from 0% to100%.
we implement git under pytorch (paszkeet al., 2017) and dgl (wang et al., 2019) based oncodes provided by zheng et al.
(2019)..all the experiments (including the baselines) arerun with the same 8 tesla-v100 gpus and thesame version of python dependencies to ensure thefairness..hyperparameters trials are listed in table 6. thevalue of hyperparameters we ﬁnally adopted arein bold.
note that we do not tune all the hyperpa-rameters, and make little effort to select the besthyperparameters for our git..we choose the ﬁnal checkpoints for test accord-ing to the micro f1 performance on the dev set.
ta-ble 9 illustrates the best epoch in which the modelachieves the highest micro f1 on the dev set andtheir according f1 score..b additional evaluation results.
shuang zeng, runxin xu, baobao chang, and lei li.
2020. double graph based reasoning for document-level relation extraction.
in proceedings of the 2020.we have showed the evaluation results of eventrecords extraction in the paper for document-level.
3542figure 6: the original complete document corresponding to the case study in figure 5. sentences in red color arepresented in figure 5..event extraction.
in this section, we also illustatethe results of entity extraction in table.
7 and eventtypes detection in table.
8. moreover, the compre-hensive results of event record extraction is shownin table.
10, including results reported in zhenget al.
(2019) with precison, recall and f1 score..c complete document for the examples.
we show an example document in figure 1 in thepaper.
to better illustrate, we translate it fromchinese into english and make some simplication.
here we present the original complete documentexample in figure 7. for the speciﬁc meanings ofargument roles, we recommend readers to refer to(zheng et al., 2019)..we also demonstrate an case study in figure 5 inthe paper.
now we also show its original chineseversion in figure 6..hyperparametersbatch sizelearning ratedropoutlayers of gcnnumber of epochsλ1λ2λ3gradient accumulation stepslayers of transformer in entity extractorlayers of transformer in decoder modulehyperparameter search trials.
value32, 640.00010.11, 2, 3, 4, 51000.051.001.0088410.table 6: hyperparameters for our proposed git..pmodel86.5dcfee-sdcfee-m 86.6greedy-dec87.5doc2edag 88.085.8git (ours).
r88.689.089.890.092.6.f187.687.888.689.089.1.table 7: results of entity extraction sub-task on thetest set.
the performance of different models are simi-lar, for the reason that they all utilize the same structureand methods to extract entities..3543[1] 证券代码：002102证券简称：冠福股份编号：2018-112。[2] 冠福控股股份有限公司关于大股东陈烈权先生部分股份补充质押及解除质押的公告。[3]本公司及董事会全体成员保证信息披露的内容真实、准确、完整，没有虚假记载、误导性陈述或者重大遗漏。[4]冠福控股股份有限公司（以下简称“公司”）近日接到公司大股东陈烈权先生函告，获悉其将持有的公司部分股份办理了补充质押及解押，具体情况如下。[5]一、本次股份补充质押情况。公司大股东陈烈权先生原于2017年10月24日质押给国泰君安证券股份有限公司（以下简称“国泰君安”）的公司股份69200000股、2018年2月8日质押给中信建投证券股份有限公司（以下简称“中信建投”）的公司股份52000000股、2018年2月26日质押给国都证券股份有限公司（以下简称“国都证券”）的公司股份52369050股，因公司近日股价下跌，分别对国君证券、中信建投及国都证券进行补充质押。[6] 上述原有质押情况详见公司分别于2017年10月27日、2018年2月12日、3月1日在《证券时报》、《中国证券报》、《上海证券报》和《证券日报》及巨潮资讯网上披露的《冠福控股股份有限公司关于大股东陈烈权先生部分股份质押及解除质押的公告》（公告编号：2017-108）、《冠福控股股份有限公司关于大股东陈烈权先生部分股份解除质押及再质押的公告》（公告编号：2018-010、2018-013）。[7]二、本次股份解除质押情况。陈烈权先生原质押给国泰君安的公司股份35500000股（占公司总股本的1.35%），因已还清国泰君安的借款，分别于2018年9月7日、9月10日在国泰君安证券股份有限公司荆州便河东路营业部办理完成质押解除手续。[8] 三、累计质押情况。截止本公告日，陈烈权先生共持有公司股份325363822股，占公司总股本的12.35%，其中处于质押状态的股份累计数为218569050股，占公司总股本的8.30%。[9] 四、备查文件[10] 1、中信建投证券股份有限公司股票质押式回购交易申请书（补充交易）；[11] 2、国都证券股份有限公司股票质押式回购交易补充质押已达成通知；[12] 3、国泰君安证券股份有限公司股票质押式回购交易协议书。[13] 特此公告。[14] 冠福控股股份有限公司董事会[15] 二○一八年九月十二日efmodeldcfee-s81.5dcfee-m 79.899.3greedy-decdoc2edag 99.098.8git (ours).
er94.092.499.999.899.8.eu eo ep overall82.378.996.896.897.9.
85.784.295.494.196.6.
93.892.999.699.599.6.
91.490.099.098.999.2.table 8: f1 scores results of event types detection sub-task on the test set.
all the models obtains more than 90.0micro f1 score.
git slightly outperform doc2edag..modeldcfee-sdcfee-mgreedy-decdoc2edaggit (ours).
best epoch ef51.352.557.575.278.3.
8687908989.er73.069.176.085.287.6.eu eo ep overall44.143.955.171.674.7.
51.447.249.380.080.9.
58.655.957.077.979.8.
58.755.859.178.780.7.table 9: the best epoch in which the models achieve the highest micro f1 score on the dev set and the corre-sponding performance..3544-.
-.
-.
-.
-.
-.
-.
-.
1f.r.llarevo.p.-.
-.
-.
-.
3.
..06.
6.
..65.
0.
..16.
5.
..77.
..308.
..445.
..255.
..194.
..057.
4.
..87.
..776.
..185.
..408.
..308.
3.
..28.
1f...936.
..926.
1.
..26.
..377.
..165.
..835.
..245.
..847.
7.
..77.pe.r.6.
..36.
4.
..66.
7.
..84.
8.
..47.
8.
..94.
4.
..25.
4.
..04.
0.
..37.
9.
..67.p.3.
..46.
8.
..95.
7.
..58.
0.
..08.
2.
..46.
3.
..55.
1.
..28.
7.
..67.
6.
..87.
1f.6.
..64.
9.
..44.
..315.
0.
..57.
7.
..64.
6.
..44.
0.
..05.
5.
..37.
..367.oe.r.6.
..24.
5.
..74.
..604.
0.
..96.
5.
..64.
7.
..64.
6.
..04.
1.
..07.
..327.p.4.
15.
5.
24.
7.
96.
1.
28.
9.
64.
8.
24.
8.
46.
2.
77.
7.
08.
1f.3.
54.
2.
44.
2.
15.
8.
17.
5.
74.
8.
54.
2.
15.
8.
96.
3.
47.ue.r.4.
53.
9.
93.
8.
04.
0.
56.
0.
93.
4.
14.
7.
04.
6.
16.
6.
66.p.7.
26.
5.
94.
7.
86.
2.
08.
8.
06.
4.
15.
0.
96.
4.
08.
9.
38.
1f.1.
38.
8.
08.
9.
87.
3.
78.
0.
08.
3.
37.
4.
97.
4.
88.
8.
09.re.r.8.
18.
0.
87.
9.
47.
6.
38.
0.
67.
5.
17.
3.
57.
8.
68.
2.
98.p.5.
48.
7.
38.
3.
38.
3.
19.
5.
48.
2.
57.
9.
38.
0.
09.
3.
29.
1f.1.
15.
6.
54.
9.
85.
2.
07.
7.
64.
7.
24.
7.
75.
0.
17.
4.
37.fe.r.6..14.
7..04.
8..64.
5..46.
8..73.
9..04.
6..54.
7..46.
5..86.p...066.
..815.
..597.
..177.
..116.
..644.
..587.
..787.
..987.
♦s-eefcd.♦m-eefcd.♦ced-ydeerg.♦gade2cod.♠s-eefcd.♠m-eefcd.♠ced-ydeerg.♠gade2cod.♠)sruo(.
ti.g.ledom.table 10: comprehensive results of event record extraction.
results with ♦ are results reported in zheng et al.
(2019).
results with are ♠ results we implement on our own.
our git consistently outperform other baselines..3545figure 7: the original complete document corresponding to the running example in figure 1. sentences in redcolor are presented in figure 1..3546[1] 证券代码：300126 证券简称：锐奇股份公告编号: 2014-075。[2] 上海锐奇工具股份有限公司关于控股股东股份减持计划实施进展的公告。[3] 本公司及董事会全体成员保证信息披露的内容真实、准确、完整，没有虚假记载、误导性陈述或者重大遗漏。[4] 上海锐奇工具股份有限公司(以下简称”公司”)于2014年11月1日在中国证券监督管理委员会指定的创业板信息披露网站披露了《关于控股股东股份减持计划的公告》(公告编号2014-074)。[5] 公司于2014年11月6日接到公司控股股东吴明厅先生的《股份减持告知函》。[6] 吴明厅先生于2014年11月6日通过深圳证券交易所大宗交易方式减持了其直接持有的公司无限售条件流通股7200000股，占公司目前总股本的2.34%。[7] 一、股东减持情况。吴明厅先生本次减持的公司股份7200000股为其直接持有的公司无限售条件流通股，占公司总股本的2.34%，本次减持的公司股份全部转让给吴晓婷女士。[8]吴晓婷女士为吴明厅先生的女儿，两人为父女关系，根据相关规定被认定为一致行动人。[9]二、其他相关说明。1、本次减持没有违反《深圳证券交易所创业板股票上市规则》、《上市公司解除限售存量股份转让指导意见》等有关法律法规及公司规章制度。[10] 2、本次减持不存在违反《证券法》、《上市公司收购管理办法》等法律、行政法规、部门规章、规范性文件和深圳证券交易所《创业板信息披露业务备忘录第18号：控股股东、实际控制人股份减持信息披露》等规定的情况。[11] 3、本次减持后，吴明厅先生直接持有公司总股本的比例下降为32.08%，通过上海瑞浦投资有限公司持有公司总股本的14.02%，合计持有公司总股本的46.82%，仍为公司控股股东。[12] 4、本次减持后，吴明厅、上海瑞浦投资有限公司、应媛琳、吴晓依、吴晓婷作为一致行动人，其所合计持有的公司股份权益并未减少，仍为公司总股本的56.22%。[13]三、备查文件。[14] 1、吴明厅先生的《股份减持告知函》。[15] 2．深交所要求的其他文件。[16]上海锐奇工具股份有限公司董事会。[17] 2014年11月6日。equityholdertradedsharesstartdateenddatelaterholdingsharesaverageprice吴明厅720000股2014年11月6日2014年11月6日nullnullequityholdertradedsharesstartdateenddatelaterholdingsharesaverageprice吴晓婷720000股2014年11月6日2014年11月6日720000股nullequityunderweightequityoverweight