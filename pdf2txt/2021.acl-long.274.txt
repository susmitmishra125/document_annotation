document-level event extraction via heterogeneousgraph-based interaction model with a tracker.
runxin xu1, tianyu liu1, lei li3 and baobao chang1,2âˆ—1key laboratory of computational linguistics, peking university, moe, china2peng cheng laboratory, shenzhen, china3bytedance ai labrunxinxu@gmail.com,lileilab@bytedance.com{tianyu0421,chbb}@pku.edu.cn.
abstract.
document-level event extraction aims to rec-ognize event information from a whole pieceof article.
existing methods are not effectivedue to two challenges of this task: a) the tar-get event arguments are scattered across sen-tences; b) the correlation among events in adocument is non-trivial to model.
in this pa-per, we propose heterogeneous graph-basedinteraction model with a tracker (git) tosolve the aforementioned two challenges.
forthe ï¬rst challenge, git constructs a hetero-geneous graph interaction network to captureglobal interactions among different sentencesand entity mentions.
for the second, git in-troduces a tracker module to track the ex-tracted events and hence capture the interde-pendency among the events.
experiments ona large-scale dataset (zheng et al., 2019) showgit outperforms the existing best methods by2.8 f1.
further analysis reveals git is ef-fective in extracting multiple correlated eventsand event arguments that scatter across thedocument.
our code is available at https://github.com/runxinxu/git..figure 1: an example document from a chinesedataset proposed by zheng et al.
(2019) in the ï¬nancialdomain, and we translate it into english for illustration.
entity mentions are colored.
due to space limitation,we only show four associated sentences and three argu-ment roles of each event type.
the complete originaldocument can be found in appendix c. eu: equity un-derweight, eo: equity overweight..1.introduction.
event extraction (ee) is one of the key and chal-lenging tasks in information extraction (ie), whichaims to detect events and extract their argumentsfrom the text.
most previous methods (chen et al.,2015; nguyen et al., 2016; liu et al., 2018; yanget al., 2019; du and cardie, 2020b) focus onsentence-level ee, extracting events from a sin-gle sentence.
the sentence-level model, however,fails to extract events whose arguments spread inmultiple sentences, which is much more commonin real-world scenarios.
hence, extracting events atthe document-level is critical.
it has attracted muchattention recently (yang et al., 2018; zheng et al.,2019; du and cardie, 2020a; du et al., 2020)..*corresponding author..though promising, document-level ee still facestwo critical challenges.
firstly, the argumentsof an event record may scatter across sentences,which requires a comprehensive understanding ofthe cross-sentence context.
figure 1 illustrates anexample that one equity underweight (eu) and oneequity overweight (eo) event records are extractedfrom a ï¬nancial document.
it is less challengingto extract the eu event because all the related ar-guments appear in the same sentence (sentence2).
however, for the arguments of eo record, nov6, 2014 appears in sentence 1 and 2 while xiaot-ing wu in sentence 3 and 4. it would be quitechallenging to identify such events without con-sidering global interactions among sentences andentity mentions.
secondly, a document may ex-press several correlated events simultaneously, andrecognizing the interdependency among them is.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3533â€“3546august1â€“6,2021.Â©2021associationforcomputationallinguistics3533[1] on nov 6, 2014, the company received a letterof share reduction frommingting wu, theshareholder of the company.
[2] mingting wu decreased his holding of 7.2 million shares of the company on the shenzhen stock exchange on nov 6, 2014.
[3]the 7.2 millionshares of the company mingting wureduced this time were transferred to xiaoting wu.
[4] xiaoting wu is the daughter of mingting wu, and they were identified as persons acting in concert according to relevant regulations.eventtypeequityholdertradedsharesstartdate7.2 millionnov 6, 2014xiaotingwueoeumingtingwu7.2 millionnov 6, 2014â€¦â€¦â€¦fundamental to successful extraction.
as shown infigure 1, the two events are interdependent becausethey correspond to exactly the same transactionand therefore share the same startdate.
effectivemodeling on such interdependency among the cor-related events remains a key challenge in this task.
yang et al.
(2018) extracts events from a cen-tral sentence and query the neighboring sen-tences for missing arguments, which ignores thecross-sentence correspondence between augments.
though zheng et al.
(2019) takes a ï¬rst step to fusethe sentences and entities information via trans-former, they neglect the interdependency amongevents.
focusing on single event extraction, duand cardie (2020a) and du et al.
(2020) concate-nate multiple sentences and only consider a singleevent, which lacks the ability to model multipleevents scattered in a long document..to tackle the aforementioned two challenges, inthis paper, we propose a heterogeneous graph-based interaction model with a tracker (git)for document-level ee.
to deal with scatteredarguments across sentences, we focus on theglobal interactions among sentences and entitymentions.
speciï¬cally, we construct a hetero-geneous graph interaction network with mentionnodes and sentence nodes, and model the inter-actions among them by four types of edges (i.e.,sentence-sentence edge, sentence-mention edge,intra-mention-mention edge, and inter-mention-mention edge) in the graph neural network.
in thisway, git jointly models the entities and sentencesin the document from a global perspective..to facilitate the multi-event extraction, we targeton the global interdependency among correlatedevents.
concretely we propose a tracker module tocontinually tracks the extracted event records witha global memory.
in this way, the model is encour-aged to incorporate the interdependency with othercorrelated event records while predicting..we summarize our contributions as follows:.
â€¢ we construct a heterogeneous graph interac-tion network for document-level ee.
with dif-ferent heterogeneous edges, the model couldcapture the global context for the scatteredevent arguments across different sentences..â€¢ we introduce a novel tracker module to trackthe extracted event records.
the tracker easesthe difï¬culty of extracting correlated events,as interdependency among events would betaken into consideration..â€¢ experiments show git outperforms the pre-vious state-of-the-art model by 2.8 f1 on thelarge-scale public dataset (zheng et al., 2019)with 32, 040 documents, especially on cross-sentence events and multiple events scenarios(with 3.7 and 4.9 absolute increase on f1)..2 preliminaries.
we ï¬rst clarify some important notions.
a) entitymention: a text span within document that refersto an entity object; b) event argument: an entityplaying a speciï¬c event role.
event roles are pre-deï¬ned for each event type; c) event record: anentry of a speciï¬c event type containing argumentsfor different roles in the event.
for simplicity, weuse record for short in the following sections..following zheng et al.
(2019), given a docu-ment composed of sentences d = {si}|d|i=1 anda sentence containing a sequence of words si ={wj}|si|j=1, the task aims to handle three sub-tasks: 1) entity extraction: extracting entities e ={ei}|e|i=1 from the document to serve as argumentcandidates.
an entity may have multiple mentionsacross the document.
2) event types detection: de-tecting speciï¬c event types that are expressed bythe document.
3) event records extraction: ï¬nd-ing appropriate arguments for the expressed eventsfrom entities, which is the most challenging andalso the focus of our paper.
the task does not re-quire to identify event triggers (zeng et al., 2018;liu et al., 2019b), which reduces manual effort ofannotation and the application scenarios becomesmore extensive..3 methodology.
as shows in figure 2, git ï¬rst extracts candi-date entities through sentence-level neural extrac-tor (sec 3.1).
then we construct a heterogeneousgraph to model the interactions among sentencesand entity mentions (sec 3.2), and detect eventtypes expressed by the document (sec 3.3).
fi-nally we introduce a tracker module to continu-ously track all the records with global memory, inwhich we utilize the global interdependency amongrecords for multi-event extraction (sec 3.4)..3.1 entity extractiongiven a sentence s = {wj}|s|s into a sequence of vectors {gj}|si|.
j=1 âˆˆ d, we encodej=1 using trans-.
3534figure 2: overview of our git.
firstly, sentences of the document are fed into the encoder to obtain contextualizedrepresentation, followed by a crf layer to extract entities.
then git constructs a heterogeneous graph interactionnetwork with mention nodes and sentence nodes, which captures the global interactions among them based ongcns.
after obtaining document-aware representations of entities and sentences, git detects event types andextracts records through the decoding module with a tracker.
the tracker tracks extracted records with globalmemory, based on which the decoding module incorporates global interdependency among correlated event records.
different entities are marked by different colors.
m: mingting wu.
x: xiaoting wu.
n: nov 6, 2014. s: 7.2 million..former (vaswani et al., 2017):.
{g1, .
.
.
, g|s|} = transformer({w1, .
.
.
, w|s|}).
the word representation of wj is a sum of the cor-responding token and position embeddings..we extract entities at the sentence level and for-mulate it as a sequence tagging task with bio (be-gin, inside, other) schema.
we leverage a condi-tional random ï¬eld (crf) layer to identify entities.
for training, we minimize the following loss:.
lner = âˆ’.
log p (ys|s).
(1).
(cid:88).
sâˆˆd.
where ys is the golden label sequence of s. forinference, we use viterbi algorithm to decode thelabel sequence with the maximum probability..3.2 heterogeneous graph interaction.
network.
an event may span multiple sentences in the docu-ment, which means its corresponding entity men-tions may also scatter across different sentences.
identifying and modeling these entity mentions inthe cross-sentence context is fundamental in doc-ument ee.
thus we build a heterogeneous graphg which contains entity mention nodes and sen-tence nodes in the document d. in the graph g,interactions among multiple entity mentions and.
sentences can be explicitly modeled.
for each en-tity mention node e, we initialize node embed-ding h(0)e = mean({gj}jâˆˆe) by averaging therepresentation of the contained words.
for eachsentence node s, we initialize node embeddingh(0)s = max({gj}jâˆˆs) + sentpos(s) by max-pooling all the representation of words within thesentence plus sentence position embedding..to capture the interactions among sentences and.
mentions, we introduce four types of edges..sentence-sentence edge (s-s) sentence nodesare fully connected to each other with s-s edges.
in this way, we can easily capture the global prop-erties in the document with sentence-level interac-tions, e.g., the long range dependency between anytwo separate sentences in the document would bemodeled efï¬ciently with s-s edges..sentence-mention edge (s-m) we model thelocal context of an entity mention in a speciï¬csentence with s-m edge, speciï¬cally the edge con-necting the mention node and the sentence node itbelongs to..intra-mention-mention edge (m-mintra) weconnect distinct entity mentions in the same sen-tences with m-mintra edges.
the co-occurrence ofmentions in a sentence indicates those mentionsare likely to be involved in the same event.
we.
3535global memoryencodercrf layerclassifierentitypledgeentityfreezeentityoverweightâ€¦â€¦decodingmodule[1]mingtingwu decreased... 7.2 million shares ... on  nov 6, 2014.
[2]the 7.2 million shares ... mingtingwu â€¦ to xiaotingwu.
[3] xiaotingwu is the daughter of mingtingwu â€¦123nmmmxxssmxns123record 1record 2representationsdocument sentencesheterogeneous interaction graph networktypes detection and records extractionmsxn123mention nodesentence nodesentence-sentence edgesentence-mention edgeintra-mention-mention edgeinter-mention-mention edgetrackerğ“›ğ§ğğ«ğ“›ğ«ğğœğ¨ğ«ğğ“›ğğğ­ğğœğ­explicitly model this indication by m-mintra edges..inter-mention-mention edge (m-minter) theentity mentions that corresponds to the same entityare fully connected with each other by m-minteredges.
as in document ee, an entity usually cor-responds to multiple mentions across sentences,we thus use m-minter edge to track all the appear-ances of a speciï¬c entity, which facilitates the longdistance event extraction from a global perspective.
in section.
4.5, experiments show that all ofthese four kinds of edges play an important rolein event detection, and the performance would de-crease without any of them..after heterogeneous graph construction *, we ap-ply multi-layer graph convolution network (kipfand welling, 2017) to model the global interactionsinspired by zeng et al.
(2020).
given node u atthe l-th layer, the graph convolutional operation isdeï¬ned as follows:ï£«.
ï£¶.
h(l+1)u.
= relu.
ï£­.
(cid:88).
(cid:88).
kâˆˆk.
vâˆˆnk(u) (cid:83){u}.
1cu,k.
w (l).
k h(l).
v.ï£¸.
where k represents differenttypes of edges,w (l)k âˆˆ rdmÃ—dm is trainable parameters.
nk(u)denotes the neighbors for node u connected in k-thtype edge and cu,k is a normalization constant.
wethen derive the ï¬nal hidden state hu for node u,.
hu = wa[h(0).
u ; h(1).
u ; .
.
.
; h(l)u ].
where h(0)and l is the number of gcn layers..u is the initial node embedding of node u,.
1 h(cid:62).
2 .
.
.
h(cid:62).
finally, we obtain the sentence embedding ma-|d|] âˆˆ rdmÃ—|d| and entitytrix s = [h(cid:62)embedding matrix e âˆˆ rdmÃ—|e|.
the i-th entitymay have many mentions, where we simply usestring matching to detect entity coreference follow-ing zheng et al.
(2019) , and the entity embeddingei is computed by the average of its mention nodeembedding, ei = mean({hj}jâˆˆmention(i)).
in thisway, the sentences and entities are interactivelyrepresented in a context-aware way..3.3 event types detection.
since a document can express events of differenttypes, we formulate the task as a multi-label classi-ï¬cation and leverage sentences feature matrix s to.
*traditional methods in sentence-level ee also utilizegraph to extract events (liu et al., 2018; yan et al., 2019),based on the dependency tree.
however, our interaction graphis heterogeneous and have no demands for dependency tree..figure 3: the decoding module of git.
three eq-uity freeze records have been extracted completely,and git is predicting the startdate role for the eq-uity pledge records (in the dashed frame), based onthe global memory where tracker tracks the recordson-the-ï¬‚y.
both entity e and f are predicted as thelegal startdate role while a is not.
pre-deï¬ned argu-ment roles are shown in the blue box, and git extractsrecords in this order.
capital letters (a-k) refer to dif-ferent entities.
a path from root to leaf node representsone unique event record..detect event types:.
a = multihead(q, s, s) âˆˆ rdmÃ—tr = sigmoid(a(cid:62)wt) âˆˆ rt.
where q âˆˆ rdmÃ—t and wt âˆˆ rdm are train-able parameters, and t denotes the number ofpossible event types.
multihead refers to thestandard multi-head attention mechanism withquery/key/value.
therefore, we derive the eventtypes detection loss with golden label (cid:98)r âˆˆ rt :.
ldetect = âˆ’.
(cid:98)rt = 1.log p (rt|d).
t(cid:88).
(cid:16).
i.
(cid:17).
t=1(cid:16).
+ i.
(cid:17).
(cid:98)rt = 0.log (1 âˆ’ p (rt|d)).
(2).
3.4 event records extraction.
since a document is likely to express multiple eventrecords and the number of records cannot be knownin advance, we decode records by expanding atree orderly as previous methods did (zheng et al.,2019).
however, they treat each record indepen-dently.
instead, to incorporate the interdependencyamong event records, we propose a tracker mod-ule, which improves the model performance..to be self-contained, we introduce the orderedin each step,.
tree expanding in this paragraph..3536equityfreezeequityholderfrozesharesstartdatelegalinstitutionacdefhiequitypledgepledgerabkpledgeestartdatea     d     f      iglobal memorya      bcompleteduncompletedaâ€¦b     c     g     ja      kâ€¦trackervirtual nodevirtual nodebjcga     c     e     hefwe extract event records of a speciï¬c event type.
the arguments extraction order is predeï¬ned sothat the extraction is modeled as a constrained treeexpanding taskâ€ .
taking equity freeze records asan example, as shown in figure 3, we ï¬rstly extractequityholder, followed by frozeshares and others.
starting from a virtual root node, the tree expandsby predicting arguments in a sequential order.
asthere may exist multiple eligible entities for theevent argument role, the current node will expandseveral branches during extraction, with differententities assigned to the current role.
this branchingoperation is formulated as multi-label classiï¬cationtask.
in this way, each path from the root node tothe leaf node is identiï¬ed as a unique event record..interdependency exists extensively among dif-ferent event records.
for example, as shown infigure 1, an equity underweight event recordis closely related to an equity overweight eventrecord, and they may share some key argumentsor provide useful reasoning information.
to takeadvantage of such interdependency, we propose anovel tracker module inspired by memory network(weston et al., 2015).
intuitively, the tracker con-tinually tracks the extracted records on-the-ï¬‚y andstore the information into a global memory.
whenpredicting arguments for current record, the modelwill query the global memory and therefore makeuse of useful interdependency information of otherrecords..in detail, for the i-th record path consisting ofa sequence of entities, the tracker encodes thecorresponding entity representation sequence ui =[ei1, ei2, ...] into an vector gi with an lstm (lasthidden state) and add event type embedding.
thenthe compressed record information is stored in theglobal memory g, which is shared across differentevent types as shown in figure 3. for extraction,given a record path ui âˆˆ rdmÃ—(jâˆ’1) with the ï¬rstj âˆ’ 1 arguments roles, we predict the j-th roleby injecting role-speciï¬c information into entityrepresentations, e = e + rolej , where rolejis the role embedding for the j-th role.
then weconcatenate e, sentences feature s, current entitiespath ui, and the global memory g, followed bya transformer to obtain new entity feature matrix(cid:101)e âˆˆ rdmÃ—|e|, which contains global role-speciï¬c.
information for all entity candidates.â€¡.
[ (cid:101)e, (cid:101)s, (cid:101)ui, (cid:101)g] = transformer([e; s; ui; g]).
we treat the path expansion as a multi-label clas-siï¬cation problem with a binary classiï¬er over (cid:101)ei,i.e., predicts whether the i-th entity is the next ar-gument role for the current record and expand thepath accordingly as shown in figure 3..during training, we minimize the following loss:.
lrecord = âˆ’.
log p (yn.
t |n).
(3).
(cid:88).
|e|(cid:88).
nâˆˆnd.
t=1.
where nd denotes the nodes set in the eventrecords tree, and ynif thett-th entity is validate for the next argument in noden, then yn.
is the golden label..t = 1, otherwise yn.
t = 0..3.5 training.
we sum the losses coming from three sub-taskswith different weight respectively in eq.
(1), (2)and (3) as follows:.
lall = Î»1lner + Î»2ldetect + Î»3lrecord.
more training details are shown in appendix a..4 experiments.
4.1 dataset.
we evaluate our model on a public dataset proposedby zheng et al.
(2019)Â§, which is constructed fromchinese ï¬nancial documents.
it consists of up to32, 040 documents which is the largest document-level ee dataset by far.
it focuses on ï¬ve eventtypes: equity freeze (ef), equity repurchase (er),equity underweight (eu), equity overweight (eo)and equity pledge (ep), with 35 different kinds ofargument roles in total.
we follow the standard splitof the dataset, 25, 632/3, 204/3, 204 documentsfor training/dev/test set.
the dataset is quite chal-lenging, as a document has 20 sentences and con-sists of 912 tokens on average.
besides, there areroughly 6 sentences involved for an event record,and 29% documents express multiple events..â€¡to distinguish different parts in the concatenated vector,we also add segment embedding, which is omitted in eq.
3.4..Â§https://github.com/dolphin-zs/.
â€ we simply adopt the order used by zheng et al.
(2019)..doc2edag/blob/master/data.zip.
3537model.
ef.
er.
eu eo ep overall.
model.
i.ii.
iii.
iv.
46.7dcfee-sdcfee-m 42.7greedy-dec57.7doc2edag 71.0.
80.073.379.488.4.
47.545.851.269.8.
46.744.650.073.5.
56.153.854.274.8.git (ours).
73.4.
90.8.
74.3.
76.3.
77.7.
60.356.661.077.5.
80.3.table 1: f1 scores on test set.
git achieves the bestperformance.
we also list the results reported in zhenget al.
(2019) in appendix b, and git consistently out-performs other baselines.
ef/er/eu/eo/ep refer tospeciï¬c event types, and overall denotes micro f1..4.2 experiments setting.
in our implementation of git, we use 8 and 4 lay-ers transformer (vaswani et al., 2017) in encodingand decoding module respectively.
the dimensionsin hidden layers and feed-forward layers are thesame as previous work (zheng et al., 2019), i.e.,768 and 1, 024. we also use l = 3 layers of gcn,and set dropout rate to 0.1, batch size to 64. git istrained using adam (kingma and ba, 2015) as opti-mizer with 1e âˆ’ 4 learning rate for 100 epochs.
weset Î»1 = 0.05, Î»2 = Î»3 = 1 for the loss function..4.3 baselines and metrics.
yang et al.
(2018) proposes dcfee that extractsarguments from the identiï¬ed central sentence andqueries surrounding sentences for missing argu-ments.
the model has two variants, dcfee-s anddcfee-m. dcfee-s produces one record at atime, while dcfee-m produces multiple possi-ble argument combinations by the closest distancefrom the central sentence.
besides, doc2edag(zheng et al., 2019) uses transformer encoder to ob-tain sentence and entity embeddings, followed byanother transformer to fuse cross-sentence context.
then multiple events are extracted simultaneously.
greedy-dec is a variant of doc2edag, which pro-duces only one record greedily..three sub-tasks of the document-level ee areall evaluated by f1 score.
due to limited space,we leave the results of entity extraction and eventtypes detection in appendix b, which shows gitonly slightly outperform doc2edag, because wemainly focus on event record extraction and themethods are similar to doc2edag for these twosub-tasks.
in the following, we mainly report andanalyze the results of event record extraction..dcfee-s64.6dcfee-m 54.8greedy-dec67.4doc2edag 79.6.
70.054.168.082.4.
57.751.560.878.4.
52.347.150.272.0.git (ours).
81.9.
85.7.
80.0.
75.7.table 2: f1 scores on four sets with growing averagenumber of involved sentences for records (increasesfrom i to iv).
the highest improvement of git comesfrom event records involving the most sentences (setiv) by 3.7 f1 score compared with doc2edag..4.4 main results.
overall performance.
the results of the overallperformance on the document-level ee dataset isillustrated in table 1. as table 1 shows, our gitconsistently outperforms other baselines, thanksto better modelling of global interactions and in-terdependency.
speciï¬cally, git improves 2.8 mi-cro f1 compared with the previous state-of-the-art,doc2edag, especially 4.5 improvement in equityunderweight (eu) event type..cross-sentence records scenario.
there aremore than 99.5% records of the test set are cross-sentence event records, and the extraction becomesgradually more difï¬cult as the number of their in-volved sentences grows.
to veriï¬es the effective-ness of git to capture cross-sentence information,we ï¬rst calculate the average number of sentencesthat the records involve for each document, and sortthem in ascending order.
then we divide them intofour sets i/ii/iii/iv with equal size.
documents inset.
iv is considered to be the most challengingas it requires the most number of sentences to suc-cessfully extract records.
as table 2 shows, gitconsistently outperforms doc2edag, especiallyon the most challenging set.
iv that involves themost sentences, by 3.7 f1 score.
it suggests thatgit can well capture global context and mitigatethe arguments-scattering challenge, with the helpof the heterogeneous graph interaction network..multiple records scenario.
git introduces thetracker to make use of global interdependencyamong event records, which is important in mul-tiple records scenario.
to illustrate its effective-ness, we divide the test set into single-record set(s.) containing documents with one record, andmulti-record set (m.) containing those with multi-ple records.
as shown in table.
3, f1 score on m..3538model.
ef.
er.
eu.
eo.
ep.
overall.
s. m..s. m..s. m..s. m..s. m..s. m..dcfee-s55.7dcfee-m 45.3greedy-dec74.0doc2edag 79.7.
38.140.540.763.3.
83.076.182.290.4.
55.550.650.070.7.
52.348.361.574.7.
41.443.135.663.3.
49.245.763.476.1.
43.643.329.470.2.
62.458.178.684.3.
52.251.236.569.3.
69.063.277.881.0.
50.349.437.067.4.git (ours).
81.9.
65.9.
93.0.
71.7.
82.0.
64.1.
80.9.
70.6.
85.0.
73.5.
87.6.
72.3.table 3: f1 scores on single-record (s.) and multi-record (m.) sets..model.
git- s-s- s-m- m-mintra- m-minter- graph.
f1.
80.3-1.4-1.0-1.3-1.1-2.0.i.ii.
iii.
iv.
81.9-0.9-1.6-0.5-0.5-1.8.
85.7-0.1-1.7-1.4-1.6-1.5.
80.0-1.9-0.7-2.4-1.4-2.0.
75.7-2.3-0.7-1.5-1.7-2.5.table 4: the decrease of f1 scores on ablation studyfor gitâ€™s heterogeneous graph interaction network.
re-moving the heterogeneous graph leads to signiï¬cantdrop on f1, especially for records involving the mostsentences (i.e., âˆ’2.5 f1 on set iv)..model.
p.r.f1.
s. m..git.
82.3git-ot -0.6-1.0git-opgit-nt -2.8.
78.4-0.4-1.6+0.1.
80.3-0.5-1.2-1.3.
87.6-0.8-1.0-1.3.
72.3-0.7-1.5-1.5.table 5: performance of git on ablation study for thet racker module.
the removal of the tracker (git-nt) brings about higher f1 decrease on m. than thaton s.. s.: single-record set, m.: multi-record set..is much lower than that on s., indicating it is chal-lenging to extract multiple records.
however, gitstill surpasses other strong baselines by 4.9 âˆ¼ 35.3on multi-record set (m.).
this is because git isaware of other records through the t racker mod-ule, and leverage the interdependency informationto improve the performanceÂ¶..Â¶nguyen et al.
(2016) maintain three binary matrices tomemorize entities and events states.
although they aimat sentence-level ee that contains fewer entities and eventrecords, it would be also interesting to compare with them andwe leave it as future work..figure 4: f1 scores on documents with different num-ber of event records.
the f1 gap between w/ (git) andw/o tracker (git-nt) becomes wider as the number ofevent records of documents increases..4.5 analysis.
we conduct further experiments to analyze the keymodules in git more deeply..on the effect of heterogeneous graph interac-tion network.
the heterogeneous graph we con-structed contains four types of edges.
to exploretheir functions, we remove one type of edges at atime, and remove the whole graph network ï¬nally.
results are shown in table 4, including micro f1and f1 on the four sets, which are divided by thenumber of involved sentences for records as we didbefore.
the micro f1 would decreases 1.0 âˆ¼ 1.4without a certainty type of edge.
besides, removingthe whole graph causes an signiï¬cant drop by 2.0f1, especially for set iv by 2.5, which requiresthe most number of sentences to extract the eventrecord.
it demonstrates that the graph interactionnetwork helps improve the performance, especiallyon records involving many sentences, and all kindsof edges play an important role for extraction..on the effect of tracker module.
git canleverage interdependency among records based onthe information of other event records tracked bytracker.
to explore its effect, ï¬rstly, we removethe global interdependency information betweenrecords of different event types, by clearing theglobal memory whenever we extract events for an-.
35396365676971732 - 34 - 5>=6f1 scorethe number of records of documentsgitgit-otgit-ntdoc2edagfigure 5: the case study of our proposed git and doc2edag, with their key prediction difference colored inred.
related entities are colored in blue.
git successfully extract totalholdingshares and totalpledgedshares forrecord 2, while doc2edag fails.
the complete content are provided in appendix c..other new event type (git-own type).
next, weremove all the tracking information except the ownpath for a record, to explore whether the trackingof other records makes effect indeed (git-ownpath).
finally, we remove the whole tracker mod-ule (git-no tracker).
as table 5 shows, the f1in git-ot/git-op decreases by 0.5/1.2, suggest-ing the interdependency among records of both thesame and different event types do play an essentialrole.
besides, their f1 decrease in m. by 0.7/1.5 aremore than those in s. by 0.8/1.0, verifying the ef-fectiveness of the tracker in multi-event scenarios.
moreover, the performances are similar betweengit-op and git-nt, which also provides evidencethat other records do help.
we also reveal f1 ondocuments with different number of records in fig-ure 4. the gap between models with or withouttracker raises as the number of records increases,which validates the effectiveness of our tracker..4.6 case study.
figure 5 demonstrates a case of the predictions ofdoc2edag and git for equity pledge (ep) eventtypes.
the totalholdingshares and totalpledged-shares information lies in sentence 8, while thepledgedshares and pledgee information for record2 lies in sentence 5. though doc2edag fails toextract these arguments in record 2 (colored inred), git succeeds because it can capture interac-tions between long-distance sentences, and utilizethe information of record 1 (325.4 million and218.6 million) thanks to the tracker model..5 related work.
sentence-level event extraction.
previous ap-proaches mainly focus on sentence-level event.
extraction.
chen et al.
(2015) propose a neuralpipeline model that identiï¬es triggers ï¬rst and thenextracts argument roles.
nguyen et al.
(2016) use ajoint model to extract triggers and argument rolessimultaneously.
some studies also utilize depen-dency tree information (liu et al., 2018; yan et al.,2019).
to utilize more knowledge, some studiesleverage document context (chen et al., 2018; zhaoet al., 2018), pre-trained language model (yanget al., 2019), and explicit external knowledge (liuet al., 2019a; tong et al., 2020) such as wordnet(miller, 1995).
du and cardie (2020b) also try toextract events in a question-answer way.
thesestudies usually conduct experiments on sentence-level event extraction dataset, ace05 (walker et al.,2006).
however, it is hard for the sentence-levelmodels to extract multiple qualiï¬ed events span-ning across sentences, which is more common inreal-world scenarios..document-level event extraction.
document-level ee has attracted more and more attention re-cently.
yang and mitchell (2016) use well-deï¬nedfeatures to handle the event-argument relationsacross sentences, which is, unfortunately, quitenontrivial.
yang et al.
(2018) extract events froma central sentence and ï¬nd other arguments fromneighboring sentences separately.
although zhenget al.
(2019) use transformer to fuse sentencesand entities, interdependency among events is ne-glected.
du and cardie (2020a) try to encode thesentences in a multi-granularity way and du et al.
(2020) leverage a seq2seq model.
they conductexperiments on muc-4 (sundheim, 1992) datasetwith 1, 700 documents and 5 kinds of entity-basedarguments, and it is formulated as a table-ï¬llingtask, coping with single event record of single event.
3540â€¦ [5] the shareholder of the company, quanliechen,pledged 52.4 millionto gdzq  co., ltd.in 2018, and supplemented the pledge recently because of the decline of the share price.
â€¦ [7]since the borrowings have been paid off, quanliechencompleted the pledge cancellation procedures of 35.5 millionthat were pledged to gtja co., ltd.on nov 7, 2018.
[8] as of today, quanliechenholds a total of 325.4 millionof the company, and there are still 218.6 millionin pledge status.
â€¦quanliechenquanliechenpledger   pledgedsharespledgee   totalholdingsharestotalpledgedshares35.5 milliongtja  co., ltd.325.4 million218.6 million52.4 milliongdzq co., ltd.nullnullâ€¦â€¦â€¦doc2edagquanliechenquanliechenpledger   pledgedsharespledgee   totalholdingsharestotalpledgedshares35.5 milliongtja co., ltd.325.4 million218.6 million52.4 milliongdzq co., ltd.325.4 million218.6 millionâ€¦â€¦â€¦gitno.12no.12type.
however, our work is different from thesestudies in that a) we utilize heterogeneous graph tomodel the global interactions among sentences andmentions to capture cross-sentence context, b) andwe leverage the global interdependency throughtracker to extract multiple event records of multi-ple event types..6 conclusion.
although promising in practical application,document-level ee still faces some challenges suchas arguments-scattering phenomenon and multi-ple correlated events expressed by a single docu-ment.
to tackle the challenges, we introduce het-erogeneous graph-based interaction model witha tracker (git).
git uses a heterogeneous graphinteraction network to model global interactionsamong sentences and entity mentions.
git alsouses a tracker to track the extracted records toconsider global interdependency during extraction.
experiments on large-scale public dataset (zhenget al., 2019) show git outperforms previous state-of-the-art by 2.8 f1.
further analysis veriï¬es theeffectiveness of git especially in cross-sentenceevents extraction and multi-event scenarios..acknowledgments.
the authors would like to thank changzhi sun,mingxuan wang, and the anonymous review-ers for their thoughtful and constructive com-ments.
this paper is supported in part by the na-tional key r&d program of china under grandno.2018aaa0102003, the national science foun-dation of china under grant no.61936012 and61876004..references.
samy bengio, oriol vinyals, navdeep jaitly, andnoam shazeer.
2015. scheduled sampling for se-quence prediction with recurrent neural networks.
in proceedings of the 28th international confer-ence on neural information processing systems(neurips)..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (acl-ijcnlp)..yubo chen, hang yang, kang liu, jun zhao, and yan-tao jia.
2018. collective event detection via a hier-.
archical and bias tagging networks with gated multi-in proceedings of thelevel attention mechanisms.
2018 conference on empirical methods in naturallanguage processing (emnlp)..xinya du and claire cardie.
2020a.
document-levelevent role ï¬ller extraction using multi-granularitycontextualized encoding.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics (acl)..xinya du and claire cardie.
2020b.
event extractionin pro-by answering (almost) natural questions.
ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp)..xinya du, alexander m. rush, and claire cardie.
2020. document-level event-based extraction us-ing generative template-ï¬lling transformers.
arxivpreprint arxiv:2008.09249..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations(iclr)..thomas n. kipf and max welling.
2017..semi-supervised classiï¬cation with graph convolutionalnetworks.
in 5th international conference on learn-ing representations (iclr)..jian liu, yubo chen, and kang liu.
2019a.
exploit-ing the ground-truth: an adversarial imitation basedknowledge distillation approach for event detection.
in proceedings of the 33rd aaai conference on ar-tiï¬cial intelligence (aaai)..shulin liu, yang li, feng zhang, tao yang, and xin-peng zhou.
2019b.
event detection without triggers.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..xiao liu, zhunchen luo, and heyan huang.
2018.jointly multiple events extraction via attention-in proceed-based graph information aggregation.
ings of the 2018 conference on empirical methodsin natural language processing (emnlp)..george a. miller.
1995. wordnet: a lexical database.
for english.
commun.
acm..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentneural networks.
in proceedings of the 2016 confer-ence of the north american chapter of the associa-tion for computational linguistics (naacl)..adam paszke, sam gross, soumith chintala, gregorychanan, edward yang, zachary devito, zeminglin, alban desmaison, luca antiga, and adamlerer.
2017. automatic differentiation in pytorch.
in nips-w..3541beth m. sundheim.
1992. overview of the fourth mes-sage understanding evaluation and conference.
infourth message uunderstanding conference (muc-4)..meihan tong, bin xu, shuai wang, yixin cao, leihou, juanzi li, and jun xie.
2020. improving eventdetection via open-domain trigger knowledge.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics (acl)..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, Å‚ ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems (neurips)..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilingualin philadelphia: linguistic datatraining corpus.
consortium..minjie wang, lingfan yu, da zheng, quan gan,yu gai, zihao ye, mufei li, jinjing zhou, qi huang,chao ma, ziyue huang, qipeng guo, hao zhang,haibin lin, junbo zhao, jinyang li, alexander jsmola, and zheng zhang.
2019. deep graph li-brary: towards efï¬cient and scalable deep learn-iclr workshop on representationing on graphs.
learning on graphs and manifolds..jason weston, sumit chopra, and antoine bordes.
2015. memory networks.
in 3rd international con-ference on learning representations (iclr)..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated atten-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp)..bishan yang and tom m. mitchell.
2016. joint extrac-tion of events and entities within a document context.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics (naacl)..hang yang, yubo chen, kang liu, yang xiao, and junzhao.
2018. dcfee: a document-level chinese ï¬-nancial event extraction system based on automati-in proceedings of thecally labeled training data.
56th annual meeting of the association for compu-tational linguistics (acl)..sen yang, dawei feng, linbo qiao, zhigang kan,and dongsheng li.
2019. exploring pre-trained lan-guage models for event extraction and generation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics (acl)..conference on empirical methods in natural lan-guage processing (emnlp).
association for com-putational linguistics..ying zeng, yansong feng, rong ma, zheng wang, ruiyan, chongde shi, and dongyan zhao.
2018. scaleup event extraction learning via automatic train-in proceedings of the thirty-ing data generation.
second aaai conference on artiï¬cial intelligence(aaai)..yue zhao, xiaolong jin, yuanzhuo wang, and xueqicheng.
2018. document embedding enhanced eventdetection with hierarchical and supervised attention.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (acl)..shun zheng, wei cao, wei xu, and jiang bian.
2019.doc2edag: an end-to-end document-level frame-work for chinese ï¬nancial event extraction.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp)..a training details.
to mitigate the error propagation due to the gapbetween training and inference phrase (i.e., theextracted entities are ground truth during trainingbut predicted results during inference), we adoptscheduled sampling strategy (bengio et al., 2015)as zheng et al.
(2019) did.
we gradually switchthe entity extraction results from golden label towhat the model predicts on its own.
speciï¬cally,from epoch 10 to epoch 20, we linearly increase theproportion of predicted entity results from 0% to100%.
we implement git under pytorch (paszkeet al., 2017) and dgl (wang et al., 2019) based oncodes provided by zheng et al.
(2019)..all the experiments (including the baselines) arerun with the same 8 tesla-v100 gpus and thesame version of python dependencies to ensure thefairness..hyperparameters trials are listed in table 6. thevalue of hyperparameters we ï¬nally adopted arein bold.
note that we do not tune all the hyperpa-rameters, and make little effort to select the besthyperparameters for our git..we choose the ï¬nal checkpoints for test accord-ing to the micro f1 performance on the dev set.
ta-ble 9 illustrates the best epoch in which the modelachieves the highest micro f1 on the dev set andtheir according f1 score..b additional evaluation results.
shuang zeng, runxin xu, baobao chang, and lei li.
2020. double graph based reasoning for document-level relation extraction.
in proceedings of the 2020.we have showed the evaluation results of eventrecords extraction in the paper for document-level.
3542figure 6: the original complete document corresponding to the case study in figure 5. sentences in red color arepresented in figure 5..event extraction.
in this section, we also illustatethe results of entity extraction in table.
7 and eventtypes detection in table.
8. moreover, the compre-hensive results of event record extraction is shownin table.
10, including results reported in zhenget al.
(2019) with precison, recall and f1 score..c complete document for the examples.
we show an example document in figure 1 in thepaper.
to better illustrate, we translate it fromchinese into english and make some simplication.
here we present the original complete documentexample in figure 7. for the speciï¬c meanings ofargument roles, we recommend readers to refer to(zheng et al., 2019)..we also demonstrate an case study in figure 5 inthe paper.
now we also show its original chineseversion in figure 6..hyperparametersbatch sizelearning ratedropoutlayers of gcnnumber of epochsÎ»1Î»2Î»3gradient accumulation stepslayers of transformer in entity extractorlayers of transformer in decoder modulehyperparameter search trials.
value32, 640.00010.11, 2, 3, 4, 51000.051.001.0088410.table 6: hyperparameters for our proposed git..pmodel86.5dcfee-sdcfee-m 86.6greedy-dec87.5doc2edag 88.085.8git (ours).
r88.689.089.890.092.6.f187.687.888.689.089.1.table 7: results of entity extraction sub-task on thetest set.
the performance of different models are simi-lar, for the reason that they all utilize the same structureand methods to extract entities..3543[1] è¯åˆ¸ä»£ç ï¼š002102è¯åˆ¸ç®€ç§°ï¼šå† ç¦è‚¡ä»½ç¼–å·ï¼š2018-112ã€‚[2] å† ç¦æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸å…³äºå¤§è‚¡ä¸œé™ˆçƒˆæƒå…ˆç”Ÿéƒ¨åˆ†è‚¡ä»½è¡¥å……è´¨æŠ¼åŠè§£é™¤è´¨æŠ¼çš„å…¬å‘Šã€‚[3]æœ¬å…¬å¸åŠè‘£äº‹ä¼šå…¨ä½“æˆå‘˜ä¿è¯ä¿¡æ¯æŠ«éœ²çš„å†…å®¹çœŸå®ã€å‡†ç¡®ã€å®Œæ•´ï¼Œæ²¡æœ‰è™šå‡è®°è½½ã€è¯¯å¯¼æ€§é™ˆè¿°æˆ–è€…é‡å¤§é—æ¼ã€‚[4]å† ç¦æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°â€œå…¬å¸â€ï¼‰è¿‘æ—¥æ¥åˆ°å…¬å¸å¤§è‚¡ä¸œé™ˆçƒˆæƒå…ˆç”Ÿå‡½å‘Šï¼Œè·æ‚‰å…¶å°†æŒæœ‰çš„å…¬å¸éƒ¨åˆ†è‚¡ä»½åŠç†äº†è¡¥å……è´¨æŠ¼åŠè§£æŠ¼ï¼Œå…·ä½“æƒ…å†µå¦‚ä¸‹ã€‚[5]ä¸€ã€æœ¬æ¬¡è‚¡ä»½è¡¥å……è´¨æŠ¼æƒ…å†µã€‚å…¬å¸å¤§è‚¡ä¸œé™ˆçƒˆæƒå…ˆç”ŸåŸäº2017å¹´10æœˆ24æ—¥è´¨æŠ¼ç»™å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°â€œå›½æ³°å›å®‰â€ï¼‰çš„å…¬å¸è‚¡ä»½69200000è‚¡ã€2018å¹´2æœˆ8æ—¥è´¨æŠ¼ç»™ä¸­ä¿¡å»ºæŠ•è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°â€œä¸­ä¿¡å»ºæŠ•â€ï¼‰çš„å…¬å¸è‚¡ä»½52000000è‚¡ã€2018å¹´2æœˆ26æ—¥è´¨æŠ¼ç»™å›½éƒ½è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°â€œå›½éƒ½è¯åˆ¸â€ï¼‰çš„å…¬å¸è‚¡ä»½52369050è‚¡ï¼Œå› å…¬å¸è¿‘æ—¥è‚¡ä»·ä¸‹è·Œï¼Œåˆ†åˆ«å¯¹å›½å›è¯åˆ¸ã€ä¸­ä¿¡å»ºæŠ•åŠå›½éƒ½è¯åˆ¸è¿›è¡Œè¡¥å……è´¨æŠ¼ã€‚[6] ä¸Šè¿°åŸæœ‰è´¨æŠ¼æƒ…å†µè¯¦è§å…¬å¸åˆ†åˆ«äº2017å¹´10æœˆ27æ—¥ã€2018å¹´2æœˆ12æ—¥ã€3æœˆ1æ—¥åœ¨ã€Šè¯åˆ¸æ—¶æŠ¥ã€‹ã€ã€Šä¸­å›½è¯åˆ¸æŠ¥ã€‹ã€ã€Šä¸Šæµ·è¯åˆ¸æŠ¥ã€‹å’Œã€Šè¯åˆ¸æ—¥æŠ¥ã€‹åŠå·¨æ½®èµ„è®¯ç½‘ä¸ŠæŠ«éœ²çš„ã€Šå† ç¦æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸å…³äºå¤§è‚¡ä¸œé™ˆçƒˆæƒå…ˆç”Ÿéƒ¨åˆ†è‚¡ä»½è´¨æŠ¼åŠè§£é™¤è´¨æŠ¼çš„å…¬å‘Šã€‹ï¼ˆå…¬å‘Šç¼–å·ï¼š2017-108ï¼‰ã€ã€Šå† ç¦æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸å…³äºå¤§è‚¡ä¸œé™ˆçƒˆæƒå…ˆç”Ÿéƒ¨åˆ†è‚¡ä»½è§£é™¤è´¨æŠ¼åŠå†è´¨æŠ¼çš„å…¬å‘Šã€‹ï¼ˆå…¬å‘Šç¼–å·ï¼š2018-010ã€2018-013ï¼‰ã€‚[7]äºŒã€æœ¬æ¬¡è‚¡ä»½è§£é™¤è´¨æŠ¼æƒ…å†µã€‚é™ˆçƒˆæƒå…ˆç”ŸåŸè´¨æŠ¼ç»™å›½æ³°å›å®‰çš„å…¬å¸è‚¡ä»½35500000è‚¡ï¼ˆå å…¬å¸æ€»è‚¡æœ¬çš„1.35%ï¼‰ï¼Œå› å·²è¿˜æ¸…å›½æ³°å›å®‰çš„å€Ÿæ¬¾ï¼Œåˆ†åˆ«äº2018å¹´9æœˆ7æ—¥ã€9æœˆ10æ—¥åœ¨å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è†å·ä¾¿æ²³ä¸œè·¯è¥ä¸šéƒ¨åŠç†å®Œæˆè´¨æŠ¼è§£é™¤æ‰‹ç»­ã€‚[8] ä¸‰ã€ç´¯è®¡è´¨æŠ¼æƒ…å†µã€‚æˆªæ­¢æœ¬å…¬å‘Šæ—¥ï¼Œé™ˆçƒˆæƒå…ˆç”Ÿå…±æŒæœ‰å…¬å¸è‚¡ä»½325363822è‚¡ï¼Œå å…¬å¸æ€»è‚¡æœ¬çš„12.35%ï¼Œå…¶ä¸­å¤„äºè´¨æŠ¼çŠ¶æ€çš„è‚¡ä»½ç´¯è®¡æ•°ä¸º218569050è‚¡ï¼Œå å…¬å¸æ€»è‚¡æœ¬çš„8.30%ã€‚[9] å››ã€å¤‡æŸ¥æ–‡ä»¶[10] 1ã€ä¸­ä¿¡å»ºæŠ•è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è‚¡ç¥¨è´¨æŠ¼å¼å›è´­äº¤æ˜“ç”³è¯·ä¹¦ï¼ˆè¡¥å……äº¤æ˜“ï¼‰ï¼›[11] 2ã€å›½éƒ½è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è‚¡ç¥¨è´¨æŠ¼å¼å›è´­äº¤æ˜“è¡¥å……è´¨æŠ¼å·²è¾¾æˆé€šçŸ¥ï¼›[12] 3ã€å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è‚¡ç¥¨è´¨æŠ¼å¼å›è´­äº¤æ˜“åè®®ä¹¦ã€‚[13] ç‰¹æ­¤å…¬å‘Šã€‚[14] å† ç¦æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸è‘£äº‹ä¼š[15] äºŒâ—‹ä¸€å…«å¹´ä¹æœˆåäºŒæ—¥efmodeldcfee-s81.5dcfee-m 79.899.3greedy-decdoc2edag 99.098.8git (ours).
er94.092.499.999.899.8.eu eo ep overall82.378.996.896.897.9.
85.784.295.494.196.6.
93.892.999.699.599.6.
91.490.099.098.999.2.table 8: f1 scores results of event types detection sub-task on the test set.
all the models obtains more than 90.0micro f1 score.
git slightly outperform doc2edag..modeldcfee-sdcfee-mgreedy-decdoc2edaggit (ours).
best epoch ef51.352.557.575.278.3.
8687908989.er73.069.176.085.287.6.eu eo ep overall44.143.955.171.674.7.
51.447.249.380.080.9.
58.655.957.077.979.8.
58.755.859.178.780.7.table 9: the best epoch in which the models achieve the highest micro f1 score on the dev set and the corre-sponding performance..3544-.
-.
-.
-.
-.
-.
-.
-.
1f.r.llarevo.p.-.
-.
-.
-.
3.
..06.
6.
..65.
0.
..16.
5.
..77.
..308.
..445.
..255.
..194.
..057.
4.
..87.
..776.
..185.
..408.
..308.
3.
..28.
1f...936.
..926.
1.
..26.
..377.
..165.
..835.
..245.
..847.
7.
..77.pe.r.6.
..36.
4.
..66.
7.
..84.
8.
..47.
8.
..94.
4.
..25.
4.
..04.
0.
..37.
9.
..67.p.3.
..46.
8.
..95.
7.
..58.
0.
..08.
2.
..46.
3.
..55.
1.
..28.
7.
..67.
6.
..87.
1f.6.
..64.
9.
..44.
..315.
0.
..57.
7.
..64.
6.
..44.
0.
..05.
5.
..37.
..367.oe.r.6.
..24.
5.
..74.
..604.
0.
..96.
5.
..64.
7.
..64.
6.
..04.
1.
..07.
..327.p.4.
15.
5.
24.
7.
96.
1.
28.
9.
64.
8.
24.
8.
46.
2.
77.
7.
08.
1f.3.
54.
2.
44.
2.
15.
8.
17.
5.
74.
8.
54.
2.
15.
8.
96.
3.
47.ue.r.4.
53.
9.
93.
8.
04.
0.
56.
0.
93.
4.
14.
7.
04.
6.
16.
6.
66.p.7.
26.
5.
94.
7.
86.
2.
08.
8.
06.
4.
15.
0.
96.
4.
08.
9.
38.
1f.1.
38.
8.
08.
9.
87.
3.
78.
0.
08.
3.
37.
4.
97.
4.
88.
8.
09.re.r.8.
18.
0.
87.
9.
47.
6.
38.
0.
67.
5.
17.
3.
57.
8.
68.
2.
98.p.5.
48.
7.
38.
3.
38.
3.
19.
5.
48.
2.
57.
9.
38.
0.
09.
3.
29.
1f.1.
15.
6.
54.
9.
85.
2.
07.
7.
64.
7.
24.
7.
75.
0.
17.
4.
37.fe.r.6..14.
7..04.
8..64.
5..46.
8..73.
9..04.
6..54.
7..46.
5..86.p...066.
..815.
..597.
..177.
..116.
..644.
..587.
..787.
..987.
â™¦s-eefcd.â™¦m-eefcd.â™¦ced-ydeerg.â™¦gade2cod.â™ s-eefcd.â™ m-eefcd.â™ ced-ydeerg.â™ gade2cod.â™ )sruo(.
ti.g.ledom.table 10: comprehensive results of event record extraction.
results with â™¦ are results reported in zheng et al.
(2019).
results with are â™  results we implement on our own.
our git consistently outperform other baselines..3545figure 7: the original complete document corresponding to the running example in figure 1. sentences in redcolor are presented in figure 1..3546[1] è¯åˆ¸ä»£ç ï¼š300126 è¯åˆ¸ç®€ç§°ï¼šé”å¥‡è‚¡ä»½å…¬å‘Šç¼–å·: 2014-075ã€‚[2] ä¸Šæµ·é”å¥‡å·¥å…·è‚¡ä»½æœ‰é™å…¬å¸å…³äºæ§è‚¡è‚¡ä¸œè‚¡ä»½å‡æŒè®¡åˆ’å®æ–½è¿›å±•çš„å…¬å‘Šã€‚[3] æœ¬å…¬å¸åŠè‘£äº‹ä¼šå…¨ä½“æˆå‘˜ä¿è¯ä¿¡æ¯æŠ«éœ²çš„å†…å®¹çœŸå®ã€å‡†ç¡®ã€å®Œæ•´ï¼Œæ²¡æœ‰è™šå‡è®°è½½ã€è¯¯å¯¼æ€§é™ˆè¿°æˆ–è€…é‡å¤§é—æ¼ã€‚[4] ä¸Šæµ·é”å¥‡å·¥å…·è‚¡ä»½æœ‰é™å…¬å¸(ä»¥ä¸‹ç®€ç§°â€å…¬å¸â€)äº2014å¹´11æœˆ1æ—¥åœ¨ä¸­å›½è¯åˆ¸ç›‘ç£ç®¡ç†å§”å‘˜ä¼šæŒ‡å®šçš„åˆ›ä¸šæ¿ä¿¡æ¯æŠ«éœ²ç½‘ç«™æŠ«éœ²äº†ã€Šå…³äºæ§è‚¡è‚¡ä¸œè‚¡ä»½å‡æŒè®¡åˆ’çš„å…¬å‘Šã€‹(å…¬å‘Šç¼–å·2014-074)ã€‚[5] å…¬å¸äº2014å¹´11æœˆ6æ—¥æ¥åˆ°å…¬å¸æ§è‚¡è‚¡ä¸œå´æ˜å…å…ˆç”Ÿçš„ã€Šè‚¡ä»½å‡æŒå‘ŠçŸ¥å‡½ã€‹ã€‚[6] å´æ˜å…å…ˆç”Ÿäº2014å¹´11æœˆ6æ—¥é€šè¿‡æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€å¤§å®—äº¤æ˜“æ–¹å¼å‡æŒäº†å…¶ç›´æ¥æŒæœ‰çš„å…¬å¸æ— é™å”®æ¡ä»¶æµé€šè‚¡7200000è‚¡ï¼Œå å…¬å¸ç›®å‰æ€»è‚¡æœ¬çš„2.34%ã€‚[7] ä¸€ã€è‚¡ä¸œå‡æŒæƒ…å†µã€‚å´æ˜å…å…ˆç”Ÿæœ¬æ¬¡å‡æŒçš„å…¬å¸è‚¡ä»½7200000è‚¡ä¸ºå…¶ç›´æ¥æŒæœ‰çš„å…¬å¸æ— é™å”®æ¡ä»¶æµé€šè‚¡ï¼Œå å…¬å¸æ€»è‚¡æœ¬çš„2.34%ï¼Œæœ¬æ¬¡å‡æŒçš„å…¬å¸è‚¡ä»½å…¨éƒ¨è½¬è®©ç»™å´æ™“å©·å¥³å£«ã€‚[8]å´æ™“å©·å¥³å£«ä¸ºå´æ˜å…å…ˆç”Ÿçš„å¥³å„¿ï¼Œä¸¤äººä¸ºçˆ¶å¥³å…³ç³»ï¼Œæ ¹æ®ç›¸å…³è§„å®šè¢«è®¤å®šä¸ºä¸€è‡´è¡ŒåŠ¨äººã€‚[9]äºŒã€å…¶ä»–ç›¸å…³è¯´æ˜ã€‚1ã€æœ¬æ¬¡å‡æŒæ²¡æœ‰è¿åã€Šæ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€åˆ›ä¸šæ¿è‚¡ç¥¨ä¸Šå¸‚è§„åˆ™ã€‹ã€ã€Šä¸Šå¸‚å…¬å¸è§£é™¤é™å”®å­˜é‡è‚¡ä»½è½¬è®©æŒ‡å¯¼æ„è§ã€‹ç­‰æœ‰å…³æ³•å¾‹æ³•è§„åŠå…¬å¸è§„ç« åˆ¶åº¦ã€‚[10] 2ã€æœ¬æ¬¡å‡æŒä¸å­˜åœ¨è¿åã€Šè¯åˆ¸æ³•ã€‹ã€ã€Šä¸Šå¸‚å…¬å¸æ”¶è´­ç®¡ç†åŠæ³•ã€‹ç­‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„ã€éƒ¨é—¨è§„ç« ã€è§„èŒƒæ€§æ–‡ä»¶å’Œæ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€ã€Šåˆ›ä¸šæ¿ä¿¡æ¯æŠ«éœ²ä¸šåŠ¡å¤‡å¿˜å½•ç¬¬18å·ï¼šæ§è‚¡è‚¡ä¸œã€å®é™…æ§åˆ¶äººè‚¡ä»½å‡æŒä¿¡æ¯æŠ«éœ²ã€‹ç­‰è§„å®šçš„æƒ…å†µã€‚[11] 3ã€æœ¬æ¬¡å‡æŒåï¼Œå´æ˜å…å…ˆç”Ÿç›´æ¥æŒæœ‰å…¬å¸æ€»è‚¡æœ¬çš„æ¯”ä¾‹ä¸‹é™ä¸º32.08%ï¼Œé€šè¿‡ä¸Šæµ·ç‘æµ¦æŠ•èµ„æœ‰é™å…¬å¸æŒæœ‰å…¬å¸æ€»è‚¡æœ¬çš„14.02%ï¼Œåˆè®¡æŒæœ‰å…¬å¸æ€»è‚¡æœ¬çš„46.82%ï¼Œä»ä¸ºå…¬å¸æ§è‚¡è‚¡ä¸œã€‚[12] 4ã€æœ¬æ¬¡å‡æŒåï¼Œå´æ˜å…ã€ä¸Šæµ·ç‘æµ¦æŠ•èµ„æœ‰é™å…¬å¸ã€åº”åª›ç³ã€å´æ™“ä¾ã€å´æ™“å©·ä½œä¸ºä¸€è‡´è¡ŒåŠ¨äººï¼Œå…¶æ‰€åˆè®¡æŒæœ‰çš„å…¬å¸è‚¡ä»½æƒç›Šå¹¶æœªå‡å°‘ï¼Œä»ä¸ºå…¬å¸æ€»è‚¡æœ¬çš„56.22%ã€‚[13]ä¸‰ã€å¤‡æŸ¥æ–‡ä»¶ã€‚[14] 1ã€å´æ˜å…å…ˆç”Ÿçš„ã€Šè‚¡ä»½å‡æŒå‘ŠçŸ¥å‡½ã€‹ã€‚[15] 2ï¼æ·±äº¤æ‰€è¦æ±‚çš„å…¶ä»–æ–‡ä»¶ã€‚[16]ä¸Šæµ·é”å¥‡å·¥å…·è‚¡ä»½æœ‰é™å…¬å¸è‘£äº‹ä¼šã€‚[17] 2014å¹´11æœˆ6æ—¥ã€‚equityholdertradedsharesstartdateenddatelaterholdingsharesaveragepriceå´æ˜å…720000è‚¡2014å¹´11æœˆ6æ—¥2014å¹´11æœˆ6æ—¥nullnullequityholdertradedsharesstartdateenddatelaterholdingsharesaveragepriceå´æ™“å©·720000è‚¡2014å¹´11æœˆ6æ—¥2014å¹´11æœˆ6æ—¥720000è‚¡nullequityunderweightequityoverweight