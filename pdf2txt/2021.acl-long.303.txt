abcd: a graph framework to convert complex sentencesto a covering set of simple sentences.
yanjun gao and ting-hao (kenneth) huang and rebecca j. passonneaupennsylvania state university{yug125,txh710,rjp49}@psu.edu.
abstract.
atomic clauses are fundamental text units forunderstanding complex sentences.
identify-ing the atomic sentences within complex sen-tences is important for applications such assummarization, argument mining, discourseanalysis, discourse parsing, and question an-swering.
previous work mainly relies on rule-based methods dependent on parsing.
we pro-pose a new task to decompose each complexsentence into simple sentences derived fromthe tensed clauses in the source, and a novelproblem formulation as a graph edit task.
ourneural model learns to accept, break, copy ordrop elements of a graph that combines wordadjacency and grammatical dependencies.
thefull processing pipeline includes modules forgraph construction, graph editing, and sen-tence generation from the output graph.
weintroduce desse, a new dataset designed totrain and evaluate complex sentence decompo-sition, and minwiki, a subset of minwikisplit.
abcd achieves comparable performance astwo parsing baselines on minwiki.
on desse,which has a more even balance of complex sen-tence types, our model achieves higher accu-racy on the number of atomic sentences thanan encoder-decoder baseline.
results includea detailed error analysis..1.introduction.
atomic clauses are fundamental text units forunderstanding complex sentences.
the ability todecompose complex sentences facilitates researchthat aims to identify, rank or relate distinct pred-ications, such as content selection in summariza-tion (fang et al., 2016; peyrard and eckle-kohler,2017), labeling argumentative discourse units inargument mining (jo et al., 2019) or elementarydiscourse units in discourse analysis (mann andthompson, 1986; burstein et al., 1998; demir et al.,2010), or extracting atomic propositions for ques-tion answering (pyatkin et al., 2020).
in this work,.
origss1ss2.
sokuhi was born in fujian and was ordained at 17.sokuhi was born in fujian.
sokuhi was ordained at 17..figure 1: example of a complex sentence (orig) rewrittenas two simple sentences (ss1, ss2).
underlined words in thesource are preserved in the same order in the two outputs, theconjunction and (red font) is dropped, and the subject sokuhi(blue font) is copied to the second simple sentence..we propose a new task to decompose complex sen-tences into a covering set of simple sentences, withone simple output sentence per tensed clause inthe source sentence.
we focus on tensed clausesrather than other constituents because they are syn-tactically and semantically more prominent, thusmore essential in downstream tasks like argumentmining, summarization, and question answering..the complex sentence decomposition task weaddress has some overlap with related nlp al-gorithms, but each falls short in one or more re-spects.
elementary discourse unit (edu) segmen-tation segments source sentences into a sequence ofnon-overlapping spans (carlson et al., 2003; wanget al., 2018).
the output edus, however, are not al-ways complete clauses.
text simpliﬁcation rewritescomplex sentences using simpler vocabulary andsyntax (zhang and lapata, 2017).
the output, how-ever, does not preserve every tensed clause in theoriginal sentence.
the split-and-rephrase (sprp)task aims to rewrite complex sentences into sets ofshorter sentences, where an output sentence canbe derived from non-clausal constituents in thesource (narayan et al., 2017).
in contrast to thepreceding methods, we convert each tensed clausein a source sentence, including each conjunct in aconjoined vp, into an independent simple sentence.
unlike edu segmentation, a belief verb and itsthat-complement do not lead to two output units.
unlike text simpliﬁcation, no propositions in thesource are omitted from the output.
unlike sprp, aphrase that lacks a tensed verb in the source cannot.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3919–3931august1–6,2021.©2021associationforcomputationallinguistics3919lead to a distinct sentence in the output..figure 1 shows an example complex sentence(orig) with conjoined verb phrases and its rewriteinto two simple sentences (sss).
observe thatbesides producing two sentences from one, thusbreaking the adjacency between words, words in-side the verb phrases (underlined in the ﬁgure) re-main in the same linear order in the output; thesingle subject sokuhi in the source is copied to themore distant verb phrase.
finally, the connectiveand is dropped.
we ﬁnd that most rewrites of com-plex sentences into simple sentences that preservethe one-to-one mapping of source tensed predica-tion with target simple sentence involve similaroperations.
building on these observations, we pro-pose a neural model that learns to accept, break,copy or drop elements of a special-purpose sen-tence graph that represents word adjacency andgrammatical dependencies, so the model can learnbased on both kinds of graph proximity.
we alsointroduce desse (decomposed sentences fromstudents essays), a new annotated dataset to sup-port our task..the rest of the paper presents two evaluationdatasets, our full pipeline, and our abcd model.
experimental results show that abcd achievescomparable or better performance than baselines.
1.
2 related work.
related work falls largely into parsing-basedmethods, neural models that rewrite, and neuralsegmenters.
gao et al.
(2019) propose a decompo-sition parser (dcp) that extracts vp constituentsand clauses from complex sentences as part ofa summarization evaluation tool.
niklaus et al.
(2019a) present a system (dissim) based on pars-ing to extract simple sentences from complex ones.
jo et al.
(2020) propose seven rules to extract com-plete propositions from parses of complex ques-tions and imperatives for argumentation mining.
though performance of these methods depends onparser quality, they often achieve very good perfor-mance.
we include two whose code is available(dcp, dissim) among our baselines..sprp models are based on encoder-decoderarchitectures, and the output is highly depend-ing on the training corpus.
aharoni and gold-berg (2018) present a copy-augmented network(copy512) based on (gu et al., 2016) that encour-.
1abcd is available at https://github.com/.
serenayj/abcd-acl2021..ages the model to copy most words from the origi-nal sentence to the output.
as it achieves improve-ment over an earlier encoder-decoder sprp model(narayan et al., 2017), we include copy512 amongour baselines..finally, recent neural edu segmenters (wanget al., 2018; li et al., 2018) achieve state-of-the-artperformance on a discourse relation corpus, rst-dt (carlson et al., 2003).
as they do not outputcomplete sentences, we do not include any amongour baselines..our abcd model leverages the detailed infor-mation captured by parsing methods, and the pow-erful representation learning of neural models.
aspart of a larger pipeline that converts input sen-tences to graphs, abcd learns to predict graphedits for a post processor to execute..3 datasets.
here we present desse, a corpus we collectedfor our task, and minwiki, a modiﬁcation of an ex-isting sprp corpus (minwikisplit (niklaus et al.,2019b)) to support our aims.
we also give a briefdescription of differences in their distributions.
neural models are heavily biased by the distribu-tions in their training data (niven and kao, 2019),and we show that desse has a more even balanceof linguistic phenomena..3.1 desse.
desse is collected in an undergraduate socialscience class, where students watched video clipsabout race relations, and wrote essays in a blogenvironment to share their opinions with the class.
it was created to support analysis of student writ-ing, so that different kinds of feedback mechanismscan be developed regarding sentence organization.
students have difﬁculty with revision to addresslack of clarity in their writing (kuhn et al., 2016),such as non-speciﬁc uses of connectives, run onsentences, repetitive statements and the like.
thesemake desse different from corpus with expertwritten text, such as wikipedia and newspaper.
theannotation process is unique in that it involves iden-tifying where to split a source complex sentenceinto distinct clauses, and how to rephrase each re-sulting segment as a semantically complete simplesentence, omitting any discourse connectives.
itdiffers from corpora that identify discourse unitswithin sentences, such as rst-dt (carlson et al.,2003) and ptdb (prasad et al., 2008), because.
3920• orig: (i believe that talking about race more in a civilway can only improve our society), || but i can see whyother people may have a different opinion..• rephrase 1: i believe that talking about race more in a.civil way can only improve our society..• rephrase 2: i can see why other people may have a.different opinion..figure 2: an original sentences from desse with an intra-sentential connective (but), a verb that takes a clausal argu-ment.
the annotation ﬁrst splits the sentence (at ||), thenrephrases each segment into a simple sentence, dropping theconnective..dataset.
minwikidesse.
disc.
vp- wh- & restric.
conn. conj.
rel.
cl.
rel.
cl.
26%34%.
58%66%.
36%22%.
10%32%.
that-comp.
0%24%.
table 1: prevalence of ﬁve linguistic phenomena in 50 ran-domly selected examples per dataset.
categories are not mu-tually exclusive..clauses are explicitly rewritten as simple sentences.
it differs from split-and-rephrase corpora such asminwikisplit, because of the focus in desse onrephrased simple sentences that have a one-to-onecorrespondence to tensed clauses in the originalcomplex sentence.
desse is also used for connec-tive prediction tasks, as in (gao et al., 2021).2.we perform our task on amazon mechanicalturk (amt).
in a series of pilot tasks on amt, weiteratively designed annotation instructions and anannotation interface, while monitoring quality.
fig-ure 2 illustrates two steps in the annotation: iden-tiﬁcation of n split points between tensed clauses,and rephrasing the source into n + 1 simple clauses,where any connectives are dropped.
the instruc-tions ask annotators to focus on tensed clauses oc-curring in conjoined or subordinate structures, rel-ative clauses, parentheticals, and conjoined verbphrases, and to exclude gerundive phrases, inﬁnti-val clauses, and clausal arguments of verbs.
theﬁnal version of the instructions describes the twoannotation steps, provides a list of connectives, andillustrates a positive and negative example.3 thetraining and tests sets contains 12k and 790 exam-ples, respectively..3.2 minwikisplit.
minwikisplit has 203k complex sentences andtheir rephrased versions (niklaus et al., 2019b)..2desse and minwiki are available at https://.
github.com/serenayj/desse..3in step 2, the interface checked for any remaining con-nectives, to warn annotators.
details about the interface andquality control are included in appendix a..it is built from wikisplit, a text simpliﬁcationdataset derived from wikipedia revision histories(narayan et al., 2017), modiﬁed to focus on min-imal propositions that cannot be further decom-posed.
it was designed for simplifying complexsentences into multiple simple sentences, wherethe simple sentences can correspond to a very widerange of structures from the source sentences, suchas prepositional or adjectival phrases.
to best uti-lize this corpus for our purposes, we selected a sub-sample where the number of tensed verb phrasesin the source sentences matches the number ofrephrased propositions.
the resulting minwikicorpus has an 18k/1,075 train/test split..3.3 linguistic phenomena.
table 1 presents prevalence of syntactic pat-terns characterizing complex sentences in the twodatasets.
four are positive examples of one-to-onecorrespondence of tensed clauses in the source withsimple sentences in the rephrasings: discourse con-nectives (disc.
conn.), vp-conjunction, clausesintroduced by wh- subordinating conjunctions (e.g.,when, whether, how) combined with non-restrictiverelative clauses (wh- & rel.
cl.
), and restrictiverelative clauses (restric.
rel.
cl.).
the sixth col-umn (negative examples) covers clausal arguments,which are often that-complements of verbs that ex-press belief, speaking, attitude, emotion, and soon.
minwiki has few of the latter, presumablydue to the genre difference between opinion essays(desse) and wikipedia (minwiki)..4 problem formulation.
we formulate the problem of converting complexsentences into covering sets of simple sentences asa graph segmentation problem.
each sentence isrepresented as a word relation graph (wrg), a di-rected graph constructed from each input sentencewith its dependency parse.
every word token andits positional index becomes a wrg vertex.
forevery pair of words, one or more edges are addedas follows: a neighbor edge that indicates that thepair of words are linearly adjacent; a dependencyedge that shows every pair of words connected bya dependency relation, adding critical grammaticalrelations, such as subject..figure 3 shows an example sentence and a sim-pliﬁed version of its wrg (edge directions are notshown, for readability).
vertices are labeled withword-index pairs in red font, and edges are labeled.
39215 system overview.
the full processing pipeline consists of ﬁve ma-jor components, as shown in figure 4. three pre-processing modules handle the wrg graph con-struction, conversion of graph triples to vectors,and creation of distant supervision labels for thegraph.
the fourth component is the abcd neuralmodel that learns to label a wrg graph, whichis described in section 6. the last part of thepipeline is a post-processing module to segmentwrg graphs based on the labels learned by theabcd model, and to map each graph segment toa simple sentence..graph constructor the ﬁrst module in the sys-tem is a graph constructor that converts an inputsentence and its dependency parse into a collectionof vertices and edges.
it is used during training andinference.
it ﬁrst extracts words and their indicesfrom the input sentences of the training examplesfor the vertices of each wrg graph.
a directededge and ngbh label is assigned to all pairs of ad-jacent words.
a directed edge and label is alsoassigned to every governing and dependent wordpair (cf.
figure 3)..edge triples db the edge triples db, which isused during training and inference, creates vectorrepresentations for the input edge triples sets foreach training instance, using latent representationslearned by an encoder component of the abcdmodel.
using the word indices, a function maps thesource and target words from every triple into itshidden representation learned by the encoder, andthe triple’s edge label is converted into a one-hotencoding with dimension d. for an edge triples setwith m triples, the source and target word hiddenstates are each stacked into an m × h matrix, andthe one-hot vectors for edge labels are stacked intoan m × d matrix.
these three source, target, edgematrices that represent an edge triple set are thenfed into an attention layer, as discussed in section 6..distant supervision label creator the ex-pected supervision for our task is the choice ofedit type for each triple, where the ground truthconsists of pairs of an input sentence, and one ormore output simple sentences.
we use distant su-pervision where we automatically create edit labelsfor each triple based on the alignment between theoriginal input sentence and the set of output simplesentences.
in the distant supervision label creator.
figure 3: example complex sentence (orig), ground truthoutput (ss 1 and ss 2), and wrg (best seen in color; edgedirections and punctuation omitted for readability).
verticesare word tokens and their indices, edges are neighbor (ngbh)and/or dependency relations.
dashed lines represent edges tobreak, the green curved line represents an edge to copy, theopen circle node for and-6 is for drop, and all other partsof the graph get accept.
at bottom left is a fragment of thecorresponding edge triple set..as ngbh for neighboring words, or with the tagscorresponding to their dependency relations, suchas nsubj between sokuhi-1 and ordained-13.
anedge can have both types of relation, e.g.
neighborand dependency for was-12 and ordained-13.
thegraph is stored as an edge triple set, a set of tripleswith (source node, target node, label) representingeach pair of words connected by an edge, as shownin figure 3, bottom left.
given a sentence and itswrg, our goal is to decompose the graph into nconnected components (cc) where each cc is laterrewritten as an output simple sentence.
to performthe graph decomposition, decisions are made onevery edge triple.we deﬁne four edit types:.
• accept: retain the triple in the output• break: break the edge between a pair of.
words.
• copy: copy a target word into a cc• drop: delete the word from the output ccs.
a training example consists of an input sentence,and one or more output sentences.
if the inputsentence is complex, the ground truth output con-sists of multiple simple sentences.
the next sectionpresents the abcd pipeline.
two initial mod-ules construct the wrg graphs for each input sen-tence, and the abcd labels for the edge triplesets based on the ground truth output.
a neuralmodel learns to assign abcd labels to input wrggraphs, and a ﬁnal graph segmenter generates sim-ple sentences from the labeled wrg graphs.
de-tails about the neural model are in the subsequentsection..3922figure 4: abcd system overview during training (top) and inference (bottom)..by indices, and output a simple sentence..module, for every triple, we check the followingconditions: if the edge is a ”neighbor” relation, andboth source and target words are in the same out-put simple sentence, we mark this pair with edittype a; if the source and target words of a tripleoccur in different output simple sentences, the cor-responding edit is b; if the source and target are inthe same output simple sentence, and the only edgeis a dependency label (meaning that they are notadjacent in the original sentence), we mark this pairas c; ﬁnally, if a word is not in any output simplesentence, we mark the corresponding type as d..graph segmenter this module segments thegraph into connected components using predictededits, and generates the output sentences, as partof the inference pipeline.
there are four stagesconsisting of: graph segmentation, traversal, sub-ject copying, and output rearranging.
in the graphsegmentation stage, the module ﬁrst performs ac-tions on every triple per the predicted edit: if theedit is a, no action is taken; if the edit is b, theedge between the pair of words is dropped; givenc, the edge is dropped, and the edge triple is storedin a temporary list for later retrieval; if the editis d, the target word is dropped from the outputgraphs.
after carrying out the predicted edits, werun a graph traversal algorithm on modiﬁed edgetriples to ﬁnd all ccs, using a modiﬁed version ofthe depth-first-search algorithm with linear timeproposed in (tarjan, 1972; nuutila and soisalon-soininen, 1994).
for each cc, the vertices are keptand the edges are dropped.
then we enter the sub-ject copying stage: for each source, target pair inthe temporary list mentioned earlier, we copy theword to the cc containing the target.
finally forevery cc, we arrange all words in their linear order.
figure 5: architecture for abcd model..6 neural model.
the abcd model consists of three neural mod-ules depicted in figure 5: a sentence encoder tolearn a hidden representation for the input sentence,a self-attention layer to generate attention scoreson every edge label, and a classiﬁer that generatesa predicted distribution over the four edit types,based on the word’s hidden representation, the edgelabel representation, and the attention scores..6.1 sentence representation.
the sentence representation module has twocomponents: a word embedding look up layerbased on glove (pennington et al., 2014), anda bidirectional lstm (hochreiter and schmidhu-ber, 1997) (see figure 5).
given an input sentencelength l, and the hidden state dimension m , theoutput from this module is l × m .
for a word withindex i in the input sentence, we generate its hiddenrepresentation hi such that it combines the hiddenstates from forward and backward lstms, with.
3923hi ∈ rm .
a positional encoding function is addedto the word embeddings.
we found this particu-larly helpful in our task, presumably because thesame word type at different positions might havedifferent relations with other words, captured bydistinct learned representations.
our experimentscompare bilstm training from scratch to use ofbert (devlin et al., 2019), to see if pre-trainedrepresentations are helpful..to utilize the learned word representations in thecontext of the relational information captured in thewrg graph, we send the sentence representation tothe edge triple db and extract representations hiand hj for the source and target words, based on in-dices i and j. a one-hot vector with dimensionalityn encodes relations between pairs of source andtarget words; each edge triple is thus converted intothree vectors: hsrc, htgt, drel.
we take position-wise summation over all one hot vectors if there ismore than one label on an edge..6.2 edge self-attention.
attention has been useful for many nlp tasks.
in our model, we adapt the multi-head self attentionmechanism (vaswani et al., 2017) to learn impor-tance weights on types of edit operations, as shownin the middle green block in figure 5. given medge triples, we ﬁrst stack all source vectors hsrcinto a matrix hsrc, and operate the same way onhtgt and drel to obtain htgt and drel, such thathsrc, htgt ∈ rm×m , and drel ∈ rm×n .
thesethree matrices are the input to self-attention.
for ev-ery head of the multi-head attention, we ﬁrst obtaina feature representation with the three parametersv, k, q mapping to sources, targets and relations,respectively, then compute a co-efﬁcient e with alearnable parameter w e as follows:.
e = leakyrelu(w e(v hsrc; khtgt; qdrel))(1)where e ∈ rm×1.
then we compute the attentionscores by taking a softmax over e:.
datasetminwikidesseminwikidesse.
b.a.dc6.57%85.23% 4.58% 3.60%74.77% 2.39% 5.62% 17.21%0.21350.01670.08760.0200.
0.41640.2658.
0.35330.6266.table 2: distributions (top) and inverse class weights(bottom) for the four edit labels on both minwiki anddesse datasets..6.3 edit classiﬁcation.
the last component of our neural model is aclassiﬁer, as shown at the right of figure 5. to ag-gregate the feature representation from the previouslayer, we ﬁrst concatenate the three matrices hsrc,htgt, drel into one representation, and multiply theattention scores as follows:.
h (cid:48) = α(hsrc; htgt, drel).
(4).
an mlp layer then takes h (cid:48) as its input and gener-ates the output distribution over the four edit typesfor each edge triple:.
outm = sof tmax(m lp (h (cid:48))).
(5).
where outm ∈ rm×4..as an alternative to mlp, we also investigateda bilinear classiﬁer, which has proved efﬁcient incapturing ﬁne-grained differences in features forclassiﬁcation task (dozat and manning, 2017).
thebilinear layer ﬁrst takes hsrc and htgt as input andgenerates transposed bilinear features :srcw ahtgt + b.outputbi = h (cid:124).
(6).
where w a, b are learnable parameters.
then wesum the bilinear features with the mlp decisionsand apply softmax on the result to get the ﬁnaldistribution over the four edit labels:.
outb = sof tmax(outputbi + m lp (h (cid:48))) (7).
where outb ∈ rm×4.
we use cross entropy lossbetween predicted edit types and gold edit typescreated from distant supervision (see above)..head = sof tmax(e).
(2).
6.4 training.
finally, we concatenate all head attentions together,and pass them through a linear layer to learn therelations between heads, and generate the ﬁnal at-tention scores:.
α = w (concat((head1, head2, .
.
.))
(3)α ∈ rm×1.
the attention scores are sent to the nextmodule to help the classiﬁer make its decision..the class balance for our task is highly skewed:the frequency of class a is much higher than theother three classes, as shown in the top portion oftable 2. to mitigate the impact on training, weadopt the inverse class weighting for cross entropyloss introduced in (huang et al., 2016).
with thisweighting, loss is weighted heavily towards rareclasses, which forces the model to learn more about.
3924the rare cases.
table 2 shows the weights for fouredit labels on both datasets.
on minwiki, a occursthe most and has the lowest weights as 0.0167, asharp contrast to b,c,d.
on desse, both a andd occur frequently while b and c have lower fre-quency with higher weights, at 0.6266 and 0.2658.desse has fewer b, and more c and d than min-wiki.
from this perspective, minwiki is “simpler”than desse because there are fewer edits on rewrit-ing the sentences.
this might be due to the differ-ent distributions of linguistic phenomena in the twodatasets (see table 1).
in the next section, we willshow that abcd shows stronger improvementson complicated edits.
training details are in theappendix..7 experiments.
we carry out two intrinsic evaluations of abcdperformance on minwiki and desse.
section 7.1presents an intrinsic evaluation of abcd variantson edit prediction, with error analysis and abla-tion studies.
section 7.2 compares the best abcdmodel with several baselines on the quality of out-put propositions.
we discuss evaluation metrics insection 7.3. results show that abcd models showconsistently good performance compared to otherbaseline models on both datasets..7.1.intrinsic evaluation on edit prediction.
we report f1 scores on all four edit types fromabcd and its model variants.
we compare twoclassiﬁers as mentioned in previous sections andinvestigate the difference between using bilstmand bert with ﬁne-tuning, to see if pre-trainedknowledge is useful for the task..table 3 presents results on minwiki and dessefrom the four model settings.
all models per-form better on minwiki than desse, and bil-stm+bilinear shows the best performance on both,with f1 scores of 0.82 and 0.67 on minwiki anddesse respectively.
presumably this reﬂects thegreater linguistic diversity of desse shown in ta-ble 1. the lower performance from bert variantsindicates the pre-trained knowledge is not helpful.
among the four edit types, all models have highf1 scores on a across datasets, high f1 on c forminwiki, but not on desse.
b and d show lowerscores, and all four models report lower f1 on bthan d on both datasets..to examine the signiﬁcant drop on b and d fromminwiki to desse, table 4 presents error anal-.
ysis on pairs of gold labels and predictions for band d, using predictions from bilstm+mlp.
themodel does poorly on b in both datasets, comparedwith predictions of 36.1% for a on minwiki, onon desse, 27.42% for a and 15.18% for c. themodel has high agreement on d from minwiki,but predicts 42.63% a on desse.
we suspect thatimproved feature representation could raise perfor-mance; that is, pairs of words and their relationsmight be a weak supervision signal for b and d..we conducted an ablation study on the inverseclass weights mentioned in section 6 on minwiki.
after removing the weights, the model fails to learnother classes and only predicts a due to the highlyimbalanced label distributions, which demonstratesthe beneﬁt of weighting the loss function.
we alsoablate positional encoding which leads to f1 scoresof 0.90 for a, 0.51 for c, and 0 for both b and d,indicating the importance of positional encoding..7.2.intrinsic evaluation of output sentences.
for baselines, we use copy512 and dissim,which both report performance on wikisplit in pre-vious work.
we also include dcp, which relieson three rules applied to token-aligned dependencyand constituency parses: dcpvp extracts clauseswith tensed verb phrases; dcpsbar extracts sbarsubtrees from constituency trees; dcprecur recur-sively applies the preceding rules..for evaluation, we use bleu with four-grams(bl4) (papineni et al., 2002) and bertscore(bs) (zhang et al., 2019).
we also include de-scriptive measures speciﬁc to our task.
to indicatewhether a model retains roughly the same num-ber of words as the source sentence in the targetoutput, we report average number of tokens persimple sentence (#t/ss).
to capture the correspon-dence between the number of target simple sen-tences in the ground truth and model predictions,we use percentage of samples where the modelpredicts the correct number of simple sentences(match #ss).
bl4 captures the 4-gram alignmentsbetween candidate and reference word strings, butfails to assess similarity of latent meaning.
bs ap-plies token-level matching through contextualizedword embeddings, therefore evaluates candidateson their word meanings.
for each example, weﬁrst align each simple sentence in the ground truthwith a prediction, compute the pairwise bl4 andbs scores, and take the average as the score forthe example.
a predicted output sentence with no.
3925category.
bilstm.
bert.
bilstm.
bert.
minwiki.
desse.
abcdall.
mlp0.980.480.990.800.78.bilinear mlp0.930.410.950.390.72.
0.980.480.990.840.82.bilinear mlp0.910.340.890.490.66.
0.860.360.980.750.74.bilinear mlp0.880.310.890.450.63.
0.880.420.780.540.67.bilinear0.870.280.550.450.57.table 3: performance (f1) of our model and its variants on minwiki (n=1075) and desse (n=790)..data.
gold.
minwiki.
desse.
bdbd.predicted.
a36.1014.0127.4242.63.b48.330.1446.623.44.c5.590.4615.185.08.d9.9885.3810.7648.84.table 4: percentage (%) of count of predicted labels wheregold labels are b and d from abcd bilstm+mlp..correspondent in the ground truth, or a ground truthsentence with no correspondent in the predicted,will add 0 to the numerator and 1 to the denomina-tor of this average..table 5 presents results from the baselines andour abcd best variant, bilstm with two classi-ﬁers.
none of the models surpasses all others onboth datasets.
all models show lower performanceon desse than minwiki, again an indication thatdesse is more challenging.
on minwiki, abcdis competitive with copy512, the best performingmodel, with a narrow gap on match#ss (0.65%)and bleu4 (4.58).
on desse, abcd bl4 andbs surpass all baselines.
abcd performance is2.34% less than dcprecur on match #ss, but bil-stm+mlp output sentences have an average lengthof 8.85, which is closer to the gold average lengthof 9.07, in contrast to much longer output fromdcprecur of 14.16. to summarize, abcd achievescompetitive results on both datasets..7.3 error analysis.
while table 4 presents error analysis on pre-dictions of b that lead to an incorrect number ofoutputs, here we examine test sentences from bothdatasets where the prediction and ground truth havethe same number of outputs.
table 6 shows the to-tal number of examples for minwiki (1,075) andfor the positive examples in desse (dessepos,521).
the m columns for each dataset give thenumber of examples where the number of targetsin the ground truth matches the number of targetspredicted by the model.
on minwiki, abcd hasmarginally better bl4 and bs scores than copy512,but copy512 has 7 more cases with the correct num-.
ber of outputs.
for desse, we restrict attentionto the positive examples (minwiki has no negativeexamples), because copy512 and abcd performequally well on the negative examples.
by the bl4and bs scores on dessepos, copy512 appears toperform much better than abcd, but these scoresare on 20 out of 521 examples (3.8%).
althoughabcd’s scores are lower, it produces the correctnumber of output sentences in 47.4% of cases forthe mlp, and 48.1% for the bilin..figure 6 shows three complex sentences fromdesse with the annotated rewriting, and pre-dicted propositions from copy512 and abcd mlp.
copy512 correctly decomposes only one of the ex-amples and copies the original input on the othertwo samples.
on the one example where copy pro-duces two simple sentences, it alters the sentencemeaning by replacing the word “genetics” withthe word “interesting”.
this exposes a drawbackof encoder-decoder architectures on the proposi-tion identiﬁcation task, that is, the decoder canintroduce words that are not in the input sentence,therefore failing to preserve the original meaning.
in contrast, abcd shows good performance onall three sentences by producing the same numberof simple sentences as in the annotated rewriting.
especially for the third sentence, which containsan embedded clause, “which has been the mainmission since 9/11”, the ﬁrst proposition written bythe annotator is not grammatically correct, and thesubject of the second proposition is a pronoun it,referring to the semantic subject our main mission.
nonetheless, abcd generates two propositions,both of which are grammatically correct and mean-ing preserving..8 discussion.
in this section, we discuss limitations of abcdto guide future work.
the ﬁrst limitation is thelow performance of abcd on b. we observe thatin desse, some annotators did not break the sen-tences appropriately.
we randomly selected 50samples, and found 13 out of 50 (26%) examples.
3926group.
model.
match bleu4 bertsc.
match bleu4 bertsc.
minwiki.
desse.
parsing.
encoder-decoder copy.
dissimdcpvpdcpsbardcprecur.
64.2028.8019.3531.7880.9675.8076.38table 5: performance of baselines and our models on minwiki test set (n=1075, #t/ss = 10.03), and desse test set (n=790,#t/ss =9.07).
we report numbers of token per propositions (#t/ss), number of input sentences that have match number ofoutput between prediction and ground truth in percentage (match #ss%), bleu with four-gram and bertscore..37.8947.2548.0234.4445.9153.4241.57.
94.4264.5049.0758.0895.9692.9190.28.
89.5460.1859.8961.3788.7190.2394.78.abcd bilstm.
mlpbilin.
#t/ss9.5915.9917.2414.1618.138.858.10.
#ss(%)40.0042.4044.8155.6336.2053.2952.66.
#t/ss8.5014.8219.0716.309.379.379.53.
#ss(%)68.4645.4917.4967.9079.2678.6176.72.he did not do anything wrong, yet he was targeted and his family was murdered..orighuman he did not do anything wrong.
|| he was targeted.
|| his family was murdered.
copyhe did not do anything wrong, he was targeted and his family was murdered.
abcd he did not do anything wrong.|| he was targeted.
|| his family was murdered.
orighumancopyabcd i guess i always knew it was genetics.|| i didnt know why our features are the way that they are.
orighuman our main mission, which has been the main mission since 9/11.|| it is to eliminate terrorism wherever it may exist.
copyabcd our main mission has been the main mission.
|| mission is to eliminate terrorism wherever it may exist..i guess i always knew it was genetics but i didnt know why our features are the way that they are.
i guess i always knew it was genetics.
|| i didnt know why our features are the way that they are.
i guess i always knew it was interesting.|| i didnt know why our features are the way that they are..our main mission, which has been the main mission since 9/11 is to eliminate terrorism wherever it may exist..same as orig.
figure 6: three input complex sentences (orig) from desse, with the annotated rewriting (human), and thepredicted propositions from copy and abcd..copymlpbilin.
dessepos(n=521)bl4m92.482078.4924774.25251.minwiki (n=1075)bsbl4m97.1688.8185297.2189.5984596.9492.00825table 6: performance of copy512 and our abcd bilstmmodels on all positive samples from minwiki and desse testset.
columns show the raw count of complex sentences whereprediction has correct number of outputs (m), bl4 and bs..bs98.6695.7398.21.where annotators add breaks to rewrite nps andinﬁnitives as clauses.
this introduces noise intothe data.
another reason of lower performanceon b might be attributed to the current design ofabcd that neglects sequential relations amongall words.
among all edge triples where it failsto assign b, 67% and 27.42% are with ngbh rela-tions on minwiki and desse, respectively.
twopossibilities for improving performance to inves-tigate are enhancements to the information in thewrg graph, and re-formulating the problem intosequence labeling of triples..the second limitation pertains mainly to desse.
in the training data, 34.7% of sentences haveoov words.
for example, we noticed thatannotators sometimes introduced personal pro-nouns (e.g.he/she/they) in their rewrites of vp-conjunction,instead of copying the subjects,they substituted a demonstrative pronounor.
(e.g.this/these) for clausal arguments.
this couldbe addressed by expanding the edit types to includethe ability to insert words from a restricted in-sertion vocabulary.
nevertheless, our model has asmall performance gap with copy512 on minwiki,and outperforms the baselines on desse..a third issue is whether abcd would general-ize to other languages.
we expect abcd wouldperform well on european languages with existingdependency and constituency parsers, and with anannotated dataset..9 conclusion.
we presented a new task to decompose complexsentences into simple ones, along with desse, anew dataset designed for this task.
we proposedthe neural abcd model to predict four edits opera-tions on sentence graphs, as part of a larger pipelinefrom our graph-edit problem formulation.
abcdperformance comes close to or outperforms theparsing-based and encoder-decoder baselines.
ourwork selectively integrates modules to capitalizeon the linguistic precision of parsing-based meth-ods, and the expressiveness of graphs for encodingdifferent aspects of linguistic structure, while stillcapitalizing on the power of neural networks forrepresentation learning..3927references.
roee aharoni and yoav goldberg.
2018. split andrephrase: better evaluation and stronger baselines.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume2: short papers), pages 719–724..jill burstein, karen kukich, susanne wolff, chi lu,and martin chodorow.
1998. enriching automatedin dis-essay scoring using discourse marking.
course relations and discourse markers..lynn carlson, daniel marcu,.
and mary ellenokurowski.
2003. building a discourse-tagged cor-pus in the framework of rhetorical structure theory.
in current and new directions in discourse and dia-logue, pages 85–112.
springer..seniz demir, sandra carberry, and kathleen f mc-coy.
2010. a discourse-aware graph-based content-selection framework.
in proceedings of the 6th in-ternational natural language generation confer-ence..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-in 5th international conference on learninging.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..yimai fang, haoyue zhu, ewa muszy´nska, alexanderkuhnle, and simone teufel.
2016. a proposition-in proceedings ofbased abstractive summariser.
coling 2016, the 26th international conferenceon computational linguistics: technical papers,pages 567–578..chris fournier.
2013. evaluating text segmentation us-in proceedings of theing boundary edit distance.
51st annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1702–1712, soﬁa, bulgaria.
association for compu-tational linguistics..yanjun gao, chen sun, and rebecca j. passonneau.
2019. automated pyramid summarization evalua-tion.
in proceedings of the 23rd conference on com-putational natural language learning (conll),pages 404–418, hong kong, china.
association forcomputational linguistics..jiatao gu, zhengdong lu, hang li, and victor okincorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..chen huang, yining li, chen change loy, and xiaooutang.
2016. learning deep representation for im-balanced classiﬁcation.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion, pages 5375–5384..yohan jo, elijah mayﬁeld, chris reed, and eduardhovy.
2020. machine-aided annotation for ﬁne-grained proposition types in argumentation.
in pro-ceedings of the 12th language resources and eval-uation conference, pages 1008–1018..yohan jo, jacky visser, chris reed, and eduard hovy.
2019. a cascade model for proposition extraction inargumentation.
in proceedings of the 6th workshopon argument mining, pages 11–24, florence, italy.
association for computational linguistics..deanna kuhn, laura hemberger, and valerie khait.
2016. tracing the development of argumentive writ-ing in a discourse-rich context.
written communica-tion, 33(1):92–121..jing li, aixin sun, and shaﬁq joty.
2018. segbot: ageneric neural text segmentation model with pointerin proceedings of the 27th internationalnetwork.
joint conference on artiﬁcial intelligence (ijcai),pages 4166–4172..william c mann and sandra a thompson.
1986. re-lational propositions in discourse.
discourse pro-cesses, 9(1):57–90..shashi narayan, claire gardent, shay cohen, andanastasia shimorina.
2017. split and rephrase.
inemnlp 2017: conference on empirical methods innatural language processing, pages 617–627..yanjun gao, ting-hao huang, and rebecca j. passon-neau.
2021. learning clause representation fromdependency-anchor graph for connective prediction.
in proceedings of the fifteenth workshop on graph-based methods for natural language processing(textgraphs-15), pages 54–66, mexico city, mex-ico.
association for computational linguistics..christina niklaus, matthias cetto, andr´e freitas, andsiegfried handschuh.
2019a.
dissim: a discourse-aware syntactic text simpliﬁcation framework forenglish and german.
in proceedings of the 12th in-ternational conference on natural language gener-ation, pages 504–507, tokyo, japan.
association forcomputational linguistics..3928tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2019. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..xingxing zhang and mirella lapata.
2017. sentencesimpliﬁcation with deep reinforcement learning.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages584–594..christina niklaus, andr´e freitas, and siegfried hand-schuh.
2019b.
minwikisplit: a sentence splittingin proceedingscorpus with minimal propositions.
of the 12th international conference on natural lan-guage generation, pages 118–123, tokyo, japan.
association for computational linguistics..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language ar-guments.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 4658–4664..esko nuutila and eljas soisalon-soininen.
1994. onﬁnding the strongly connected components in ainformation processing letters,directed graph.
49(1):9–14..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..maxime peyrard and judith eckle-kohler.
2017.supervised learning of automatic pyramid foroptimization-based multi-document summarization.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1084–1094..rashmi prasad, nikhil dinesh, alan lee, eleni milt-sakaki, livio robaldo, aravind k joshi, and bon-nie l webber.
2008. the penn discourse treebank2.0. in lrec.
citeseer..valentina pyatkin, ayal klein, reut tsarfaty, and idodagan.
2020. qadiscourse-discourse relations as qapairs: representation, crowdsourcing and baselines.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 2804–2819..robert tarjan.
1972. depth-ﬁrst search and linearsiam journal on computing,.
graph algorithms.
1(2):146–160..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..yizhong wang, sujian li, and jingfeng yang.
2018.toward fast and accurate neural discourse segmen-in proceedings of the 2018 conference ontation.
empirical methods in natural language processing,pages 962–967..3929a annotation instruction in desse.
here we present the instructions for annotators,.
as shown by figure 7..figure 8: interface of splitting the sentence.
figure 7: instruction for desse annotation.
figure 9: interface of rewriting the segments from fig-ure 8 into complete sentences.
the instructions illustrate the two phases of an-notation.
the annotator ﬁrst chooses whether toadd one or more split points to an input sentence,where the word after a split point represents theﬁrst word of a new segment.
once an annotator hasidentiﬁed the split points (ﬁrst page of the amtinterface, shown as figure 8), a second page of theinterface appears.
figure 9 shows the second viewwhen annotators rewrite the segments.
every spanof words deﬁned by split points (or the originalsentence if no split points), appears in its own textentry box for the annotator to rewrite.
annotatorscannot submit if they remove all the words from atext entry box.
they are instructed to rewrite eachtext span as a complete sentence, and to leave outthe discourse connectives..several kinds of auto-checking and warningsare applied in the interface to ensure quality.
if arewrite contains a discourse connective, a warningbox pops up asking if they should drop the dis-course connective before submitting it.
a warningbox will show up if annotators use vocabulary out-side the original sentence.
to prevent annotatorsfrom failing to rewrite, we monitored the output,checking for cases where they submitted the textspans with no rewriting.
annotators were prohib-ited to submit if the interface detected an empty.
rewrite box or the total lengths of the rewrites aretoo short compared to the source sentence.
wewarned annotators by email that if they failed toproduce complete sentences in the rewrite boxes,they would be blocked.
some annotators wereblocked, but most responded positively to the warn-ings..b quality control in desse.
to test the clarity of instruction and interface, theinitial 500 sentences were used for evaluating thetask quality, each labeled by three turkers (73 turk-ers overall), using three measures of consistency,all in [0,1].
average pairwise boundary similar-ity (fournier, 2013), a very conservative measureof whether annotators produce the same numberof segments with boundaries at nearly the samelocations, was 0.55. percent agreement on numberof output substrings was 0.80. on annotations withthe same number of segments, we measured theaverage jaccard score (ratio of set intersection toset union) of words in segments from different an-notators, which was 0.88, and words from rephras-ings, which was 0.73. with all metrics close to 1,and boundary similarity above 0.5, we concluded.
3930quality was already high.
during the actual datacollection, quality was higher because we moni-tored quality on a daily basis and communicatedwith turkers who had questions..c experiment settings.
we trained our model on a linux machine withfour nvidia rtx 2080 ti gpus.
we conductedgrid search for the hyper-parameters, with learningrage in the range of [1e-2, 1e-5] (step size 0.0005),weight decay between [0.90, 0.99], hidden size[200, 800] (step size 200).
final parameters are setwith adam optimizer and learning rate at 1e − 4,weight decay 0.99, embedding dropout at 0.2, max-imum epoch as 100 with early stop.
we use glove100 dimension vectors, hidden size of network as800. we set the number of heads in self-attention as4, corresponding to the four edit types.
with batchsize 64, it takes about 6 hours to train minwiki and4 hours for desse.
for bert ﬁne-tuning, we use1e − 4 learning rate, weight decay at 0.99..3931