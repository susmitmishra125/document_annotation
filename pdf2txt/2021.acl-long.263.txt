structured sentiment analysis as dependency graph parsing.
jeremy barnes*, robin kurtz†, stephan oepen*, lilja øvrelid*and erik velldal**university of oslo, department of informatics†national library of sweden, kblab{ jeremycb | oe | liljao | erikve } @ifi.uio.norobin.kurtz@kb.se.
abstract.
structured sentiment analysis attempts to ex-tract full opinion tuples from a text, but overtime this task has been subdivided into smallerand smaller sub-tasks, e.g., target extractionor targeted polarity classiﬁcation.
we arguethat this division has become counterproduc-tive and propose a new uniﬁed framework toremedy the situation.
we cast the structuredsentiment problem as dependency graph pars-ing, where the nodes are spans of sentimentholders, targets and expressions, and the arcsare the relations between them.
we performexperiments on ﬁve datasets in four languages(english, norwegian, basque, and catalan)and show that this approach leads to strongimprovements over state-of-the-art baselines.
our analysis shows that reﬁning the sentimentgraphs with syntactic dependency informationfurther improves results..1.introduction.
structured1 sentiment analysis, i.e., the task ofpredicting a structured sentiment graph like theones in figure 1, can be theoretically cast as aninformation extraction problem in which one at-tempts to ﬁnd all of the opinion tuples o =oi, .
.
.
, on in a text.
each opinion oi is a tuple(h, t, e, p) where h is a holder who expresses a po-larity p towards a target t through a sentimentexpression e, implicitly deﬁning pairwise relation-ships between elements of the same tuple.
liu(2012) argues that all of these elements2 are essen-tial to fully resolve the sentiment analysis problem..1we use the term ‘structured sentiment’ distinctly from al-mars et al.
(2017), who use it to refer to the latent hierarchicalstructure of sentiment aspects.
we instead use ‘structured’ torefer to predicting sentiment graphs as a structured predictiontask, as opposed to the many text classiﬁcation task that arefound in sentiment analysis..2liu (2012)’s deﬁnition replaces sentiment expression with.
the time when the opinion was expressed..however, most research on sentiment analysis fo-cuses either on a variety of sub-tasks, which avoidsperforming the full task, or on simpliﬁed and ideal-ized tasks, e.g., sentence-level binary polarity clas-siﬁcation..we argue that the division of structured senti-ment into these sub-tasks has become counterpro-ductive, as reported experiments are often not sen-sitive to whether a given addition to the pipelineimproves the overall resolution of sentiment, ordo not take into account the inter-dependencies ofthe various sub-tasks.
as such, we propose a uni-ﬁed approach to structured sentiment which jointlypredicts all elements of an opinion tuple and theirrelations.
moreover, we cast sentiment analysis asa dependency graph parsing problem, where thesentiment expression is the root node, and the otherelements have arcs which model the relationshipsbetween them.
this methodology also enables usto take advantage of recent improvements in seman-tic dependency parsing (dozat and manning, 2018;oepen et al., 2020; kurtz et al., 2020) to efﬁcientlylearn a sentiment graph parser..this perspective also allows us to unify a num-ber of approaches, including targeted, and opiniontuple mining.
we aim to answer rq1: whethergraph-based approaches to structured sentimentoutperform state-of-the-art sequence labeling ap-proaches, and rq2: how to best encode structuredsentiment as parsing graphs.
we perform experi-ments on ﬁve standard datasets in four languages(english, norwegian, basque, catalan) and showthat graph-based approaches outperform state-of-the-art baselines on all datasets on several standardmetrics, as well as our proposed novel (unlabeledand labeled) sentiment graph metrics.
we furtherpropose methods to inject linguistic structure intothe sentiment graphs using syntactic dependencies.
our main contributions are therefore 1) proposinga holistic approach to structured sentiment through.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3387–3402august1–6,2021.©2021associationforcomputationallinguistics3387figure 1: a structured sentiment graph is composed of a holder, target, sentiment expression, their relationshipsand a polarity attribute.
holders and targets can be null..sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance,and 3) extensive experimental results that outper-form state-of-the-art baselines.
finally, we releasethe code and datasets3 to enable future work on thisproblem..2 related work.
structured sentiment analysiscan be brokendown into ﬁve sub-tasks: i) sentiment expressionextraction, ii) sentiment target extraction, iii) senti-ment holder extraction, iv) deﬁning the relationshipbetween these elements, and v) assigning polarity.
previous work on information extraction has usedpipeline methods which ﬁrst extract the holders,targets, and expressions (tasks i - iii) and subse-quently predict their relations (task iv), mostly onthe mpqa dataset (wiebe et al., 2005).
crfs anda number of external resources (sentiment lexicons,dependency parsers, named-entity taggers) (choiet al., 2006; yang and cardie, 2012) are strong base-lines.
given the small size of the training data andthe complicated task, these techniques often stilloutperform neural models, such as bilstms (kati-yar and cardie, 2016).
transition-based end-to-end approaches have shown some potential (zhanget al., 2019).
however, all of this work ignores thepolarity classiﬁcation subtask..targeted sentiment analysis only concentrateson extracting sentiment targets (task ii) and classi-fying the polarity directed towards them (task iv)(jiang et al., 2011; mitchell et al., 2013).
recentshared tasks on aspect-based sentiment analysis(absa) (pontiki et al., 2014, 2015, 2016) also in-clude target extraction and polarity classiﬁcationsubtasks.
joint approaches perform on par withpipeline methods (li et al., 2019a) and multitaskmodels can perform even better (he et al., 2019).
finally, pretrained language models (devlin et al.,.
2019) can also lead to improvements on the absadata (li et al., 2019b)..end2end sentiment analysis is a recently pro-posed subtask which combines targeted sentiment(tasks ii and v) and sentiment expression extraction(task i), without requiring the resolution of relation-ships between targets and expressions.
wang et al.
(2016) augment the absa datasets with sentimentexpressions, but provide no details on the annota-tion process or any inter-annotator agreement.
heet al.
(2019) make use of this data and propose amulti-layer cnn (imn) to create hidden represen-tations h which are then fed to a target and opinionextraction module (ae), which is also a multi-layercnn.
this module predicts ˆyae, a sequence of biotags4 that predict the presence or absence of targetsand expressions.
after jointly predicting the targetsand expressions, a second multi-layer cnn with aﬁnal self-attention network is used to classify thepolarity, again as sequence labeling task (as).
thissecond module combines the information from hand ˆyae by incorporating the predicted probabil-ity of a token to be a target in the formulation ofself-attention.
finally, an iterative message-passingalgorithm updates h using the predictions from allthe modules at the previous timestep..chen and qian (2020) instead propose relation-aware collaborative learning (racl).
thismodel creates task speciﬁc representations by ﬁrstembedding a sentence, passing through a sharedfeed-forward network and ﬁnally a task-speciﬁccnn.
this approach then models interactions be-tween each pair of sub-tasks (target extraction, ex-pression extraction, sentiment classiﬁcation) by cre-ating pairwise weighted attention representations.
these are then concatenated and used to create thetask-speciﬁc predictions.
the authors ﬁnally stackseveral racl layers, using the output from theprevious layer as input for the next..3code and datasets available at https://github..com/jerbarnes/sentiment_graphs..4the tags include {bio}-{target,expression}.
3388some     others     give     the      new     umuc      5      stars     -      don't     believe     them   .
positivenegativeholdertargetexpressiontargetexpressionboth models perform well on the augmented se-meval data, but it is unlikely that these annotationsare adequate for full structured sentiment, as wanget al.
(2016) only provide expression annotationsfor sentences that have targets, generally only in-clude sentiment-bearing words (not phrases), anddo not specify the relationship between target andexpression..finally, the recently proposed aspect sentimenttriplet extraction (peng et al., 2019; ?)
attempts toextract targets, expressions and their polarity.
how-ever, the datasets used are unlikely to be adequate,as they augment available targeted datasets, butdo not report annotation guidelines, procedure, orinter-annotator agreement..graph parsing: syntactic dependency graphsare regularly used in applications, supplying themwith necessary grammatical information (mintzet al., 2009; cui et al., 2005; bj¨orne et al., 2009;johansson and moschitti, 2012; lapponi et al.,2012).
the dependency graph structures used inthese systems are predominantly restricted to trees.
while trees are sufﬁcient to encode syntactic depen-dencies, they are not expressive enough to handlemeaning representations, that require nodes to havemultiple incoming arcs, or having no incoming arcsat all (kuhlmann and oepen, 2016).
while much ofthe early research on parsing these new structures(oepen et al., 2014, 2015) focused on specializeddecoding algorithms, dozat and manning (2018)presented a neural dependency parser that essen-tially relies only on its neural network structureto predict any type of dependency graph withoutrestrictions to certain structures.
using the parser’sability to learn arbitrary dependency graphs, kurtzet al.
(2020) phrased the task of negation resolution(morante and blanco, 2012; morante and daele-mans, 2012) as a graph parsing task.
this trans-formed the otherwise ﬂat representations to depen-dency structures that directly encode the often over-lapping relations between the building blocks ofmultiple negation instances at the same time.
in asimpler fashion, yu et al.
(2020) exploit the parserof dozat and manning (2018) to predict spans ofnamed entities..are shown in table 1. the largest available struc-tured sentiment dataset is the norecfine dataset(øvrelid et al., 2020), a multi-domain dataset ofprofessional reviews in norwegian, annotated forstructured sentiment.
multibeu and multibca(barnes et al., 2018) are hotel reviews in basqueand catalan, respectively.
mpqa (wiebe et al.,2005) annotates news wire text in english.
finally,dsunis (toprak et al., 2010) annotate english re-views of online universities and e-commerce.
inour experiments, we use only the university re-views, as the e-commerce reviews have a largenumber of ‘polar targets’, i.e., targets with a polar-ity, but no accompanying sentiment expression..while all the datasets annotate holders, targets,and expressions, the frequency and distributionof these vary.
regarding holders, mpqa hasthe most (2,054) and dsunis has the fewest (94),whereas norecfine has the largest proportion oftargets (8,923) and expressions (11,115).
the aver-age length of holders (2.6 tokens) and targets (6.1tokens) in mpqa is also considerably higher thanthe others..it is also worth pointing out that mpqa anddsunis additionally include neutral polarity.
in thecase of mpqa the neutral class refers to verbswhich are subjective but do not convey polarity, e.g.,‘say’, ‘opt for’.
in dsunis, however, the neutral la-bel tends to indicate expressions that could entailmixed polarity or are polar under the right condi-tions, e.g., ‘the classes were not easy’ is consideredneutral, as it is possible for difﬁcult classes to bedesirable at a university.
multibeu, and multibcaalso have labels for strong positive and strong neg-ative, which we map to positive and negative, re-spectively.
finally, norecfine includes intensityannotations (strong, normal, slight), which we dis-regard for the purposes of these experiments..4 modeling.
this section describes how we deﬁne and encodesentiment graphs, detail the neural dependencygraph models, as well as two state-of-the-art base-lines for end-to-end sentiment analysis (target andexpression extraction, plus polarity classiﬁcation)..3 datasets.
4.1 graph representations.
we here focus on datasets that annotate the full taskof structured sentiment as described initially.
weperform experiments on ﬁve structured sentimentdatasets in four languages, the statistics of which.
structured sentiment graphs as in figure 1 are di-rected graphs, that are made up of a set of labelednodes and a set of unlabeled edges connecting pairsof nodes.
nodes in the structured sentiment graphs.
3389sentences.
holders.
targets.
expressions.
polarity.
#.
avg..#.
avg.
max.
#.
avg.
max.
#.
avg.
max.
+ neu.
−.
norecfine.
multibca.
multibeu.
mpqa.
dsunis.
traindevtest.
traindevtest.
traindevtest.
traindevtest.
traindevtest.
863415311272.
1174168336.
1064152305.
450016221681.
2253232318.
16.716.917.2.
15.613.314.7.
10.510.710.7.
252324.
20920.
898120110.
1691552.
2053358.
1306377371.
651712.
1.11.01.0.
1.11.51.1.
1.11.11.1.
2.62.62.8.
1.21.11.3.
1233.
475.
622.
271632.
234.
67781152993.
1695211430.
1285153337.
1382449405.
1252151198.
1.92.02.0.
2.42.32.6.
1.41.31.4.
6.15.36.4.
1.21.21.2.
351520.
181012.
968.
564142.
536.
844814321235.
1981258518.
1684204440.
1656552479.
837106139.
4.95.14.9.
2.62.62.7.
2.22.52.2.
2.42.12.0.
1.91.72.0.
403130.
19914.
1089.
1488.
965.
5684988875.
1272151313.
1406168375.
675241166.
4954077.
000.
000.
000.
27110589.
1491918.
2756443358.
708107204.
2783665.
658202199.
61092103.table 1: statistics of the datasets, including number of sentences and average length (in tokens) per split, as wellas average and max lengths (in tokens) for holder, target, and expression annotations.
additionally, we include thedistribution of polarity – restricted to positive, neutral, and negative – in each dataset..can span over multiple tokens and may have mul-tiple incoming edges.
the resulting graphs canhave multiple entry points (roots), are not neces-sarily connected, and not every token is a node inthe graph.
the sentence’s sentiment expressionscorrespond to the roots of the graphs, connectingexplicitly to their respective holders and targets.
inorder to apply the algorithm of dozat and manning(2018), we simplify these structures into bi-lexicaldependency graphs visualized in figure 2. here,nodes correspond one-to-one to the tokens of the se-quence and follow the same linear order.
the edgesare drawn as arcs in the half-plane above the sen-tence, connecting heads to dependents.
similarly tothe source structures, the graphs can have multipleroots and nodes can have multiple or no incomingarcs.
for some rare instances of structured senti-ment graphs, the reduction to dependency graphsis lossy, as they do not allow multiple arcs to sharethe same head and dependent.
this results in aslight mismatch of the learned and aimed-for rep-resentations..the choice of how to encode the sentimentgraphs as parsing graphs opens for several alter-nate representations depending on the choice ofhead/dependent status of individual tokens in thetarget/holder/expression spans of the sentimentgraph.
we here propose two simple parsing graphrepresentations: head-ﬁrst and head-ﬁnal, which.
metric name.
level.
strictness.
+/−.
holder f1target f1exp.
f1targeted f1uf1lf1nsf1.
sf1.
token-leveltoken-leveltoken-leveltoken-levelgraph arcsgraph arcssentiment-graphsentiment-graph.
partialpartialpartialexactexactexactexact graph,partial tokenexact graph,partial token.
nononoyesnoyesno.
yes.
table 2: metrics used to evaluate performance.
col-umn +/− indicates whether polarity is included or not.
the main metrics are targeted f1, which allows us tocompare to methods that do not perform the full task,and sf1, which best represents the full task..are shown in figure 2. for head-ﬁrst, we set theﬁrst token of the sentiment expression as a rootnode, and similarly set the ﬁrst token in each holderand token span as the head of the span with all othertokens within that span as dependents.
the labelssimply denote the type of relation (target/holder)and for sentiment expressions, additionally encodethe polarity.
head-ﬁnal is similar, but instead setsthe ﬁnal token of spans as the heads, and the ﬁnaltoken of the sentiment expression as the root node..3390holder.
exp:pos.
exp:neg.
holder.
exp:pos.
target.
target.
target.
target.
exp:neg.
some.
others.
give.
the.
new.
umuc.
5.stars.
-.
don’t.
believe.
them..holder.
exp:pos.
exp:neg.
holder.
target.
target.
target.
exp:pos.
exp:neg.
target.
some.
others.
give.
the.
new.
umuc.
5.stars.
-.
don’t.
believe.
them..(a).
(b).
figure 2: two parsing graph proposals to encode the sentiment graph: (a) head-ﬁrst, where the ﬁrst token of anyspan is the head, and (b) head-ﬁnal, where the ﬁnal token is the head..4.2 proposed model.
the neural graph parsing model used in this workis a reimplementation of the neural parser by dozatand manning (2018) which was used by kurtzet al.
(2020) for negation resolution.
the parserlearns to score each possible arc to then ﬁnallypredict the output structure simply as a collectionof all positively scored arcs.
the base of the net-work structure is a bidirectional lstm (bilstm),that processes the input sentence both from left-to-right and right-to-left, to create contextualized rep-resentations c1, .
.
.
, cn = bilstm(w1, .
.
.
, wn)where wi is the concatenation of a word embed-ding, pos tag embedding, lemma embedding, andcharacter embedding created by a character-basedlstm for the ith token.
in our experiments, wefurther augment the token representations with pre-trained contextualized embeddings from multilin-gual bert (xu et al., 2019).
we use multilingualbert as several languages did not have availablemonolingual bert models at the time of the ex-periments (catalan, norwegian)..the contextualized embeddings are then pro-cessed by two feedforward neural networks (fnn),creating specialized representations for potentialheads and dependents, hi = fnnhead(ci) anddi = fnndep(ci).
the scores for each possible arc-label combination are computed by a ﬁnal bilineartransformation using the tensor u .
its inner dimen-sion corresponds to the number of sentiment graphlabels plus a special none label, indicating the ab-.
sence of an arc, which allows the model to predictarcs and labels jointly, score(hi, dj) = h(cid:62).
i u dj..4.3 baselines.
we compare our proposed graph prediction ap-proach with three state-of-the-art baselines5 forextracting targets and expressions and predictingthe polarity: imn6, racl7, as well as racl-bert, which also incorporates contextualized em-beddings.
instead of using bertlarge, we usethe cased bert-multilingual-base in order to fairlycompare with our own models.
note, however, thatour model does not update the mbert representa-tions, putting it at a disadvantage to racl-bert.
we also compare with previously reported extrac-tion results from barnes et al.
(2018) and øvrelidet al.
(2020)..5 evaluation.
as we are interested not only in extraction or clas-siﬁcation, but rather in the full structured sentimenttask, we propose metrics that capture the relationsbetween all predicted elements, while enablingcomparison with previous state-of-the-art modelson different subtasks.
the main metrics we use torank models are targeted f1 and sentiment graphf1..5despite having state-of-the-art results on mpqa, we donot compare with katiyar and cardie (2016) as they use dif-ferent dataset splits, 10-fold cross-validation, and their code isnot available..6imn code available at https://github.com/.
ruidan/imn-e2e-absa..7https://github.com/nlpwm-whu/racl..3391dataset.
model.
spans.
targeted.
parsing graph.
sent.
graph.
holder f1.
target f1.
exp.
f1.
f1.
uf1.
lf1.
nsf1.
sf1.
norecfine.
multibeu.
multibca.
mpqa.
dsunis.
øvrelid et al.
(2020)imnraclracl-bert.
head-ﬁrsthead-ﬁnal.
barnes et al.
(2018)†imnraclracl-bert.
head-ﬁrsthead-ﬁnal.
barnes et al.
(2018)†imnraclracl-bert.
head-ﬁrsthead-ﬁnal.
imnraclracl-bert.
head-ﬁrsthead-ﬁnal.
imnraclracl-bert.
head-ﬁrsthead-ﬁnal.
42.4---.
51.160.4∗.
54.0---.
60.460.5.
56.0---.
43.037.1.
43.846.3.
---.
---.
28.037.4.
31.335.945.647.2.
50.154.8.
57.048.255.459.9.
64.064.0.
64.056.365.467.5.
72.571.2.
24.332.620.0.
51.049.5.
33.039.344.6.
39.942.1.
31.348.755.456.3.
54.455.5.
54.065.270.772.6.
73.972.1.
52.060.967.670.3.
29.637.831.2.
48.146.0.
27.440.238.2.
71.1∗67.1.
40.345.5∗.
-18.020.130.3.
30.531.9.
-39.548.256.8.
57.856.9.
-32.549.152.4.
55.0∗53.9.
1.211.817.8.
33.5∗18.6.
17.922.827.3.
26.729.6.
----.
----.
----.
---.
---.
----.
----.
----.
---.
---.
----.
----.
----.
---.
---.
39.248.0∗.
31.537.7∗.
37.039.2∗.
29.531.2∗.
64.660.8.
60.056.0.
58.058.0.
54.754.7.
66.8∗62.7.
62.1∗58.1.
62.059.7.
56.853.7.
40.041.4.
36.938.0.
24.526.1.
17.418.8.
35.338.1.
31.433.9.
31.034.3∗.
25.026.5.
----.
----.
----.
---.
---.
table 3: experiments comparing our sentiment graph approaches (head-ﬁrst/head-ﬁnal) using mbert with thesequence-labeling baselines (imn, racl, racl-bert).
underlined numbers indicate the best result for themetric and dataset.
∗ indicates approach is signiﬁcantly better than second best (p < 0.05), as determined by abootstrap with replacement test.
† indicates results that are not comparable, as they were calculated with 10-foldcross-validation..token-level f1 for holders, targets, and ex-pressions to easily compare our models topipeline models, we evaluate how well these mod-els are able to identify the elements of a sentimentgraph with token-level f1..targeted f1 this is a common metric in targetedsentiment analysis (also referred to as f1-i (heet al., 2019) or absa f1 (chen and qian, 2020)).
a true positive requires the combination of exactextraction of the sentiment target, and the correctpolarity..parsing graph metrics we additionally com-pute graph-level metrics to determine how well themodels predict the unlabeled and labeled arcs of theparsing graphs: unlabeled f1 (uf1), labeled f1(lf1).
these measure the amount of (in)correctlypredicted arcs and labels, as the harmonic mean ofprecision and recall (oepen et al., 2014).
these.
metrics inform us of the local properties of thegraph, and do not overly penalize a model if a fewedges of a graph are incorrect..sentiment graph metrics the two metrics thatmeasure how well a model is able to capture thefull sentiment graph (see figure 1) are non-polarsentiment graph f1 (nsf1) and sentiment graphf1 (sf1).
for nsf1, each sentiment graph is a tu-ple of (holder, target, expression), while for sf1 weinclude polarity (holder, target, expression, polar-ity).
a true positive is deﬁned as an exact match atgraph-level, weighting the overlap in predicted andgold spans for each element, averaged across allthree spans.
for precision we weight the numberof correctly predicted tokens divided by the totalnumber of predicted tokens (for recall, we divideinstead by the number of gold tokens).
we allowfor empty holders and targets..33926 experiments.
all sentiment graph models use token-levelmbert representations in addition to word2vecskip-gram embeddings openly available from thenlpl vector repository8 (fares et al., 2017).
wetrain all models for 100 epochs and keep the modelthat performs best regarding lf1 on the dev set(targeted f1 for the baselines).
we use defaulthyperparameters from kurtz et al.
(2020) (see ap-pendix) and run all of our models ﬁve times withdifferent random seeds and report the mean (stan-dard deviation shown as well in table 8 in theappendix).
we calculate statistical difference be-tween the best and second best models through abootstrap with replacement test (berg-kirkpatricket al., 2012).
as there are 5 runs, we require that 3of 5 be statistically signiﬁcant at p < 0.05. table 3shows the results for all datasets..on norecfine, the baselines imn, racl, andracl-bert perform well at extracting targets(35.9, 45.6, and 47.2 f1, respectively) and expres-sions (48.7/55.4/56.3), but struggle with the full tar-geted sentiment task (18.0/20.1/30.3).
the graph-based models extract targets better (50.1/54.8) andhave comparable scores for expressions (54.4/55.5).
the holder extraction scores have a similar range(51.1/60.4).
these patterns hold throughout theother datasets, where the proposed graph modelsnearly always perform best on extracting spans,although racl-bert achieves the best scoreon extracting targets on dsunis (44.6 vs. 42.1).
the graph models also outperform the strongestbaseline (racl-bert) on targeted sentiment onall 5 datasets, although this difference is oftennot statistically signiﬁcant (norecfine head-ﬁrst,multibeu head-ﬁnal) and racl-bert is betterthan head-ﬁrst on dsunis..regarding the graph metrics, the results dependhighly on the dataset, with uf1 and lf1 rang-ing from 35.3/31.4 (dsunis head-ﬁrst) to 66.8/62.1(multibca head-ﬁrst).
sentiment graph metricsnsf1 and sf1 have a similar, though slightly lowerrange (24.5/17.7 – 62.0/56.8).
the graph and senti-ment graph metrics do not correlate perfectly, how-ever, as uf1 and lf1 on mpqa are relatively good.
8nordic language processing laboratory vector repo.
:http://vectors.nlpl.eu/repository/.
we used300-dimensional embeddings trained on english wikipediaand gigaword for english (model id 18 in the repo.
), and 100-dimensional embeddings trained on the 2017 conll corporafor all others; basque (id 32), catalan (id 34), and norwegianbokm˚al (id 58)..# h.ﬁrst h.ﬁnal racl.
norecfinemultibeumultibcampqadsunis.
14745744010.
63.368.972.255.456.9.
67.865.973.758.543.1.
65.629.228.228.831.4.table 4: number of sentences with multiple targets (#)and macro f1 on the target extraction task for head-ﬁnal and racl.
head-ﬁnal is consistently better thanracl on extracting multiple targets..(40.0/36.9 and 41.4/38.0 for head-ﬁrst and head-ﬁnal, respectively), but the nsf1 and sf1 are poor(24.5/17.4 and 26.1/18.8)..on average imn is the weakest baseline, fol-lowed by racl and then racl-bert.
the mainimprovement that racl-bert gives over raclon these datasets is seen in the targeted metric,i.e., the contextualized representations improve thepolarity classiﬁcation more than the extraction task.
the proposed graph-based models are consistentlythe best models across the metrics and datasets..regarding graph representations,.
the differ-ences between head-ﬁrst and head-ﬁnal are gen-erally quite small.
head-ﬁrst performs better onmultibca and slightly better on multibeu, whilefor the others (norecfine, mpqa, and dsunis)head-ﬁnal is better.
this suggests that the mainbeneﬁt is the joint prediction of all spans and rela-tionships, and that the speciﬁc graph representationmatters less..7 analysis.
in this section we perform a deeper analysis of themodels in order to answer the research questions..7.1 do syntactically informed sentiment.
graphs improve results?.
our two baseline graph representations, head-ﬁrstand head-ﬁnal, are crude approximations of lin-guistic structure.
in syntactic and semantic depen-dency graphs, heads are often neither the ﬁrst orlast word, but rather the most salient word accord-ing to various linguistic criteria.
first, we enrichthe dependency labels to distinguish edges that areinternal to a holder/target/expression span fromthose that are external and perform experimentsby adding an ‘in label’ to non-head nodes withinthe graph, which we call +inlabel.
we further in-form the head selection of the parsing graphs withsyntactic information in the dep.
edges parsing.
3393spans.
targeted.
graph.
sent.
graph.
holder f1.
target f1.
exp.
f1.
uf1.
lf1 nsf1.
sf1.
norecfinemultibeumultibcampqadsunis.
1.22.90.48.27.9.
5.00.61.68.81.2.
3.40.81.65.24.3.f1.
4.21.12.17.26.4.
2.81.02.06.63.9.
2.71.41.87.35.7.
4.61.23.35.43.6.
4.01.42.85.16.0.table 5: average gains in percentage points by including mbert representations..dep.
edges has the strongest positive effect onthe nsf1 and sf1 (an avg.
2.52 and 2.22 per-centage point (pp) over head-ﬁnal, respectively).
however, this average is pulled down by poorer per-formance on the english datasets.
removing thesetwo, the average beneﬁt is 5.2 and 4.2 for nsf1 andsf1, respectively.
on span extraction and targetedsentiment, however, dep.
edges leads to poorerscores overall.
dep.
labels does not lead to anyconsistent improvements.
these results indicatethat incorporating syntactic dependency informa-tion is particularly helpful for the full structuredsentiment task, but that these beneﬁts do not alwaysshow at a more local level, i.e., span extraction..7.2 do graph models perform better onsentences with multiple targets?.
we hypothesize that predicting the full sentimentgraph may have a larger effect on sentences withmultiple targets.
therefore, we create a subset ofthe test data containing sentences with multipletargets and reevaluate head-ﬁrst, head-ﬁnal, andracl-bert on the target extraction task.
table 4shows the number of sentences with multiple tar-gets and the target span extraction score for eachmodel.
on this subset, head-ﬁrst and head-ﬁnaloutperform racl-bert on 9 of 10 experiments,conﬁrming the hypothesis that the graph modelsimprove on examples with multiple targets..7.3 how much does mbert contribute?.
we also perform experiments without mbert(shown in table 7 in the appendix) and show theaverage gains (over all 6 graph setups) of includingit in table 5. adding the mbert features leadsto average improvements in all experiments: forextracting spans an average gain of 4.1 pp for hold-ers, 3.4 for targets, and 3.1 for expressions.
fortargeted sentiment there is a larger gain of 4.2 pp,while for the parsing graph metrics uf1 and lf1 thegains are more limited (3.3 pp/ 3.8 pp) and similarlyfor nsf1 and sf1 (3.6 pp/ 3.9 pp).
the gains are.
figure 3: average beneﬁt of each graph annotationscheme (y-axis) on the evaluation metrics (x-axis) inpercentage points.
the results are averaged acrossdatasets..graphs, where we compute the dependency graphfor each sentence9 and set the head of each spanto be the node that has an outgoing edge in thecorresponding syntactic graph.
as there can bemore than one such edge, we default to the ﬁrst.
a manual inspection showed that this approachsometimes set unlikely dependency label types asheads, e.g., punct, obl.
therefore, we suggesta ﬁnal approach, dep.
labels, which ﬁlters outthese unlikely heads.
the full results are shownin table 8 in the appendix.
the implementationof the graph structure has a large effect on all met-rics, although the speciﬁc results depend on thedataset.
we plot the average effect of each imple-mentation across all datasets in figure 3, as wellas each individual dataset (figures 4–8 in the ap-pendix).
+inlabel tends to improve results on thenon-english datasets, consistently increasing targetand expression extraction and targeted sentiment.
it also generally improves the graph scores uf1and lf1 on the non-english datasets..9we use spacy (honnibal et al., 2020) for english, stanza(qi et al., 2020) for basque and catalan and udpipe (strakaand strakov´a, 2017) for norwegian..3394holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first420247.4 analysis of polarity predictions.
references.
norecfinemultibeumultibcampqadsunis.
57.0 (1.5)75.7 (0.8)71.7 (2.4)38.5 (1.4)44.5 (2.4).
table 6: polarity f1 scores (unweighted and weighted)of models augmented with mbert on the head-ﬁnalsetup.
we report average and standard deviation over 5runs..largest for the english datasets (mpqa, dsunis)followed by norecfine, and ﬁnally multibca andmultibeu.
this corroborates the bias towards en-glish and similar languages that has been found inmultilingual language models (artetxe et al., 2020;conneau et al., 2020) and motivates the need forlanguage-speciﬁc contextualized embeddings..in this section we zoom in on polarity, in orderto quantify how well models perform at predict-ing only polarity.
as the polarity annotations arebound to the expressions, we consider true positivesto be any expression that overlaps the gold expres-sion and has the same polarity.
table 6 shows thatthe polarity predictions are best on and multibca,followed by norecfine and dsunis, and ﬁnallympqa.
this is likely due to the number of do-mains and characteristics of the data.
norecfinecontains many domains and has longer expressions,while mpqa contains many highly ambiguous po-lar expressions, e.g., ‘said’, ‘asked’, which havedifferent polarity depending on the context..8 conclusion.
in this paper, we have proposed a dependencygraph parsing approach to structured sentimentanalysis and shown that these models outperformstate-of-the-art sequence labeling models on ﬁvebenchmark datasets..using parse trees as input has shown promisefor sentiment analysis in the past, either to guidea tree-based algorithm (socher et al., 2013; taiet al., 2015) or to create features for sentimentmodels (nakagawa et al., 2010; almeida et al.,2015).
however, to the authors’ knowledge, thisis the ﬁrst attempt to directly predict dependency-based sentiment graphs..in the future, we would like to better exploitthe similarities between dependency parsing and.
sentiment graph parsing, either by augmenting thetoken-level representations with contextualized vec-tors from their heads in a dependency tree (kurtzet al., 2020) or by multi-task learning to depen-dency parse.
we would also like to explore differ-ent graph parsing approaches, e.g., perin (samueland straka, 2020)..acknowledgements.
this work has been carried out as part of the santproject (sentiment analysis for norwegian text),funded by the research council of norway (grantnumber 270908)..the computations were performed on resourcesprovided by uninett sigma2 - the national in-frastructure for high performance computing anddata storage in norway..abdulqader almars, xue li, xin zhao, ibrahim a.ibrahim, weiwei yuan, and bohan li.
2017. struc-tured sentiment analysis.
in advanced data miningand applications, pages 695–707, cham.
springerinternational publishing..mariana s. c. almeida, cl´audia pinto, helena figueira,pedro mendes, and andr´e f. t. martins.
2015.aligning opinions: cross-lingual opinion miningwith dependencies.
in proceedings of the 53rd an-nual meeting of the association for computationallinguistics and the 7th international joint confer-ence on natural language processing (volume 1:long papers), pages 408–418, beijing, china.
as-sociation for computational linguistics..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..jeremy barnes, toni badia, and patrik lambert.
2018.multibooked: a corpus of basque and catalan ho-tel reviews annotated for aspect-level sentiment clas-in proceedings of the eleventh interna-siﬁcation.
tional conference on language resources and eval-uation (lrec-2018), miyazaki, japan.
europeanlanguages resources association (elra)..taylor berg-kirkpatrick, david burkett, and danklein.
2012. an empirical investigation of statis-in proceedings of thetical signiﬁcance in nlp.
2012 joint conference on empirical methods in nat-ural language processing and computational nat-ural language learning, pages 995–1005, jeju is-land, korea.
association for computational linguis-tics..3395jari bj¨orne,.
juho heimonen, filip ginter, anttiairola, tapio pahikkala, and tapio salakoski.
2009.extracting complex biological events with richin proceedings of thegraph-based feature sets.
workshop on current trends in biomedical naturallanguage processing: shared task, bionlp ’09,pages 10–18, stroudsburg, pa, usa.
associationfor computational linguistics..zhuang chen and tieyun qian.
2020. relation-awarecollaborative learning for uniﬁed aspect-based sen-timent analysis.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 3685–3694, online.
association forcomputational linguistics..yejin choi, eric breck, and claire cardie.
2006. jointextraction of entities and relations for opinion recog-in proceedings of the 2006 conference onnition.
empirical methods in natural language processing,pages 431–439, sydney, australia.
association forcomputational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..hang cui, renxu sun, keya li, min-yen kan, and tat-seng chua.
2005. question answering passage re-trieval using dependency relations.
in proceedingsof the 28th annual international acm sigir con-ference on research and development in informa-tion retrieval, sigir ’05, pages 400–407, salvador,brazil.
association for computing machinery..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2018.simpler but more accurate semantic dependencyin proceedings of the 56th annual meet-parsing.
ing of the association for computational linguis-tics (volume 2: short papers), pages 484–490, mel-bourne, australia.
association for computationallinguistics..murhaf fares, andrey kutuzov, stephan oepen, anderik velldal.
2017. word vectors, reuse, and replica-bility: towards a community repository of large-textin proceedings of the 21st nordic con-resources.
ference on computational linguistics, pages 271–276, gothenburg, sweden.
association for compu-tational linguistics..ruidan he, wee sun lee, hwee tou ng, and danieldahlmeier.
2019. an interactive multi-task learn-ing network for end-to-end aspect-based sentimentanalysis.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 504–515, florence, italy.
association forcomputational linguistics..matthew honnibal,.
ines montani, soﬁe van lan-deghem,spacy:and adriane boyd.
2020.industrial-strength natural language processing inpython..long jiang, mo yu, ming zhou, xiaohua liu, andtiejun zhao.
2011. target-dependent twitter senti-ment classiﬁcation.
in proceedings of the 49th an-nual meeting of the association for computationallinguistics: human language technologies, pages151–160, portland, oregon, usa.
association forcomputational linguistics..richard johansson and alessandro moschitti.
2012.relational features in fine-grained opinion anal-ysis.
computational linguistics, 39(3):473–509..arzoo katiyar and claire cardie.
2016. investigatinglstms for joint extraction of opinion entities andrelations.
in proceedings of the 54th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 919–929, berlin,germany.
association for computational linguis-tics..marco kuhlmann and stephan oepen.
2016. towardsa catalogue of linguistic graph banks.
computa-tional linguistics, 42(4):819–827..robin kurtz, stephan oepen, and marco kuhlmann.
2020. end-to-end negation resolution as graph pars-ing.
in proceedings of the 16th international con-ference on parsing technologies and the iwpt 2020shared task on parsing into enhanced universal de-pendencies, pages 14–24, online.
association forcomputational linguistics..emanuele lapponi, erik velldal, lilja øvrelid, andjonathon read.
2012. uio2: sequence-labelingnegation using dependency features.
in proceed-ings of the first joint conference on lexical andcomputational semantics - volume 1: proceedingsof the main conference and the shared task, andvolume 2: proceedings of the sixth internationalworkshop on semantic evaluation, semeval ’12,pages 319–327, stroudsburg, pa, usa.
associationfor computational linguistics..xin li, lidong bing, piji li, and wai lam.
2019a.
auniﬁed model for opinion target extraction and targetin proceedings of the aaaisentiment prediction.
conference on artiﬁcial intelligence, pages 6714–6721..xin li, lidong bing, wenxuan zhang, and wai lam.
2019b.
exploiting bert for end-to-end aspect-based sentiment analysis.
in proceedings of the 5th.
3396workshop on noisy user-generated text (w-nut2019), pages 34–41, hong kong, china.
associationfor computational linguistics..parsing.
proceedings of the 9th international work-shop on semantic evaluation (semeval 2015), pages915–926..bing liu.
2012. sentiment analysis and opinion min-.
ing.
morgan & claypool publishers..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation ex-in proceedings oftraction without labeled data.
the joint conference of the 47th annual meeting ofthe acl and the 4th international joint conferenceon natural language processing of the afnlp,pages 1003–1011, suntec, singapore.
associationfor computational linguistics..margaret mitchell, jacqui aguilar, theresa wilson,and benjamin van durme.
2013. open domain tar-in proceedings of the 2013 con-geted sentiment.
ference on empirical methods in natural languageprocessing, pages 1643–1654, seattle, washington,usa.
association for computational linguistics..roser morante and eduardo blanco.
2012..*sem2012 shared task: resolving the scope and focusof negation.
in *sem 2012: the first joint confer-ence on lexical and computational semantics – vol-ume 1: proceedings of the main conference and theshared task, and volume 2: proceedings of the sixthinternational workshop on semantic evaluation (se-meval 2012), pages 265–274, montr´eal, canada.
as-sociation for computational linguistics..roser morante.
and walter daelemans..2012.conandoyle-neg: annotation of negation cuesin pro-and their scope in conan doyle stories.
ceedings of the eighth international conference onlanguage resources and evaluation (lrec’12),istanbul, turkey.
europeanpages 1563–1568,language resources association (elra)..tetsuji nakagawa, kentaro inui, and sadao kurohashi.
2010. dependency tree-based sentiment classiﬁca-tion using crfs with hidden variables.
in humanlanguage technologies: the 2010 annual confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 786–794,los angeles, california.
association for computa-tional linguistics..stephan oepen, omri abend, lasha abzianidze, jo-han bos, jan hajic, daniel hershcovich, bin li,tim o’gorman, nianwen xue, and daniel zeman.
2020. mrp 2020: the second shared task oncross-framework and cross-lingual meaning rep-in proceedings of the conllresentation parsing.
2020 shared task: cross-framework meaning rep-resentation parsing, pages 1–22, online.
associa-tion for computational linguistics..stephan oepen, marco kuhlmann, yusuke miyao,daniel zeman, silvie cinkova, dan flickinger, janhajic, and zdenka uresova.
2015. semeval 2015task 18: broad-coverage semantic dependency.
stephan oepen, marco kuhlmann, yusuke miyao,daniel zeman, dan flickinger, jan hajic, angelinaivanova, and yi zhang.
2014. semeval 2014 task8: broad-coverage semantic dependency parsing.
proceedings of the 8th international workshop onsemantic evaluation (semeval 2014), pages 63–72..lilja øvrelid, petter mæhlum, jeremy barnes, and erikvelldal.
2020. a ﬁne-grained sentiment dataset forin proceedings of the 12th languagenorwegian.
resources and evaluation conference, pages 5025–5033, marseille, france.
european language re-sources association..haiyun peng, lu xu, lidong bing, fei huang, wei lu,and luo si.
2019. knowing what, how and why:a near complete solution for aspect-based sentimentanalysis..maria pontiki, dimitris galanis, haris papageorgiou,ion androutsopoulos, suresh manandhar, moham-mad al-smadi, mahmoud al-ayyoub, yanyanzhao, bing qin, orph´ee de clercq, v´eroniquehoste, marianna apidianaki, xavier tannier, na-talia loukachevitch, evgeniy kotelnikov, nuria bel,salud mar´ıa jim´enez-zafra, and g¨uls¸en eryi˘git.
2016. semeval-2016 task 5: aspect based senti-ment analysis.
in proceedings of the 10th interna-tional workshop on semantic evaluation (semeval-2016), pages 19–30, san diego, california.
associa-tion for computational linguistics..maria pontiki, dimitris galanis, haris papageorgiou,suresh manandhar, and ion androutsopoulos.
2015.semeval-2015 task 12: aspect based sentimentin proceedings of the 9th internationalanalysis.
workshop on semantic evaluation (semeval 2015),pages 486–495, denver, colorado.
association forcomputational linguistics..maria pontiki, dimitris galanis, john pavlopoulos,harris papageorgiou,ion androutsopoulos, andsuresh manandhar.
2014. semeval-2014 task 4: as-pect based sentiment analysis.
in proceedings of the8th international workshop on semantic evaluation(semeval 2014), pages 27–35, dublin, ireland.
as-sociation for computational linguistics..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational linguis-tics..david samuel and milan straka.
2020..´ufal atmrp 2020: permutation-invariant semantic pars-ing in perin.
in proceedings of the conll 2020.
3397shared task: cross-framework meaning represen-tation parsing, pages 53–64, online.
association forcomputational linguistics..processing and computational natural languagelearning, pages 1335–1345, jeju island, korea.
as-sociation for computational linguistics..juntao yu, bernd bohnet, and massimo poesio.
2020.named entity recognition as dependency parsing.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages6470–6476, online.
association for computationallinguistics..meishan zhang, qiansheng wang, and guohong fu.
2019. end-to-end neural opinion extraction with atransition-based model.
information systems, 80:56– 63..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..milan straka and jana strakov´a.
2017. tokenizing,pos tagging, lemmatizing and parsing ud 2.0 withudpipe.
in proceedings of the conll 2017 sharedtask: multilingual parsing from raw text to univer-sal dependencies, pages 88–99, vancouver, canada.
association for computational linguistics..kai sheng tai, richard socher, and christopher d.manning.
2015. improved semantic representationsfrom tree-structured long short-term memory net-in proceedings of the 53rd annual meet-works.
ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing (volume 1: long papers),pages 1556–1566, beijing, china.
association forcomputational linguistics..cigdem toprak, niklas jakob, and iryna gurevych.
2010. sentence and expression level annotation ofin proceed-opinions in user-generated discourse.
ings of the 48th annual meeting of the associationfor computational linguistics, pages 575–584, up-psala, sweden.
association for computational lin-guistics..wenya wang, sinno jialin pan, daniel dahlmeier, andxiaokui xiao.
2016. recursive neural conditionalrandom ﬁelds for aspect-based sentiment analysis.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages616–626, austin, texas.
association for computa-tional linguistics..janyce wiebe, theresa wilson, and claire cardie.
2005. annotating expressions of opinions and emo-tions in language.
language resources and evalua-tion, 39(2-3):165–210..hu xu, bing liu, lei shu, and philip yu.
2019. bertpost-training for review reading comprehension andaspect-based sentiment analysis.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 2324–2335, minneapolis,minnesota.
association for computational linguis-tics..bishan yang and claire cardie.
2012. extracting opin-ion expressions with semi-markov conditional ran-dom ﬁelds.
in proceedings of the 2012 joint con-ference on empirical methods in natural language.
3398a appendix.
figure 4: average beneﬁt of each graph annotationscheme (y-axis) on the evaluation metrics (x-axis) fornorecfine..figure 7: average beneﬁt of each graph annotationscheme (y-axis) on the evaluation metrics (x-axis) inpercentage points for mpqa..figure 5: average beneﬁt of each graph annotationscheme (y-axis) on the evaluation metrics (x-axis) formultibeu..figure 8: average beneﬁt of each graph annotationscheme (y-axis) on the evaluation metrics (x-axis in per-centage points) for dsunis..figure 6: average beneﬁt of each graph annotationscheme (y-axis) on the evaluation metrics (x-axis) inpercentage points for multibca..figure 9: average beneﬁt of each graph annota-tion scheme (y-axis) on the evaluation metrics (x-axis) in percentage points.
the results on norecfine,multibeu, multibca..3399holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first42024holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first42024holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first42024holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first42024holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first42024holder f1target f1exp f1targeted f1uf1lf1nsf1sf1dep.
labelsdep.
edgeshead-final+inlabelhead-finalhead-first+inlabelhead-first42024spans.
targeted.
graph.
sent.
graph.
holder f1.
target f1.
exp.
f1.
uf1.
lf1.
nsf1.
sf1.
enifceron.ueb.itlum.acb.itlum.aqpm.sinusd.imnracl.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
imnracl.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
imnracl.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
imnracl.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
imnracl.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
--.
--.
--.
--.
--.
48.4 (2.2)50.4 (4.0)57.0 (3.3)57.9 (1.7)54.4 (3.9)51.6 (2.6).
60.8 (3.8)59.8 (1.6)57.0 (2.0)53.7 (1.2)53.1 (1.9)52.0 (3.8).
41.9 (2.8)42.4 (2.6)40.4 (2.5)36.4 (2.2)42.6 (6.1)43.8 (3.4).
35.2 (1.1)35.6 (1.4)37.1 (1.3)37.0 (0.5)35.4 (1.5)36.7 (0.6).
25.6 (5.1)22.9 (5.7)29.2 (8.4)30.2 (8.9)33.9 (5.4)21.3 (18.1).
35.945.6.
47.1 (1.6)47.6 (2.5)49.4 (0.9)50.1 (1.3)49.0 (2.5)46.5 (3.0).
48.255.4.
64.1 (1.4)64.3 (0.9)66.0 (1.6)64.0 (2.4)63.8 (1.7)63.0 (1.1).
56.365.4.
69.8 (1.7)70.9 (0.8)69.9 (1.5)69.1 (1.1)69.1 (0.5)70.3 (0.8).
24.332.6.
40.5 (1.8)41.6 (1.1)42.1 (1.0)42.3 (1.2)39.1 (2.0)39.2 (2.3).
33.039.3.
36.8 (3.5)38.6 (3.9)38.1 (2.0)38.2 (2.9)39.2 (2.9)40.0 (1.5).
48.755.4.
52.0 (1.6)51.0 (1.3)52.1 (1.8)52.6 (0.4)51.4 (1.7)50.7 (2.7).
65.270.7.
72.2 (0.7)71.9 (0.8)72.2 (0.6)72.9 (0.6)71.0 (1.1)71.3 (1.9).
60.967.6.
68.9 (1.4)69.9 (0.9)66.8 (0.8)65.4 (0.6)67.3 (0.6)67.2 (1.1).
29.637.8.
41.7 (1.7)42.4 (2.3)41.9 (0.8)41.6 (1.6)41.6 (1.1)40.4 (2.1).
27.440.2.
39.0 (1.5)38.6 (2.8)39.5 (2.4)38.8 (2.0)39.3 (3.4)38.4 (2.4).
f1.
18.020.1.
33.0 (1.4)27.3 (1.1)26.0 (0.6)29.6 (0.6)26.7 (3.1)26.7 (1.9).
39.548.2.
53.9 (1.8)57.9 (1.9)55.5 (1.7)54.9 (2.0)53.7 (1.7)54.0 (1.0).
32.553.1.
57.3 (2.0)50.9 (1.3)50.8 (2.6)52.9 (0.9)50.6 (1.3)50.6 (1.8).
1.211.8.
22.6 (3.1)14.0 (0.9)13.3 (1.9)15.2 (2.5)12.3 (0.8)12.6 (1.2).
17.922.8.
23.4 (1.8)18.3 (2.7)21.8 (1.1)24.4 (2.7)22.2 (2.7)21.4 (4.3).
--.
-.
-.
-.
-.
--.
-.
-.
-.
-.
--.
-.
-.
-.
-.
--.
-.
-.
-.
-.
37.6 (0.5)36.9 (0.5)45.1 (1.2)45.0 (1.0)39.3 (1.1)36.7 (1.1).
29.8 (0.4)29.4 (0.8)35.2 (1.1)35.2 (0.5)31.5 (1.3)28.3 (0.8).
32.9 (1.6)32.9 (1.1)34.4 (0.7)35.1 (1.6)47.2 (0.9)33.4 (1.8).
26.1 (1.5)25.8 (0.5)27.2 (0.9)27.0 (1.3)36.0 (1.1)25.4 (1.8).
62.9 (0.6)62.6 (0.6)60.2 (0.8)60.1 (1.5)59.0 (1.3)59.5 (1.1).
58.2 (0.3)57.5 (1.1)55.5 (0.9)54.9 (1.7)54.5 (1.6)54.9 (1.1).
58.5 (2.3)57.3 (1.5)59.6 (0.8)57.1 (3.2)59.0 (1.6)58.6 (2.8).
54.7 (2.6)53.6 (1.3)56.3 (1.0)53.5 (3.3)55.6 (1.8)54.6 (2.4).
64.2 (0.7)64.4 (0.9)60.9 (0.6)60.6 (0.9)59.3 (0.7)61.0 (0.5).
59.9 (0.8)59.6 (0.7)57.1 (0.8)57.0 (0.6)55.7 (1.1)57.1 (0.8).
58.2 (2.3)55.7 (2.0)57.7 (1.0)58.0 (1.9)57.5 (2.1)57.8 (1.4).
53.3 (2.2)50.7 (2.1)53.3 (1.3)53.5 (2.0)52.8 (1.8)52.7 (1.9).
32.2 (1.3)32.9 (0.8)35.7 (0.8)35.5 (0.7)28.9 (1.7)28.9 (1.6).
28.2 (1.4)28.9 (0.9)31.7 (0.5)31.7 (0.6)24.8 (1.5)24.5 (1.1).
19.4 (1.5)20.4 (1.0)18.7 (0.7)19.6 (0.6)19.0 (0.9)18.9 (0.8).
12.4 (1.7)13.2 (1.2)12.5 (1.6)12.6 (0.8)11.9 (1.1)11.6 (1.0).
32.9 (1.7)33.7 (2.8)31.1 (2.1)32.4 (2.1)32.4 (2.1)32.1 (1.5).
25.9 (1.7)26.4 (1.6)26.0 (1.0)28.4 (2.1)26.8 (2.0)27.2 (1.5).
27.8 (1.4)25.9 (3.4)29.1 (3.3)28.1 (2.9)29.3 (1.5)28.2 (1.1).
18.8 (2.0)15.2 (2.2)20.4 (2.0)22.4 (3.4)19.8 (1.2)20.5 (3.1).
table 7: experiments without contextualized embeddings..3400racl-bert.
59.9.
72.6.
56.8.racl-bert.
67.5.
70.3.
52.4.holder f1.
target f1.
exp.
f1.
racl-bert.
47.2.
56.3.eni.fceron.ueb.itlum.acb.itlum.aqpm.sinusd.head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
head-ﬁrst+inlabelhead-ﬁnal+inlabeldep.
edgesdep.
labels.
spans.
targeted.
graph.
sent.
graph.
51.1 (3.2)51.6 (2.8)60.4 (1.2)57.1 (3.0)54.0 (3.4)52.7 (5.6).
60.4 (2.2)59.6 (1.9)60.5 (2.2)58.1 (2.4)58.8 (4.2)56.3 (2.1).
43.0 (1.3)43.1 (2.2)37.1 (4.2)34.9 (4.1)46.3 (3.1)45.6 (2.9).
43.8 (1.8)43.1 (1.5)46.3 (1.8)45.6 (2.5)44.0 (1.5)43.7 (0.9).
-.
-.
-.
-.
-.
28.0 (7.7)30.9 (9.9)37.4 (11.6)30.6 (16.4)32.7 (12.1)30.8 (5.8).
50.1 (3.4)52.7 (0.7)54.8 (1.6)55.2 (1.0)53.6 (1.5)53.6 (0.3).
64.0 (2.4)65.9 (0.9)64.0 (2.3)64.7 (1.1)64.8 (1.4)65.4 (0.9).
72.5 (1.0)73.4 (1.0)71.2 (0.6)70.7 (1.4)70.3 (0.6)70.3 (1.1).
51.0 (1.9)51.5 (1.0)49.5 (0.9)49.4 (2.1)48.5 (1.2)47.7 (2.3).
39.9 (2.2)38.4 (3.3)42.1 (2.7)38.9 (3.1)39.9 (2.8)38.9 (0.9).
54.4 (1.6)54.6 (1.4)55.5 (1.5)56.3 (1.3)55.0 (0.9)54.4 (1.5).
73.9 (1.0)74.2 (0.7)72.1 (1.2)72.0 (0.7)71.2 (0.8)72.9 (1.1).
71.1 (0.8)70.3 (1.0)67.1 (1.7)68.2 (1.0)69.2 (1.4)69.1 (1.7).
48.1 (0.8)47.5 (1.1)46.0 (1.1)45.6 (1.1)46.3 (1.9)47.5 (0.8).
40.3 (0.6)40.6 (2.9)45.5 (2.4)45.2 (2.7)44.8 (4.0)43.1 (1.2).
f1..30.3.
30.5 (2.3)32.2 (1.4)31.9 (1.3)34.8 (1.0)32.7 (1.6)32.7 (1.6).
57.8 (2.4)59.2 (0.9)56.9 (1.7)58.5 (1.4)54.0 (1.8)54.9 (0.8).
55.0 (0.9)55.8 (1.8)53.9 (2.2)53.5 (0.7)53.4 (1.5)53.9 (1.5).
33.5 (3.1)21.3 (0.4)21.9 (1.4)20.7 (1.0)18.9 (2.3)21.9 (0.7).
26.7 (2.1)26.7 (2.4)29.6 (1.7)28.1 (3.7)28.9 (3.6)27.8 (1.9).
uf1.
-.
39.2 (0.5)39.6 (0.8)48.0 (1.3)48.7 (1.2)41.5 (0.7)40.7 (0.8).
64.6 (1.0)64.7 (0.7)60.8 (0.8)60.6 (1.1)59.9 (0.4)60.0 (0.9).
66.8 (0.5)66.2 (0.3)62.7 (0.4)63.4 (0.5)60.8 (0.4)62.5 (0.6).
40.0 (1.0)40.6 (0.5)41.4 (0.7)40.4 (1.5)35.4 (1.3)35.6 (1.2).
35.3 (0.9)34.2 (2.1)38.1 (1.9)37.3 (2.7)37.3 (2.5)35.7 (1.5).
-.
-.
-.
-.
lf1.
-.
31.5 (0.5)32.0 (0.7)37.7 (1.4)38.3 (1.0)33.8 (0.4)32.2 (0.5).
60.0 (1.6)60.3 (1.1)56.0 (1.1)56.6 (0.7)55.5 (0.7)55.6 (0.8).
62.1 (0.5)61.5 (0.6)58.1 (0.8)58.7 (0.6)57.5 (0.6)59.1 (0.6).
36.9 (1.2)37.5 (0.5)38.0 (0.5)37.2 (1.9)31.9 (1.2)32.0 (1.3).
31.4 (1.3)30.7 (2.5)33.9 (2.3)33.3 (2.1)33.8 (2.7)32.1 (1.6).
-.
-.
-.
-.
nsf1.
-.
-.
-.
-.
-.
37.0 (2.6)37.6 (1.2)39.2 (1.7)40.5 (1.1)50.9 (0.3)38.2 (1.4).
58.0 (1.1)59.8 (1.1)58.0 (2.1)59.8 (1.6)60.9 (1.6)60.5 (1.1).
62.0 (1.1)61.1 (1.0)59.7 (1.1)60.9 (1.1)60.7 (1.0)60.4 (1.0).
24.5 (2.3)24.5 (1.3)26.1 (0.7)25.2 (1.7)24.2 (1.6)24.0 (0.8).
31.0 (1.4)30.5 (2.1)34.3 (4.2)29.4 (2.8)33.2 (4.5)31.3 (1.1).
sf1.
-.
29.5 (2.4)29.5 (1.2)31.2 (1.6)31.7 (1.1)39.4 (0.4)30.0 (1.2).
54.7 (1.6)56.1 (1.6)54.7 (1.8)56.9 (1.8)57.4 (1.6)57.1 (1.1).
56.8 (0.7)56.0 (1.0)53.7 (2.4)55.1 (1.2)55.6 (0.9)55.8 (1.2).
17.4 (2.7)17.3 (1.0)18.8 (0.7)17.8 (1.3)16.3 (1.9)17.2 (0.8).
25.0 (1.3)25.4 (2.3)26.5 (3.5)23.7 (2.4)27.3 (4.1)25.3 (2.0).
-.
-.
-.
-.
table 8: experiments with mbert..racl-bert.
20.0.
31.2.
17.8.racl-bert.
44.6.
38.2.
27.3.
3401nvidia p100, 16 gib ramintel xeon-gold 6138 2.0 ghz00:31:43 (multibeu) – 07:40:54 (norecfine)https://github.com/jerbarnes/sentiment_graphs/srcbest assignmentword2vec skipgram 100d.
gpu infrastructurecpu infrastructuretraining durationmodel implementationhyperparameterembeddingcontexualized embedding mbertembeddings trainablenumber of epochsbatch sizebeta1beta2l2hidden lstmhidden char lstmlayers lstmdim mlpdim embeddingdim char embeddingearly stoppingpos styleattentionmodel interpolationloss interpolationlstm implementationchar implementationemb dropout typebridgedropout embeddingdropout edgedropout labeldropout main recurrentdropout recurrent chardropout main ffdropout char ffdropout char linear.
false1005000.953e-092001003200100800xposbilinear0.50.025drop connectconvolvedreplacedpa+0.20.20.30.20.30.40.30.3.
3402