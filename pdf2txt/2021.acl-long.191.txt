protaugment: unsupervised diverse short-texts paraphrasing for intentdetection meta-learningthomas dopierre12 and christophe gravier1 andand wilfried logerais2.
1laboratoire hubert curienumr cnrs 5516universit´e jean monnetsaint- ´etienne, francefirstname.lastname@univ-st-etienne.fr.
abstract.
recent research considers few-shot intent de-tection as a meta-learning problem: the modelis learning to learn from a consecutive setof small tasks named episodes.
in this work,we propose protaugment, a meta-learningalgorithm for shorttexts classiﬁcation ap-plied to the intent detection task.
protaug-ment is a novel extension of prototypical net-works (snell et al., 2017) that limits over-ﬁttingon the bias introduced by the few-shots classi-ﬁcation objective at each episode.
it relies ondiverse paraphrasing: a conditional languagemodel is ﬁrst ﬁne-tuned for paraphrasing, anddiversity is later introduced at the decodingstage at each meta-learning episode.
the di-verse paraphrasing is unsupervised as it is ap-plied to unlabelled data and then fueled to theprototypical network training objective as aconsistency loss.
protaugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and with-out the need to ﬁne-tune a conditional languagemodel on a given application domain..1.introduction.
intent detection, a sub-ﬁeld of text classiﬁcation,involves classifying user-generated short-texts intointent classes, usually for conversational agentsapplications (casanueva et al., 2020).
since con-versational agent applications are domain-speciﬁc,intent detection is a challenging task because oflabeled data scarcity and the number of classes (in-tents) it usually involves (dopierre et al., 2020).
as a consequence, recent research (snell et al.,2017; ren et al., 2018) considers few-shot intentdetection as a meta-learning problem: the modelis trained to classify user utterances from a con-secutive set of small tasks named episodes.
eachepisode contains a limited number of c classesalongside a limited number of k labeled data foreach of the c classes – this is usually referred to as.
2meeticparis, france.
{t.dopierre,w.logerais}@meetic-corp.com.
a c-way k-shots setup.
at test time, the algorithmis evaluated on classes that were not seen duringtraining.
that is the reason why meta-learningis sometimes referred to as learning to learn: itmimics human abilities to learn iteratively fromdifferent and small tasks.
meta-learning has suc-cessfully been applied to a wide set of nlp tasks:hypernym detection (yu et al., 2020), low resourcemachine translation (gu et al., 2018), machine un-derstanding tasks (dou et al., 2019) or structuredquery generation (huang et al., 2018).
most meta-learning algorithms (section 2) were developed inthe course of the last 5 years.
it has recently beenempirically demonstrated that comparative studiesin follow-up papers of (snell et al., 2017) are debat-able – for short texts classiﬁcation – because of thetwo following main issues (dopierre et al., 2021).
first, comparative studies involve simple and lim-ited datasets in terms of number and separability ofclasses (snips (coucke et al., 2018), a very popu-lar dataset, includes only 7 classes, with the currentbest model performing over 99% accuracy (caoet al., 2020)).
second, as we further better under-stand (niven and kao, 2019), ﬁne-tune (liu et al.,2019b; hao et al., 2020) and reﬁne (khetan andkarnin, 2020) bert-derived models, it is not clearif the different meta-learning frameworks can beconsidered state-of-the-art due to their architectureor due to the improvements of available text en-coders at the time of conception.
(dopierre et al.,2021) concludes that prototypical networks (snellet al., 2017) (that were using lstm-based text en-coders when introduced in nlp) are actually thestate-of-the-art for intent detection when equippedwith a ﬁne-tuned bert text encoder model.
ul-timately, improving prototypical networks havetherefore been proven to be a very challenging taskin reality..a cornerstone challenge is that meta-learningmodels can easily overﬁt on the biased distribution.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2454–2466august1–6,2021.©2021associationforcomputationallinguistics2454introduced by a few training examples (yang et al.,2021).
in order to prevent overﬁtting and inspiredby (xie et al., 2020), we introduce an unsuper-vised diverse paraphrasing loss in the prototypicalnetworks framework.
a key idea is consistencylearning: by augmenting unlabeled user utterances,protaugment enforce a more robust text repre-sentation learning.
unfortunately, back-translationis a poor data augmentation strategy for short-texts:neural machine translation provides very similar(if not the same) sentences to the original ones,which hinders its ability to provide diverse augmen-tations (section 5.3).
consequently, in this work,we transfer a denoising autoencoder pre-trained onthe sequence-to-sequence task (lewis et al., 2020)to the paraphrase generation task and then use it togenerate paraphrases.
as ﬁne-tuning is very efﬁ-cient for such a model, it is not easy to optimizeit for diverse paraphrasing.
(goyal and durrett,2020) presents an approach for diverse paraphras-ing that reorders the original sentence to guide theconditional language model to generate diverse sen-tences.
the diversity in that work is provided bythe reordering of the elements, which surprisinglyaffects the attention mechanism.
in (liu et al.,2020), expression diversity is part of the unsuper-vised paraphrasing system supported by simulatedannealing.
both approaches imply domain trans-fer, and consequently, as many diverse paraphras-ing models to maintain as the number of consid-ered application domains, which do not scale verywell.
in this work, we instead introduce diversity inthe downstream decoding algorithm used for para-phrase generation.
diverse decoding methods aremostly extensions to the beam search algorithm,including noise-based algorithms (cho, 2016), it-erative beam search (kulikov et al., 2019), clus-tered beam search (tam, 2020) and diverse beamsearch (vijayakumar et al., 2018).
there is no clearoptimal solution, the choice is task-speciﬁc anddependent on one’s tolerance for lower quality out-puts as a diversity/ﬂuency trade-off (ippolito et al.,2019).
while diverse beam search allows control-ling the diversity/ﬂuency trade-off partially, we fur-ther demonstrate that adding constraints to diversebeam search in order to generate tokens not seenin the input sentence (that is, constrained diversebeam search) is a simple yet powerful strategy tofurther improve the diversity of the paraphrases.
paired with paraphrasing user utterances and itsconsistency loss incorporated in prototypical net-.
works, our model is the best method for intent de-tection meta-learning on 4 public datasets, withneither extra labeling efforts nor domain-speciﬁcconditional language model ﬁne-tuning.
we alsoshow that protaugment, having access to only10 samples of each class of the training data, stillsigniﬁcantly outperforms a prototypical networkwhich is given access to all samples of the sametraining data..2 neural architectures for meta-learning.
past works on meta-learning for classiﬁcation tasksinvestigate how to best predict a query point’s classat an episode scale.
this process is bounded to theset of the c classes considered in a given episode.
matching networks (vinyals et al., 2016) predictthe class of a query point as the average cosine dis-tance between the query vector and all support vec-tors for each class.
prototypical networks (snellet al., 2017) extend matching networks: after ob-taining support vectors from the encoder, a classprototype is produced via a class-wise vector aver-aging operation.
all query points are then predictedwith respect to their distance (cosine or euclidean)to all prototypes.
like prototypical networks, re-lation networks (sung et al., 2018) emerged fromcomputer vision application and were later suc-cessfully applied to nlp (zhang et al., 2018).
theyintroduce a relation module, which captures the re-lationship between data points: instead of using apre-deﬁned distance (euclidean or cosine most ofthe time), this approach allows such networks tolearn this metric by themselves.
this is achievedusing either a shallow feed-forward sub-networkor a neural tensor layer relation module (socheret al., 2013) (intermediate learnable matrices).
an-other extension to prototypical networks is pro-vided in (ren et al., 2018).
unlabeled data areincorporated using two distinct approaches: i) tak-ing unlabeled data from the same classes as theepisode or ii) using any unlabeled data and incorpo-rating both a distractor cluster and masking strategyto minimize the impact of distant unlabeled points.
the ﬁrst approach is unrealistic for meta-learning,as it implies knowing the unlabeled data class.
thesecond method assumes that all the noise is cen-tered around a single distractor cluster and intro-duces an additional hyperparameter for masking– which is hardly ﬁne-tuneable for small few-shotdatasets..24553 background.
3.1 notations.
meta-learning algorithms are trained using a spe-ciﬁc procedure made of consecutive episodes.
letcep be the set of c classes sampled for the cur-rent training episode, such as cep ⊂ ctrain, wherectrain is the set of all classes available for train-ing.
we note ctest, the set of classes used fortesting, with ctrain ∩ ctest = ∅.
each classc ∈ cep comes with k labeled samples, usedas support.
the set of c × k samples are usu-ally referred to as s, the support set, so that s ={(x1, y1), .
.
.
, (xc×k, yc×k)}.
we denote sc theset of support examples labeled with class c. eachepisode comes with a query set q, which servesas the episode-scale optimization – the model pa-rameters are updated based on the prediction losson q, given s as an input.
qc is the set of queryexamples labeled with class c..3.2 prototypical networks.
in prototypical networks, each class is mapped to arepresentative point, called prototype.
each sampleis ﬁrst encoded into a vector using an embeddingfunction fφ with learnable parameters φ – this isthe function we want to optimize.
using theseembeddings, we compute each prototype pc, c ∈cep as the mean vector of embedded support pointsbelonging to the class c, as described in equation 1..pc =.
1k.(cid:88).
fφ(xi).
(xi,yi)∈sc.
(1).
given those prototypes and a distance function d,prototypical networks assign a label to a querypoint by computing the softmax over distances be-tween this point’s embedding and the prototypes,as in equation 2. in the original paper, (snell et al.,2017) use the euclidean distance and we also ob-served consistent slightly worse results with thecosine distance..pφ(y = c|x) = softmax (−d(fφ(x), pc)).
(2).
the supervised loss function ¯l is the average nega-tive log-probability of the correct class assignmentsfor all query points.
at test time, episodes arecreated using classes from ctest, and accuracy ismeasured as the query points assignments, givenprototypes derived from the support points..4 protaugment.
in this section, we present our semi-supervised ap-proach protaugment.
along with the labeleddata randomly chosen at each episode, this ap-proach uses u unlabeled data randomly drawnfrom the whole dataset – that is, data from train-ing, validation, and test labels.
we ﬁrst do a dataaugmentation step from this unlabeled data, wherewe obtain m paraphrases for each unlabeled sen-tence.
the mth paraphrase of x will be denoted ˜xm.
then, given unlabeled data and their paraphrases,we compute a fully unsupervised loss.
finally, wecombine both the supervised loss ¯l (the prototypi-cal network loss using labeled data) and unsuper-vised loss (denoted ˜l) and run back-propagation toupdate the model’s parameters..4.1 generating augmentations through.
paraphrasing.
the bart (lewis et al., 2020) modelis atransformer-based neural machine translation ar-chitecture that is trained to remove artiﬁcially cor-rupted text from the input thanks to an autoencoderarchitecture.
while it is trained to reconstruct theoriginal noised input, it can be ﬁne-tuned for task-speciﬁc conditional generation by minimizing thecross-entropy loss on new training input-outputpairs (bevilacqua et al., 2020).
in protaugment,we ﬁne-tune a pre-trained bart model on the para-phrasing task.
the paraphrase sentence pairs weuse for this task are taken from 3 different para-phrase detection datasets1: quora (sharma et al.,2019), msr (zhao and wang, 2010), and googlepaws-wiki (yang et al., 2019; zhang et al., 2019).
those datasets have different sizes, and the largestone – quora – consist of 149,263 pairs of du-plicate questions.
to balance turns of sentences(questions/non questions paraphrases), 50% of ourﬁne-tuning paraphrase datasets is made of quora,5.6% of msr and 44.4% paws-wiki.
this yields94,702 sentence pairs to train the model on theparaphrasing task.
we include both code and dataon our github repository 2..using this ﬁne-trained paraphrasing model, wecan generate paraphrases of unlabeled sentences,hopefully having paraphrases representing thesame intents as the original sentences.
to add somediversity in the generated paraphrases, we use di-.
1we take only pairs that are paraphrases of each other since.
these are paraphrase detection datasets.
2https://github.com/tdopierre/protaugment.
2456figure 1: protaugment illustrated on a 3-way 2-shot short text classiﬁcation meta-learning task (c = 3, k = 2).
bart is pre-trained for the paraphrasing task on three datasets: quora (sharma et al., 2019), msr (zhao and wang,2010) and google paws-wiki (yang et al., 2019; zhang et al., 2019).
the paraphrase model is used to paraphraseunlabeled samples but equipped with diversity strategies (back translation being proposed as a baseline).
the ﬁnalloss is computed using a loss annealing scheduler, which is expected to smooth the supervised (given shots) andunsupervised (augmented unlabeled sentences) prediction errors to yield parameter gradients.
a new episode meanssampling other classes along with their support and query points..verse beam search (dbs) instead of the regularbeam search.
as vijayakumar et al.
(2018) hasshown in the original paper, adding a dissimilarityterm during the decoding step helps the model pro-duce sequences that are quite far from each otherwhile still retaining the same meaning.
the nextsection describes how we constrained this decodingto enforce even more diversity among generatedparaphrases in protaugment..4.2 constrained user utterances generation.
while dbs enforces diversity between the gen-erated sentences, it does not ensure diversity be-tween the generated paraphrases and the originalsentences.
it was formerly designed for tasks thatdo not need this diversity with the original sen-tence (translation, image captioning, question gen-eration).
to enforce that our generated paraphrasesare diverse enough, we further constraint dbs byforbidding using parts of the original sentences.
inthe following paragraphs, we introduce two forbid-ding strategies..unigram masking.
in this strategy, we randomlyselect tokens from the input sentence which will be.
forbidden at the generation step.
the goal hereis to force the model to use different words inthe generated sentences than it saw in the origi-nal sentences.
each word of the input sentence israndomly masked using a probability pmask.
theunderlying assumption is that forbidding tokens atthe beginning of a sentence with a higher probabil-ity than the end of the sentence may have a greaterimpact on the beam search algorithm.
indeed, asthe decoding is a conditional task based on priorgenerated tokens, masking the ﬁrst tokens may sig-niﬁcantly impact diversity.
we therefore introducetwo additional variants: one where we put moreprobability on the ﬁrst tokens and the reverse wherethere is more weight in the last tokens.
to ensurethat all three variants mask the same amount oftokens on average, we ensure the area under thecurve of the three probability functions are equalto a ﬁxed value noted pmask..bi-gram masking another strategy we consideris to prevent the paraphrasing model from generat-ing the same bi-grams as in the original sentence.
this time, we are not masking any single word but.
2457loss annealingschedulerparaphrase trainingquoramsrpawsparaphrase datasetsc-wayk-shot(support)queryepisode #1...paraphrasegeneration withdiversitydecodingunlabeleddataepisode sampler diverse beamsearch (dbs)constraineddbspre-trainedtranslation modelsback-translationfew-shot modelepisode #1bart pre-trainedmodelaugmentationsoriginalback-propagationepisode #junlabeledepisode #1episode #j1.
pick a method2.
generateaugmentations3.
add toepisodeforcing the model to change the sentence’s struc-ture, which will, hopefully, increase the diversityof the generated paraphrases..4.3 unsupervised diverse paraphrasing loss.
after generating paraphrases for each unlabeledsentence, we create unlabeled prototypes.
for eachunlabeled sentence xu ∈ u , we derive the unla-beled prototype pxu as the average embedding ofthe paraphrases of xu (equation 3)..the goal here is to mainly use the supervisedloss ﬁrst so that the model gets a sense of the clas-siﬁcation task.
then, incorporating more and moreknowledge from unlabeled samples will make themodel more robust to noise, which is essentialas it is constantly tested on classes it has neverseen before.
we explore three different strategiesfor gradually increasing the unsupervised contribu-tion: a linear approach (α = 1), an aggressive one(α = 0.25), and a conservative one (α = 4)..pxu =.
fφ(˜xmu ).
(3).
1m.m(cid:88).
m=1.
5 experiments.
5.1 datasets.
after obtaining the unlabeled prototypes, wecompute the distances between all unlabeled sam-ples and all unlabeled prototypes.
given such dis-tances, we model the probability of each unlabeledsample being assigned to each unlabeled prototype(equation 4), as in the supervised part of the pro-totypical networks – except this time, it is fullyunsupervised.
this probability should be close to1 between an unlabeled sample and its associatedunlabeled prototype and close to 0 otherwise..pφ(u = v|xu) = softmax (−d(fφ(xu), pxv )) (4).
given assign probabilities between unlabeledsamples and unlabeled prototypes, we can computea fully unsupervised cross-entropy loss ˜l, train-ing the model to bring each sentence closer to itsaugmentations’ prototype and further from the pro-totypes of other unlabeled sentences.
recall thatfφ is the embedding function with φ as learnableparameters (section 3.2)..after obtaining both supervised loss ¯l and unsu-pervised loss ˜l, we combine them into the ﬁnal lossl using a loss annealing scheduler (see equation 5),which will gradually incorporate the unsupervisedloss as training progresses..l = tα × ˜l + (1 − tα) × ¯l ;.
t ∈ (0, 1) (5).
we consider the dialoglue benchmark (mehriet al., 2020), a set of natural language under-standing benchmark for task-oriented dialogue,which contains three datasets for intent detec-tion: banking77, hwu64 and clinic150 –the three datasets were already available prior therelease of dialoglue.
additionally, we also con-sider the liu57 intent detection dataset, as it con-tains the same order of magnitude of intent classesand is user-generated as well.
all datasets are pub-lic and in english..banking77.
banking77 thedataset(casanueva et al., 2020) classiﬁes 13, 083user utterances related to into 77 differentintents.
this dataset i) is speciﬁc to a singledomain (banking) and ii) requires a ﬁne-grainedunderstanding to classify due to intents beingvery similar.
following (mehri et al., 2020) andcontrary to (casanueva et al., 2020), we designatea validation set along a training and a testing setfor that dataset (table 1)..hwu64 hwu64 (xingkun liu and rieser, 2019)classiﬁes 25, 716 user utterances with 64 user in-tents.
it features intents spanning across 21 do-mains (alarm, audio, audiobook, calendar, cooking,datetime, .
.
.
).
when separating training, valida-tion, and test labels, we ensure each domain is rep-.
dataset.
#sentences.
#classestrain/valid/test (total).
availablesentences/class.
#tokens/sentence.
banking77hwu64clinic150liu.
13, 08311, 03622, 50025, 478.
25/25/27(77)23/16.4/24.6(64)50/50/50(150)18/18/18(54).
170 ± 33172 ± 40150 ± 0472 ± 831.
11.7 ± 7.66.6 ± 2.98.5 ± 3.37.5 ± 3.4.table 1: main statistics of intent detection evaluation datasets.
for hwu64, each split’s number of classes varies ateach run to ensure there is no cross-split domain, hence the decimal number..2458resented only in one set of labels.
this ensures themodel learns to discriminate between both intentsand domains..clinic150 this dataset (larson et al., 2019)classiﬁes 150 user intents in perfectly equally-distributed classes.
this chatbot-like style datasetwas initially designed to detect out-of-scopequeries, though, in our experiments, we discard theout-of-scope class and only keep the 150 labeledclasses to work with, as in (mehri et al., 2020)..liu57 introduced by liu et al.
(2019a), this in-tent detection dataset is composed of 54 classes.
itwas collected on amazon mechanical turk, whereworkers were asked to formulate queries for a givenintent with their own words.
it is highly imbal-anced: the most (resp.
least) common class holds5, 920 (resp.
24) samples.
5.2 experimental settings.
conditionallanguage model and languagemodel.
for the bart ﬁne-tuning process, we usedthe defaults hyper-parameters reported in (lewiset al., 2020), and we ﬁne-tuned the bart model fora single epoch (two hours on a titan rtx gpu).
increasing the number of epochs for ﬁne-tuningbart degrades performances on the intent detec-tion task:the downstream diverse beam searchstruggles to ﬁnd diverse enough beam groups sincethe model perplexity has been lower with furtherﬁne-tuning (this is also hinted in (bevilacqua et al.,2020)).
our text encoder fφ is a bert-basemodel, and the embedding of a given sentence is thelast layer hidden state of the ﬁrst token of this sen-tence.
for each dataset, this model is ﬁne-tuned onthe masked language modeling task for 20 epochs.
then, the encoder of our meta learner is initializedusing the weights of this ﬁne-tuned model..datasets from a dataset point-of-view, we cre-ate two data proﬁles: full (all the training datasetis available, the usual meta-learning scenario) andlow (only 10 samples are available for each train-ing class, an even more challenging meta-learningscenario in which a model meta-learns on very fewsamples per training class).
all experimental se-tups are run 5 times.
for each run, we randomlyselect training, validation, and testing classes, aswell as the samples for the low setting.
we trainthe few-shot models for a maximum of 10, 000c-way k-shots episodes, evaluating and testing ev-ery 100 episodes, stopping early if the evaluation.
accuracy has not progressed for at least 20 evalua-tions.
we evaluate and test using 600 episodes, asin other few-shot works (snell et al., 2017; chenet al., 2019).
we compare the systems in the follow-ing standard few-shot evaluation scenarios: 5-way1-shot, and 5-way 5-shots..paraphrasing.
at each episode, we draw u = 5unlabeled samples to generate paraphrases from.
for the back-translation baseline, we use thepublicly available3 translation models from thehelsinki-nlp team.
we use the followingpivot languages: fr, es, it, de, nl, which yields5 augmentations for each unlabeled sentence.
forour experiments with diverse beam search, wegenerate sentences using 15 beams, group theminto 5 groups of 3 beams.
in each group, we selectthe generated sentence which is the most differentfrom the input sentence using bleu as a metricfor diversity.
this yields m = 5 paraphrases foreach unlabeled sentence, as in the back-translationbaseline.
dbs uses a diversity penalty parameterto penalize words that have already been generatedby other beams to enforce diversity.
as advised inthe original dbs paper (vijayakumar et al., 2018),we set the diversity penalty to 0.5 in our exper-iments, which provides diversity while limitingmodel hallucinations.
our unigram masking strat-egy’s masking probability is set to pmask = 0.7found by linear search from 0 to 1 with steps of0.1..orig: how long will my transfer be pending for?
back: how long will my transfer be on hold?
dbs 0: how long will my transfer be pending?
i am in ﬁrst year.
dsb 1: when are all transfers coming up and how many days are they expected?
dbs 2: if i have a transfer for a while, how long should i wait for it?.
orig: i am not sure where my phone is.
back: i don’t know where my phone is.
dbs 0: i am not really sure where my phone is locateddsb 1: how can i ﬁnd the location of any android mobiledbs 2: i don’t know where is my cell phone.
orig: can you play m3 ﬁleback: can you read m3 ﬁledbs 0: m3 ﬁles: can i play the entire m3 ﬁle?
dsb 1: is there any way to play 3m ﬁles on earth without downloading itdbs 2: is there any way to play m3 ﬁles on windows?.
table 2: examples of sentences (orig) paraphrased usingback translation (back), vanilla diverse beam search –dbs (dbs 0), dbs with unigram masking (dbs 1) anddbs with bigram masking (dbs 2)...5.3 evaluation of paraphrase diversity.
we evaluate the diversity of paraphrases for eachmethod, and report results for two representativedatasets in table 3 (due to space limitations, the.
3https://huggingface.co/models?search=helsinki-nlp.
2459report for all datasets is given in appendix b).
foreach paraphrasing method and each dataset, met-rics are computed over unlabeled sentences andtheir paraphrases.
to assess the diversity of para-phrases generated by the different methods, thepopular bleu metric in neural machine transla-tion is a poor choice (bawden et al., 2020).
weuse the bi-gram diversity (dist-2) metric as pro-posed by (ippolito et al., 2019), which computesthe number of distinct 2-grams divided by the totalamount of tokens.
we also report the average simi-larity (denoted use) within each sentence set, usingthe universal sentence encoder as an independentsentence encoder.
results show that paraphrasesobtained with back-translation are too close to eachother, resulting in a high sentence similarity andlow bi-gram diversity.
on the other hand, dbsgenerates more diverse sentences with a lower sim-ilarity.
our masking strategies strengthen this effectand yield even more diversity.
the measured diver-sity strongly correlates with the average accuracyof the intent detection task (table 4)..banking77.
hwu64.
dist-2.
use.
dist-2.
use.
back-translationdbsdbs+bigramdbs+unigram.
0.1830.2000.2280.343.
0.8960.8070.7020.613.
0.3070.3400.3500.407.
0.8880.7690.6920.628.table 3: paraphrase diversity measures.
for dist-2 (resp.
use) higher values (resp.
lower) indicates more diversity..5.4.intent detection results.
in this section, we discuss the accuracy results forthe different meta-learners, for the standard 5-wayand {1, 5}-shots meta-learning scenarios, as pro-vided in table 4. the reported metric is the accu-racy on the test set at the iteration where the valida-tion set’s accuracy is maximal.
our dbs+unigramstrategy row corresponds to the flat maskingstrategy, with pmask = 0.7. first, all methodsaugmented with unsupervised diverse paraphrasingoutperform prototypical networks.
however, backtranslation demonstrates only a limited improve-ment over the vanilla prototypical network due totheir narrow diversity for short texts.
using para-phrases from dbs yields better results – about 0.5points over bt, on average –, hinting that using di-verse paraphrases in the unsupervised consistencyloss allows the few-shot model to build more robust.
sentence representations and therefore provides im-proved generalization capacities.
those results areconsistent across the different datasets, except forclinic for which accuracies are all very high, mak-ing all methods hardly separable.
the dataset isnot challenging enough, or in other words, meta-learning is robust to unbalanced short text classiﬁ-cation problems given the nature of that dataset..these results illustrate the need for unsuper-vised paraphrasing and show that using diverseparaphrases provide a signiﬁcant performance leap.
in the 1-shot (resp.
5-shot) scenario, our bestmeta-learner improves prototypical networks by5.27 (resp.
2.85) points on average.
rememberthat these improvements are made in an unsuper-vised manner hence at no additional cost.
slightlydifferent from to (xie et al., 2020), we do notﬁnd statistical differences depending on the rateat which ˜l is annealed in protaugment loss(α ∈ {0.25, 1, 4}), which makes it easier to tune –our unsupervised loss serves as a consistency regu-larization.
due to space limitations, this analysis isavailable in appendix d..adding our masking strategies on top of dbshas a signiﬁcant impact on all datasets, with theunigram variant being up about 2 points over thevanilla dbs on average.
on all datasets exceptclinic, given only 10 labeled samples per class(low proﬁle), it even outperforms the supervisedbaseline which is given the full training data (fullproﬁle).
this means that protaugment does bet-ter than prototypical networks with much less – 15times, and up to 47 times, depending on the dataset– labeled sentences per class.
those results indicatethat our method more than compensates for thelack of labeled data and that no matter the amountof data available for the training class, there is aperformance ceiling you cannot overcome withoutadding unsupervised knowledge from the valida-tion and test classes.
in the full proﬁle, when givenall the training data, our method greatly surpassesthe prototypical network – 3.58 points given 1 shot,on average.
moreover, protaugment is not onlysuited for the case where very little training data isavailable (low proﬁle): when sampling shots fromthe entire training dataset (full proﬁle), it outper-forms a fully supervised baseline.
furthermore,note that our method is consistently more stablethan the supervised baselines, as its average stan-dard deviation over the different runs is much lowerthan the vanilla prototypical network..2460dataproﬁle.
method.
banking.
hwu.
liu.
clinic.
datasets.
accuracy stats.
(av g ± st d).
k = 1 k = 5 k = 1 k = 5 k = 1 k = 5 k = 1 k = 5.k = 1.k = 5.lowproﬁle.
fullproﬁle.
82.20prototypical network83.83ours w/ bt83.10ours w/ dbsours w/ dbs+bigram86.04ours w/ dbs+unigram 87.23.
86.28prototypical network87.46ours w/ bt86.94ours w/ dbsours w/ dbs+bigram88.14ours w/ dbs+unigram 89.56.
91.5792.1692.5693.5594.29.
93.9494.4794.5094.7094.71.
74.3778.7080.0682.0983.70.
77.0981.3182.3584.0584.34.
86.4889.3690.2191.5791.29.
89.0291.4491.6892.1492.55.
80.0680.8482.3183.6085.16.
82.7684.1484.4285.2986.11.
89.6290.8791.6492.7193.00.
91.3792.6792.6293.2393.70.
94.2994.0693.7095.1195.92.
96.0595.1994.8595.7796.49.
98.1097.6297.8398.2398.56.
98.6198.3698.4198.5098.74.
82.73 ± 2.3284.36 ± 1.1584.80 ± 1.2686.71 ± 1.1488.00 ± 1.22.
85.55 ± 2.2087.02 ± 1.3687.14 ± 1.3688.31 ± 1.4389.13 ± 1.13.
91.44 ± 1.9292 .50 ± 0.9493.06 ± 0.9994.01 ± 1.0594.29 ± 0.76.
93.24 ± 1.2294.23 ± 0.8294.30 ± 0.6094.64 ± 0.5994.92 ± 0.57.table 4: 5-way 1-shots and 5-way 5-shots accuracy on the test sets for each dataset.
the ours method is protaug-ment (unsupervised consistency loss using diverse paraphrases) equipped with different paraphrasing strategies.
for each dataset × c-way k-shot setting, we compute the average and the standard deviation over the 5 runs (seesection 5.2), so that the last two columns contains average accuracy and ± the average standard deviations.
foreach data proﬁle, we highlight the best method in bold.
we underline the methods on the low proﬁle which performbetter than the prototypical networks on the full proﬁle.
we trained 400 different meta-learners – 5 methods, 2 dataproﬁles, 4 datasets, 2 meta-learning setup (k = 1, 5) and 5 runs for each conﬁguration..5.5 masking strategies.
we experimented with three variants of the unigramstrategy (section 4.2), each assigning a differentdrop chance to each token depending on its posi-tion in the input sentence.
in our experiments, wedid not observe any signiﬁcant difference in perfor-mance when putting more weight on the ﬁrst tokens(down), or last tokens (up), or the same weight onall tokens (ﬂat) (detailed results in appendix c).
we also conducted experiments where we tune thevalue pmask, from 0 to 1, selecting 0.7 as the besttrade-off (figure 2).
this ﬁgure also clearly showsthat the clinic dataset is one order of magnitudeeasier to solve than the other datasets..6 conclusion.
in this work, we proposed protaugment, anarchitecture for meta-learning for the problem ofclassifying user-generated short-texts (intents).
weﬁrst introduced an unsupervised paraphrasing con-sistency loss in the prototypical network’s frame-work to improve its representational power.
then,while the recent diverse beam search algorithm wasdesigned to enforce diversity between the gener-ated paraphrases, it does not ensure diversity be-tween the generated paraphrases and the originalsentences.
to make up for the latter, we introduceconstraints in the diverse beam search generation,further increasing the diversity.
our thorough eval-uation demonstrates that protaugment offers asigniﬁcant leap in accuracy for the most recent and.
figure 2: 5-way 1-shot accuracy of dbs-unigram-ﬂatmethod using different values of pmask.
setting this valueto 0 corresponds to the vanilla dbs without maskingstrategies..challenging datasets.
protaugment vastly out-performs prototypical networks, which was foundto be the best meta-learning framework for short-texts (dopierre et al., 2021) against unsupervised-extended prototypical networks (ren et al., 2018),matching networks (vinyals et al., 2016), rela-tion networks (sung et al., 2018), and inductionnetworks (geng et al., 2019), thereby making pro-taugment the new state-of-the-art for this task.
we provide the source code of protaugment aswell as code for evaluations reported in this paperon a public repository 4.
4https://github.com/tdopierre/protaugment.
24610.00(dbsbase)0.100.200.300.400.500.600.700.800.901.00pmask0.800.810.820.830.840.850.860.870.880.890.900.910.920.930.940.950.960.970.980.991.00dbs+unigram_flat accuracybanking77hwu64liuclinicacknowledgments.
we are thankful for the discussion we had withmichele bevilacqua, marco maru, and robertonavigli from sapienza university about diversity innatural language generation.
we also would liketo thank anrt 5 for making partnerships betweencompanies and universities happen..references.
rachel bawden, biao zhang, lisa yankovskaya, andret¨attar, and matt post.
2020. a study in improvingbleu reference coverage with diverse automaticin findings of the association forparaphrasing.
computational linguistics: emnlp 2020, pages918–932, online.
association for computational lin-guistics..michele bevilacqua, marco maru, and roberto navigli.
2020. generationary or “how we went beyond wordsense inventories and learned to gloss”.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages7207–7221, online.
association for computationallinguistics..xu cao, deyi xiong, chongyang shi, chao wang, yaomeng, and changjian hu.
2020. balanced joint ad-versarial training for robust intent detection and slotﬁlling.
in proceedings of the 28th international con-ference on computational linguistics, pages 4926–4936..i˜nigo casanueva, tadas temˇcinas, daniela gerz,matthew henderson, and ivan vuli´c.
2020. efﬁcientintent detection with dual sentence encoders.
in pro-ceedings of the 2nd workshop on natural languageprocessing for conversational ai, pages 38–45, on-line.
association for computational linguistics..wei-yu chen, yen-cheng liu, zsolt kira, yu-chiangwang, and jia-bin huang.
2019. a closer look atfew-shot classiﬁcation.
in international conferenceon learning representations..kyunghyun cho.
2016. noisy parallel approximatedecoding for conditional recurrent language model.
arxiv preprint arxiv:1605.03835..alice coucke, alaa saade, adrien ball, and bluche et al.
2018. snips voice platform: an embedded spokenlanguage understanding system for private-by-designvoice interfaces.
arxiv preprint arxiv:1805.10190..thomas dopierre, christophe gravier, and thomaslogerais.
2021. a neural few-shot text classiﬁca-tion reality check.
in proc.
of eacl 2021..thomas dopierre, christophe gravier, julien suber-caze, and wilfried logerais.
2020. few-shot pseudo-labeling for intent detection.
in proceedings of the.
5https://www.anrt.asso.fr/fr.
28th international conference on computational lin-guistics, pages 4993–5003, barcelona, spain (on-line).
international committee on computational lin-guistics..zi-yi dou, keyi yu, and antonios anastasopoulos.
investigating meta-learning algorithms for2019.low-resource natural language understanding tasks.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1192–1197, hong kong, china.
association for computa-tional linguistics..ruiying geng, binhua li, yongbin li, xiaodan zhu,ping jian, and jian sun.
2019. induction networksfor few-shot text classiﬁcation.
in proceedings ofthe 2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3904–3913, hong kong,china.
association for computational linguistics..tanya goyal and greg durrett.
2020. neural syntacticpreordering for controlled paraphrase generation.
inproceedings of the 58th annual meeting of the associ-ation for computational linguistics, pages 238–252,online.
association for computational linguistics..jiatao gu, yong wang, yun chen, victor o. k. li,and kyunghyun cho.
2018. meta-learning for low-in proceed-resource neural machine translation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 3622–3631,brussels, belgium.
association for computationallinguistics..yaru hao, li dong, furu wei, and ke xu.
2020. in-vestigating learning dynamics of bert ﬁne-tuning.
in proceedings of the 1st conference of the asia-paciﬁc chapter of the association for computationallinguistics and the 10th international joint confer-ence on natural language processing, pages 87–92,suzhou, china.
association for computational lin-guistics..po-sen huang, chenglong wang, rishabh singh, wen-tau yih, and xiaodong he.
2018. natural languageto structured query generation via meta-learning.
inproceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 2 (short papers), new orleans, louisiana.
association for computational linguistics..daphne ippolito, reno kriz, jo˜ao sedoc, mariakustikova, and chris callison-burch.
2019. compar-ison of diverse decoding methods from conditionalin proceedings of the 57th an-language models.
nual meeting of the association for computationallinguistics, pages 3752–3762, florence, italy.
asso-ciation for computational linguistics..ashish khetan and zohar karnin.
2020. schubert:optimizing elements of bert.
in proceedings of the.
246258th annual meeting of the association for compu-tational linguistics, pages 2807–2818, online.
asso-ciation for computational linguistics..lakshay sharma, laura graesser, nikita nangia, andutku evci.
2019. natural language understandingwith the quora question pairs dataset..ilia kulikov, alexander miller, kyunghyun cho, andjason weston.
2019. importance of search and eval-inuation strategies in neural dialogue modeling.
proceedings of the 12th international conference onnatural language generation, pages 76–87, tokyo,japan.
association for computational linguistics..stefan larson, anish mahendran, joseph j peper,christopher clarke, andrew lee, parker hill,jonathan k kummerfeld, kevin leach, michael alaurenzano, lingjia tang, et al.
2019. an evalua-tion dataset for intent classiﬁcation and out-of-scopeprediction.
arxiv preprint arxiv:1909.02027..mike lewis, yinhan liu, naman goyal, marjanghazvininejad, abdelrahman mohamed, omer levy,veselin stoyanov, and luke zettlemoyer.
2020.bart: denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-prehension.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7871–7880, online.
association for computa-tional linguistics..xianggen liu, lili mou, fandong meng, hao zhou,jie zhou, and sen song.
2020. unsupervised para-in proceedingsphrasing by simulated annealing.
of the 58th annual meeting of the association forcomputational linguistics, pages 302–312, online.
association for computational linguistics..xingkun liu, arash eshghi, pawel swietojanski, andverena rieser.
2019a.
benchmarking natural lan-guage understanding services for building conversa-tional agents.
arxiv preprint arxiv:1903.05566..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach..shikib mehri, mihail eric, and dilek hakkani-tur.
2020.dialoglue: a natural language understanding bench-mark for task-oriented dialogue..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language argu-ments.
in proceedings of the 57th annual meeting ofthe association for computational linguistics, pages4658–4664, florence, italy.
association for compu-tational linguistics..mengye ren, eleni triantaﬁllou, sachin ravi, jakesnell, kevin swersky, joshua b. tenenbaum, hugolarochelle, and richard s. zemel.
2018. meta-learning for semi-supervised few-shot classiﬁcation.
in 6th international conference on learning rep-resentations, iclr 2018, vancouver, bc, canada,april 30 - may 3, 2018, conference track proceed-ings.
openreview.net..jake snell, kevin swersky, and richard zemel.
2017.prototypical networks for few-shot learning.
in ad-vances in neural information processing systems,pages 4077–4087..richard socher, danqi chen, christopher d manning,and andrew ng.
2013. reasoning with neural ten-insor networks for knowledge base completion.
advances in neural information processing systems,pages 926–934..flood sung, yongxin yang, li zhang, tao xiang,philip h.s.
torr, and timothy m. hospedales.
2018.learning to compare: relation network for few-shotlearning.
in the ieee conference on computer vi-sion and pattern recognition (cvpr)..yik-cheung tam.
2020. cluster-based beam search forpointer-generator chatbot grounded by knowledge.
computer speech & language, 64:101094..ashwin k. vijayakumar, michael cogswell, ram-prasaath r. selvaraju, qing sun, stefan lee, david j.crandall, and dhruv batra.
2018. diverse beamsearch for improved description of complex scenes.
in proceedings of the thirty-second aaai confer-ence on artiﬁcial intelligence, (aaai-18), the 30th in-novative applications of artiﬁcial intelligence (iaai-18), and the 8th aaai symposium on educationaladvances in artiﬁcial intelligence (eaai-18), neworleans, louisiana, usa, february 2-7, 2018, pages7371–7379.
aaai press..oriol vinyals, charles blundell, timothy lillicrap,daan wierstra, et al.
2016. matching networks forone shot learning.
in advances in neural informationprocessing systems, pages 3630–3638..qizhe xie, zihang dai, eduard h. hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in advances in neuralinformation processing systems 33: annual confer-ence on neural information processing systems 2020,neurips 2020, december 6-12, 2020, virtual..pawel swietojanski xingkun liu, arash eshghi and ver-ena rieser.
2019. benchmarking natural languageunderstanding services for building conversationalagents.
in proceedings of the tenth internationalworkshop on spoken dialogue systems technology(iwsds), pages xxx–xxx, ortigia, siracusa (sr),italy.
springer..shuo yang, lu liu, and min xu.
2021. free lunchfor few-shot learning: distribution calibration.
inproceedings of the 9th international conference onlearning representations, page (accepted paper toappear), online.
openreview..yinfei yang, yuan zhang, chris tar, and jasonbaldridge.
2019. paws-x: a cross-lingual adversar-ial dataset for paraphrase identiﬁcation.
in proceed-ings of the 2019 conference on empirical methods.
2463in natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 3687–3692, hongkong, china.
association for computational linguis-tics..changlong yu, jialong han, haisong zhang, andwilfred ng.
2020. hypernymy detection for low-resource languages via meta learning.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 3651–3656, on-line.
association for computational linguistics..wen zhang, jiawei hu, yang feng, and qun liu.
2018.reﬁning source representations with relation net-works for neural machine translation.
in proceedingsof the 27th international conference on computa-tional linguistics, pages 1292–1303, santa fe, newmexico, usa.
association for computational lin-guistics..yuan zhang, jason baldridge, and luheng he.
2019.paws: paraphrase adversaries from word scrambling.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 1298–1308,minneapolis, minnesota.
association for computa-tional linguistics..shiqi zhao and haifeng wang.
2010. paraphrases andin coling 2010: paraphrases andapplications.
applications–tutorial notes, pages 1–87, beijing,china.
coling 2010 organizing committee..2464a diverse paraphrase samples.
orig: are you considered a bot?
back: do you consider yourself a bot?
dbs 0: is there any way to be considered a bot?
dsb 1: is there any way to know if someone is bot?
dbs 2: is there any way to know if a person is bot?.
orig: what is the square root of 31back: what is the square root of 31dbs 0: how can i ﬁnd square roots of 31 in mathematics?
dsb 1: how can i ﬁnd quadratic roots of 31?
dbs 2: what is square roots and what is its effect on 31.orig: how long will my transfer be pending for?
back: how long will my transfer be on hold?
dbs 0: how long will my transfer be pending?
i am in ﬁrst year.
dsb 1: when are all transfers coming up and how many days are they expected?
dbs 2: if i have a transfer for a while, how long should i wait for it?.
orig: how to cancel a transactionback: how to cancel a transactiondbs 0: what are some ways to cancel a transaction (in any way)?
dsb 1: what are some ways of cancelling an account (in some cases also involving transaction ).
dbs 2: is there any way in which i can cancel an existing transaction?.
orig: i am not sure where my phone is.
back: i don’t know where my phone is.
dbs 0: i am not really sure where my phone is locateddsb 1: how can i ﬁnd the location of any android mobiledbs 2: i don’t know where is my cell phone.
orig: what do i need to do for a refund?
back: what do i need to do to get a refund?
dbs 0: what should i do now that i am not getting a refund?
dsb 1: what are things that should be done before resumption of service ( reimbursed)?
dbs 2: what should you do if you want to refund your period of data?.
orig: does pizza hut have my order readyback: does pizza hut has my order readydbs 0: does the pizza hut have all your orders ready?
dsb 1: does pizza hut have all your orders made up?
dbs 2: does the pizza hut have all your orders ready for delivery?.
orig: go silent for a dayback: quiet for a day.
dbs 0: do you stay silent for a day or go silent for another daydsb 1: to the rest of the day, stay peaceful and collected.
dbs 2: so, to the rest of the day, go silent only..orig: what’s the recipe for ﬁsh soupback: what is the recipe for ﬁsh soupdbs 0: how do you make ﬁsh soup?
how is the recipe determined?
dsb 1: how can you recipe for ﬁsh-sugary food?
dbs 2: what are the recipes for fish soup and how is it prepared?.
orig: find easy recipe for almond milkback: find an easy recipe for almond milkdbs 0: what are some good recipe for almond milk?
dsb 1: what are some good ways of making almond milk?
dbs 2: how do i make almond milk for a beginner?.
orig: will i need to wear a coat today?
back: should i wear a coat today?
dbs 0: today, do i need to put on a coatdsb 1: should i wear a coat and what kind of coatdbs 2: what should i wear to work today, and why.
orig: can you play m3 ﬁleback: can you read m3 ﬁledbs 0: m3 ﬁles: can i play the entire m3 ﬁle?
dsb 1: is there any way to play 3m ﬁles on earth without downloading itdbs 2: is there any way to play m3 ﬁles on windows?.
table 5: additional paraphrases samples..2465b paraphrase diversity evaluation.
banking77.
hwu64.
liu.
clinic.
bleu dist-2.
use.
bleu dist-2.
use.
bleu dist-2.
use.
bleu dist-2.
use.
back-translationdbsdbs+bigramdbs+unigram.
56.034.20.10.2.
0.1830.2000.2280.343.
0.8960.8070.7020.613.
40.219.50.10.5.
0.3070.3400.3500.407.
0.8880.7690.6920.628.
47.719.70.40.5.
0.2680.2930.2930.351.
0.8920.7500.6640.596.
43.922.30.20.3.
0.2050.2360.2570.323.
0.9030.8050.7170.644.table 6: paraphrase evaluation on all 4 datasets.
the unigram variant exposed here is using the ﬂat masking strategywith pmask = 0.7..c masking tokens depending on their position.
datasets.
accuracy stats.
(av g ± st d).
method.
banking.
hwu.
liu.
clinic.
k = 1 k = 5 k = 1 k = 5 k = 1 k = 5 k = 1 k = 5.k = 1.k = 5.dbs+unigram-ﬂatdbs+unigram-downdbs+unigram-up.
87.2387.4386.18.
94.2994.1494.12.
83.7083.0683.30.
91.2992.1491.21.
85.1684.8785.14.
93.0093.3393.15.
95.9295.9395.84.
98.5698.6198.30.
88.00 ± 1.2287.82 ± 0.8487.62 ± 1.23.
94.29 ± 0.7694.55 ± 0.7194.20 ± 0.70.table 7: performances of dbs+unigram strategies putting either more chance to mask ﬁrst tokens (down), lasttokens (up), or the same chance to all tokens (ﬂat).
all strategies use pmask = 0.7. overall, there is no signiﬁcantdifference between the three strategies..d loss annealing strategy.
method.
banking.
hwu.
liu.
clinic.
α.k = 1 k = 5 k = 1 k = 5 k = 1 k = 5 k = 1 k = 5.k = 1.k = 5.dbs+unigram-ﬂat.
10.254.
87.2386.7186.90.
94.2994.1794.14.
83.7082.7183.26.
91.2991.1992.35.
85.1685.5284.48.
93.0093.1193.17.
95.9295.9995.69.
98.5698.4498.49.
88.00 ± 1.2287.73 ± 1.0987.58 ± 1.64.
94.29 ± 0.7694.23 ± 0.8594.54 ± 0.81.datasets.
accuracy stats.
(av g ± st d).
table 8: performances of dbs+unigram strategies with different values of the loss annealing parameter α. allstrategies use pmask = 0.7. overall, there is no signiﬁcant difference when changing the value of α..2466