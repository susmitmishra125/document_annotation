what context features can transformer language models use?.
joe o’connor.
jacob andreas.
massachusetts institute of technology{joeoc,jda}@mit.edu.
abstract.
transformer-based language models beneﬁtfrom conditioning on contexts of hundreds tothousands of previous tokens.
what aspectsof these contexts contribute to accurate modelprediction?
we describe a series of experi-ments that measure usable information by se-lectively ablating lexical and structural infor-mation in transformer language models trainedon english wikipedia.
in both mid- and long-range contexts, we ﬁnd that several extremelydestructive context manipulations—includingshufﬂing word order within sentences anddeleting all words other than nouns—removeless than 15% of the usable information.
ourresults suggest that long contexts, but not theirdetailed syntactic and propositional content,are important for the low perplexity of currenttransformer language models.1.
1.introduction.
recent years have seen a signiﬁcant improvementin the predictive accuracy of neural language mod-els (lms), owing to a combination of improve-ments in model architecture (especially transform-ers; vaswani et al.
2017) and training infrastructure(wolf et al., 2020).
the most striking change, rela-tive to both recurrent neural lms (mikolov et al.,2010) and count-based models (kneser and ney,1995), is the length of the context that these modelscan effectively condition on.
while count-basedlms in production speech recognition and machinetranslation systems typically used 10–20 tokens at amaximum (e.g., brown, 2011), and recurrent lmshave an effective context size of 200 (khandelwalet al., 2018), the predictive accuracy of transformerlms appears to improve when conditioning on asmany as a thousand previous tokens (beltagy et al.,2020).
a signiﬁcant amount of recent work has.
1code for all experiments in this paper is available at.
https://github.com/lingo-mit/context-ablations..focused on making use of even longer contextscomputationally feasible (rae et al., 2019; wanget al., 2020; child et al., 2019; dai et al., 2019;kitaev et al., 2020)..but despite empirical evidence that long contextsare helpful, little is understood about why.
if thefuture of language modeling will include a focuson contexts of increasing size, it is important toﬁrst understand what contextual information con-tributes to accurate prediction in current models.
this paper offers an answer to that question via thev-information framework of xu et al.
(2020).
v-information, discussed more in section 2, providesa formal framework for reasoning about how muchusable information a computationally constrainedpredictor (like a neural lm) can extract from aninput.
our experiments measure the amount of us-able information that is added when increasing lmcontext size, then attempt to pinpoint the source ofthis information by ablating features of the addedcontext (via controlled shufﬂing and word deletion)and measuring the resulting loss of model predic-tive power.
while this framework is general, wefocus on transformer lms..our work is closely related to an earlier study bykhandelwal et al.
(2018), which measured changesin a pre-trained lstm lm when context wordswere permuted and deleted at evaluation time.
butneural language models are known to be highlysensitive to distributional shifts—and in particularmight be unable to use information from long-rangecontext but still be adversely affected when thestructure of that context changes at evaluation time.
directly measuring usable information makes itpossible to clearly distinguish accuracy decreasesthat result from loss of information and decreasesthat result from out-of-distribution inputs..our experiments reveal a number of surprisingfacts about the use of long- and mid-range contextin transformers.
while increasing context length.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages851–864august1–6,2021.©2021associationforcomputationallinguistics851from 256 to 768 tokens is beneﬁcial (decreasingperplexity by roughly 4%), many destructive trans-formations of this context (including transforma-tions that cause large changes in the paradigm ofkhandelwal et al.
2018) remove essentially no us-able information.
our results suggest that for cur-rent models, the primary carriers of information inlong-range context are content words and local co-occurrence statistics: deleting function words andshufﬂing within local windows both have very littleeffect on models’ predictive power.
context mat-ters, but not all features of context matter equally;as discussed in section 5, these results motivatefuture language modeling research focused on al-ternative context representations rather than simplymore tokens..2 approach.
a language model (lm) places a probability distri-bution p(x) over discrete token sequences x. mostlearned lms do so by decomposing p(x) accordingto the chain rule and modeling the conditional dis-tribution over a single target token given a (ﬁxed-or variable-length) context of previous tokens:.
p(x) =.
p(xi | x0, x1, .
.
.
, xi−1) ..(1).
(cid:89).
i.in transformer language models, this conditionaldistribution is modeled via a sequence of alternat-ing neural feed-forward layers and self-attentionlayers; see vaswani et al.
(2017) for more details.
while input sequences x can in principle bemade arbitrarily long, there are both theoreticaland practical limits to transformers’ ability to makeeffective use of it (hahn, 2020; wang et al., 2019).
here, we wish to understand when (and why) in-creasing the size of the context improves modelpredictions..usable information consider a hypotheticallm context consisting of the tokens the user’spassword is.
.
.
.
this context suggests that subse-quent tokens will be a password: (hopefully!)
ahigh-entropy sequence.
now suppose this contextis extended to include earlier tokens, becomingthe user’s hashed password is ave$@to9!.
theuser’s password is.
.
.
.
information-theoretically,this context is extremely informative: only a smallnumber of passwords will hash to the given string,and a predictor capable of testing all passwordswould be able to identify the candidates and signif-icantly reduce its uncertainty about future tokens..but in practice, this extra context is useless: noknown efﬁcient predictor can learn anything aboutthe password from its hash code, and the extra con-text has not made the language modeling problemany easier.
this is an extreme case, but a simi-lar intuition applies to more conventional questionsabout language models.
a newspaper article whoseﬁrst sentence begins a dog bit a man is likely toend very differently from one that begins a manbit a dog.
can lms reason effectively about thisdistinction, or is it (like a hashed password) com-putationally inaccessible to current models?.
a framework for answering questions of this.
kind was introduced by xu et al.
(2020):deﬁnition 1. the usable predictive information(formally, predictive v-information) from a ran-dom variable x to a random variable y as:iv (x → y ) =(cid:2) infp1∈v− (cid:2) infp2∈v.
−e log p2(y | x)(cid:3) (2).
−e log p1(y )(cid:3).
for a class v of distributions p..intuitively, this deﬁnition measures how muchextra information about y can be extracted fromx by any predictor in v. in language modeling,we will take y to be the target word, x its con-text, and v a class of parametric models.
whilethis deﬁnition generalizes shannon mutual infor-mation (shannon, 1948) and has deep connectionsto other information-theoretic quantities (see xuet al.
2020 for details) it ultimately corresponds toa simple and common-sense evaluation: if we wantto know how much the extra context x helps a lan-guage model, we should train a model p1 withoutaccess to x, train a model p2 with access to x, andcompare the accuracy of their predictions..measuring what is used but the original ques-tion raised by the introduction was not just howmuch information is contributed by context.
it isalready well-established that conditioning on longcontexts is helpful, with existing experiments onlong-range transformers effectively implementingthe measurement in eq.
(2).
instead, we want toknow what information in this context is actuallyused by models..as a prototypical example, let us hypothesizethat more than ﬁve tokens away from the target,models are only able to extract usable informationfrom nouns.
(in our experiments in section 3, this“long-range context” will be considerably longerthan 5 words.)
for example, given the sentence:.
852pierre vinken, 61 years old, will join the board asa nonexecutive director nov. 29..we hypothesize that the lm distributions:.
p1(director | pierre vinken, 61 years old, will.
join the board as a nonexecutive).
(3).
,≈ p2(director | pierre vinken years(cid:124)(cid:125).
(cid:123)(cid:122)noun-only context.
the board as a nonexecutive(cid:124)(cid:123)(cid:122)(cid:125)ordinary context.)
,.
(4).
and more generally that.
iv (x0:n → xn).
≈ iv ([nouns(x0:n−5), xn−5:n] → xn).
(5).
is.
of.
the.
sequence.
where xi:jtokens[xi, xi+1, .
.
.
, xj−1], v is a class of lms,and nouns is a context ablation that extractsonly the nouns from a given string.
that is, wehypothesize that the amount of usable informationcontributed by the full context x0:n is the sameas the amount contributed by the ablated context[nouns(x0:n−5), xn−5:n], so ablation removes noinformation..the experiments in this paper generalize thisexperimental framework to other context ablationsand hypotheses.
let f be an ablation and k aninteger offset, and denote an ablated context:.
fk(x) = [f (x0:n−k), xn−k:n].
(6).
and an ablated negative log-likelihood:.
l(θ, f, k) = −e log pθ(xn | fk(x0:n)).
(7).
then, we can measure the effect of each ablation fon usable information via the following quantity:.
deﬁnition 2. the ablated information due to anablation f at an offset k is:.
a(f, k) =.
iv (x0:n→xn)−iv (fk(x0:n)→xn).
iv (x0:n→xn)−iv (xn−k:n→xn).
=.
inf θ l(θ,f,k)−inf θ(cid:48) l(θ(cid:48),n)inf θ(cid:48)(cid:48) l(θ(cid:48)(cid:48),n−k)−inf θ(cid:48) l(θ(cid:48),n).
,.
(8).
(9).
where l(θ, i) is the (unablated) negative log-likelihood −e log pθ(xn | xn−i:n)..intuitively, a(f, k) measures how much of theusable information added by an extra k tokens (thedenominator) is removed by applying the ablationf to those k tokens (the numerator).
if it is close to0, almost no information is removed; if it is closeto 1, almost all information is removed..figure 1: calculation ofthe ablated likelihoodl(nouns, (cid:96) : m ∼ n) (eq.
(10)).
a context ablationnouns (which deletes all non-noun words) is applied tothe ﬁrst (cid:96) tokens of the context, and likelihood is com-puted on the last n − m (unablated) context tokens..evaluation in practice eq.
(9) provides a gen-eral framework for answering our core question inthis paper: for a diverse set of context ablations andoffsets, we will measure how much information islost when a given ablation is applied at a given off-set.
a few modiﬁcations are required to turn thisequation into a practical evaluation scheme:.
held-out evaluation: eq.
(7) involves an expec-tation over the sequence distribution p(x).
in prac-tice, lms must be trained on ﬁnite corpora, creat-ing a risk of overﬁtting (zhang et al., 2016).
toaddress this issue, we approximate the inﬁmum ineq.
(7) by ﬁtting θ1 on a training set, and comput-ing ablated information on a held-out validationset.
all reported results are an average of held-outlikelihoods from two random initializations..1|x|.
(cid:80)|x|.
(cid:80)x.batching: given a ﬁxed (training or test)dataset of strings x and a maximum context sizeof m, eq.
(7) should be estimated empiricallyas − 1| fk(xi−m:i)).
i=0 log p(xi|x |this requires re-computing model predictionsonce for every token in the dataset.
however,the transformer models we use here support ef-ﬁcient batch inference:training data is pre-segmented into sequences of at most length n, and− 1i=0 log p(xi | fk(x0:i)) can be com-|x |nputed in a single forward pass.
this is considerablymore efﬁcient but means that most tokens are eval-uated with a context of length < n. as a compro-mise to ensure that evaluations contain long-rangecontext, we accumulate losses on a subset:.
(cid:80)x.
(cid:80)n.l(θ, f, (cid:96) : m ∼ n) = −.
1|x |(n − m).
(cid:88).
(cid:96)+n(cid:88).
x.i=(cid:96)+m.
log pθ(xi | [f (x0:(cid:96)), x(cid:96):i]).
(10).
(visualized in fig.
1).
this can be read as “(cid:96) tokensof f -ablated context, followed by m to n tokensof unablated context”.
we will write l(θ, m ∼ n).
853transformer lmpierre vinken years   |will join the board   |asadirectoranovdirector   |ℓℓ+mℓ+nablated contextordered contextℒ(,ℓ:m∼n)nounswhen only unablated context is used.
because ofthe large number of experiments in this paper, weuse eq.
(10) for all training and evaluation..model, data and training details for all exper-iments, our lm uses the gpt-2 model architec-ture (radford et al., 2019) in the implementationof wolf et al.
(2020) with default hyperparame-ters.
all models are trained from scratch on thewikitext-103 dataset (merity et al., 2016), an en-glish language modeling benchmark.
aside fromablations, no preprocessing is applied.
a spe-cial separator token is inserted between ablatedand unablated context.
the training set contains103,221,021 words, while the evaluation set con-tains 217,646 words..a note on evaluation as in past work on eval-uating language models (brown et al., 1992), ourevaluation of relative predictive information ulti-mately bottoms out in a conditional entropy (log-perplexity).
recent work has shown that other met-rics, such as diversity of outputs, are important forevaluating the quality of lms as models for lan-guage generation (hashimoto et al., 2019; cacciaet al., 2020).
generation also depends on a numberof other factors, such as choice of decoding proce-dure (caglayan et al., 2020).
here, we focus onlms as predictive models, measuring their abilityto place an accurate distribution over future wordsand sentences, rather than their ability to gener-ate useful or coherent text (see appendix c).
wewant to emphasize that these results below apply tolanguage models speciﬁcally, and not transformersapplied to nlp tasks in general—the same analysismight give very different conclusions if applied to,e.g., question answering or summarization..3 experiments.
in this section, we attempt to determine what in-formation in transformer lm contexts is usableby measuring ablated information (eq.
(9)).
sec-tions 3.1 and 3.2 describe our main results, withsection 3.1 focused on ordering and section 3.2focused on lexical information.
section 3.3 com-pares these results to ablations applied at evaluationtime.
section 3.4 explores whether contexts can befurther manipulated to improve model predictions..3.1 does order matter?.
in this section we will examine the effects of differ-ent augmentations to the order within long-range.
context.
we ﬁrst train a no information model tominimize l(θ, 0 ∼ 512) and a full informationmodel to minimize l(θ, 512 ∼ 1024).
for eachcontext ablation f , we train a model to minimizel(θ, f, 512 : 0 ∼ 512).
each ablation has accessto more information than the no information model(because it conditions on extra tokens) and lessinformation than the full information model (be-cause an ablation has been applied to those tokens).
note that the lm operates on bpe-derived sub-word tokens for consistency with the way gpt-2 istypically used, but all ablations are deﬁned at theword level, meaning, e.g., that we shufﬂe wordsrather than tokens..we use these trained models to calculate ab-lated information (eq.
(9)).
to explore the ef-fect of different context lengths, we stratify eval-uation of the ablated information into two con-ditions: a mid-range condition in which likeli-hoods in eq.
(9) are of the form l(·, f, 512 : 0 ∼256), and a long-range condition with likelihoodsl(·, f, 512 : 256 ∼ 512).
(we call the former“mid-range” rather than “short-range” because mosttokens are still predicted with signiﬁcant unab-lated context; our experiments do not character-ize sentence-internal modeling of syntactic well-formedness.)
results are shown in figure 2 anddiscussed below..overall word order.
shufﬂe all.
61 n.v., director the of mr. vinken dutch group.
asnonexecutive the 29. is vinken, years elsevier join old,publishing a nov. will pierre board chairman.
shuf.
trigrams globally.
publishing group.
n.v., the dutch mr. vinken is join theboard as a nonexecutive years old, will chairman ofelsevier pierre vinken, 61 director nov. 29..in the shufﬂe all ablation, f shufﬂes words uni-formly at random, forcing the model to treat ablatedcontext as a bag of words.
in the shuf.
trigramsglobally ablation, the context is divided up into non-overlapping trigrams, the order of which is thenpermuted uniformly at random.
shufﬂing all wordsremoves 41% of usable information in the mid-range condition and 84% in the long-range condi-tion: ordering information is important even veryfar from the target.
on the other hand, shufﬂingall trigrams removes 31% of usable information inthe mid-range condition and 50% in the long-rangecondition: local co-occurrence statistics carry asigniﬁcant amount of usable information..854sent.).
(1) and (2) were also recently exploredby pham et al.
(2020) in models for entailment,and more complex shufﬂing procedures have beenexplored in neuroscience contexts (mollica et al.,2020).
here, (2) and (3) are chosen because theypreserve local co-occurrence statistics ((3) morethan (2)), while (2) also preserves the general lin-ear information ﬂow of the sentence..notably, the shuf.
within trigrams (14% and41%) and the shuf.
trigrams within sent.
(16% and35%) ablations both remove relatively little usableinformation in both the mid- and long-range condi-tions.
usable information is decreased only slightlyby ablations that preserve local co-occurrencestatistics and/or linear information ﬂow.
(thisincludes transformations like man bites dog → dogbites man with signiﬁcant effects on semantics!)
inthe long-range condition, uniform shufﬂing withinsentences produces a larger effect, removing 55%of usable information..sentence order.
shuf.
sent..mr. vinken is chairman of elsevier n.v., the dutchpublishing group.
pierre vinken, 61 years old, will jointhe board as a nonexecutive director nov. 29..next, sentences are shufﬂed within the contextwhile their internal word order is unchanged.
inthe mid-range condition, this produces results com-parable to the trigram shufﬂing experiments above(removing 17% of usable information); in the long-range condition, it has an even smaller effect (14%).
together with the previous experiment these resultssuggest that prediction accuracy depends on infor-mation about local word co-occurrence, but notﬁne-grained word order or global position..(a) mid-range condition (ﬁrst 256 tokens after ablation).
(b) long-range condition (tokens 256-512 after ablation).
figure 2: effect of word order on usable information.
bar labels show “change in ablated likelihood (ablatedinformation)”.
the x axis shows ablated likelihood.
error bars represent 95% conﬁdence intervals.
word-order changes that preserve local ordering remove onlya small amount of information, while shufﬂing or re-placement with thematically similar text remove more..word order within sentences.
shuf.
within sent..61 director as the old, join will a nov. board nonexecutiveyears vinken, 29. pierre is publishing the vinken n.v., mr.group.
chairman elsevier of dutch.
shuf.
within trigrams.
vinken, pierre 61 will old, years the board join anonexecutive as nov. director 29. mr. vinken is ofelsevier chairman the dutch n.v., group.
publishing.
order of entire sections.
replace w/ old.
shuf.
trigrams within sent..years old, will as a nonexecutive join the board pierrevinken, 61 director nov. 29. n.v., the dutch chairman ofelsevier mr. vinken is publishing group..words are shufﬂed only within sentences accordingto one of three procedures: (1) a uniform randompermutation of all the words in the sentence (shuf.
within sent.
), (2) a uniform random permutationof the words within each non-overlapping trigramin the sentence (shuf.
within trigrams), and (3) auniform random permutation of the order of thetrigrams within the sentence (shuf.
trigrams within.
rudolph agnew, 55 years old and former chairman ofconsolidated gold fields plc, was named anonexecutive director of this british industrialconglomerate..a possible hypothesis about lm behavior is that themain function of long-range context is to providemore information about the general topic of thedocument, including clues about vocabulary andstyle.
to test this, the ablation replaces its entireinput with the 512 tokens that immediately precedeit in the source document (which in general willbe topically similar).
this transformation removessigniﬁcant information in both mid- and long-rangeconditions (55% and 69%).
long-range context is.
8554.204.254.304.354.404.45bitsfull informationshuf.
within trigramsshuf.
trigrams within sent.sent.
shuf.shuf.
within sent.shuf.
trigrams globallyshuffle allreplace w/ oldno information4.19 (0%)+0.04 (14%)+0.04 (16%)+0.04 (17%)+0.07 (26%)+0.08 (31%)+0.10 (41%)+0.14 (55%)4.45 (100%)4.174.184.194.204.214.224.23bitsfull informationsent.
shuf.shuf.
trigrams within sent.shuf.
within trigramsshuf.
trigrams globallyshuf.
within sent.replace w/ oldshuffle allno information4.17 (0%)+0.01 (14%)+0.02 (35%)+0.02 (41%)+0.02 (50%)+0.03 (55%)+0.03 (69%)+0.04 (84%)4.22 (100%)n & vb.
pierre vinken years will join board director nov. mr.vinken chairman elsevier n.v. publishing group.
n & vb & adj.
pierre vinken years old will join board nonexecutivedirector nov. mr. vinken chairman elsevier n.v. dutchpublishing group.
cont.
words (n & vb & adj & adv).
pierre vinken years old will join board nonexecutivedirector nov. mr. vinken chairman elsevier n.v. dutchpublishing group.
func.
words.
, 61 , the as a 29 .
is of , the ..as in the initial example from section 2, weretain only words whose part of speech tag is in agiven set.
we use the spacy model (honnibal et al.,2020) for part-of-speech tagging, and examine ﬁvesets: (1) nouns only, (2) nouns and verbs, (3) nouns,verbs, and adjectives, (4) content words (nouns,verbs, adjectives, and adverbs), and (5) functionwords (all words except nouns, verbs, adjectives,and adverbs)..in the mid-range condition, deleting all wordsbut nouns removes only 20% of usable informa-tion; deleting all but nouns and verbs removes only13%.
most usable information, even in mid-rangecontext, appears to be captured by nouns and verbs.
retaining only function words causes a consider-ably greater loss of information..in the long-range condition, results are evenretaining only content wordsmore striking:improves predictions over the “fullinforma-tion” experiment.
like shannon information, v-information is deﬁned to be non-negative (xu et al.,2020), and the result in fig.
3 is a consequenceof our ﬁnite-sample approximation based on held-out likelihood.
the effect is robust across multipletraining runs from random initializations.
as thereis a signiﬁcant gap between the training and vali-dation perplexity of our model (roughly 11%), wehypothesize that this change occurs because theablation preserves semantic content while reducingthe original model’s ability to overﬁt.
we believethis is an important subject for future investigation..named entities.
named entities.
pierre vinken 61 years old nov. 29 vinken elsevier n.v.dutch.
as an alternative to the topic hypothesis evaluatedunder “order of entire sections” above, we might.
(a) mid-range condition (ﬁrst 256 tokens after context).
(b) long-range condition (tokens 256-512 after context).
figure 3: effect of word identity on usable informa-tion.
labels are as in fig.
2. several ablations, includ-ing deletion of all words except nouns, preserve mostusable information in the mid-range condition, and im-prove model accuracy in the in the long range..not simply a source of topic information: earliertext on the same theme is in some cases nearly asuninformative as no text at all..3.2 do all words matter?.
our next experiments focus on lexical rather thanstructural information, using ablations that deleteselected words from the context.
training and eval-uation setups are exactly as in section 3.1. here,unlike the previous section, ablations will generallycause the number of tokens in a given context todecrease; in this case ablations also insert paddingtokens to the beginning of the context window topreserve the original number of tokens.
results areshown in fig.
3..parts of speech.
n.pierre vinken years board director nov. mr. vinkenchairman elsevier n.v. publishing group.
8564.204.254.304.354.404.45bitsfull informationcont.
wordsn&vb&adjn&vbncommonnamed entitiesrarefunc.
wordsno information4.19 (0%)+0.02 (9%)+0.03 (11%)+0.03 (13%)+0.05 (20%)+0.10 (38%)+0.10 (39%)+0.15 (58%)+0.18 (69%)4.45 (100%)4.154.164.174.184.194.204.214.224.23bitscont.
wordsn&vb&adjn&vbnfull informationnamed entitiescommonrarefunc.
wordsno information+-0.01 (-31%)+-0.01 (-29%)+-0.01 (-22%)+-0.00 (-9%)4.17 (0%)+0.01 (31%)+0.02 (33%)+0.03 (73%)+0.04 (89%)4.22 (100%)hypothesize that long-range contexts are useful be-cause they provide a reservoir of named entitieslikely to be referred to again.
here, the ablationretains only spans tagged as named entities or quan-tities by spacy.
while signiﬁcantly worse than thenoun ablation discussed above, retaining only en-tities results removes only about a third of usableinformation in both conditions (39% and 31%)..word frequency.
common.
pierre years old join board director .
mr. chairmandutch publishing group ..rare.
vinken nonexecutive nov. vinken elsevier n.v..another natural question is whether rare words orfrequent words are more important: informationabout frequent context words might help modelsestimate ﬁne-grained document-level frequenciesof those words account for most of the terms ineq.
(7); rare words are likely to be more informa-tive about the content of the document itself..we partition the vocabulary into a set of rarewords, corresponding to the least frequent ∼ 98%of word types and 20% of word tokens, and fre-quent words, the most frequent ∼ 2% of types and80% of tokens.
both ablations remove a signiﬁcantamount of information relative to the pos-basedablations above, but retaining only frequent wordsimproves perplexity relative to rare words in boththe mid- and long-range conditions..appendix b presents versions of these experi-ments trained and evaluated on even longer con-texts.
conclusions are largely the same as above..3.3 evaluating on augmented data.
we motivated the use of v-information in section 2by arguing that it more clearly distinguished be-tween prediction errors attributable to loss of in-formation and prediction errors attributable to mal-formed and out-of-distribution model inputs.
toput our results in context, we repeat several of theprevious experiments in the evaluation paradigmof khandelwal et al.
(2018), which is designedto measure test-time sensitivity rather than usableinformation..we train a new model to minimize l(θ, 512 ∼1024) while randomly truncating the ﬁrst 512 con-text tokens and replacing them with padding tokens(to ensure that the model has seen padding tokensat training time).
we then evaluate this model on.
(a) mid-range condition (ﬁrst 256 tokens after ablation).
(b) long-range condition (tokens 256-512 after ablation).
figure 4: loss of information resulting from ablationsat evaluation time only.
x-axis and labels show ablatednegative log-likelihoods.
some locality-preserving ab-lations (high pmi, shuf.
sent.)
have a small effect, butmost affect likelihood signiﬁcantly (including lexicalablations that do not remove usable information)..the set of ablations shown in section 3.1 and sec-tion 3.2. for the full information model in fig.
4,we evaluate on ordered context windows with nopadding tokens; for the no information model, weevaluate on context windows in which the ﬁrst 512tokens are all padding tokens..in the mid-range condition, the least destructiveablations are shufﬂing within trigrams and shufﬂingthe order of trigrams within sentences: models ap-pear to be reasonably robust to this kind of datatransformation without speciﬁc training on it.
im-portantly, lexical ablation experiments have a largeimpact in this evaluation, underlining the extent towhich the two experimental paradigms characterizedifferent aspects of model behavior.
figure 5 inappendix a shows a side-by-side comparison ofthese experiments and the ones in sections 3.1–3.2..3.4 making better language models?.
the lexical ablation experiments in section 3.2 in-dicated that model accuracy could be improved by.
8574.204.254.304.354.404.454.504.55bitsfull informationshuf.
within trigramsshuf.
trigrams within sent.sent.
shuf.shuf.
within sent.cont.
wordsn&vb&adjshuf.
trigrams globallyn&vbncommonnamed entitiesshuffle allrarereplace w/ oldno informationfunc.
words4.18+0.06+0.08+0.10+0.14+0.16+0.16+0.16+0.16+0.18+0.20+0.20+0.22+0.24+0.254.48+0.364.144.164.184.204.22bitsfull informationsent.
shuf.shuf.
trigrams within sent.shuf.
within trigramsshuf.
trigrams globallyreplace w/ oldn&vb&adjcont.
wordsn&vbnnamed entitiesshuf.
within sent.rareshuffle allcommonno informationfunc.
words4.15+0.00+0.02+0.03+0.03+0.03+0.03+0.03+0.03+0.03+0.03+0.04+0.04+0.04+0.054.20+0.07selective deletion of context words.
can this ef-fect be exploited to further improve models?
asa simple experiment, we attempted to replace allpadding tokens in the nouns+verbs ablation of sec-tion 3.2 with nouns and verbs from further backin the context—effectively providing the modelwith an even longer-range view of an informativecontext representation..this experiment slightly increased usable infor-mation in the mid-range condition (0.2%), but de-creased it in the long range-range condition (0.6%).
longer contexts, even of a kind previously foundto be informative, did not provide additional us-able information.
these results are consistent withour earlier hypothesis that the previously observedeffect resulted from a reduction in overﬁtting—ifremoving information increased performance by re-ducing overﬁtting, then it is reasonable that addinginformation back results in more overﬁtting..4 related work.
contextin count-based and discriminativelms the earliest learned lms were count-based(e.g., kneser and ney, 1995):they estimatedp(xn | x0:n) based on a (smoothed) empirical n-gram frequency #(x0:n)/#(x0:n−1) (where #(x)is the number of times the sequence x appears intraining data).
as the number of distinct n-gramcounts grows exponentially in n, it was typicallyset to a small value.
count-based models have aclear dependence on context: any token within thelast n words that also appears in a training n-gramis relevant, anything further back is not..subsequent models improved on these by allow-ing the use of skip-grams, caches, and feature-based models (goodman, 2001; bengio et al.,2003).
some of these in principle allowed the useof unlimited-length contexts, but only by imposingstrong restrictions on the ways in which contextfeatures could interact..context in rnn lms recurrent neural networklanguage models (mikolov et al., 2010; elman,1990) provide a more expressive mechanism forthe use of long-range context: models write to arecurrent “state vector” which can be carried ar-bitrarily far into the future.
computational issueslimit the effective context size such models canbe practically trained on, but this size is still sig-niﬁcantly greater the models mentioned above: aspreviously noted, khandelwal et al.
(2018) revealedinﬂuence from up to 200 tokens of context.
similar.
effects are reported by sankar et al.
(2019) for neu-ral dialogue models, and li et al.
(2016) describean alternative procedure for ablating contexts..context in transformer lms transformers in-troduce yet another mechanism for extracting infor-mation from long-range context: attention.
atten-tion is also used with rnns, but typically with justa single head—the hidden state still carries mostof the information.
in transformers, context entersinto predictions primarily via unbounded randomaccess.
these models appear to beneﬁt from signif-icantly longer contexts than previous models..some recent work that investigates the behaviorof individual transformer attention heads (clarket al., 2019; voita et al., 2019).
this work ﬁnds thatcertain attention heads are sensitive to things likeword frequency, positional information, and certainsyntactic phenomena.
while extremely informativeabout the computational structures implemented byﬁxed models, these approaches do not necessarilyreveal anything about usable information: indeed,patterns of attention do not necessarily correlatewith model predictions (jain and wallace, 2019)..other related work our ﬁnding that ﬁne-grained ordering information contributes little us-able information is consistent with rae et al.
(2019)’s ﬁnding that long-range contexts couldbe informatively summarized in ﬁxed-sized vec-tors; our ﬁnding that most usable information iscarried by nouns is consistent with earlier ﬁnd-ings about both specialized neural architectures(henaff et al., 2016) and discourse representa-tions in feature-based models (barzilay and la-pata, 2008).
our approach also shares similar mo-tivations to information-theoretic work on prob-ing (voita and titov, 2020; pimentel et al., 2020),which uses related tools to interpret linguistic struc-ture in lm representations rather than characteriz-ing their effect on lm predictions.
several recentpapers have explored the effect of training-time andtest-time ablations in models for other data analysistasks: pham et al.
(2020) ﬁnd that shufﬂing exper-iments have a limited effect on the accuracy ofmodels for natural language inference, while perezet al.
(2021) describe several experiments aimed atintroducing usable information for several questionanswering and sentence understanding tasks..8585 discussion.
we have investigated the extent to which trans-former models can use structural and lexical infor-mation in long-range contexts for english languagemodeling.
experiments demonstrated that this in-formation is primarily contained in content wordsand local ordering statistics: ablations that removeother kinds of information from context have littleeffect on models’ predictive accuracies.
in contrast,retaining only information about document identityor named entities causes signiﬁcant drops in pre-dictive accuracy: the effectiveness of long contextsis not explained by the presence of topic or namedentity information alone..crucial to obtaining these results was a mea-sure of ablated usable information grounded in theaccuracy of models trained and tested on ablatedcontexts.
past work on context in lms has pri-marily measured the inﬂuence of evaluation-timeablations.
sometimes these two notions of context-sensitivity coincide (e.g., trigram shufﬂing) andsometimes they do not (e.g., removal of lexical in-formation).
our results also offer a jumping-offpoint for future modeling work.
they motivatemore efﬁcient, compressed context representationsthat better preserve the information that is usable bycurrent models.
they motivate more accurate mod-els by developing new context representations thatmake currently unusable information more promi-nent..several questions remain unanswered by ourexperiments.
do ablations affect the quality oftext generated by models?
(in particular, doesthe usable information added by long contexts im-prove predictability of syntax, semantics, or simplydocument-level word frequency statistics?)
morefundamentally, do observations about usable infor-mation reﬂect limitations of transformers or fun-damental, (shannon-)information-theoretic proper-ties of english?
our results suggest that at leastsome of these effects are model-speciﬁc: delet-ing function words cannot add information, butimproves held-out model accuracy.
a completeanswer to this question will require more detailedexploration, including a better understanding ofhuman predictions in comparable settings..acknowledgments.
lincoln laboratory supercomputing center forproviding hpc resources that contributed to theresults reported within this paper..impact statement.
across initial exploration, evaluation conditionsand training runs, experiments in this paper re-quired roughly 100 training runs on the wikitext-103 dataset.
as discussed in section 2, model sizeand batched evaluation were both used to mini-mize the energy demands of these experiments; ex-periments themselves were performed at the mas-sachusetts green hpc center, a carbon-neutral su-percomputing facility.
ultimately, results in sec-tion 3 provide guidance toward the design of mod-els that use context more efﬁciently and motivatethe large-scale empirical study conducted here..references.
regina barzilay and mirella lapata.
2008. modelinglocal coherence: an entity-based approach.
compu-tational linguistics, 34(1):1–34..iz beltagy, matthew e. peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv:2004.05150..yoshua bengio, r´ejean ducharme, pascal vincent, andchristian janvin.
2003. a neural probabilistic lan-guage model.
the journal of machine learning re-search, 3:1137–1155..peter f brown, stephen a della pietra, vincent jdella pietra, jennifer c lai, and robert l mercer.
1992. an estimate of an upper bound for the entropyof english.
computational linguistics, 18(1):31–40..ralf d brown.
2011. the cmu-ebmt machine trans-.
lation system.
machine translation, 25(2):179..massimo caccia, lucas caccia, william fedus, hugolarochelle, joelle pineau, and laurent charlin.
in interna-2020. language gans falling short.
tional conference on learning representations..ozan caglayan, pranava madhyastha, and lucia spe-cia.
2020. curious case of language generationin proceed-evaluation metrics: a cautionary tale.
ings of the 28th international conference on com-putational linguistics, pages 2322–2328, barcelona,spain (online).
international committee on compu-tational linguistics..thanks to carina kauf and greta tuckute, evelinafedorenko and roger levy for valuable discus-sions.
we acknowledge the mit supercloud and.
rewon child, scott gray, alec radford,.
ilya sutskever.
2019.quences withsparsehttps://openai.com/blog/sparse-transformers..transformers..andgenerating long se-url.
859kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..jeffrey l elman.
1990. finding structure in time.
cog-.
nitive science, 14(2):179–211..joshua t goodman.
2001. a bit of progress in lan-guage modeling.
computer speech & language,15(4):403–434..michael hahn.
2020. theoretical limitations of self-attention in neural sequence models.
transactionsof the association for computational linguistics,8:156–171..tatsunori hashimoto, hugh zhang, and percy liang.
2019. unifying human and statistical evaluation fornatural language generation.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1689–1701, minneapolis, min-nesota.
association for computational linguistics..mikael henaff, jason weston, arthur szlam, antoinebordes, and yann lecun.
2016. tracking the worldstate with recurrent entity networks.
in iclr..matthew honnibal,.
ines montani, soﬁe van lan-deghem,spacy:and adriane boyd.
2020.industrial-strength natural language processing inpython..sarthak jain and byron c. wallace.
2019. attention is.
not explanation.
in naacl-hlt..urvashi khandelwal, he he, peng qi, and dan juraf-sky.
2018. sharp nearby, fuzzy far away: how neu-in proceedingsral language models use context.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 284–294, melbourne, australia.
associationfor computational linguistics..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
in inter-national conference on learning representations..reinhard kneser and hermann ney.
1995. improvedbacking-off for m-gram language modeling.
in 1995international conference on acoustics, speech, andsignal processing, volume 1, pages 181–184.
ieee..jiwei li, will monroe, and dan jurafsky.
2016. un-derstanding neural networks through representationerasure.
arxiv preprint arxiv:1612.08220..stephen merity, caiming xiong, james bradbury, andrichard socher.
2016. pointer sentinel mixture mod-els.
corr, abs/1609.07843..tom´aˇs mikolov, martin karaﬁ´at, luk´aˇs burget, janˇcernock`y, and sanjeev khudanpur.
2010. recurrentneural network based language model.
in eleventhannual conference of the international speech com-munication association..f. mollica, matthew siegelman, evgeniia diachek,s. piantadosi, zachary mineroff, richard futrell,hope h. kean, peng qian, and e. fedorenko.
2020.composition is the core driver of the language-neurobiology of language,selective network.
1:104–134..ethan perez, douwe kiela, and kyunghyun cho.
2021.rissanen data analysis: examining dataset char-arxiv preprintacteristics via description length.
arxiv:2103.03872..thang m pham, trung bui, long mai, and anhnguyen.
2020. out of order: how important isthe sequential order of words in a sentence in nat-ural language understanding tasks?
arxiv preprintarxiv:2012.15180..tiago pimentel, josef valvoda, rowan hall maudslay,ran zmigrod, adina williams, and ryan cotterell.
information-theoretic probing for linguistic2020.structure.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4609–4622, online.
association for computa-tional linguistics..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..jack w. rae, anna potapenko, siddhant m. jayakumar,and timothy p. lillicrap.
2019. compressive trans-formers for long-range sequence modelling..chinnadhurai sankar, sandeep subramanian, christo-pher pal, sarath chandar, and yoshua bengio.
2019.do neural dialog systems use the conversation his-tory effectively?
an empirical study.
arxiv preprintarxiv:1906.01603..claude e shannon.
1948. a mathematical theory ofcommunication.
the bell system technical journal,27(3):379–423..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in i. guyon, u. v. luxburg, s. bengio,h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..860elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..elena voita and ivan titov.
2020..information-theoretic probing with minimum description length.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 183–196, online.
association for computa-tional linguistics..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r bowman.
2019.super-glue: a stickier benchmark for general-purposearxiv preprintlanguage understanding systems.
arxiv:1905.00537..sinong wang, belinda li, madian khabsa, hanself-arxiv preprint.
fang, and hao ma.
2020.attention with linear complexity.
arxiv:2006.04768..linformer:.
thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..yilun xu, shengjia zhao, jiaming song, russell stew-art, and stefano ermon.
2020. a theory of usable in-formation under computational constraints.
in inter-national conference on learning representations..chiyuan zhang, samy bengio, moritz hardt, ben-jamin recht, and oriol vinyals.
2016. understand-ing deep learning requires rethinking generalization.
arxiv preprint arxiv:1611.03530..a comparison of experimental.
paradigms.
in figure 5 we show the contrast between the ex-perimental paradigm of sections 3.1–3.2 and thatof section 3.3. especially for the experiments in-volving parts of speech, we see a signiﬁcant differ-ence in both the quantitative and qualitative resultsacross the two paradigms..(a) mid-range condition (ﬁrst 256 tokens after ablation).
(b) long-range condition (tokens 256-512 after ablation).
figure 5: comparison of model performance in thetrain+eval and eval-only settings.
the units representthe percentage of the gap between the full informationand no information models/contexts.
that way, if apoint falls on the dotted y = x line, then that ablationhas the same relative effect in each paradigm.
if a pointfalls above the dotted line, then that ablation leads tobetter relative performance in the train+eval paradigm,and if a point falls below the dotted line, then that abla-tion leads to better relative performance in the eval-onlyparadigm..b longer context window.
here we report the results of repeating the experi-ments of sections 3.1 and 3.2 with ablated contexts.
of size 1024 tokens instead of 512 tokens in or-der to verify that the behavior we observed is notspeciﬁc to the size of context window we chose..8614020020406080100120140normalized accuracy (train+eval ablation)4020020406080100120140normalized accuracy (eval ablation)full informationcont.
wordsn&vb&adjn&vbshuf.
within trigramsshuf.
trigrams within sent.sent.
shuf.nshuf.
within sent.shuf.
trigrams globallycommonnamed entitiesshuffle allreplace w/ oldrarefunc.
wordsno information4020020406080100120140normalized accuracy (train+eval ablation)4020020406080100120140normalized accuracy (eval ablation)cont.
wordsn&vb&adjn&vbnfull informationsent.
shuf.named entitiescommonshuf.
trigrams within sent.shuf.
within trigramsshuf.
trigrams globallyshuf.
within sent.replace w/ oldrareshuffle allfunc.
wordsno information(a) mid-range condition (ﬁrst 256 tokens after ablation).
(a) mid-range condition (ﬁrst 256 tokens after ablation).
(b) long-range condition (tokens 256-512 after ablation).
(b) long-range condition (tokens 256-512 after ablation).
figure 7: effect of word identity on usable informa-tion.
labels are as in fig.
6. ablated contexts contain1024 tokens, but results are consistent with results on512-token contexts..figure 6: effect of word order on usable information.
bar labels show “change in ablated likelihood (ablatedinformation)”.
the x axis shows ablated likelihood.
er-ror bars represent 95% conﬁdence intervals.
ablatedcontexts contain 1024 tokens, but results are consistentwith results on 512-token contexts..c sample generations.
the purpose of this section is to verify that modelstrained on ablated contexts can still generate textthat is comparable to text generated by a modeltrained with full contextual information.
we selecta prompt from a randomly chosen wikipedia articlein the wikitext-103 validation set; each modelgenerates a sentence (after ﬁnishing the sentence inprogress) given the appropriately ablated versionof the prompt.
the prompt consists of 768 tokens,the last 256 of which remain unchanged for allversions of the prompt, so that the ablations are inthe long range relative to the point of generation.
the prompt and generations are as follows:.
prompt:.
at two independent schools for boys:sussex house school, a day school inchelsea’s cadogan square, and the cityof london school, a day school on thenorth bank of the river thames in lon-don’s ﬁnancial district (known as the city.
of london).
attending school becamedifﬁcult for radcliffe after the release ofthe ﬁrst harry potter ﬁlm, with some fel-low pupils becoming hostile, though hesays it was people just trying to ”have acrack at the kid that plays harry potter”rather than jealousy..as his acting career began to consumehis schedule, radcliffe continued his ed-ucation through on-set tutors.
he admit-ted he was not very good at school, con-sidering it useless and ﬁnding the work”really difﬁcult.” he achieved a gradesin the three as-level exams that he tookin 2006, but decided to take a break fromeducation and did not go to college oruniversity.
part of his reasoning was thathe already knew he wanted to act andwrite, and that it would be difﬁcult tohave a normal college experience.
”thepaparazzi, they’d love it,” he told detailsmagazine in 2007.
”if there were anyparties going on, they’d be tipped off asto where they were.”.
8624.204.254.304.354.404.45bitsfull informationsent.
shuf.shuf.
within sent.shuffle allreplace w/ oldno information4.16 (0%)+0.06 (21%)+0.08 (29%)+0.14 (49%)+0.21 (73%)4.45 (100%)4.154.164.174.184.194.204.214.224.23bitsfull informationsent.
shuf.shuf.
within sent.shuffle allno informationreplace w/ old4.16 (0%)+0.01 (24%)+0.04 (65%)+0.06 (88%)4.22 (100%)+0.06 (100%)4.204.254.304.354.404.45bitsfull informationn&vb&adjn&vbnnamed entitiesfunc.
wordsno information4.16 (0%)+0.04 (13%)+0.05 (17%)+0.07 (23%)+0.12 (41%)+0.22 (76%)4.45 (100%)4.164.184.204.22bitsn&vb&adjfull informationn&vbnnamed entitiesno informationfunc.
words+-0.00 (-6%)4.16 (0%)+0.00 (2%)+0.01 (15%)+0.03 (48%)4.22 (100%)+0.07 (114%)= = career = =.
= = = harry potter = = =.
in 2000, producer david heyman askedradcliffe to audition for the role of harrypotter for the ﬁlm adaptation of harrypotter and the philosopher’s stone, thebest-selling book by british author j.k.rowling.
rowling had been searchingfor an unknown british actor to person-ify the character, and the movie’s directorchris columbus recalled thinking, ”thisis what i want.
this is harry potter”, af-ter he saw a video of the young actor indavid copperﬁeld.
eight months later,and after several auditions, radcliffe wasselected to play the part.
rowling alsoendorsed the selection saying, ”i don’tthink chris columbus could have founda better harry.” radcliffe’s parents origi-nally turned down the offer, as they hadbeen told that it would involve six ﬁlmsshot in los angeles.
warner bros. in-stead offered radcliffe a two-movie con-tract with shooting in the uk; radcliffewas unsure at the time if he would do anymore than that..the release of harry potter and thephilosopher’s stone (released as harrypotter and the sorcerer’s stone in theunited states) took place in 2001. rad-cliffe received a seven ﬁgure salary forthe lead role, but asserted that the feewas ”not that important” to him; his par-ents chose to invest the money for him.
the ﬁlm was highly popular and was metwith positive reviews, and critics took no-tice of radcliffe: ”radcliffe is the em-bodiment of every reader’s imagination.
it is wonderful to see a young hero whois so scholarly looking and ﬁlled with cu-riosity and who connects with very realemotions, from solemn intelligence andthe delight of discovery to deep familylonging, ”wrote bob graham of the sanfrancisco chronicle..a year later radcliffe starred in harrypotter and the chamber of secrets, thesecond installment of the series.
review-ers were positive about the lead actors’performances but had polarised opinions.
on the movie as a whole.
the 2004 re-lease harry potter and the prisoner ofazkaban was the third ﬁlm in the series.
radcliffe ’s performance was panned bynew york times journalist a.o.
scott,who wrote that watson had to carry himwith her performance.
next was harrypotter and the goblet of fire in 2005.the ﬁlm was the second-highest gross.
full information:.
sing potter ﬁlm in the series at the timeof its release.
after appearing in harrypotter and the philosopher’s stone, rad-cliffe tried to avoid trying to improve hisrole..shufﬂe all:.
sing potter ﬁlm in the series (behindharry potter and the chamber of se-crets), the highest grossing harry potterﬁlm in the series.
the following year,harry potter and the goblet of fire wasthe top-grossing potter ﬁlm, making itthe highest grossing harry potter ﬁlm inthe series..shuf.
within sent.:.
sing potter ﬁlm in the series (until 2009’sgold glove), selling 1.5 million ticketsin its ﬁrst week.
the title of the ﬁlm’ssequel was also a major factor in the suc-cess of philosopher, while its underper-formance dropped because of series 3’ssci-fi channel..shuf.
within trigrams:.
sing potter ﬁlm at the time of its re-lease.
the ﬁlm had a ﬁnancial successworldwide and received ﬁve academyaward nominations at the 56th academyawards..shuf.
trigrams within sent.:.
sing potter ﬁlm in the series at the unitedstates box ofﬁce.
his appearance inharry potter and the philosopher’s stonedid not make it to the united states until2006..863shuf.
trigrams globally:.
named entities:.
sing animated ﬁlm in the series..= = appearances extended persons (1990– 2014) = =.
sing animated ﬁlm in the series..rare:.
= = part two = =.
common:.
sing animated ﬁlm in the series.
a reviewin the new york times found that hanswas not as strong as watson but as wellas mr. trough and mr. trough..sing potter ﬁlm at the time and was thethird highest grossing ﬁlm of 2006, thehighest-grossing ﬁlm in the series.
in2006, radcliffe acted in his ﬁrst majorrole in harry potter and the deathly hal-lows – part i, where he played harry’schief engineer, harry potter’s primaryantagonist..sent.
shuf.:.
sing potter ﬁlm in the series at the time ofits release.
at the end of 2009, new back-ers were looking for a leading role in theﬁlm, with radcliffe starting the role ofharry potter and the philosopher’s stone..cont.
words:.
sing harry potter ﬁlm of all time afterharry potter and the goblet of fire, thehighest-grossing ﬁlm of the 2007–2008ﬁnancial year.
the ﬁlm’s success meantthat he and his wife bought a four-bedroom apartment in the 1960s, “readyto keep them warm”, adding “they couldnever make another effort to make some-thing exciting for people who were usu-ally very aware of the housing shortagebut hadn’t been very conscious of that”..n & vb & adj:.
sing animated ﬁlm in the series after thelion king.
a similar amount of praisecame in 2008 when the magic king wasreleased..n & vb:.
sing animated ﬁlm in the series.
a sig-niﬁcant amount of critical reviews weregiven by publications in the washingtonpost, san francisco chronicle, and thewashington post.
the ﬁlm was releasedin theaters in 2008..n:.
sing animated ﬁlm in the series..= appearances = =.
864