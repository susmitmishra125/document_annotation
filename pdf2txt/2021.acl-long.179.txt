better than average: paired evaluation of nlp systems.
maxime peyrard∗, wei zhao†, steffen eger†, robert west∗∗epfl, switzerland†technische universität darmstadt, germany{maxime.peyrard,robert.west}@epfl.ch{zhao,eger}@aiphes.tu-darmstadt.de.
abstract.
evaluation in nlp is usually done by com-paring the scores of competing systems inde-pendently averaged over a common set of testinstances.
in this work, we question the useof averages for aggregating evaluation scoresinto a ﬁnal number used to decide which sys-tem is best, since the average, as well as alter-natives such as the median, ignores the pair-ing arising from the fact that systems are eval-uated on the same test instances.
we illus-trate the importance of taking the instance-level pairing of evaluation scores into accountand demonstrate, both theoretically and em-pirically, the advantages of aggregation meth-ods based on pairwise comparisons, such asthe bradley–terry (bt) model, a mechanismbased on the estimated probability that a givensystem scores better than another on the testset.
by re-evaluating 296 real nlp evalua-tion setups across four tasks and 18 evaluationmetrics, we show that the choice of aggrega-tion mechanism matters and yields differentconclusions as to which systems are state ofthe art in about 30% of the setups.
to facil-itate the adoption of pairwise evaluation, werelease a practical tool for performing the fullanalysis of evaluation scores with the mean,median, bt, and two variants of bt (elo andtrueskill), alongside functionality for appro-priate statistical testing..1.introduction.
research is driven by evaluation results, with at-tention and resources being focused on methodsidentiﬁed as state of the art (sota).
the properdesign of evaluation methodology is thus crucialto ensure progress in the ﬁeld.
in nlp, evalua-tion usually consists in comparing the averagedscores of competing systems over a common setof test instances.
indeed, averaging scores inde-pendently for each system and declaring the onewith the highest average to be best is particularly.
figure 1: motivating example (synthetic data).
eval-uation scores of systems a, b, and c for ﬁve test in-stances.
all systems have the same mean.
c is betterthan a on all instances but one, so bt declares c > aalso, b is better than a on all instances but one, so btdeclares b > a, whereas the median of a is greater, andthe means are the same.
overall, mean and median failto capture the complex instance-level pairing..simple, well understood, and mirrors the expectedrisk minimization paradigm used to train systems.
here, we critically assess the speciﬁc choice ofthe average to aggregate evaluation scores.
in par-ticular, we emphasize that there is a natural in-stance-level pairing between the evaluation scoresof systems, which aggregation mechanisms suchas the mean or median fail to take into account: asthey produce a score for each system independently,systems that have the same set of scores (but poten-tially in different order) cannot be distinguished..consider the three systems a, b, and c comparedon ﬁve test instances in fig.
1. despite a complexpairing structure, they all have the same mean scoreacross test instances.
moreover, even though bis better than a on all test instances but one, themedian of a is greater than the median of b..in this work, we discuss an alternative aggrega-tion mechanism: the bradley–terry (bt) model.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2301–2315august1–6,2021.©2021associationforcomputationallinguistics230112345index of test instances2.55.07.510.012.515.017.5evaluation scores of systems  (higher is better)system amean (same for all)system bmedianssystem c(bradley and terry, 1952).
bt compares sys-tems for each test instance and estimates the latentstrength of systems based on how frequently onesystem scores higher than another.
such pairedmechanisms have already been successfully usedto aggregate human judgments (novikova et al.,2018; sedoc and ungar, 2020); for example, wmtevaluation protocols regularly employ trueskill(herbrich et al., 2007), a bayesian variant of bt(sakaguchi et al., 2014)..contributions.
we contribute the ﬁrst comprehen-sive analysis of the bt model (especially vis-à-vismean and median) as an aggregation mechanismfor comparing system scores in nlp..(i) we illustrate the importance of accounting forinstance-level pairing and discuss the conditionsunder which the mean, median, and bt disagreeabout the ordering of systems.
in sec.
3, we drawparallels with the ﬁeld of statistical testing, wherepaired statistical tests are recommended when com-paring paired variables.
thus, we argue that pairedaggregation mechanisms such as bt are more ro-bust alternatives to the mean and median.
we sup-port this argument with simulations in sec.
4..(ii) we show that the differences between mean,median, and bt matter in practice.
by re-evalu-ating 296 real nlp evaluation setups across fourtasks and 18 evaluation metrics, different aggrega-tion mechanisms yield different conclusions as towhich systems are sota in about 30% of the setups(sec.
5).
these results hold when replacing bt bythe elo (elo, 1978) and trueskill variants..(iii) we discuss further advantages and potentiallimitations of bt, alongside possible resolutions,in sec.
7..(iv) we recommend replacing the mean by btin future evaluations of nlp systems.
to easethe adoption of more robust aggregation mecha-nisms, we release pairformance,1 a practical toolfor performing full analyses of evaluation scoreswith mean, median, bt, and two variants of bt(elo and trueskill).
the tool reports paired evalua-tion results alongside appropriate statistical testingfor all ﬁve aggregation mechanisms and variousvisualization functionalities to elucidate the pairingstructure between system scores..code and data for replicating our analyses and.
experiments is available online.2.
1https://github.com/epfl-dlab/.
pairformance.
2https://github.com/epfl-dlab/bt-eval.
2 aggregation of evaluation results.
in this section, we brieﬂy present the three aggre-gation mechanisms we consider..2.1 terminology.
a standard evaluation setup typically consists offour elements:.
1. at least two systems, a and b, to compare,with latent strengths λa and λb that we aim toestimate..2. a test set t = (cid:8)(xl, yl) : l = 1, .
.
.
, n(cid:9) consist-ing of n test instances, where xl is the inputand yl is the ground-truth target output..3. an evaluation metric m for scoring systemoutputs based on target outputs yl, resultingin the sequence of evaluation scores ma =(cid:104)m(a(xl), yl) : l = 1, .
.
.
, n(cid:105) for system a.
4. an aggregation mechanism θ that decideswhether system a is better than b based onthe evaluation scores of the two systems.
weuse θt,m(a, b) = θ(ma, mb) to denote thecomparison mechanism between a and b onthe test set t with evaluation metric m. here,θ outputs its guess about which system is thebest (or declares the comparison inconclusiveif the difference is not statistically signiﬁcant).
for simplicity, we drop the dependency on tand m in the notation, simply writing θ(a, b)..for example in text summarization, xl is a sourcedocument from the test set, ylits correspond-ing reference summary, and m might be rouge(lin, 2004).
the decision mechanism θ usuallycompares the individual systems’ mean evaluationscores, where the system with the highest meanscore (here mean rouge score) is declared better..consistent evaluation result.
we say that the out-come of such an evaluation is consistent if it recov-ers the ordering of systems implied by the inherentstrengths of systems: θ(a, b) = a ⇐⇒ λa > λb..probabilistic model.
as commonly done in the lit-erature on statistical testing, we view the evaluationscores of a system a as n indexed random variables:x (l)a , l = 1, .
.
.
, n, where n is the size of the test set.
note that this sequence of random variables is notnecessarily i.i.d.
furthermore, even though systemsa and b are independent, their evaluation scoresare not, since there is an instance-level pairing.
in-tuitively, knowing the score of a on an instance(xl, yl) can provide information about the expected.
2302performance of b. for example, if a scores highlybecause (xl, yl) is an easy instance, one might ex-pect b to also score highly..2.2 aggregation mechanisms.
we now introduce three aggregation mechanisms θ.we investigate their properties in subsequent sec-tions..mean.
this is the current standard: the systemwith the highest average score is declared thestrongest.
we denote this aggregation mechanismas mean.
the average score of system a is com-puted as ea = 1n.x (l)a ..n(cid:80)l=1.
median.
the median is an interesting alternative tothe mean because it is robust to outliers.
here, thesystem with the highest median score is declared tobe the strongest.
the median score ma of a systema is the central value in the sorted list of evaluationscores of a. we denote this aggregation mechanismas median..bradley-terry.
the third option examined here isthe bradley–terry (bt) model (bradley and terry,1952).
while mean and median compute scoresfor systems a and b independently, bt is a func-.
bttion of the joint random variableestimates the relative strengths ˆλa and ˆλb of thetwo systems a and b, by comparing the evaluationscores for each test instance:.
a , x (l)x (l).
(cid:16).
(cid:17).
b.p(a > b) =.
ˆλaˆλa + ˆλb.
..(1).
intuitively, p(a > b) is the probability that, for anygiven test instance, a scores higher than b. the btmodel chooses ˆλa and ˆλb in order to best explainthe observations.
the system with the highest ˆλ isdeclared strongest..when considering only two systems, the la-tent strength ˆλa is the number of instances forwhich a scores better than b (and similarly forˆλb).
when the number of systems is greater thantwo, bt solves an iterative optimization algorithmthat is guaranteed to converge to a unique solu-tion (bradley and terry, 1952).
we give detailsabout bt and its computation in the general casein appendix e..we denote as bt the decision mechanism basedon the bt model.
while it is much less commonthan mean and median, we will see below thatbt satisﬁes interesting properties making it a morerobust alternative..3 comparison of assumptions.
since the roles played by a and b are symmetri-cal, we now assume without loss of generality thatsystem a is better, i.e., λa > λb.
proposition 1. if λa > λb then.
• mean consistent ⇐⇒ ea − eb > 0,• median consistent ⇐⇒ ma − mb > 0,• bt consistent ⇐⇒ ma−b > 0,.where es and ms are the mean and median of theevaluation scores of system s, and ma−b is themedian of the differences between the evaluationscores of a and b. note that es, ms, and ma−b areall random variables..the proof is given in appendix b. note that,whereas the expectation is linear (ea −eb = ea−b),the median is not (in general, ma − mb (cid:54)= ma−b)..robustness to ouliers.
the mean is not robust tooutliers: ea−b can be swayed above or below thethreshold of 0 by a small number of test instancesfor which the difference between system scoresis large.
on the contrary, the median is a robuststatistic that cannot be easily inﬂuenced by outliers.
similarly, bt is robust to outliers because its deci-sion is based on the median of differences ma−b..a and x (l).
importance of pairing.
the critical differencebetween bt, mean, and median, is that only btpreserves the pairing information.
both mean andmedian compute a statistic from the (unordered)set of scores x (l)b independently and thencompare the aggregate statistics, losing the pairingstructure.
if the pairing actually does not matter,any permutation of the indices of system scoresleaves the distribution of paired evaluation scoresunchanged.
this happens, for example, when botha and x (l)x (l)however, in the general case, the pairing mat-ters.
one particular example is when there existdifferent types of test instances and systems behavedifferently for different types, e.g., when there areeasy instances on which all systems have higherscores.
for example, consider the three systemsand their evaluation scores on ﬁve test instances infig.
1. system a is worse than c on all instancesbut one, so c > a according to bt, yet the medianof a is greater than the median of c (10 vs. 7).
atthe same time, b outperforms c on all instances.
b are i.i.d.3.
3more generally, when the two sequences of random vari-.
ables are exchangeable..2303but one, so b > c according to bt.
for medianand mean, which ignore the pairing, a and b arecompletely equivalent, even though there is a cleardifference regarding which system is more likely tobe the best.
this difference is revealed in the pair-ing structure.
in general, any mechanism ignoringthe pairing cannot capture the difference betweena and b..choosing an aggregation mechanism.
in prop.
1,we stated the conditions for each mechanism to beconsistent.
choosing an aggregation mechanismfor a speciﬁc evaluation setup boils down to de-ciding what condition is more likely to hold in thesetup.
note that none of the conditions implies anyother condition in prop.
1..when comparing bt against mean (or me-dian), there are three possible scenarios: (i) btagrees with mean (or median), (ii) bt is consis-tent but mean (or median) is not, and (iii) mean(or median) is consistent but bt is not..in case (i), it does not matter whether we use bt.
or mean (or median)..in case (ii), for most instances, the better systemhas a higher score than the worse system, but mean(or median) fails.
for example, mean may beswayed by outliers, and median may be swayedby jumps in score lists as in the example above..in case (iii), for most instances, the better systemhas a lower score than the worse system, yet par-ticular variations in the marginals make the meanor median get the ordering correct.
this is a verypeculiar scenario: for mean, it implies that on thefew instances on which the better system did bet-ter, the difference between evaluation scores waslarge enough to lift the mean of the better systemabove the other.
we argue that if one really be-lieves that the evaluation setup is likely to be incase (iii), then one does not trust the evaluationsetup in the ﬁrst place.
it corresponds to assumingthat the observed scores are inconsistent for themajority of test instances.
if this is the case, oneshould rather improve the evaluation setup (e.g.,metric, test set) in order to be more representativeof the phenomena that one desires to capture..overall, the condition making bt consistent ap-pears to be the most natural one.
trusting meanor median more than bt implies holding an un-intuitive belief about the evaluation setup, namelythat the better system does worse than the worsesystem on a majority of test instances..from another perspective, the random variablesea − eb (mean) and ma − mb (median) are lesslikely to be (correctly) greater than zero in the pres-ence of (i) complex pairing structures or (ii) out-liers.
the variable ma−b (bt), on the contrary, isnot affected by complex pairings or outliers..3.1 graphical criterion.
b is greater than that of x (l).
fig.
2 summarizes the problem of ignoring the pair-ing and offers a graphical criterion to understandthe decisions made by mean, median, and bt.
in each plot, the densities are estimated by placingtest instances at coordinates given by the evaluationscores of the two systems.
the evaluation scoresof a (green) are on the x-axis, and the evaluationscores of b (blue) on the y-axis.
we also plot themarginal distributions of evaluation scores, fromwhich we can read off means and medians.
whenthe mean of x (l)a , the twoextended lines representing the means meet in theupper triangle (above the line xa = xb), and analo-gously for the median.
but mean and median beingonly functions of the marginals, they completely ig-nore the pairing.
fig.
2 illustrates this by depictingthree completely different pairing structures wherethe marginals (and thus the means and medians)of a and b remain unchanged.
(in appendix a.1,we explain how to generate inﬁnitely many suchexamples.)
on the contrary, bt, being a propertyof the pairing (the 2d density), predicts that b isbetter than a when there is more mass in the uppertriangle, i.e., more instances for which b scoreshigher than a. in the middle ﬁgure, the pairingindicates that a is better than b, in disagreementwith the decisions of mean and median..3.2 connection with statistical testing.
the above discussion about the differences betweenmean, median, and bt has interesting parallelswith statistical testing..when comparing the means of two systems overthe same test set, the recommended statistical testis the paired t-test (fisher, 1935).
when comparingmedians instead of means, the appropriate test isthe sign test, which measures whether the medianof the difference is signiﬁcantly differerent fromzero.
interestingly, the statistic of the sign testis precisely the one in the condition for bt to beconsistent (see prop.
1).
wilcoxon’s signed-ranktest (wilcoxon, 1945) is often used as an alternativeto the sign test because it has more statistical power(at the cost of making more assumptions).
however,.
2304figure 2: these 2d plots represent the distribution of test instances with coordinates given by the scores of the twosystems being compared, i.e., the x-axis is the score x (l)a of system a on some test instance (xl, yl), and the y-axis isthe score x (l)b of system b on the same instance.
while the 3 plots represent different instance-level performancesof a and b, the marginal (unpaired) distribution of scores of a and b remain unchanged.
from such 2d plots, notonly do we see the global structure of the pairing between the scores of a and b, we can also read off the decisionof mean, median and bt based on simple geometrical criteria: (i) if the prolongation of the means intersectabove the xa = xb line, then mean predicts that a is better, (ii) if the prolongation of the medians intersect abovethe xa = xb line, then median predicts that a is better, (iii) if there is more mass in the upper-left triangle, thenbt predicts that system a is better.
the latter case corresponds to most of the test instances being located in theupper-left triangle (a > b).
the half-space with more mass is shaded..divine et al.
(2018) showed that wilcoxon’s signed-rank test does not always properly account for thepairing of data, unlike the sign test..when performing statistical testing, it seems ob-vious that we should use the paired version of testswhen the data is naturally paired (rankel et al.,2011).
even works discussing statistical testing innlp recommend wilcoxon’s signed-rank test (gra-ham, 2015; owczarzak et al., 2012; dror et al.,2018).
yet, to obtain aggregated scores for sys-tems, the community still mostly uses aggregationmechanisms that ignore the pairing, such as mean.
median is the outlier-resistant version of mean,and bt is the paired variant of median.
wheneverone recommends a paired test of medians, such asthe sign test or wilcoxon’s signed-rank test, to ob-tain p-values, one should use bt to compare systemscores..4 simulations with synthetic data.
next, we perform simulations to extend the anal-ysis of the previous section to (i) n > 2 systems,(ii) ﬁnitely many test samples, (iii) a practical im-plementation of bt (for n > 2 systems, bt is aniterative optimization algorithm, as discussed inappendix e)..we synthesize evaluation scores with variousproperties starting with systems of predeﬁned im-plicit strengths λi.
to create situations where thepairing of evaluation scores matters, we introduce.
multiple test instance types.
for each type, systemsperform differently but still have the same relativestrength (p(a > b)), differing only by an addedoffset.
for example, the evaluation scores obtainedby a and b could be sampled from n (λa, σ)and n (λb, σ) for one test instance type, and byn (λa + (cid:15), σ) and n (λb + (cid:15), σ) for another type,with (cid:15) being the offset.
we sample evaluation se-tups by varying the following properties: the num-ber of systems, the number of test instances, thepercentage of outliers, the numbers of test instancetypes, and the level of noise.
this results in 2,880simulated evaluation setups.
in appendix a.2, wegive the detailed algorithm and parameters used togenerate the data..in fig.
3, we report kendall’s τ between the la-tent scores λi and the aggregated scores estimatedby mean, median, and bt.
when the evaluationsetup does not present any difﬁculty (fig.
3(a, b)),all aggregation mechanisms work equally well(within each other’s 95% error bounds), improv-ing with more samples (fig.
3(b)) and deteriorat-ing with more systems (fig.
3(a)).
unsurprisingly,mean fails in the presence of outliers, whereasmedian and bt are unaffected (fig.
3(c, e, f)).
when several types of test instances are considered,median begins to fail (fig.
3(d)), which is madeworse when outliers are also present (fig.
3(f)).
overall, bt is more robust and does not fail whenthe pairing matters fig.
3(g, h)..2305102030405060scores of a102030405060scores of bmedianmeanxa=xb102030405060scores of a102030405060scores of bmedianmeanxa=xb102030405060scores of a102030405060scores of bmedianmeanxa=xbfigure 3: the y-axis is the kendall’s τ correlation between latent scores λi of systems and the scores obtainedafter aggregating simulated evaluation scores with mean, median, or bt.
fig.
3(a) and fig.
3(b) corresponds tothe intuitive case where no problem occurs (no outliers, no pairing issues).
fig.
3(c) adds outlier problems only,and fig.
3(d) adds pairing issues only by increasing the number of types of test instances.
fig.
3(e) and (f) showthe combined effect of outliers and pairing issues.
finally, fig.
3(g) and fig.
3(h) collect all the simulations.
theerror bars represent 95% conﬁdence intervals obtained with bootstrap resampling..5 analysis of empirical data.
5.1 comparison of bt, mean, and median.
in this section, we perform large-scale experi-ments using real evaluation scores from four nlgtasks.
for summarization, we use the tac-08,tac-09, tac-11 and cnn/dm (hermann et al.,2015) datasets.
for machine translation, we usethe shared tasks of wmt-17 (bojar et al., 2017),wmt-18 (ma et al., 2018), and wmt-19 (ma et al.,2019).
for image captioning, we use the mscoco(lin et al., 2014) dataset, and for dialogue, weuse the personachat and topicalchat (mehri andeskenazi, 2020) datasets.
the evaluation scoresare obtained with a total of 18 different evaluationmetrics: bleu-[1,2,3,4] (papineni et al., 2002),rouge-[1,2,l] (lin, 2004), rouge-we-[1,2](ng and abrecht, 2015), js-[1,2] (lin et al., 2006),s3-[pyr, resp] (peyrard et al., 2017), cider (vedan-tam et al., 2015), chrfpp (popovic, 2017), me-teor (lavie and agarwal, 2007), moverscore(zhao et al., 2019), and bertscore (zhang et al.,2020).
some metrics are only available for sometask; e.g., cider, meteor are only availablefor the image captioning task.
we provide detailsabout datasets, metrics, and their statistics in ap-pendix a.3..overall, across datasets and metrics we have296 evaluation setups, 73,471 pairs of systems, and91,197 test instances.
we also experiment withsub-sampling different sizes of test sets (see ap-pendix a.3) to simulate varying train/dev/test splitsor cross-validation..in table 1, we report the disagreement between ag-gregation mechanisms over all the data with threemeasures: the percentage of pairs ranked in a differ-ent order (rescaled version of kendall’s τ ), the per-centage of setups where the state-of-the-art (sota)systems are different, and the percentage of se-tups where the top 3 systems are different (com-pared as sets).
a signiﬁcant fraction of pairs ofsystems (about 10%) are ranked differently by dif-ferent mechanisms.
more importantly, top systemsare often different (in about 40% of setups for top1 and 50% for top 3).
we can conclude that thechoice of aggregation mechanism has a real impacton evaluation outcome.
the observed disagreementbetween the three aggregation metrics implies thatwe are not in the case depicted by fig.
3(a) andfig.
3(b), i.e., the pairing matters and there are out-liers in real data.
in the next paragraphs, we breakdown the disagreement per evaluation metric, task,and test set size.
detailed results are provided inappendix c..which metrics are impacted most?
we reportin fig.
4(a) the percentage of disagreement betweenaggregation mechanisms per metric averaged overdatasets, when subsampling test sets of differentsizes uniformly (see appendix a.3 for details).
while most metrics are available for all four tasks,meteor and cider are only available for thecaptioning task.
therefore, the observed disagree-ments for these metrics may be a feature of the taskinstead of the metrics.
interestingly, recent metrics.
230602040number of systems0.51.0a) no outliers - no pairing issues0.000.010.02percentage of outliersc) with outliers - no pairing issues246810number of test instances typese) with outliers - with pairing issues50100150200number of samplesg) all simulations50100150200number of samples0.51.0kendall's  with true strengthsb) no outliers - no pairing issues246810number of test instances typesd) no outliers - with pairing issues0.000.010.02percentage of outliers (varying test types)f) with outliers - with pairing issues02040number of systemsh) all simulationsmeanmedianbtdisagree.
(cid:54)= sota (cid:54)= top-3.
mean vs.medianmean vs. btmedian vs. bt.
4%9%9%.
18%40%41%.
30%49%55%.
table 1: disagreement between aggregation mecha-nisms.
the ﬁrst column shows the percentage of sys-tem pairs ordered differently by two aggregation mech-anisms.
the second column shows the percentage ofsetups where two aggregation mechanisms ﬁnd differ-ent sota, and the third column shows the percentage ofsetups where the top-3 systems are different (comparedas sets)..such as bertscore and moverscore seem lessaffected.
on the other hand, bleu variants arethe most impacted, particularly when comparingmean or median against bt.
the disagreementbetween mean and median is stable across met-rics.
in general, mean and median are more inagreement with one another than they are with bt,which indicates that pairing issues have a strongereffect than outliers..which tasks are impacted most?
fig.
4(b) sum-marizes an analysis as above, but across tasks in-stead of metrics.
again, to control for the fact thatsome tasks may have larger datasets, we subsampleuniformly from various test set sizes.
the resultsare averaged over evaluation metrics.
machinetranslation and summarization suffer the least whiledialogue and image captioning display larger dis-agreement between aggregation mechanisms.
thissuggests important future research directions toimprove the evaluation setups in these tasks..importance of dataset size.
in fig.
4(c), we re-port disagreement across test set sizes, while av-eraging over datasets and evaluation metrics.
itis reassuring to observe that with larger test sets,the different mechanisms tend to agree more, suchthat it matters less which one is actually chosen.
however, for mean vs. bt and median vs. bt,the disagreement does not continue to decrease be-low 10% with more test instances.
for mean andbt the disagreement is lower but exhibits the samebehavior, never falling below a certain threshold..different perspectives on uncertainty.
in stan-dard evaluation setups, not only system scores arereported but also whether the differences are sta-tistically signiﬁcant (dror et al., 2018).
therefore,we ask how often differences that are statisticallysigniﬁcant for one test are also statistically signif-.
icant for another.
the details of this experimentsare presented in appendix d and show, perhaps un-surprisingly, different behavior for different tests.
in particular, the paired t-test is the one that mostoften ﬁnds differences to be signiﬁcant (for 41%of pairs); mood’s test, an unpaired test to comparemedians, ﬁnds signiﬁcance for only 21% of pairs;and the sign test and wilcoxon’s sign-rank test (re-lated to bt) are in between (for 35% and 40% ofthe pairs, respectively)..sources of disagreement.
based on the analysisof sec.
3, we know that the difference betweenmean and median is due to the presence of sta-tistical outliers, while the difference between me-dian and bt is due to the presence of differenttest instance types (fig.
3).
with real nlp datasets,in fig.
4, we observe some discrepancy betweenmean and median, indicating the presence of out-liers.
there is even more disagreement betweenmedian and bt, indicating the presence of differ-ent types of test instances, as illustrated in fig.
3..6 related work.
several studies have made a critical assessmentof the standard evaluation methodologies.
for ex-ample, freitag et al.
(2020) demonstrate the ad-vantages of carefully choosing which referencesto use for nlg evaluation.
mathur et al.
(2020)show that outliers matter in practice.
recently, gra-ham et al.
(2020) draws attention on test set size.
several works have emphasized the importanceof careful statistical testing (rankel et al., 2011;owczarzak et al., 2012; graham, 2015; dror et al.,2018).
they recommend paired statistical tests.
finally, novikova et al.
(2018) report that “rela-tive rankings yield more discriminative results thanabsolute assessments”, which further motivates ag-gregation mechanisms like bt..aggregations.
pairwise comparison mechanismsdate back to thurstone (1927).
subsequently, thebradley-terry (bt) model has become a standardpairwise comparison model (bradley and terry,1952).
in nlp, bt-inspired mechanisms havesometimes been used to aggregate human assess-ments.
for instance, deriu et al.
(2020) rankedchatbots regarding their ability to mimic conversa-tional behavior of humans.
item response theory(irt) has a similar formulation as bt, but alsoestimates the difﬁculty of each test instances us-ing a latent-variable bayesian model (dras, 2015)..2307figure 4: this ﬁgure measures the percentage of disagreement between each pair of aggregation mechanismsacross different dimensions with real evaluation setups.
fig.
4(a) shows the disagreement per evaluation metricaveraged over tasks and uniformly subsampled test set sizes, fig.
4(b) shows the disagreement per task averagedover evaluation metrics and uniformly subsampled test set sizes, and fig.
4(c) shows the disagreement across testset sizes averaged over tasks and evaluation metrics..irt has been applied to perform dataset ﬁltering(lalor et al., 2016, 2019), evaluate chatbots fromhuman assessments (sedoc and ungar, 2020), andaggregate human assessments in machine transla-tion (dras, 2015).
elo (elo, 1978) and trueskill(herbrich et al., 2007) are famous extensions ofthe bt model commonly used to rate players inthe context of gaming or sports events.
elo viewsplayer strengths as normally distributed randomvariables.
trueskill is a bayesian variant of elo.
since 2015, the workshop on machine translation(wmt) has been using trueskill to rank modelsbased on human assessments following the method-ology of sakaguchi et al.
(2014).
we provide adetailed presentation and comparison of bt, elo,and trueskill in appendix g, and make both eloand trueskill available as alternatives to bt in thereleased tool.
the arguments in favor of bt madein this work transfer to its variants, including irt,elo, and trueskill, and the conclusions drawn fromthe experiments of sec.
5 still hold when replacingbt by elo or trueskill (appendix g).
our workextends previous works that has considered bt vari-ants by analyzing the potential causes for disagree-ment with mean and median and by measuringthe disagreement in real nlp evaluation setups..7 discussion.
we brieﬂy discuss some possible questions raisedby the use of bt-like metrics, with more detailsprovided in appendix e, f, g, and h..extension to other evaluation setups.
the exper-iments of sec.
5 focus on reference-based nlgevaluation metrics.
however, the arguments laidout throughout the paper apply beyond this setup.
any comparison of systems based on score aggre-gation is susceptible to suffer from outliers andcomplex pairing structures (e.g., fig.
2).
futurework should replicate our experimental setup forreference-free nlg (zhao et al., 2020), classiﬁca-tion, or regression tasks..type imbalance.
imagine a test set with a major-ity of easy instances and few hard ones.
a systema could perform slightly worse than b on easy in-stances but much better on hard ones and will bedeclared worse by bt.
if one views this decisionas problematic then one should probably acknowl-edge that the test set is not representative of whatshould be measured.
if hard instances matter morethere should be a majority of them in the test set.
hoping that mean will be swayed to output theintuitive ordering of systems from a minority of testinstances is a peculiar expectation to have about theevaluation setup.
to diagnose such pathologicalcases, our tool, pairformance, offers the possibilityto view pairwise plots (as in fig.
2) and histograms.
2308bleu-1bleu-2bleu-3meteorciderrouge-we-2rouge-we-1rouge-1rouge-2s3-respchrfppmoverscorebertscore0.00.10.20.3percentagedisagreementa)permetricdisagreementdialoguecaptio.summ.mt0.00.10.2percentagedisagreementb)pertaskdisagreement010002000300040005000sizeoftestset0.050.100.150.20percentagedisagreementc)pertestsetsizedisagreementmeanvs.btmedianvs.btmeanvs.medianof score differences.
more generally, better ag-gregation mechanisms such as bt do not solve allpotential problems of evaluation methodologies.
other aspects (such as choosing evaluation metricsor meaningful, representative, and large test sets)are all independent of the choice of aggregationmechanism, but also critical to the quality of theevaluation..transitivity.
bt is not computed independentlyfor each system, and it can happen that adding orremoving a baseline impacts the scores of other sys-tems.
we explain this phenomenon in appendix fand show that it is rarely a problem in real data.
more generally, we discuss the connection witharrow’s impossibility theorem in the context of theaggregation of social preferences (arrow, 1950).
the pairformance tool gets around this difﬁcultyby offering the possibility of analyzing each pairof systems independently..relaxing assumptions.
bt assumes that the rel-ative strengths of systems remain constant acrosstest instances.
this might not always be true, es-pecially when some systems are crafted for somespeciﬁc kind of instances but perform badly on oth-ers.
in such cases, bt still produces meaningful andeasily interpretable results but fails to capture thelatent structure of system strengths.
several reﬁne-ments of bt are possible; e.g., item response theoryextends bt by modeling instance difﬁculty, and eloand trueskill allow system strengths to be stochas-tic and vary across instances.
these reﬁnementscome at the cost of introducing new parameters,and it remains unclear how to choose these param-eters in practice.
future work should investigatesystematic ways to choose these parameters..tool description.
we release pairformance, a toolfor performing full diagnostic analyses based onan evaluation dataframe made of the evaluationscores of systems and baselines.
it can performthe analysis based on mean, median, bt, elo,and trueskill.
for each aggregation technique, itoutputs a full pairwise analysis of all pairs of sys-tems.
for mean and median it compares scoredifferences for pairs of systems.
for bt, elo, andtrueskill, it estimates the probability that one sys-tem is better than another.
all analysis is accompa-nied by appropritate statistical testing.
see fig.
5for an example based on the bt mechanism.
fur-thermore, the tool can plot the histogram of paireddifferences x (l)b , allowing for the direct iden-.
a − x (l).
figure 5: pairwise system comparison with bt for ma-chine translation with rouge-1, as output by the pair-formance tool released as part of this work..tiﬁcation of pathological patterns such as thosediscussed above..8 conclusion.
we performed a critical assessment of the standardnlp evaluation methodology based on averagedscores, which ignores the natural instance-levelpairing of evaluation scores when comparing sys-tems.
we showed the importance of the pairing anddemonstrated the advantages of paired mechanismssuch as bradley–terry (bt) over more standard ag-gregation schemes such as the mean or median.
the choice of aggregation mechanism matters inreal evaluation setups, and we therefore recom-mend bt as a robust aggregation mechanism.
tofacilitate adoption, we release pairformance, a newtool to perform full analyses of system scores usingbt and two of its variants, elo and trueskill..acknowledgments.
we thank the anonymous reviewers for their insight-ful comments and suggestions, which greatly im-proved the ﬁnal version of the paper.
with supportfrom swiss national science foundation (grant200021_185043), european union (tailor, grant952215), and gifts from google, facebook, mi-crosoft..references.
kenneth j. arrow.
1950. a difﬁculty in the conceptjournal of political economy,.
of social welfare.
58(4):328–346..ondˇrej bojar, yvette graham, and amir kamran.
2017.results of the wmt17 metrics shared task.
inproceedings of the second conference on machine.
2309jhu-pbmt.4972jaist.4859afrl-mitll-syscomb.4902lium-nmt.4888afrl-mitll-m2w-nr1.4901promt-smt.4737uedin-nmt.4931online-g.0online-a.0online-b.0jhu-pbmt.4972jaist.4859afrl-mitll-syscomb.4902lium-nmt.4888afrl-mitll-m2w-nr1.4901promt-smt.4737uedin-nmt.4931online-g.0online-a.0online-b.0-0.53*0.37*0.36*0.31*0.30*0.28*0.24*0.22*0.17*0.47*-0.41*0.41*0.38*0.31*0.32*0.24*0.22*0.19*0.63*0.59*-0.460.26*0.39*0.36*0.33*0.28*0.24*0.64*0.59*0.54-0.440.41*0.34*0.37*0.29*0.24*0.69*0.62*0.74*0.56-0.41*0.39*0.36*0.28*0.26*0.70*0.69*0.61*0.59*0.59*-0.430.420.33*0.31*0.72*0.68*0.64*0.66*0.61*0.57-0.420.34*0.33*0.76*0.76*0.67*0.63*0.64*0.580.58-0.44*0.40*0.78*0.78*0.72*0.71*0.72*0.67*0.66*0.56*-0.410.83*0.81*0.76*0.76*0.74*0.69*0.67*0.60*0.59-translation, pages 489–513, copenhagen, denmark.
association for computational linguistics..ralph allan bradley and milton e. terry.
1952. rankanalysis of incomplete block designs: i. the methodof paired comparisons.
biometrika, 39(3/4):324–345..jan deriu, don tuggener, pius von däniken, jon andercampos, alvaro rodrigo, thiziri belkacem, aitorsoroa, eneko agirre, and mark cieliebak.
2020.spot the bot: a robust and efﬁcient framework forthe evaluation of conversational dialogue systems.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3971–3984, online.
association for computa-tional linguistics..george w. divine, h. james norton, anna e.barón, and elizabeth juarez-colunga.
2018. thewilcoxon–mann–whitney procedure fails as a testof medians.
the american statistician, 72(3):278–286..mark dras.
2015. squibs: evaluating human pairwisepreference judgments.
computational linguistics,41(2):309–317..rotem dror, gili baumer, segev shlomov, and roi re-ichart.
2018. the hitchhiker’s guide to testing statis-tical signiﬁcance in natural language processing.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1383–1392, melbourne, aus-tralia.
association for computational linguistics..arpad e. elo.
1978. the rating of chessplayers, past.
and present.
arco publishing..ronald a. fisher.
1935. the design of experiments..oliver and boyd, edinburgh..markus freitag, david grangier, and isaac caswell.
2020. bleu might be guilty but references are notin proceedings of the 2020 conferenceinnocent.
on empirical methods in natural language process-ing (emnlp), pages 61–71, online.
association forcomputational linguistics..yvette graham.
2015. re-evaluating automatic sum-marization with bleu and 192 shades of rouge.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages128–137.
association for computational linguis-tics..yvette graham, barry haddow, and philipp koehn.
2020. statistical power and translationese in ma-chine translation evaluation.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 72–81, on-line.
association for computational linguistics..ralf herbrich, tom minka, and thore graepel.
2007.trueskill™: a bayesian skill rating system.
in ad-vances in neural information processing systems,volume 19, pages 569–576.
mit press..karl moritz hermann, tomas kocisky, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readin advances in neural informa-and comprehend.
tion processing systems, volume 28, pages 1693–1701. curran associates, inc..john p. lalor, hao wu, and hong yu.
2016. build-ing an evaluation scale using item response theory.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages648–657, austin, texas.
association for computa-tional linguistics..john p. lalor, hao wu, and hong yu.
2019. learn-ing latent parameters without human response pat-terns: item response theory with artiﬁcial crowds.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4249–4259, hong kong, china.
association for computa-tional linguistics..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with highlevels of correlation with human judgments.
inproceedings of the second workshop on statisticalmachine translation, statmt ’07, pages 228–231,stroudsburg, pa, usa.
association for computa-tional linguistics..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..chin-yew lin, guihong cao, jianfeng gao, and jian-yun nie.
2006. an information-theoretic approachto automatic evaluation of summaries.
in proceed-ings of the human language technology confer-ence of the naacl, main conference, pages 463–470, new york city, usa.
association for compu-tational linguistics..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr dollár,and c. lawrence zitnick.
2014. microsoft coco:common objects in context.
in computer vision –eccv 2014, pages 740–755, cham.
springer inter-national publishing..qingsong ma, ondˇrej bojar, and yvette graham.
2018.results of the wmt18 metrics shared task: bothcharacters and embeddings achieve good perfor-mance.
in proceedings of the third conference onmachine translation: shared task papers, pages671–688, belgium, brussels.
association for com-putational linguistics..qingsong ma, johnny wei, ondˇrej bojar, and yvettegraham.
2019. results of the wmt19 metricsshared task: segment-level and strong mt sys-in proceedings of thetems pose big challenges.
fourth conference on machine translation (volume.
2310conference on empirical methods in natural lan-guage processing, pages 467–473, edinburgh, scot-land, uk.
association for computational linguis-tics..keisuke sakaguchi, matt post,.
and benjaminvan durme.
2014. efﬁcient elicitation of annota-tions for human evaluation of machine translation.
in proceedings of the ninth workshop on statisti-cal machine translation, pages 1–11, baltimore,maryland, usa.
association for computationallinguistics..joão sedoc and lyle ungar.
2020. item response the-ory for efﬁcient human evaluation of chatbots.
inproceedings of the first workshop on evaluationand comparison of nlp systems, pages 21–33, on-line.
association for computational linguistics..louis leon thurstone.
1927. a law of comparativejudgement.
psychological review, 34:278–286..ramakrishna vedantam, c. lawrence zitnick, anddevi parikh.
2015. cider: consensus-based im-in ieee conferenceage description evaluation.
on computer vision and pattern recognition, cvpr2015, boston, ma, usa, june 7-12, 2015, pages4566–4575..frank wilcoxon.
1945..individual comparisons by.
ranking methods.
biometrics bulleting, 6:80–83..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..wei zhao, goran glavaš, maxime peyrard, yang gao,robert west, and steffen eger.
2020. on the lim-itations of cross-lingual encoders as exposed byreference-free machine translation evaluation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1656–1671, online.
association for computational lin-guistics..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m. meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 563–578, hongkong, china.
association for computational lin-guistics..2: shared task papers, day 1), pages 62–90, flo-rence, italy".
association for computational lin-guistics..nitika mathur, timothy baldwin, and trevor cohn.
2020. tangled up in bleu: reevaluating the eval-uation of automatic machine translation evaluationin proceedings of the 58th annual meet-metrics.
ing of the association for computational linguistics,pages 4984–4997, online.
association for computa-tional linguistics..shikib mehri and maxine eskenazi.
2020. usr: anunsupervised and reference free evaluation metricfor dialog generation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 681–707, online.
association forcomputational linguistics..jun-ping ng and viktoria abrecht.
2015. better sum-marization evaluation with word embeddings forrouge.
in proceedings of the 2015 conference onempirical methods in natural language processing,pages 1925–1930, lisbon, portugal.
association forcomputational linguistics..jekaterina novikova, ondˇrej dušek, and verena rieser.
2018. rankme: reliable human ratings for naturalin proceedings of the 2018language generation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 72–78, new orleans, louisiana.
associationfor computational linguistics..karolina owczarzak, john m. conroy, hoa trangdang, and ani nenkova.
2012. an assessment ofthe accuracy of automatic evaluation in summa-in proceedings of workshop on evalua-rization.
tion metrics and system comparison for automaticsummarization, pages 1–9.
association for compu-tational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automaticevaluation of machine translation.
in proceedingsof the 40th annual meeting on association for com-putational linguistics, acl ’02, pages 311–318,stroudsburg, pa, usa.
association for computa-tional linguistics..maxime peyrard, teresa botschen,.
and irynagurevych.
2017.learning to score systemsummaries for better content selection evaluation.
in proceedings of the workshop on new frontiersin summarization..maja popovic.
2017. chrf++: words helping charac-ter n-grams.
in proceedings of the second confer-ence on machine translation, wmt 2017, copen-hagen, denmark, september 7-8, 2017, pages 612–618..peter rankel, john conroy, eric slud, and dianneo’leary.
2011. ranking human and machine sum-in proceedings of the 2011marization systems..2311type 1.type 2.type 3.type 4.type 5.sb.
2328.
5045.
4030.
7065.
6050.table 2: example of two systems s and b with theirstrengths λti,s and λti,b, i ∈ [1, 5] associated to each typeof test instances.
types..a reproducibility.
in this section, we give additional details to ensurethe reproducibility of our experiments.
further-more, the code and data to reproduce each ﬁg-ure and table of the main paper is available at:https://github.com/epfl-dlab/bt-eval..a.1 pairing examples.
it is straightforward to generate examples wherethe marginal distribution of the evaluation scoresof two systems remain unchanged even when thepairing varies..to do so, one can deﬁne k types of test instances.
for each type ti, each system has a probability dis-tribution of scores for this type: n (λti,s, 1).
so forinstances of type ti, the system s has score λti,s inexpectation with a variance of σ2 = 1. similarly,another system b can have different λti,b parame-ters.
an example is given in table 2..now, observe that permuting the columns of swithout changing the row b leaves the marginaldistribution of s and b unchanged but changes thepairing.
then, one can simply iterate over all per-mutations of the row s to obtain many differentpairings with ﬁxed marginal distributions..a.2 simulation.
we discuss the synthetic data and experiments de-picted in fig.
3..to introduce pairing issues, we create a variablenumber of test instance types: ntypes.
for eachtest type, each system has a different distributionof scores.
on test type ti, the system s j has a nor-mal distribution of scores: n (λi, j, σ2), where weﬁx σ2 = 1 throughout our experiments.
for eachsystem, the λi, j are sampled uniformly from [0, 1].
depending on the values of λi, j, the score distribu-tion of system s j can become multimodal.
when,there is only one test type, the score of each sys-tem s j is a normal n (λ j, σ2).
in that case, thepairing can be ignored and mean and median areexpected to work well..for outliers, we deﬁne f as the fraction of testinstances on which systems’ scores are not drawnfrom their distribution scores.
for such instances,we ﬁrst draw the scores for each systems accordingto their distribution and then perform a randompermutation, so that each system receives a scorethat is not sampled from its score distribution..then, we vary the number of systems presentin the evaluation nsys and the number of test in-stances m. each choice of ntypes, f , nsys, and mgives a dataframe corresponding to an evaluationsetup on which we can compare mean, median,and bt against the true latent strengths of systemsλi, j. the evaluation and the y-axis in fig.
3 isthen the kendall’s τ between the ordering resultingfrom mean, median, or bt against the orderingresulting from the λi, j..we consider the following variations for the pa-.
rameters of the experiments:.
• ntypes ∈ {1, 3, 5, 10},• f ∈ {0., 0.01, 0.025},• nsys ∈ {2, 3, 5, 10, 25, 50},• m ∈ {10, 30, 100, 200}..in total, we have: 4 · 3 · 6 · 4 = 288 parameterchoices.
for each we sample 10 datasets result-ing in 2, 880 synthetic evaluation setups..a.3 real data.
each of the dataset we use contains the evaluationresults of a varying number of systems for a varyingnumber of evaluation metrics:.
summarization: cnn/dm (hermann et al.,2015): 11,432 test instances, 12 summarizationsystems, and 13 evaluation metrics.
tac-08: 48test instances, 58 summarization systems, and 13evaluation metrics.
tac-09: 44 test instances, 55summarization systems, and 13 evaluation met-rics.
tac-11: 44 test instances, 50 summarizationsystems, and 13 evaluation metrics.
captioning:mscoco (lin et al., 2014): 40,504 test instances,12 systems, and 7 evaluation metrics.
dialogue:topical-chat (mehri and eskenazi, 2020): 60 testinstances, 5 systems, and 13 evaluation metrics.
persona-chat (mehri and eskenazi, 2020): 60 testinstances, 4 systems, and 13 evaluation metrics.
mt: wmt-17 (bojar et al., 2017): evaluated with11 evaluation metrics, we have the following pairs:lv-en (2,001 instances, 9 systems), de-en (3,004instances, 11 systems), ru-en (3,001 instances, 9systems), tr-en (3,007 instances, 10 systems), and.
2312zh-en (2,001 instances, 16 systems).
wmt-18 (maet al., 2018): evaluated with 13 evaluation metricswe have the following pairs: de-en (2,998 instances,16 systems), et-en (2,000 instances, 14 systems),ﬁ-en (3,000 instances, 9 systems), ru-en (3,000 in-stances, 8 systems), and zh-en (3,981 instances,14 systems).
wmt-19 (ma et al., 2019): evalu-ated with 13 evaluation metrics we have the fol-lowing pairs: de-en (2,000 instances, 16 systems),ﬁ-en (1,996 instances, 12 systems), gu-en (1,016instances, 12 systems), kk-en (1,000 instances, 11systems), lt-en (1,000 instances, 11 systems), ru-en (2,000 instances, 14 systems), and zh-en (2,000instances, 15 systems)..the evaluation metrics considered are: bleu-[1,2,3,4] (papineni et al., 2002), rouge-[1,2,l](lin, 2004), rouge-we-[1,2] (ng and abrecht,2015), js-[1,2] (lin et al., 2006), s3-[pyr, resp](peyrard et al., 2017), cider (vedantam et al.,2015), chrfpp (popovic, 2017), meteor (lavieand agarwal, 2007), moverscore (zhao et al.,2019), and bertscore (zhang et al., 2020).
thisis a total of 18 metrics..sub-sampling test set sizes.
in experiments re-ported by fig.
4 the results are averaged after re-sampling test sets of different sizes.
the test setsizes used are: [10, 50, 100, 500, 1000, 5000].
re-sults broken down per dataset and per metric thatdoes not need resampling of test set sizes is pro-posed in appendix c..a.4.
implementations.
we implement bt with scipy.org and numpy.
forthe statistical tests, we use the default implemen-tation from scipy.org.
for elo, we implement awrapper around existing code: https://github.
com/ddm7018/elo.
similarly, for trueskill, we im-plement a wrapper around existing code: https://pypi.org/project/trueskill/..b proof of proposition 1.proof.
we observe that the case of the mean andthe median are direct by deﬁnition..a > x (l).
ma−b > 0 is equivalent to saying that for morethan 50% of instances, x (l)b , i.e., a is betterthan b on more than 50% of instances.
on theother hand, bt correctly gives a better than b ⇐⇒p(a > b) > p(b > a) ⇐⇒ p(a > b) > 12 , i.e., ais better than b on more than 50% of instances.
so,bt is consistent ⇐⇒ a is better than b on morethan 50% of instances ⇐⇒ ma−b > 0..c disagreement breakdown.
compared to experiments in the main paper, weprovide a more detailed breakdown of the disagree-ment in table 3..d different view on uncertainty.
as argued in the main paper ( sec.
3.2), the choiceof aggregation mechanism bears strong similaritieswith the choice of statistical test.
thus, we measurein how many setups difference between systemsthat are statistically signiﬁcant according to onetest are also signiﬁcant according to another..we compare: paired t-test (usually to comparemeans), the mood’s median test, and the sign test(consistent with bt).
we also add the wilcoxonsign-rank test as it was often recommended by pre-vious work (owczarzak et al., 2012; dror et al.,2018)..in fig.
6, we plot the frequency with which testj yields a signiﬁcant difference among the pairsof systems for which the test i has already yieldeda signiﬁcant difference.
the diagonal depicts theoverall percentage of pairs of systems for whichthe test ﬁnds a signiﬁcant difference.
note that thematrix is not symmetric..interestingly, when the mood’s median test saysthe difference between two system is signiﬁcant,98% of the times it is also the case for the pairedt-test and 89% of the times it is also the case forthe sign test.
so the mood’s median is the mostrestrictive, ﬁnding less often signiﬁcant differencethan the other two.
in comparison, the sign testand the wilcoxon’s sign-rank test ﬁnd signiﬁcantdifferences between systems much more frequently.
in general, the paired t-test is the one ﬁnding dif-ferences the most frequently..e details about the bradley–terry model.
given a pair of systems si and s j, the bradley–terry model estimates the probability pi, j that thesystem si is better than the system s j based on their.
relative strengths:.
λiλi+λ j.bt estimates these parameters λi for each of then systems from the observed results of evaluation.
we denote as ωi, j the number of instances for whichsi scores higher than s j. note that, in our setup,there is one comparison per test instance.
in themain paper, we said that the solutions for ˆλ arefound in closed-form for n = 2. when the numberof systems is greater than 2, the parameters are.
2313rouge-wemean/bt med/bt mean/med mean/bt med/bt mean/med mean/bt med/bt mean/med mean/bt med/bt mean/med mean/bt med/bt mean/med.
moverscore.
bertscore.
rouge.
bleu.
tac08.
tac09.
tac11.
cnn/dm.
wmt17.
wmt18.
wmt19.
tc.
pc.
mscoco.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
disagree.
(cid:54)= sota(cid:54)= top3.
.09.43.73.
.08.00.70.
.07.37.73.
.14.53.97.
.07.17.43.
.09.67.77.
.07.32.54.
.26.53.57.
.28.50.33.
.201.01.0.
.13.73.77.
.13.00.70.
.12.67.87.
.17.80.97.
.08.19.57.
.09.63.74.
.08.36.42.
.22.43.60.
.24.50.33.
.181.01.0.
.15.47.77.
.13.00.70.
.12.50.83.
.12.83.90.
.05.14.40.
.03.24.25.
.04.25.30.
.34.66.63.
.32.63.43.
.12.00.17.
.07.33.61.
.08.00.63.
.06.42.58.
.08.00.73.
.07.28.56.
.11.55.56.
.10.44.48.
.24.52.57.
.25.42.42.
.18.031.0.
.13.52.80.
.16.00.87.
.13.64.88.
.07.00.49.
.07.42.63.
.11.65.69.
.11.45.54.
.19.46.56.
.23.53.60.
.14.031.0.
.14.47.81.
.16.00.82.
.12.61.87.
.02.00.24.
.04.23.29.
.04.26.39.
.04.18.30.
.24.40.60.
.22.43.55.
.03.00.47.
.12.58.87.
.07.00.48.
.05.33.60.
.06.00.90.
.07.35.57.
.12.61.66.
.11.46.51.
.27.53.62.
.21.28.37.
---.
.06.20.65.
.15.00.73.
.13.67.93.
.05.00.42.
.08.40.67.
.12.6777.
.11.48.54.
.28.63.55.
.22.33.72.
---.
.13.47.80.
.16.00.75.
.12.65.92.
.02.00.48.
.04.19.40.
.04.66.40.
.05.16.33.
.22.45.47.
.22.30.63.
---.
.05.10.43.
.06.00.33.
.04.40.57.
.07.00.00.
.03.22.26.
.06.47.57.
.05.32.54.
.28.63.63.
.12.33.23.
---.
.11.50.73.
.14.00.70.
.11.63.87.
.06.00.00.
.04.15.37.
.06.49.58.
.04.25.41.
.19.33.60.
.20.47.30.
---.
.12.47.70.
.13.00.70.
.10.63.80.
.08.00.00.
.03.24.37.
.04.18.33.
.05.33.46.
.29.53.60.
.19.50.27.
---.
.05.13.60.
.06.00.43.
.04.27.43.
.08.00.90.
.03.15.23.
.06.43.57.
.04.31.39.
.18.30.53.
.13.30.27.
---.
.11.17.93.
.12.00.73.
.11.73.87.
.08.00.90.
.04.22.27.
.06.47.58.
.04.17.26.
.24.40.57.
.12.37.20.
---.
.12.27.87.
.12.00.67.
.10.63.83.
.04.00.06.
.03.24.33.
.03.31.19.
.05.35.39.
.20.27.57.
.13.43.07.
---.
table 3: disagreement between aggregation mechanisms per dataset and per metric..dates (at step t):.
.
.
(cid:88).
ˆλi = wi.
.
−1.
ωi, j + ω j,ii + λ(t)λ(t).
j.λ(t+1)i.
=.
, ∀i..i(cid:54)= jˆλi(cid:80)k.ˆλk.
.
, ∀i,.
(3).
(4).
it can be shown that starting from a random λ thisalgorithm improves the log-likelihood at every iter-ation and converges to a unique maximum..for the practical implementation, only a thresh-old (cid:15) deﬁning when to stop has to be decided.
wechoose to stop iterating when at step t, if the newvector of parameter λ remains close to the previousone: (cid:107)λ(t+1) − λ(t)(cid:107)2 < (cid:15).
throughout our experi-ments, we always set (cid:15) = 1 · 10−9..figure 6: in this matrix, the cell in row i and columnj indicates the frequency with which the test j ﬁndsa difference signiﬁcant among the pairs of systems forwhich the test i has found the difference signiﬁcant.
forexample, when the mood’s median test ﬁnds a signiﬁ-cant difference between a pair, 98% of the times, thepaired t-test also ﬁnds the difference signiﬁcant..found by an iterative optimization algorithm thatmaximizes the following log-likelihood:.
f transitivity with bt and arrow’s.
theorem.
l (λ) =.
ωi, j log(λi) − ωi, j log(λi + λ j),.
n(cid:88).
n(cid:88).
i=1.
j=1.
(2).
where λ = [λ1, .
.
.
, λn]..denote wi as the number of comparison in whichsystem i is better: wi = (cid:80)j ωi, j. then, the algo-rithm iteratively performs the following two up-.
one possibly counter-intuitive behaviour of bt isthat adding or removing a baseline can impact thescores and ordering of other systems.
for example,consider two systems a and b with the followingscores: ma = [1, 2, 3] and mb = [2, 3, 1].
then,bt identiﬁes system b has better with a relativestrengths of 23 .
now suppose another system c isadded with scores mc = [3, 2, 1], running bt onthese 3 systems together gives the result that allsystems have an equal strength, so now b is notseen as better than a..2314pairedt-test(mean)mood’stest(median)signtest(bt)wilcoxon’stest(bt)wilcoxon’stest(bt)signtest(bt)mood’stest(median)pairedt-test(mean)0.420.500.740.940.980.210.890.980.890.550.350.900.960.510.760.41disagree..(cid:54)= sota (cid:54)= top-3.
mean vs. medianmean vs. btmedian vs. btmean vs. elomedian vs. elomean vs. trueskillmedian vs. trueskillbt vs. elobt vs. trueskillelo vs. trueskill.
4%9%9%20%19%18%17%16%18%18%.
18%40%41%55%56%44%46%38%53%45%.
30%49%55%84%84%76%79%75%72%71%.
table 4: global disagreement (as in table 1) be-tween aggregation mechanisms repeated with elo andtrueskill..g.2 trueskill.
trueskill (herbrich et al., 2007) is bayesian variantof the elo rating system.
it also updates the ratingsof systems online, i.e., ratings change as new testinstances arrive.
now, the strength of a system siis represented by a normal distribution, n (λi, σ2i ).
in contrast to elo, each player has its own variance.
the update follows bayes rule, but is intractablein general, so message passing approximation areoften employed..h comparison of elo, trueskill, and bt.
we repeat the experiments of table 1 from the mainpaper by replacing bt with elo and trueskill withtheir default parameters.
the results are shown intable 4. with elo and trueskill, the same conclu-sions from the main paper hold, i.e., paired aggrega-tion mechanisms exhibit signiﬁcant disagreementwith mean and median.
some discrepancies be-tween bt, elo, and trueskill remain which calls forfurther investigations about which one to choose..we search for triple of systems which exhibitthis pattern in our data and couldn’t ﬁnd any aslong as we use more than 10 test instance..can we hope to ﬁx this weakness?
arrow’s im-possibility theorem says no (arrow, 1950).
oursetup matches very well the problem of aggregat-ing social preferences from voters.
in this context,arrow (1950) proved that no aggregation mecha-nism with more than 2 voters and 3 possibilities cansimulataneously meet the 3 following criterion: (i)monotonicity: if every voter prefers x over y , thenthe aggregation ranks x above y , (ii) (iaa) theaggregated preference between x and y should re-main unchanged if voter preferences between otherpairs change, and (iii) no dictators: the outcome isnot decided by a single voter.
in our framework,voters are test instance and preferences are givenby the evaluation metrics.
bt can fail on the secondcriteria, and mean and median can be dictatorial(as seen in the paper).
a way around this problemis to remain with pairwise comparisons of systemsn < 3 and use bt.
in that case, there is no possibil-ity for bt to fail on iia..g variants of bt: elo and trueskill.
bt has been extended in various ways.
we discusshere two important variants that we incorporate inour analysis tool: elo and trueskill..g.1 elo ratings.
the elo rating (elo, 1978) is variant of the btwith an online update rule, i.e., the rating of sys-tems (players) is updated as new test instances (newgames) arrive.
as bt, elo computes the probabilitythat systems si beats system s j. now, the t-th testinstance arrives and system si receives the score siand system s j receives the score s j. we update therating r based on this observed difference δi, j:.
r(t+1)k.= r(t) + k.δi, j −.
(cid:18).
(cid:19).
,.
qiqi + q j.
(5).
where k is parameter that has to be chosen, r therating of some system, and q plays a role analo-gous to λk in bt.
k controls how much each newinstance can change the ratings.
it can be shownthat, implicitly, elo corresponds to a version of btwhere the strength of systems is represented by anormal distribution: λi + (cid:15)i, (cid:15)i ∼ n (0, σ2), witha variance σ2 shared by all players (elo, 1978).
inour implementation, we provide the user with theability to choose k and set it to 20 by default..2315