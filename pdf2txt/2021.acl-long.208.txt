the limitations of limited context for constituency parsing.
yuchen licarnegie mellon universityyuchenl4@andrew.cmu.edu.
andrej risteskicarnegie mellon universityaristesk@andrew.cmu.edu.
abstract.
incorporating syntax into neural approaches innlp has a multitude of practical and scientiﬁcbeneﬁts.
for instance, a language model thatis syntax-aware is likely to be able to producebetter samples; even a discriminative modellike bert with a syntax module could be usedfor core nlp tasks like unsupervised syntacticparsing.
rapid progress in recent years wasarguably spurred on by the empirical successof the parsing-reading-predict architecture of(shen et al., 2018a), later simpliﬁed by theorder neuron lstm of (shen et al., 2019).
most notably, this is the ﬁrst time neural ap-proaches were able to successfully perform un-supervised syntactic parsing (evaluated by var-ious metrics like f-1 score)..however, even heuristic (much less fully math-ematical) understanding of why and whenthese architectures work is lagging severelyin this work, we answer representa-behind.
tional questions raised by the architectures in(shen et al., 2018a, 2019), as well as sometransition-based syntax-aware language mod-els (dyer et al., 2016): what kind of syntac-tic structure can current neural approachesto syntax represent?
concretely, we groundthis question in the sandbox of probabilisticcontext-free-grammars (pcfgs), and identifya key aspect of the representational powerthe amount and direc-of these approaches:tionality of context that the predictor has ac-cess to when forced to make parsing deci-sion.
we show that with limited context (eitherbounded, or unidirectional), there are pcfgs,for which these approaches cannot representthe max-likelihood parse; conversely, if thecontext is unlimited, they can represent themax-likelihood parse of any pcfg..1.introduction.
neural approaches have been steadily making theirway to nlp in recent years.
by and large however,.
the neural techniques that have been scaled-up themost and receive widespread usage do not explic-itly try to encode discrete structure that is naturalto language, e.g.
syntax.
the reason for this isperhaps not surprising: neural models have largelyachieved substantial improvements in unsupervisedsettings, bert (devlin et al., 2019) being the de-facto method for unsupervised pre-training in mostnlp settings.
on the other hand unsupervised syn-tactic tasks, e.g.
unsupervised syntactic parsing,have long been known to be very difﬁcult tasks(htut et al., 2018).
however, since incorporatingsyntax has been shown to improve language model-ing (kim et al., 2019b) as well as natural languageinference (chen et al., 2017; pang et al., 2019; heet al., 2020), syntactic parsing remains importanteven in the current era when large pre-trained mod-els, like bert (devlin et al., 2019), are available.
arguably, the breakthrough works in unsuper-vised constituency parsing in a neural manner were(shen et al., 2018a, 2019), achieving f1 scores 42.8and 49.4 on the wsj penn treebank dataset (htutet al., 2018; shen et al., 2019).
both of these ar-chitectures, however (especially shen et al., 2018a)are quite intricate, and it’s difﬁcult to evaluate whattheir representational power is (i.e.
what kinds ofstructure can they recover).
moreover, as subse-quent more thorough evaluations show (kim et al.,2019b,a), these methods still have a rather largeperformance gap with the oracle binary tree (whichis the best binary parse tree according to f1-score)— raising the question of what is missing in thesemethods..we theoretically answer both questions raisedin the prior paragraph.
we quantify the represen-tational power of two major frameworks in neuralapproaches to syntax: learning a syntactic distance(shen et al., 2018a,b, 2019) and learning to parsethrough sequential transitions (dyer et al., 2016;chelba, 1997).
to formalize our results, we con-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2675–2687august1–6,2021.©2021associationforcomputationallinguistics2675sider the well-established sandbox of probabilisticcontext-free grammars (pcfgs).
namely, we ask:when is a neural model based on a syntacticdistance or transitions able to represent the max-likelihood parse of a sentence generated from apcfg?.
we focus on a crucial “hyperparameter” com-mon to practical implementations of both familiesof methods that turns out to govern the representa-tional power: the amount and type of context themodel is allowed to use when making its predic-tions.
brieﬂy, for every position t in the sentence,syntactic distance models learn a distance dt to theprevious token — the tree is then inferred from thisdistance; transition-based models iteratively con-struct the parse tree by deciding, at each position t,what operations to perform on a partial parse up totoken t. a salient feature of both is the context, thatis, which tokens is dt a function of (correspond-ingly, which tokens can the choice of operations attoken t depend on)?.
we show that when the context is either bounded(that is, dt only depends on a bounded windowaround the t-th token) or unidirectional (that is,dt only considers the tokens to the left of the t-th token), there are pcfgs for which no distancemetric (correspondingly, no algorithm to choosethe sequence of transitions) works.
on the otherhand, if the context is unbounded in both directionsthen both methods work: that is, for any parse, wecan design a distance metric (correspondingly, asequence of transitions) that recovers it..this is of considerable importance: in practi-cal implementations the context is either bounded(e.g.
in shen et al., 2018a, the distance metric isparametrized by a convolutional kernel with a con-stant width) or unidirectional (e.g.
in shen et al.,2019, the distance metric is computed by a lstm,which performs a left-to-right computation)..this formally conﬁrms a conjecture of htut et al.
(2018), who suggested that because these modelscommit to parsing decision in a left-to-right fash-ion and are trained as a part of a language model,it may be difﬁcult for them to capture sufﬁcientlycomplex syntactic dependencies.
our techniquesare fairly generic and seem amenable to analyzingother approaches to syntax.
finally, while the ex-istence of a particular pcfg that is problematicfor these methods doesn’t necessarily imply thatthe difﬁculties will carry over to real-life data, thepcfgs that are used in our proofs closely track lin-.
guistic intuitions about difﬁcult syntactic structuresto infer: the parse depends on words that comemuch later in the sentence..2 overview of results.
we consider several neural architectures that haveshown success in various syntactic tasks, mostnotably unsupervised constituency parsing andsyntax-aware language modeling.
the generalframework these architectures fall under is as fol-lows: to parse a sentence w = w1w2...wn with atrained neural model, the sentence w is input intothe model, which outputs ot at each step t, and ﬁ-nally all the outputs {ot}nt=1 are utilized to producethe parse..given unbounded time and space resources, by aseminal result of siegelmann and sontag (1992), anrnn implementation of this framework is turingcomplete.
in practice it is common to restrict theform of the output ot in some way.
in this paper,we consider the two most common approaches, inwhich ot is a real number representing a syntacticdistance (section 2.1) (shen et al., 2018a,b, 2019)or a sequence of parsing operations (section 2.2)(chelba, 1997; chelba and jelinek, 2000; dyeret al., 2016).
we proceed to describe our results foreach architecture in turn..2.1 syntactic distance.
syntactic distance-based neural parsers train a neu-ral network to learn a distance for each pair ofadjacent words, depending on the context surround-ing the pair of words under consideration.
thedistances are then used to induce a tree structure(shen et al., 2018a,b)..for a sentence w = w1w2...wn, the syntacticdistance between wt−1 and wt (2 ≤ t ≤ n) isdeﬁned as dt = d(wt−1, wt | ct), where ct is thecontext that dt takes into consideration 1. we willshow that restricting the surrounding context ei-ther in directionality, or in size, results in a poorrepresentational power, while full context confersessentially perfect representational power with re-spect to pcfgs..concretely, if the context is full, we show:.
theorem (informal, full context).
for sentence wgenerated by any pcfg, if the computation of dthas as context the full sentence and the positionindex under consideration, i.e.
ct = (w, t) and.
1note that this is not a conditional distribution—we use.
this notation for convenience..2676dt = d(wt−1, wt | ct), then dt can induce the maxi-mum likelihood parse of w ..on the ﬂipside, if the context is unidirectional(i.e.
unbounded left-context from the start ofthe sentence, and even possibly with a boundedlook-ahead), the representational power becomesseverely impoverished:.
theorem (informal, limitation of left-to-right pars-ing via syntactic distance).
there exists a pcfg gsuch that for any distance measure dt whose com-putation incorporates only bounded context in atleast one direction (left or right), e.g.
ct = (w0, w1, ..., wt+l(cid:48))dt = d(wt−1, wt | ct).
the probability that dt induces the max likelihood.
parse is arbitrarily low..in practice,.
for computational efﬁciency,parametrizations of syntactic distances fall into theabove assumptions of restricted context (shen et al.,2018a).
this puts the ability of these models tolearn a complex pcfg syntax into considerabledoubt.
for formal deﬁnitions, see section 4.2. forformal theorem statements and proofs, see section5..subsequently we consider on-lstm, an archi-tecture proposed by shen et al.
(2019) improvingtheir previous work (shen et al., 2018a), whichalso is based on learning a syntactic distance, butin (shen et al., 2019) the distances are reduced fromthe values of a carefully structured master forgetgate (see section 6).
while we show on-lstmcan in principle losslessly represent any parse tree(theorem 3), calculating the gate values in a leftto right fashion (as is done in practice) is subjectto the same limitations as the syntactic distanceapproach:.
theorem (informal, limitation of syntactic dis-tance estimation based on on-lstm).
there ex-ists a pcfg g for which the probability that thesyntactic distance converted from an on-lstminduces the max likelihood parse is arbitrarily low..for a formal statement, see section 6 and in.
particular theorem 4..2.2 transition-based parsing.
in principle, the output ot at each position t ofa left-to-right neural models for syntactic parsingneed not be restricted to a real-numbered distanceor a carefully structured vector.
it can also be a.combinatorial structure — e.g.
a sequence of tran-sitions (chelba, 1997; chelba and jelinek, 2000;dyer et al., 2016).
we adopt a simpliﬁcation of theneural parameterization in (dyer et al., 2016) (seedeﬁnition 4.7)..with full context, dyer et al.
(2016) describesan algorithm to ﬁnd a sequence of transitions torepresent any parse tree, via a “depth-ﬁrst, left-to-right traversal” of the tree.
on the other hand,without full context, we prove that transition-basedparsing suffers from the same limitations:.
theorem (informal, limitation of transition-basedparsing without full context).
there exists a pcfgg, such that for any learned transition-basedparser with bounded context in at least one direc-tion (left or right), the probability that it returnsthe max likelihood parse is arbitrarily low..for a formal statement, see section 7, and in.
particular theorem 5.remark.
there is no immediate connection be-tween the syntactic distance-based approaches (in-cluding on-lstm) and the transition-based pars-ing framework, so the limitations of transition-based parsing does not directly imply the statednegative results for syntactic distance or on-lstm, and vice versa..2.3 the counterexample family.
most of our theorems proving limitations onbounded and unidirectional context are based on apcfg family (deﬁnition 2.1) which draws inspi-rations from natural language already suggested in(htut et al., 2018): later words in a sentence canforce different syntactic structures earlier in thesentence.
for example, consider the two sentences:“i drink coffee with milk.” and “i drink coffee withfriends.” their only difference occurs at their verylast words, but their parses differ at some earlierwords in each sentence, too, as shown in figure 1.to formalize this intuition, we deﬁne the follow-.
ing pcfg..deﬁnition 2.1 (right-inﬂuenced pcfg).
let m ≥2, l(cid:48) ≥ 1 be positive integers.
the grammar gm,l(cid:48)has starting symbol s, other non-terminals.
ak, bk, al.
k, ar.
k, b(cid:48).
k for all k ∈ {1, 2, ..., m},.
and terminals.
ai for all i ∈ {1, 2, ..., m + 1 + l(cid:48)},.
cj for all j ∈ {1, 2, ..., m}..2677figure 1: the parse trees of the two sentences: “i drink coffee with milk.” and “i drink coffee with friends.”.
theironly difference occurs at their very last words, but their parses differ at some earlier words in each sentence.
figure 2: the structure of the parse tree of string lk = a1a2...am+1+l(cid:48)ck ∈ l(gm,l(cid:48)).
note that any lk1 and lk2are almost the same except for the last token: the preﬁx a1a2...am+1+l(cid:48) is shared among all strings in l(gm,l(cid:48)).
however, their parses differ with respect to where ak is split.
the last token ck is unique to lk and hence determinesthe correct parse according to gm,l(cid:48)..the rules of the grammar are.
kar.
k w. prob.
1.s → akbk, ∀k ∈ {1, 2, .
.
.
, m} w. prob.1/mak → alalarbk →∗ b(cid:48)b(cid:48).
k →∗ a1a2...ak w. prob.
1k →∗ ak+1ak+2...am+1 w. prob.
1.k →∗ am+2am+3...am+1+l(cid:48) w. prob.
1.kck w. prob.
1.in which →∗ means that the left expands into theright through a sequence of rules that conformto the requirements of the chomsky normal form(cnf, deﬁnition 4.4).
hence the grammar gm,l(cid:48)is in cnf..the language of this grammar is.
l(gm,l(cid:48))={lk=a1a2...am+1+l(cid:48)ck : 1 ≤ k ≤ m}..the parse of an arbitrary lk is shown in figure 2.each lk corresponds to a unique parse determinedby the choice of k. the structure of this pcfg is.
such that for the parsing algorithms we considerthat proceed in a “left-to-right” fashion on lk, be-fore processing the last token ck, it cannot infer thesyntactic structure of a1a2...am+1 any better thanrandomly guessing one of the m possibilities.
thisis the main intuition behind theorems 2 and 5.remark.
while our theorems focus on the limita-tion of “left-to-right” parsing, a symmetric argu-ment implies the same limitation of “right-to-left”parsing.
thus, our claim is that unidirectional con-text (in either direction) limits the expressive powerof parsing models..3 related works.
neural models for parsing were ﬁrst successfullyimplemented for supervised settings, e.g.
(vinyalset al., 2015; dyer et al., 2016; shen et al., 2018b).
unsupervised tasks remained seemingly out ofreach, until the proposal of the parsing-reading-predict network (prpn) by shen et al.
(2018a),whose performance was thoroughly veriﬁed by ex-tensive experiments in (htut et al., 2018).
the.
2678follow-up paper (shen et al., 2019) introducingthe on-lstm architecture simpliﬁed radically thearchitecture in (shen et al., 2018a), while still ulti-mately attempting to ﬁt a distance metric with thehelp of carefully designed master forget gates.
sub-sequent work by kim et al.
(2019a) departed fromthe usual way neural techniques are integrated innlp, with great success: they proposed a neuralparameterization for the em algorithm for learninga pcfg, but in a manner that leverages semanticinformation as well — achieving a large improve-ment on unsupervised parsing tasks.2.
in addition to constituency parsing, dependencyparsing is another common task for syntactic pars-ing, but for our analyses on the ability of variousapproaches to represent the max-likelihood parseof sentences generated from pcfgs, we focus onthe task of constituency parsing.
moreover, it’simportant to note that there is another line of workaiming to probe the ability of models trained with-out explicit syntactic consideration (e.g.
bert) tonevertheless discover some (rudimentary) syntacticelements (bisk and hockenmaier, 2015; linzenet al., 2016; choe and charniak, 2016; kuncoroet al., 2018; williams et al., 2018; goldberg, 2019;htut et al., 2019; hewitt and manning, 2019; reifet al., 2019).
however, to-date, we haven’t beenable to extract parse trees achieving scores thatare close to the oracle binarized trees on standardbenchmarks (kim et al., 2019b,a)..methodologically, our work is closely related toa long line of works aiming to characterize the rep-resentational power of neural models (e.g.
rnns,lstms) through the lens of formal languages andformal models of computation.
some of the worksof this ﬂavor are empirical in nature (e.g.
lstmshave been shown to possess stronger abilities torecognize some context-free language and evensome context-sensitive language, compared withsimple rnns (gers and schmidhuber, 2001; suz-gun et al., 2019) or grus (weiss et al., 2018; suz-gun et al., 2019)); some results are theoretical innature (e.g.
siegelmann and sontag (1992)’s proofthat with unbounded precision and unbounded timecomplexity, rnns are turing-complete; related re-sults investigate rnns with bounded precision andcomputation time (weiss et al., 2018), as well as.
2by virtue of not relying on bounded or unidirectionalcontext, the compound pcfg (kim et al., 2019a) eschews thetechniques in our paper.
speciﬁcally, by employing a bidirec-tional lstm inference network in the process of constructinga tree given a sentence, the parsing is no longer “left-to-right”..memory (merrill, 2019; hewitt et al., 2020).
ourwork contributes to this line of works, but focuseson the task of syntactic parsing instead..4 preliminaries.
in this section, we deﬁne some basic concepts andintroduce the architectures we will consider..4.1 probabilistic context-free grammar.
first recall several deﬁnitions around formal lan-guage, especially probabilistic context free gram-mar:deﬁnition 4.1 (probabilistic context-free grammar(pcfg)).
formally, a pcfg (chomsky, 1956) is a5-tuple g = (σ, n, s, r, π) in which σ is the setof terminals, n is the set of non-terminals, s ∈ nis the start symbol, r is the set of production rulesof the form r = (rl → rr), where rl ∈ n , rris of the form b1b2...bm, m ∈ z+, and ∀i ∈{1, 2, ..., m}, bi ∈ (σ ∪ n ).
finally, π : r (cid:55)→[0, 1] is the rule probability function, in which foranyr = (a → b1b2...bm) ∈ r,.
π(r) is the conditional probability.
p (rr = b1b2...bm | rl = a)..deﬁnition 4.2 (parse tree).
let tg denote the setof parse trees that g can derive.
each t ∈ tg isassociated with yield(t) ∈ σ∗, the sequence ofterminals composed of the leaves of t and pt (t) ∈[0, 1], the probability of the parse tree, deﬁned bythe product of the probabilities of the rules in thederivation of t.deﬁnition 4.3 (language and sentence).
the lan-guage of g is.
l(g) = {s ∈ σ∗ : ∃t ∈ tg, yield(t) = s}..t∈tg(s) pt (t)..each s ∈ l(g) is called a sentence in l(g), andis associated with the set of parses tg(s) = {t ∈tg | yield(t) = s}, the set of max likelihoodparses, arg maxt∈tg(s) pt (t), and its probabilityps(s) = (cid:80)deﬁnition 4.4 (chomsky normal form (cnf)).
apcfg g = (σ, n, s, r, π) is in cnf (chomsky,1959) if we require, in addition to deﬁnition 4.1,that each rule r ∈ r is in the form a → b1b2where b1, b2 ∈ n \ {s}; a → a where a ∈σ, a (cid:54)= (cid:15); or s → (cid:15) which is only allowed if theempty string (cid:15) ∈ l(g)..every pcfg g can be converted into a pcfg g(cid:48)in cnf such that l(g) = l(g(cid:48)) (hopcroft et al.,2006)..26794.2 syntactic distance.
4.4 transition-based parsing.
the parsing-reading-predict networks (prpn)(shen et al., 2018a) is one of the leading approachesto unsupervised constituency parsing.
the parsingnetwork (which computes the parse tree, hence theonly part we focus on in our paper) is a convo-lutional network that computes the syntactic dis-tances dt = d(wt−1, wt) (deﬁned in section 2.1)based on the past l words.
a deterministic greedytree induction algorithm is then used to producea parse tree as follows.
first, we split the sen-tence w1...wn into two constituents, w1...wt−1 andwt...wn, where t ∈ argmax{dt}nt=2 and form theleft and right subtrees of t. we recursively repeatthis procedure for the newly created constituents.
an algorithmic form of this procedure is includedas algorithm 1 in appendix a..note that, due to the deterministic nature of thetree-induction process, the ability of prpn to learna pcfg is completely contingent upon learning agood syntactic distance..4.3 the ordered neuron architecture.
building upon the idea of representing the syntacticinformation with a real-valued distance measureat each position, a simple extension is to associateeach position with a learned vector, and then use thevector for syntactic parsing.
the ordered-neuronlstm (on-lstm, shen et al., 2019) proposesthat the nodes that are closer to the root in theparse tree generate a longer span of terminals, andtherefore should be less frequently “forgotten” thannodes that are farther away from the root.
thedifference in the frequency of forgetting is capturedby a carefully designed master forget gate vector ˜f ,as shown in figure 3 (in appendix b).
formally:.
deﬁnition 4.5 (master forget gates, shen et al.,2019).
given the input sentence w = w1w2...wnand a trained on-lstm, running the on-lstmon w gives the master forget gates, which are asequence of d-dimensional vectors { ˜ft}nt=1, inwhich at each position t, ˜ft = ˜ft(w1, ..., wt) ∈[0, 1]d. moreover, let ˜ft,j represent the j-th dimen-sion of ˜ft.
the on-lstm architectures requiresthat ˜ft,1 = 0, ˜ft,d = 1, and.
∀i < j,.
˜ft,i ≤ ˜ft,j..in addition to outputting a single real numbereddistance or a vector at each position t, a left-to-rightmodel can also parse a sentence by outputting asequence of “transitions” at each position t, an ideaproposed in some traditional parsing approaches(sagae and lavie, 2005; chelba, 1997; chelba andjelinek, 2000), and also some more recent neuralparameterization (dyer et al., 2016)..we introduce several items of notation:.
• zt.
i : the i-th transition performed when readingin wt, the t-th token of the sentence.
w = w1w2...wn..• nt: the number of transitions performed be-tween reading in the token wt and reading inthe next token wt+1..• zt: the sequence of transitions after reading.
in the preﬁx w1w2...wt of the sentence..zt = {(zj.
1, zj.
2, ..., zjnj.)
| j = 1..t}..• z: the parse of the sentence w .
z = zn..we base our analysis on the approach introducedin the parsing version of (dyer et al., 2016), thoughthat work additionally proposes a generator ver-sion.
3.deﬁnition 4.6 (transition-based parser).
atransition-based parser uses a stack (initialized toempty) and an input buffer (initialized with thesentence w1...wt).
at each position t, based on acontext ct, the parser outputs a sequence of parsingi=1, where each zttransitions {zti can be one ofthe following transitions (deﬁnition 4.7).
theparsing stops when the stack contains one singleconstituent, and the buffer is empty..i }nt.
deﬁnition 4.7 (parser transitions, dyer et al.,2016).
a parsing transition can be one of the fol-lowing three types:.
• nt(x) pushes a non-terminal x onto the.
stack..• shift: removes the ﬁrst terminal from the.
input buffer and pushes onto the stack..when parsing a sentence, the real-valued masterforget gate vector ˜ft at each position t is reduced toa single real number representing the syntactic dis-tance dt at position t (see (1)) (shen et al., 2018a).
then, use the syntactic distances to obtain a parse..3dyer et al.
(2016) additionally proposes some generatortransitions.
for simplicity, we analyze the simplest form: weonly allow the model to return one parse, composed of theparser transitions, for a given input sentence.
note that thissimpliﬁed variant still confers full representational power inthe “full context” setting (see section 7)..2680• reduce: pops from the stack until anopen non-terminal is encountered, then popsthis non-terminal and assembles everythingpopped to form a new constituent, labels thisnew constituent using this non-terminal, andﬁnally pushes this new constituent onto thestack..will describe an assignment of {dt : 2 ≤ t ≤ n}such that their order matches the level at which thebranches split in t .
speciﬁcally, ∀t ∈ [2, n], let atdenote the lowest common ancestor of wt−1 and wtin t .
let d(cid:48)t denote the shortest distance betweenat and the root of t .
finally, let dt = n − d(cid:48)t. as aresult, {dt : 2 ≤ t ≤ n} induces t ..in appendix section c, we provide an exampleof parsing the sentence “i drink coffee with milk”using the set of transitions given by deﬁnition 4.7.the different context speciﬁcations and the corre-sponding representational powers of the transition-based parser are discussed in section 7..5 representational power of neural.
syntactic distance methods.
in this section we formalize the results on syntacticdistance-based methods.
since the tree inductionalgorithm always generates a binary tree, we con-sider only pcfgs in chomsky normal form (cnf)(deﬁnition 4.4) so that the max likelihood parse ofa sentence is also a binary tree structure..to formalize the notion of “representing” a.pcfg, we introduce the following deﬁnition:.
deﬁnition 5.1 (representing pcfg with syntacticdistance).
let g be any pcfg in chomsky normalform.
a syntactic distance function d is said to beable to p-represent g if for a set of sentences inl(g) whose total probability is at least p, d cancorrectly induce the tree structure of the max likeli-hood parse of these sentences without ambiguity..remark.
ambiguities could occur when, for ex-ample, there exists t such that dt = dt+1.
in thiscase, the tree induction algorithm would have tobreak ties when determining the local structure forwt−1wtwt+1.
we preclude this possibility in deﬁ-nition 5.1..in the least restrictive setting, the whole sentencew , as well as the position index t can be taken intoconsideration when determining each dt.
we provethat under this setting, there is a syntactic distancemeasure that can represent any pcfg..theorem 1 (full context).
let ct = (w, t).
for each pcfg g in chomsky normal form,there exists a syntactic distance measure dt =d(wt−1, wt | ct) that can 1-represent g..remark.
since any pcfg can be converted tochomsky normal form (hopcroft et al., 2006), the-orem 1 implies that given the whole sentence andthe position index as the context, the syntactic dis-tance has sufﬁcient representational power to cap-ture any pcfg.
it does not state, however, that thewhole sentence and the position are the minimalcontextual information needed for representabil-ity nor does it address training (i.e.
optimization)issues..on the ﬂipside, we show that restricting the con-text even mildly can considerably decrease the rep-resentational power.
namely, we show that if con-text is bounded even in a single direction (to theleft or to the right), there are pcfgs on which anysyntactic distance will perform poorly 4.
(note inthe implementation (shen et al., 2018a) the contextonly considers a bounded window to the left.).
theorem 2 (limitation of left-to-right parsing viasyntactic distance).
let w0 = (cid:104)s(cid:105) be the sentencestart symbol.
let the context.
ct = (w0, w1, ..., wt+l(cid:48)).
∀(cid:15) > 0, there exists a pcfg g in chomsky nor-mal form, such that any syntactic distance measuredt = d(wt−1, wt | ct) cannot (cid:15)-represent g..proof.
let m > 1/(cid:15) be a positive integer.
considerthe pcfg gm,l(cid:48) in deﬁnition 2.1..for any k ∈ [m], consider the string lk ∈l(gm,l(cid:48)).
note that in the parse tree of lk, therule s → akbk is applied.
hence, ak and ak+1are the unique pair of adjacent non-terminals ina1a2...am+1 whose lowest common ancestor is theclosest to the root in the parse tree of lk.
then, inorder for the syntactic distance metric d to inducethe correct parse tree for lk, dk must be the uniquemaximum in {dt : 2 ≤ t ≤ m + 1}..however, d is restricted to be in the form.
dt = d(wt−1, wt | w0, w1, ..., wt+l(cid:48))..proof.
for any sentence s = s1s2...sn ∈ l(g),let t be its max likelihood parse tree.
since g isin chomsky normal form, t is a binary tree.
we.
4in theorem 2 we prove the more typical case, i.e.
un-bounded left context and bounded right context.
the othercase, i.e.
bounded left context and unbounded right context,can be proved symmetrically..2681note that ∀1 ≤ k1 < k2 ≤ m, the ﬁrst m + 1 + l(cid:48)tokens of lk1 and lk2 are the same, which impliesthat the inferred syntactic distances.
{dt : 2 ≤ t ≤ m + 1}.
are the same for lk1 and lk2 at each position t. thus,it is impossible for d to induce the correct parsetree for both lk1 and lk2.
hence, d is correct onat most one lk ∈ l(gm,l(cid:48)), which corresponds toprobability at most 1/m < (cid:15).
therefore, d cannot(cid:15)-represent gm,l(cid:48)..remark.
in the counterexample, there are only mpossible parse structures for the preﬁx a1a2...am+1.
hence, the proved fact that the probability of be-ing correct is at most 1/m means that under therestrictions of unbounded look-back and boundedlook-ahead, the distance cannot do better than ran-dom guessing for this grammar.
remark.
the above theorem 2 formalizes the in-tuition discussed in (htut et al., 2018) outliningan intrinsic limitation of only considering boundedcontext in one direction.
indeed, for the pcfg con-structed in the proof, the failure is a function of thecontext, not because of the fact that we are using adistance-based parser..note that as a corollary of the above theorem, ifthere is no context (ct = null) or the context isboth bounded and unidirectional, i.e..ct = wt−lwt−l+1...wt−1wt,.
then there is a pcfg that cannot be (cid:15)-representedby any such d..6 representational power of the ordered.
neuron architecture.
in this section, we formalize the results character-izing the representational power of the on-lstmarchitecture.
the master forget gates of the on-lstm, { ˜ft}nt=2 in which each ˜ft ∈ [0, 1]d, encodethe hierarchical structure of a parse tree, and shenet al.
(2019) proposes to carry out unsupervisedconstituency parsing via a reduction from the gatevectors to syntactic distances by setting:.
ˆdft = d −.
˜ft,j for t = 2..n.(1).
d(cid:88).
j=1.
first we show that the gates in on-lstm inprinciple form a lossless representation of anyparse tree..theorem 3 (lossless representation of a parsetree).
for any sentence w = w1w2...wn withparse tree t in any pcfg in chomsky normal form,there exists a dimensionality d ∈ z+, a sequenceof vectors { ˜ft}nt=2 in which each ˜ft ∈ [0, 1]d, suchthat the estimated syntactic distances via (1) inducethe structure of t ..proof.
by theorem 1, there is a syntactic distancemeasure {dt}nt=2 that induces the structure of t(such that ∀t, dt (cid:54)= dt+1)..for each t = 2..n, set ˆdt = k if dt is the k-tht=2, breaking ties arbitrar-t=2 also.
smallest entry in {dt}nily.
then, each ˆdt ∈ [1, n − 1], and { ˆdt}ninduces the structure of t ..let d = n − 1. for each t = 2..n, let ˜ft =(0, ..., 0, 1, ..., 1) whose lower ˆdt dimensions are 0and higher d − ˆdt dimensions are 1. then,.
ˆdft = d −.
˜ft,j = d − (d − ˆdt) = ˆdt..d(cid:88).
j=1.
therefore, the calculated { ˆdf.
t }n.t=2 induces the.
structure of t ..although theorem 3 shows the ability of themaster forget gates to perfectly represent any parsetree, a left-to-right parsing can be proved to beunable to return the correct parse with high proba-bility.
in the actual implementation in (shen et al.,2019), the (real-valued) master forget gate vectors{ ˜ft}nt=1 are produced by feeding the input sentencew = w1w2...wn to a model trained with a lan-guage modeling objective.
in other words, ˜ft,j iscalculated as a function of w1, ..., wt, rather thanthe entire sentence..as such, this left-to-right parser is subject to.
similar limitations as in theorem 2:.
theorem 4 (limitation of syntactic distance esti-mation based on on-lstm).
for any (cid:15) > 0, thereexists a pcfg g in chomsky normal form, suchthat the syntactic distance measure calculated with(1), ˆdf.
t , cannot (cid:15)-represent g..proof.
since by deﬁnition 4.5, ˜ft,j is a functionof w1, ..., wt, the estimated syntactic distance ˆdftis also a function of w1, ..., wt.
by theorem 2,even with unbounded look-back context w1, ..., wt,there exists a pcfg for which the probability thatˆdft induces the correct parse is arbitrarily low..26827 representational power oftransition-based parsing.
8 conclusion.
in this section, we analyze a transition-based pars-ing framework inspired by (dyer et al., 2016;chelba and jelinek, 2000; chelba, 1997)..again, we proceed to say ﬁrst that “full context”confers full representational power.
namely, usingthe terminology of deﬁnition 4.6, we let the contextct at each position t be the whole sentence w andthe position index t. note that any parse tree can begenerated by a sequence of transitions deﬁned indeﬁnition 4.7. indeed, dyer et al.
(2016) describesan algorithm to ﬁnd such a sequence of transitionsvia a “depth-ﬁrst, left-to-right traversal” of the tree.
proceeding to limited context, in the setting oftypical left-to-right parsing, the context ct consistsof all current and past tokens {wj}tj=1 and all pre-vious parses {(zj1, ..., zjj=1.
we’ll again provenjeven stronger negative results, where we allow anoptional look-ahead to l(cid:48) input tokens to the right.
theorem 5 (limitation of transition-based parsingwithout full context).
for any (cid:15) > 0, there existsa pcfg g in chomsky normal form, such thatfor any learned transition-based parser (deﬁnition4.6) based on contextct = ({wj}t+l(cid:48).
j=1 , {(zj.
1, ..., zjnj.
j=1),.
)}t.)}t.the sum of the probabilities of the sentences inl(g) for which the parser returns the maximumlikelihood parse is less than (cid:15)..proof.
let m > 1/(cid:15) be a positive integer.
considerthe pcfg gm,l(cid:48) in deﬁnition 2.1..note that ∀k, s → akbk is applied to yieldstring lk.
then in the parse tree of lk, ak andak+1 are the unique pair of adjacent terminals ina1a2...am+1 whose lowest common ancestor is theclosest to the root.
thus, different lk requires a dif-ferent sequence of transitions within the ﬁrst m + 1input tokens, i.e.
{zt.
i }i≥1, 1≤t≤m+1.
for each w ∈ l(gm,l(cid:48)), before the last to-ken wm+2+l(cid:48) is processed, based on the commonpreﬁx w1w2...wm+1+l(cid:48) = a1a2...am+1+l(cid:48), it isequally likely that w = lk, ∀k, w. prob.
1/m each.
moreover, when processing wm+1, the boundedlook-ahead window of size l(cid:48) does not allow accessto the ﬁnal input token am+2+l(cid:48) = ck..thus, ∀1 ≤ k1 < k2 ≤ m, it is impossiblefor the parser to return the correct parse tree forboth lk1 and lk2 without ambiguity.
hence, theparse is correct on at most one lk ∈ l(g), whichcorresponds to probability at most 1/m < (cid:15)..in this work, we considered the representationalpower of two frameworks for constituency pars-ing prominent in the literature, based on learninga syntactic distance and learning a sequence of it-erative transitions to build the parse tree — in thesandbox of pcfgs.
in particular, we show thatif the context for calculating distance/deciding ontransitions is limited at least to one side (which istypically the case in practice for existing architec-tures), there are pcfgs for which no good distancemetric/sequence of transitions can be chosen toconstruct the maximum likelihood parse..this limitation was already suspected in (htutet al., 2018) as a potential failure mode of leadingneural approaches like (shen et al., 2018a, 2019)and we show formally that this is the case.
thepcfgs with this property track the intuition thatbounded context methods will have issues whenthe parse at a certain position depends heavily onlatter parts of the sentence..the conclusions thus suggest re-focusing our at-tention on methods like (kim et al., 2019a) whichhave enjoyed greater success on tasks like unsu-pervised constituency parsing, and do not fall inthe paradigm analyzed in our paper.
a question ofdeﬁnite further interest is how to augment modelsthat have been successfully scaled up (e.g.
bert)in a principled manner with syntactic information,such that they can capture syntactic structure (likepcfgs).
the other question of immediate impor-tance is to understand the interaction between thesyntactic and semantic modules in neural architec-tures — information is shared between such mod-ules in various successful architectures, e.g.
(dyeret al., 2016; shen et al., 2018a, 2019; kim et al.,2019a), and the relative pros and cons of doing thisare not well understood.
finally, our paper purelyfocuses on representational power, and does notconsider algorithmic and statistical aspects of train-ing.
as any model architecture is associated withits distinct optimization and generalization consid-erations, and natural language data necessitates themodeling of the interaction between syntax andsemantics, those aspects of considerations are wellbeyond the scope of our analysis in this paper usingthe controlled sandbox of pcfgs, and are interest-ing directions for future work..2683references.
yonatan bisk and julia hockenmaier.
2015. probingthe linguistic strengths and limitations of unsuper-in proceedings of thevised grammar induction.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 1: long papers), pages 1395–1404, beijing,china.
association for computational linguistics..ciprian chelba.
1997. a structured language model.
in 35th annual meeting of the association for com-putational linguistics and 8th conference of theeuropean chapter of the association for computa-tional linguistics, pages 498–500, madrid, spain.
association for computational linguistics..ciprian chelba and frederick jelinek.
2000. struc-tured language modeling.
computer speech & lan-guage, 14(4):283 – 332..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstmin proceedings offor natural language inference.
the 55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1657–1668, vancouver, canada.
associationfor computational linguistics..do kook choe and eugene charniak.
2016. parsingas language modeling.
in proceedings of the 2016conference on empirical methods in natural lan-guage processing, pages 2331–2336, austin, texas.
association for computational linguistics..n. chomsky.
1956. three models for the description oflanguage.
ire transactions on information theory,2(3):113–124..noam chomsky.
1959. on certain formal propertiesof grammars.
information and control, 2(2):137 –167..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a. smith.
2016. recurrent neural networkgrammars.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 199–209, san diego, california.
association for computational linguistics..yoav goldberg.
2019. assessing bert’s syntactic abili-.
ties..qi he, han wang, and yue zhang.
2020. enhanc-ing generalization in natural language inference bysyntax.
in findings of the association for computa-tional linguistics: emnlp 2020, pages 4973–4978,online.
association for computational linguistics..john hewitt, michael hahn, surya ganguli, percyliang, and christopher d. manning.
2020. rnnscan generate bounded hierarchical languages withoptimal memory.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1978–2010, online.
as-sociation for computational linguistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..john e. hopcroft, rajeev motwani, and jeffrey d. ull-man.
2006. introduction to automata theory, lan-guages, and computation (3rd edition).
addison-wesley longman publishing co., inc., usa..phu mon htut, kyunghyun cho, and samuel bowman.
2018. grammar induction with neural languagein proceedingsmodels: an unusual replication.
of the 2018 emnlp workshop blackboxnlp: an-alyzing and interpreting neural networks for nlp,pages 371–373, brussels, belgium.
association forcomputational linguistics..phu mon htut, jason phang, shikha bordia, andsamuel r. bowman.
2019. do attention headsarxiv,in bertabs/1911.12246..track syntactic dependencies?.
yoon kim, chris dyer, and alexander rush.
2019a.
compound probabilistic context-free grammars forgrammar induction.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 2369–2385, florence, italy.
asso-ciation for computational linguistics..yoon kim, alexander rush, lei yu, adhiguna kun-coro, chris dyer, and g´abor melis.
2019b.
unsu-pervised recurrent neural network grammars.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 1105–1117,minneapolis, minnesota.
association for computa-tional linguistics..f. gers and j. schmidhuber.
2001..lstm recur-rent networks learn simple context-free and context-ieee transactions on neuralsensitive languages.
networks, 12 6:1333–40..adhiguna kuncoro, chris dyer, john hale, dani yo-gatama, stephen clark, and phil blunsom.
2018.lstms can learn syntax-sensitive dependencieswell, but modeling structure makes them better.
in.
2684building bridges, pages 44–54, florence.
associa-tion for computational linguistics..oriol vinyals, ł ukasz kaiser, terry koo, slav petrov,ilya sutskever, and geoffrey hinton.
2015. gram-mar as a foreign language.
in advances in neuralinformation processing systems, volume 28, pages2773–2781.
curran associates, inc..gail weiss, yoav goldberg, and eran yahav.
2018.on the practical computational power of ﬁnite pre-cision rnns for language recognition.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 740–745, melbourne, australia.
asso-ciation for computational linguistics..adina williams, andrew drozdov, and samuel r.bowman.
2018. do latent tree learning models iden-transac-tify meaningful structure in sentences?
tions of the association for computational linguis-tics, 6:253–267..proceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1426–1436, melbourne, aus-tralia.
association for computational linguistics..tal linzen, emmanuel dupoux, and yoav goldberg.
2016. assessing the ability of lstms to learnsyntax-sensitive dependencies.
transactions of theassociation for computational linguistics, 4:521–535..william merrill.
2019. sequential neural networksin proceedings of the workshop onas automata.
deep learning and formal languages: buildingbridges, pages 1–13, florence.
association for com-putational linguistics..deric pang, lucy h. lin, and noah a. smith.
2019.improving natural language inference with a pre-trained parser..emily reif, ann yuan, martin wattenberg, fernanda bviegas, andy coenen, adam pearce, and been kim.
2019. visualizing and measuring the geometry ofbert.
in advances in neural information processingsystems, volume 32, pages 8594–8603.
curran as-sociates, inc..kenji sagae and alon lavie.
2005. a classiﬁer-basedparser with linear run-time complexity.
in proceed-ings of the ninth international workshop on pars-ing technology, pages 125–132, vancouver, britishcolumbia.
association for computational linguis-tics..yikang shen, zhouhan lin, chin wei huang, andaaron courville.
2018a.
neural language modelingin interna-by jointly learning syntax and lexicon.
tional conference on learning representations..yikang shen, zhouhan lin, athul paul jacob, alessan-dro sordoni, aaron courville, and yoshua bengio.
2018b.
straight to the tree: constituency parsingwith neural syntactic distance.
in proceedings of the56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1171–1180, melbourne, australia.
association forcomputational linguistics..yikang shen, shawn tan, alessandro sordoni, andaaron courville.
2019. ordered neurons: integrat-ing tree structures into recurrent neural networks.
ininternational conference on learning representa-tions..hava t. siegelmann and eduardo d. sontag.
1992. onthe computational power of neural nets.
in proceed-ings of the fifth annual workshop on computationallearning theory, colt ’92, page 440–449, newyork, ny, usa.
association for computing machin-ery..mirac suzgun, yonatan belinkov, stuart shieber, andsebastian gehrmann.
2019. lstm networks canin proceedings of theperform dynamic counting.
workshop on deep learning and formal languages:.
2685a tree induction algorithm based on syntactic distance.
the following algorithm is proposed in (shen et al., 2018a) to create a parse tree based on a given syntacticdistance..algorithm 1: tree induction based on syntactic distancedata: sentence w = w1w2...wn, syntactic distances dt = d(wt−1, wt | ct), 2 ≤ t ≤ nresult: a parse tree for winitialize the parse tree with a single node n0 = w1w2...wn;while ∃ leaf node n = wiwi+1...wj where i < j do.
find k ∈ arg maxi+1≤k≤j dk ;create the left child nl and the right child nr of n ;nl ← wiwi+1...wk−1 ;nr ← wkwk+1...wj ;.
endreturn the parse tree rooted at n0..b on-lstm intuition.
see figure 3 below, which is excerpted from (shen et al., 2019) with minor adaptation to the notation..figure 3: relationship between the parse tree, the block view, and the on-lstm.
excerpted from (shen et al.,2019) with minor adaptation to the notation..2686c examples of parsing transitions.
table 1 below shows an example of parsing the sentence “i drink coffee with milk” using the set oftransitions given by deﬁnition 4.7, which employs the parsing framework of (dyer et al., 2016).
theparse tree of the sentence is given by.
s.v.drink.
np.
n.i.vp.
np.
np.
n.pp.
p.n.coffee.
with.
milk.
stack.
(s(s | (np(s | (np | (n(s | (np | (n | i(s | (np (n i))(s | (np (n i)) | (vp(s | (np (n i)) | (vp | (v(s | (np (n i)) | (vp | (v | drink(s | (np (n i)) | (vp | (v drink)(s | (np (n i)) | (vp | (v | drink) |.
(s | (np (n i)) | (vp | (v drink) |.
(np.
(np | (np.
(s | (np (n i)) | (vp | (v drink) |.
(np | (np | (n.(s | (np (n i)) | (vp | (v drink) |.
(np | (np | (n | coffee.
(s | (np (n i)) | (vp | (v drink) |.
(np | (np (n coffee)).
(s | (np (n i)) | (vp | (v drink) |(np | (np (n coffee)) | (pp(s | (np (n i)) | (vp | (v drink) |(np | (np (n coffee)) | (pp | (p(s | (np (n i)) | (vp | (v drink) |.
bufferactioni drink coffee with milk nt(s)i drink coffee with milk nt(np)i drink coffee with milk nt(n)i drink coffee with milk shiftdrink coffee with milkdrink coffee with milkdrink coffee with milkdrink coffee with milkcoffee with milkcoffee with milk.
reducent(vp)nt(v)shiftreducent(np).
coffee with milk.
nt(np).
coffee with milk.
coffee with milk.
with milk.
with milk.
with milk.
with milk.
milk.
milk.
milk.
nt(n).
shift.
reduce.
nt(pp).
nt(p).
shift.
reduce.
nt(n).
shift.
reduce.
(np | (np (n coffee)) | (pp | (p | with.
(s | (np (n i)) | (vp | (v drink) |.
(np | (np (n coffee)) | (pp | (p with).
(s | (np (n i)) | (vp | (v drink) |.
(np | (np (n coffee)) | (pp | (p with) | (n.(s | (np (n i)) | (vp | (v drink) |.
(np | (np (n coffee)) | (pp | (p with) | (n | milk.
(s (np (n i)) (vp (v drink).
(np (np (n coffee)) (pp (p with) (n milk))))).
table 1: transition-based parsing of the sentence “i drink coffee with milk”..2687