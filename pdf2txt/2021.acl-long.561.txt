can sequence-to-sequence models crack substitution ciphers?.
nada aldarrab and jonathan mayuniversity of southern californiainformation sciences institute{aldarrab,jonmay}@isi.edu.
abstract.
decipherment of historical ciphers is a chal-lenging problem.
the language of the tar-get plaintext might be unknown, and cipher-text can have a lot of noise.
state-of-the-artdecipherment methods use beam search anda neural language model to score candidateplaintext hypotheses for a given cipher, as-suming the plaintext language is known.
wepropose an end-to-end multilingual model forsolving simple substitution ciphers.
we testour model on synthetic and real historical ci-phers and show that our proposed method candecipher text without explicit language identi-ﬁcation while still being robust to noise..1.introduction.
libraries and archives have many enciphered doc-uments from the early modern period.
exampledocuments include encrypted letters, diplomaticcorrespondences, and books from secret societies(figure 1).
previous work has made historical ci-pher collections available for researchers (petters-son and megyesi, 2019; megyesi et al., 2020).
de-cipherment of classical ciphers is an essential stepto reveal the contents of those historical documents.
in this work, we focus on solving 1:1 substitu-tion ciphers.
current state-of-the-art methods usebeam search and a neural language model to scorecandidate plaintext hypotheses for a given cipher(kambhatla et al., 2018).
however, this approachassumes that the target plaintext language is known.
other work that both identiﬁes language and deci-phers relies on a brute-force guess-and-check strat-egy (knight et al., 2006; hauer and kondrak, 2016).
we ask: can we build an end-to-end model thatdeciphers directly without relying on a separatelanguage id step?.
the contributions of our work are:.
tution ciphers without explicit plaintext lan-guage identiﬁcation, which we demonstrateon ciphers of 14 different languages..• we conduct extensive testing of the proposedmethod in different realistic deciphermentconditions; different cipher lengths, no-spaceciphers, and ciphers with noise, and demon-strate that our model is robust to these condi-tions..• we apply our model on synthetic ciphers aswell as on the borg cipher, a real historicalcipher.1 we show that our multilingual modelcan crack the borg cipher using the ﬁrst 256characters of the cipher..2 the decipherment problem.
decipherment conditions vary from one cipher toanother.
for example, some cleartext might befound along with the encrypted text, which gives ahint to the plaintext language of the cipher.
in othercases, called known-plaintext attacks, some de-coded material is found, which can be exploited tocrack the rest of the encoded script.
however, in aciphertext-only attack, the focus of this paper, thecryptanalyst only has access to the ciphertext.
thismeans that the encipherment method, the plaintextlanguage, and the key are all unknown..in this paper, we focus on solving 1:1 substitu-tion ciphers.
we follow nuhn et al.
(2013) andkambhatla et al.
(2018) and use machine transla-tion notation to formulate our problem.
we denotethe ciphertext as f n1 = f1 .
.
.
fj .
.
.
fn and theplaintext as em.
1 = e1 .
.
.
ei .
.
.
em .2.in a 1:1 substitution cipher, plaintext is en-crypted into a ciphertext by replacing each plain-text character with a unique substitute according.
1https://cl.lingfil.uu.se/~bea/borg/2unless there is noise or space restoration, n = m ; see.
• we propose an end-to-end multilingual de-cipherment model that can solve 1:1 substi-.
sections 5.4 and 5.2..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7226–7235august1–6,2021.©2021associationforcomputationallinguistics72263.1 decipherment as a sequence-to-sequence.
translation problem.
to cast decipherment as a supervised translationtask, we need training data, i.e.
pairs of <f n1 , em1 >to train on.
we can create this data using randomlygenerated substitution keys (figure 2a).
we canthen train a character-based sequence-to-sequencedecipherment model and evaluate it on held-outtext which is also encrypted with (different) ran-domly generated substitution keys.
however, ifwe attempt this experiment using the transformermodel described in section 3.3, we get abysmalresults (see section 5.1 for scoring details)..increasing the amount of training data won’thelp; there are 26!
≈ 4 × 1026 possible keys for en-glish ciphers, and even if every key is represented,most of the training data will still be encoded withkeys that are not used to encode the test data.
infact, since each training example uses a differentkey, we cannot assume that a character type has anyparticular meaning.
the fundamental assumptionbehind embeddings is therefore broken.
in the nextsection, we describe one way to overcome thesechallenges..3.2 frequency analysis.
to address the aforementioned challenges, we em-ploy a commonly used technique in cryptanalysiscalled frequency analysis.
frequency analysis isattributed to the great polymath, al-kindi (801-873 c.e.)
(dooley, 2013).
this technique has beenused in previous decipherment work (hauer andkondrak, 2016; kambhatla et al., 2018).
it is basedon the fact that in a given text, letters and lettercombinations (n-grams) appear in varying frequen-cies, and that the character frequency distributionis roughly preserved in any sample drawn from agiven language.
so, in different pairs of <f n1 >,we expect the frequency distribution of charactersto be similar..1 , em.
to encode that information, we re-map each ci-phertext character to a value based on its frequencyrank (figure 2b).
this way, we convert any cipher-text to a “frequency-encoded” cipher.
intuitively,by frequency encoding, we are reducing the numberof possible substitution keys (assuming frequencyrank is roughly preserved across all ciphers froma given language).
this is only an approximation,but it helps restore the assumption that there is acoherent connection between a symbol and its typeembedding.
for example, if the letters “e” and “i”.
a) the copiale cipher.3.
b) the borg cipher..figure 1: historical cipher examples..to a substitution table called the key.
for example:the plaintext word “doors” would be enciphered to“kffml” using the substitution table:.
cipher plain.
kfml.dors.the decipherment goal is to recover the plaintext.
given the ciphertext..3 decipherment model.
inspired by character-level neural machine transla-tion (nmt), we view decipherment as a sequence-to-sequence translation task.
the motivation be-hind using a sequence-to-sequence model is:.
• the model can be trained on multilingual data(gao et al., 2020), making it potentially possi-ble to obtain end-to-end multilingual decipher-ment without relying on a separate languageid step..• due to transcription challenges of historical ci-phers (section 5.4), ciphertext could be noisy.
we would like the model to have the abilityto recover from that noise by inserting, delet-ing, or substituting characters while generat-ing plaintext.
sequence-to-sequence modelsseem to be good candidates for this task..3https://cl.lingfil.uu.se/~bea/.
copiale/.
7227(a) input: example ciphers encoded in random keys.
output: plaintext in target language..(b) input: example ciphers encoded according to frequency ranks in descending order.
output: plaintext in target language..figure 2: decipherment as a sequence-to-sequence translation problem.
(a) shows the original ciphers being fedto the model.
(b) shows the same ciphers after frequency encoding..are the most frequent characters in english, then inany 1:1 substitution cipher, they will be encoded as“0” or “1” instead of a randomly chosen character..3.3 the transformer.
we follow the character-based nmt approachin gao et al.
(2020) and use the transformermodel (vaswani et al., 2017) for our decipher-ment problem.
the transformer is an attention-based encoder-decoder model that has been widelyused in the nlp community to achieve state-of-the-art performance on many sequence modelingtasks.
we use the standard transformer architec-ture, which consists of six encoder layers and sixdecoder layers as described in gao et al.
(2020)..4 data.
for training, we create 1:1 substitution ciphers for14 languages using random keys.
for english, weuse english gigaword (parker et al., 2011).
wescrape historical text from project gutenberg for 13other languages, namely: catalan, danish, dutch,finnish, french, german, hungarian, italian, latin,norwegian, portuguese, spanish, and swedish.4table 1 summarizes our datasets.
following previ-ous literature (nuhn et al., 2013; aldarrab, 2017;kambhatla et al., 2018), we lowercase all charac-ters and remove all non-alphabetic and non-spacesymbols.
we make sure ciphers do not end in themiddle of a word.
we strip accents for languagesother than english..5 experimental evaluation.
to make our experiments comparable to previouswork (nuhn et al., 2013; kambhatla et al., 2018),.
4our dataset is available at https://github.com/.
nadaaldarrab/s2s-decipherment.
languagecatalandanishdutchfinnishfrenchgermanhungarianitalianlatinnorwegianportuguesespanishswedish.
words915,5952,077,92930,350,14522,784,17239,400,5873,273,602497,4024,587,0271,375,804706,43510,841,17120,165,7313,008,680.characters4,953,51611,205,300177,835,527168,886,663226,310,82720,927,0653,145,45127,786,7548,740,8083,673,89562,735,255114,663,95716,993,146.table 1: summary of data sets obtained from projectgutenberg..we create test ciphers from the english wikipediaarticle about history.5 we use this text to create ci-phers of length 16, 32, 64, 128, and 256 characters.
we generate 50 ciphers for each length.
we followthe same pre-processing steps to create trainingdata..we carry out four sets of experiments tostudy the effect of cipher length, space encipher-ment/removal, unknown plaintext language, andtranscription noise.
finally, we test our models ona real historical cipher, whose plaintext languagewas not known until recently..as an evaluation metric, we follow previous lit-erature (kambhatla et al., 2018) and use symbolerror rate (ser).
ser is the fraction of incorrectsymbols in the deciphered text.
for space restora-tion experiments (section 5.2), we use translationedit rate (ter) (snover et al., 2006), but on the.
5https://en.wikipedia.org/wiki/history.
7228character level.
we deﬁne character-level ter as:.
ter =.
# of edits# of reference characters.
(1).
where possible edits include the insertion, deletion,and substitution of single characters.
when theciphertext and plaintext have equal lengths, ser isequal to ter..we use fairseq to train our models (ott et al.,2019).
we mostly use the same hyperparametersas gao et al.
(2020) for character nmt, except thatwe set the maximum batch size to 10k tokens anduse half precision ﬂoating point computation forfaster training.
the model has about 44m param-eters.
training on a tesla v100 gpu takes about110 minutes per epoch.
we train for 20 epochs.
decoding takes about 400 character tokens/s.
weuse a beam size of 100. unless otherwise stated,we use 2m example ciphers to train, 3k ciphers fortuning, and 50 ciphers for testing in all experiments.
we report the average ser on the 50 test ciphersof each experiment..5.1 cipher length.
we ﬁrst experiment with ciphers of length 256 us-ing the approach described in section 3.1 (i.e.
we1 , emtrain a transformer model on pairs of <f n1 >without frequency encoding).
as expected, themodel is not able to crack the 50 test ciphers, re-sulting in an ser of 71.75%.
for the rest of theexperiments in this paper, we use the frequencyencoding method described in section 3.2..short ciphers are more challenging than longerones.
following previous literature, we report re-sults on different cipher lengths using our method.
table 2 shows decipherment results on ciphers oflength 16, 32, 64, 128, and 256. for the 256 lengthciphers, we use the aforementioned 2m train and3k development splits.
for ciphers shorter than256 characters, we increase the number of exam-ples such that the total number of characters re-mains nearly constant, at about 512m characters.
we experiment with training ﬁve different models(one for each length) and training a single modelon ciphers of mixed lengths.
in the latter case, wealso use approx.
512m characters, divided equallyamong different lengths.
the results in table 2show that our model achieves comparable resultsto the state-of-the-art model of kambhatla et al.
(2018) on longer ciphers, including perfect deci-pherment for ciphers of length 256. the table also.
shows that our method is more accurate than kamb-hatla et al.
(2018) for shorter, more difﬁcult ciphersof lengths 16 and 32. in addition, our method pro-vides the ability to train on multilingual data, whichwe use to attack ciphers with an unknown plaintextlanguage as described in section 5.3..5.2 no-space ciphers.
the inclusion of white space between words makesdecipherment easier because word boundaries cangive a strong clue to the cryptanalyst.
in manyhistorical ciphers, however, spaces are hidden.
forexample, in the copiale cipher (figure 1a), spacesare enciphered with special symbols just like otheralphabetic characters (knight et al., 2011).
in otherciphers, spaces might be omitted from the plain textbefore enciphering, as was done in the zodiac-408cipher (nuhn et al., 2013).
we test our method infour scenarios:.
1. ciphers with spaces (comparable to kamb-.
hatla et al.
(2018))..2. ciphers with enciphered spaces.
in this case,we treat space like other cipher characters dur-ing frequency encoding as described in sec-tion 3.2..3. no-space ciphers.
we omit spaces in both.
(source and target) sides..4. no-space ciphers with space recovery.
weomit spaces from source but keep them on thetarget side.
the goal here is to train the modelto restore spaces along with the decipherment..table 3 shows results for each of the four scenar-ios on ciphers of length 256. during decoding, weforce the model to generate tokens to match sourcelength.
results show that the method is robust toboth enciphered and omitted spaces.
in scenario 4,where the model is expected to generate spaces andthus the output length differs from the input length,we limit the output to exactly 256 characters, butwe allow the model freedom to insert spaces whereit sees ﬁt.
the model generates spaces in accuratepositions overall, leading to a ter of 1.88%..5.3 unknown plaintext language.
while combing through libraries and archives, re-searchers have found many ciphers that are not ac-companied with any cleartext or keys, leaving theplaintext language of the cipher unknown (megyesi.
7229beam nlm(kambhatla et al., 2018)beam (nlm + freqmatch)(kambhatla et al., 2018)transformer + freq + separate models (this work)transformer + freq + single model (this work).
cipher length6432.
128.
256.
16.
26.80.
5.80.
0.07.
0.01.
0.00.
31.0020.6219.38.
2.901.442.44.
0.070.411.22.
0.020.020.02.
0.000.000.00.table 2: ser (%) for solving 1:1 substitution ciphers of various lengths using our decipherment method..cipher typeciphers with spacesciphers with enciphered spacesno-space ciphersno-space ciphers + generate spaces.
ter(%)0.000.000.771.88.table 3: ter (%) for solving 1:1 substitution ciphersof length 256 with different spacing conditions..et al., 2020).
to solve that problem, we train asingle multilingual model on the 14 different lan-guages described in section 4. we train on a totalof 2.1m random ciphers of length 256 (dividedequally among all languages).
we report results asthe number of training languages increases whilekeeping the total number of 2.1m training exam-ples ﬁxed (table 4).
increasing the number of lan-guages negatively affects performance, as we ex-pected.
however, our experiments show that the14-language model is still able to decipher 700total test ciphers with an average ser of 0.68%.
since we are testing on 256-character ciphers, thistranslates to no more than two errors per cipher onaverage..very challenging and can introduce more types ofnoise, including the addition and deletion of somecharacters during character segmentation (yin et al.,2019).
we test our model on three types of randomnoise: insertion, deletion, and substitution.
weexperiment with different noise percentages forciphers of length 256 (table 5).
we report theresults of training (and testing) on ciphers with onlysubstitution noise and ciphers that have all threetypes of noise (divided equally).
we experimentallyﬁnd that training the models with 10% noise givesthe best overall accuracy, and we use those modelsto get the results in table 5. our method is able todecipher with up to 84% accuracy on ciphers with20% of random insertion, deletion, and substitutionnoise.
figure 3 shows an example output for acipher with 15% noise.
the model recovers mostof the errors, resulting in a ter of 5.86%.
one ofthe most challenging noise scenarios, for example,is the deletion of the last two characters from theword “its.” the model output the word “i,” whichis a valid english word.
of course, the more noisethere is, the harder it is for the model to recoverdue to error accumulation..5.4 transcription noise.
5.5 the borg cipher.
real historical ciphers can have a lot of noise.
thisnoise can come from the natural degradation of his-torical documents, human mistakes during a man-ual transcription process, or misspelled words bythe author, as in the zodiac-408 cipher.
noise canalso come from automatically transcribing histor-ical ciphers using optical character recognition(ocr) techniques (yin et al., 2019).
it is thus cru-cial to have a robust decipherment model that canstill crack ciphers despite the noise..hauer et al.
(2014) test their proposed methodon noisy ciphers created by randomly corruptinglog2(n ) of the ciphertext characters.
however,automatic transcription of historical documents is.
the borg cipher is a 400-page book digitized bythe biblioteca apostolica vaticana (figure 1b).6the ﬁrst page of the book is written in arabicscript, while the rest of the book is enciphered us-ing astrological symbols.
the borg cipher was ﬁrstautomatically cracked by aldarrab (2017) usingthe noisy-channel framework described in knightet al.
(2006).
the plaintext language of the bookis latin.
the deciphered book reveals pharmaco-logical knowledge and other information about thattime..we train a latin model on 1m ciphers and use.
6http://digi.vatlib.it/view/mss_borg..lat.898..7230# lang3714.ca--0.34.da--1.29.nl--0.79.en0.040.080.25.ﬁ--0.20.fr0.230.340.20.de-0.300.41.hu--0.64.it-1.231.52.la-1.381.43.no--0.41.pt-0.480.69.es0.390.400.72.sv--0.70.avg0.290.600.68.table 4: ser (%) for solving 1:1 substitution ciphers using a multilingual model trained on a different number oflanguages.
each language is evaluated on 50 test ciphers generated with random keys..source.
target.
output.
3 2 11 11 2 6 4 15 0 _ 16 0 1 6 _ d 20 12 9 i5 2 4 3 1 _ 2 3 _ d 15 0 3 6 _ 2 s22 _ 18 i16 0 99 _ 2 1 _ 6 13 0 _ 1 4 i7 19 3 4 5 4 10 2 3 i13 10 0 _ 7 5 _ 8 d 5 5 0 11 0 3 6 _ 10 2 14 1 0i21 1 _ 2 3 8 _ 0 5 5 0 10 6 1 i0 _ 13 4 1 6 7 s5 4 2 3 s6 _ 2 9 1 7 i18 _ 8 0 16 2 6 0 _ 6 130 _ 3 2 6 14 d 0 _ s3 5 _ d 4 1 6 7 d 17 _ s5 3 8 _ 4 d d _ 14 1 0 5 s0 9 3 0 1 1 _ 16 17_ 8 i5 4 1 10 14 1 1 4 s23 19 _ s2 13 0 _ 1 s11 14 s24 17 _ 7 5 _ 6 13 i21 0 _ 8 4 1 10 4 129 4 3 0 _ 2 1 _ 2 i7 3 _ 0 3 8 _ 4 3 _ s5 6 1 0 s14 s12 _ 2 3 8 _ 1 d _ 2 _ 18 d 17 _ 7 i20 5 i9_ 12 11 7 15 4 8 4 s2 19 _ 12 0 11 i12 0 d d 10 d 4 15 0n a r r a t i v e _ b e s t _ e x p l a i n s _ a n _ e v e n t _ a s _ w e l l _ a s _ t h e _ s i g n i f i ca n c e _ o f _ d i f f e r e n t _ c a u s e s _ a n d _ e f f e c t s _ h i s t o r i a n s _ a l s o _ d eb a t e _ t h e _ n a t u r e _ o f _ h i s t o r y _ a n d _ i t s _ u s e f u l n e s s _ b y _ d i s cu s s i n g _ t h e _ s t u d y _ o f _ t h e _ d i s c i p l i n e _ a s _ a n _ e n d _ i n _ i t s e l f _ an d _ a s _ a _ w a y _ o f _ p r o v i d i n g _ p e r s p e c t i v en a r r a t i v e _ b e s t _ e x p l a i n s _ a n _ e v e n t _ a s _ w e l l _ a s _ t h e _ s i g n i f i ca n c e _ o f _ d i f f e r e n t _ c a u s e s _ a n d _ e f f e c t i v e _ h i s t o r i a n s _ a l s o _d e b a t e _ t h e _ n a t u r e _ o f _ v i s i t o r s _ a n d _ i_ u s e f u l n e s s _ b y _ di s c u s s i n g _ t h e _ s t u d y _ o f _ t h e _ d i s c i p l i n e _ a s _ a n _ e n d _ i n _ i t s e lf _ a n d _ a s _ a _ w a y _ o f _ p r o v i d i n g _ p e r s p e c t i v.figure 3: example system output for a cipher with 15% random noise (shown in red).
substitutions, insertions,and deletions are denoted by letters s, i, and d, respectively.
the system recovered 34/40 errors (ter is 5.86%).
highlighted segments show the errors that the system failed to recover from..% noise510152025.noise typesub sub, ins, del2.871.105.872.4010.585.2816.1711.4827.4317.63.table 5: ter (%) for solving 1:1 substitution cipherswith random insertion, deletion, and substitution noise.
these models have been trained with 10% noise..the ﬁrst 256 characters of the borg cipher to testour model.
our model is able to decipher the textwith an ser of 3.91% (figure 4).
we also try our14-language multilingual model on this cipher, andobtain an ser of 5.47%.
while we cannot directlycompare to aldarrab (2017), who do not reportser, this is a readable decipherment and can beeasily corrected by latin scholars who would beinterested in such a text..6 anagram decryption.
to further test the capacity of our model, we exper-iment with a special type of noise.
in this section,we address the challenging problem of solving sub-stitution ciphers in which letters within each wordhave been randomly shufﬂed.
anagramming is atechnique that can be used to further disguise substi-tution ciphers by permuting characters.
various the-ories about the mysterious voynich manuscript, forexample, suggest that some anagramming schemewas used to encode the manuscript (reddy andknight, 2011).
hauer and kondrak (2016) pro-pose a two-step approach to solve this problem.
first, they use their 1:1 substitution cipher solver(hauer et al., 2014) to decipher the text.
thesolver is based on tree search for the key, guidedby character-level and word-level n-gram languagemodels.
they adapt the solver by relaxing the letterorder constraint in the key mutation component ofthe solver.
they then re-arrange the resulting deci-phered characters using a word trigram languagemodel..72317 related work.
deciphering substitution ciphers is a well-studiedproblem in the natural language processing com-munity, e.g., (hart, 1994; olson, 2007; ravi andknight, 2008; corlett and penn, 2010; nuhn et al.,2013, 2014; hauer et al., 2014; aldarrab, 2017).
many of the recent proposed methods search forthe substitution table (i.e.
cipher key) that leadsto a likely target plaintext according to a charac-ter n-gram language model.
the current state-of-the-art method uses beam search and a neural lan-guage model to score candidate plaintext hypothe-ses from the search space for each cipher, alongwith a frequency matching heuristic incorporatedinto the scoring function (kambhatla et al., 2018).
this method, which is comparable in results to ourmethod on longer ciphers and slightly weaker onshorter ciphers, assumes prior knowledge of thetarget plaintext language.
our method, by contrast,can solve substitution ciphers from different lan-guages without explicit language identiﬁcation..recent research has looked at applying otherneural models to different decipherment problems.
greydanus (2017) ﬁnd an lstm model can learnthe decryption function of polyalphabetic substi-tution ciphers when trained on a concatenation of<key + ciphertext> as input and plaintext as out-put.
our work looks at a different problem.
wetarget a ciphertext-only-attack for short 1:1 substi-tution ciphers.
gomez et al.
(2018) propose ci-phergan, which uses a generative adversarialnetwork to ﬁnd a mapping between the characterembedding distributions of plaintext and ciphertext.
this method assumes the availability of plenty ofciphertext.
our method, by contrast, does not re-quire a large amount of ciphertext.
in fact, all ofour experiments were evaluated on ciphers of 256characters or shorter..early work on language identiﬁcation fromciphertext uses the noisy-channel deciphermentmodel (knight et al., 2006).
speciﬁcally, theexpectation-maximization algorithm is used tolearn mapping probabilities, guided by a pre-trained n-gram language model.
this deciphermentprocess is repeated for all candidate languages.
the resulting decipherments are ranked based onthe probability of the ciphertext using the learnedmodel, requiring a brute-force guess-and-check ap-proach that does not scale well as more languagesare considered.
hauer and kondrak (2016) usetechniques similar to ours, incorporating character.
figure 4: the ﬁrst 132 characters of the borg cipherand its decipherment.
errors are underlined.
cor-rect words are: pulegi, benedicti, crispe, ozimi, andfeniculi..we try a one-step, end-to-end anagram decryp-tion model.
in our sequence-to-sequence formu-lation, randomly shufﬂed characters can confusethe training.
we thus represent an input cipher asa bag of frequency-mapped characters, nominallypresented in frequency rank order (figure 5).
weuse the english gigaword dataset to train a 256character model on the sorted frequencies and teston the aforementioned test set of 50 ciphers (afterapplying random anagramming).
following hauerand kondrak (2016), we report word accuracy onthis task.
our model achieves a word accuracy of95.82% on the 50 wikipedia ciphers..hauer and kondrak (2016) report results ona test set of 10 long ciphers extracted from 10wikipedia articles about art, earth, europe, ﬁlm,history, language, music, science, technology, andwikipedia.
ciphers have an average length of 522characters.
they use english europarl to train theirlanguage models (koehn, 2005).
to get compara-ble results, we trained a model on ciphers of length525 created from the english side of the spanish-english europarl dataset.
our model achieved aword accuracy of 96.05% on hauer and kondrak’stest set.
training on english gigaword gave a wordaccuracy of 97.16%, comparable to the 97.72%word accuracy reported by hauer and kondrak(2016).
this shows that our simple model can crackrandomly anagrammed ciphers, which hopefullyinspires future work on other cipher types..7232(1) t h e _ i n v e n t i o n _ o f _ w r i t i n g _ s y s t e m s(2) j c z _ m r b z r j m k r _ k f _ w u m j m r e _ a o a j z g a(3) c j z _ k z m r b r j m r _ f k _ e w u j m m r _ z g o a j a a(4) 6 0 3 _ 5 3 1 2 7 2 0 1 2 _ 8 5 _ 11 9 10 0 1 1 2 _ 3 13 12 4 0 4 4(5) 0 3 6 _ 0 1 1 2 2 2 3 5 7 _ 5 8 _ 0 1 1 2 9 10 11 _ 0 3 4 4 4 12 13(6) t h e _ i n v e n t i o n _ o f _ b r i t a i n _ s y s t e m s.figure 5: example anagram encryption and decryption process: (1) original plaintext (2) after applying a 1:1 sub-stitution key (3) after anagramming (this is the ciphertext) (4) after frequency encoding (5) after sorting frequencies.
this is fed to transformer (6) system output (errors are highlighted)..frequency, decomposition pattern frequency, andtrial decipherment in order to determine the lan-guage of a ciphertext..8 conclusion and future work.
in this work, we present an end-to-end decipher-ment model that is capable of solving simple sub-stitution ciphers without the need for explicit lan-guage identiﬁcation.
we use frequency analysis tomake it possible to train a multilingual transformermodel for decipherment.
our method is able todecipher 700 ciphers from 14 different languageswith less than 1% ser.
we apply our method onthe borg cipher and achieve 5.47% ser using themultilingual model and 3.91% ser using a mono-lingual latin model.
in addition, our experimentsshow that these models are robust to different typesof noise, and can even recover from many of them.
to the best of our knowledge, this is the ﬁrst appli-cation of sequence-to-sequence neural models fordecipherment..we hope that this work drives more research inthe application of contextual neural models to thedecipherment problem.
it would be interesting todevelop other techniques for solving more com-plex ciphers, e.g.
homophonic and polyalphabeticciphers..acknowledgements.
this research is based upon work supported bythe ofﬁce of the director of national intelligence(odni), intelligence advanced research projectsactivity (iarpa), via afrl contract fa8650-17-c-9116.
the views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the ofﬁcialpolicies or endorsements, either expressed or im-plied, of the odni, iarpa, or the u.s. govern-ment.
the u.s. government is authorized to re-produce and distribute reprints for governmental.
purposes notwithstanding any copyright annotationthereon..ethics statement.
this work, like all decipherment work, is con-cerned with the decoding of encrypted commu-nications, and thus the methods it describes aredesigned to reveal information that has been de-liberately obfuscated and thus violate the privacyof the authors.
however, the class of problems itaddresses, 1:1 substitution ciphers, are known tobe relatively weak forms of encryption, once popu-lar, but long considered obsolete.
thus, the majorpractical use of this work as a decryption tool isin the ability to quickly decode ancient ciphertexts,such as the borg cipher, the contents of which areinteresting for historical purposes but are not indanger of revealing secrets of any living person.
modern encryption schemes such as rsa, blow-ﬁsh, or aes cannot be defeated by the methodspresented here..we have demonstrated our work’s effectivenesson ciphers of 14 alphabetic languages.
the ap-proaches presented here may be less effective onother orthographic systems such as abjads (whichhave fewer explicit symbols and more inherent am-biguity), abugidas (which have more explicit sym-bols and thus are conceivably less tractable), orlogographic systems (which have many more ex-plicit symbols).
we caution that more explorationneeds to be done before relying on the methodspresented here when decoding ancient historicalciphertexts that are not encodings of alphabeticplaintext..it is possible, though unlikely, that incorrect con-clusions can be drawn if the approaches presentedin this work yield false results.
for instance, in fig-ure 1b, the word decoded as peniculi (towels)should in fact be decoded as feniculi (fennel);similar examples can be seen in figure 3. the trans-lation “seed of towels” being far less likely than.
7233“seed of fennel“ in context, we would expect easydetection of this kind of error.
we recommend thatthese methods not be trusted exclusively, but ratherthat they be used as one tool in a cryptologist’s kit,alongside language expertise and common sense,such that incoherent decodings may be given a care-ful look and correction..references.
nada aldarrab.
2017. decipherment of historicalmanuscripts.
master’s thesis, university of south-ern california..eric corlett and gerald penn.
2010. an exact a*method for deciphering letter-substitution ciphers.
in proceedings of the 48th annual meeting of theassociation for computational linguistics, pages1040–1047, uppsala, sweden.
association for com-putational linguistics..john f. dooley.
2013. a brief history of cryptologyand cryptographic algorithms.
springer interna-tional publishing..yingqiang gao, nikola i. nikolov, yuhuang hu, andcharacter-levelrichard h.r.
hahnloser.
2020.translation with self-attention.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1591–1604, online.
as-sociation for computational linguistics..aidan n. gomez, sicong huang, ivan zhang, bryan m.li, muhammad osama, and lukasz kaiser.
2018.unsupervised cipher cracking using discrete gans.
corr, abs/1801.04883..sam greydanus.
2017. learning the enigma with re-current neural networks.
corr, abs/1708.07576..george w. hart.
1994. to decode short cryptograms..commun.
acm, 37(9):102–108..bradley hauer, ryan hayward, and grzegorz kon-drak.
2014. solving substitution ciphers with com-bined language models.
in proceedings of coling2014, the 25th international conference on compu-tational linguistics: technical papers, pages 2314–2325, dublin, ireland.
dublin city university andassociation for computational linguistics..bradley hauer and grzegorz kondrak.
2016. decod-ing anagrammed texts written in an unknown lan-guage and script.
tacl, 4:75–86..nishant kambhatla, anahita mansouri bigvand, andanoop sarkar.
2018. decipherment of substitutionin proceed-ciphers with neural language models.
ings of the 2018 conference on empirical methodsin natural language processing, pages 869–874,brussels, belgium.
association for computationallinguistics..kevin knight, beáta megyesi, and christiane schaefer.
2011. the copiale cipher.
in proceedings of the 4thworkshop on building and using comparable cor-pora: comparable corpora and the web, pages 2–9, portland, oregon.
association for computationallinguistics..kevin knight, anish nair, nishit rathod, and kenjiyamada.
2006. unsupervised analysis for deci-in proceedings of the col-pherment problems.
ing/acl 2006 main conference poster sessions,pages 499–506, sydney, australia.
association forcomputational linguistics..philipp koehn.
2005. europarl: a parallel corpus forin conference pro-the tenth machine translation summit,.
statistical machine translation.
ceedings:pages 79–86, phuket, thailand.
aamt, aamt..beáta megyesi, bernhard esslinger, alicia fornés, nilskopal, benedek láng, george lasry, karl de leeuw,eva pettersson, arno wacker, and michelle wald-ispühl.
2020. decryption of historical manuscripts:the decrypt project.
cryptologia, 44(6):545–559..malte nuhn, julian schamper, and hermann ney.
2013.beam search for solving substitution ciphers.
in pro-ceedings of the 51st annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1568–1576, soﬁa, bulgaria.
associa-tion for computational linguistics..malte nuhn, julian schamper, and hermann ney.
2014.improved decipherment of homophonic ciphers.
inproceedings of the 2014 conference on empiricalmethods in natural language processing (emnlp),pages 1764–1768, doha, qatar.
association forcomputational linguistics..edwin olson.
2007..short simple substitution ciphers.
31(4):332–342..robust dictionary attack ofcryptologia,.
myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andmichael auli.
2019. fairseq: a fast, extensiblein proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..robert parker, david graff, junbo kong, ke chen,and kazuaki maeda.
2011. gigaword ﬁfth editionldc2011t07..eva pettersson and beata megyesi.
2019. matchingkeys and encrypted manuscripts.
in proceedings ofthe 22nd nordic conference on computational lin-guistics, pages 253–261, turku, finland.
linköpinguniversity electronic press..sujith ravi and kevin knight.
2008. attacking de-cipherment problems optimally with low-order n-in proceedings of the 2008 confer-gram models.
ence on empirical methods in natural language.
7234processing, pages 812–819, honolulu, hawaii.
as-sociation for computational linguistics..sravana reddy and kevin knight.
2011. what weknow about the voynich manuscript.
in proceedingsof the 5th acl-hlt workshop on language tech-nology for cultural heritage, social sciences, andhumanities, pages 78–86, portland, or, usa.
as-sociation for computational linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a studyof translation edit rate with targeted human annota-tion.
in in proceedings of association for machinetranslation in the americas, pages 223–231..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..xusen yin, nada aldarrab, beata megyesi, andkevin knight.
2019. decipherment of historicalin 2019 international confer-manuscript images.
ence on document analysis and recognition (ic-dar), pages 78–85..7235