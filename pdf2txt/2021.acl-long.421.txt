matching distributions between model and data: cross-domainknowledge distillation for unsupervised domain adaptation.
bo zhang1, xiaoming zhang1,∗, yun liu2, lei cheng3, zhoujun li21 school of cyber science and technology, beihang university, china2 state key laboratory of software development environment, beihang university, china3 shenzhen research institute of big data, shenzhen, china{zhangnet,yolixs,gzliuyun,lizj}@buaa.edu.cn, leicheng@sribd.cn.
abstract.
unsupervised domain adaptation (uda)aims to transfer the knowledge of source do-main to the unlabeled target domain.
exist-ing methods typically require to learn to adaptthe target model by exploiting the source dataand sharing the network architecture acrossdomains.
however, this pipeline makes thesource data risky and is inﬂexible for deploy-this paper tacklesing the target model.
a novel setting where only a trained sourcemodel is available and different network ar-chitectures can be adapted for target domainin terms of deployment environments.
wepropose a generic framework named cross-domain knowledge distillation (cdkd) with-out needing any source data.
cdkd matchesthe joint distributions between a trained sourcemodel and a set of target data during dis-tilling the knowledge from the source modelto the target domain.
as a type of impor-tant knowledge in the source domain,forthe ﬁrst time, the gradient information is ex-ploited to boost the transfer performance.
ex-periments on cross-domain text classiﬁcationdemonstrate that cdkd achieves superior per-formance, which veriﬁes the effectiveness inthis novel setting..1.introduction.
annotating sufﬁcient training data is usually anexpensive and time-consuming work for diverseapplication domains.
unsupervised domain adap-tation (uda) aims at solving this learning prob-lem in the unlabeled target domain by utilizing theabundant knowledge in an existing domain calledsource domain, even when these domains may havedifferent distributions.
this technique has moti-vated research on cross-domain text classiﬁcation(chen et al., 2019; ye et al., 2020; gururanganet al., 2020).
one of the important knowledge inthe source domain is the labels of samples.
cur-rent methods mainly leverage the labeled source.
∗ corresponding author..data and unlabeled target data to learn the domain-invariant features (tzeng et al., 2014; ganin andlempitsky, 2015) and the discriminative features(saito et al., 2017; ge et al., 2020) that are sharedacross different domains..unfortunately, sometimes we are forbidden ac-cess to the source data, which are distributed ondifferent devices and usually contain private infor-mation, e.g., user proﬁle.
existing methods cannotsolve the uda problem without the source data yet.
in addition, it is necessary to adapt the target do-main with a ﬂexible network architecture differentfrom the source domain in terms of different de-ployment requirements for different domains.
butmost of works (liang et al., 2020; li et al., 2020)are required to share the same network architecturebetween different domains.
in this paper, we pro-pose a novel uda setting: only a trained sourcemodel and a set of unlabeled target data are pro-vided, and the target model is allowed to have dif-ferent network architectures with the trained sourcemodel.
it differs from the vanilla uda in that atrained source model instead of source data is pro-vided as supervision to the unlabeled target domainwhen learning to adapt the model.
such a settingsatisﬁes privacy policy and effective delivery, andhelps deploy the target model ﬂexibly according tothe target application..our setting seems somewhat similar to knowl-edge distillation (kd) (hinton et al., 2015), wherea trained teacher model teaches a student modelwith different architecture on the same task overa set of unlabeled data.
kd assumes that the em-pirical distribution of the data used for training thestudent model matches the distribution associatedwith the trained teacher model.
nevertheless, inour setting, the unlabeled data and teacher (source)model have different distributions.
one of simpleyet generic solution for our setting is to match thedistributions between source and target domainsunder the process of distilling the knowledge.
how-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5423–5433august1–6,2021.©2021associationforcomputationallinguistics5423ever, it is quite challenging to reduce the shifts be-tween a known distribution (e.g., a trained sourcemodel) and the empirical distribution of data (e.g.,target data).
prior methods minimize a distancemetric of domain discrepancy, such as maximummean discrepancy (mmd) (tzeng et al., 2014) tomatch the distributions across domains in termsof the source and target data.
unfortunately, theempirical evaluation of these metrics is unavailablesince we cannot access the source data..in this paper, we propose a generic frame-work named cross-domain knowledge distillation(cdkd).
speciﬁcally, we deﬁne a joint kernelizedstein discrepancy (jksd) that measures the largestdiscrepancy over the hilbert space of functions be-tween empirical sample expectations of target do-main and source distribution expectations.
inspiredby the works (liu et al., 2016), the source distri-bution expectations are being zero via the effect ofstein operator such that we can evaluate the discrep-ancy of joint distributions without any source data.
we embed jksd criterion into deep network wheremulti-view features including activations, gradientsand class probabilities in the source model are ex-ploited to explore the domain-invariant and discrim-inative features across domains.
in addition, wefurther maximize jksd using adversarial strategywhere the multi-view features are integrated intodomain adaptation abundantly.
finally, cdkd islearnt by joint optimizing both kd objective (hin-ton et al., 2015) and jksd.
the main contributionsare outlined as,.
• we propose to investigate the problem ofuda without needing source data by explor-ing the distribution discrepancy between asource model and a set of target data.
weadapt the target domain with different networkarchitecture ﬂexibly in terms of different de-ployment environments..• for the ﬁrst time, the gradient information ofthe source domain is exploited to boost theuda performance.
mu et al.
(2020) shows akey intuition that per-sample gradients containtask-relevant discriminative information..• we experiment under two amazon reviewdatasets for cross-domain text classiﬁcation,which demonstrates that cdkd still has ob-vious performance advantage in all settingsthough without needing any source data..2 related work.
2.1 unsupervised domain adaptation (uda).
uda aims at learning a model which can gener-alize across different domains following differentprobability distributions.
existing works mainly fo-cus on how to learn domain-invariant features anddiscriminative features that are shared across differ-ent domains.
moment matching, e.g., maximummean discrepancy (mmd) (tzeng et al., 2014) andadversarial learning (ganin and lempitsky, 2015)are commonly used to learn domain-invariant fea-tures by aligning the marginal distributions.
tolearn discriminative features for uda, self-trainingmethods (saito et al., 2017; zou et al., 2019) trainthe target classiﬁer in terms of the pseudo labelsof target data.
these works committed to improvethe quality of pseudo labels including introduc-ing mutual learning (ge et al., 2020) and dual in-formation maximization (ye et al., 2020).
theother line of learning discriminative features is tomatch the conditional distributions across domainsby aligning multiple domain-speciﬁc layers (longet al., 2017, 2018) or making an explicit hypothe-sis between conditional distributions (wang et al.,2018; yu et al., 2019; fang et al., 2020).
stn (yaoet al., 2019) explores the class-conditional distribu-tions to approximate the discrepancy between theconditional distributions via soft-mmd.
the work(zhang et al., 2021) derives a novel criterion con-ditional mean discrepancy (cmd) to measure theshifts between conditional distributions in tensor-product hilbert space directly..however, these methods assume the target userscan access to the source data, which is unsafe andsometimes unpractical since source data may be pri-vate and decentralized.
therefore, the recent workspropose to generalize a target model over a set ofunlabeled target data only in terms of the supervi-sion of a trained source model.
shot (liang et al.,2020) learns the target-speciﬁc feature extractionmodule by using both information maximizationand self-training strategy.
li et al.
(2020) improvethe target model through target-style data based ongenerative adversarial network (gan) where thegan and the target model are collaborated withoutsource data.
unfortunately, they require that thetarget model must share the same network architec-ture with the source model.
meanwhile, multi-viewfeatures in the source model including activationand gradient are not exploited which also contributemost to the domain adaptation..5424figure 1: the proposed cdkd framework for uda without source data..2.2 knowledge distillation (kd).
kd transfers the knowledge from a cumbersomemodel to a small model that is more suitable fordeployment (hinton et al., 2015).
the general tech-nique of kd involves using a teacher-student strat-egy, where a large deep teacher model trained fora given task teaches shallower student model onthe same task (yim et al., 2017; chen et al., 2018).
the teacher and student models are trained basedon the same data.
these kd methods make an as-sumption that the training data and the distributionassociated with the teacher model are independentand identically distributed.
however, sometimeswe are required to train a student model in a newdomain that the teacher model is not familiar, i.e,the domain shifts exist between the new domainand the domain that the teacher model is trained.
the proposed cdkd is able to relieve the domainshifts adaptively during distilling the knowledge..3 methodology.
we address the unsupervised domain adaptation(uda) task with only a trained source model andwithout access to source data.
we consider k-wayclassiﬁcation.
formally, in this novel setting, weare given a trained source model fs : x (cid:55)→ yand a target domain dt = {xi}mi=1 ⊂ x with munlabeled samples.
here, the goal of cross-domainknowledge distillation (cdkd) is to learn a targetmodel ft : x (cid:55)→ y and infer {yi}mi=1, with onlydt and fs available.
the target model ft is allowedto have different network architecture with fs..cdkd is a special kd which consists of a trainedteacher model fs, a student model ft and unla-beled data dt as well.
but it differs from kd inthat the empirical distribution of dt don’t match.
the distribution associated with the trained modelfs.
therefore, it is necessary to introduce distri-bution adaptation to eliminate the biases betweenthe source and target domains during distilling theknowledge.
speciﬁcally, as shown in figure 1(a),we ﬁrst introduce kd to distill the knowledge tothe target domain in terms of the class probabili-ties produced by the source model fs.
then, weintroduce a novel criterion jksd to match the jointdistributions across domains by evaluating the shiftbetween a known distribution and a set of data.
this is the ﬁrst work to explore the distributiondiscrepancy between a model and a set of data inuda task..3.1 distilling knowledge to target domain.
given a target sample x ∈ dt, the target modelft : x (cid:55)→ y produces class probabilities by us-ing a “softmax” output layer that converts the log-its p = (p1, · · · , pk) into a probability ft(x) =(q1, · · · , qk),.
qi =.
exp(pi/t )j exp(pj/t ).
(cid:80).
where t is a temperature used for generating“softer” class probabilities.
we optimize the targetmodel ft by minimizing the following objective forknowledge distillation,.
lkd = −.
fs(x)(cid:62) log ft(x).
(1).
1m.(cid:88).
x∈dt.
in our paper, the setting of temperature follows thework (hinton et al., 2015): a high temperature t isadopted to compute ft(x) during training, but afterit has been trained it uses a temperature of 1..5425(cid:41)(cid:38)(cid:37)(cid:68)(cid:70)(cid:78)(cid:69)(cid:82)(cid:81)(cid:72)knowledgedistillation(cid:2208)(cid:2208)(cid:2163)(cid:2201)(cid:4666)(cid:2208)(cid:4667)(cid:1858)(cid:1871)(cid:4666)(cid:1876)(cid:4667)(cid:3404)(cid:1833)(cid:1871)(cid:4666)(cid:1846)(cid:1871)(cid:4666)(cid:1876)(cid:4667)(cid:4667)(cid:1858)(cid:1872)(cid:4666)(cid:1876)(cid:4667)(cid:3404)(cid:1833)(cid:1872)(cid:4666)(cid:1846)(cid:1872)(cid:4666)(cid:1876)(cid:4667)(cid:4667)target data(cid:2163)(cid:2202)(cid:4666)(cid:2208)(cid:4667)(cid:1846)(cid:1871)(cid:1833)(cid:1871)source extractorsource classifiertarget extractortarget classifier(cid:54)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:55)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)knowledge distillationjoint kernelized stein discrepancy(cid:3)(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:3)(cid:51)(cid:85)(cid:82)(cid:69)(cid:68)(cid:69)(cid:76)(cid:79)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)(cid:42)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87)(cid:3)(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)unlabeledtarget data(cid:2260)(cid:2163)(cid:2201)(cid:4666)(cid:2208)(cid:4667)(cid:2260)(cid:2208)(cid:3)(cid:2163)(cid:2202)(cid:4666)(cid:2208)(cid:4667)(cid:45)(cid:46)(cid:54)(cid:39)(cid:41)(cid:38)(cid:46)(cid:81)(cid:82)(cid:90)(cid:39)(cid:76)(cid:86)(cid:87)(cid:76)(cid:79)(cid:79)(cid:1858)(cid:1871)(cid:4666)(cid:1876)(cid:4667)(cid:55)(cid:72)(cid:80)(cid:83)(cid:72)(cid:85)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:41)(cid:38)(cid:54)(cid:82)(cid:73)(cid:87)(cid:80)(cid:68)(cid:91)(cid:41)(cid:38)(cid:37)(cid:68)(cid:70)(cid:78)(cid:69)(cid:82)(cid:81)(cid:72)(cid:41)(cid:38)(cid:54)(cid:82)(cid:73)(cid:87)(cid:80)(cid:68)(cid:91)(cid:1846)(cid:1872)(cid:1833)(cid:1872)(cid:11)(cid:68)(cid:12)(cid:3)(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:89)(cid:76)(cid:72)(cid:90)(cid:3)(cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:72)(cid:85)(cid:85)(cid:72)(cid:71)(cid:17)(cid:11)(cid:70)(cid:12)(cid:3)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:38)(cid:71)(cid:46)(cid:39)(cid:17)(cid:11)(cid:69)(cid:12)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:83)(cid:76)(cid:83)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:82)(cid:88)(cid:85)(cid:3)(cid:38)(cid:71)(cid:46)(cid:39)(cid:3)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:90)(cid:82)(cid:85)(cid:78)(cid:17)(cid:41)(cid:38)(cid:1847)(cid:1848)(cid:2260)(cid:2163)(cid:2201)(cid:4666)(cid:2208)(cid:4667)(cid:2260)(cid:2208)(cid:2206)3.2.joint kernelized stein discrepancy.
in traditional uda setting, joint maximum meandiscrepancy (jmmd) (long et al., 2017) has beenapplied to measure the discrepancy in joint distribu-tions of different domains, and it can be estimatedempirically using ﬁnite samples of source and tar-get domains.
speciﬁcally, suppose k : x ×x (cid:55)→ rand l : y × y (cid:55)→ r are the positive deﬁnite ker-nels with feature maps φ(·) : x (cid:55)→ f and ψ(·) :y (cid:55)→ g for domains of x and y , respectively thatcorresponds to reproducing kernel hilbert space(rkhs) f and g .
let cpxy : g (cid:55)→ f be theuncentered cross covariance operator that be de-xy = e(x,y)∼p [φ(x) ⊗ ψ(y)].
jmmdﬁned as cpmeasures the shifts in joint distributions p (x, y)and q(x, y) by.
j(p, q) = sup.
eq(f (x)g(y)) − ep (f (x)g(y)).
f ⊗g∈h.
=(cid:107)cq.
xy − cpwhere h is a unit ball in f ⊗ g ..xy (cid:107)f ⊗g.
in our setting, unfortunately, the empirical esti-mation of jmmd is unavailable since we cannotaccess the source data ds directly (the empiricalestimation of jmmd is in appendix a.1).
ker-nelized stein discrepancy (ksd) as a statisticaltest for goodness-of-ﬁt can test whether a set ofsamples are generated from a marginal probability(chwialkowski et al., 2016; liu et al., 2016).
in-spired by ksd, we introduce joint ksd (jksd) toevaluate the discrepancy between a known distri-bution p (x, y) and a set of data ˆq = {xi, yi}mi=1obtained from a distribution q(x, y)..assume the dimension of x is d (x = rd), i.e.,x = (x1, · · · , xd), ∀x ∈ x .
we denote by f d =f × · · · f the hilbert space of d × 1 vector-valuedfunctions f = {f1, · · · , fd} with fi ∈ f , and withan inner product (cid:104)f, f (cid:48)(cid:105)f d = (cid:80)di(cid:105)f forf (cid:48) ∈ f d. we begin by deﬁning a stein operatorap : f d ⊗ g (cid:55)→ f d ⊗ g acting on functionsf ∈ f d and g ∈ g.i=1 (cid:104)fi, f (cid:48).
(ap f ⊗g)(x, y) = g(y) ( ∇xf (x)+f (x)∇x log p (x, y) )(cid:62) 1d.
(2).
where ∇x log p (x, y) = ∇xp (x,y)∈ rd×1,p (x,y)∇xf (x) = ( ∂f1(x), · · · , ∂fd(x)) ∈ rd×1 for x =∂xd∂x1(x1, · · · , xd) and 1d is a d × 1 vector with all ele-ments equal to 1. the expectation of stein operatorap over the distribution p is equal to 0.ep (ap f ⊗ g)(x, y) = 0.
(3).
which can be proved easily by (chwialkowski et al.,2016, lemma 5.1).
the stein operator ap canbe expressed by deﬁning a function ξxy over thespace f d ⊗ g that depends on gradients of thelog-distribution and the kernel,.
ξxy =∇xφ(x) ⊗ ψ(y).
+(∇x log p (x, y))φ(x) ⊗ ψ(y).
(4).
thus, (ap f ⊗ g)(x, y) can be presented as an in-ner product, i.e., (cid:104)f ⊗ g, ξxy(cid:105)f d⊗g .
now, we candeﬁne jksd and express it in the rkhs by re-placing the term f (x)g(y) in j(p, q) as our steinoperator,.
s(p, q) := sup.
eq(ap f ⊗ g)(x, y).
f ⊗g∈h(cid:48).
− ep (ap f ⊗ g)(x, y).
= sup eq(ap f ⊗ g)(x, y)= sup (cid:104)f ⊗ g, eqξxy(cid:105)f d⊗g=(cid:107)eqξxy(cid:107)f d⊗g.
where h(cid:48) is a unit ball in f d ⊗ g .
this makesit clear why eq.
3 is a desirable property: wecan compute s(p, q) by computing the hilbert-schmidt norm (cid:107)eqξxy(cid:107), without need to accessthe data obtained from p ..we can empirically estimate s2(p, q) based onthe known probability p and ﬁnite samples ˆq ={(xi, yi)}mi=1 ∼ q(x, y) in term of kernel tricksas follows,.
ˆs2(p, q) =.
1m2 tr(∇2kl + 2υl + ωl).
(5).
(∇2k)i,j = (cid:10)∇xiφ(xi), ∇xj φ(xj)(cid:11)υi,j = (∇xik(xi, xj))(cid:62)∇xj log p (xj, yj)(cid:16).
f d.ωi,j = k(xi, xj).
∇xi log p (xi, yi)(cid:62)∇xj log p (xj, yj)(cid:1).
where l = {l(yi, yj)} is the kernel gram matrix,(cid:104)∇xφ(x), ∇x(cid:48)φ(x(cid:48))(cid:105)f d = (cid:80)d, all thematrices ∇2k, υ, ω and l are in rm×m, andtr(m) is the trace of the matrix m. (refer to ap-pendix a.2 for detail.).
∂k(x,x(cid:48))∂xi∂x(cid:48)i.i=1.
in our experiments, we adopt gaussian ker-nel k(x1, x2) = exp(− 1σ2 (cid:107)x1 − x2(cid:107)2) where itsderivative ∇x1k(x1, x2) ∈ rd and (∇2k)i,j ∈ rcan be computed numerically,(cid:18).
(cid:19).
∇x1k(x1, x2) = k(x1, x2)(cid:18) 2d.
(∇2k)i,j = k(x1, x2).
−.
2σ2 (x1 − x2)4(cid:107)x1 − x2(cid:107)2σ4.
(cid:19).
σ2 −.
5426remark.
based on the virtue of goodness-ﬁt testtheory, we will have s(p, q) = 0 if and only ifp = q (chwialkowski et al., 2016).
instead ofapplying uniform weights as mmd does, jksdapplies non-uniform weights βi,j,.
marginal distribution directly, we approximate itby evaluating the cosine similarity of the represen-tations outputted from the source model and targetmodel, i.e.,.
ˆs2(p, q) =.
βi,jl(yi, yj).
(cid:88).
i,j.
where βi,j = (∇2k + 2υ + ω)i,j is, in turn, deter-mined by the activation-based and gradient-basedfeatures of the known probability p .
jksd com-putes a dynamic weight βi,j to decide whether thesample i shares the same label with other sample jin the target domain.
different from cluster-basedmethods (liang et al., 2020), jksd assigns eachsample a label according to all the data in the targetdomain instead of the centroid of each category.
the computation of centroid severely suffers fromin contrast,the noise due to the domain shifts.
our solution is more suitable for uda because weavoid to use the untrusted intermediate results (i.e.,the centroid of each category) to infer the labels..3.3 training.
the pipeline of our cdkd framework is shown infigure 1(b).
the source model parameterized by adnn consists of two modules: a feature extractorts : x (cid:55)→ z s and a classiﬁer gs : z s (cid:55)→ y, i.e.,fs(x) = gs(ts(x)).
the target model ft = tt◦gtalso has two modules where we use parallel nota-tions tt(·; θt ) : x (cid:55)→ z t and gt(·; θg) : z t (cid:55)→ yfor target model.
note here in our experiments, thedimension of the latent representations of sourcemodel is set equal to the target model, i.e., z s =z t = rd.
the extractors ts and tt are allowed toadopt different network architectures..the input space x is usually highly sparsewhere the kernel function cannot capture sufﬁ-cient features to measure the similarity.
there-fore, we evaluate jksd based on latent represen-tations of target samples, i.e., ˆq = {(z, y)|z =tt(x), y = gt(z), x ∈ dt} ∼ q(z, y).
in eq.
5, it is required to evaluate the joint probabilityp (y = y, z = z) = p(y|z)p(z) over a sample(z, y) obtained from ˆq.
the probability p(y|z)that the sample follows conditional distribution ofthe source domain p (y|z) can be evaluated asp(y|z) = y(cid:62)gs(z).
similarly, the term p(z) rep-resents the probability that the target representationz follows the marginal distribution p (z) of thesource domain.
since we cannot access the source.
p(z) =.
cos(z, ts(x)) +.
12.
12.where x = t −1(z) is the sample corresponding toz for any z ∈ ˆq.
formally, the term ∇z log p (z, y)in eq.
5 can be computed as.
t.∇z log p (z, y) =.
y(cid:62)∇zgs(z) +.
1p(y|z).
∇zp(z)p(z).
where ∇zgs(z) ∈ rk×d is a jacobian matrix ofthe target latent representation with respect to thesource classiﬁer gs..we propose to train the target model ft by jointlydistilling the knowledge from the source domainand reducing the shifts in the joint distributions viajksd,.
lkd + µ ˆs2(p, q).
minθt ,θg.
where µ > 0 is a tradeoff parameter for jksd..in order to maximize the test power of jksd, werequire the class of functions h ∈ f d⊗g to be richenough.
meanwhile, kernel-based metrics usuallysuffer from vanishing gradients for low-bandwidthkernels.
we are enlightened by (long et al., 2017)which introduces the adversarial training to circum-vent these issues.
speciﬁcally, we multiple fullyconnected layers u and v parameterized by θuand θv to jksd, i.e., k(xi, xj) and l(yi, yj) arereplaced as k(u (xi), u (xj)) and l(v (yi), v (yj))in eq.
5. we maximize jksd with respect tothe new parameters θu and θv to maximize thetest power of jksd such that the samples in thetarget domain are made more discriminative byabundantly exploiting the activation and gradientfeatures in the source domain.
as shown in figure1(c), the target model ft can be optimized by thefollowing adversarial objective,.
minθt ,θg.
maxθu ,θv.
lkd + µ ˆs2(p, q).
(6).
4 experiments.
4.1 setup.
to testify its versatility, we evaluate the proposedmodel in two tasks including uda and knowledgedistillation..5427table 1: classiﬁcation accuracy (%) on amazon-feature dataset using mlp extractor..models.
source onlytrain on target.
tca (pan et al., 2010)bda (wang et al., 2017)gfk (gong et al., 2012)ddc (tzeng et al., 2014)revgrad (ganin and lempitsky, 2015)daan (yu et al., 2019)shot (liang et al., 2020).
kd (µ = 0.0)our method.
d→b e→b k→b b→d e→d d→e b→k e→k avg.
71.881.7.
62.262.766.577.776.978.475.1.
71.977.0.
69.481.7.
59.558.763.074.874.770.975.2.
70.774.6.
69.581.7.
64.062.565.573.174.768.575.3.
72.776.1.
78.182.3.
62.464.366.379.680.277.081.1.
78.780.8.
69.382.3.
62.762.163.477.876.175.576.0.
65.077.2.
75.985.5.
66.367.064.080.379.477.379.0.
80.681.8.
77.685.8.
65.163.469.278.579.378.780.6.
80.582.5.
81.185.8.
73.874.573.383.584.184.084.7.
82.383.6.
74.183.4.
64.564.466.478.278.276.378.4.
75.379.2.amazon-review1 is a benchmark dataset for do-main adaptation in text classiﬁcation task.
twoversions of amazon review datasets are used toevaluate models.
the work provides a simpliﬁedamazon-review dataset (amazon-feature) col-lected from four distinct domains: books (b), dvd(d), electronics (e) and kitchen (k).
each domaincomprises 4,000 samples with 400d feature rep-resentations and 2 categories (positive and nega-tive).
zhang et al.
(2021) collected a larger datasetcalled amazon-text from amazon-review withthe same domains in amazon-feature to test themodel performance for large-scale transfer learning.
the review texts are divided into two categories ac-cording to user rating, i.e., positive (5 stars) andnegative (1 star).
there are 10,000 original reviewtexts in each category and 20,000 texts in each do-main.
the notation s→t represents the transferlearning from the source domain s to the targetdomain t..baselines.
for the bulk of experiments the fol-lowing baselines are evaluated.
the source-onlymodel is trained only over source domain and testedover target-domain data while train-on-targetmodel is trained and tested over target-domain datadirectly.
we compare with conventional domainadaptation methods: transfer component analysis(tca) (pan et al., 2010), balanced distributionadaptation (bda) (wang et al., 2017), geodesicflow kernel (gfk) (gong et al., 2012), deepdomain confusion (ddc) (tzeng et al., 2014),domain adversarial neural networks (revgrad)(ganin and lempitsky, 2015) and dynamic ad-versarial adaptation network (daan) (yu et al.,2019).
we compare with shot (liang et al.,2020) for the uda task without the source data..we also compare with the knowledge distillationmethod (kd) (hinton et al., 2015) in our setting.
in our experiments, three different extractors areselected.
for amazon-feature dataset, the extrac-tor is simply modeled as a typical 3-layer fullyconnected network (mlp) to transform 400d in-puts into 50d latent feature vectors.
two types ofnetworks are leveraged for amazon-text dataset toencode the original review texts, i.e., textcnn andbertgru.
textcnn (kim, 2014) is a text convo-lutional network that consists of 150 convolutionalﬁlters with 3 different window sizes.
we also eval-uate the performance of cross-domain text classiﬁ-cation on a pre-trained language model, i.e., bert(devlin et al., 2019).
we freeze bert model andconstruct a 2-layer bi-directional gru (cho et al.,2014) to learn from the representations produced bybert.
the classiﬁer is modeled as a 2-layer fullyconnected network for all the settings.
for cdkd,we consider to learn the source model fs by min-imizing the standard cross-entropy loss.
we ran-domly specify a 0.7/0.3 split in the source datasetand generate the optimal source model based on thevalidation split.
u and v are modeled as weightmatrices..we implement all deep methods based on py-torch framework, and bert model is implementedand pre-trained by pytorch-transformers2.
weadopt gaussian kernel with bandwidth set to me-dian pairwise squared distances on the trainingdata (gretton et al., 2012).
the temperature tis set to 10 during training.
we use adamw op-timizer (loshchilov and hutter, 2019) with batchsize of 128 and the learning rate annealing strat-egy in (long et al., 2017):it is adjusted dur-ing back propagation using the following formula:.
2https://github.com/huggingface/.
1http://jmcauley.ucsd.edu/data/amazon/.
transformers.
5428table 2: classiﬁcation accuracy (%) on amazon-text dataset using textcnn and bertgru extractors..models.
e→b k→b b→d e→d k→d b→e d→e d→k avg.
source onlytrain on targetddc (tzeng et al., 2014)revgrad (ganin and lempitsky, 2015)daan (yu et al., 2019)shot (liang et al., 2020)kd (µ = 0.0)our method.
source onlytrain on targetddc (tzeng et al., 2014)revgrad (ganin and lempitsky, 2015)daan (yu et al., 2019)shot (liang et al., 2020)kd (µ = 0.0)our method.
using textcnn as extractor.
68.783.769.671.773.372.471.774.0.
85.193.287.887.588.786.585.687.8.
69.783.769.972.071.172.170.072.7.
85.193.286.683.785.787.287.088.0.
81.289.182.081.983.081.980.983.2.
91.694.992.292.792.091.992.292.8.
75.889.176.878.576.174.073.876.6.
88.694.991.290.589.890.090.190.4.using bertgru as extractor.
70.389.176.568.873.177.274.776.3.
89.594.990.988.290.489.390.191.8.
68.785.472.570.273.572.875.277.0.
85.092.687.385.085.587.286.687.6.
62.885.470.269.270.973.365.675.0.
84.692.687.087.286.686.087.287.8.
64.985.563.469.471.172.567.174.3.
84.394.487.486.688.885.986.387.2.
70.386.472.672.774.074.572.476.1.
86.793.888.887.788.488.088.189.2.table 3: classiﬁcation accuracy (%) on knowledge distillation task..models.
textcnnbertgru.
kd (hinton et al., 2015)cdkd (our).
e→b k→b b→d e→d k→d b→e d→e d→k avg.
69.583.8.
83.183.8.
67.484.4.
81.883.5.
79.791.3.
87.087.9.
72.987.0.
86.386.7.
71.288.6.
85.886.6.
70.284.8.
82.683.9.
64.679.1.
78.582.3.
65.579.7.
78.281.8.
70.184.8.
82.984.6.η0.
ηp =(1+10p)0.75 where p is the training progresslinearly changing from 0 to 1 and η0 is set to 0.001.we apply the same strategy in (ganin and lempit-sky, 2015) to adjust the factor µ dynamically, i.e.,we gradually change it from 0 to 1 by a progressiveschedule: µp =1+exp(−10p) − 1..2.
4.2 results.
in the ﬁrst experiment, we compare with the con-ventional domain adaptation methods where thesource model and target model share the same net-work architectures.
the classiﬁcation accuracy re-sults on the amazon-feature dataset for domainadaptation based on mlp are shown in table 1.some of the observations and analysis are listed asfollows.
(1) the performance of traditional udamethods (e.g., tca, gfk and bda) is worse thansource-only model, i.e., negative transfer learningoccurs in all transfer tasks.
these models directlydeﬁne kernel over sparse input vectors such that thekernel function cannot capture sufﬁcient features tomeasure the similarity.
the deep transfer methodsoutperform all the traditional methods, suggestingthat embedding domain adaptation modules into.
deep network can reduce domain discrepancy sig-niﬁcantly.
(2) the average accuracy of cdkd isslightly 1.0% higher than other deep transfer meth-ods (ddc, revgrad, daan and shot) overall.
itveriﬁes the positive effect of transferring the knowl-edge from trained source model without accessingthe source data..table 2 shows the classiﬁcation performanceof deep uda models based on textcnn andbertgru over a large dataset amazon-text.
fortextcnn extractor, we have following analysis.
cdkd achieves superior performance over priormethods by larger margins compared to smalldataset amazon-feature.
compared to ddc andrevgrad that obtains the domain-invariant features,cdkd can learn discriminative information fromthe source model by minimizing jksd criterion.
shot assumes that the target outputs should besimilar to one-hot encoding.
however, the one-hot encoding used in shot is noisy and untrusteddue to the domain shifts.
different from shot,we match the joint distributions across domains interms of multi-view features rather than only classprobabilities when adapting the target model.
by.
5429figure 2: accuracy (%) results of cdkd and its abla-tions..going from textcnn to extremely deep bertgru,we attain a more in-depth understanding of featuretransferability.
bertgru-based models outperformtextcnn-based models signiﬁcantly, which showsbert enables learning more transferable represen-tations for uda.
our cdkd has a slight advantagecompared to other models overall under the pow-erful transferability of bertgru.
it reveals the ne-cessity of designing a moment matching approachto incorporate activation and gradient features intodomain adaptation for reducing the losses causedby the lack of source data..in the second experiment, we compare with thekd model where the knowledge in bertgru is dis-tilled to the textcnn-based model.
we generatethe optimal bertgru as the teacher model basedon the source dataset.
the textcnn model usesbert tokenizer tool to guarantee the same inputspace between two models.
we randomly specifya 0.5/0.2/0.3 split in the target dataset where wetrain and select textcnn-based model based onthe train split and validation split respectively.
theresult is reported in table 3 in terms of the test split.
the average accuracy of cdkd is 1.6% higherthan original kd and approaches to the teachermodel bertgru.
signiﬁcantly, the accuracy scoresof tasks d → e and d → k are higher than bert-gru.
this is attributed to distribution adaptationwhere extra performance is also gained from jksdbesides the guidance of the teacher model..4.3 analysis.
ablation study.
we conduct the ablation exper-iments to see the contributions of gradient infor-mation (g) and the adversarial strategy (a), whichare evaluated with textcnn extractor for udatask.
by ablating cdkd, we have two baselinesof cdkd-g (w/o g) and cdkd-a (w/o a).
forcdkd-g, we set the gradient of log-distribution∇xj log p (xj, yj) ∈ rd×1 to a constant, i.e.,1d (1, 1, ..., 1)(cid:62) while we optimize cdkd withoutadversarial strategy for cdkd-a.
from the resultsin figure 2, cdkd-g and cdkd-a perform worse.
figure 3: accuracy (%) result of cdkd and kd fordifferent source models..figure 4: accuracy (%) result of cdkd for varyingbatch sizes..than cdkd but still better than kd, suggesting thatgradient information and the adversarial strategyboth contribute to the improvements of our model.
the gradient information is one type of importantknowledge in the source domain, but all previousmethods ignore its importance for uda.
effects of source model accuracy.
here westudy how the performance of target model are in-ﬂuenced by the source model accuracy, which areanalyzed based on b → e task using textcnnextractor.
we randomly obtain 9 optimal sourcemodels using different seeds over b dataset, andtrain cdkd and kd models based on differentsource models for b → e task.
figure 3 showsthe classiﬁcation accuracy of cdkd and kd byvarying accuracy of source models tested over edataset.
cdkd obtains similar performance underdifferent source models, indicating that cdkd isnot very sensitive to the quality of source models.
however, the curves of kd is unstable, i.e., theperformance of kd is vulnerable to the impact ofthe source models, because different source modelsfollow the different distributions.
obviously, jksdplays a crucial role in determining the effects ofalleviating this distribution discrepancy among dif-ferent source models.
effects of batch size.
batch size is a key parame-ter to optimize jksd metric because it is requiredto compute kernel over a min-batch of data.
figure4 shows the classiﬁcation accuracy of cdkd byvarying batch size in {64, 128, 256, 512}.
the ex-periment shows that cdkd is not sensitive to batchsize when batch size is larger than 64, suggesting.
5430b -> ee -> bk -> d7072747678accuracy (%)  cdkdcdkd-gcdkd-akd(b)69.269.469.669.87070.270.470.670.87171.27172737475767778source model accuracy (%)target model accuracy (%)  kd (µ=0)cdkdb -> ee -> bk -> d72747678accuracy (%)  64128256512(c)that cdkd don’t need a very large batch size foraccurate estimation of jksd..5 conclusion.
in this paper, we shed a new light on the challengesof uda without needing source data.
speciﬁcally,we provided a generic framework named cdkdto learn a classiﬁcation model over a set of un-labeled target data by making use of the knowl-edge of the activation and gradient informationin the trained source model.
cdkd learned thecollective knowledge across different domains in-cluding domain-invariant and discriminative fea-tures by matching the joint distributions between atrained source model and a set of target data.
exper-iments for cross-domain text classiﬁcation testiﬁedthat cdkd still achieves advantages for uda taskthough without any source data and improves theperformance of kd task when the trained teachermodel doesn’t match the training data..acknowledgments.
this work was supported in part by national nat-ural science foundation of china under grant62001309, in part by the opening project of bei-jing key laboratory of internet culture and digitaldissemination research and in part by the openresearch fund from shenzhen research instituteof big data (no.
2019orf01012)..references.
xilun chen, ahmed hassan awadallah, hany has-san, wei wang, and claire cardie.
2019. multi-source cross-lingual model transfer: learning whatto share.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 3098–3112, florence, italy.
associationfor computational linguistics..yuntao chen, naiyan wang, and zhaoxiang zhang.
2018. darkrank: accelerating deep metric learn-ing via cross sample similarities transfer.
in aaai,pages 2852–2859..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..kacper chwialkowski, heiko strathmann, and arthurgretton.
2016. a kernel test of goodness of ﬁt.
vol-ume 48 of proceedings of machine learning re-search, pages 2606–2615, new york, new york,usa.
pmlr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..xianghong fang, haoli bai, ziyi guo, bin shen,steven hoi, and zenglin xu.
2020. dart: domain-adversarial residual-transfer networks for unsuper-vised cross-domain image classiﬁcation.
neuralnetworks..yaroslav ganin and victor lempitsky.
2015. unsuper-vised domain adaptation by backpropagation.
in in-ternational conference on machine learning, pages1180–1189..yixiao ge, dapeng chen, and hongsheng li.
2020.mutual mean-teaching: pseudo label reﬁnery forunsupervised domain adaptation on person re-in 8th international conference onidentiﬁcation.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020..boqing gong, yuan shi, fei sha, and kristen grauman.
2012. geodesic ﬂow kernel for unsupervised do-main adaptation.
in 2012 ieee conference on com-puter vision and pattern recognition, pages 2066–2073. ieee..arthur gretton, karsten m borgwardt, malte j rasch,bernhard sch¨olkopf, and alexander smola.
2012. akernel two-sample test.
journal of machine learn-ing research, 13(mar):723–773..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages8342–8360, online.
association for computationallinguistics..geoffrey e hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
arxiv: machine learning..yoon kim.
2014..convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational lin-guistics..5431rui li, qianfen jiao, wenming cao, hau-san wong,and si wu.
2020. model adaptation: unsuperviseddomain adaptation without source data.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition, pages 9641–9650..jian liang, dapeng hu, and jiashi feng.
2020. do wereally need to access the source data?
source hypoth-esis transfer for unsupervised domain adaptation.
ininternational conference on machine learning..qiang liu, jason lee, and michael jordan.
2016.a kernelized stein discrepancy for goodness-of-ﬁttests.
in international conference on machine learn-ing, pages 276–284..mingsheng long, zhangjie cao, jianmin wang, andmichael i jordan.
2018. conditional adversarial do-main adaptation.
in advances in neural informationprocessing systems, pages 1640–1650..mingsheng long, han zhu,.
jianmin wang, andmichael i jordan.
2017. deep transfer learning within international confer-joint adaptation networks.
ence on machine learning, pages 2208–2217..ilya loshchilov and frank hutter.
2019. decoupledin 7th international.
weight decay regularization.
conference on learning representations..fangzhou mu, yingyu liang, and yin li.
2020. gradi-ents as features for deep representation learning.
in8th international conference on learning represen-tations, iclr 2020..sinno jialin pan, ivor w tsang, james t kwok, andqiang yang.
2010. domain adaptation via transfercomponent analysis.
ieee transactions on neuralnetworks, 22(2):199–210..kuniaki saito, yoshitaka ushiku, and tatsuya harada.
2017. asymmetric tri-training for unsupervised do-in international conference onmain adaptation.
machine learning, pages 2988–2997..eric tzeng, judy hoffman, ning zhang, kate saenko,and trevor darrell.
2014. deep domain confusion:maximizing for domain invariance.
arxiv preprintarxiv:1412.3474..jindong wang, yiqiang chen, shuji hao, wenjie feng,and zhiqi shen.
2017. balanced distribution adap-tation for transfer learning.
in 2017 ieee interna-tional conference on data mining (icdm), pages1129–1134.
ieee..jindong wang, wenjie feng, yiqiang chen, han yu,meiyu huang, and philip s yu.
2018. visual do-main adaptation with manifold embedded distribu-in proceedings of the 26th acmtion alignment.
international conference on multimedia, pages 402–410..yuan yao, yu zhang, xutao li, and yunming ye.
2019.heterogeneous domain adaptation via soft transfernetwork.
in proceedings of the 27th acm interna-tional conference on multimedia, page 1578–1586..hai ye, qingyu tan, ruidan he, juntao li, hwee toung, and lidong bing.
2020. feature adaptation ofpre-trained language models across languages anddomains with robust self-training.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 7386–7399, online.
association for computational lin-guistics..junho yim, donggyu joo, jihoon bae, and junmo kim.
2017. a gift from knowledge distillation: fast op-timization, network minimization and transfer learn-ing.
in proceedings of the ieee conference on com-puter vision and pattern recognition, pages 4133–4141..chaohui yu, jindong wang, yiqiang chen, and meiyuhuang.
2019. transfer learning with dynamic ad-versarial adaptation network.
in international con-ference on data mining, pages 778–786.
ieee..bo zhang, xiaoming zhang, yun liu, and lei chen.
2021. discriminative feature adaptation via con-ditional mean discrepancy for cross-domain textin international conference onclassiﬁcation.
database systems for advanced applications (das-faa), pages 104–119..yang zou, zhiding yu, xiaofeng liu, bvk kumar,and jinsong wang.
2019. conﬁdence regularizedin proceedings of the ieee interna-self-training.
tional conference on computer vision, pages 5982–5991..a appendices.
a.1 empirical evaluation of jmmd.
jmmd j(p, q) measures the shifts in joint distri-butions p (x, y) and q(x, y) by.
supf ⊗g∈h.
eq(f (x)g(y)) − ep (f (x)g(y)).
= sup eq ((cid:104)f ⊗ g, φ(x) ⊗ ψ(y)(cid:105))− ep ((cid:104)f ⊗ g, φ(x) ⊗ ψ(y)(cid:105)).
(cid:68).
= sup.
f ⊗ g, cqxy − cpxy(cid:13)(cid:13)(cid:13)f ⊗g.
xy − cpxy.
(cid:13)(cid:13)cq(cid:13).
=.
(cid:69).
f ⊗g.
given a source domain ds = {(xsa.p (x, y){(xtj, ytestimation of jmmd is,.
andj=1 ∼ q(x, y),.
i )}ndomain dt.
i=1 ∼=the empirical.
j)}m.i , ys.
target.
ˆj 2(p, q) =.
1m2 tr(kttltt)tr(kstlts).
1n2 tr(ksslss) +2mni , xtj) and (lst)i,j =j) are gram matrices, and tr(a) is the trace.
where (kst)i,j = k(xsl(ys.
i , yt.
(7).
−.
5432of the matrix a. the eq.
7 applies the sourcedata kst, kss, lst and lss to compute the scoreof jmmd, which cannot adapt to our new settingobviously.
note here that the jmmd used in ourpaper is a simpliﬁed version of (long et al., 2017),where we only consider two variables..a.2 empirical evaluation of jksd.
denote λxy = ∇xφ(x) + φ(x)(∇x log p (x, y))where ξxy can be represented as ξxy = λxy ⊗ψ(y).
the empirical evaluation of jksd can be computedas,.
(cid:107)eqξxy(cid:107)2 = (cid:104)eqξxy, eqξxy(cid:105).
=eqeq(cid:48)=eqeq(cid:48)=eqeq(cid:48).
(cid:10)λxy ⊗ ψ(y), λx(cid:48)y(cid:48) ⊗ ψ(y(cid:48))(cid:11)(cid:10)ψ(y), ψ(y(cid:48))(cid:11)(cid:10)λxy, λx(cid:48)y(cid:48)g(cid:10)λxy, λx(cid:48)y(cid:48).
f df d l(y, y(cid:48)).
(cid:11).
(cid:11).
where eq(cid:48)[·] refers to e(x(cid:48),y(cid:48))∼q[·]..for f = (f1, · · · , fd) ∈ f d and g =(g1, · · · , gd) ∈ f d, the inner product betweenf and g is deﬁned as (cid:104)f, g(cid:105) = (cid:80)di=1 (cid:104)fi, gi(cid:105)f .
the inner productbased on this deﬁnition,(cid:104)∇xφ(x), ∇x(cid:48)φ(x(cid:48))(cid:105)f d can be evaluated as.
d(cid:88).
i=1.
(cid:28) ∂φ(x)∂xi.
,.
∂φ(x(cid:48))∂x(cid:48)i.
(cid:29).
f.=.
d(cid:88).
i=1.
∂k(x, x(cid:48))∂xi∂x(cid:48)i.similar to (chwialkowski et al., 2016), we cancompute h(x, y, x(cid:48), y(cid:48)) = (cid:10)λxy, λx(cid:48)y(cid:48).
f d as,.
(cid:11).
∇x log p (x, y)(cid:62)∇x(cid:48) log p (x(cid:48), y(cid:48))k(x, x(cid:48))+ ∇x log p (x, y)(cid:62)∇x(cid:48)k(x, x(cid:48))+ ∇x(cid:48) log p (x(cid:48), y(cid:48))(cid:62)∇xk(x, x(cid:48))+ (cid:10)∇xφ(x), ∇x(cid:48)φ(x(cid:48))(cid:11).
f d.thus, jksd s2(p, q) is the expectation ofh(x, y, x(cid:48), y(cid:48))l(y, y(cid:48)) over the distribution q,.
s2(p, q) = eqeq(cid:48)h(x, y, x(cid:48), y(cid:48))l(y, y(cid:48)).
given a set of samples dt = {(xi, yi)}m.i=1 ∼.
q(x, y), we can evaluate s2(p, q) as.
1m2.
(cid:88).
(cid:88).
x,y.
x(cid:48),y(cid:48).
h(x, y, x(cid:48), y(cid:48))l(y, y(cid:48)).
which can be represented in the matrix form asshown in eq.
5..5433