measuring and increasing context usage incontext-aware machine translation.
patrick fernandes1,2,3.
kayo yin1.
graham neubig1.
andr´e f. t. martins2,3,4.
1language technologies institute, carnegie mellon university, pittsburgh, pa2instituto superior t´ecnico & lumlis (lisbon ellis unit), lisbon, portugal3instituto de telecomunicac¸ ˜oes, lisbon, portugal4unbabel, lisbon, portugal.
{pfernand, kayoy, gneubig}@cs.cmu.edu.
andre.t.martins@tecnico.ulisboa.pt.
abstract.
recent work in neural machine translation hasdemonstrated both the necessity and feasibil-ity of using inter-sentential context — contextfrom sentences other than those currently be-ing translated.
however, while many currentmethods present model architectures that theo-retically can use this extra context, it is oftennot clear how much they do actually utilize itat translation time.
in this paper, we introducea new metric, conditional cross-mutual infor-mation, to quantify the usage of context bythese models.
using this metric, we measurehow much document-level machine translationsystems use particular varieties of context.
weﬁnd that target context is referenced more thansource context, and that conditioning on alonger context has a diminishing effect on re-sults.
we then introduce a new, simple train-ing method, context-aware word dropout, toincrease the usage of context by context-awaremodels.
experiments show that our methodincreases context usage and that this reﬂectson the translation quality according to metricssuch as bleu and comet, as well as per-formance on anaphoric pronoun resolution andlexical cohesion contrastive datasets.1.
1.introduction.
while neural machine translation (nmt) is re-ported to have achieved human parity in some do-mains and language pairs (hassan et al., 2018),these claims seem overly optimistic and no longerhold with document-level evaluation (toral et al.,2018; l¨aubli et al., 2018).
recent work on context-aware nmt attempts to alleviate this discrepancyby incorporating the surrounding context sentences(in either or both the source and target sides) in thetranslation system.
this can be done by, for ex-ample, feeding context sentences to standard nmt.
1https://github.com/neulab/contextual-mt.
figure 1: illustration of how we can measure contextusage by a model qm t as the amount of informationgained when a model is given the context c and sourcex vs when the model is only given the x..models (tiedemann and scherrer, 2017), using dif-ferent encoders for context (zhang et al., 2018),having cache-based memories (tu et al., 2018a),or using models with hierarchical attention mecha-nisms (miculicich et al., 2018; maruf et al., 2019a)— more details in §2.
while such works report gainsin translation quality compared to sentence-levelbaselines trained on small datasets, recent workhas shown that, in more realistic high-resourcedscenarios, these systems fail to outperform sim-pler baselines with respect to overall translationaccuracy, pronoun translation, or lexical cohesion(lopes et al., 2020)..we hypothesize that one major reason for theselacklustre results is due to the fact that models withthe architectural capacity to model cross-sententialcontext do not necessarily learn to do so whentrained with existing training paradigms.
how-ever, even quantifying model usage of context isan ongoing challenge; while contrastive evalua-tion has been proposed to measure performanceon inter-sentential discourse phenomena (m¨ulleret al., 2018; bawden et al., 2018), this approachis conﬁned to a narrow set of phenomena, such aspronoun translation and lexical cohesion.
a tool-box to measure the impact of context in broadersettings is still missing..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6467–6478august1–6,2021.©2021associationforcomputationallinguistics6467source:.
the church is merciful.
.
.
it always welcomes the misguided lamb..target:.
baseline.
die kirche ist barmherzig.
.
.
es heisst die fehlgeleiteten sch¨aﬂein immerwillkommen..context-aware es heisst die fehlgeleiteten sch¨aﬂein immer.
context-awarew/ our method.
willkommen.
sie heisst die fehlgeleiteten sch¨aﬂein immerwillkommen..table 1: example where context (italic) is needed tocorrectly translate the pronoun “it”.
both the sentence-level baseline and context-aware model fail to correctlytranslate it while the context-aware model trained withcoword dropout correctly captures the context..to address the limitations above, we take inspi-ration from the recent work of bugliarello et al.
(2020) and propose a new metric, conditionalcross-mutual information (cxmi, §3), to mea-sure quantitatively how much context-aware mod-els actually use the provided context by comparingthe model distributions over a dataset with and with-out context.
figure 1 illustrates how it measurescontext usage.
this metric applies to any proba-bilistic context-aware machine translation model,not only the ones used in this paper.
we releasea software package to encourage the use of thismetric in future context-aware machine translationresearch.
we then perform a rigorous empiricalanalysis of the cxmi between the context and tar-get for different context sizes, and between sourceand target context.
we ﬁnd that: (1) context-awaremodels use some information from the context, butthe amount of information used does not increaseuniformly with the context size, and can even leadto a reduction in context usage; (2) target contextseems to be used more by models than source con-text..given the ﬁndings, we next consider how toencourage models to use more context.
specif-ically, we introduce a simple but effective vari-ation of word dropout (sennrich et al., 2016a)for context-aware machine translation, dubbedcoword dropout (§4).
put simply, we randomlydrop words from the current source sentence by re-placing them with a placeholder token.
intuitively,this encourages the model to use extra-sententialinformation to compensate for the missing informa-tion in the current source sentence.
we show thatmodels trained with coword dropout not onlyincrease context usage compared to models trained.
without it but also improve the quality of transla-tion, both according to standard evaluation metrics(bleu and comet) and according to contrastiveevaluation based on inter-sentential discourse phe-nomena such as anaphoric pronoun resolution andlexical cohesion (§4.2, table 1)..2 context-aware neural machine.
translation.
we are interested in learning a system that trans-lates documents consisting of multiple sentencesbetween two languages.2 more formally, givena corpus of parallel documents in two languages,d = {d1, ..., dn }, where each document is asequence of source and target sentences, d ={(x(1), y(1)), ..., (x(k), y(k))}, we are interestedin learning the mapping between the two lan-guages..we consider the typical (auto-regressive) neuralmachine translation system qθ parameterized by θ.the probability of translating x(i) into y(i) giventhe context of the sentence c(i) is.
qθ(y(i)|x(i), c(i)) =.
qθ(y(i)t.|x(i), y(i).
<t, c(i)).
t(cid:89).
t=1.
where y(i)represents the tth token of sentencety(i).
this context can take various forms.
onone end, we have the case where no context ispassed, c(i) = ∅, and the problem is reduced tosentence-level translation.
on the other end, wehave the case where all the source sentences and allthe previous generated target sentences are passedas context c(i) = {x(1), ..., x(k), y(1), ..., y(i−1)}.
as mentioned, there are many architectural ap-proaches to leveraging context (see §5 for a morecomplete review), and the methods that we presentin this paper are compatible with most architecturesbecause they do not specify how the model qθ usesthe context.
in experiments, we focus mostly onthe simpler approach of concatenating the contextto the current sentences (tiedemann and scherrer,2017).
recent work by lopes et al.
(2020) hasshown that, given enough data (either through pre-training or larger contextual datasets), this simpleapproach tends to be competitive with or even out-perform its more complex counterparts.
2here, a “document” could be an actual document but itcould also represent other contextual collections of text, suchas a sequence of dialogue utterances..64683 measuring context usage.
3.1 conditional cross-mutual information.
while context-aware models allow use of context,they do not ensure contextual information is ac-tually used: models could just be relying on thecurrent source sentence and/or previously gener-ated target words from the same sentence whengenerating the output..contrastive evaluation, where models are as-sessed based on the ability to distinguish correcttranslations from contrastive ones, is a commonway to assess the ability of context-aware modelsto capture speciﬁc discourse phenomena that re-quire inter-sentential context, such as anaphora res-olution (m¨uller et al., 2018) and lexical cohesion(bawden et al., 2018).
however, these methodsonly provide an indirect measure of context usagewith respect to a limited number of phenomena andcan fail to capture other, unknown ways in whichthe model might be using context.
kim et al.
(2019)showed that most improvements to translation qual-ity are due to non-interpretable usages of context,such as the introduction of noise that acts as a reg-ularizer to the encoder/decoder.
this problem isfurther exacerbated by the fact that there is no cleardeﬁnition of what entails “context usage”..in a different context, bugliarello et al.
(2020) in-troduced cross-mutual information (xmi), to mea-sure the “difﬁculty” of translating between differ-ent language pairs in sentence-level neural machinetranslation.
given a language model qlm for a tar-get sentence y and a translation model qm t fortranslating from x to y , xmi is deﬁned as:.
xmi(x → y ) = hqlm (y ) − hqm t (y |x),.
where hqlm denotes the cross-entropy of the tar-get sentence y under the language model qlm andhqm t the conditional cross-entropy of y given xunder the translation model qm t .
this allows us tomeasure how much information the source sentencegives us about the target sentence (an analogue ofmutual information for cross-entropy).
in the casewhere qlm and qm t perfectly model the underly-ing probabilities we would have xmi(x → y ) =mi(x, y ), the true mutual information..taking inspiration from the above, we pro-pose conditional cross-mutual information(cxmi), a new measure of the inﬂuence of contexton a model’s predictions.
this is done by consid-ering an additional variable for the context c and.
measuring how much information the context cprovides about the target y given the source x.this can then be formulated as.
cxmi(c → y |x) =hqm ta.
(y |x) − hqm tc.
(y |x, c).
is the entropy of a context-agnosticwhere hqm tarefers tomachine translation model, and hqm tca context-aware machine translation model.
thisquantity can be estimated (see appendix a for amore formal derivation) over an held-out test setwith n sentence pairs and the respective contextas:.
cxmi(c → y |x) ≈.
−.
1n.n(cid:88).
i=1.
log.
qm ta(y(i)|x(i))qm tc (y(i)|x(i), c(i)).
while qm ta and qm tc can, in theory, be any mod-els, we are interested in removing any confoundingfactors other than the context that might lead toinstability in the estimates of the distributions.
forexample, if qm ta and qm tc use completely differ-ent models, it would not be clear if the difference inthe probability estimates is due to the introductionof context or due to other extraneous factors suchas differences in architectures, training regimens,or random seeds.
to address this we consider asingle model, qm t , that is able to translate withand without context (more on how this achieved in§3.2).
we can then set the context-agnostic modeland the contextual model to be the same modelqm ta = qm tc = qm t .
this way we attributethe information gain to the introduction of context.
throughout the rest of this work, when we refer-ence “context usage” we will precisely mean thisinformation gain (or loss)..3.2 experiments.
data we experiment with a document-level trans-lation task by training models on the iwslt2017(cettolo et al., 2012) dataset for language pairsen → de and en → fr (with approximately200k sentences for both pairs).
we use the test sets2011-2014 as validation sets and the 2015 as testsets.
to address the concerns pointed out by lopeset al.
(2020) that gains in performance are due to theuse of small training corpora and weak baselines,we use paracrawl (espl`a et al., 2019) and performsome data cleaning based on language identiﬁca-tion tools, creating a pretraining dataset of around.
646982m and 104m sentence pairs for en → de anden → fr respectively..all data is encoded/vectorized with byte-pairencoding (sennrich et al., 2016b) using the senten-cepiece framework (kudo and richardson, 2018).
for the non-pretrained case, we use 20k vocabu-lary size shared across source/target, while for thepretrained case we use a 32k vocabulary size..besides translation quality, we also evaluate ourmodels on two contrastive datasets for differentdiscourse phenomena to better assess the abilityof our models to capture context (more on this in§4.2):.
• for the en → de language pair, we evalu-ate on the contrapro dataset (m¨uller et al.,2018), targeting anaphoric pronoun resolu-tion.
source-side sentences contain the en-glish anaphoric pronoun it while target-sidesentences contain the corresponding germantranslations er, sie or es.
contrastive erro-neous translations are automatically createdby replacing the correct pronoun with one ofthe other two.
the test set contains 4,000examples for each target pronoun type andcontext is needed to correctly disambiguate.
context includes the four previous sentences.
• for the en → fr language pair, we evaluateon the dataset by bawden et al.
(2018) target-ing anaphoric pronoun resolution and lexicalcohesion.
it contains 200 manually curatedexamples for each phenomenon.
anaphoraexamples include singular and plural personaland possessive pronouns that require contextto be correctly inferred and the dataset is bal-anced such that a model that does not use con-text can only achieve 50% accuracy.
contextincludes the previous sentence.
models and optimization for all our experi-ments, we consider an encoder-decoder trans-former architecture (vaswani et al., 2017).
in par-ticular, we train the transformer small (hidden sizeof 512, feedforward size of 1024, 6 layers, 8 at-tention heads).
for the pretrained setup, we alsopre-train a transformer large architecture (hiddensize of 1024, feedforward size of 4096, 6 layers, 16attention heads) and subsequently ﬁne-tune on theiwsl2017 datasets..as in vaswani et al.
(2017), we train using theadam optimizer with β1 = 0.9 and β2 = 0.98 anduse an inverse square root learning rate scheduler,.
with an initial value of 10−4 and 5 × 10−4 forpretrained and non-pretrained cases respectively,and with a linear warm-up in the ﬁrst 4000 steps.
we train the models with early stopping on thevalidation perplexity..we train all our models on top of the fairseq.
framework (ott et al., 2019)..what context matters?
to assess the relative importance of differentcontext sizes on both the source and targetside, we start by considering two models, onefor the source-side context and one for thetarget-side context, that receive context of sizek, c(i) = {x(i−k), .
.
.
, x(i−1)} or c(i) ={y(i−k), .
.
.
, y(i−1)}.
during training, k is selectedrandomly to be in {1, .
.
.
, 4} for every example.
this way the model is trained to translate the samesource without and with different context sizes andis thus able to translate based on any context sizein that interval..figure 2 shows the cxmi values computed overthe test set as a function of the context size for boththe source-side and target-side contextual modelsfor both the non-pretrained and pretrained regimensfor the en → de language pair.
results for theen → fr language pair are similar and can befound in appendix b..for the non-pretrained case, for both the sourceand target context, the biggest jump in context us-age is when we increase the context size from 0to 1. after that, increasing the context size leadsto diminishing increases in context usage and evenreduced context usage for the source-side context.
interestingly, when the model is stronger, such asin the pretrained case, we can see that it can lever-age target-side context even better than the non-pretrained case, with a similar trend of diminishingincreases in context usage for both regimes.
how-ever, this is not the case for the source-side context,and it seems that the pretrained model is barelyable to use the contextual information on this side..overall, for this regime, we can conclude thathaving a context size of one or two previous sen-tences on both sides is beneﬁcial to the model, andthat target-side context is slightly more used thansource-side context.
this appears to corroboratethe ﬁndings of bawden et al.
(2018) that target-sidecontext is more effective than the source context..6470·10−2.
·10−2.
2.
1.
0.i.mxc.sourcetarget.
−1.
0.
2.
1.
0.sourcetarget.
1.
2context size.
3.
4.
−1.
0.
1.
2context size.
3.
4.figure 2: cxmi values for the en → de as a function of source and target context sizes for non-pretrained (left)and pretrained (right) models..context size.
(1).
4.1 context-aware word dropout.
4.increasing context usage.
rpb.
(2).
0.315---.
(3).
0.206---.
1234.
0.3650.3660.3670.366.table 2: point-biserial correlation coefﬁcients on thecontrastive datasets with pretrained models for differ-ent context sizes.
measured on contrapro (1) and baw-den et al.
(2018), both for pronoun resolution (2) andlexical cohesion (3).
bold values mean the correlationis statistically signiﬁcant with p < 0.01..does cxmi really measure context usage?
to assert that cxmi correlates with interpretablemeasures of context usage, we perform a corre-lation analysis with the performance in the con-trastive datasets mentioned.
in these datasets, us-age of context is evident where the model picks theright answer when it is passed the context and isnot able to do so when no context is given.
thustable 2 shows the point-biserial correlation coefﬁ-cient3 between the per-sample cxmi and binaryrandom variable and a binary variable that takesthe value 1 if the contextual model picks the cor-rect translation and the non-contextual model picksthe incorrect one, for different context sizes on thepretrained model.
we can see that there is a statis-tically signiﬁcant correlation between both values,which strengthens the notion that cxmi capturesprevious measures of context usage to some extent..3the point-biserial correlation coefﬁcient is a special caseof the pearson correlation coefﬁcient when one of the randomvariables is dichotomous..motivated by the above results demonstrating thelimited context usage of models trained using thestandard mle training paradigm, particularly withrespect to more distant context, we now ask thequestion: “is it possible to modify the trainingmethodology to increase context usage by themodel?” as an answer, we extend a popular reg-ularization technique used in sentence-level ma-chine translation, word dropout (sennrich et al.,2016a), to the context-aware setting.
the idea be-hind context-aware word (coword) dropout is tomodel the translation probability between x(i) andy(i) as.
pθ(y(i)|x(i)) =.
pθ(y(i)t.|˜x(i), y(i).
<t, c(i)),.
t(cid:89).
t=1.
where ˜x(i) is a perturbed version of the currentsource sentence generated by randomly droppingtokens and replacing them with a mask token givena dropout probability p:.
r(i)t ∼ bernoulli(p).
˜x(i)t =.
(cid:40).
(cid:104)mask(cid:105)x(i)t.if r(i)t = 1otherwise..in the case where no context is passed c(i) = ∅,coword dropout reduces to word dropout.
theintuition behind such a perturbation is that, by drop-ping information from the current source and notthe context, we increase the relative reliability ofcontext c(i), therefore providing the inductive bias.
6471·10−2.
i.mxc.2.
1.
0.p = 0.0p = 0.1p = 0.2p = 0.3.
4.
0.
1.
2context size.
3.figure 3: cxmi values as a function target context sizefor different values of coword dropout.
that context is important for the translation.
wewill see in §4.2 that this inductive bias is beneﬁ-cial and that coword dropout not only improvesperformance but also increases context usage..4.2 experiments.
setup as in §3.2, we consider transformer mod-els trained on the iwslt2017 for both en → deand en → fr, both from scratch and pretrainedusing the procedure previously described.
in par-ticular, due to ﬁndings in the previous section, weconsider models with either only target-side con-text or both source-side and target-side context..context usage to assess if our proposed regu-larization technique, coword dropout, increasescontext usage by models, we train a model usingthe same dynamic context size setting used in §3.2.
figure 3 plots the cxmi values on the test setas a function of the target context size as we in-crease the dropout value p. we see that increasingthis value consistently increases context usage ac-cording to cxmi across different context sizes.
note that, at test time, coword dropout is dis-abled, which means that it provides inductive biasonly during training and models learn to use morecontext by themselves..table 3 illustrates some examples where theincreased the per-samplecoword dropoutcxmi signiﬁcantly.
while the model only has ac-cess to target context, we present the source contextfor clarity.
in the ﬁrst example, while the sourceis a complete sentence, the target is only a frag-ment of one so the context helps complete it.
inthe other two examples shown, we can see that con-text helps disambiguate the gender of the german.
translation of the english pronoun it.
interestingly,the words that use context the most according tocxmi match very closely to the ones that nativespeakers annotated..translation quality to evaluate if the increasedusage of context correlates with better machinetranslation quality, based on the previous experi-ments on context usage and values for coworddropout, we consider three models trained withﬁxed-size context:.
• a baseline that has no context, reducing to.
sentence-level model ie: i.e., c(i) = ∅;.
• a one-to-two model having as context the pre-vious target sentence, i.e., c(i) = {y(i−1)};.
• a two-to-two model having as context the pre-vious source sentence and the previous targetsentence, i.e., c(i) = {x(i−1), y(i−1)}..in addition, to explore the beneﬁts of coworddropout in other architectures, we also train aone-to-two multi-encoder (jean et al., 2017) trans-former small model (more details in appendix §c).
for all models with target context, when decoding,we use the previous decoded sentences as targetcontext..table 4 shows the performance across three dif-ferent seeds of the baseline and contextual mod-els for both the non-pretrained and pretrained set-ting, with increasing values of coword dropoutp. we also run the baseline with coworddropout (which, as said previously, reduces to worddropout) to ensure that improvements were notonly due to regularization effects on the currentsource/target.
we report the standard bleu score(papineni et al., 2002) calculated using sacrebleu(post, 2018) and comet, a more accurate evalu-ation method using multilingual embeddings (reiet al., 2020)..for the non-pretrained case, we can see that acoword dropout value p > 0 consistently im-proves the performance of the contextual modelswhen compared to models running with p = 0and with the sentence-level baseline with the samevalues for word dropout.
for the pretrained case,the improvements are not as noticeable, althoughmodels trained with coword dropout still alwaysoutperform models trained without it.
this is per-haps a reﬂection of the general trend that bettermodels are harder to improve..6472source context.
source.
target context.
target.
∆cxmi.
more people watchedgames because it wasfaster.
the ball comes offtrack..i really think that thislie that we’ve beensold about disability isthe greatest injustice.
it was more entertain-ing.
youknowdon’twhere it’s going tolandit makes life hard forus.
mehr menschen sa-hen zu, die spielewurden schnellerder ballkontrolle.
ist außer.
meiner meinungnach ist diese luge¨uber behinderungeineschreiendeungerechtigkeit.
und unterhaltsamer..0.53.sie wissen nicht, woer landet..0.33.sie macht uns dasleben schwer..0.25.table 3: examples where models with coword dropout use the target context more than models trained withoutit.
word highlighted blue in the context are used to disambiguate translations while highlighted green in the targetuse context according to native speakers.
words underlined in the target are the ones with the highest per-wordcxmi i.e.
the ones that use the most context according to the model.
en → de.
en → fr.
w/ pretraining.
w/ pretraining.
bleu comet bleu comet bleu comet bleu comet.
p.0.00.10.2.
0.00.10.2.
0.00.10.2.
26.3627.2626.97.
26.6027.3627.33.
26.8527.7227.21.baseline.
1-to-2.
2-to-2.
0.0830.1590.163.
0.0870.1740.193.
0.0900.1690.177.
35.1035.1535.13.
35.2234.9234.75.
34.4734.5134.65.
0.5210.5250.524.
0.5280.5270.524.
0.4710.5220.525.
37.6238.1638.34.
37.5938.2538.27.
37.5438.3038.15.
0.4500.4720.474.
0.4500.4720.485.
0.4530.4670.468.
42.9843.2842.99.
42.8942.8842.90.
42.9742.9542.88.
0.6790.6790.678.
0.6720.6770.678.
0.6740.6760.675.table 4: results on iwslt2017 with different probabilities for coword dropout.
averaged across three runs foreach method..en → de.
en → fr.
bleu comet bleu comet.
26.3627.2626.97.
26.6427.4527.31.
0.0830.1590.163.
0.1040.1900.190.
37.6238.1638.34.
37.8537.9838.30.
0.4500.4720.474.
0.4660.4600.484.p.0.00.10.2.
0.00.10.2.baseline.
multi.
table 5: results on iwslt2017 for a multi-encoder1-to-2 model with different probabilities for coworddropout.
averaged across three runs for each method..table 5 shows that coword dropoutisalso helpful for the multi-encoder model, withcoword dropout helping signiﬁcantly.
thisshows that this method could be helpful for context-aware architectures other than concatenation-based..discourse phenomena while automatic metricssuch as bleu and comet allow us to measuretranslation quality, they mostly target sentence-level quality and do not speciﬁcally focus onphenomena that require context-awareness.
con-trastive datasets, as described in §3.2, allow us tomeasure the performance of context-aware modelsin speciﬁc discourse phenomena by comparing theprobability of correct translation against the con-trastive translations.
models that capture the tar-geted discourse phenomena well will consistentlyrank the correct translation higher than the con-trastive ones.
while there is a disconnect betweenthe translation (done via decoding) and contrastiveevaluation, it is currently the best way to measurea model’s performance on context-aware discoursephenomena..6473en → de.
w/ pretraining.
en → fr.
w/ pretraining.
p.pronouns.
pronouns.
pronouns cohesion pronouns cohesion.
baseline.
1-to-2.
2-to-2.
0.0.
0.00.10.2.
0.00.10.2.
42.96.
57.3658.7060.72.
61.0666.0065.47.
46.57.
76.7976.2877.52.
80.3380.3579.97.
50.00.
68.1672.3372.99.
72.1673.9973.99.
50.00.
49.9951.4952.16.
50.9952.4952.49.
50.00.
86.8386.4985.66.
85.6687.1688.49.
50.00.
56.8356.6656.49.
64.3365.9963.99.table 6: results on anaphoric pronoun resolution and lexical cohesion contrastive datasets with different proba-bilities for coword dropout.
averaged across three runs for each method..en → de.
en → fr.
5 related work.
baseline.
multi.
p.0.0.
0.00.10.2.pronouns.
pronouns cohesion.
42.96.
42.8547.2947.57.
50.00.
49.7451.7452.50.
50.00.
49.9950.2450.99.table 7: results on anaphoric pronoun resolutionand lexical cohesion contrastive datasets for the multi-encoder 1-to-2 model with different probabilities forcoword dropout.
averaged across three runs foreach method..table 6 shows the average performance over thecontrastive datasets of the baseline and contextualmodels for both the (non-)pretrained settings, withincreasing values of coword dropout p. we cansee that in general, increasing coword dropoutleads to improved performance, particularly forthe non-pretrained case.
this gain is particularlyclear for pronoun resolution and the en → delanguage pair.
we hypothesise that this is due to thesmall size of the contrastive sets for the en → frlanguage pair, which leads to high variance..table 7 similarly shows that coword dropoutimproves the performance of the multi-encodermodel across all phenomena, which again showsthat our proposed regularization method has ben-eﬁts for multiple architectures for context-awaremachine translation.
curiously, when these mod-els are trained without coword dropout, theyachieve performance similar to the sentence-levelbaseline, while when dropout is applied, they areable to effectively start using context..context-aware machine translation therehave been many works in the literature that tryto incorporate context into nmt systems.
tiede-mann and scherrer (2017) ﬁrst proposed the simpleapproach of concatenating the previous sentencesin both the source and target side to the input to thesystem; jean et al.
(2017), bawden et al.
(2018),and zhang et al.
(2018) used an additional context-speciﬁc encoder to extract contextual features fromthe previous sentences; maruf and haffari (2018)and tu et al.
(2018b) used cache-based memoriesto encode context; wang et al.
(2017) used a hier-archical rnn to encode the global context fromall previous sentences; miculicich et al.
(2018) andmaruf et al.
(2019a) used hierarchical attentionnetworks to encode context; chen et al.
(2020)added document-level discourse structure informa-tion to the input; sun et al.
(2020) trained a simpleconcatenation-based model with varying contextsize during training to have a model that is able totranslate with any context size, similar to what isdone in this work.
similarly to what we do withcoword dropout, jean and cho (2019) attemptedto maximise sensitivity to context by introducinga margin-based regularization term to explicitlyencourage context usage..for a more detailed overview, maruf et al.
(2019b) extensively describe the different ap-proaches and how they leverage context.
whilethese models lead to improvements with small train-ing sets, lopes et al.
(2020) showed that the im-provements are negligible when compared with theconcatenation baseline when using larger datasets..6474however, importantly, both our metric cxmi formeasuring context usage and the proposed regu-larization method of coword dropout, can theo-retically be applied to any of the above-mentionedmethods..evaluation in terms of evaluation, most previouswork focuses on targeting a system’s performanceon contrastive datasets for speciﬁc inter-sententialdiscourse phenomena.
m¨uller et al.
(2018) builta large-scale dataset for anaphoric pronoun res-olution, bawden et al.
(2018) manually createda dataset for both pronoun resolution and lexicalchoice and voita et al.
(2019) created a dataset thattargets deixis, ellipsis and lexical cohesion.
sto-janovski et al.
(2020) showed through adversarialattacks that models that do well on other contrastivedatasets rely on surface heuristics and create a con-trastive dataset to address this.
in contrast, ourcxmi metric is phenomenon-agnostic and can bemeasured with respect to all phenomena that re-quire context in translation..information-theoretic analysis bugliarelloet al.
(2020) ﬁrst proposed cross-mutual infor-mation (xmi) in the context of measuring thedifﬁculty of translating between languages.
ourwork differs in that we propose a conditionalversion of xmi, where s is always observed, andwe use it to assess the information gain of contextrather than the difﬁculty of translating differentlanguages..6.implications and future work.
we introduce a new, architecture-agnostic, metricto measure how context-aware machine translationmodels are using context and propose a simple reg-ularization technique to increase context usage bythese models.
our results are theoretically applica-ble to almost all recently proposed context-awaremodels and future work should go about measuringexactly how much these models leverage contextand if coword dropout also improves contextusage and performance in these..we also hope this work motivates exploring(c)xmi for other uses cases where measuring therelevance/usage of inputs to a particular modelother than context-aware machine translation.
itcould, for example, be used in conditional languagemodelling to analyse how the inputs we are condi-tioning on are being used by the model..7 acknowledgements.
we would like to thank all the members of deep-spin, neulab, and unbabel who provided feed-back on earlier versions of this work.
this workwas supported by the european research council(erc stg deepspin 758969), by the p2020 pro-grams maia and unbabel4eu (lisboa-01-0247-feder-045909 and lisboa-01-0247-feder-042671), and by the fundac¸ ˜ao para a ciˆencia e tec-nologia through contracts sfrh/bd/150706/2020and uidb/50008/2020..references.
rachel bawden, rico sennrich, alexandra birch, andbarry haddow.
2018. evaluating discourse phenom-ena in neural machine translation.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 1304–1313, new orleans, louisiana.
association for computational linguistics..emanuele bugliarello, sabrina j. mielke, anto-nios anastasopoulos, ryan cotterell, and naoakiokazaki.
2020. it’s easier to translate out of englishthan into it: measuring neural translation difﬁcultyby cross-mutual information.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1640–1649, online.
as-sociation for computational linguistics..mauro cettolo, christian girardi, and marcello fed-erico.
2012. wit3: web inventory of transcribedand translated talks.
in proceedings of the 16th an-nual conference of the european association for ma-chine translation, pages 261–268, trento, italy.
eu-ropean association for machine translation..junxuan chen, xiang li, jiarui zhang, chulun zhou,jianwei cui, bin wang, and jinsong su.
2020. mod-eling discourse structure for document-level neuralin proceedings of the firstmachine translation.
workshop on automatic simultaneous translation,pages 30–36, seattle, washington.
association forcomputational linguistics..miquel espl`a, mikel forcada, gema ram´ırez-s´anchez,and hieu hoang.
2019. paracrawl: web-scale paral-lel corpora for the languages of the eu.
in proceed-ings of machine translation summit xvii volume 2:translator, project and user tracks, pages 118–119,dublin, ireland.
european association for machinetranslation..hany hassan, anthony aue, chang chen, vishalchowdhary,jonathan clark, christian feder-mann, xuedong huang, marcin junczys-dowmunt,william lewis, mu li, shujie liu, tie-yan liu,renqian luo, arul menezes, tao qin, frank seide,xu tan, fei tian, lijun wu, shuangzhi wu, yingce.
6475xia, dongdong zhang, zhirui zhang, and mingzhou.
2018. achieving human parity on auto-matic chinese to english news translation.
corr,abs/1803.05567..s´ebastien jean and kyunghyun cho.
2019. context-aware learning for neural machine translation.
corr, abs/1903.04715..sebastien jean, stanislas lauly, orhan firat, andkyunghyun cho.
2017. does neural machine trans-lation beneﬁt from larger context?.
yunsu kim, duc thanh tran, and hermann ney.
2019.when and why is document-level context useful inneural machine translation?
in proceedings of thefourth workshop on discourse in machine trans-lation (discomt 2019), pages 24–34, hong kong,china.
association for computational linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..samuel l¨aubli, rico sennrich, and martin volk.
2018.has machine translation achieved human parity?
ain proceed-case for document-level evaluation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 4791–4796,brussels, belgium.
association for computationallinguistics..ant´onio lopes, m. amin farajian, rachel bawden,michael zhang, and andr´e f. t. martins.
2020.document-level neural mt: a systematic compari-son.
in proceedings of the 22nd annual conferenceof the european association for machine transla-tion, pages 225–234, lisboa, portugal.
european as-sociation for machine translation..sameen maruf and gholamreza haffari.
2018. docu-ment context neural machine translation with mem-in proceedings of the 56th annualory networks.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1275–1284, melbourne, australia.
association for compu-tational linguistics..sameen maruf, andr´e f. t. martins, and gholam-reza haffari.
2019a.
selective attention for context-aware neural machine translation.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 3092–3102, minneapolis,minnesota.
association for computational linguis-tics..sameen maruf, fahimeh saleh, and gholamreza haf-fari.
2019b.
a survey on document-level ma-chine translation: methods and evaluation.
arxiv,abs/1912.08494..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neu-ral machine translation with hierarchical attentionin proceedings of the 2018 conferencenetworks.
on empirical methods in natural language process-ing, pages 2947–2954, brussels, belgium.
associa-tion for computational linguistics..mathias m¨uller, annette rios, elena voita, and ricosennrich.
2018. a large-scale test set for the eval-uation of context-aware pronoun translation in neu-ral machine translation.
in proceedings of the thirdconference on machine translation: research pa-pers, pages 61–72, brussels, belgium.
associationfor computational linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..ricardo rei, craig stewart, ana c farinha, and alonlavie.
2020. comet: a neural framework for mtevaluation.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2685–2702, online.
associa-tion for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016a.
edinburgh neural machine translation sys-tems for wmt 16. in proceedings of the first con-ference on machine translation: volume 2, sharedtask papers, pages 371–376, berlin, germany.
as-sociation for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..dario stojanovski, benno krojer, denis peskov, andalexander fraser.
2020. contracat: contrastivecoreference analytical templates for machine transla-tion.
in proceedings of the 28th international con-ference on computational linguistics, pages 4732–4749, barcelona, spain (online).
international com-mittee on computational linguistics..6476zewei sun, mingxuan wang, hao zhou, chengqizhao, shujian huang, jiajun chen, and lei li.
2020.capturing longer context for document-level neuralmachine translation: a multi-resolutional approach.
arxiv, abs/2010.08961..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the third workshop on discourse in machinetranslation, pages 82–92, copenhagen, denmark.
association for computational linguistics..antonio toral, sheila castilho, ke hu, and andyway.
2018. attaining the unattainable?
reassessingclaims of human parity in neural machine translation.
in proceedings of the third conference on machinetranslation: research papers, pages 113–123, brus-sels, belgium.
association for computational lin-guistics..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018a.
learning to remember translation historywith a continuous cache.
transactions of the associ-ation for computational linguistics, 6:407–420..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018b.
learning to remember translation historywith a continuous cache.
transactions of the associ-ation for computational linguistics, 6:407–420..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..elena voita, rico sennrich, and ivan titov.
2019.when a good translation is wrong in context:context-aware machine translation improves ondeixis, ellipsis, and lexical cohesion.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1198–1212, flo-rence, italy.
association for computational linguis-tics..longyue wang, zhaopeng tu, andy way, and qun liu.
2017. exploiting cross-sentence context for neuralin proceedings of the 2017machine translation.
conference on empirical methods in natural lan-guage processing, pages 2826–2831, copenhagen,denmark.
association for computational linguis-tics..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 533–542, brussels, bel-gium.
association for computational linguistics..6477a estimating cxmi.
b cxmi for en → fr.
let s denote a random variable over source sen-tences, t a random variable over target sentencesand c a random variable over possible context.
we assume these random variables are distributedaccording to some true, unknown distributionp(s, t, c).
the cross-entropy between the true dis-tribution p and a probabilistic context-aware neuraltranslation model qm tc (t|s, c) is deﬁned as.
hqm t (t |s, c) =.
(cid:88).
(cid:88).
(cid:88).
−.
s∈v ∗s.t∈v ∗t.c∈v ∗c.p(s, t, c) log qm t (s, t, c).
s , v ∗.
t , v ∗.
where v ∗c represent the space of possi-ble source sentences, target sentences and con-texts respectively.
since we do not know thetrue distribution p, we cannot compute this quan-tity exactly.
however, given a dataset of samples{(s(i), t(i), c(i))}ni=0 assumed to be drawn from p,we can estimate this quantity using the monte carloestimator.
hqm tc.
(t |s, c) ≈.
−.
1n.n(cid:88).
i=0.
log qm tc (s(i), t(i), c(i)).
(cid:80).
c∈v ∗c.if we consider.
the marginal p(s, t) =p(s, t, c), we can by a similar argument ob-tain an estimate for the cross-entropy for a context-agnostic neural translation model qm ta as:.
hqm ta.
(t |s) ≈ −.
log qm ta(s(i), t(i)).
1n.n(cid:88).
i=0.
this leads trivially to the estimator for the cross-.
mutual information:.
cxmi(c → y |x) ≈.
−.
1n.n(cid:88).
i=1.
log.
qm ta(y(i)|x(i))qm tc (y(i)|x(i), c(i)).
·10−2.
2.
1.
0.
2.
1.
0.i.mxc.i.mxc.sourcetarget.
−1.
0·10−2.
1.
2.
3.
4.sourcetarget.
−1.
0.
1.
2context size.
3.
4.figure 4: cxmi values as a function of source andtarget context sizes for non-pretrained (top) and pre-trained (bottom) models for the en → fr languagepair..c multi-encoder.
for the multi-encoder model, we take the approachof initializing a separate transformer encoder forthe context, with shared input-output embeddingswith the original encoder (or decoder in the caseof target context).
the tokens in the current sen-tence attend to the context by the means of cross-attention.
there are several other ways of formu-lation a multi-encoder context-aware systems, andexploring them is left for future research..6478