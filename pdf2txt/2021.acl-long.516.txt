detecting propaganda techniques in memes.
dimitar dimitrov,1 bishr bin ali,2 shaden shaar,3 firoj alam,3fabrizio silvestri,4 hamed firooz,5 preslav nakov, 3 and giovanni da san martino61 soﬁa university “st.
kliment ohridski”, bulgaria, 2 king’s college london, uk,3 qatar computing research institute, hbku, qatar4 sapienza university of rome, italy, 5 facebook ai, usa, 6 university of padova, italymitko.bg.ss@gmail.com, bishrkc@gmail.com.
{sshaar, fialam, pnakov}@hbku.edu.qa, mhfirooz@fb.com.
fsilvestri@diag.uniroma1.it, dasan@math.unipd.it.
abstract.
propaganda can be deﬁned as a form of com-munication that aims to inﬂuence the opinionsor the actions of people towards a speciﬁcgoal; this is achieved by means of well-deﬁnedrhetorical and psychological devices.
propa-ganda, in the form we know it today, can bedated back to the beginning of the 17th cen-tury.
however, it is with the advent of the in-ternet and the social media that it has startedto spread on a much larger scale than before,thus becoming major societal and political is-sue.
nowadays, a large fraction of propagandain social media is multimodal, mixing textualwith visual content.
with this in mind, here wepropose a new multi-label multimodal task: de-tecting the type of propaganda techniques usedin memes.
we further create and release a newcorpus of 950 memes, carefully annotated with22 propaganda techniques, which can appearin the text, in the image, or in both.
our anal-ysis of the corpus shows that understandingboth modalities together is essential for detect-ing these techniques.
this is further conﬁrmedin our experiments with several state-of-the-artmultimodal models..1.introduction.
social media have become one of the main com-munication channels for information disseminationand consumption, and nowadays many people relyon them as their primary source of news (perrin,2015).
despite the many beneﬁts that social mediaoffer, sporadically they are also used as a tool, bybots or human operators, to manipulate and to mis-lead unsuspecting users.
propaganda is one suchcommunication tool to inﬂuence the opinions andthe actions of other people in order to achieve apredetermined goal (ipa, 1938)..warning: this paper contains meme examples and.
words that are offensive in nature..propaganda is not new.
it can be traced backto the beginning of the 17th century, as reportedin (margolin, 1979; casey, 1994; martino et al.,2020), where the manipulation was present at pub-lic events such as theaters, festivals, and duringgames.
in the current information ecosystem, ithas evolved to computational propaganda (wool-ley and howard, 2018; martino et al., 2020), whereinformation is distributed through technologicalmeans to social media platforms, which in turnmake it possible to reach well-targeted communi-ties at high velocity.
we believe that being awareand able to detect propaganda campaigns wouldcontribute to a healthier online environment..propaganda appears in various forms and hasbeen studied by different research communities.
there has been work on exploring network struc-ture, looking for malicious accounts and coordi-nated inauthentic behavior (cresci et al., 2017;yang et al., 2019; chetan et al., 2019; pachecoet al., 2020)..in the natural language processing community,propaganda has been studied at the documentlevel (barr´on-cedeno et al., 2019; rashkin et al.,2017), and at the sentence and the fragment lev-els (da san martino et al., 2019).
there havealso been notable datasets developed, including(i) tshp-17 (rashkin et al., 2017), which con-sists of document-level annotation labeled withfour classes (trusted, satire, hoax, and propaganda);(ii) qprop (barr´on-cedeno et al., 2019), which usesbinary labels (propaganda vs. non-propaganda),and (iii) ptc (da san martino et al., 2019), whichuses fragment-level annotation and an inventory of18 propaganda techniques.
while that work hasfocused on text, here we aim to detect propagandatechniques from a multimodal perspective.
this isa new research direction, even though large part ofpropagandistic social media content nowadays ismultimodal, e.g., in the form of memes..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6603–6617august1–6,2021.©2021associationforcomputationallinguistics6603figure 1: examples of memes from our dataset.
image sources: (a) 1, (a) 2; (b) 1, (b) 2; (c); (d) 1, (d) 2; (e) 1, (e)2; (f) 1, (f) 2; licenses: (a) 1, (c), (d) 2; (a) 2, (b) 2, (f) 1; (b) 1, (f) 2; (d) 1; (e) 1; (e) 2.memes are popular in social media as they canbe quickly understood with minimal effort (diresta,2018).
they can easily become viral, and thus itis important to detect malicious ones quickly, andalso to understand the nature of propaganda, whichcan help human moderators, but also journalists, byoffering them support for a higher level analysis.
figure 1 shows some examples of memes1 andpropaganda techniques.
example (a) applies trans-fer, using symbols (hammer and sickle) and colors(red), that are commonly associated with commu-nism, in relation to the two republicans shownin the image; it also uses name calling (traitors,moscow mitch, moscow’s bitch).
the meme in (b)uses both smears and glittering generalities.
theone in (c) expresses smears and suggest that joebiden’s campaign is only alive because of main-stream media.
the examples in the second rowshow some less common techniques.
example (d)uses appeal to authority to give credibility to astatement that rich politicians are crooks, and thereis also a thought-terminating clich´e used to dis-courage critical thought on the statement in theform of the phrase “we know”, thus implyingthat the clintons are crooks, which is also smears..then, example (e) uses both appeal to (strong)emotions and flag-waving as it tries to play on pa-triotic feelings.
finally, example (f) has reductionad hitlerum as ilhan omars’ actions are related tosuch of a terrorist (which is also smears; moreover,the word hate expresses loaded language)..the above examples illustrate that propagandatechniques express shortcuts in the argumentationprocess, e.g., by leveraging on the emotions of theaudience or by using logical fallacies to inﬂuenceit.
their presence does not necessarily imply thatthe meme is propagandistic.
thus, we do not an-notate whether a meme is propagandistic (just thepropaganda techniques it contains), as this wouldrequire, among other things, to determine its intent.
our contributions can be summarized as follows:.
• we formulate a new multimodal task: pro-paganda detection in memes, and we discusshow it relates and differs from previous work..• we develop a multi-modal annotation schema,and we create and release a new dataset forthe task, consisting of 950 memes, which wemanually annotate with 22 propaganda tech-niques.2.
1in order to avoid potential copyright issues, all memes weshow in this paper are our own recreation of existing memes,using images with clear licenses..2the corpus and the code used in our experiments areavailable at https://github.com/di-dimitrov/propaganda-techniques-in-memes..6604• we perform manual analysis, and we showthat both modalities (text and images) are im-portant for the task..• we experiment with several state-of-the-arttextual, visual, and multimodal models, whichfurther conﬁrm the importance of both modal-ities, as well as the need for further research..2 related work.
computationalpropaganda computationalpropaganda is deﬁned as the use of automaticapproaches to intentionally disseminate misleadinginformation over social media platforms (woolleyand howard, 2018).
the information that isdistributed over these channels can be textual,visual, or multi-modal.
of particular importanceare memes, which can be quite effective atspreading multimodal propaganda on social mediaplatforms (diresta, 2018).
the current informationecosystem and virality tools, such as bots, enablememes to spread easily, jumping from one targetgroup to another.
as of present, attempts tolimit the spread of such memes have focused onanalyzing social networks and looking for fakeaccounts and bots to reduce the spread of suchcontent (cresci et al., 2017; yang et al., 2019;chetan et al., 2019; pacheco et al., 2020)..textual content most research on propagandadetection has focused on analyzing textual con-tent (barr´on-cedeno et al., 2019; rashkin et al.,2017; da san martino et al., 2019; martino et al.,2020).
rashkin et al.
(2017) developed thetshp-17corpus, which uses document-level an-notation and is labeled with four classes: trusted,satire, hoax, and propaganda.
tshp-17 was de-veloped using distant supervision, i.e., all articlesfrom a given news outlet share the label of thatoutlet.
the articles were collected from the englishgigaword corpus and from seven other unreliablenews sources.
among them two were propagan-distic.
they trained a model using word n-gramrepresentation with logistic regression and reportedthat the model performed well only on articles fromsources that the system was trained on..barr´on-cedeno et al.
(2019) developed a newcorpus, qprop , with two labels: propagandavs. non-propaganda.
they also experimented ontshp-17 and qprop corpora, where for thetshp-17 corpus, they binarized the labels: pro-paganda vs. any of the other three categories..they performed massive experiments, investi-gated writing style and readability level, and trainedmodels using logistic regression and svms.
theirﬁndings conﬁrmed that using distant supervision,in conjunction with rich representations, might en-courage the model to predict the source of the ar-ticle, rather than to discriminate propaganda fromnon-propaganda.
similarly, habernal et al.
(2017,2018) developed a corpus with 1.3k arguments an-notated with ﬁve fallacies, including ad hominem,red herring, and irrelevant authority, which di-rectly relate to propaganda techniques..a more ﬁne-grained propaganda analysis wasdone by da san martino et al.
(2019).
they de-veloped a corpus of news articles annotated with18 propaganda techniques.
the annotation was atthe fragment level, and enabled two tasks: (i) bi-nary classiﬁcation —given a sentence in an article,predict whether any of the 18 techniques has beenused in it; (ii) multi-label multi-class classiﬁcationand span detection task —given a raw text, identifyboth the speciﬁc text fragments where a propa-ganda technique is being used as well as the type ofthe technique.
on top of this work, they proposeda multi-granular deep neural network that capturessignals from the sentence-level task and helps to im-prove the fragment-level classiﬁer.
subsequently, asystem was developed and made publicly available(da san martino et al., 2020)..multimodal content previous work has ex-plored the use of multimodal content for detect-ing misleading information (volkova et al., 2019),deception (glenski et al., 2019), emotions and pro-paganda (abd kadir et al., 2016), hateful memes(kiela et al., 2020; lippe et al., 2020; das et al.,2020), antisemitism (chandra et al., 2021) andpropaganda in images (seo, 2014).
volkova et al.
(2019) proposed models for detecting misleadinginformation using images and text.
they devel-oped a corpus of 500,000 twitter posts consistingof images labeled with six classes: disinformation,propaganda, hoaxes, conspiracies, clickbait, andsatire.
then, they modeled textual, visual, and lexi-cal characteristics of the text.
glenski et al.
(2019)explored multilingual multimodal content for de-ception detection.
they had two multi-class clas-siﬁcation tasks: (i) classifying social media postsinto four categories (propaganda, conspiracy, hoax,or clickbait), and (ii) classifying social media postsinto ﬁve categories (disinformation, propaganda,conspiracy, hoax, or clickbait)..6605multimodal hateful memes have been the targetof the popular “hateful memes challenge”, whichthe participants addressed using ﬁne-tuned state-of-art multi-modal transformer models such as vil-bert (lu et al., 2019), multimodal bitransform-ers (kiela et al., 2019), and visualbert (li et al.,2019) to classify hateful vs. not-hateful memes(kiela et al., 2020).
lippe et al.
(2020) exploreddifferent early-fusion multimodal approaches andproposed various methods that can improve theperformance of the detection systems..our work differs from the above research interms of annotation, as we have a rich inventoryof 22 ﬁne-grained propaganda techniques, whichwe annotate separately in the text and then jointlyin the text+image, thus enabling interesting analy-sis as well as systems for multi-modal propagandadetection with explainability capabilities..3 propaganda techniques.
propaganda comes in many forms and over timea number of techniques have emerged in the lit-erature (torok, 2015; miller, 1939; da san mar-tino et al., 2019; shah, 2005; abd kadir and sauf-ﬁyan, 2014; ipa, 1939; hobbs, 2015).
differ-ent authors have proposed inventories of propa-ganda techniques of various sizes: seven techniques(miller, 1939), 24 techniques weston (2018), 18techniques (da san martino et al., 2019), just smearas a technique (shah, 2005), and seven techniques(abd kadir and saufﬁyan, 2014).
we adapted thetechniques discussed in (da san martino et al.,2019), (shah, 2005) and (abd kadir and saufﬁyan,2014), thus ending up with 22 propaganda tech-niques.
among our 22 techniques, the ﬁrst 20 areused for both text and images, while the last twoappeal to (strong) emotions and transfer are re-served for labeling images only.
below, we providethe deﬁnitions of these techniques, which are in-cluded in the guidelines the annotators followed(see appendix a.2) for more detal..1. loaded language: using speciﬁc words andphrases with strong emotional implications (ei-ther positive or negative) to inﬂuence an audi-ence..2. name calling or labeling: labeling the objectof the propaganda campaign as something thatthe target audience fears, hates, ﬁnds undesir-able or loves, praises..3. doubt: questioning the credibility of someone.
or something..4. exaggeration / minimisation: either repre-senting something in an excessive manner: mak-ing things larger, better, worse (e.g., the best ofthe best, quality guaranteed) or making some-thing seem less important or smaller than it re-ally is (e.g., saying that an insult was actuallyjust a joke)..5. appeal to fear / prejudices: seeking to buildsupport for an idea by instilling anxiety and/orpanic in the population towards an alternative.
in some cases, the support is built based onpreconceived judgements..6. slogans: a brief and striking phrase that mayinclude labeling and stereotyping.
slogans tendto act as emotional appeals..7. whataboutism: a technique that attempts todiscredit an opponent’s position by chargingthem with hypocrisy without directly disprovingtheir argument..8. flag-waving: playing on strong national feel-ing (or to any group; e.g., race, gender, politicalpreference) to justify or promote an action or anidea..9. misrepresentation of.
someone’s position(straw man): substituting an opponent’sproposition with a similar one, which is thenrefuted in place of the original proposition..10. causal oversimpliﬁcation: assuming a singlecause or reason when there are actually multiplecauses for an issue.
this includes transferringblame to one person or group of people withoutinvestigating the complexities of the issue..11. appeal to authority: stating that a claim istrue simply because a valid authority or experton the issue said it was true, without any othersupporting evidence offered.
we also includehere the special case where the reference is notan authority or an expert, which is referred to astestimonial in the literature..12. thought-terminating clich´e: words orphrases that discourage critical thought andmeaningful discussion about a given topic.
they are typically short, generic sentences thatoffer seemingly simple answers to complexquestions or that distract the attention awayfrom other lines of thought..660613. black-and-white fallacy or dictatorship: pre-senting two alternative options as the only pos-sibilities, when in fact more possibilities exist.
as an the extreme case, tell the audience ex-actly what actions to take, eliminating any otherpossible choices (dictatorship)..14. reductio ad hitlerum: persuading an audienceto disapprove an action or an idea by suggestingthat the idea is popular with groups hated in con-tempt by the target audience.
it can refer to anyperson or concept with a negative connotation..15. repetition: repeating the same message overand over again, so that the audience will eventu-ally accept it..16. obfuscation, intentional vagueness, confu-sion: using words that are deliberately not clear,so that the audience may have their own interpre-tations.
for example, when an unclear phrasewith multiple possible meanings is used withinan argument and, therefore, it does not supportthe conclusion..17. presenting irrelevant data (red herring): in-troducing irrelevant material to the issue be-ing discussed, so that everyone’s attention isdiverted away from the points made..18. bandwagon attempting to persuade the targetaudience to join in and take the course of ac-tion because “everyone else is taking the sameaction.”.
19. smears: a smear is an effort to damage orcall into question someone’s reputation, by pro-pounding negative propaganda.
it can be appliedto individuals or groups..20. glittering generalities (virtue): these arewords or symbols in the value system of thetarget audience that produce a positive imagewhen attached to a person or an issue..21. appeal to (strong) emotions: using imageswith strong positive/negative emotional implica-tions to inﬂuence an audience..22. transfer: also known as association, this is atechnique that evokes an emotional response byprojecting positive or negative qualities (praiseor blame) of a person, entity, object, or valueonto another one in order to make the latter moreacceptable or to discredit it..4 dataset.
we collected memes from our own private face-book accounts, and we followed various facebookpublic groups on different topics such as vaccines,politics (from different parts of the political spec-trum), covid-19, gender equality, and more.
wewanted to make sure that we have a constant streamof memes in the newsfeed.
we extracted memes atdifferent time frames, i.e., once every few days fora period of three months.
we also collected someold memes for each group in order to make sure wecovered a larger variety of topics..4.1 annotation process.
we annotated the memes using the 22 propagandatechniques described in section 3 in a multilabelsetup.
the motivation for multilabel annotationis that the content in the memes often expressesmultiple techniques, even though such a settingadds complexity both in terms of annotation and ofclassiﬁcation.
we also chose to consider annotat-ing spans because the propaganda techniques canappear in the different chunk(s), which is also inline with recent research (da san martino et al.,2019).
we could not consider annotating the visualmodality independently because all memes containthe text as part of the image..the annotation team included six members, bothfemale and male, all ﬂuent in english, with qualiﬁ-cations ranging from undergrad, to msc and phddegrees, including experienced nlp researchers;this helped to ensure the quality of the annotation.
no incentives were provided to the annotators.
theannotation process required understanding the tex-tual and the visual content, which poses a greatchallenge for the annotator.
thus, we divided itinto ﬁve phases, as discussed below and as shownin figure 2. among these phases there were threestages, (i) pilot annotations to train the annotatorsto recognize the propaganda techniques, (ii) inde-pendent annotations by three annotators for eachmeme (phase 2 and 4), (iii) consolidation (phase3 and 5), where the annotators met with the otherthree team members, who acted as consolidators,and all six discussed every single example in detail(even those for which there was no disagreement).
we chose pybossa3 as our annotation platformas it provides the functionality to create a customannotation interface that can ﬁt our needs in eachphase of the annotation..3https://pybossa.com.
6607figure 2: examples of the annotation interface of each phase of the annotation process..4.1.1 phase 1: filtering and text editingphase 1 is about ﬁltering some of the memes ac-cording to our guidelines, e.g., low-quality memes,and such containing no propaganda technique.
weautomatically extracted the textual content usingocr, and then post-edited it to correct for poten-tial ocr errors.
we ﬁltered and edited the textmanually, whereas for extracting the text, we usedthe google vision api.4 we presented the originalmeme and the extracted text to an annotator, whohad to ﬁlter and to edit the text in phase 1 as shownin figure 2. for ﬁltering and editing, we deﬁned aset of rules, e.g., we removed hard to understand, orlow-quality images, cartoons, memes with no pic-ture, no text, or for which the textual content wasstrongly dominant and the visual content was min-imal and uninformative, e.g., a single-color back-ground.
more details about ﬁltering and editing aregiven in appendix a.1.1 and a.1.2..4.1.2 phase 2: text annotationin phase 2, we presented the edited textual con-tent of the meme to the annotators as shown infigure 2. we asked the annotators to identify thepropaganda techniques in the text and to select thecorresponding text spans for each of them..4.1.3 phase 3: text consolidationphase 3 is the consolidation step of the annotationsfrom phase 2 as shown in figure 2. this phasewas essential for ensuring the quality, and it furtherserved as an additional training opportunity for theentire team, which we found very useful..4http://cloud.google.com/vision.
4.1.4 phase 4: multimodal annotation.
step 4 is multimodal meme annotation, i.e., con-sidering both the textual and the visual content inthe meme.
in this phase, we show the meme, thepost-edited text, and the consolidated propagandalabels from phase 3 (text only) to the annotators,as shown in phase 4 from figure 2. we intention-ally provided the consolidated text labels to theannotators in this phase because we wanted themto focus on the techniques that require the presenceof the image rather than to reannotate those fromthe text.5.
4.1.5 phase 5: multimodal consolidation.
this is the consolidation phase for phase 4; thesetup is like for the consolidation at phase 3, asshown in figure 2..note that, in the majority of the cases, the mainreason why two annotations of the same mememight differ was due to one of the annotators notspotting some of the techniques, rather than be-cause there was a disagreement on what techniqueshould be chosen for a given textual span or whatthe exact boundaries of the span for a given tech-nique instance should be.
in the rare cases in whichthere was an actual disagreement and no clear con-clusion could be reached during the discussionphase, we resorted to discarding the meme (therewere ﬁve such cases in total)..5ideally, we would have wanted to have also a phase toannotate propaganda techniques when showing the imageonly; however, this is hard to do in practice as the text isembedded as part of the pixels in the image..66084.2 quality of the annotations.
we assessed the quality of the annotations for theindividual annotators from phases 2 and 4 (thus,combining the annotations for text and images) tothe ﬁnal consolidated labels at phase 5, followingthe setting in (da san martino et al., 2019).
sinceour annotation is multilabel, we computed krippen-dorff’s α, which supports multi-label agreementcomputation (artstein and poesio, 2008; passon-neau, 2006).
the results are shown in table 1 andindicate moderate to perfect agreement (landis andkoch, 1977)..agreement pair.
krippendorff’s α.
4.3 statistics.
annotator 1 vs. consolidatedannotator 2 vs. consolidatedannotator 3 vs. consolidated.
average.
0.830.910.56.
0.77.table 1: inter-annotator agreement..propaganda techniques.
text-only meme#len..#.
loaded languagename calling/labelingsmearsdoubtexaggeration/minimisationslogansappeal to fear/prejudicewhataboutismglittering generalities (virtue)flag-wavingrepetitioncausal oversimpliﬁcationthought-terminating clich´eblack-and-white fallacy/dictatorshipstraw manappeal to authorityreductio ad hitlerumobfuscation, int.
vagueness, confusionpresenting irrelevant databandwagontransferappeal to (strong) emotions.
7612.414082.6226617.118613.71856.69724.706010.125422.834514.07445.18421.953314.48284.072511.922415.962220.051312.6959.8515.458.4— —— —.
49234760211110070916711255143627264035237759590.total.
2,119 2,488.table 2: statistics about the propaganda techniques.
for each technique, we show the average length of itsspan (in number of words) as well as the number of in-stances of the technique as annotated in the text only vs.annotated in the entire meme..figure 3: propaganda techniques per meme..after the ﬁltering in phase 1 and the ﬁnal consol-idation, our dataset consists of 950 memes.
themaximum number of sentences per meme is 13,but most memes comprise only very few sentences,with an average of 1.68. the number of wordsranges between 1 and 73 words, with an averageof 17.79±11.60.
in our analysis, we observed thatsome propaganda techniques were more textual,e.g., loaded language and name calling, whileothers, such as transfer, tended to be more image-related..table 2 shows the number of instances of eachtechnique when using unimodal (text only, i.e., af-ter phase 3) vs. multimodal (text + image, i.e., af-ter phase 5) annotations.
note also that a totalof 36 memes had no propaganda technique anno-tated.
we can see that the most common tech-niques are smears, loaded language, and namecalling/labeling, covering 63%, 51%, and 36% ofthe examples, respectively.
these three techniquesalso form the most common pairs and triples inthe dataset as shown in table 3. we further showthe distribution of the number of propaganda tech-niques per meme in figure 3. we can see that mostmemes contain more than one technique, with amaximum of 8 and an average of 2.61..table 2 shows that the techniques can be foundboth in the textual and in the visual content of thememe, thus suggesting the use of multimodal learn-ing approaches to effectively exploit all informationavailable.
note also that different techniques havedifferent span lengths.
for example, loaded lan-guage is about two words long, e.g., violence, massshooter, and coward.
however, techniques suchas whataboutism need much longer spans with anaverage length of 22 words..660912345678# of distinct persuasion techniques in a meme050100150200250# of memes185261246137592123most common.
freq..loaded lang., name call./labeling, smearsname call./labeling, smears, transferloaded language, smears, transferappeal to emotions, loaded lang., smearsexagg./minim., loaded lang., smears.
loaded language, smearsname calling/labeling, smearsloaded language, name calling/labelingsmears, transferexaggeration/minimisation, smears.
17840333028.
3092562437563.triples.
pairs.
table 3: the ﬁve most common triples and pairs in ourcorpus and their frequency..5 experiments.
among the learning tasks that can be deﬁned on ourcorpus, here we focus on the following one: givena meme, ﬁnd all the propaganda techniques used init, both in the text and in the image, i.e., predict thetechniques as per phase 5..5.1 models.
we used two na¨ıve baselines.
first, a randombaseline, where we assign a technique uniformly atrandom.
second, a majority class baseline, whichalways predicts the most frequent class: smears..unimodal:text only.
for the text-based uni-modal experiments, we used bert (devlin et al.,2019), which is a state-of-the-art pre-trained trans-former, and fasttext (joulin et al., 2017), which cantolerate potentially noisy text from social media asit is trained on word and character n-grams..unimodal:image.
for the image-based uni-modal experiments, we used resnet152 (he et al.,2016), which was successfully applied in a relatedsetup (kiela et al., 2019)..multimodal: unimodally pretrained for themultimodal experiments, we trained separate mod-els on the text and on the image, bert and resnet-152, respectively, and then we combined them us-ing (a) early fusion multimodal bitransformers(mmbt) (kiela et al., 2019), (b) middle fusion(feature concatenation), and (c) late fusion (com-bining the predictions of the models).
for middlefusion, we took the output of the second-to-lastlayer of resnet-152 for the visual part and the out-put of the [cls] token from bert, and we fedthem into a multilayer network..multimodal: joint models.
we further experi-mented with models trained using a multimodalobjective.
in particular, we used vilbert (luet al., 2019), which is pretrained on conceptualcaptions (sharma et al., 2018), and visual bert(lin et al., 2014), which is pretrained on the ms-coco dataset (lin et al., 2014)..5.2 experimental settings.
we split the data into training, development, andtesting with 687 (72%), 63 (7%), and 200 (21%)examples, respectively.
since we are dealing witha multi-class multi-label task, where the labels areimbalanced, we chose micro-average f1 as ourmain evaluation measure, but we also report macro-average f1..we used the multimodal framework (mmf)(singh et al., 2020).
we trained all models on teslap100-pcie-16gb gpu with the following manu-ally tuned hyper-parameters (on dev): batch size of32, early stopping on the validation set optimizingfor f1-micro, sequence length of 128, adamw asan optimizer with learning rate of 5e-5, epsilon of1e-8, and weight decay of 0.01. all reported resultsare averaged over three runs with random seeds.
the average execution time for bert was 30 min-utes, and for the other models it was 55 minutes..6 experimental results.
table 4 shows the results for the models in sec-tion 5.1. rows 1 and 2 show a random and a major-ity class baseline, respectively.
rows 3-5 show theresults for the unimodal models.
while they all out-perform the baselines, we can see that the modelbased on visual modality only, i.e., resnet-152(row 3), performs worse than models based on textonly (rows 4-5).
this might indicate that identify-ing the techniques in the visual content is a hardertask than in texts.
moreover, bert signiﬁcantlyoutperforms fasttext, which is to be expected as itcan capture contextual representation better..rows 6-8 present results for multimodal fusionmodels.
the best one is bert + resnet-152 (+2points over fasttext + resnet-152).
we observethat early fusion models (rows 7-8) outperform latefusion ones (row 6).
this makes sense as late fusionis a simple mean of the results of each modality,while early fusion has a more complex architectureand trains a separate multi-layer perceptron for thevisual and for the textual features..6610type.
# model.
f1-micro.
f1-macro.
baseline.
unimodal.
1random2 majority class.
345.resnet-152fasttextbert.
multimodal.
bert + resnet-152 (late fusion)fasttext + resnet-152 (mid-fusion)bert + resnet-152 (mid-fusion).
6789 mmbt10 vilbert cc11 visualbert coco.
table 4: evaluation results..7.0629.04.
29.9233.5637.71.
30.7336.1238.1244.2346.7648.34.
5.153.12.
7.995.2515.62.
1.147.5210.568.318.9911.87.we can also see that both mid-fusion models(rows 7-8) improve over the corresponding text-only ones (rows 3-5).
finally, looking at the resultsin rows 9-11, we can see that each multimodalmodel consistently outperforms each of the uni-modal models (rows 1-8).
the best results areachieved with vilbert cc (row 10) and visual-bert coco (row 11), which use complex repre-sentations that combine the textual and the visualmodalities.
overall, we can conclude that multi-modal approaches are necessary to detect the useof propaganda techniques in memes, and that pre-trained transformer models seem to be the mostpromising approach..7 conclusion and future work.
we have proposed a new multi-class multi-labelmultimodal task: detecting the type of propagandatechniques used in memes.
we further created andreleased a corpus of 950 memes annotated with 22propaganda techniques, which can appear in thetext, in the image, or in both.
our analysis of thecorpus has shown that understanding both modal-ities is essential for detecting these techniques,which was further conﬁrmed in our experimentswith several state-of-the-art multimodal models..in future work, we plan to extend the datasetin size, including with memes in other languages.
we further plan to develop new multi-modal mod-els, speciﬁcally tailored to ﬁne-grained propagandadetection, aiming for deeper understanding of thesemantics of the meme and of the relation betweenthe text and the image.
a number of promisingideas have been already tried by the participantsin a shared task based on this data at semeval-2021 (dimitrov et al., 2021), which can serve as aninspiration when developing new models..ethics and broader impact.
user privacy our dataset only includes memesand it does not contain any user information..biases any biases found in the dataset are unin-tentional, and we do not intend to do harm to anygroup or individual.
we note that annotating pro-paganda techniques can be subjective, and thus itis inevitable that there would be biases in our gold-labeled data or in the label distribution.
we addressthese concerns by collecting examples from a va-riety of users and groups, and also by following awell-deﬁned schema, which has clear deﬁnitions.
our high inter-annotator agreement makes us con-ﬁdent that the assignment of the schema to the datais correct most of the time..misuse potential we ask researchers to be awarethat our dataset can be maliciously used to unfairlymoderate memes based on biases that may or maynot be related to demographics and other infor-mation within the text.
intervention with humanmoderation would be required in order to ensurethis does not occur..intended use we present our dataset to encour-age research in studying harmful memes on theweb.
we believe that it represents a useful resourcewhen used in the appropriate manner..acknowledgments.
this research is part of the tanbih mega-project,6which is developed at the qatar computing re-search institute, hbku, and aims to limit the im-pact of “fake news,” propaganda, and media biasby making users aware of what they are reading..6http://tanbih.qcri.org/.
6611references.
shamsiah abd kadir, anitawati lokman,.
t. tsuchiya.
2016.propaganda in youtube videos.
science and technology, vol (9):1–8..andemotion and techniques ofindian journal of.
shamsiah abd kadir and ahmad saufﬁyan.
2014. acontent analysis of propaganda in harakah newspa-journal of media and information warfare,per.
5:73–116..ron artstein and massimo poesio.
2008. inter-coderagreement for computational linguistics.
computa-tional linguistics, 34(4):555–596..alberto barr´on-cedeno,.
israa jaradat, giovannida san martino, and preslav nakov.
2019. proppy:organizing the news based on their propagandisticinformation processing & management,content.
56(5):1849–1864..ralph d. casey.
1994. what is propaganda?
histori-.
ans.org..mohit chandra, dheeraj pailla, himanshu bhatia,aadilmehdi sanchawala, manish gupta, manishshrivastava, and ponnurangam kumaraguru.
2021.
“subverting the jewtocracy”: online antisemitismdetection using multimodal deep learning.
arxivpreprint arxiv:2104.05947..aditya chetan, brihi joshi, hridoy sankar dutta, andtanmoy chakraborty.
2019. corerank: ranking todetect users involved in blackmarket-based collusiveretweeting activities.
in proceedings of the twelfthacm international conference on web search anddata mining, wsdm ’19, pages 330–338, mel-bourne, australia..stefano cresci, roberto di pietro, marinella petroc-chi, angelo spognardi, and maurizio tesconi.
2017.the paradigm-shift of social spambots: evidence,theories, and tools for the arms race.
in proceedingsof the 26th international conference on world wideweb companion, www’17, page 963–972, repub-lic and canton of geneva, che..giovanni da san martino, shaden shaar, yifan zhang,seunghak yu, alberto barr´on-cedeno, and preslavnakov.
2020. prta: a system to support the analysisof propaganda techniques in the news.
in proceed-ings of the annual meeting of association for com-putational linguistics, acl ’20, pages 287–293..giovanni da san martino, seunghak yu, albertobarr´on-cede˜no, rostislav petrov,and preslavnakov.
2019. fine-grained analysis of propagandathe 2019in news articles.
conference on empirical methods in natural lan-guage processing and 9th international joint con-ference on natural language processing, emnlp-ijcnlp’19, pages 5636–5646, hong kong, china..in proceedings of.
abhishek das, japsimar singh wahi, and siyao li.
2020. detecting hate speech in multi-modal memes.
arxiv preprint arxiv:2012.14891..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, naacl-hlt ’19, pages 4171–4186, min-neapolis, mn, usa..dimitar dimitrov, bishr bin ali, shaden shaar, firojalam, fabrizio silvestri, hamed firooz, preslavnakov, and giovanni da san martino.
2021.semeval-2021 task 6: detection of persuasion tech-in proceedings of theniques in texts and images.
international workshop on semantic evaluation, se-meval ’21..renee diresta.
2018. computational propaganda: ifyou make it trend, you make it true.
the yale review,106(4):12–29..maria glenski, e. ayton, j. mendoza, and svitlanavolkova.
2019. multilingual multimodal digital de-ception detection and disinformation spread acrosssocial platforms.
arxiv, abs/1909.05838..ivan habernal, raffael hannemann, christian pol-lak, christopher klamm, patrick pauli, and irynagurevych.
2017. argotario: computational argu-mentation meets serious games.
in proceedings ofthe 2017 conference on empirical methods in natu-ral language processing, emnlp ’17, pages 7–12,copenhagen, denmark..ivan habernal, patrick pauli, and iryna gurevych.
2018. adapting serious game for fallacious argu-mentation to german: pitfalls, insights, and bestin proceedings of the eleventh inter-practices.
national conference on language resources andevaluation, lrec ’18, pages 3329–3335, miyazaki,japan..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, cvpr ’16,pages 770–778, las vegas, nv, usa..renee hobbs.
2015. mind over media: analyzingcontemporary propaganda.
media education lab..ipa.
1938. in institute for propaganda analysis, editor,propaganda analysis.
volume i of the publicationsof the institute for propaganda analysis.
new york,ny, usa..ipa.
1939. in institute for propaganda analysis, editor,propaganda analysis.
volume ii of the publicationsof the institute for propaganda analysis.
new york,ny, usa..armand joulin, edouard grave, piotr bojanowski, andtomas mikolov.
2017. bag of tricks for efﬁcient textclassiﬁcation.
in proceeding of the 15th conferenceof the european chapter of the association for com-putational linguistics, eacl ’17, pages 427–431,valencia, spain..6612douwe kiela, suvrat bhooshan, hamed firooz, ethanperez, and davide testuggine.
2019. supervisedmultimodal bitransformers for classifying imagesin proceedings of the neuips workshopand text.
on visually grounded interaction and language,vigil@neurips ’19, vancouver, canada..douwe kiela, hamed firooz, aravind mohan, vedanujgoswami, amanpreet singh, pratik ringshia, anddavide testuggine.
2020. the hateful memes chal-lenge: detecting hate speech in multimodal memes.
in proceedings of the annual conference on neuralinformation processing systems, neurips ’20, van-couver, canada..j richard landis and gary g koch.
1977. the mea-surement of observer agreement for categorical data.
biometrics, pages 159–174..liunian harold li, mark yatskar, da yin, cho-juihsieh, and kai-wei chang.
2019. visualbert: asimple and performant baseline for vision and lan-guage.
arxiv preprint arxiv:1908.03557..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, eccv ’14, pages 740–755, zurich, switzerland..phillip lippe, nithin holla, shantanu chandra, san-thosh rajamanickam, georgios antoniou, ekaterinashutova, and helen yannakoudakis.
2020. a multi-modal framework for the detection of hateful memes.
arxiv preprint arxiv:2012.12871..jiasen lu, dhruv batra, devi parikh, and stefan lee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagein proceedings of the conference on neu-tasks.
ral information processing systems, neurips ’19,pages 13–23, vancouver, canada..v. margolin.
1979. the visual rhetoric of propaganda..information design journal, 1:107–122..giovanni da san martino, stefano cresci, albertobarr´on-cede˜no, seunghak yu, roberto di pietro,and preslav nakov.
2020. a survey on computa-tional propaganda detection.
in proceedings of thetwenty-ninth international joint conference on ar-tiﬁcial intelligence, ijcai ’20, pages 4826–4832..clyde r. miller.
1939. the techniques of propaganda.
from “how to detect and analyze propaganda,” anaddress given at town hall.
the center for learning..diogo pacheco, alessandro flammini, and filippomenczer.
2020. unveiling coordinated groups be-hind white helmets disinformation.
in proceedingsof the web conference 2020, www ’20, pages 611–616, new york, usa..rebecca passonneau.
2006. measuring agreement onset-valued items (masi) for semantic and pragmaticannotation.
in proceedings of the fifth internationalconference on language resources and evaluation,lrec ’06, pages 831–836, genoa, italy..andrew perrin.
2015. social media usage.
pew re-.
search center, pages 52–68..hannah rashkin, eunsol choi, jin yea jang, svitlanavolkova, and yejin choi.
2017. truth of varyingshades: analyzing language in fake news and politi-cal fact-checking.
in proceedings of the conferenceon empirical methods in natural language process-ing, emnlp ’17, pages 2931–2937, copenhagen,denmark..hyunjin seo.
2014. visual propaganda in the age ofsocial media: an empirical analysis of twitter im-ages during the 2012 israeli–hamas conﬂict.
visualcommunication quarterly, 21(3):150–161..anup shah.
2005. war, propaganda and the media..global issues..piyush sharma, nan ding, sebastian goodman, andradu soricut.
2018.conceptual captions: acleaned, hypernymed, image alt-text dataset for au-in proceedings of thetomatic image captioning.
56th annual meeting of the association for com-putational linguistics, acl ’18, pages 2556–2565,melbourne, australia..amanpreet singh, vedanuj goswami, vivek natara-jan, yu jiang, xinlei chen, meet shah, marcusrohrbach, dhruv batra, and devi parikh.
2020.mmf: a multimodal framework for vision and lan-guage research..robyn torok.
2015. symbiotic radicalisation strate-gies: propaganda tools and neuro linguistic program-ming.
in australian security and intelligence con-ference, asic ’15, pages 58–65, canberra, aus-tralia..svitlana volkova, ellyn ayton, dustin l. arendt,zhuanyi huang, and brian hutchinson.
2019. ex-plaining multimodal deceptive news prediction mod-els.
in proceedings of the international aaai con-ference on web and social media, icwsm ’19,pages 659–662..anthony weston.
2018. a rulebook for arguments..hackett publishing..samuel c woolley and philip n howard.
2018. com-putational propaganda: political parties, politicians,and political manipulation on social media.
oxforduniversity press..kai-cheng yang, onur varol, clayton a davis, emilioferrara, alessandro flammini, and filippo menczer.
2019. arming the public with artiﬁcial intelligenceto counter social bots.
human behavior and emerg-ing technologies, 1(1):48–61..6613appendix.
a annotation instructions.
a.1 guidelines for annotators - phases 1.the annotators were presented with the followingguidelines during phase 1 for ﬁltering and editingthe text of the memes..a.1.1 choice of memes/filtering criteria.
in order to ensure consistency for our data, we de-ﬁned meme as a photograph-style image with ashort text on top.
we asked the annotators to ex-clude memes with the below characteristics.
dur-ing this phase, we ﬁltered out 111 memes..• images with diagrams/graphs/tables..• memes for which no multimodal analysis ispossible: e.g., only text, only image, etc..• cartoons..a.1.2 rules for text editingwe used the google vision api7 to extract thetext from the memes.
as the output of the systemsometimes contains errors, a manual checking wasneeded.
thus, we deﬁned several text editing rulesas listed below, and we applied them to the textualcontent extracted from each meme..1. when the meme is a screenshot of a socialnetwork account, e.g., whatsapp, the username and login can be removed as well as alllike, comment, and share elements..2. remove the text related to logos that are not.
part of the main text..3. remove all text related to ﬁgures and tables..4. remove all text that is partially hidden by animage, so that the sentence is almost impossi-ble to read..5. remove text that is not from the meme, but onbanners and billboards carried on by demon-strators, street advertisements, etc..6. remove the author of the meme if it is signed..7. if the text is in columns, ﬁrst put all text fromthe ﬁrst column, then all text from the nextcolumn, etc..8. rearrange the text, so that there is one sen-.
tence per line, whenever possible..7http://cloud.google.com/vision.
9. if there are separate blocks of text in differentlocations of the image, separate them by ablank line.
however, if it is evident that textblocks are part of a single sentence, keep themtogether..a.2 guidelines for annotators - phases 2-5.the annotators were presented with the followingguidelines.
in these phases, the annotations wereperformed by three annotators..a.2.1 annotation phase 2given the list of propaganda techniques for the text-only annotation task, as described in section a.3(techniques 1-20), and the textual content of ameme, the task is to identify which techniques ap-pear in the text and the exact span for each of them..a.2.2 annotation phase 4in this phase, the task was to identify which of the22 techniques, described in section a.3, appear inthe meme, i.e., both in the text and in the visualcontent.
note that some of the techniques occurringin the text might be identiﬁed only in this phasebecause the image provides a necessary context..a.2.3 consolidation (phase 3 and 5)in this phase, the three annotators met together withother consolidators and discussed each annotation,so that a consensus on each of them is reached.
these phases are devoted to checking existing an-notations.
however, when a novel instance of atechnique is observed during the consolidation, itis added..a.3 deﬁnitions of propaganda techniques.
1. presenting irrelevant data (red herring) in-troducing irrelevant material to the issue beingdiscussed, so that everyone’s attention is divertedaway from the points made..example 1: in politics, defending one’s own poli-cies regarding public safety – “i have worked hardto help eliminate criminal activity.
what we needis economic growth that can only come from thehands of leadership.”example 2: “you may claim that the death penaltyis an ineffective deterrent against crime – but whatabout the victims of crime?
how do you think sur-viving family members feel when they see the manwho murdered their son kept in prison at their ex-pense?
is it right that they should pay for theirson’s murderer to be fed and housed?”.
6614figure 4: memes from our dataset.
image sources: (a) 1, (a) 2; (b) 1, (b) 2, (b) 3, (b) 4, (b) 5, (b) 6; (c) 1, (c) 2;(d) 1, (d) 2; (e); (f); licenses: (b) 6, (c) 1, (f); (a) 2,(b) 1, (b) 5, (c) 2, (d) 1; (a) 1; (b) 2, (b) 3, (b) 4, (c) 2; (e).
2. misrepresentation of someone’s position(straw man) when an opponent’s proposition issubstituted with a similar one, which is then refutedin place of the original proposition..example:zebedee: what is your view on the christian god?
mike: i don’t believe in any gods, including thechristian one.
zebedee: so you think that we are here by accident,and all this design in nature is pure chance, andthe universe just created itself?
mike: you got all that from me stating that i justdon’t believe in any gods?.
3. whataboutism a technique that attempts todiscredit an opponent’s position by charging themwith hypocrisy without directly disproving theirargument.
example 1: a nation deﬂects criticism of its recenthuman rights violations by pointing to the historyof slavery in the united states.
example 2:“qatar spending profusely on neymar,not ﬁghting terrorism”.
4. causal oversimpliﬁcation assuming a singlecause or reason when there are actually multiplecauses for an issue.
it includes transferring blameto one person or group of people without investi-gating the complexities of the issue.
an example isshown in figure 4(b)..example 1: “president trump has been in ofﬁcefor a month and gas prices have been skyrocketing.
the rise in gas prices is because of him.”example 2: the reason new orleans was hit sohard with the hurricane was because of all theimmoral people who live there..5. obfuscation, intentional vagueness, confu-sion using words which are deliberately not clearso that the audience may have their own interpreta-tions.
for example, when an unclear phrase withmultiple deﬁnitions is used within the argumentand, therefore, it does not support the conclusion.
example: it is a good idea to listen to victims oftheft.
therefore if the victims say to have the thiefshot, then you should do that..6. appeal to authority stating that a claim istrue simply because a valid authority or expert onthe issue said it was true, without any other sup-porting evidence offered.
we consider the specialcase in which the reference is not an authority oran expert in this technique, although it is referredto as testimonial in literature.
example 1: richard dawkins, an evolutionary bi-ologist and perhaps the foremost expert in the ﬁeld,says that evolution is true.
therefore, it’s true.
example 2: “according to serena williams, ourforeign policy is the best on earth.
so we are in theright direction.”.
66157. black-and-white fallacy presenting two al-ternative options as the only possibilities, whenin fact more possibilities exist.
we include dicta-torship, which happens when we leave only onepossible option, i.e., when we tell the audience ex-actly what actions to take, eliminating any otherpossible choices.
an example of this technique isshown in figure 4(c).
example 1: you must be a republican or democrat.
you are not a democrat.
therefore, you must be arepublican.
example 2: i thought you were a good person, butyou weren’t at church today..8. name calling or labeling labeling the ob-ject of the propaganda campaign as either some-thing the target audience fears, hates, ﬁnds undesir-able or loves, praises.
examples: republican congressweasels, bush thelesser.
note that here lesser does not refer to thesecond, but it is pejorative..9. loaded language using speciﬁc words andphrases with strong emotional implications (eitherpositive or negative) to inﬂuence an audience.
example 1: “[...] a lone lawmaker’s childish shout-ing.”example 2: “how stupid and petty things have be-come in washington.”.
10. exaggeration or minimisation either rep-resenting something in an excessive manner: mak-ing things larger, better, worse (e.g., the best ofthe best, quality guaranteed) or making somethingseem less important or smaller than it really is(e.g., saying that an insult was just a joke).
anexample meme is shown in figure 4(a).
example 1: “democrats bolted as soon as trump’sspeech ended in an apparent effort to signal theycan’t even stomach being in the same room as thepresident.”example 2: “we’re going to have unbelievable in-telligence.”.
11. flag-waving playing on strong national feel-ing (or to any group, e.g., race, gender, politicalpreference) to justify or promote an action or idea.
example 1: “patriotism mean no questions” (thisis also a slogan)example 2: “entering this war will make us havea better future in our country.”.
12. doubt questioning the credibility of some-one or something.
example: a candidate talks about his opponentand says: “is he ready to be the mayor?”.
13. appeal to fear/prejudice seeking to buildsupport for an idea by instilling anxiety and/orpanic in the population towards an alternative.
insome cases the support is built based on precon-ceived judgements.
an example is shown in fig-ure 4(c).
example 1: “wither we go to war or we will perish.”note that, this is also a black and white fallacy.
example 2: “we must stop those refugees as theyare terrorists.”.
14. slogans a brief and striking phrase that mayinclude labeling and stereotyping.
slogans tend toact as emotional appeals.
example 1: “the more women at war.
.
.
the soonerwe win.”example 2: “make america great again!”.
thought-terminating clich´e words or15.phrases that discourage critical thought and mean-ingful discussion about a given topic.
they aretypically short, generic sentences that offer seem-ingly simple answers to complex questions or thatdistract attention away from other lines of thought.
examples: it is what it is; it’s just common sense;you gotta do what you gotta do; nothing is perma-nent except change; better late than never; mindyour own business; nobody’s perfect; it doesn’tmatter; you can’t change human nature..16. bandwagon attempting to persuade the tar-get audience to join in and take the course of actionbecause “everyone else is taking the same action”.
example 1: would you vote for clinton as presi-dent?
57% say “yes.”example 2: 90% of citizens support our initiative.
you should..17. reductio ad hitlerum persuading an audi-ence to disapprove an action or idea by suggestingthat the idea is popular with groups hated in con-tempt by the target audience.
it can refer to anyperson or concept with a negative connotation.
anexamples is shown in figure 4(d).
example 1: “do you know who else was doingthat?
hitler!”example 2: “only one kind of person can think inthat way: a communist.”.
661618. repetition repeating the same message overand over again so that the audience will eventuallyaccept it..we further give statistics about the number ofparameters for each model, so that one can get anidea about their complexity:.
• resnet-152: 60,300,000.
• fasttext: 6,020.
• bert (bert-base-uncased): 110,683,414.
• fasttext + resnet-152 (early fusion):.
11,194,398.
170,983,752.
• bert + resnet-152.
(late.
fusion):.
• mmbt: 110,683,414.
• vilbert cc: 112,044,290.
• visualbert coco: 247,782,404.
19. smears a smear is an effort to damage orcall into question someone’s reputation, by pro-pounding negative propaganda.
it can be applied toindividuals or groups.
an example meme is shownin figure 4(a)..20. glittering generalities these are words orsymbols in the value system of the target audiencethat produce a positive image when attached toa person or issue.
peace, hope, happiness, secu-rity, wise leadership, freedom, “the truth”, etc.
are virtue words.
virtue can be also expressed inimages, where a person or an object is depictedpositively.
in figure 4(f), we provide an exampleto depict such a scenario..21. transfer also known as association, thisis a technique of projecting positive or negativequalities (praise or blame) of a person, entity, ob-ject, or value onto another to make the secondmore acceptable or to discredit it.
it evokes anemotional response, which stimulates the target toidentify with recognized authorities.
often highlyvisual, this technique often uses symbols (e.g., theswastikas used in nazi germany, originally a sym-bol for health and prosperity) superimposed overother visual images..22. appeal to (strong) emotions using imageswith strong positive/negative emotional implica-tions to inﬂuence an audience.
figure 4(f) showsan example..b hyper-parameter values.
in this section, we list the values of the hyper-parameters we used when training our models..• batch size: 32.
• optimizer: adamw.
– learning rate: 5e-5– epsilon: 1e-8– weight decay: 0.01.
• max sequence length: 128.
• number of epochs: 37.
• early stopping: f1-micro on dev set.
6617