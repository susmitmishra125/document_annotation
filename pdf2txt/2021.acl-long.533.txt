the statistical advantage of automatic nlg metrics at the system level.
johnny tian-zheng wei.
robin jia.
department of computer science, university of southern california{jtwei, robinjia}@usc.edu.
abstract.
estimating the expected output quality of gen-eration systems is central to nlg.
this paperqualiﬁes the notion that automatic metrics arenot as good as humans in estimating system-level quality.
statistically, humans are unbi-ased, high variance estimators, while metricsare biased, low variance estimators.
we com-pare these estimators by their error in pairwiseprediction (which generation system is better?)
using the bootstrap.
measuring this error iscomplicated: predictions are evaluated againstnoisy, human predicted labels instead of theground truth, and metric predictions ﬂuctuatebased on the test sets they were calculated on.
by applying a bias-variance-noise decomposi-tion, we adjust this error to a noise-free, in-ﬁnite test set setting.
our analysis comparesthe adjusted error of metrics to humans and aderived, perfect segment-level annotator, bothof which are unbiased estimators dependent onthe number of judgments collected.
in mt, weidentify two settings where metrics outperformhumans due to a statistical advantage in vari-ance: when the number of human judgmentsused is small, and when the quality differencebetween compared systems is small.1.
1.introduction.
automatic metrics are involved in many develop-mental settings for natural language generation(nlg) systems.
in machine translation (mt), met-rics like bleu (papineni et al., 2002) enable set-tings where the amount of human effort requiredwould be infeasible, such as architecture or hyper-parameter search (britz et al., 2017).
as objec-tive, reproducible quantities, bleu scores facil-itate cross-paper comparisons (post, 2018).
his-torically, progress in mt has been attributed to its.
1the data and code to reproduce our analysesare available at https://github.com/johntzwei/metric-statistical-advantage..dist.
of estimator(cid:91)δms,s(cid:48) )(cid:91)δhs,s(cid:48) ).
human (.
metric (.
bias.
0.δhs,s(cid:48) (true difference).
figure 1: distribution of estimators for the true differ-ence in system quality δhs,s(cid:48) between two generationsystems (for illustrative purposes).
notation is deﬁnedin §2.3.
an estimate incurs prediction error if its signis opposite to the true difference.
while humans pro-vide an unbiased estimator of the difference, a biasedestimator derived from a metric can have a smaller er-ror probability (shaded areas) due to its lower variance.
evidence supporting the illustration can be found in §5..use (callison-burch et al., 2006).
metrics are anactive research area in many nlg subﬁelds, in-cluding summarization (lin, 2004), dialogue (taoet al., 2018), and image captioning (anderson et al.,2016), which seek to realize the goal of quick andreliable automatic evaluation..in all these subﬁelds, the primary goal when con-ducting evaluation is typically to compare nlgsystems.
both human annotators and automaticmetrics produce segment-level scores, i.e., scoresfor individual examples, so comparing systems re-quires aggregating segment-level scores into anoverall system-level score for each system.
ide-ally, we would compare systems by their expectedhuman annotator score (an average over inﬁnitehuman judgments), which we term the true systemquality.
in practice, we can only estimate this ex-pectation with a sample mean over a ﬁnite numberof human judgments.
metrics offer a cheaper alter-native: we can instead compare systems by theiraggregate metric scores on a number of system out-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6840–6854august1–6,2021.©2021associationforcomputationallinguistics6840puts.
when comparing systems, we care primarilyabout how well we estimate the difference of theirtrue system qualities, and in particular the sign ofthis difference (i.e., which system is better), whichwe term the true pairwise label..there is a gap in our understanding of system-level metrics.
to recount a perplexing anecdote, inthe most recent edition of the wmt metrics sharedtask (mathur et al., 2020b), initial human evalu-ation disagreed with most metrics on a pairwiseprediction of two translation systems.
in a manualre-evaluation, the second round results favored themetrics.
our paper offers a statistical explanationfor how humans could go “wrong”: even if humanestimation for the difference in system quality isunbiased, it has high variance.
on the other hand,while estimators based on metrics are biased, theyhave low variance.
it is therefore possible for met-rics to give a more accurate pairwise predictionthan humans when the bias is small (see illustrationin figure 1).
our paper explores this distinction inthe following three questions:.
(1) how can we evaluate system-level metrics?
when observing estimator error in terms of pair-wise predictions, predictions are evaluated againstnoisy, human predicted labels rather than theground truth.
in addition, metric predictions ﬂuc-tuate based on the sample of outputs from the gen-eration system.
to disentangle these properties,we examine observed estimator error under a bias-variance-noise decomposition.
under simulation,we ﬁnd that the label noise and metric varianceaccount for a small fraction of observed errorin both mt and summarization..(2) how good are these metrics?
we comparethe errors of metric estimators computed on an inﬁ-nite number of system outputs, against human esti-mators with varying amounts of human judgment.
we also derive the error of a perfect segment-levelannotator (i.e.
they provide noiseless/expected hu-man scores for each output), which is also unbiasedand judgment dependent.
empirically, some mtmetrics exceed the performance of unbiased es-timators with a small number of judgments..(3) what are the limits of system-level evalua-tion?
the perfect segment-level annotator, as thenoiseless human, provides an optimistic estimatefor the number of human judgments necessary toachieve a ﬁxed performance.
with a power anal-ysis, we can analytically calculate the number ofjudgments necessary to detect differences between.
systems of varying sizes.
when differences insystem quality are small, a prohibitively largenumber of perfect annotator judgments are re-quired to give a correct pairwise prediction..2 formalization.
2.1 system-level scores.
we will now formalize scoring at the system level,adopting notation from chaganty et al.
(2018).
letx be a distribution over inputs (e.g.
source sen-tences), and s be a set of systems (e.g.
all transla-tion systems in wmt).
each system s ∈ s takesinput x ∼ x and returns output z = s(x) (e.g.
zis a translation).
let h(z) be a random variablerepresenting a human judgment according to someevaluation prompt (e.g.
translation adequacy, from0-100).
a central quantity of interest is the qualityof system s, deﬁned as.
(1).
µhs = ex∼x [h(s(x))]and is not directly observable as it requires inﬁnitehuman judgment.
we can estimate (1) with a ﬁnitetest set of n examples.
let x(1), .
.
.
, x(n) i.i.d.∼ xbe a sampled test set and z(1), .
.
.
, z(n) be the setof outputs where each z(i) = s(x(i)).
humanjudgments are sampled independently as y(i) ∼h(z(i)).
the sample mean.
(cid:99)µhs =.
1n.n(cid:88).
i=1.
y(i).
(2).
is an unbiased estimator of (1).
only (2) is observ-able, which is a noisy approximation of (1)..a cheaper alternative to estimating the true qual-ity scores is with an estimator based on an auto-matic metric.
let m (e.g.
bertscore) be anautomatic metric that takes as input any number ofoutputs from a system s and produces score.
(3).
s = m (z(1), .
.
.
, z(n))(cid:100)µms is a biased estimator of µh.
where (cid:100)µms .
as the testset is sampled, the metric score has non-zero vari-ance.
note that while we use the greek letter µ, onlysome system-level metrics (e.g.
rouge) are aver-ages of their segment-level counterparts (their scores = 1i=1 m (z(i))).
empir-decomposes to (cid:100)µmnically, we ﬁnd that metrics using other aggrega-tion strategies have convergent properties similarto an average (see appendix b).
we sidestep thisby deﬁning the “true” metric score ass = m (z(1), .
.
.
, z(m))µm.
(cid:80)n.(4).
6841for test sets of size m sufﬁciently large so that thistrue score is nearly constant..and the observed difference as.
2.2 problems in evaluating with correlation.
research in system-level metrics have a traditionof evaluating metric correlation to human judg-ment with the pearson correlation coefﬁcient (re-iter, 2018).
formally, these evaluations compare(cid:99)rm = corrs( (cid:99)µh.
s ) for different metrics m ..s , (cid:100)µm.
recently, mathur et al.
(2020a) highlights twoissues with the use of correlation: first, pearson’sr is neither interpretable nor reﬂective of system-level metric use in practice.
second, outlier sys-tems (systems with very high/low human/metricscores) can arbitrarily inﬂate pearson’s r, and out-lier systems often exist.
mathur et al.
(2020a) pro-pose evaluating metric accuracy in pairwise predic-tion (can the metric differentiate which generationsystem is better?)
as an alternative that mitigatesthe issues mentioned above..s , µh.
we add two points that apply to any measure ofmetric performance, correlation or pairwise predic-tions: first, metrics cannot be perfect due to noisein human labels.
for instance, while r ranges from[−1, 1], even for the metric that predicts µhs it hascorrs( (cid:99)µhs ) < 1 due to noise in (cid:99)µhs .
it is un-clear what is the true upper bound of performancewe can expect to achieve.
second, direct measure-ment of any performance measure on our datasetsintroduces sample bias (engstrom et al., 2020).
fors and (cid:100)µmcorrelation, (cid:99)rm could be high because (cid:99)µhshappened to align for this data collection, but arepeat experiment could yield different results.
amore holistic view is to give an estimate of averagecase performance.2.
the evaluation methodology we derive in §4addresses the latter points we raise for pairwise pre-dictions and mean squared error (which has directrelationship to the correlation).
however, we alsobelieve that pairwise predictions is a step in theright direction, and our discussion continues withpairwise predictions..2.3 pairwise predictions.
we will now formalize pairwise predictions.
forsystems s, s(cid:48) ∈ s, deﬁne the true difference intheir system scores as.
s,s(cid:48) = µhδh.
s − µhs(cid:48).
(5).
2pearson’s r was not formulated for individual distribu-s for each datapoint, so applying the william’s.
tions (cid:99)µhtest (graham and baldwin, 2014) also falls short here..s and (cid:100)µm.
s,s(cid:48) = (cid:99)µh(cid:100)δh.
s − (cid:99)µhs(cid:48).
(6).
s,s(cid:48) and (cid:100)δm.
and likewise for the differences δms,s(cid:48) w.r.t.
to a metric m .
in practice, we are interested inthe pairwise prediction of s and s(cid:48) i.e.
whetherδhs,s(cid:48)judgments (we observe (cid:100)δh.
?
> 0, given that we have collected humans,s(cid:48) ≶ 0), or computeds,s(cid:48) ≶ 0).
refer to.
metric scores (we observe (cid:100)δmfigure 1 for an illustration..to operationalize the pairwise prediction of s.(7).
s,s(cid:48)).
and s(cid:48), let the true pairwise labels,s(cid:48) = sign(δh∆hbe deﬁned as the central quantity of interest.
deﬁnethe human predicted pairwise label as(cid:91)s,s(cid:48) = sign( (cid:100)δh∆hs,s(cid:48))and likewise for the true and estimated predictions(cid:91)∆m∆ms,s(cid:48) w.r.t.
to a metric m. the 0-1 clas-siﬁcation loss for metric m on this example is.
s,s(cid:48) and.
(8).
s,s(cid:48),.
l(∆h.
(cid:91)s,s(cid:48)) = i[∆h∆mand the pairwise error of an estimator is the lossincurred averaged over all pairwise examples.
ide-ally, we could calculate the true error of m.(cid:91)∆ms,s(cid:48)].
s,s(cid:48) (cid:54)=.
(9).
errtrue(m ) = es[l(∆h.
s,s(cid:48), ∆m.
s,s(cid:48))].
(10).
but we can only compute an error of m with noisyhuman labels and metric scores estimated fromﬁnite sized test sets.
errobs(m ) = ex ,s[l(.
(cid:91)∆hs,s(cid:48),.
(cid:92)∆ms,s(cid:48))].
(11).
which is typically estimated when we calculatemetric pairwise accuracy from our datasets..3 datasets.
3.1 wmt16-19 metrics shared task.
data.
we use the past 4 years of to-english transla-tion data from the wmt metrics shared task (bojaret al., 2016b, 2017; ma et al., 2018, 2019).3 acrossall years and language pairs, there are 261 mt sys-tems.
pairs of mt systems are extracted withineach year, within each language pair, resulting in1324 pairwise examples.
for each output of an mtsystem, there are one or more humans judgementsand one reference for metric scoring.
1306-5117outputs were collected for each mt system totaling.
3the wmt20 metrics shared task data was not publicly.
available at the time of submission..6842errobs(·).
error componentsbias.
c0noise.
c1var.
errobs(·).
error componentsbias.
c0noise.
c1var.
optimal (∆h∗s,s(cid:48) )(cid:92)∆hhuman (s,s(cid:48) )bertscorechrfbleurt**bleuter.
0.047.
0.0650.1020.1240.1280.1410.184.
0.000.
0.0190.0030.0100.0050.0080.002.
0.000.
0.0000.0860.1050.1080.1270.173.
0.047.
0.0470.0130.0090.0160.0070.009.optimal (∆h∗s,s(cid:48) )(cid:92)∆hhuman (s,s(cid:48) )rougemeteorrouge-webertscoresupert***.
0.045.
0.0670.2960.2960.3170.3300.390.
0.000.
0.022-0.0060.0040.007-0.0040.000.
0.000.
0.0000.2940.2870.3010.3380.382.
0.045.
0.0460.0080.0050.008-0.0040.008.table 1: decomposition of the pairwise error of different metrics (left: wmt, right: summeval).
highlighted inbold is the largest error component.
10k boostrap trials are conducted for estimation of the expectations (estimationerror < 10−3).
*denotes an estimator assumed to be unbiased in the simulation.
**bleurt is evaluated only onwmt2019.
***supert is a reference-less metric..about 1312-5612 judgments, depending on the yearand language pair.
for ease of interpretation, wealways use raw direct assessment judgments whichrange from 0-100..metrics.
we evaluate the performance of thethree metrics included in sacrebleu (bleu, ter,chrf; post, 2018; koehn et al., 2007).
these threehave also participated in every year of the met-rics task as baselines.
in addition, we includetwo recently developed metrics: bertscore(zhang et al., 2020) and bleurt (sellam et al.,2020).
both metrics are found to effectively utilizecontextual embeddings (devlin et al., 2019), andbleurt is a learned metric (tuned on data outsideof wmt2019).
for all metrics, we use the defaultsettings for scoring.
since bleurt is trained onwmt15-18, we test it only on wmt2019 pairs..3.2 summeval.
data.
the summeval dataset (fabbri et al., 2020)contains 100 outputs from 17 summarization sys-tems.
this results in 136 pairwise examples.
foreach system output, 3 expert judgments, and 11references for metric scoring.
each summarizationis judged in four categories from 0-5: coherence,consistency, ﬂuency, and relevance.
to computesystem-level human scores for a system, we ﬁrst av-erage over categories for an aggregate expert score,and then average the aggregated expert scores persystem.
metric scores for system outputs werecomputed against as many references as possible.
metrics.
we evaluate the performance of sev-eral metrics that were found to be effective at thesystem-level in fabbri et al.
(2020).
this includesthe traditional rouge-4 (lin, 2004) summariza-tion metric, its extension rouge-we (ng andabrecht, 2015), and meteor (lavie and agar-wal, 2007).
in addition, we include two metrics.
based on bert (devlin et al., 2019).
bertscore(zhang et al., 2020), also present in the wmt anal-ysis, and supert (gao et al., 2020), which is areference-less metric for summarization..4 decomposing observed metric error.
two sources of variation distinguish the observedpairwise error (11) from the true error in (10) —the noise in the human predicted labels due to ﬁnitejudgements, and the variance in the metric due toﬁnite test sets.
approximating (11) is straightfor-ward with the bootstrap, but disentangling the errorfrom these two sources of variation requires morecare.
with the bias-variance-noise decomposition,we can adjust our observed error estimates to thenoise-free, inﬁnite test set setting of the true error..4.1 the bias-variance-noise decomposition.
the bias-variance-noise decomposition due todomingos (2000) decomposes the observed pair-wise error in (11) w.r.t.
two constant labels for anypairwise example on systems s, s(cid:48) ∈ s:• the true pairwise label for this example is.
∆h∗.
s,s(cid:48) := arg min.
y∈{−1,1}.
ex [l(.
(cid:91)∆hs,s(cid:48), y)].
(12).
and the estimator that produces these true labelshas, by deﬁnition, the lowest observed error.
inthe decomposition, the human predicted labelnoise and metric bias is deﬁned relative to the truelabels.
assuming the central limit theorem (proofin appendix a), we actually have ∆h∗as deﬁned in eq.
(5)..s,s(cid:48) = ∆h.
s,s(cid:48).
• the main prediction of a metric for this ex.
is.
∆m ∗.
s,s(cid:48) = arg min.
y∈{−1,1}.
ex [l(.
(cid:91)∆ms,s(cid:48), y)].
(13).
and we assume that the metric prediction con-verges onto the main prediction as the test data.
6843increases for s and s(cid:48) (empirically validated inappendix b).
in the decomposition, the metricvariance is deﬁned relative to the main prediction..starting from the loss incurred by m on this pair-wise example, the decomposition gives us.
ex [l(.
(cid:91)∆hs,s(cid:48),.
(cid:91)∆ms,s(cid:48))] = c0noise((cid:91)∆ms,s(cid:48)) + c1var(.
(cid:91)∆hs,s(cid:48)).
+ bias(.
(cid:91)∆ms,s(cid:48)).
(14).
where.
• noise(.
(cid:91)s,s(cid:48)) = e[l(∆h.
(cid:91)s,s(cid:48), ∆h∗∆hs,s(cid:48))] where thenoise is an irreducible loss incurred by comput-ing pairwise accuracy to the human predictedlabels instead of the true labels.
note that thisnoise term also exactly corresponds to the lowestachievable observable error (see §4.2)..• bias(.
(cid:91)s,s(cid:48)) = l(∆h∗∆m.
s,s(cid:48), ∆m ∗.
s,s(cid:48)) where the biastois 0 if the main prediction is correct (w.r.t.
the true label), and 1 otherwise.
note that thisterm is also the true error of a metric estimatorin a noise-free, inﬁnite test set setting.
for unbi-ased estimators this term is zero, as their mainprediction matches the true label..• var(.
(cid:91)s,s(cid:48)) = e[l(∆m.
(cid:91)s,s(cid:48), ∆m ∗∆ms,s(cid:48))] where thevariance is a likelihood that the estimator deviatesfrom its main prediction under random sampling..• c0 = 2px (.
(cid:91)s,s(cid:48) = ∆h∗∆m.
s,s(cid:48)) − 1 which means thatthe inﬂuence of label noise on the error becomessmall if the estimator prediction are close to ran-dom chance.
when the estimator gives constantpredictions, the sign of c0 is dependent on theestimator’s correctness.
(cid:91)s,s(cid:48) = ∆h∗∆m.
s,s(cid:48) and c1 = −1 other-wise.
variance can both increase and decreasethe observed error.
if the estimator is unbiased,the variance causes the prediction to from thecorrect main prediction.
on the other hand, fora biased estimator, deviation from its incorrectmain prediction occasionally decreases the error..• c1 = 1 if.
unlike the decomposition for mean squared error,the interaction between the c0 and var terms onlyallows the error of two hypothetical settings to beread off directly from the table: when noise −→ 0,corresponding to estimator error when computedagainst the ground truth; or when noise + var −→ 0,when the ground truth is used and metrics haveaccess to an inﬁnite test set for scoring..4.2 a lower bound for the observed error.
by deﬁnition the constant estimator that producesthe true pairwise labels ∆h∗s,s(cid:48) (deﬁned in (12)) foreach pairwise example has the lowest possible ob-servable error.
the observable error of this op-timal estimator is exactly e[l(s,s(cid:48))] =(cid:91)∆hnoise(s,s(cid:48)).
since this estimator is constant ithas no variance, and since it is instantiated bydeﬁnition it has no bias.
analytically, the ob-served error of any estimator is lower bounded by(cid:91)∆hs,s(cid:48)) and is the agreement of our humannoise(predicted labels with the ground truth..(cid:91)s,s(cid:48), ∆h∗∆h.
4.3 best-faith estimation with the bootstrap.
assuming the bootstrap (efron and tibshirani,1993) which is a common procedure in nlp (droret al., 2018), we can estimate the expectation quan-tities in the decomposition.
by assuming that sam-pling with replacement from our datasets approx-imates real sampling, we can repeatedly simulatethe quantity in an expectation.
taking the meanover trials gives the bootstrap estimate of the expec-tation.
we emphasize that this is a regular applica-tion of a widely accepted technique—the bootstrapassumption allows us to study problems that wouldbe impossible due to the cost of repeat experiments..4.4 results.
the following analyses refer to the error compo-nents (averaged over all examples) from the simu-lated decomposition presented in table 1..the noise component almost always accountsfor a small fraction of the total error.
we foundthis to be counterintuitive—while the lowest ob-servable error (optimal predictions, see §4.2) incurabout 5% error on both datasets, the inﬂuence ofthe noise is much smaller than those errors suggest.
for the constant c0 scaling the noise, c0 = 0 ifthe metric prediction is near random.
since thec0noise term on average is small two cases holdtrue: when humans are uncertain about the ex-ample (noise term large) metrics are as well (c0term small), and when metrics are certain aboutthe examples (c0 term large) humans are as well(noise term small).
the second case empiricallyshows studying the sampling distribution of met-rics (koehn, 2004; berg-kirkpatrick et al., 2012)is effective, as metric certainty in the difference ofsystem quality often implies human certainty..metric variance introduces little to the pair-.
6844sections compare these adjusted errors..5.2 simulating the perfect annotator.
while we can estimate the lower bound to thepairwise error for a given dataset (in §4.2), it isachieved by a constant estimator using system-levelground truth.
comparing segment-level metricsagainst the unbiased “perfect annotator”, or thebest scorer at the segment-level, is more informa-tive.
at the high-level, we can simulate scoringwith the perfect annotator at n judgments using thehuman estimator at n(cid:48) > n judgments to match thevariance of the perfect annotator estimator..let’s start from the unbiased human estimator(cid:99)µhs (2).
recall that the estimator is a sample mean,so its variance is var( (cid:99)µhs ) = var(h(x))/n.
aninsight from chaganty et al.
(2018) gives us thedecomposition of the variance of h(x)var(h(x)) = var(e[h(x)|x])+ e[var(h(x)|x)].
(15).
with the law of total variance.
in words, the vari-ance term can be thought of as the variance of eachoutput sentence’s true quality score (some trans-lations produced by s are better than others) andthe expectation term is the noise introduced by thehumans when estimating the quality of a sentence(human scores have mean 0 noise around an out-put’s true quality score)..one intuition is that even if a perfect annotatorgives the correct score for each sentence, everytime, there is still some unavoidable variance in theestimator due to the variance of the hypotheticalquality scores for each output.
to formalize this no-tion, let p (x) = e[h(x)|x] be the human scoringfunction of a “perfect annotator”, and the estimator(cid:99)µps be an empirical mean of n independent samplesfrom p (x) similar to eq.
(1).
as a sample mean,var( (cid:99)µps ) = var(p (x))/n.
relating this to (15)var(h(x)) = e[var(h(x)|x)]+var(p (x)) (16).
and while var(p (x)) is not directly observable, wecan calculate var(h(x)) with the sample varianceon all the human judgments, and e[var(h(x)|x)]with a pooled variance over variances from repeathuman judgments on the same output sentence..our ﬁnal step considers the efﬁciency ratio r =var(h(x))/var(p (x)).
if we are interested in theperfect annotator estimator at n judgments, the hu-.
figure 2: comparison of metrics to human and perfectannotator estimators with varying number of judgmentsin wmt.
errors are adjusted to an idealized settingwhere true predictions are used for evaluation and met-rics are computed on inﬁnite test sets; here metric pre-dictions become constant, so their errors are constant.
shaded in grey is the region where bertscore is su-perhuman.
results for summeval are in appendix d..wise error, because it is low.
alternatively, met-rics stand to gain little from using more test setexamples.
in mt, dropping both the noise and vari-ance components for the error results in at mosta 1 or 2 percent reduction in the observed error(see §9 for the implications in metrics research).
metrics generally have low variance, so at the testset sizes of wmt and summeval, they are likelyto converge to their main predictions..5 comparing to the human estimator.
in §4, several mt metrics approach the error of thewmt human evaluation.
the wmt human evalua-tion is expensive, using thousands of judgments pertranslation system.
while each human judgmenthas associated monetary cost, once a large test setis collected, running metrics only incurs computa-tional cost.
this section explores this asymmetry,and seeks to understand how much metric predic-tions are worth, in terms of human judgments..5.1 noise-free, variance-free error estimates.
we wish to give our best comparison between met-rics and unbiased estimators (humans or the perfectannotator).
ideally, metrics would be given theirbest chance to perform, by using an inﬁnite testset.
with the decomposition, we can adjust metricerrors estimates to a noise-free and inﬁnite test setsetting by taking only their bias component.
forhuman and perfect annotator estimators, we canadjust their errors to a noise-free setting by tak-ing only the variance component.
the following.
6845100200300400500600700number of judgments0.090.130.10errtrue(m)bertsbleufhumanperfect annotatormay not share the sensitivity property, and our useof crowdworkers may be biased w.r.t.
professionallinguists..in terms of average pairwise error, mt met-rics have an equivalence to a high number ofhuman judgments.
since the error of the hu-man estimator monotonically increases as the num-ber of judgments decrease, each mt metric hasa breakeven point.
metrics outperform humanestimators using judgments below this threshold.
bertscore is as accurate as using a human esti-mator with 600 judgments per system, or the per-fect annotator estimator with 300 judgments, acrossthe wmt dataset.
we highlight the statistical ad-vantage in variance many metrics share, and thatthis advantage offers a possibility that metrics canoutperform humans, determined by which humanestimator the metric is compared against.
this isa consequence of the general fact that humans areunbiased, high-variance estimators, and metrics arebiased, low-variance estimators, as depicted in fig-ure 1. for metrics such as bertscore or chrf,the bias is low as well, which gives it remarkablygood error properties..6 the limits of human evaluation.
the perfect annotator provides optimistic ﬁguresfor human annotation, providing the best perfor-mance for a ﬁxed number of judgments, and requir-ing the least judgments for a ﬁxed performance.
in§5, we saw that the perfect annotator is weak at lownumber of judgments, due to its non-zero variance.
in this section we identify another consequence ofthe perfect annotator’s variance, where estimatingsmall differences in system quality is hard..6.1 power analysis of the perfect annotator.
the performance of an unbiased estimator is depen-dent on their variance and the effect size it is tryingto detect.
this section performs a power analysisto determine how much annotator effort is neededto reliably detect the correct pairwise judgment be-tween two systems (card et al., 2020).
to makean optimistic estimate, we assume our annotatorvariance is close to that of a perfect annotator.
wemake two assumptions to apply a basic power anal-ysis for the estimation of the difference of systemquality between two systems: normality and equalvariance across groups.
for parameters α = 0.05(false positive rate) and β = 0.95 (false negativerate), we can analytically compute the number of.
table 2: power analysis for the number of judgmentsneeded to give a pairwise prediction between two sys-tems at .9 accuracy (α = 0.05, β = 0.95) under ttest as-sumptions (normality, equal variance) in wmt.
wmtratings are on a 0-100 scale, and the perfect annotatorvariance in wmt19 was 19.27. darker cells indicateless feasible experiments, and the colors are set on alog scale.
results for summeval are in appendix d..man estimator at n(cid:48) = rn judgments has variance.
var( (cid:99)µh.
s ) =.
var(h(x))rnvar(p (x))n.=.
= var( (cid:99)µps ).
(17).
(18).
s and (cid:99)µh.
and we invoke the central limit theorem to claimboth (cid:99)µps are normal.
this completes ourreasoning that for scoring on the system-level, sam-pling n(cid:48) = nr human judgments is nearly equiv-alent to sampling n perfect annotator judgments.
see appendix c for step-by-step derivations forthe perfect annotator variance in our datasets..5.3 results.
the following analyses refer to the comparison ofmetric estimators to unbiased estimators at varyingnumber of judgments for wmt in figure 2..judgments from the perfect annotator havelow variance, like those of professional linguists.
while we do not have data from professional lin-guists, we can qualitatively compare them to theperfect annotator.
a growing body of mt litera-ture focuses on professional linguists (freitag et al.,2020; mathur et al., 2020b), and there are at leasttwo known properties of their judgments:theirjudgments have better interannotator agreement(contain less noise), and they are more sensitive tolinguistic phenomena.
the perfect annotator has nonoise, as they assign a constant score to each seg-ment.
however, the perfect annotator in wmt isbetter described as a noiseless crowdworker.
withthe biases of crowdworkers, the perfect annotator.
684612345difference in system quality252321191715true mean std.
deviation32489812436122032130127499687630571720110222925573325491435919187664693208711757531502437571671941603116972926130173347078910judgments needed to ensure our pairwise judgmentis at least β(1 − α) ≈ 90% accurate.
table 2 con-tains power analyses for different instantiations ofannotator variance and effect size..in wmt, detecting a difference of 1 pointrequires at least 10k perfect annotator judg-ments, for different instantiations of its vari-ance.
to put this in perspective, the top 5 zh-entranslation systems in wmt19 differed by less than3 points (barrault et al., 2019).
depending on howmuch is paid per judgment, this cost can quicklybecome infeasible.
here, the merit of such a taskmay be argued, as knowing a small difference ex-ists between two systems may not always be pro-ductive.
from a scientiﬁc perspective, many nlgtechniques will yield small improvements, and notbeing able to detect small differences means wewill not know whether these techniques are useful..6.2 metrics more easily achieve signiﬁcance.
since metrics tend to have lower variance, met-rics often achieve signiﬁcance in estimating thedifference of system qualities, when humans can-not.
for instance, bertscore achieves signiﬁ-cance in estimating quality differences over half ofthe pairwise examples where humans do not (seeappendix §e).
in extreme cases, human evalu-ation is nearly as bad as ﬂipping a coin, butthe metric can still offer a consistent predictionbetween two systems.
when comparing systemssimilar in quality, practitioners must accept that thenumber of possible analyses are limited.
in ablationstudies where similar systems are often compared,metrics may be our only insight into system per-formance.
with white-box metrics such as bleu,value can be derived from qualitative insight (e.g.
systems with high bleu score have high n-gramoverlap with the reference set).
in addition, we mayqualitatively analyze output statistics not intendedto correlate with humans judgment at all (neubiget al., 2019)..7 caveats to the analysis.
our analysis assumes that the human judgments areunbiased.
in wmt16-19, direct assessment (gra-ham et al., 2013) was used to elicit judgments froma combination of crowdworkers and researchers.
direct assessment (da) uses an adequacy evalua-tion prompt (“rate how much you agree that theoutput translation adequately expresses the mean-ing of the reference translation”) and asks contribu-.
tors to rate on a 0-100 scale..the unbiased ground truth is not a ﬁxed goal-post.
a number of factors are known to change theeventual ranking of translation systems with humanscoring.
employing a different collection method-ology, such as human translation edit rate (hter)of instead of da, can result in divergent systemrankings (graham et al., 2016).
in an earlier editionof wmt, da judgments were collected with botha grammaticality prompt and an adequacy prompt,corresponding to different system rankings by therespective attribute (bojar et al., 2016a).
severalstudies have shown scoring differences betweenprofessional linguists and crowdworkers which aredue in part to the fact that linguists are more sen-sitive to linguistic phenomena (fabbri et al., 2019;freitag et al., 2019)..the goals of an evaluation should be decidedby the practitioner.
we do not give suggestionson any particular goals, and practitioners shouldunderstand what their application is, and whichevaluation is the best approximation (refer to gattand krahmer, 2018).
unfortunately, since the exist-ing data in this domain is limited, our analyses arelimited as well.
however, the statistical techniquesapply to any empirical method.
we hope that ouranalysis inspires others to think about statisticallimits in this domain..8 pushing the limits of evaluation.
to push the limits of what can be evaluated, weneed to improve on fundamental aspects of humanevaluation.
on the human side, we may focus oncreating larger effect sizes or reducing noise byadopting new annotation schemes (l¨aubli et al.,2018; shapira et al., 2019) or employing profes-sional linguists (fabbri et al., 2020; toral et al.,2018).
to make the human estimator more efﬁ-cient, we may consider adaptive data collectiontechniques to stop data collection early when signif-icance is achieved, in a statistically sound manner(johari et al., 2017)..strategies combining human and metric evalu-ation are also shown to have potential.
variancereduction techniques can be applied to the humanestimator by taking advantage of strong metrics(chaganty et al., 2018).
another bottleneck in hu-man evaluation is in the random sampling of thetest set.
metrics could form the basis of an impor-tance sampling procedure to choose test sets thatwould best differentiate two systems, as a form of.
6847robust evaluation (chaganty et al., 2017)..on the metric side, if we can reliably estimatemetric bias, we can skip human evaluation alto-gether when the metric is known to be good.
prob-abilistic reinterpretations of current metrics couldbe a useful technique for conﬁdence estimation(keith and o’connor, 2018).
optimistically, met-rics could have provable guarantees, ensuring thecorrectness of metric decisions (jia et al., 2019)..9 best practices for metrics research.
we reinterpret problems in evaluating metrics withcorrelation (§2.2) as a set of guidelines for metricsresearch.
to next year’s organizers of the wmtmetrics shared task and the broader metrics com-munity we suggest the following: (1) pairwise ac-curacy has desirable properties as an evaluationmeasure for metrics.
our bias-variance-noise de-composition shows that the observed pairwise ac-curacy is very close to the true pairwise accuracyfrom a noise-free, inﬁnite test set setting (§4.4).
wesuggest the use of pairwise accuracy as it reﬂectsmetric performance well (which may be veriﬁed us-ing this analysis).
as a normalized form of pairwiseaccuracy, kendall’s τ is also a suitable measure.
(2)since pairwise accuracy is computed against noisyhuman predictions, on average, it should be impos-sible for metrics to achieve a perfect accuracy.
wesuggest providing an upper bound of metric perfor-mance (§4.2) to clarify how much improvement ispossible for metrics on the dataset..10 related work.
the fact that a manual evaluation can be weak,and an automatic one can be better is gaining at-tention in the metrics community.
mathur et al.
(2020b) studied a disagreement between crowd-workers and metrics, and a reevaluation favoredthe metrics over the human prediction.
recently,freitag et al.
(2021) shows that metrics can achievehigher agreement with professional linguists thancrowdworkers in judging translation systems.
theirresults ﬁt into our formalization: if we assume pro-fessional linguists are unbiased, the bias and vari-ance properties of metrics combined are superior tothose of crowdworkers.
our analysis assumes thatcrowdworkers are unbiased, where they assumeprofessional linguists are instead..we wish to highlight several works which in-spired the elements of ours: chaganty et al.
(2018)and hashimoto et al.
(2019) formalize metrics as.
statistical estimators and provide understanding oftheir statistical properties and limits.
in the replica-tion of imagenet, engstrom et al.
(2020) foundthat dataset bias accounted for classiﬁer perfor-mance differences between the original and thereplicated dataset, and provide a decompositionfor the sources of error.
in automated essay scor-ing, scorers are often evaluated against noisy hu-man judgment, and loukina et al.
(2020) devel-oped the prmse to calculate the mse betweenscorer prediction and the true judgment, rather thannoisy judgment.
finally, in bioinformatics, li et al.
(2020) derive an upper bound of the r2 coefﬁ-cient due to experimental noise when regressing onexperiment-derived results..11 conclusion.
through rigorous comparison between metrics, hu-mans, and the perfect segment-level annotator, weidentify the settings where metrics outperform hu-mans due to a statistical advantage in variance.
these results challenge the notion that metrics arealways secondary to human evaluation.
instead, weencourage practitioners to understand when humanevaluation is weak, and when metrics are necessary.
finally, we hope to provide tools for analysis andfuture directions for evaluation..acknowledgments.
discussions with nitika mathur, markus freitag,and thibault sellam led to several insights.
nelsonliu and tianyi zhang provided feedback on ourﬁrst draft, and anonymous reviewers provided feed-back on the submitted draft.
nanyun peng advisedthe ﬁrst author, and on this work.
alex fabbri pro-vided a scored version of the summeval dataset.
we thank all who have made our work possible..references.
peter anderson, basura fernando, mark johnson, andstephen gould.
2016. spice: semantic proposi-tional image caption evaluation.
in computer vision- eccv 2016 - 14th european conference, amster-dam, the netherlands, october 11-14, 2016, pro-ceedings, part v, volume 9909 of lecture notes incomputer science, pages 382–398.
springer..lo¨ıc barrault, ondˇrej bojar, marta r. costa-juss`a,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, christof monz, mathias m¨uller,santanu pal, matt post, and marcos zampieri.
2019..6848findings of the 2019 conference on machine transla-tion (wmt19).
in proceedings of the fourth con-ference on machine translation (volume 2: sharedtask papers, day 1), pages 1–61, florence, italy.
as-sociation for computational linguistics..taylor berg-kirkpatrick, david burkett, and danklein.
2012. an empirical investigation of statis-in proceedings of thetical signiﬁcance in nlp.
2012 joint conference on empirical methods in nat-ural language processing and computational nat-ural language learning, pages 995–1005, jeju is-land, korea.
association for computational linguis-tics..ondrej bojar, c. federmann, b. haddow, philippkoehn, matt post, and lucia specia.
2016a.
tenyears of wmt evaluation campaigns: lessons learnt.
in proceedings of the lrec 2016 workshop “trans-lation evaluation – from fragmented tools anddata sets to an integrated ecosystem”..ondˇrej bojar, yvette graham, and amir kamran.
2017.results of the wmt17 metrics shared task.
inproceedings of the second conference on machinetranslation, pages 489–513, copenhagen, denmark.
association for computational linguistics..ondˇrej bojar, yvette graham, amir kamran, andmiloˇs stanojevi´c.
2016b.
results of the wmt16metrics shared task.
in proceedings of the first con-ference on machine translation: volume 2, sharedtask papers, pages 199–231, berlin, germany.
as-sociation for computational linguistics..denny britz, anna goldie, minh-thang luong, andquoc le.
2017. massive exploration of neural ma-in proceedings ofchine translation architectures.
the 2017 conference on empirical methods in natu-ral language processing, pages 1442–1451, copen-hagen, denmark.
association for computationallinguistics..chris callison-burch, miles osborne, and philippkoehn.
2006. re-evaluating the role of bleu in ma-in 11th conference ofchine translation research.
the european chapter of the association for com-putational linguistics, trento, italy.
association forcomputational linguistics..dallas card, peter henderson, urvashi khandelwal,robin jia, kyle mahowald, and dan jurafsky.
2020.inwith little power comes great responsibility.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9263–9274, online.
association for computa-tional linguistics..arun chaganty, stephen mussmann, and percy liang.
2018. the price of debiasing automatic metrics innatural language evalaution.
in proceedings of the56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 643–653, melbourne, australia.
associationfor computational linguistics..arun chaganty, ashwin paranjape, percy liang, andimportance sam-christopher d. manning.
2017.pling for unbiased on-demand evaluation of knowl-in proceedings of the 2017edge base population.
conference on empirical methods in natural lan-guage processing, pages 1038–1048, copenhagen,denmark.
association for computational linguis-tics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..pedro m. domingos.
2000. a uniﬁed bias-variancedecomposition and its applications.
in proceedingsof the seventeenth international conference on ma-chine learning (icml 2000), stanford university,stanford, ca, usa, june 29 - july 2, 2000, pages231–238.
morgan kaufmann..rotem dror, gili baumer, segev shlomov, and roi re-ichart.
2018. the hitchhiker’s guide to testing statis-tical signiﬁcance in natural language processing.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1383–1392, melbourne, aus-tralia.
association for computational linguistics..bradley efron and robert tibshirani.
1993. an intro-.
duction to the bootstrap.
springer..logan engstrom, andrew ilyas, shibani santurkar,dimitris tsipras, jacob steinhardt, and aleksandermadry.
2020. identifying statistical bias in datasetreplication.
corr, abs/2005.09619..alexander fabbri, irene li, tianwei she, suyi li, anddragomir radev.
2019. multi-news: a large-scalemulti-document summarization dataset and abstrac-tive hierarchical model.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 1074–1084, florence, italy.
association for computational linguistics..alexander r fabbri, wojciech kry´sci´nski, bryansocher,summeval: re-arxiv.
mccann, caiming xiong, richardand dragomir radev.
2020.evaluating summarization evaluation.
preprint arxiv:2007.12626..markus freitag, isaac caswell, and scott roy.
2019.ape at scale and its implications on mt evaluationbiases.
in proceedings of the fourth conference onmachine translation (volume 1: research papers),pages 34–44, florence, italy.
association for com-putational linguistics..markus freitag, george foster, david grangier, vireshratnakar, qijun tan, and wolfgang macherey.
2021..6849experts, errors, and context: a large-scale study ofhuman evaluation for machine translation..markus freitag, david grangier, and isaac caswell.
2020. bleu might be guilty but references are notin proceedings of the 2020 conferenceinnocent.
on empirical methods in natural language process-ing (emnlp), pages 61–71, online.
association forcomputational linguistics..yang gao, wei zhao, and steffen eger.
2020. su-pert: towards new frontiers in unsupervised evalu-ation metrics for multi-document summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1347–1354, online.
association for computational lin-guistics..albert gatt and emiel krahmer.
2018. survey of thestate of the art in natural language generation: coretasks, applications and evaluation.
j. artif.
intell.
res., 61:65–170..yvette graham and timothy baldwin.
2014. testingfor signiﬁcance of increased correlation with humanin proceedings of the 2014 conferencejudgment.
on empirical methods in natural language process-ing (emnlp), pages 172–176, doha, qatar.
associ-ation for computational linguistics..yvette graham, timothy baldwin, meghan dowling,maria eskevich, teresa lynn, and lamia tounsi.
2016. is all that glitters in machine translation qual-ity estimation really gold?
in proceedings of col-ing 2016, the 26th international conference oncomputational linguistics: technical papers, pages3124–3134, osaka, japan.
the coling 2016 orga-nizing committee..yvette graham, timothy baldwin, alistair moffat, andjustin zobel.
2013. continuous measurement scalesin human evaluation of machine translation.
in pro-ceedings of the 7th linguistic annotation workshopand interoperability with discourse, pages 33–41,soﬁa, bulgaria.
association for computational lin-guistics..tatsunori hashimoto, hugh zhang, and percy liang.
2019. unifying human and statistical evaluation fornatural language generation.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1689–1701, minneapolis, min-nesota.
association for computational linguistics..robin jia, aditi raghunathan, kerem g¨oksel, andpercy liang.
2019. certiﬁed robustness to adver-in proceedings of thesarial word substitutions.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 4129–4142, hong kong,china.
association for computational linguistics..ramesh johari, pete koomen, leonid pekelis, anddavid walsh.
2017. peeking at a/b tests: why itin proceedingsmatters, and what to do about it.
of the 23rd acm sigkdd international conferenceon knowledge discovery and data mining, halifax,ns, canada, august 13 - 17, 2017, pages 1517–1525.
acm..katherine keith and brendan o’connor.
2018.uncertainty-aware generative models for inferringin proceedings ofdocument class prevalence.
the 2018 conference on empirical methods innatural language processing, pages 4575–4585,brussels, belgium.
association for computationallinguistics..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..samuel l¨aubli, rico sennrich, and martin volk.
2018.has machine translation achieved human parity?
ain proceed-case for document-level evaluation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 4791–4796,brussels, belgium.
association for computationallinguistics..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231, prague, czech repub-lic.
association for computational linguistics..gang li, jan zrimec, boyang ji, jun geng, johan lars-brink, aleksej zelezniak, jens nielsen, and mar-tin km engqvist.
2020. performance of regressionmodels as a function of experiment noise..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..anastassia loukina, nitin madnani, aoife cahill, liliyao, matthew s. johnson, brian riordan, anddaniel f. mccaffrey.
2020. using prmse to evalu-ate automated scoring systems in the presence of la-bel noise.
in proceedings of the fifteenth workshop.
6850on innovative use of nlp for building educationalapplications, pages 18–29, seattle, wa, usa → on-line.
association for computational linguistics..ehud reiter.
2018. a structured review of the validityof bleu.
computational linguistics, 44(3):393–401..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..ori shapira, david gabay, yang gao, hadar ro-nen, ramakanth pasunuru, mohit bansal, yael am-sterdamer, and ido dagan.
2019. crowdsourcinglightweight pyramids for manual summary evalua-tion.
in proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 1 (long and short papers), pages 682–687, minneapolis, minnesota.
association for com-putational linguistics..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method forautomatic evaluation of open-domain dialog sys-in proceedings of the thirty-second aaaitems.
conference on artiﬁcial intelligence,(aaai-18),the 30th innovative applications of artiﬁcial intel-ligence (iaai-18), and the 8th aaai symposiumon educational advances in artiﬁcial intelligence(eaai-18), new orleans, louisiana, usa, february2-7, 2018, pages 722–729.
aaai press..antonio toral, sheila castilho, ke hu, and andyway.
2018. attaining the unattainable?
reassess-ing claims of human parity in neural machine trans-in proceedings of the third conference onlation.
machine translation, volume 1: research papers,pages 113–123, belgium, brussels.
association forcomputational linguistics..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in 8th inter-uating text generation with bert.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..qingsong ma, ondˇrej bojar, and yvette graham.
2018.results of the wmt18 metrics shared task: bothcharacters and embeddings achieve good perfor-mance.
in proceedings of the third conference onmachine translation: shared task papers, pages671–688, belgium, brussels.
association for com-putational linguistics..qingsong ma, johnny wei, ondˇrej bojar, and yvettegraham.
2019. results of the wmt19 metricsshared task: segment-level and strong mt sys-in proceedings of thetems pose big challenges.
fourth conference on machine translation (volume2: shared task papers, day 1), pages 62–90, flo-rence, italy.
association for computational linguis-tics..nitika mathur, timothy baldwin, and trevor cohn.
2020a.
tangled up in bleu: reevaluating the eval-uation of automatic machine translation evaluationin proceedings of the 58th annual meet-metrics.
ing of the association for computational linguistics,pages 4984–4997, online.
association for computa-tional linguistics..nitika mathur, johnny wei, markus freitag, qing-song ma, and ond ˚a™ej bojar.
2020b.
resultsin proceed-of the wmt20 metrics shared task.
ings of the fifth conference on machine translation,pages 688–725, online.
association for computa-tional linguistics..graham neubig, zi-yi dou, junjie hu, paul michel,danish pruthi, and xinyi wang.
2019. compare-mt:a tool for holistic comparison of language genera-in proceedings of the 2019 confer-tion systems.
ence of the north american chapter of the asso-ciation for computational linguistics (demonstra-tions), pages 35–41, minneapolis, minnesota.
asso-ciation for computational linguistics..jun-ping ng and viktoria abrecht.
2015. better sum-marization evaluation with word embeddings forrouge.
in proceedings of the 2015 conference onempirical methods in natural language processing,pages 1925–1930, lisbon, portugal.
association forcomputational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..6851a equivalence between optimal.
main predictions.
refer to figures 3 and 4..prediction and true system differences.
there is a slight difference between the deﬁnitionof the true difference in (5) which we can alterna-tively deﬁne as.
∆h.
s,s(cid:48) = sign(e[ (cid:99)µh.
s − (cid:99)µh.
s(cid:48)]).
(19).
and the deﬁnition of the optimal prediction ∆h∗s,s(cid:48)in (12), which is positive when.
px ( (cid:99)µh.
s − (cid:99)µh.
s(cid:48) > 0) >.
(20).
12.and the two are not immediately equivalent.
how-ever, if we assume that the central limit theoremapplies (which can be reasonable as our samples − (cid:99)µhmeans always have n > 100) and x = (cid:99)µhs(cid:48)is normal, the cdf of x is.
f (x) = φ((x − e[x])/var(x)).
(21).
where φ is the cdf of the standard normal distri-bution.
since the standard normal is centered andsymmetric, φ(x) > 1/2 ⇐⇒ x > 0. togetherwe have.
f (x) >.
⇐⇒ e[x] > x.
(22).
12.where for x = 0 the left and right hand sides areequivalent to (20) and (19), respectively..b convergence of metric predictions to.
the main prediction.
a key assumption in interpreting the results fromthe bias-variance-noise decomposition in §4 is thatas system-level metrics have access to more outputsfor evaluation, metric predictions converges ontothe main prediction..for many metrics, their system-level score is themean of their segment-level scores (e.g.
bleurt,bertscore, rouge etc.).
this is true for allsummarization metrics.
for these metrics, assum-ing the central limit theorem allows us to provethat metrics converge to the main prediction, sim-ilar to the proof in appendix a. however, somemt metrics (bleu, ter, and chrf) are not sim-ple averages of their segment-level scores, makingthem harder to analyze..for system-level metrics that are not simple av-erages, we analytically observe that their aggrega-tion method is similar to a mean (e.g.
bleu isa macro-average).
we empirically verify that asthe system-level metric evaluates on more test setoutputs, their pairwise predictions converge to the.
figure 3: average agreement of the main predictionto metric predictions computed from varying test setsizes in wmt.
the main predictions were derived fromall of our data.
each point was an estimated with 10kbootstrap trials.
as the size of the test set increases, wesee that the agreement monotonically increases.
notethat only bleurt and bertscore are means oftheir segment-level scores..figure 4: average agreement of the main prediction tometric predictions evaluated on varying test set sizes insummeval.
the main predictions were derived fromall of our data.
each point was an estimated with 10kbootstrap trials.
as the size of the test set increases, wesee that the agreement monotonically increases.
notethat all metrics are means of their segment-level scores..68522004006008001000size of test set0.890.900.910.920.930.940.95average agreement  to main predictionbleurtbleuterfberts50100150200250300size of test set0.750.800.850.900.95average agreement  to main predictionrougemeteormsbertssupertc efﬁciency ratios for the perfect.
d summeval analysis results.
annotator.
with repeat human judgments for a given outputexample, we can estimate the variance of the per-fect annotator (or true segment-level score vari-ance) in wmt and summeval.
for wmt, we useonly valid judgments (system and repeat judg-ments), and discard all attention check judgments(bad ref judgments).
for summeval, we use thedataset as is..in wmt, we analyze all the to-english datagrouped by year.
we believe this grouping is appro-priate because the to-english evaluation is batchedtogether every year.
direct assessment, whichwmt uses to collect human judgments (grahamet al., 2013), is a score assigned by crowdworkers toan english translation while referring to an englishreference, requiring only monolingual knowledge..(cid:112)var(h(x))(cid:112)e[var(h(x)|x)](cid:112)var(p (x))var(h(x))/var(p (x)).
2016.
2017.
2018.
2019.
30.0117.5324.361.52.
29.6522.9618.762.50.
28.2119.5720.331.93.
28.8121.4219.272.24.table 3: step-by-step derivation for the efﬁciency ra-tio r (fourth row) of the perfect annotator estimator forwmt16-19 as deﬁned in §4.1.
square roots are takenso that values are in terms of the original units (stan-dard deviations, judgments range from 0-100).
thesewere calculated on to-english data only..(cid:112)var(h(x))(cid:112)e[var(h(x)|x)](cid:112)var(p (x))var(h(x))/var(p (x)).
expert.
turker.
0.7170.2930.6551.201.
0.7450.4750.5741.686.table 4: step-by-step derivation for the efﬁciency ra-tio r (fourth row) of the perfect annotator estimator forsummeval as deﬁned in §4.1.
square roots are takenso that values are in terms of the original units (stan-dard deviations, judgments range from 1-5).
note thatthere is little agreement between experts and turkers atthe system level..the main analyses in §5 and §6 are presented forsummeval here.
when comparing expert humansto metrics, no metric comes close to expert per-formance at any number of expert judgments.
forthe power analysis, small differences are also hardto detect, similar to the ﬁndings in wmt.
notethat while the perfect expert requires relatively lessjudgments compared to the perfect crowdworkerin wmt, judgments from experts are likely to bemuch more expensive..figure 5: comparison of metrics to human and perfectannotator estimators with varying number of judgmentsin summeval.
errors are adjusted to an idealized set-ting where true predictions are used for evaluation andmetrics are computed on inﬁnite test sets; here metricpredictions become constant, so their errors are con-stant.
no metric comes close to expert performance atany number of judgments (rouge, the best perform-ing summarization metric, has error 0.221)..table 5: power analysis for the number of judgmentsneeded from the perfect expert to give a pairwisejudgment between two systems at .9 accuracy (α =0.05, β = 0.95) under ttest assumptions (normality,equal variance) in summeval.
summeval ratings areon a 1-5 scale, and the true segment quality variancewas 0.655. darker cells indicate less feasible experi-ments, and the colors are set on a log scale..685350100150200250300number of judgments0.080.100.120.140.16errtrue(m)humanperfect annotator0.10.20.30.40.5difference in system quality0.70.640.580.520.460.4true mean std.
deviation254963928516110421315342381358717504391961117214073531589058110227712471468342109454354567e metric and human signiﬁcance.
breakdown.
for the pairwise examples in wmt, we provide theco-occurrence of signiﬁcance for metric and humanestimators.
refer to figures 6 and 7 for analyseson bertscore and bleurt, respectively..figure 6: co-occurrence of bertscore and humansigniﬁcance on pairs in wmt16-19.
pairs are orderedso that the human difference in system quality is alwayspositive.
signiﬁcance is tested with a one-sided boot-strap resampling test, in the direction of the differencefor both humans and metrics with 1k trials at α = 0.05.bertscore achieves signiﬁcance more than half ofthe time when humans cannot..figure 7: co-occurrence of bleurt and human sig-niﬁcance on pairs in wmt19.
pairs are ordered so thatthe human difference in system quality is always posi-tive.
signiﬁcance is tested with a one-sided bootstrapresampling test, in the direction of the difference forboth humans and metrics with 1k trials at α = 0.05.bleurt achieves signiﬁcance more than half of thetime when humans cannot..6854sig worsenot sigsig betterbertssig betternot sighuman43889628116137200400600800sig worsenot sigsig betterbleurtsig betternot sighuman322339363950100150200