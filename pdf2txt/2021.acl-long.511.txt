a novel estimator of mutual information for learning to disentangletextual representations.
pierre colombo†(cid:63), pablo piantanida∗, chloé clavel(cid:63)(cid:63)télécom paristech, université paris saclayibm gbs france∗laboratoire des signaux et systèmes (l2s), centralesupelec cnrs universite paris-saclaypierre.colombo@ibm.comchloe.clavel@telecom-paris.frpablo.piantanida@centralesupelec.fr.
†.
abstract.
learning disentangled representations of tex-tual data is essential for many natural languagetasks such as fair classiﬁcation, style transferand sentence generation, among others.
theexistent dominant approaches in the context oftext data either rely on training an adversary(discriminator) that aims at making attributevalues difﬁcult to be inferred from the latentcode or rely on minimising variational boundsof the mutual information between latent codeand the value attribute.
however, the availablemethods suffer of the impossibility to providea ﬁne-grained control of the degree (or force)of disentanglement.
in contrast to adversar-ial methods, which are remarkably simple, al-though the adversary seems to be performingperfectly well during the training phase, af-ter it is completed a fair amount of informa-tion about the undesired attribute still remains.
this paper introduces a novel variational up-per bound to the mutual information betweenan attribute and the latent code of an encoder.
our bound aims at controlling the approxima-tion error via the renyi’s divergence, leadingto both better disentangled representations andin particular, a precise control of the desirabledegree of disentanglement than state-of-the-artmethods proposed for textual data.
further-more, it does not suffer from the degeneracy ofother losses in multi-class scenarios.
we showthe superiority of this method on fair classiﬁ-cation and on textual style transfer tasks.
ad-ditionally, we provide new insights illustrat-ing various trade-offs in style transfer when at-tempting to learn disentangled representationsand quality of the generated sentence..1.introduction.
learning disentangled representations hold a cen-tral place to build rich embeddings of high-dimensional data.
for a representation to be disen-tangled implies that it factorizes some latent cause.
or causes of variation as formulated by (bengioet al., 2013).
for example, if there are two causesfor the transformations in the data that do not gen-erally happen together and are statistically distin-guishable (e.g., factors occur independently), amaximally disentangled representation is expectedto present a sparse structure that separates thosecauses.
disentangled representations have beenshown to be useful for a large variety of data, suchas video (hsieh et al., 2018), image (sanchez et al.,2019), text (john et al., 2018), audio (hung et al.,2018), among others, and applied to many differenttasks, e.g., robust and fair classiﬁcation (elazar andgoldberg, 2018), visual reasoning (van steenkisteet al., 2019), style transfer (fu et al., 2017), con-ditional generation (denton et al., 2017; burgesset al., 2018), few shot learning (kumar verma et al.,2018), among others..in this work, we focus our attention on learningdisentangled representations for text, as it remainsoverlooked by (john et al., 2018).
perhaps, oneof the most popular applications of disentangle-ment in textual data is fair classiﬁcation (elazarand goldberg, 2018; barrett et al., 2019) and sen-tence generation tasks such as style transfer (johnet al., 2018) or conditional sentence generation(cheng et al., 2020b).
for fair classiﬁcation, per-fectly disentangled latent representations can beused to ensure fairness as the decisions are takenbased on representations which are statistically in-dependent from–or at least carrying limited infor-mation about–the protected attributes.
however,there exists a trade-offs between full disentangledrepresentations and performances on the target task,as shown by (feutry et al., 2018), among others.
for sequence generation and in particular, for styletransfer, learning disentangled representations aimat allowing an easier transfer of the desired style.
to the best of our knowledge, a depth study ofthe relationship between disentangled representa-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6539–6550august1–6,2021.©2021associationforcomputationallinguistics6539−.
tions based either on adversarial losses solely ors and quality of the generated sen-on vclu btences remains overlooked.
most of the previousstudies have been focusing on either trade-offs be-tween metrics computed on the generated sentences(tikhonov et al., 2019) or performance evaluationof the disentanglement as part of (or convolutedwith) more complex modules.
this enhances theneed to provide a fair evaluation of disentanglementmethods by isolating their individual contributions(yamshchikov et al., 2019; cheng et al., 2020b).
methods to enforce disentangled representationscan be grouped into two different categories.
theﬁrst category relies on an adversarial term in thetraining objective that aims at ensuring that sensi-tive attribute values (e.g.
race, sex, style) as statis-tically independent as possible from the encodedlatent representation.
interestingly enough, sev-eral works (john et al., 2018; elazar and gold-berg, 2018; bao et al., 2019; yi et al., 2020; jainet al., 2019; zhang et al., 2018; hu et al., 2017),elazar and goldberg (2018) have recently shownthat even though the adversary teacher seems to beperforming remarkably well during training, afterthe training phase, a fair amount of informationabout the sensitive attributes still remains, and canbe extracted from the encoded representation.
thesecond category aim at minimising mutual infor-mation (mi) between encoded latent representationand the sensitive attribute values, i.e., without re-sorting to an adversarial discriminator.
mi actsas an universal measure of dependence since itcaptures non-linear and statistical dependencies ofhigh orders between the involved quantities (kin-ney and atwal, 2014).
however, estimating mihas been a long-standing challenge, in particularwhen dealing with high-dimensional data (paninski,2003; pichler et al., 2020).
recent methods relyon variational upper bounds.
for instance, (chenget al., 2020b) study vclub-s (cheng et al., 2020a)for sentence generation tasks.
although this ap-proach improves on previous state-of-the-art meth-ods, it does not allow to ﬁne-tuning of the desireddegree of disentanglement, i.e., it enforces light orstrong levels of disentanglement where only fewfeatures relevant to the input sentence remain (seefeutry et al.
(2018) for further discussion)..1.1 our contributions.
we develop new tools to build disentangled textualrepresentations and evaluate them on fair classiﬁ-.
cation and two sentence generation tasks, namely,style transfer and conditional sentence generation.
our main contributions are summarized below:.
• a novel objective to train disentangled rep-resentations from attributes.
to overcomesome of the limitations of both adversariallosses and vclub-s we derive a novel up-per bound to the mi which aims at correct-ing the approximation error via either thekullback-leibler (ali and silvey, 1966) orrenyi (rényi et al., 1961) divergences.
thiscorrection terms appears to be a key featureto ﬁne-tuning the degree of disentanglementcompared to vclub-s..• applications and numerical results.
first, wedemonstrate that the aforementioned surro-gate is better suited than the widely used ad-versarial losses as well as vclub-s as it canprovide better disentangled textual representa-tions while allowing ﬁne-tuning of the desireddegree of disentanglement.
in particular, weshow that our method offers a better accuracyversus disentanglement trade-offs for fair clas-siﬁcation tasks.
we additionally demonstratethat our surrogate outperforms both methodswhen learning disentangled representationsfor style transfer and conditional sentence gen-eration while not suffering (or degenerating)when the number of classes is greater than two,which is an apparent limitation of adversarialtraining.
by isolating the disentanglementmodule, we identify and report existing trade-offs between different degree of disentangle-ment and quality of generated sentences.
thelater includes content preservation betweeninput and generated sentences and accuracyon the generated style..2 main deﬁnitions and related works.
we introduce notations, tasks, and closely relatednwork.
consider a training seti=1}{paired with attribute valuesof n sentences xiwhich indicates a discreteyi1, .
.
.
,attribute to be disentangled from the resulting rep-resentations.
we study the following scenarios:.
∈ x|y|}.
∈ y ≡ {.
(xi, yi).
=.
d.disentangled representations.
learning disen-tangled representations consists in learning a modeld that maps feature inputs x to a vec-mtor of dimension d that retains as much as possibleinformation of the original content from the input.
x → r.:.
6540sentence but as little as possible about the unde-sired attribute y .
in this framework, content isdeﬁned as any relevant information present in xthat does not depend on y ..0, 1}.
m∈ {.
applications to binary fair classiﬁcation.
thetask of fair classiﬁcation through disentangled rep-resentations aims at building representations thatare independent of selective discrete (sensitive) at-tributes (e.g., gender or race).
this task consists inlearning a modelthat maps any:x → {input x to a label l.
the goal of the learner0, 1}is to build a predictor that assigns each x to either0 or 1 “oblivious” of the protected attribute y. re-cently, much progress has been made on devisingappropriate means of fairness, e.g., (zemel et al.,2013; zafar et al., 2017; mohri et al., 2019).
in par-ticular, (xie et al., 2017; barrett et al., 2019; elazarand goldberg, 2018) approach the problem basedon adversarial losses.
more precisely, these ap-proaches consist in learning an encoder that mapsx into a representation vector hx, a critic cθc whichattempts to predict y, and an output classiﬁer fθdused to predict l based on the observed hx.
theclassiﬁer is said to be fair if there is no statisticalinformation about y that is present in hx (xie et al.,2017; elazar and goldberg, 2018)..:.
m.x × y → x.applications to conditional sentence genera-tion.
the task of conditional sentence genera-tion consists in taking an input text containingspeciﬁc stylistic properties to then generate a re-alistic (synthetic) text containing potentially dif-ferent stylistic properties.
it requests to learn amodelthat maps a pair of in-puts (x, yt) to a sentence xg, where the outcomesentence should retain as much as possible of theoriginal content from the input sentence while hav-ing (potentially a new) attribute yg.
proposed ap-proaches to tackle textual style transfer (zhanget al., 2020; xu et al., 2019) can be divided into twomain categories.
the ﬁrst category (prabhumoyeet al., 2018; lample et al., 2018) uses cycle lossesbased on back translation (wieting et al., 2017)to ensure that the content is preserved during thetransformation.
whereas, the second category lookto explicitly separate attributes from the content.
this constraint is enforced using either adversarialtraining (fu et al., 2017; hu et al., 2017; zhanget al., 2018; yamshchikov et al., 2019) or mi min-imisation using vclub-s (cheng et al., 2020b).
traditional adversarial training is based on an en-coder that aims to fool the adversary discriminator.
by removing attribute information from the contentembedding (elazar and goldberg, 2018).
as wewill observe, the more the representations are dis-entangled the easier is to transfer the style but atthe same time the less the content is preserved.
inorder to approach the sequence generation tasks,we build on the style-embedding model by (johnet al., 2018) (styleemb) which uses adversariallosses introduced in prior work for these dedicatedtasks.
during the training phase, the input sentenceis fed to a sentence encoder, namely fθe, whilethe input style is fed to a separated style encoder,namely f s.
during the inference phase, the desiredθestyle–potentially different from the input style–isprovided as input along with the input sentence..3 model and training objective.
this section describes the proposed approach tolearn disentangled representations.
we ﬁrst reviewmi along with the model overview and then, we de-rive the variational bound we will use, and discussconnections with adversarial losses..3.1 model overview.
the mi is a key concept in information theory formeasuring high-order statistical dependencies be-tween random quantities.
given two random vari-ables z and y , the mi is deﬁned by.
i(z; y ) = ezy.
log.
(cid:20).
pzy (z, y )pz(z)py (y ).
(cid:21).
,.
(1).
where pzy is the joint probability density function(pdf) of the random variables (z, y ), with pz andpy representing the respective marginal pdfs.
miis related to entropy h(y ) and conditional entropyh(y.z) as follows:|.
i(z; y ) = h(y ).
h(y.
−.
z).
|.
(2).
our models for fair classiﬁcation and sequence gen-eration share a similar structure.
these rely on anencoder that takes as input a random sentence xand maps it to a random representation z using adeep encoder denoted by fθe.
then, classiﬁcationand sentence generation are performed using eithera classiﬁer or an auto-regressive decoder denotedby fθd.
we aim at minimizing mi between the la-tent code represented by the random variable (rv)z = fθe(x) and the desired attribute representedby the rv y .
the objective of interest(fθe) isdeﬁned as:.
l.(fθe).
l.
down.
(fθe)≡ l(cid:124)(cid:125)(cid:123)(cid:122)downstream task.
+λ.
·.
,i(fθe(x); y )(cid:125)(cid:123)(cid:122)(cid:124)disentangled.
(3).
6541l.wheredown.
represents a downstream speciﬁc(target task) loss and λ is a meta-parameter thatcontrols the sensitive trade-off between disentan-glement (i.e., minimizing mi) and success in thedownstream task (i.e., minimizing the target loss).
in sec.
5, we illustrate theses different trade-offs..applications to fair classiﬁcation and sen-tence generation.
for fair classiﬁcation, we followstandard practices and optimize the cross-entropybetween prediction and ground-truth labels.
in thesentence generation taskdown.
represents the neg-ative log-likelihood between individual tokens..l.3.2 a novel upper bound on mi.
estimating the mi is a long-standing challenge asthe exact computation (paninski, 2003) is onlytractable for discrete variables, or for a limitedfamily of problems where the underlying data-distribution satisﬁes smoothing properties, see re-cent work by (pichler et al., 2020).
different fromprevious approaches leading to variational lowerbounds (belghazi et al., 2018; hjelm et al., 2018;oord et al., 2018), in this paper we derive an es-timator based on a variational upper bound to themi which control the approximation error basedon the kullback-leibler and the renyi divergences(daudel et al., 2020)..theorem 1 (variational upper bound on mi) let(z, y ) be an arbitrary pair of rvs with (z, y )∼pzy according to some underlying pdf, and let(cid:98)y |z be a conditional variational distribution onqthe attributes satisfying pzy(cid:98)y |z, i.e.,qabsolutely continuous.
then, we have that.
pz.
(cid:28).
·.
(cid:20).
(cid:90).
ey.
≤.
i(z; y ).
(cid:104).
ey z.log q.
(cid:98)y |z(y.log.
(cid:105)z).
q.
(cid:98)y |z(y|+ kl(cid:0)pzy.
−.
|.
z)pz(z)dz.
+.
pz.
q.
·.
(cid:107).
(cid:98)y |z.
(cid:1),(4).
where kl(cid:0)pzyqgence.
similarly, we have that for any α > 1,.
(cid:1) denotes the kl diver-.
pz(cid:107).
(cid:98)y |z.
·.
(cid:20).
(cid:90).
ey.
≤.
i(z; y ).
(cid:104).
ey z.log q.
(cid:98)y |z(y.log.
(cid:105)z).
q.
(cid:98)y |z(y|(cid:0)pzy.
+ dα.
−.
|.
z)pz(z)dz.
+.
pz.
q.
·.
(cid:107).
(cid:98)y |z.
(cid:1),(5).
−.
1)dα.
(cid:0)pzy.
(cid:1) = log ezy [where (α(cid:107)rα−1(z, y )] denotes the renyi divergence andr(z, y) =.
supp(pzy )..py |z (y|z)(cid:98)y |z (y|z) , for (z, y)q.
(cid:98)y |z.
pz.
q.
·.
∈.
(cid:21).
(cid:21).
proof: the upper bound on h(y ) is a direct ap-plication of the the (donsker and varadhan, 1985)representation of kl divergence while the lowerz) follows from the monotonicitybound on h(y|(cid:1).
property of the function: αq·further details are relegated to appendix a..(cid:0)pzy.
dα.
(cid:98)y |z.
pz.
(cid:55)→.
(cid:107).
remark: it is worth to emphasise that the kl di-vergence in (4) and renyi divergence in (5) controlthe approximation error between the exact entropyand its corresponding bound..q.
≡.
pz.
from theoretical bounds to trainable surro-gates to minimize mi: it is easy to check thatthe inequalities in (eq.
4) and (eq.
5) are tight(cid:98)y |z almost surely forprovided that pzy·some adequate choice of the variational distribution.
however, the evaluation of these bounds requiresto obtain an estimate of the density-ratio r(z, y).
density-ratio estimation has been widely studiedin the literature (see (sugiyama et al., 2012) andreferences therein) and conﬁdence bounds has beenreported by (kpotufe, 2017) under some smoothingassumption on underlying data-distribution pzy .
in this work, we will estimate this ratio by using acritic cθr which is trained to differentiate betweena balanced dataset of positive i.i.d samples comingfrom pzy and negative i.i.d samples coming frompz.
then, for any pair (z, y), the density-q.
(cid:98)y |z ·(z,y))(z,y)) ,ratio can be estimated by r(z, y)where σ() indicates the sigmoid function and·cθr(z, y) is the unnormalized output of the critic.
it is worth to mention that after estimating this ra-tio, the previous upper bounds may not be strictbounds so we will refer them as surrogates..σ(cθr1−σ(cθr.
≈.
3.3 comparison to existing methods.
adversarial approaches: in order to enhance ourunderstanding of why the proposed approach basedon the minimization of the mi using our varia-tional upper bound in th.
1 may lead to a bettertraining objective than previous adversarial losses,we discuss below the explicit relationship betweendenotemi and cross-entropy loss.
let ya random attribute and let z be a possibly high-dimensional representation that needs to be disen-tangled from y .
then,.
∈ y.i(z; y ).
h(y ).
≥= const.
−.
−.
ey z.log q.
(cid:98)y |z(y.
(cid:105).
z)|.
(6).
ce( (cid:98)y.z),.
(cid:104).
|.
where ce( (cid:98)ysponding to the adversarial discriminator q.z) denotes the cross-entropy corre-|(cid:98)y |z, not-.
6542q.
(cid:107).
−.
pz.
h(y.
(cid:98)y |z ·.
z)|.
ing that y comes from an unknown distribution onwhich we have no inﬂuence h(y ) is an unknownconstant, and using that the approximation error:(cid:1) = ce( (cid:98)ykl(cid:0)qzyz).
eq.
6|shows that the cross-entropy loss leads to a lowerbound (up to a constant) on the mi.
although thecross-entropy can lead to good estimates of theconditional entropy, the adversarial approaches forclassiﬁcation and sequence generation by (barrettet al., 2019; john et al., 2018) which consists inmaximizing the cross-entropy, induces a degener-acy (unbounded loss) as λ increases in the underly-ing optimization problem.
as we will observe innext section, our variational upper bound in th.
1> 2.can overcome this issue, in particular forvclub-s: different from our method, chenget al.
(2020a) introduce ivclub which is an upperbound on mi deﬁned by.
|y|.
ivclub(y ; z) =ey z[log py |z(y.z)]|ey ez[log py |z(y.
−.
(7).
z)].
|.
it would be worth to mention that this bound fol-lows a similar approach to the previously intro-duced bound in (feutry et al., 2018)..4 experimental setting.
4.1 datasets.
fair classiﬁcation task.
we follow the experimen-tal protocol of (elazar and goldberg, 2018).
themain task consists in predicting a binary label rep-resenting either the sentiment (positive/negative)or the mention.
the mention task aims at predict-ing if a tweet is conversational.
here the consid-ered protected attribute is the race.
the datasethas been automatically constructed from dial cor-pus (blodgett et al., 2016) which contained raceannotations over 50 million of tweets.
sentimenttweets are extracted using a list of predeﬁned emo-jis and mentions are identiﬁed using @mentionstokens.
the ﬁnal dataset contains 160k tweets forthe training and two splits of 10k tweets for valida-tion and testing.
splits are balanced such that therandom estimator is likely to achieve 50% accuracy.
style transfer for our sentence generation task,we conduct experiments on three different datasetsextracted from restaurant reviews in yelp.
theﬁrst dataset, referred to as syelp, contains 444101,63483, and 126670 labelled short reviews (at most20 words) for train, validation, and test, respec-tively.
for each review a binary label is assigned.
depending on its polarity.
following (lample et al.,2018), we use a second version of yelp, referred toas fyelp, with longer reviews (at most 70 words).
it contains ﬁve coarse-grained restaurant categorylabels (e.g., asian, american, mexican, bars anddessert).
the multi-category fyelp is used to ac-cess the generalization capabilities of our methodsto a multi-class scenario..4.2 metrics for performance evaluation.
efﬁciency measure ofthe disentanglementmethods.
(barrett et al., 2019) report that ofﬂineclassiﬁers (post training) outperform clearly adver-sarial discriminators.
we will re-training a classi-ﬁer on the latent representation learnt by the modeland we will report its accuracy..measure of performance within the fair clas-siﬁcation task.
in the fair classiﬁcation task weaim at maximizing accuracy on the target task andso we will report the corresponding accuracy..measure of performance within sentence gen-eration tasks.
sentences generated by the modelare expected to be ﬂuent, to preserve the input con-tent and to contain the desired style.
for style trans-fer, the desired style is different from the input stylewhile for conditional sentence generation, both in-put and output styles should be similar.
neverthe-less, automatic evaluation of generative models fortext is still an open problem.
we measure the styleof the output sentence by using a fasttext classiﬁer(joulin et al., 2016b).
for content preservation, wefollow (john et al., 2018) and compute both: (i)the cosine measure between source and generatedsentence embeddings, which are the concatenationof min, max, and mean of word embedding (sen-timent words removed), and (ii) the bleu scorebetween generated text and the input using sacre-bleu from (post, 2018).
motivated by previouswork, we evaluate the ﬂuency of the language withthe perplexity given by a gpt-2 (radford et al.,2019) pretrained model performing ﬁne-tuning onthe training corpus.
we choose to report the log-perplexity since we believe it can better reﬂects theuncertainty of the language model (a small varia-tion in the model loss would induce a large changein the perplexity due to the exponential term).
be-sides the automatic evaluation, we further test ourdisentangled representation effectiveness by humanevaluation results are presented in tab.
1.conventions and abbreviations.
adv refersto a model trained using the adversarial loss;.
6543vclub-s, kl refers to a model trained usingthe vclub-s and kl surrogate (see eq.
14)respectively; and dα refers to a model trainedbased on the α-renyi surrogate (eq.
15), for α1.3, 1.5, 1.8.
∈.
{.
.
}.
5 numerical results.
in this section, we present our results on the fairclassiﬁcation and binary sequence generation tasks,see ssec.
5.1 and ssec.
5.2, respectively.
we addi-tionally show that our variational surrogates to themi–contrarily to adversarial losses–do not sufferin multi-class scenarios (see ssec.
5.3)..5.1 applications to fairness.
upper bound on performances.
we ﬁrst exam-ine how much of the protected attribute we canbe recovered from an unfair classiﬁer (i.e., trainedwithout adversarial loss) and how well does suchclassiﬁer perform.
results are reported in fig.
1.we observe that we achieve similar scores than theones reported in previous studies (barrett et al.,2019; elazar and goldberg, 2018).
this experi-ment shows that, when training to solve the maintask, the classiﬁer learns information about the pro-tected attribute, i.e., the attacker’s accuracy is betterthan random guessing.
in the following, we com-pare the different proposed methods to disentanglerepresentations and obtain a fairer classiﬁer..methods comparisons.
fig.
1 shows the resultsof the different models and illustrates the trade-offsbetween disentangled representations and the targettask accuracy.
results are reported on the testsetfor both sentiment and mention tasks when race isthe protected.
we observe that the classiﬁer trainedwith an adversarial loss degenerates for λ > 5 sincethe adversarial term in eq.
3 is inﬂuencing muchthe global gradient than the downstream term (i.e.,cross-entropy loss between predicted and goldendistribution).
remarkably, both models trained tominimize either the kl or the renyi surrogate donot suffer much from the aforementioned multi-class problem.
for both tasks, we observe thatthe kl and the renyi surrogates can offer betterdisentangled representations than those inducedby adversarial approaches.
in this task, both thekl and renyi achieve perfect disentangled rep-resentations (i.e., random guessing accuracy onprotected attributes) with a 5% drop in the accu-racy of the target task, when perfectly maskingthe protected attributes.
as a matter of fact, we ob-.
serve that vclub-s provides only two regimes: ei-ther a “light” protection (attacker accuracy around60%), with almost no loss in task accuracy (λ < 1),or a strong protection (attacker accuracy around50%), where a few features relevant to the targettask remain.1 on the sentiment task, we can drawsimilar conclusions.
however, the renyi’s surro-gate achieves slightly better-disentangled represen-tations.
overall, we can observe that our proposedsurrogate enables good control of the degree ofdisentangling.
additionally, we do not observe adegenerated behaviour–as it is the case with adver-sarial losses–when λ increases.
furthermore, oursurrogate allows simultaneously better disentan-gled representations while preserving the accuracyof the target task..5.2 applications to binary polarity transfer.
in the previous section, we have shown that theproposed surrogates do not suffer from limitationsof adversarial losses and allow to achieve betterdisentangled representations than existing methodsrelying on vclub-s. disentanglement modulesare a core block for a large number of both styletransfer and conditional sentence generation algo-rithms (tikhonov et al., 2019; yamshchikov et al.,2019; fu et al., 2017) that place explicit constraintsto force disentangled representations.
first, we as-sess the disentanglement quality and the controlover desired level of disentanglement while chang-ing the downstream term, which for the sentencegeneration task is the cross-entropy loss on individ-ual token.
then, we exhibit the existing trade-offsbetween quality of generated sentences, measuredby the metric introduced in ssec.
4.2, and the re-sulting degree of disentanglement.
the results arepresented for syelp.
5.2.1 evaluating disentanglementfig.
2a shows the adversary accuracy of the differ-ent methods as a function of λ. similarly to thefair classiﬁcation task, a fair amount of informationcan be recovered from the embedding learnt withadversarial loss.
in addition, we observe a cleardegradation of its performance for values λ > 1.in this setting, the renyi surrogates achieves con-sistently better results in terms of disentanglementthan the one minimizing the kl surrogate.
thecurve for renyi’s surrogates shows that exploringdifferent values of λ allows good control of the.
1this phenomenon is also reported in (feutry et al., 2018).
on a picture anonymization task..6544(a).
(c).
(b).
(d).
figure 1: numerical results on fair classiﬁcation.
trade-offs between target task and attacker accuracy are reportedin fig.
1a, fig.
1b for mention task, and fig.
1c, fig.
1d for sentiment task.
for low values of λ some points coincide.
as λ increases the level of disentanglement increases and the proposed methods using both kl (kl) and renydivergences (.
dα) clearly offer better control than existing methods..disentanglement degree.
renyi surrogate general-izes well for sentence generation.
similarly to thefairness task vclub-s only offers two regimes:"light" disentanglement with very little polaritytransfer and "strong" disentanglement..5.2.2 disentanglement in polarity transfer.
the quality of generated sentences are evaluated us-ing the ﬂuency (see fig.
3c ), the content preserva-tion (see fig.
3a), additional results using a cosinesimilarity are given in appendix d, and polarityaccuracy (see fig.
3b ).
for style transfer, andfor all models, we observe trade-offs between dis-entanglement and content preservation (measuredby bleu) and between ﬂuency and disentangle-ment.
learning disentangled representations leadsto poorer content preservation.
as a matter of fact,similar conclusions can be drawn while measuringcontent with the cosine similarity (see appendix d).
for polarity accuracy, in non-degenerated cases(see below), we observe that the model is able tobetter transfer the sentiment in presence of disen-tangled representations.
transferring style is easierwith disentangled representations, however thereis no free lunch here since disentangling also re-.
∈ {.
moves important information about the content.
itis worth noting that even in the "strong" disentan-glement regime vclub-s struggles to transfer the)polarity (accuracy of 40% for λ1, 2, 10, 15}where other models reach 80%.
it is worth not-ing that similar conclusions hold for two differentsentence generation tasks: style transfer and condi-tional generation, which tends to validate the cur-rent line of work that formulates text generation asgeneric text-to-text (raffel et al., 2019).
quality of generated sentences.
examples ofgenerated sentences are given in tab.
2 , providingqualitative examples that illustrate the previouslyobserved trade-offs.
the adversarial loss degener-5 and a stuttering phenomenonates for values λappears (holtzman et al., 2019).
tab.
1 gathers re-sults of human evaluation and show that our surro-gates can better disentangle style while preservingmore content than available methods..≥.
5.3 adversarial loss fails to disentangle.
when.
3.
|y| ≥.
in fig.
2b we report the adversary accuracy of ourdifferent methods for the values of λ using fyelp.
654510−310−210−1100101λ0.500.550.600.650.700.750.800.850.90attacker’srecoveryaccuracyupperboundupperbound10−310−210−1100101λ0.500.550.600.650.700.750.800.850.90taskaccuracyupperbound1031021011001010.500.550.600.650.700.750.800.850.90attackers recovery accuracyupper boundmodelsd=1.8d=1.5d=1.3kladvvclubs1031021011001010.500.550.600.650.700.75task accuracyupper bound(a) binary style transfert..(b) multiclass style transfert.
figure 2: disentanglement of representation learnt by fθe in the binary (left) and multi-class (i.e.,sentence generation scenario.
in the multi-class scenario the adv degenerates for λgrained control over the degree of disentanglement..≥.
= 5) (right)0.01 and offer no ﬁned-.
|y|.
(a).
(b)figure 3: numerical experiments on binary style transfer.
quality of generated sentences are evaluated usingbleu (fig.
3a); style transfer accuracy (fig.
3a); sentence ﬂuency (fig.
3c).
we report existing trade-offs betweendisentanglement and sentence generation quality.
human evaluation is reported in tab.
1..(c).
≤.
dataset with category label.
in the binary setting for1, models using adversarial loss can learn dis-λentangled representations while in the multi-classsetting, the adversarial loss degenerates for smallvalues of λ (i.e sentences are no longer ﬂuent asshown by the increase in perplexity in fig.
4c).
minimizing mi based on our surrogates seems tomitigate the problem and offer a better control ofthe disentanglement degree for various values of λthan vclu bs. further results are gathered inappendix g..−.
6 summary and concluding remarks.
we devised a new alternative method to adversariallosses capable of learning disentangled textual rep-resentation.
our method does not require adversar-ial training and hence, it does not suffer in presenceof multi-class setups.
a key feature of this methodis to account for the approximation error incurredwhen bounding the mutual information.
experi-ments show better trade-offs than both adversarialtraining and vclub-s on two fair classiﬁcationtasks and demonstrate the efﬁciency to learn dis-entangled representations for sequence generation.
as a matter of fact, there is no free-lunch for sen-.
tence generation tasks: although transferring styleis easier with disentangled representations, it alsoremoves important information about the content.
the proposed method can replace the adversary inany kind of algorithms (tikhonov et al., 2019; fuet al., 2017) with no modiﬁcations.
future workincludes testing with other type of labels such asdialog act (chapuis et al., 2020; colombo et al.,2020), emotions (witon et al., 2018), opinion (gar-cia et al., 2019) or speaker’s stance and conﬁdence(dinkar et al., 2020).
since it allows more ﬁne-grained control over the amount of disentangle-ment, we expect it to be easier to tune when com-bined with more complex models..7 acknowledgements.
the authors would like to thanks georg pichler forthe thorough reading.
the work of prof. pablo pi-antanida was supported by the european commis-sion’s marie sklodowska-curie actions (msca),through the marie sklodowska-curie if (h2020-mscaif-2017-ef-797805).
the phd of pierre isfully founded by ibm gbs france in collaborationwith telecom paris..65460.011.005.0010.0015.0020.000.60.70.80.91.0adv accuracy.1031021011001010.150.200.250.300.350.400.450.500.55adv accuracy.d=1.8d=1.5d=1.3kladvvclubs0.011.005.0010.0015.0020.00051015202530bleu0.011.005.0010.0015.0020.000.00.20.40.60.8accuracy0.011.005.0010.0015.0020.002.02.53.03.54.04.55.05.5perplexitymodelsd=1.8d=1.5d=1.3kladvvclubsreferences.
syed mumtaz ali and samuel d silvey.
1966. a gen-eral class of coefﬁcients of divergence of one distri-bution from another.
journal of the royal statisticalsociety: series b (methodological), 28(1):131–142..yu bao, hao zhou, shujian huang, lei li, lilimou, olga vechtomova, xinyu dai, and jiajunchen.
2019. generating sentences from disentan-gled syntactic and semantic spaces.
arxiv preprintarxiv:1907.05789..maria barrett, yova kementchedjhieva, yanai elazar,desmond elliott, and anders søgaard.
2019. adver-sarial removal of demographic attributes revisited.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 6331–6336..mohamed ishmael belghazi, aristide baratin, sairajeswar, sherjil ozair, yoshua bengio, aaroncourville, and r devon hjelm.
2018. mine: mu-tual information neural estimation.
arxiv preprintarxiv:1801.04062..pengyu cheng, martin renqiang min, dinghanshen, christopher malon, yizhe zhang, yi-tong li, and lawrence carin.
2020b.
improv-ing disentangled text representation learning witharxiv preprintinformation-theoretic guidance.
arxiv:2006.00693..junyoung chung, caglar gulcehre, kyunghyun cho,and yoshua bengio.
2014. empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arxiv preprint arxiv:1412.3555..pierre colombo, emile chapuis, matteo manica, em-manuel vignon, giovanna varni, and chloe clavel.
2020. guiding attention in sequence-to-sequencemodels for dialogue act prediction.
in aaai, pages7594–7601..pierre colombo, wojciech witon, ashutosh modi,james kennedy, and mubbasir kapadia.
2019.arxiv preprintaffect-driven dialog generation.
arxiv:1904.02793..thomas m. cover and joy a. thomas.
2006. elementsof information theory (wiley series in telecommuni-cations and signal processing).
wiley-interscience,usa..y. bengio, a. courville, and p. vincent.
2013. rep-resentation learning: a review and new perspectives.
ieee transactions on pattern analysis and machineintelligence, 35(8):1798–1828..kamélia daudel, randal douc, and françois portier.
inﬁnite-dimensional gradient-based descent2020.for alpha-divergence minimisation.
working paperor preprint..su lin blodgett, lisa green, and brendan o’connor.
2016. demographic dialectal variation in socialmedia: a case study of african-american english.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages1119–1130, austin, texas.
association for compu-tational linguistics..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..christopher p burgess, irina higgins, arka pal, loicmatthey, nick watters, guillaume desjardins, andalexander lerchner.
2018. understanding disentan-gling in β-vae.
arxiv preprint arxiv:1804.03599..emile chapuis, pierre colombo, matteo manica,matthieu labeau, and chloé clavel.
2020. hierar-chical pre-training for sequence labelling in spokendialog.
in proceedings of the 2020 conference onempirical methods in natural language process-ing: findings, emnlp 2020, online event, 16-20november 2020, pages 2636–2648.
association forcomputational linguistics..pengyu cheng, weituo hao, shuyang dai, jiachangliu, zhe gan, and lawrence carin.
2020a.
club:a contrastive log-ratio upper bound of mutual in-formation.
in international conference on machinelearning, pages 1779–1788.
pmlr..emily l denton et al.
2017. unsupervised learningof disentangled representations from video.
in ad-vances in neural information processing systems,pages 4414–4423..tanvi dinkar, pierre colombo, matthieu labeau, andchloé clavel.
2020. the importance of ﬁllers forin pro-text representations of speech transcripts.
ceedings of the 2020 conference on empirical meth-ods in natural language processing, emnlp 2020,online, november 16-20, 2020, pages 7985–7993.
association for computational linguistics..md donsker and srs varadhan.
1985. large devia-tions for stationary gaussian processes.
communi-cations in mathematical physics, 97(1-2):187–210..yanai elazar and yoav goldberg.
2018. adversarialremoval of demographic attributes from text data.
arxiv preprint arxiv:1808.06640..clément feutry, pablo piantanida, yoshua bengio, andpierre duhamel.
2018. learning anonymized repre-sentations with adversarial neural networks..zhenxin fu, xiaoye tan, nanyun peng, dongyanzhao, and rui yan.
2017.style transfer intext: exploration and evaluation.
arxiv preprintarxiv:1711.06861..alexandre garcia, pierre colombo, slim essid, flo-rence d’alché buc, and chloé clavel.
2019. from.
6547the token to the review: a hierarchical multi-modal approach to opinion mining.
arxiv preprintarxiv:1908.11216..r devon hjelm, alex fedorov, samuel lavoie-marchildon, karan grewal, phil bachman, adamlearn-trischler, and yoshua bengio.
2018.informationing deep representations by mutualarxiv preprintestimation and maximization.
arxiv:1808.06670..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2019. the curious case of neural textdegeneration.
arxiv preprint arxiv:1904.09751..jun-ting hsieh, bingbin liu, de-an huang, li f fei-fei, and juan carlos niebles.
2018. learning to de-compose and disentangle representations for videoprediction.
in advances in neural information pro-cessing systems, pages 517–526..zhiting hu, zichao yang, xiaodan liang, ruslansalakhutdinov, and eric p xing.
2017. towardarxiv preprintcontrolled generation ofarxiv:1703.00955..text..yun-ning hung, yi-an chen, and yi-hsuan yang.
2018. learning disentangled representations fortimber and pitch in music audio.
arxiv preprintarxiv:1811.03271..parag jain, abhijit mishra, amar prakash azad, andkarthik sankaranarayanan.
2019. unsupervisedin proceedings ofcontrollable text formalization.
the aaai conference on artiﬁcial intelligence, vol-ume 33, pages 6554–6561..hamid jalalzai, pierre colombo, chloé clavel, ericgaussier, giovanna varni, emmanuel vignon, andanne sabourin.
2020. heavy-tailed representa-tions, text polarity classiﬁcation & data augmenta-tion.
arxiv preprint arxiv:2003.11593..vineet john, lili mou, hareesh bahuleyan, and olgavechtomova.
2018. disentangled representationlearning for non-parallel text style transfer.
arxivpreprint arxiv:1808.04339..armand joulin, edouard grave, piotr bojanowski,matthijs douze, hérve jégou, and tomas mikolov.
2016a.
fasttext.zip: compressing text classiﬁcationmodels.
arxiv preprint arxiv:1612.03651..armand joulin, edouard grave, piotr bojanowski,bag oftricksarxiv preprint.
and tomas mikolov.
2016b.
for efﬁcientarxiv:1607.01759..text classiﬁcation..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..samory kpotufe.
2017..lipschitz density-ratios,vol-structured data, and data-driven tuning.
ume 54 of proceedings of machine learning re-search, pages 1320–1328, fort lauderdale, fl,usa.
pmlr..klaus krippendorff.
2018. content analysis: an intro-.
duction to its methodology.
sage publications..taku kudo.
2018. subword regularization: improvingneural network translation models with multiple sub-word candidates.
arxiv preprint arxiv:1804.10959..vinay kumar verma, gundeep arora, ashish mishra,and piyush rai.
2018. generalized zero-shot learn-in proceedings ofing via synthesized examples.
the ieee conference on computer vision and patternrecognition, pages 4281–4289..guillaume lample, sandeep subramanian, eric smith,ludovic denoyer, marc’aurelio ranzato, and y-lan boureau.
2018. multiple-attribute text rewrit-ing.
in international conference on learning rep-resentations..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2015. a diversity-promoting objec-tive function for neural conversation models.
arxivpreprint arxiv:1510.03055..juncen li, robin jia, he he, and percy liang.
2018. delete, retrieve, generate: a simple approacharxiv preprintto sentiment and style transfer.
arxiv:1804.06437..ilya loshchilov and frank hutter.
2017. decou-pled weight decay regularization.
arxiv preprintarxiv:1711.05101..mehryar mohri, gary sivek, and ananda theerthasuresh.
2019. agnostic federated learning.
arxivpreprint arxiv:1902.00146..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
arxiv preprint arxiv:1807.03748..liam paninski.
2003. estimation of entropy and mu-tual information.
neural computation, 15(6):1191–1253..georg pichler, pablo piantanida, and günther kolian-der.
2020. on the estimation of information mea-sures of continuous distributions..matt post.
2018. a call for clarity in reporting bleu.
scores.
arxiv preprint arxiv:1804.08771..shrimai prabhumoye, yulia tsvetkov, ruslan salakhut-style trans-arxiv preprint.
dinov, and alan w black.
2018.ferthrough back-translation.
arxiv:1804.09000..justin b kinney and gurinder s atwal.
2014. eq-uitability, mutual information, and the maximal in-formation coefﬁcient.
proceedings of the nationalacademy of sciences, 111(9):3354–3359..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..6548colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..alfréd rényi et al.
1961. on measures of entropy andin proceedings of the fourth berke-information.
ley symposium on mathematical statistics and prob-ability, volume 1: contributions to the theory ofstatistics.
the regents of the university of califor-nia..frank rosenblatt.
1958. the perceptron: a probabilis-tic model for information storage and organizationin the brain.
psychological review, 65(6):386..eduardo hugo sanchez, mathieu serrurier, and math-ias ortner.
2019. learning disentangled represen-tations via mutual information estimation.
arxivpreprint arxiv:1912.03915..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
the journal of machine learningresearch, 15(1):1929–1958..sjoerd van steenkiste, francesco locatello, jürgenschmidhuber, and olivier bachem.
2019. are dis-entangled representations helpful for abstract visualreasoning?
in advances in neural information pro-cessing systems, pages 14245–14258..masashi sugiyama, taiji suzuki,.
and takafumikanamori.
2012. density ratio estimation in ma-chine learning, 1st edition.
cambridge universitypress, usa..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112..alexey tikhonov, viacheslav shibaev, aleksander na-gaev, aigul nugmanova, and ivan p yamshchikov.
2019. style transfer for texts: retrain, report er-rors, compare with rewrites.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3927–3936..tim van erven and peter harremos.
2014. rényi diver-gence and kullback-leibler divergence.
ieee trans-actions on information theory, 60(7):3797–3820..john wieting, jonathan mallinson, and kevin gim-pel.
2017. learning paraphrastic sentence embed-dings from back-translated bitext.
arxiv preprintarxiv:1706.01847..wojciech witon, pierre colombo, ashutosh modi, andmubbasir kapadia.
2018. disney at iest 2018: pre-dicting emotions using an ensemble.
in proceedingsof the 9th workshop on computational approachesto subjectivity, sentiment and social media analysis,wassa@emnlp 2018, brussels, belgium, october31, 2018, pages 248–253.
association for computa-tional linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, rémi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2019.huggingface’s transformers: state-of-the-art naturallanguage processing.
arxiv, abs/1910.03771..qizhe xie, zihang dai, yulun du, eduard hovy, andgraham neubig.
2017.controllable invariancethrough adversarial feature learning.
in advances inneural information processing systems, pages 585–596..bing xu, naiyan wang, tianqi chen, and mu li.
empirical evaluation of rectiﬁed activa-arxiv preprint.
2015.tions in convolutional network.
arxiv:1505.00853..ruochen xu, tao ge, and furu wei.
2019. formalitystyle transfer with hybrid textual annotations.
arxivpreprint arxiv:1903.06353..ivan p yamshchikov, viacheslav shibaev, aleksandernagaev, jürgen jost, and alexey tikhonov.
2019.decomposing textual information for style transfer.
arxiv preprint arxiv:1909.12928..xiaoyuan yi, zhenghao liu, wenhao li, and maosongsun.
2020. text style transfer via learning style in-in proceedings ofstance supported latent space.
the twenty-ninth international joint conference onartiﬁcial intelligence, ijcai-20, pages 3801–3807.
international joint conferences on artiﬁcial intelli-gence organization..muhammad bilal zafar,.
isabel valera, manuelgomez rodriguez, and krishna p gummadi.
2017.fairness beyond disparate treatment & disparate im-pact: learning classiﬁcation without disparate mis-treatment.
in proceedings of the 26th internationalconference on world wide web, pages 1171–1180..rich zemel, yu wu, kevin swersky, toni pitassi, andcynthia dwork.
2013. learning fair representations.
volume 28 of proceedings of machine learningresearch, pages 325–333, atlanta, georgia, usa.
pmlr..6549ye zhang, nan ding, and radu soricut.
2018. shaped:shared-private encoder-decoder for text style adap-tation.
arxiv preprint arxiv:1804.04093..yi zhang, tao ge, and xu sun.
2020. parallel data aug-mentation for formality style transfer.
arxiv preprintarxiv:2005.07522..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in proceedings of the ieee inter-national conference on computer vision, pages 19–27..6550