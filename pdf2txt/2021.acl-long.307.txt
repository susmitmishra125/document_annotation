point, disambiguate and copy: incorporating bilingual dictionaries forneural machine translation.
tong zhang1,2, long zhang1,2, wei ye1,†, bo li1,2,jinan sun1, xiaoyu zhu3, wen zhao1, shikun zhang1,†1 national engineering research center for software engineering, peking university2 school of software and microelectronics, peking university3 bigo{zhangtong17,zhanglong418, wye, zhangsk}@pku.edu.cn.
abstract.
this paper proposes a sophisticated neural ar-chitecture to incorporate bilingual dictionar-ies into neural machine translation (nmt)models.
by introducing three novel compo-nents: pointer, disambiguator, and copier,our method pdc achieves the following mer-its inherently compared with previous efforts:(1) pointer leverages the semantic informationfrom bilingual dictionaries, for the ﬁrst time,to better locate source words whose transla-tion in dictionaries can potentially be used;(2) disambiguator synthesizes contextual in-formation from the source view and the targetview, both of which contribute to distinguish-ing the proper translation of a speciﬁc sourceword from multiple candidates in dictionaries;(3) copier systematically connects pointer anddisambiguator based on a hierarchical copymechanism seamlessly integrated with trans-former, thereby building an end-to-end archi-tecture that could avoid error propagation prob-lems in alternative pipeline methods.
theexperimental results on chinese-english andenglish-japanese benchmarks demonstrate thepdc’s overall superiority and effectiveness ofeach component..1.introduction.
the past several years have witnessed the remark-able success of neural machine translation (nmt),due to the development of sequence-to-sequencemethods (sutskever et al., 2014; bahdanau et al.,2015; vaswani et al., 2017).
since bilingual dic-tionaries cover rich prior knowledge, especiallyof low-frequency words, many efforts have beendedicated to incorporating bilingual dictionariesinto nmt systems.
these explorations can beroughly categorized into two broad paradigms.
theﬁrst one transforms the bilingual dictionaries intopseudo parallel sentence pairs for training (zhang.
†corresponding authors..figure 1: three key steps to translate with a bilin-gual dictionary: pointing, disambiguating and copying.
this concrete illustrative example is chosen to conve-niently show the primary intuition behind our method..and zong, 2016; zhao et al., 2020).
the secondone utilizes the bilingual dictionaries as external re-sources fed into neural architectures (luong et al.,2015; gulcehre et al., 2016; arthur et al., 2016;zhang et al., 2017b; zhao et al., 2018a,b, 2019b),which is more widely used and the focus of thispaper..in practice, bilingual dictionaries usually containmore than one translation for a word.
from a high-level perspective, we believe there are three criti-cal steps to incorporate bilingual dictionaries intonmt models as shown in figure 1: (1) pointing toa source word whose translation in dictionaries willbe used at a decoding step, (2) disambiguating mul-tiple translation candidates of the source word fromdictionaries, and (3) copying the selected transla-tion into the target side if necessary.
note thatsome works assume that only one translation existsfor each word in dictionaries (luong et al., 2015;gulcehre et al., 2016).
in this simpliﬁed scenario,the disambiguating step is unnecessary, hence thepointing and copying step can be merged into asingle step similar to the classic copying mecha-nism (gu et al., 2016).
in more practical scenarios,however, this process suffers from the followingbottlenecks corresponding to each step..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3970–3979august1–6,2021.©2021associationforcomputationallinguistics3970thesepatternsincreasebrakefrictionbetween tiresandground.这些花纹可以增强轮胎与地面之间的制动摩擦thesepatterncanincreasetireandgroundbetweenofbrakefrictionsource input : decoderoutput :bilingualdictionary :disambiguaterubfrictionclashconflictcopyreference :these patternsincreasebrakepoint123摩擦:？(1) in the pointing step, semantic informationof translations in dictionaries is underutilized.
to locate source words whose translation in dic-tionaries may be used, some works (luong et al.,2015; gulcehre et al., 2016) use a classic copymechanism, but in an oversimpliﬁed scenario men-tioned above.
more recent efforts further lever-age statistics-based pre-processing methods (zhaoet al., 2018b, 2019b) to help identify, e.g., rare ortroublesome source words.
note that the goal of lo-cating a source word is to further use its translationin dictionaries.
intuitively, by exploring rich infor-mation of a source word’s translations in dictionar-ies, we can better understand the semantic meaningof the source word and distinguish whether we canits translation candidate.
unfortunately, this in-formation is underutilized by most works, whichcould have boosted nmt performance, as shownin section 5.2..(2) in the disambiguating step, the distin-guishing information is from static prior knowl-edge or coarse-grained context information.
toselect the proper translation of one source wordfrom multiple candidates in dictionaries, in addi-tion to works that merely use the ﬁrst-rank one(luong et al., 2015; gulcehre et al., 2016), exist-ing explorations mainly involve exploiting priorprobabilities, e.g., to adjust the distribution overthe decoding vocabulary (arthur et al., 2016; zhaoet al., 2018a).
as a representative context-baseddisambiguation method, zhao et al.
(2019b) dis-tinguish candidates by matching their embeddingswith a decoder-oriented context embedding.
intu-itively, an optimal translation candidate should notonly accurately reﬂect the content of the sourcesentence, but also be consistent with the contextof the current partial target sentence.
our obser-vation is that both source information and targetinformation is critical and complementary to distin-guish candidates.
taking the source word “摩擦”in figure 1 for example, the source context of “花纹/pattern”, “轮胎/tire” and “地面/ground” helpsto identify the candidates of “rub” and “friction”in the dictionary, and the target context of “thesepatterns increase brake” further makes “friction”the best choice.
this observation inspires us to syn-thesize source information and target informationin a more ﬁne-grained manner to improve previousstraightforward disambiguation methods..ambiguating step.
existing models usually donot explicitly emphasize a separate copying step 1,since it is a trivial task in their simpliﬁed or pipelinescenario.
however, to deliver a sophisticated end-to-end architecture that avoids error propagationproblems, the pointing and disambiguating stepmust be appropriately connected as well as inte-grated into mature nmt models.
the proposedcopying step is the right place to complete this job.
to address the above problems, we propose anovel neural architecture consisting of three novelcomponents: pointer, disambiguator, and copier,to effectively incorporate bilingual dictionaries intonmt models in an end-to-end manner.
pointeris a pioneering research effort on exploiting thesemantic information from bilingual dictionariesto better locate source words whose translation indictionaries may be used.
disambiguator synthe-sizes complementary contextual information fromthe source and target via a bi-view disambiguationmechanism, accurately distinguishing the propertranslation of a speciﬁc source word from mul-tiple candidates in dictionaries.
copier couplespointer and disambiguator based on a hierarchi-cal copy mechanism seamlessly integrated withtransformer, thereby building a sophisticated end-to-end architecture.
last but not least, we design asimple and effective method to integrate byte-pairencoding (bpe) with bilingual dictionaries in ourarchitecture.
extensive experiments are performedon chinese-english and english-japanese bench-marks, and the results verify the pdc’s overallperformance and effectiveness of each component..2 background: transformer.
transformer (vaswani et al., 2017) is the most pop-ular nmt architecture, which adopts the standardencoder-decoder framework and relies solely onstacked attention mechanisms.
speciﬁcally, givena source sequence x = {x1, x2..., xn}, the modelis supposed to generate the target sequence y ={y1, y2..., ym} in an auto-regressive paradigm.
transformer encoder.
a transformer encoder isconstituted by a stack of n identical layers, each ofwhich contains two sub-layers.
the ﬁrst is a multi-head self-attention mechanism (selfatt), and thesecond is a fully connected feed-forward network(ffn).
layer normalization (ln) (ba et al., 2016)and residual connection (he et al., 2016) is em-.
(3) a copying step is required to facilitate thecollaboration between the pointing step and dis-.
1note that previous works involve copy mechanism mainly.
correspond to the pointing step..3971figure 2: an overview of our methods.
the left is our pdc module as a copy mechanism, and the right is thevanilla transformer.
for each source word xi, we obtain a set of translation candidates {c(1)i } via a bilin-gual dictionary.
to better capture their semantics, candidate embeddings are shared with target embeddings andreﬁned with self-attention before interacting with transformer’s encoder states.
the state h(cid:48) enriched by candidatesemantics is utilized by pointer to locate source words whose dictionary translations may be used.
disambigua-tor generates two disambiguation distributions over translation candidates from the source view and target view,respectively.
finally, copier connects the outputs of pointer and disambiguator via a hierarchical copy operation.., ..., c(k).
i.ployed around the two sub-layers in both encoderand decoder..(1).
˜hl = ln(selfatt(hl−1) + hl−1),hl = ln(ffn(˜hl) + ˜hl),2..., hl.
1, hl.
where hl = {hln} is the output of the l-thlayer.
the ﬁnal output hn of the last encoder layerserves as the encoder state h.transformer decoder.
similarly, the decoder em-ploys the stack structure with n layers.
besidesthe two sub-layers, an additional cross attention(crossatt) sub-layer is inserted to capture the in-formation from the encoder..then, the translation probability p(yt|y<t, x) ofthe t-th target word is produced with a softmaxlayer:.
p(yt|y<t, x) ∝ exp(wost),.
(3).
where y<t is the proceeding tokens before yt..3 methodology.
in this section, we mathematically describe ourmodel in detail.
we follow the notations in sec-tion 2. ci = {c(1)i } denotes the translationcandidates of a source word xi, derived from abilingual dictionary d.., ..., c(k).
i.
˜sl = ln(selfatt(sl−1) + sl−1),(cid:98)sl = ln(crossatt(˜sl, h, h) + ˜sl),sl = ln(ffn((cid:98)sl) + (cid:98)sl),.
(2).
where sl is the output of the l-th decoder layer andthe ﬁnal output sn is taken as the decoder state s..3.1 overview.
an overview of the proposed pdc model is shownin figure 2. pdc aims to copy the correct trans-lation candidate of the correct source word at adecoding step.
following the classic copynet (guet al., 2016), our model consists of two parts, an.
3972source embeddingdictionary embeddingtarget embedding𝑥#ldc𝑥$𝑥%𝑥&𝑦#𝑦$𝑦%𝑦&self-attffnn×dec-enc-attffnself-attn×ℎ#ℎ$ℎ%ℎ&linear & softmax𝑠*𝑞𝛾-./0source encodercandidate encoderdecoder𝑎𝑡𝑡3𝑃567𝑃-./0𝛾-./01−𝛾-./0𝑃:;7<=sourcesentencetargetsentencetranslation candidatesℎ′#ℎ′$ℎ′%ℎ′&𝑠′*𝑑′$𝑑′%𝑑′&𝑑′#𝑎𝑡𝑡*,#a𝑞𝑐#($)𝑐#(%)𝑐#(#)𝑐$($)𝑐%($)𝑐%(%)𝑐%(&)𝑐$(#)𝑐%(#)𝑐&(#)𝑑#($)𝑑#(%)𝑑#(#)𝑞dic-enc-attself-attffn𝑑#($)𝑑#(%)𝑑#(#)𝑎𝑡𝑡e,#asource-viewtarget-viewpointerdisambiguatorvanilla transformercopierbilingual dictionaryencoder-decoder translator to produce the generat-ing probability and a copy mechanism to producethe copying probability.
the above two probabili-ties will collaborate to emit the ﬁnal probability..the procedure of our copy mechanism involvesthree critical components: (1) a pointer that selectsa source word whose translation candidates willpotentially be copied, (2) a disambiguator whichdistinguishes multiple translation candidates of thesource word to ﬁnd the optimal candidate to copy,and (3) a copier that generates copying probabilityby combining the outputs from the above two com-ponents hierarchically.
we will describe the detailsof each component in the following subsection..3.2 pointer.
the pointer aims to point which source word shouldbe translated at a decoding step.
we utilize the care-fully extracted semantic information of translationcandidates to promote pointing accuracy.
speciﬁ-cally, pointer ﬁrst extracts the semantic informationof candidates with candidate-wise encoding.
thenthe candidate representations of each source wordare fused and interacted with the source represen-tations from transformer encoder.
an attentionmechanism is applied on the reﬁned source repre-sentations to point which word to be translated.
candidate encoding.
we ﬁrst construct the can-didate representations di = {d(1)i } for thecandidates of xi, through an candidate embeddingmatrix and a single layer candidate encoder.
˜di = ln(selfatt(emb(ci)) + emb(ci)),di = ln(ffn(˜di) + ˜di).
note that this candidate-wise encoder exploits.
, ..., d(k).
(4).
i.the same structure as a source encoder layer.
pointing with candidate semantics.
previousdictionary-enhanced nmt systems usually directlyutilize encoder state h and the decoder state st at t-th decoding step to point whose translation shouldbe copied in the source sentence.
intuitively, trans-lation candidates’ information contributes to point-ing the right source word, while it is underutilizedpreviously.
accordingly, we propose to explorethe semantic information of translation candidatesin our pointer.
first, we fuse multiple translationcandidates’ representations of each word by an at-tention mechanism between hi and di..d(cid:48)i =.
i,j ·d(j)αsrc.
i.; αsrc.
i,j =.
k(cid:88).
j=1.
(cid:80)k.exp(hiwd(j)i )j(cid:48)=1 exp(hiwd(j(cid:48)))(5).
i.,.
where d(cid:48)i ∈ d(cid:48) is the fused representation for allcandidates of the source word xi.
next, the encoderstate h and d(cid:48) are interacted to reﬁne the represen-tations of source words with the carefully-extractedcandidate information.
the reﬁned encoder stateh(cid:48) can be formalized as:.
˜h(cid:48) = ln(crossatt(h(cid:48), d(cid:48), d(cid:48)) + h(cid:48)),h(cid:48) = ln(ffn( ˜h(cid:48)) + ˜h(cid:48))..(6).
then, we calculate the attention score to point.
which source word to be translated:.
s(cid:48)t =.
βi · h(cid:48).
i; βi =.
n(cid:88).
i=1.
exp(stwh(cid:48)i)i(cid:48)=1 exp(stwh(cid:48).
i(cid:48)).
(cid:80)n., (7).
where βi is the pointing probability for xi.
s(cid:48)notes the reﬁned decoder state..t de-.
3.3 disambiguator.
when translating a speciﬁc word, our model hasthe whole source sentence and the partial targetsentence as inputs.
an optimal translation candi-date should not only accurately reﬂect the contentof source sentence, but also be consistent with thecontext of the partial target sentence.
thus, we pro-pose a bi-view disambiguation module to select theoptimal translation candidate in both source viewand target view.
source-view disambiguation.
source-view dis-ambiguation chooses the optimal candidate foreach word with the context information storedin source sentence.
the attention score αsrci ={αsrci,k }, which has been calculated in equa-tion 5, is employed as the source-view disambiguat-ing distribution for the k translation candidates ofxi.
this disambiguating distribution is decoding-agnostic, which means it serve as global informa-tion during decoding.
target-view diambiguation.
as analyzed in sec-tion 1, translation candidates that seem proper fromthe source view may not well ﬁt in the target con-text.
thus, we also perform a target view dis-ambiguation to narrow down which candidates ﬁtthe partial target sentence’s context.
speciﬁcally,we leverage the reﬁned decoder state s(cid:48)t to disam-biguate the multiple candidates:.
i,1 , ..., αsrc.
αtgt.
i,j =.
exp(s(cid:48)j(cid:48)=1 exp(s(cid:48).
twdtd(j)i )twdtd(j(cid:48)).
i.,.
).
(cid:80)k.(8).
where αtgtability for c(j).
i,j is the target-view disambiguating prob-.
in contrast to the decoding-agnostic.
i.
3973source-view disambiguating probability, this target-view disambiguating probability varies during de-coding steps..3.4 copier.
finally, we combine the pointing distribution andthe bi-view disambiguating distributions in a hier-archical way to constitute the copying distributionas follows:.
αi,j = [ρ × αsrc.
i,j + (1 − ρ) × αtgt.
i,j ] × βi,.
(9).
where ρ is a scaling factor to adjust the contributionfrom source-view and target-view disambiguatingprobabilities.
αi,j indicates the probability to copyc(j), the j-th translation candidate of the i-th sourceiword.
we transform this positional probability intoword-level copying probability pcopy:.
pcopy = p(yt|y<t, x, c),.
(10).
where c is the entire translation candidates for allsource word in an instance..the ﬁnal probability pﬁnal is constituted by a.linear interpolation of pgen and pcopy:.
pﬁnal(yt|y<t, x, c) = γt × pcopy + (1 − γt) × pgen,(11)where pgen denotes the the generating probabilityfrom transformer, calculated in equation 3. γt isthe dynamic weight at step t, formalized by:.
γt = sigmoid(ws(cid:48).
t)..(12).
3.5 selective bpe.
bpe (sennrich et al., 2016) is commonly used innmt to deal with the rare words by separatingthem into frequent subwords.
however, it is non-trivial to incorporate bpe into nmt systems withcopy mechanism, because the split subwords maynot match the original word appearing in dictio-naries, either in source side or target side.
simplyapplying bpe on dictionary words will complicatesthe scenario to disambiguate and copy, since themodel needs to aggregate the representations ofthese subwords for disambiguation and copy thesubwords sequentially.
as revealed in section 5.4,the experimental results demonstrate that whetherapplying original bpe on dictionary words or notwill not yield promising results..in this paper, we present a simple and effec-tive strategy named selective bpe, which only per-forms bpe on all source words and a portion of.
target words.
all of the translation candidates fromthe dictionary remain intact.
concretely, in thetarget side, we keep the target word from beingseparated into subwords if we can copy it from thetranslation candidate set c of the source sentence.
such case is formalized as:.
(cid:40).
itgt(i) =.
1,0,.if yi ∈ cif yi /∈ c.,.
(13).
where itgt(i) is the bpe indicator for yi.
a tar-get word yi will be split by selective bpe only ifitgt(i) = 0. note that selective bpe is only usedin training, since the reference of validation setsand testing sets do not need bpe..by applying selective bpe, our model can im-plicitly exploit the information of which dictionarycandidates are likely to be copied.
thus, rare wordswill be more inclined to be copied directly as awhole from the dictionary..4 experimental settings.
in this section, we elaborate on the experimentsetup to evaluate our proposed model..4.1 datasets.
we test our model on chinese-to-engish (zh-en)and english-japanese (en-ja) translation tasks..for zh-en translation, we carry out experimentson two datesets.
we use 1.25m sentence pairsfrom news corpora ldc as the training set 1. weadopt nist 2006 (mt06) as validation set.
nist2002, 2003, 2004, 2005, 2008 datasets are usedfor testing.
besides, we use the ted talks corpusfrom iwslt 2014 and 2015 (cettolo et al., 2012)including 0.22m sentence pairs for training.
weuse dev2010 with 0.9k sentence pairs for develop-ment and tst2010-2013 with 5.5k sentence pairsfor testing..for en-ja translation, we adopt wikipedia articledataset kftt2, which contains 0.44m sentencepairs for training, 1.2k sentence pairs for validationand 1.2k sentence pairs for testing..the bilingual dictionary we used is constructedby the open-source cross-lingual word translatedataset word2word (choe et al., 2020).
we limitthe maximum number of translation candidates to5 for each source word..1the training set includes ldc2002e18, ldc2003e07,of ldc2004t07,.
portion.
ldc2003e14, hansardsldc2004t08 and ldc2005t06..2http://www.phontron.com/kftt/.
3974systems.
mt06 mt02 mt03 mt04 mt05 mt08.
∆.
(cheng et al., 2019)(yang et al., 2020)(yan et al., 2020).
transformersingle-copyflat-copy.
pdc46.74pdc(w/o dict-pointer） 45.7945.80pdc(w/o tgt-view)pdc(w/o src-view)45.97.
46.9544.6947.80.
44.1145.0444.93.exsisting nmt systems.
47.06-47.72.
46.4846.5646.60.baseline nmt systems.
46.3847.2146.33.
45.0546.4746.26our nmt systems48.4347.8147.9147.90.
48.8547.5847.4347.42.
47.39-48.30.
47.0747.4846.83.
48.5747.9848.4947.92.
46.5846.04-.
44.8245.4545.38.
47.7146.3246.8147.07.
37.3837.5338.70.
34.7436.0835.19.
37.4536.5336.9936.81.
---.
ref+0.93+0.39.
+2.59+1.63+1.91+1.81.
table 1: the main results of nist zh-en task.
∆ shows the average bleu improvements over the test setscompared with transformer (ref ).
the results of our models signiﬁcantly outperform transformer (p < 0.01)..4.2 details for training and evaluation.
we implement our model on top of thumt(zhang et al., 2017a) toolkit.
the dropout rateis set to be 0.1. the size of a mini-batch is 4096.we share the parameters in target embeddings andthe output matrix of the transformer decoder.
theother hyper-parameters are the same as the defaultsettings in vaswani et al.
(2017).
the optimalvalue scaling factor ρ in bi-view disambiguationis 0.4. all these hyper-parameters are tuned onthe validation set.
we apply bpe (sennrich et al.,2016) with 32k merge operations.
the best sin-gle model in validation is used for testing.
we usemulti−bleu.perl3 to calculate the case-insensitive4-gram bleu..4.3 baselines.
our models and the baselines use bpe in experi-ments by default.
we compare our pdc with thefollowing baselines:.
• transformer is the most widely-used nmtsystem with self-attention (vaswani et al.,2017)..• single-copy is a transformer-based copymechanism that select a source word’s ﬁrst-rank translation candidate exactly followingluong et al.
(2015); gulcehre et al.
(2016)..• flat-copy is a novel copy mechanism to per-form automatic post-editing (ape) proposed.
3https://github.com/moses-smt/mosesdecoder/blob/.
master/scripts/generic/multi-bleu.perl.
by huang et al.
(2019).
note that ape fo-cuses on copying from a draft generated bya pre-trained nmt system.
we ﬁrst arrangecandidates of all source words into a sequenceas a draft and then copy this ﬂattened “draft”following huang et al.
(2019)..5 experiment results.
5.1 main results.
table 1 shows the performance of the baseline mod-els and our method variants.
we also list severalexisting robust nmt systems reported in previouswork to validate pdc’s effectiveness.
by investi-gating the results in table 1, we have the followingfour observations..first, compared with existing state-of-the-artnmt systems, pdc achieves very competitive re-sults, e.g., the best bleu scores in 4 of the 5 testsets..second, single-copy outperforms transformer,indicating that even incorporating only the ﬁrst-rank translation candidate can improve nmt mod-els.
however, since single-copy disregards manytranslation candidates in dictionaries, which couldhave been copied, the improvement is relativelysmall (e.g., +0.93 of average bleu score on thetest sets)..third, the performance of flat-copy is evenworse than single-copy, though it considers alltranslation candidates in dictionaries.
the reasonlies in that flat-copy ignores the hierarchy formedby a source sentence and the corresponding trans-lation candidates of its each word, making it much.
3975strategies.
nonestandarddictselective.
dev.
bpe targetdict src tgt(cid:55)(cid:55)43.94(cid:51) 45.16(cid:51)(cid:51)(cid:51) 45.71(cid:51)s46.74.
(cid:55)(cid:55)(cid:51)(cid:55).
testavg43.6844.7544.8446.20.table 2: the bleu scores of different bpe strategies.
for a bpe target (dict means dictionary words, srcmeans source words, and tgt means target words).
(cid:51),(cid:55) and s denote applying bpe, not applying bpe, andapplying selective bpe, respectively..the source-view and target-view disambiguation,we analyze the impact of the hyper-parameter ρ,which denotes how to weight the disambiguationdistribution generated from source-view and target-view.
in figure 3, the orange polyline showsthe bleu scores on the development set (mt06),and the blue polyline shows average bleu scoreson another ﬁve test sets.
by looking into thesetwo polylines’ trends, we ﬁnd that pdc is best-performed when ρ is 0.4, indicating neither thesource view nor the target view can be ignored oroverly dependent..these ﬁndings prove that both views’ contextualinformation is critical and complementary to iden-tify a speciﬁc source word’s proper translation, andour disambiguator synthesizes them effectively..5.4 effectiveness of selective bpe.
we demonstrate the effects of different bpe strate-gies in table 2, where none does not use bpeat all, standard adopts the same bpe strategy asdictionary-independent nmt models, dict sim-ply apply bpe to dictionary candidates in addi-tion to standard bpe, and selective is our selectivebpe.
more detailed settings of each strategy can befound in table 2, from which we can also clearlyobserve the superiority of our selective bpe strat-egy.
we attribute this superiority to the ﬁne-grainedcollaboration between selective bpe and dictio-naries, which implicitly yet effectively leveragingthe information of which dictionary candidate arelikely to be copied..it is worth mentioning that selective bpe on thetarget side will not prevent overcoming morpho-logical variance compared with standard bpe.
amorphologically inﬂected target word can be gener-ated in two ways in our system.
firstly, if the targetword is not in the candidate set, we will performstandard bpe decomposition.
in this scenario, se-.
figure 3: the effect of hyper-parameter ρ on nist zh-en translation task..more challenging to identify the proper candidateto be copied..finally, pdc substantially outperforms single-copy and flat-copy, with improvements of 1.66and 2.20 average bleu points, due to our effec-tive hierarchical copy mechanism that connects thepointer and the disambiguator, which will be fur-ther analyzed in the next sections..5.2 effectiveness of pointer.
what distinguishes our pointer from its counter-parts of other nmt models is the utilization ofsemantic information of translation candidates indictionaries.
to verify the effectiveness of thistechnical design, we implement a pdc variantnamed pdc(w/o dict-pointer) whose pointer lo-cates source words based on the encoder state (h)of the vanilla transformer instead of the dictionary-enhanced encoder state (h(cid:48)).
so the semantic infor-mation from dictionaries is not incorporated intothe pointing step..as expected, the performance of pdc(w/o dict-pointer) demonstrates a decrement of nearly 1.0average bleu score on the test sets compared withpdc, verifying the promising effect of pointer.
theresults also justify our intuition that the rich infor-mation of source words’ translations in dictionarieshelps to point the proper source word..5.3 effectiveness of disambiguator.
to investigate the effectiveness of our bi-view dis-ambiguator, we implement another two model vari-ants: pdc(w/o src-view) that is removed source-view disambiguation and pdc(w/o tgt-view) thatis removed target-view disambiguation.
as table1 shows, the performance of both models signiﬁ-cantly decrease..to further investigate the collaboration between.
39760.10.20.30.40.50.645.2545.5045.7546.0046.2546.5046.7547.00bleu45.6945.7046.0146.2046.0545.7846.0046.1646.2846.7446.5246.08devtest-avgpairs and domains (e.g., news, speech and wiki)..methodtransformerpdc.
iwslt kftt30.1219.2632.1820.71.table 3: results on the tasks of iwslt zh-en transla-tion and kftt en-ja translation..6 related work.
6.1 dictionary-enhanced nmt.
due to the rich prior information of parallel wordpairs in bilingual dictionaries, many researchershave dedicated efforts to incorporating bilingualdictionaries into nmt systems.
they either gener-ate pseudo parallel sentence pairs based on bilin-gual dictionaries to boost training (zhang andzong, 2016; zhao et al., 2020), or exploit the bilin-gual dictionaries as external resources fed into neu-ral networks (luong et al., 2015; gulcehre et al.,2016; arthur et al., 2016; zhang et al., 2017b; zhaoet al., 2018a,b, 2019b).
our work can be catego-rized into the second direction, and focus on im-proving the overall process of incorporating bilin-gual dictionaries as external knowledge into thelatest nmt systems..in particular, luong et al.
(2015); gulcehreet al.
(2016) ﬁrst employed copy mechanism (guet al., 2016) into nmt to address rare words prob-lem with one-to-one external bilingual dictionaries.
arthur et al.
(2016); zhao et al.
(2018a) exploitedthe prior probabilities from external resource toadjust the distribution over the decoding vocabu-lary.
(zhao et al., 2018b, 2019b) leverage statistics-based pre-processing method to ﬁlter out trouble-some words and perform disambiguation on multi-ple candidates.
our work extends the above ideasand reforms the overall process into a novel end-to-end framework consisting of three steps: pointing,disambiguating, and copying..6.2 copynet.
copynet is also widely used in text summarization(see et al., 2017; zhu et al., 2020), automatic post-editing (huang et al., 2019), grammar correction(zhao et al., 2019a) and so on..from a high-level perspective, our methodsshare a similar transformer-based architecture withhuang et al.
(2019) and zhu et al.
(2020).
huanget al.
(2019) employed copynet to copy from adraft generated by a pre-trained nmt system.
zhu.
figure 4: performance of transformer and pdc oneach subset with different rare word proportions.
theﬁgure is plotted based on the mt02 test set results..lective bpe is the same as standard bpe, and thetarget word will be generated in a standard way.
otherwise, if the target word is in the candidate set,it will not be decomposed and our method will en-courage the model to copy this word directly.
thus,the morphological variance problem can be simplysolved by copying..5.5 alleviation of the rare words problem.
we notice that most dictionary-based nmt worksaim to address the rare words problem.
thoughour work focuses on improving the overall processof incorporating dictionary information as externalknowledge, we also conduct a rough experimentto see how our method alleviates the rare wordsproblem..speciﬁcally, we treat a source word as a rareword if it appears less than ten times in the trainingset.
then we split the test set into subsets accordingto the rare word proportions of source sentences.
the performance on the subsets is shown in figure4. we ﬁnd that pdc outperforms transformer by alarger gap on the test subsets with more rare words(e.g., 7.18 for the proportion greater than 0.15),demonstrating that pdc can well alleviate the rarewords issue.
this observation is also consistentwith previous investigations (luong et al., 2015)..5.6 results on iwslt and kftt.
to verify pdc’s generalization capability, we fur-ther conduct experiments on the iwslt zh-entranslation task and kftt en-ja translation task.
due to space limitations, here we only report theperformance of pdc and transformer.
pdc’s su-periority can be easily observed from the resultsin table 3, indicating that pdc can be effectivelyapplied in translation tasks of different language.
39770(0,0.05](0.05,0.1](0.1,0.15](0.15,1]proportion303540455055bleu50.5247.5944.9236.6732.9452.2750.5446.3040.5240.12transformerpdcet al.
(2020) proposed a method that integratesthe operation of attending, translating, and sum-marizing to do cross-lingual summarization.
whatdistinguishes our pdc from other copy-based ar-chitectures lies in that the three novel components(pointer, disambiguator and copier) and the se-lective bpe strategy can make full and effectiveuse of dictionary knowledge..7 conclusion.
we have presented pdc, a new method to incor-porate bilingual dictionaries into nmt models,mainly involving four techniques.
(1) by inte-grating semantic information of dictionaries, theenhanced context representations help to locatesource words whose dictionary translations willpotentially be used.
(2) the source and target infor-mation is well synthesized and contribute to iden-tifying the optimal translation of a source wordamong multiple dictionary candidates, in a com-plementary way.
(3) the above two steps are thensystematically integrated based on a hierarchicalcopy mechanism.
(4) we ﬁnally equip the architec-ture with a novel selective bpe strategy carefully-designed for dictionary-enhanced nmt..experiments show that we achieve competi-tive results on the chinese-english and english-japanese translation tasks, verifying that our ap-proach favorably incorporates prior knowledge ofbilingual dictionaries..acknowledgements.
we thank anonymous reviewers for valuable com-ments.
this research was supported by the na-tional key research and development programof china under grant no.2019yfb1405802 andthe central government guided local science andtechnology development fund projects (science andtechnology innovation base projects) under grantno.206z0302g..references.
philip arthur, graham neubig, and satoshi nakamura.
incorporating discrete translation lexicons2016.into neural machine translation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 1557–1567..lei jimmy ba, jamie ryan kiros, and geoffrey e.corr,.
layer normalization..hinton.
2016.abs/1607.06450..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015..mauro cettolo, christian girardi, and marcello fed-erico.
2012. wit3: web inventory of transcribed andtranslated talks.
in proceedings of the 16th annualconference of the european association for machinetranslation, pages 261–268..yong cheng, lu jiang, and wolfgang macherey.
2019.robust neural machine translation with doubly ad-versarial inputs.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 4324–4333..yo joong choe, kyubyong park, and dongwoo kim.
2020. word2word: a collection of bilingual lexi-in proceedings ofcons for 3,564 language pairs.
the 12th language resources and evaluation con-ference, pages 3036–3045..jiatao gu, zhengdong lu, hang li, and victor okincorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640..caglar gulcehre, sungjin ahn, ramesh nallapati,bowen zhou, and yoshua bengio.
2016. pointingthe unknown words.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 140–149..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..xuancheng huang, yang liu, huanbo luan, jingfangxu, and maosong sun.
2019. learning to copyin proceedings of thefor automatic post-editing.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 6124–6134..minh-thang luong, ilya sutskever, quoc le, oriolvinyals, and wojciech zaremba.
2015. addressingthe rare word problem in neural machine translation.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages11–19..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computational.
3978yang zhao, jiajun zhang, zhongjun he, chengqingzong, and hua wu.
2018b.
addressing troublesomein proceed-words in neural machine translation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 391–400..yang zhao, jiajun zhang, yu zhou, and chengqingzong.
2020. knowledge graphs enhanced neuralmachine translation.
in proceedings of the twenty-ninth international joint conference on artiﬁcial in-telligence, ijcai 2020, pages 4039–4045.
ijcai.org..yang zhao, jiajun zhang, chengqing zong, zhongjunhe, and hua wu.
2019b.
addressing the under-translation problem from the entropy perspective.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 451–458..junnan zhu, yu zhou, jiajun zhang, and chengqingzong.
2020. attend, translate and summarize: anefﬁcient method for neural cross-lingual summariza-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 1309–1321..linguistics (volume 1: long papers), pages 1073–1083..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
advances in neural information processing systems,27:3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..jianhao yan, fandong meng, and jie zhou.
2020.multi-unit transformers for neural machine transla-in proceedings of the 2020 conference ontion.
empirical methods in natural language processing(emnlp), pages 1047–1059..jian yang, shuming ma, dongdong zhang, zhoujunimproving neural ma-li, and ming zhou.
2020.inchine translation with soft template prediction.
proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5979–5989..jiacheng zhang, yanzhuo ding, shiqi shen, yongcheng, maosong sun, huanbo luan, and yangthumt: an open source toolkitliu.
2017a.
arxiv preprintfor neural machine translation.
arxiv:1706.06415..jiacheng zhang, yang liu, huanbo luan, jingfang xu,and maosong sun.
2017b.
prior knowledge inte-gration for neural machine translation using poste-in proceedings of the 55th an-rior regularization.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1514–1523..jiajun zhang and chengqing zong.
2016. bridgingneural machine translation and bilingual dictionaries.
corr, abs/1610.07272..wei zhao, liang wang, kewei shen, ruoyu jia, andjingming liu.
2019a.
improving grammatical errorcorrection via pre-training a copy-augmented archi-tecture with unlabeled data.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 156–165..yang zhao, yining wang,.
andchengqing zong.
2018a.
phrase table as recommen-dation memory for neural machine translation.
inproceedings of the 27th international joint confer-ence on artiﬁcial intelligence, pages 4609–4615..jiajun zhang,.
3979