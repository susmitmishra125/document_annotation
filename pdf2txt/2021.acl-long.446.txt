transfer learning for sequence generation:from single-source to multi-source.
xuancheng huang1, jingfang xu4, maosong sun1,3, and yang liu1,2,3∗1dept.
of comp.
sci.
& tech., bnrist center, institute for ai, tsinghua university2institute for ai industry research, tsinghua university, beijing, china3beijing academy of artiﬁcial intelligence4sogou inc., beijing, china.
abstract.
multi-source sequence generation (msg) isan important kind of sequence generationtasks that takes multiple sources,includingautomatic post-editing, multi-source transla-tion, multi-document summarization, etc.
asmsg tasks suffer from the data scarcity prob-lem and recent pretrained models have beenproven to be effective for low-resource down-stream tasks, transferring pretrained sequence-to-sequence models to msg tasks is essential.
although directly ﬁnetuning pretrained mod-els on msg tasks and concatenating multiplesources into a single long sequence is regardedas a simple method to transfer pretrained mod-els to msg tasks, we conjecture that the di-rect ﬁnetuning method leads to catastrophicforgetting and solely relying on pretrained self-attention layers to capture cross-source infor-mation is not sufﬁcient.
therefore, we proposea two-stage ﬁnetuning method to alleviate thepretrain-ﬁnetune discrepancy and introduce anovel msg model with a ﬁne encoder to learnbetter representations in msg tasks.
experi-ments show that our approach achieves newstate-of-the-art results on the wmt17 apetask and multi-source translation task using thewmt14 test set.
when adapted to document-level translation, our framework outperformsstrong baselines signiﬁcantly.1.
1.introduction.
thanks to the continuous representations widelyused across text, speech, and image, neural net-works that accept multiple sources as input havegained increasing attention in the community (iveet al., 2019; dupont and luettin, 2000).
for ex-ample, multi-modal inputs that are complementaryhave proven to be helpful for many sequence gener-ation tasks such as question answering (antol et al.,.
∗corresponding author: yang liu1the source code is available at https://github..com/thunlp-mt/trice.
pretraining.
single-source sg.
multi-source sg.
autoencoding(e.g., bert).
bert-fused(zhu et al., 2019).
dualbert(correia and martins, 2019).
seq2seq(e.g., bart).
mbart-trans(liu et al., 2020).
this work.
table 1: comparison of various approaches to trans-ferring pretrained models to single-source and multi-source sequence generation tasks.
different fromprior studies,transferring pre-trained sequence-to-sequence models to multi-sourcesequence generation tasks..this work aims at.
2015), machine translation (huang et al., 2016),and speech recognition (dupont and luettin, 2000).
in natural language processing, multiple textualinputs have also been shown to be valuable for se-quence generation tasks such as multi-source trans-lation (zoph and knight, 2016), automatic post-editing (chatterjee et al., 2017), multi-documentsummarization (haghighi and vanderwende, 2009),system combination for nmt (huang et al., 2020),and document-level machine translation (wanget al., 2017).
we refer to this kind of tasks asmulti-source sequence generation (msg)..unfortunately, msg tasks face a severe chal-lenge: there are no sufﬁcient data to train msgmodels.
for example, multi-source translationrequires parallel corpora involving multiple lan-guages, which are usually restricted in quantity andcoverage.
recently, as pretraining language modelsthat take advantage of massive unlabeled data haveproven to improve natural language understanding(nlu) and generation tasks substantially (devlinet al., 2019; liu et al., 2019; lewis et al., 2020), anumber of researchers have proposed to leveragepretrained language models to enhance msg tasks(correia and martins, 2019; lee et al., 2020; lee,2020).
for example, correia and martins (2019)show that pretrained autoencoding (ae) models.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5738–5750august1–6,2021.©2021associationforcomputationallinguistics5738figure 1: overview of our framework.
“a”, “b”, and “c” denote sentences in different languages.
after beingpretrained on unlabeled data, the single-source sequence generation (ssg) model is ﬁnetuned on single-sourcelabeled data.
then, the ssg model is extended to the msg model by adding a ﬁne encoder upon the pretrainedencoder (i.e., the coarse encoder).
finally, the msg model is ﬁnetuned on the multi-source data.
the proposedframework aims to reduce the pretrain-ﬁnetune discrepancy and learn better multi-source representations..like bert (devlin et al., 2019) can improve auto-matic post-editing..as most recent pretrained sequence-to-sequence(seq2seq) models (song et al., 2019; lewis et al.,2020; liu et al., 2020) have demonstrated their ef-fectiveness in improving single-source sequencegeneration (ssg) tasks, we believe that pretrainedseq2seq models can potentially bring more bene-ﬁts to msg than pretrained ae models.
althoughit is easy to transfer seq2seq models to ssg tasks,transferring them to msg tasks is challenging be-cause msg takes multiple sources as the input,leading to severe pretrain-ﬁnetune discrepancies interms of both architectures and objectives..a straightforward solution is to concatenate therepresentations of multiple sources as suggested bycorreia and martins (2019).
however, we believethis approach suffers from two major drawbacks.
first, due to the discrepancy between pretrainingand msg, directly transferring pretrained modelsto msg tasks might lead to catastrophic forgetting(mccloskey and cohen, 1989; kirkpatrick et al.,2017) that results in reduced performance.
second,the pretrained self-attention layers might not fullylearn the representations of the concatenation ofmultiple sources because they do not make full useof the cross-source information..inspired by adding intermediate tasks for nlu(pruksachatkun et al., 2020; vu et al., 2020), weconjecture that inserting a proper intermediatetask between them can alleviate the discrepancy.
in this paper, we propose a two-stage ﬁnetuningmethod named gradual ﬁnetuning.
different fromprior studies, our work aims to transfer pretrainedseq2seq models to msg (see table 1).
our ap-proach ﬁrst transfers from pretrained models to.
ssg and then transfers from ssg to msg (seefigure 1).
furthermore, we propose a novel msgmodel with coarse and ﬁne encoders to differenti-ate sources and learn better representations.
on topof a coarse encoder (i.e., the pretrained encoder),a ﬁne encoder equipped with cross-attention lay-ers (vaswani et al., 2017) is added.
we refer toour approach as trice (a task-agnostic transfer-ring framework for multi-source sequence gener-ation), which achieves new state-of-the-art resultson the wmt17 ape task and the multi-sourcetranslation task using the wmt14 test set.
whenadapted to document-level translation, our frame-work outperforms strong baselines signiﬁcantly..2 approach.
figure 1 shows an overview of our framework.
first, the problem statement is described in section2.1. second, we propose to use the gradual ﬁne-tuning method (section 2.2) to reduce the pretrain-ﬁnetune discrepancy.
third, we introduce our msgmodel, which consists of the coarse encoder (sec-tion 2.3), the ﬁne encoder (section 2.4), and thedecoder (section 2.5)..2.1 problem statement.
as shown in figure 1, there are three kinds ofdataset: (1) the unlabeled multilingual dataset dpcontaining monolingual corpora in various lan-guages, (2) the single-source parallel dataset dsinvolving multiple language pairs, and (3) the multi-source parallel dataset dm.
the general objectiveis to leverage these three kinds of dataset to im-prove multi-source sequence generation tasks..formally, let x1:k = x1 .
.
.
xk be k source sen-tences, where xk is the k-th sentence.
we use xk,i.
5739single-source finetuningtransferabcbctransferon labeled data for ssgmulti-source finetuningon labeled data for msgpretrainingon unlabeled data for ssgdecodercoarse-enc.fine-enc.decoderencoderacdecoderencoderc!cdecoderencoderb!bdecoderencodera!adecoderencoderto denote the i-th word in the k-th source sentenceand y = y1 .
.
.
yj to denote the target sentencewith j words.
the msg model is given by.
pm(y|x1:k; θ) =.
p (yj|x1:k, y<j; θ),.
(1).
j(cid:89).
j=1.
is.
where yjthe j-th word in the target,y<j = y1 .
.
.
yj−1 is a partial target sentence,p (yj|x1:k, y<j; θ) is a word-level generationprobability, and θ are the parameters of the msgmodel..2.2 gradual finetuning.
as training neural models on large-scale unlabeleddatasets is time-consuming, it is a common practiceto utilize pretrained models to improve downstreamtasks by using transfer learning methods (devlinet al., 2019).
as a result, we focus on leveragingsingle-source and multi-source parallel datasets totransfer pretrained seq2seq models to msg tasks.
curriculum learning (bengio et al., 2009) aimsto learn from examples organized in an easy-to-hard order, and intermediate tasks (pruksachatkunet al., 2020; vu et al., 2020) are introduced to al-leviate the pretrain-ﬁnetune discrepancy for nlu.
inspired by these studies, we expect that chang-ing the training objective from pretraining to msggradually can reduce the difﬁculty of transferringpretrained models to msg tasks.
therefore, wepropose a two-stage ﬁnetuning method named grad-ual ﬁnetuning.
the transferring process is dividedinto two stages (see figure 1).
in the ﬁrst stage,the ssg model is transferred from denoising auto-encoding to the single-source sequence generationtask, and the model architecture is kept unchanged.
in the second stage, an additional ﬁne encoder (seesection 2.4) is introduced to transform the ssgmodel to the msg model, and the msg model isoptimized on the multi-source parallel corpus..formally, we use φp to denote the parametersof the ssg model.
without loss of generality, thepretraining process can be described as follows:.
and ˆφp are the learned parameters.
in this way,a powerful multilingual model is obtained by pre-training on the unlabeled multilingual dataset dp.
then, in the ﬁrst ﬁnetuning stage, let φs be theparameters of the ssg model, which are initializedby ˆφp.
as the single-source parallel dataset dsis not always available, we can build it from thek-source parallel dataset dm.
assume (cid:104)x1:k, y(cid:105)is a training example in dm, a training example(cid:104)x, y(cid:105) in ds can be constructed by sampling onesource from each k-source training example witha probability of 1/k.
the ﬁrst ﬁnetuning processis given by.
ls(φs) =.
− log ps(y|x; φs).
1|ds|.
(cid:88).
(cid:16).
(cid:104)x,y(cid:105)∈ds.
ˆφs = argmin.
ls(φs).
(cid:110).
(cid:111),.
φs.
where ˆφs are the learned parameters.
the learnedssg model is capable of taking inputs in multiplelanguages..in the second ﬁnetuning stage, φm, the param-eters of the coarse encoder, the decoder, and theembeddings, are initialized by ˆφs while γ are therandomly initialized parameters of the ﬁne encoder.
thus, θ = φm ∪ γ are the parameters of the msgmodel.
the second ﬁnetuning process can be de-scribed as.
lm(θ)1|dm|.
=.
(cid:88).
(cid:16).
(cid:104)x1:k ,y(cid:105)∈dm.
− log pm(y|x1:k; θ).
ˆθ = argmin.
(cid:110).
lm(θ).
(cid:111),.
θ.where pm is given by eq.
(1).
as a result, themodel is expected to learn from abundant unlabeleddata and perform well on the msg task.
in thefollowing subsections, we will describe the msgmodel architecture (see figure 2) applied in thesecond ﬁnetuning stage..(cid:17).
,.
(4).
(5).
(cid:17).
,.
(6).
(7).
lp(φp) =.
− log ps(z|˜z; φp).
(2).
(cid:17).
,.
2.3.input representation and the coarseencoder.
1|dp|.
(cid:88).
(cid:16).
z∈dp(cid:110).
φp.
ˆφp = argmin.
lp(φp).
(cid:111),.
(3).
in general, pretrained encoders are considered asstrong feature extractors to learn meaningful rep-resentations (zhu et al., 2019).
for this reason,correia and martins (2019) propose to use the pre-trained multilingual encoder to encode the bilin-gual input pair of ape.
since msg tasks usually.
where z is a sentence that could be in many lan-guages, ˜z is the corrupted sentence obtained fromz, ps is the probability modeled by the ssg model,.
5740figure 2: the architecture of our framework.
multiple sources are ﬁrst concatenated and encoded by the coarseencoder and then encoded by the ﬁne encoder to capture ﬁne-grained cross-source information.
finally, the repre-sentations are utilized by the decoder to generate the target sentence.
for simplicity, this ﬁgure only illustrates thesituation that the input contains two sources (k = 2)..have multiple sources involving different languagesand pretrained multilingual seq2seq models likembart (liu et al., 2020) usually rely on specialtokens (e.g., <en>) to differentiate languages, con-catenating multiple sources into a single long sen-tence will make the model confused about the lan-guage of the concatenated sentence (see table 6).
therefore, we propose to add additional segmentembedding to differentiate sentences in differentlanguages and encode source sentences jointly bya single pretrained multilingual encoder..formally, the input representation can be de-.
noted by.
xk,i = etok[xk,i] + epos[i] + eseg[k],.
(8).
where xk,i is the input representation of the i-th word in the k-th source sentence, and etok,epos, and eseg are the token, position, and seg-ment/language embedding matrices, respectively.
etok and epos are initialized by pretrained embed-ding matrices.
eseg is implemented as constant si-nusoidal embeddings (vaswani et al., 2017), whichis denoted by eseg[k]2i = sin(1000∗k/100002i/d),where eseg[k]2i+1 is similar to eseg[k]2i and i isthe dimension index while d is model dimension.2.
2if.
the pretrained model already contains the seg-ment/language embedding matrix, then the pretrained oneis used..then, the pretrained encoder is utilized to encode.
multiple sources:.
r(i).
1:k = ffn.
(cid:16).
selfatt.
(cid:16).
r(i−1)1:k.(cid:17)(cid:17).
,.
(9).
where selfatt(·) and ffn(·) are the self-attentionand feed-forward networks, respectively.
r(i)1:kis the representation output by the i-th encoderlayer, and r(0)1:k refers to x1 .
.
.
xk, where xk isequivalent to xk,1 .
.
.
xk,ik and ik is the numberof tokens in the k-th source sentence..however, we conjecture that indiscriminatelymodeling dependencies between words by the pre-trained self-attention layers cannot capture cross-source information adequately.
to this end, weregard the pretrained encoder as the coarse encoderand introduce a novel ﬁne encoder to learn bettermulti-source representations..2.4 the fine encoder.
to alleviate the pretrain-ﬁnetune discrepancy, weadopt the gradual ﬁnetuning method to better trans-fer from single-source to multi-source.
in theﬁrst ﬁnetuning step, the coarse encoder is usedto encode different sources individually.
as multi-ple sources are concatenated as a single source inwhich words interact by pretrained self-attentions,we conjecture that the cross-source information.
5741kvi like music .
</s> <en>ich mag musik.
</s> <de>initialized with pretrained modelsrandomly initialized self-attffnself-attcross-attffncross-attself-attself-attffnffn<fr>j’aimela musique .
</s>coarse encoderfine encoderdecodermean poolingcross-attcross-attsegment embeddingssplitqqkvkvqqpositional embeddingstoken embeddingspositional embeddingstoken embeddingslinear & softmaxj’aimela musique .
</s> <fr>cn´dn´fn´cannot be fully captured.
hence, we propose to adda randomly initialized ﬁne encoder, which consistsof self-attentions, cross-attentions, and ffns, ontop of the pretrained coarse encoder to learn mean-ingful multi-source representations.
speciﬁcally,the cross-attention sublayer is an essential part ofthe ﬁne encoder because they perform ﬁne-grainedinteraction between sources (see table 5)..formally, the architecture of the ﬁne encoder canbe described as follows.
first, the representationsof multiple sources output by the coarse encoderare divided according to the boundaries of sources:.
r(nc)1., .
.
.
, r(nc).
k = split.
r(nc)1:k.(cid:16).
(cid:17).
,.
(10).
where nc is the number of the coarse encoder lay-ers, split(·) is the split operation.
second, for eachﬁne encoder layer, the representations are fed intoa self-attention sublayer:.
b(i).
k = selfatt.
a(i−1)k.(cid:16).
(cid:17).
,.
(11).
k.where a(i−1)is the representation correspondingto the k-th source sentence output by the (i − 1)-thlayer of the ﬁne encoder, in other words, a(0)k =r(nc)is the representation output by thekself-attention sublayer of the i-th layer.
third, rep-resentations of source sentences interact through across-attention sublayer:.
.
b(i)k.cross-attention sublayer take each source’s repre-sentation as key/value separately and then combinethe outputs by mean pooling.3 formally, the dif-ferences between our decoder and the traditionaltransformer decoder are described below..first, the input representations of the i-th de-coder layer are fed into the self-attention sublayerto obtain g(i)j .
second, a separated cross-attentionsublayer is adopted by our framework to replacethe traditional cross-attention sublayer:.
p(i).
j,k = crossatt.
(cid:16).
j , a(nf )g(i)k(cid:16)p(i).
, a(nf )k.(cid:17).
(cid:17).
,.
,.
(15).
(16).
h(i).
j = meanpooling.
j,1, .
.
.
, p(i).
j,k.
k.where a(nf )is the output of the ﬁne encoder de-rived by eq.
(14), p(i)j,k is the representation corre-sponding to the k-th source, h(i)is the combinedjresult of the separated cross-attention sublayer,and the parameters of separated cross-attentionsto leverage each source are shared.
finally, a feed-forward network is the last sublayer of a decoderlayer.
in this way, the decoder in our framework canbetter handle representations of multiple sources..3 experiments.
3.1 setup.
datasets.
o(i).
(cid:16)\k = concat.
b(i).
1 , .
.
.
, b(i).
k−1, b(i).
(cid:17).
,.
k+1, .
.
.
, b(i)k(12).
we evaluated our framework on three msg tasks:(1) automatic post-editing (ape), (2) multi-sourcetranslation, and (3) document-level translation..c(i).
k = crossatt.
b(i).
k , o(i).
\k , o(i).
\k.
(cid:16).
(cid:17).
,.
(13).
where concat(·) is the concatenation operation,o(i)\k is the concatenated representation except b(i)k ,crossatt(q, k, v ) is the cross-attention sublayer,c(i)is the representation output by the cross-kattention sublayer of the i-th layer.
finally, thelast sublayer is a feedforward network:.
a(i).
k = ffn.
(cid:16).
(cid:17).
..c(i)k.(14).
after the nf -layer ﬁne encoder, the representationscorresponding to multiple sources are given to thedecoder..2.5 the decoder.
given that representations of multiple sources aredifferent from that of a single source, to better lever-age representations of multiple sources, we let the.
for the ape task, following correia and mar-tins (2019), we used the data from the wmt17ape task (english-german smt) (chatterjee et al.,2019).
the dataset contains 23k dual-source ex-amples (e.g., (cid:104)english source sentence, germantranslation, german post-edit(cid:105)) for training in anextremely low-resource setting.
we also followedcorreia and martins (2019) to adopt pseudo data(junczys-dowmunt and grundkiewicz, 2016; ne-gri et al., 2018), which contains about 8m pseudotraining examples, to evaluate our framework in ahigh-resource setting.
we adopted the dev16 fordevelopment and used test16 and test17 for testing.
for the multi-source translation task, follow-ing zoph and knight (2016), we used a subsetof the wmt14 news dataset (bojar et al., 2014),.
3there is little difference between the “parallel attentioncombination strategy” proposed by libovick`y et al.
(2018)and our method..5742models.
pretraining.
test16.
test17.
ter.
bleu.
ter.
bleu.
extremely low-resource.
forcedatt (berard et al., 2017).
—.
22.89.
—.
23.08.
65.57.dualbert (correia and martins, 2019)dualbart (correia and martins, 2019).
trice.
dualtrans (junczys-dowmunt and grundkiewicz, 2018)l2copy (huang et al., 2019).
dualbert (correia and martins, 2019)dualbart (correia and martins, 2019).
trice.
high-resource.
mbert.
mbart.
mbart.
——.
mbert.
mbart.
mbart.
18.8818.2617.41(cid:77)(cid:63).
71.6172.6573.43(cid:77)(cid:63).
19.0318.4117.75(cid:77)(cid:63).
70.6672.0872.70(cid:77)(cid:63).
17.8117.45.
16.9116.4016.09(cid:77)(cid:63).
72.7973.51.
74.2974.7475.39(cid:77)(cid:63).
18.1017.77.
17.2617.2616.91(cid:77)(cid:63).
71.7272.98.
73.4273.5674.09(cid:77)(cid:63).
table 2: results on the automatic post-editing task (extremely low- and high-resource).
“dualbart”: a methodto leverage pretrained seq2seq models adapted from “dualbert”.
please refer to appendix a.3 for detaileddescriptions of baselines and the same below.
“(cid:77)”: signiﬁcantly better than “dualbert” (p < 0.01).
“(cid:63)”:signiﬁcantly better than “dualbart” (p < 0.01)..models.
source pretraining test14.
multirnn (zoph and knight, 2016)dualtrans (junczys-dowmunt and grundkiewicz, 2018).
(de, fr).
(de, fr).
mbart-trans (liu et al., 2020)mbart-trans (liu et al., 2020)dualbart (correia and martins, 2019).
——.
mbart.
mbart.
mbart.
de.
fr.
(de, fr).
30.037.0.
31.834.840.241.5(cid:63).
trice.
(de, fr).
mbart.
table 3: results on the multi-source translation task (medium-resource).
in this task, german and french sourcesare translated to english target.
“mbart-trans”: a single-source model directly ﬁnetuned from mbart.
“(cid:63)”:signiﬁcantly better than “dualbart” (p < 0.01)..models.
#context pretraining.
san (maruf et al., 2019)qcn (yang et al., 2019)mcn (zheng et al., 2020).
mbart-trans (liu et al., 2020)mbart-doctrans (liu et al., 2020)dualbart (correia and martins, 2019).
trice.
———.
011.
1.
———.
mbart.
mbart.
mbart.
mbart.
ted.
news.
s-bleu.
d-bleu.
s-bleu.
d-bleu.
24.425.225.1.
28.128.127.828.5†‡(cid:63).
——29.1.
31.731.731.432.1†‡(cid:63).
24.822.424.9.
29.428.629.329.8†‡(cid:63).
——27.0.
31.230.231.331.7†‡(cid:63).
table 4: results on the document-level translation task (low-resource).
“s-bleu” and “d-bleu” denote bleuscores calculated at sentence- and document-level, respectively.
“#context” denotes the number of context used bycontext-aware models.
“†”: signiﬁcantly better than “mbart-trans” (p < 0.01).
“‡”: signiﬁcantly better than“mbart-doctrans” (p < 0.01).
“(cid:63)”: signiﬁcantly better than “dualbart” (p < 0.01)..5743which contains 2.4m dual-source examples (e.g.,(cid:104)german source sentence, french source sentence,english translation(cid:105)) for training, 3,000 from test13for development, and 1,503 from test14 for test-ing.4 it can be seen as a medium-resource setting.
for the document-level translation task, weused the dataset provided by maruf et al.
(2019)from iwslt2017 (ted) and news commen-tary (news), both including about 200k english-german training examples, which can be seen aslow-resource settings.
for iwslt2017, test16 andtest17 were combined as the test set, and the restserved as the development set.
for news com-mentary, test15 and test16 in wmt16 were usedfor development and testing, respectively.
wetook the nearest preceding sentence as the con-text, and then constructed the dual-source examplelike (cid:104)german context, german current sentence,english translation(cid:105)..hyper-parameters.
we adopted mbart (liu et al., 2020) as the pre-trained seq2seq model.
we set both nc and ndto 12, and nf to 1. the model dimension, the ﬁl-ter size, and the number of heads are the same asmbart.
we adopted the vocabulary of mbart,which contains 250k tokens.
we used minibatchsizes of 256, 1,024, 4,096, and 16,384 tokens forextremely low-, low-, medium-, and high-resourcesettings, respectively.
we used the developmentset to tune the hyper-parameters and select the bestmodel.
in inference, the beamsize was set to 4.please refer to appendix a.1 for more details..evaluation metricswe used case-sensitive bleu (multi-bleu.perl) andter for automatic post-editing.
for multi-sourcetranslation and document-level translation, sacre-bleu5 (post, 2018) and meteor6 was adoptedfor evaluation.
we used the paired bootstrap re-sampling (koehn, 2004) for statistical signiﬁcancetests..3.2 main results.
table 2 shows the results on the automatic post-editing task.
our framework outperforms previousmethods without pretraining (i.e., forcedatt,.
4a dual-source example can be obtained by matching two.
single-source examples..5the signature is “bleu+case.mixed+numrefs.1+smooth.
.exp+tok.13a+version.1.4.14”..6https://www.cs.cmu.edu/˜alavie/.
meteor/.
variants.
noneffn adapter (guo et al., 2020).
#para.
bleu.
0m 73.65100.7m 73.71.fine encoder (nf = 1) w/o cafine encoder (nf = 1)fine encoder (nf = 2)fine encoder (nf = 3).
12.5m 73.8416.8m 74.2133.6m 73.7050.4m 58.84.table 5: comparisons with the variants of the ﬁne en-coder.
“#para.” denotes the number of parameters and“ca” denotes the cross-attention sublayer.
“nf ” de-notes the number of the ﬁne encoder layers..dualtrans, and l2copy) by a large margin andsurpasses strong baselines with pretraining (i.e.,dualbert and dualbart), which concatenatemultiple sources into a single source, signiﬁcantlyin both extremely low- and high-resource settings.
notably, the performances of our framework inthe extremely low-resource setting are comparableto results of strong baselines without pretrainingin the high-resource setting and we achieve newstate-of-the-art results on this benchmark..table 3 demonstrates the results on the multi-source translation task.
our framework substan-tially outperforms both baselines without pre-training (i.e., multirnn and dualtrans) andwith pretraining (i.e., single-source model mbart-trans and dual-source model dualbart).
sur-prisingly, the single-source models with pretrainingare inferior to the multi-source model without pre-training, which indicates that multiple sources playan important role in the translation task..table 4 shows the results on the document-leveltranslation task.
our framework achieves signiﬁ-cant improvements over all strong baselines.
un-usually, the previous method for handling multi-ple sources (i.e., dualbart) fails to consistentlyoutperform simple sentence- and document-leveltransformer (i.e., mbart-trans and mbart-doctrans) while our framework outperformsthese strong baselines signiﬁcantly..in general, our framework shows a strong gen-eralizability across three different msg tasks andfour different data scales, which indicates that itis useful to alleviate the pretrain-ﬁnetune discrep-ancy by gradual ﬁnetuning and learn multi-sourcerepresentations by fully capturing cross-source in-formation..5744model variants.
trice– gradual ﬁnetuning– separated cross-attention– concatenated encoding– segment embedding.
bleu.
74.2173.8373.8173.6172.92.components to finetune bleu.
allthe ﬁne encoder.
74.2170.20.table 7: effect of freezing pretrained parameters..table 6: ablation study.
the case-sensitive bleuscores are calculated on the development set of the apetask for all experiments for analyses.
note that we re-move only one component at a time..ing parameters initialized by pretrained modelsand parameters initialized randomly is essentialfor achieving good performance on msg tasks..3.3 analyses.
in this subsection, we further conduct studies re-garding the variants of the ﬁne encoder, ablationsof the other proposed components, and effect offreezing parameters.
experiments are conducted onthe ape task in the extremely low-resource setting.
the bleu scores calculated on the developmentset are adopted as the evaluation metric..comparisons with the variants of the ﬁne en-coder.
table 5 demonstrates comparisons withthe variants of the ﬁne encoder.
we ﬁnd that the ﬁneencoder (see section 2.4) is effective (compared to“none”), the cross-attention sublayer is important(compared to the one without cross-attention), andour approach outperforms “ffn adapter”, which isproposed by zhu et al.
(2019) to incorporate bertinto sequence generation tasks by inserting ffnsinto each encoder layer.
we ﬁnd that stacking moreﬁne encoder layers even harms the performance(see the last three rows in table 5) which rules outthe option that the improvements owe to increasingof parameters..ablations on the other proposed components.
table 6 shows the results of the ablation study.
weﬁnd that gradual ﬁnetuning method (see section2.2) is signiﬁcantly beneﬁcial.
lines “- segmentembedding” and “- concatenated encoding” showthat concatenating multiple sources into a long se-quence and adding sinusoidal segment embeddingfor the coarse encoder are helpful (see section 2.3).
the line “- separated cross-attention” reveals thattaking each source’s representation as key/valueseparately and then combine the outputs is betterthan concatenating all the representations and dothe cross-attention jointly (see section 2.5)..effect of freezing pretrained parameters.
asshown in table 7, ﬁnetuning all parameters includ-.
3.4 adversarial evaluation.
we adopt adversarial evaluation similar to li-bovick`y et al.
(2018) which replaces one sourcewith a randomly selected sentence.
as shown intable 8, both sources play important parts and thefrench side is more important than the germanside (randomized fr vs. randomized de)..3.5 case study.
an example in multi-source translation task isshown in table 9. the four outputs at the bottomof the table are generated by the last four modelsin table 3. we ﬁnd that single-source models havedifferent errors (e.g., “each hospitals” and “trav-elling clinics”) and multi-source models ﬁx someerrors because of taking two sources.
additionally,dualbart still output erroneous “weekly”, whiletrice outputs “weekend” successfully.
we be-lieve trice is better than baselines because multi-ple sources are complementary and the ﬁne encodercould capture ﬁner cross-source information, whichhelps correct translation errors..4 related work.
4.1 multi-source sequence generation.
multi-source sequence generation includes multi-source translation (zoph and knight, 2016), auto-matic post-editing (chatterjee et al., 2017), multi-document summarization (haghighi and vander-wende, 2009), system combination for nmt(huang et al., 2020), and document-level machinetranslation (wang et al., 2017), etc.
for these tasks,researchers usually leverage multi-encoder archi-tectures to achieve better performance (zoph andknight, 2016; zhang et al., 2018; huang et al.,2019).
to address the data scarcity problem inmsg, some researchers generate pseudo corpora(negri et al., 2018; nishimura et al., 2020) to aug-ment the corpus size while others try to make useof pretrained autoencoding models (e.g., bert.
5745models.
normal.
randomized fr.
randomized de.
bleu meteor.
bleu meteor.
bleu meteor.
mbart-trans (de)mbart-trans (fr)dualbarttrice.
31.834.840.241.5.
33.937.938.939.8.
——11.313.5.
——13.115.0.
——24.923.0.
——26.423.9.table 8: adversarial evaluation on the multi-source translation task.
“randomized fr/de” denotes that the fr/desource is replaced with a randomly selected sentence..input-fr.
input-de.
dans cet hˆopital itin´erant, divers soins de sant´e sont prodigu´es.
jede dieser wochenendkliniken bietet medizinische versorgung in einerreihe von bereichen an..reference-en.
each of these weekend clinics provides a variety of medical care..mbart-trans (de) each weekend hospitals offers medical care in a number of areas.
mbart-trans (fr)dualbarttrice.
this travelling clinics provides a variety of healthcare services.
each of these weekly hospitals provides healthcare in a variety of areas.
each of these weekend clinics offers a variety of health care..table 9: example of multi-source translation.
some erroneous parts are highlighted by underlines.
mbart-trans (de/fr) takes single source (de/fr) as input while dualbart and trice take both sources as input.
webelieve that multiple sources are complementary and trice could correct errors by capturing ﬁner cross-sourceinformation..(devlin et al., 2019) and xlm-r (conneau et al.,2020)) to enhance speciﬁc msg tasks (correia andmartins, 2019; lee et al., 2020; lee, 2020).
differ-ent from these works, we propose a task-agnosticframework to transfer pretrained seq2seq mod-els to multi-source sequence generation tasks anddemonstrate the generalizability of our framework..4.2 pretraining.
in recent years, self-supervised methods haveachieved remarkable success in a wide range ofnlp tasks (devlin et al., 2019; liu et al., 2019;conneau et al., 2020; radford et al., 2019; songet al., 2019; lewis et al., 2020; liu et al., 2020).
the architectures of pretrained models can beroughly divided into three categories: autoencod-ing (devlin et al., 2019; liu et al., 2019; con-neau et al., 2020), autoregressive (radford et al.,2019), seq2seq (song et al., 2019; raffel et al.,2020; lewis et al., 2020; liu et al., 2020).
someresearchers propose to use pretrained autoencod-ing models to improve sequence generation tasks(zhu et al., 2019; guo et al., 2020) and the apetask (correia and martins, 2019).
for pretrainedseq2seq models, it is convenient to use them to ini-.
tialize single-source sequence generation modelswithout further modiﬁcation.
different from theseworks, we transfer pretrained seq2seq models tomulti-source sequence generation tasks..5 conclusion.
task-agnostic framework,we propose a novellearning fromtransferto conducttrice,single-source sequence generation including self-supervised pretraining and supervised generationto multi-source sequence generation.
with the helpof the proposed gradual ﬁnetuning method and thenovel msg model equipped with coarse and ﬁneencoders, our framework outperforms all baselineson three different msg tasks in four different datascales, which shows the effectiveness and general-izability of our framework..acknowledgments.
this work was supported by the national keyr&d program of china (no.
2017yfb0202204),national natural science foundation of china(no.61925601, no.
61772302).
we thank allanonymous reviewers for their valuable commentsand suggestions on this work..5746references.
stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in proceedings of iccv..yoshua bengio, j´erˆome louradour, ronan collobert,and jason weston.
2009. curriculum learning.
inproceedings of icml..alexandre berard, laurent besacier, and olivierpietquin.
2017. lig-cristal submission for the wmt2017 automatic post-editing task.
in proceedings ofwmt..ondˇrej bojar, christian buck, christian federmann,barry haddow, philipp koehn, christof monz, mattpost, and lucia specia.
2014. proceedings of theninth workshop on statistical machine translation.
inproceedings of the ninth workshop on statisticalmachine translation..rajen chatterjee, m amin farajian, matteo negri,marco turchi, ankit srivastava, and santanu pal.
2017. multi-source neural automatic post-editing:fbk’s participation in the wmt 2017 ape shared task.
in proceedings of wmt..rajen chatterjee, christian federmann, matteo negri,and marco turchi.
2019. findings of the wmt 2019shared task on automatic post-editing.
in proceed-ings of wmt..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, ´edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedincross-lingual representation learning at scale.
proceedings of acl..gonc¸alo m correia and andr´e ft martins.
2019. asimple and effective approach to automatic post-in proceedings ofediting with transfer learning.
acl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of naacl-hlt..st´ephane dupont and juergen luettin.
2000. audio-visual speech modeling for continuous speech recog-nition.
ieee transactions on multimedia, 2(3):141–151..junliang guo, zhirui zhang, linli xu, hao-ran wei,boxing chen, and enhong chen.
2020.incor-porating bert into parallel sequence decoding withadapters.
in proceedings of neurips..po-yao huang, frederick liu, sz-rung shiang, jeanoh, and chris dyer.
2016. attention-based multi-in proceedingsmodal neural machine translation.
of wmt..xuancheng huang, yang liu, huanbo luan, jingfangxu, and maosong sun.
2019. learning to copy forautomatic post-editing.
in proceedings of emnlp..xuancheng huang, jiacheng zhang, zhixing tan,jingfang xu,derek f. wong, huanbo luan,maosong sun, and yang liu.
2020. modeling vot-ing for system combination in machine translation.
in proceedings of ijcai..julia ive, pranava swaroop madhyastha, and luciaspecia.
2019. distilling translations with visualawareness.
in proceedings of acl..marcin junczys-dowmunt and roman grundkiewicz.
2016. log-linear combinations of monolingual andbilingual neural machine translation models for au-tomatic post-editing.
in proceedings of wmt..marcin junczys-dowmunt and roman grundkiewicz.
2018. ms-uedin submission to the wmt2018 apeshared task: dual-source transformer for automaticpost-editing.
in proceedings of wmt..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof iclr..james kirkpatrick, razvan pascanu, neil rabinowitz,joel veness, guillaume desjardins, andrei a rusu,kieran milan, john quan, tiago ramalho, ag-nieszka grabska-barwinska, et al.
2017. over-coming catastrophic forgetting in neural networks.
proceedings of the national academy of sciences,114(13):3521–3526..philipp koehn.
2004. statistical signiﬁcance tests forin proceedings of.
machine translation evaluation.
emnlp..dongjun lee.
2020. cross-lingual transformers forin proceedings of.
neural automatic post-editing.
wmt..jihyung lee, wonkee lee, jaehun shin, baikjinjung, young-kil kim, and jong-hyeok lee.
2020.postech-etri’s submission to the wmt2020 apeshared task: automatic post-editing with cross-lingual language model.
in proceedings of wmt..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of acl..aria haghighi and lucy vanderwende.
2009. explor-ing content models for multi-document summariza-tion.
in proceedings of naacl-hlt..jindrich libovick`y, jindrich helcl, and david marecek.
2018. input combination strategies for multi-sourcetransformer decoder.
in proceedings of wmt..5747yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
tacl,8:726–742..tu vu, tong wang, tsendsuren munkhdalai, alessan-dro sordoni, adam trischler, andrew mattarella-micke, subhransu maji, and mohit iyyer.
2020.exploring and predicting transferability across nlptasks.
in proceedings of emnlp..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..sameen maruf, andr´e ft martins, and gholamrezahaffari.
2019. selective attention for context-awarein proceedings ofneural machine translation.
naacl-hlt..michael mccloskey and neal j cohen.
1989. catas-trophic interference in connectionist networks: thesequential learning problem.
in psychology of learn-ing and motivation, volume 24, pages 109–165.
el-sevier..matteo negri, marco turchi, rajen chatterjee, andnicola bertoldi.
2018. escape: a large-scale syn-thetic corpus for automatic post-editing.
in proceed-ings of lrec..y. nishimura, k. sudoh, g. neubig, and s. nakamura.
2020. multi-source neural machine translation withmissing data.
taslp, 28:569–580..longyue wang, zhaopeng tu, andy way, and qun liu.
2017. exploiting cross-sentence context for neuralmachine translation.
in proceedings of emnlp..zhengxin yang,.
jinchao zhang, fandong meng,shuhao gu, yang feng, and jie zhou.
2019. en-hancing context modeling with a query-guided cap-sule network for document-level translation.
in pro-ceedings of emnlp..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of emnlp..zaixiang zheng, xiang yue, shujian huang, jiajunchen, and alexandra birch.
2020. towards makingthe most of context in neural machine translation.
inproceedings of ijcai..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tieyan liu.
2019.incorporating bert into neural machine translation.
in proceedings of iclr..barret zoph and kevin knight.
2016. multi-sourceneural translation.
in proceedings of naacl-hlt..matt post.
2018. a call for clarity in reporting bleu.
scores.
in proceedings of wmt..a experiment setup.
yada pruksachatkun,.
jason phang, haokun liu,phu mon htut, xiaoyi zhang, richard yuanzhepang, clara vania, katharina kann, and samuelbowman.
2020. intermediate-task transfer learningwith pretrained language models: when and whydoes it work?
in proceedings of acl..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21:1–67..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to sequencein proceed-pre-training for language generation.
ings of iclr..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of neurips..a.1 model conﬁgurations.
we adopted mbart (mbart-cc25) (liu et al.,2020) as the pretrained seq2seq model.
mbartis a seq2seq model obtained by multilingual de-noising pretraining on a subset of common crawlcorpus.
following mbart, we set the numberof layers of the coarse-encoder (i.e., nc) and thenumber of the decoder layers (i.e., nd) to 12. es-pecially, the number of the fine-encoder layers(i.e., nf ) was set to 1. the model dimension, theﬁlter size, and the number of heads are the sameas mbart.
we adopted the sentencepiece modelprovided by mbart for tokenization and adoptedthe vocabulary of mbart, which contains 250ktokens..a.2 hyper-parameters and evaluation.
we used minibatch sizes of 256, 1,024, 4,096, and16,384 tokens for extremely low-, low-, medium-,and high-resource settings, respectively.
in eachstage of ﬁnetuning, we used adam (kingma andba, 2015) for optimization and used the learn-ing rate decay policy described by vaswani et al..5748(2017).
we used the development set to tune thehyper-parameters and select the best model.
in in-ference, the beamsize was set to 4 and the lengthpenalty was set to 1.0 , 0.6 and 0 for ape, multi-source translation, and document-level translation,respectively.
we used four geforce rtx 2080tigpus for training.
we used case-sensitive bleu(multi-bleu.perl) and ter7 for automatic post-editing.
for multi-source translation and document-level translation, sacrebleu8 (post, 2018) andmeteor9 was used for evaluation.
we used thepaired bootstrap resampling (koehn, 2004) for sta-tistical signiﬁcance tests..a.3 baselines.
the asterisks (“*”) below denote that we reportresults of these baseline in our implementations inthe same hyper-parameter settings as our approach..automatic post-editing.
in the automatic post-editing task, we compare ourapproach with the following baselines:.
1. forcedatt (berard et al., 2017):.
amonosource model with a task-speciﬁc atten-tion mechanism..2. dualtrans (junczys-dowmunt and grund-kiewicz, 2018): a dual-source transformerbased model for ape..3. l2copy (huang et al., 2019):.
a dual-source model enabling cross-source interac-tion, which focuses on modeling copyingmechanism in ape..4. dualbert (correia and martins, 2019): theﬁrst method to use pretrained models toenhance ape, which concatenates multiplesources as a single source and uses two bertmodels to initialize the encoder and decoderseparately..5. dualbart* (correia and martins, 2019):adapting dualbert to leverage pretrainedseq2seq models by concatenating multiplesources as a single source and feeding it toseq2seq models..7http://www.cs.umd.edu/˜snover/tercom/8the signature is “bleu+case.mixed+numrefs.1+smooth.
.exp+tok.13a+version.1.4.14”.
9https://www.cs.cmu.edu/˜alavie/.
meteor/.
multi-source translationin the multi-source translation task, we compareour approach with the following baselines:.
1. multirnn (zoph and knight, 2016): a multi-source encoder-decoder model based on rnnfor machine translation..2. dualtrans*.
(junczys-dowmunt.
andgrundkiewicz, 2018): a dual-source trans-former based model..3. mbart-trans* (liu et al., 2020): a trans-ferring method that directly ﬁnetunes thepretrained single-source sequence generationmodel on the downstream task and takessingle-source input during both training andinference..4. dualbart* (correia and martins, 2019):adapting dualbert to leverage pretrainedseq2seq models by concatenating multiplesources as a single source and feeding it to theseq2seq model..document-level translationin the document-level translation task, we compareour approach with the following baselines:.
1. san (maruf et al., 2019): a context-aware.
nmt model with selective attentions..2. qcn (yang et al., 2019): a context-awarenmt model using a query-guided capsule net-work..3. mcn (zheng et al., 2020): a general-purposenmt model that is supposed to deal with any-length text..4. mbart-trans* (liu et al., 2020): a trans-ferring method that directly ﬁnetunes thepretrained single-source sequence generationmodel on the downstream task and takessingle-source input during both training andinference..5. mbart-doctrans* (liu et al., 2020): amethod for document-level translation, whichtakes k (not more than the number of sen-tences in a document) source sentences as in-put and translates k target sentences througha ssg model all at once.
for fair comparison,we set k to 2 for both mbart-doctransand our approach..57496. dualbart* (correia and martins, 2019):adapting dualbert to leverage pretrainedseq2seq models by concatenating multiplesources as a single source and feeding it to theseq2seq model..5750