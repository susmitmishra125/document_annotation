changes in european solidarity before and during covid-19:evidence from a large crowd- and expert-annotated twitter dataset.
alexandra ils†, dan liu‡, daniela grunow†, steffen eger‡† department of social sciences, goethe university frankfurt am main‡ natural language learning group, technische universit¨at darmstadt{ils,grunow}@soz.uni-frankfurt.de,dan.liu.19@stud.tu-darmstadt.de, eger@aiphes.tu-darmstadt.de.
abstract.
we introduce the well-established social scien-tiﬁc concept of social solidarity and its contes-tation, anti-solidarity, as a new problem set-ting to supervised machine learning in nlpto assess how european solidarity discourseschanged before and after the covid-19 out-break was declared a global pandemic.
to thisend, we annotate 2.3k english and germantweets for (anti-)solidarity expressions, utiliz-ing multiple human annotators and two anno-tation approaches (experts vs. crowds).
we usethese annotations to train a bert model withmultiple data augmentation strategies.
ouraugmented bert model that combines bothexpert and crowd annotations outperforms thebaseline bert classiﬁer trained with expertannotations only by over 25 points, from 58%macro-f1 to almost 85%.
we use this high-quality model to automatically label over 270ktweets between september 2019 and decem-ber 2020. we then assess the automaticallylabeled data for how statements related to eu-ropean (anti-)solidarity discourses developedover time and in relation to one another, be-fore and during the covid-19 crisis.
our re-sults show that solidarity became increasinglysalient and contested during the crisis.
whilethe number of solidarity tweets remained ona higher level and dominated the discoursein the scrutinized time frame, anti-solidaritytweets initially spiked, then decreased to (al-most) pre-covid-19 values before rising to astable higher level until the end of 2020..1.introduction.
social solidarity statements and other forms of col-lective pro-social behavior expressed in online me-dia have been argued to affect public opinion andpolitical mobilization (fenton, 2008; margolin andliao, 2018; santhanam et al., 2019; tufekci, 2014).
the ubiquity of social media enables individualsto feel and relate to real-world problems through.
solidarity statements expressed online and to actaccordingly (fenton, 2008).
social solidarity is akey feature that keeps modern societies integrated,functioning and cohesive.
it constitutes a moral andnormative bond between individuals and society,affecting people’s willingness to help others andshare own resources beyond immediate rationalindividually-, group- or class-based interests (sil-ver, 1994).
national and international crises inten-sify the need for social solidarity, as crises diminishthe resources available, raise demand for new andadditional resources, and/or require readjustmentof established collective redistributive patterns, e.g.
inclusion of new groups.
because principles of in-clusion and redistribution are contested in modernsocieties and related opinions fragmented (fenton,2008; sunstein, 2018), collective expressions ofsocial solidarity online are likely contested.
suchstatements, which we refer to as anti-solidarity,question calls for social solidarity and its framing,i.e.
towards whom individuals should show solidar-ity, and in what ways (wallaschek, 2019)..for a long time, social solidarity was consideredto be conﬁned to local, national or cultural groups.
the concept of a european society and europeansolidarity (gerhards et al., 2019), a form of solidar-ity that goes beyond the nation state, is rather new.
european solidarity gained relevance with the riseand expansion of the european union (eu) andits legislative and administrative power vis-`a-visthe eu member states since the 1950s (baglioniet al., 2019; gerhards et al., 2019; koos and seibel,2019; lahusen and grasso, 2018).
after decadesof increasing european integration and institution-alization, the eu entered into a continued succes-sion of deep crises, beginning with the europeanfinancial crisis in 2010 (gerhards et al., 2019).
experiences of recurring european crises raise con-cerns regarding the future of european society andits foundation, european solidarity.
eurosceptics.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1623–1637august1–6,2021.©2021associationforcomputationallinguistics1623and right-wing populists claim that social solidar-ity is, and should be, conﬁned within the nationstate, whereas supporters of the european projectsee european solidarity as a means to overcomethe great challenges imposed on eu countries andits citizens today (gerhards et al., 2019).
to date,it is an open empirical question how strong andcontested social solidarity really is in europe, andhow it has changed since the onset of the covid-19 pandemic.
against this background, we askwhether we can detect changes in the debates oneuropean solidarity before and after the outbreakof covid-19.
our contributions are:.
(i) we provide a novel twitter corpus annotatedfor expressions of social solidarity and anti-solidarity.
our corpus contains 2.3k human-labeled tweets from two annotation strategies(experts vs. crowds).
moreover, we provide over270k automatically labeled tweets based on anensemble of bert classiﬁers trained on the ex-pert and crowd annotations..(ii) we train bert on crowd- and expert annotationsusing multiple data augmentation and transferlearning approaches, achieving over 25 pointsimprovement over bert trained on expert anno-tations alone..(iii) we present novel empirical evidence regardingchanges in european solidarity debates beforeand after the outbreak of the covid-19 pan-demic.
our ﬁndings show that both expressedsolidarity and anti-solidarity escalated with theoccurrence of incisive political events, such asthe onset of the ﬁrst european lockdowns..and.
data.
ourhttps://github.com/lalashiwoya/socialsolidaritycovid19..code.
are.
available.
from.
2 related work.
social solidarity in the social sciences.
in thesocial sciences, social solidarity has always been akey topic of intellectual thought and empirical in-vestigation, dating back to seminal thinkers such asrousseau and durkheim (silver, 1994).
whereasearlier empirical research was mostly conﬁned tosurvey-based (baglioni et al., 2019; gerhards et al.,2019; koos and seibel, 2019; lahusen and grasso,2018) or qualitative approaches (franceschelli,2019; g´omez garrido et al., 2018; heimann et al.,2019), computational social science just startedtackling concepts as complex as solidarity as part.
of natural language processing (nlp) approaches(santhanam et al., 2019)..in (computational) social science, several studiesinvestigated the european migration crisis and/orthe financial crisis as displayed in media dis-courses.
these studies focused on differences inperspectives and narratives between mainstreammedia and twitter, using topic models (nerghesand lee, 2019), and the coverage and kinds of sol-idarity addressed in leftist and conservative news-paper media (wallaschek, 2019, 2020a), as wellas relevant actors in discourses on solidarity, usingdiscourse network measures (wallaschek, 2020b).
while these studies offer insight into solidarity dis-courses during crises, they all share a strong focuson mainstream media, which is unlikely to pub-licly reject solidarity claims (wallaschek, 2019).
social media, in contrast, allows its users to perpet-uate, challenge and open new perspectives on main-stream narratives (nerghes and lee, 2019).
a ﬁrstattempt to study solidarity expressed by social me-dia users during crises has been presented by san-thanam et al.
(2019).
they assessed how emojis areused in tweets expressing solidarity relating to twocrises through hashtag-based manual annotation—ignoring actual content of the tweets—and utilizinga lstm network for automatic classiﬁcation.
theirapproach, while insightful, provides a rather sim-ple operationalization of solidarity, which neglectsits contested, consequential and obligatory aspectsvis-`a-vis other social groups..the current state of social science research oneuropean social solidarity poses a puzzle.
on theone hand, most survey research paints a rather opti-mistic view regarding social solidarity in the eu,despite marked cross-national variation (binnerand scherschel, 2019; dragolov et al., 2016; ger-hards et al., 2019; lahusen and grasso, 2018).
onthe other hand, the rise of political polarization andeurosceptic political parties (baker et al., 2020;nicoli, 2017) suggests that the opinions, orienta-tions and fears of a potentially growing politicalminority is underrepresented in this research.
peo-ple holding extreme opinions have been found tobe reluctant to participate in surveys and adopttheir survey-responses to social norms (social desir-ability bias) (bazo vienrich and creighton, 2017;heerwegh, 2009; janus, 2010).
research indicatesthat such minorities may grow in times of crises,with both short-term and long-term effects for pub-lic opinion and political trust (gangl and giustozzi,.
16242018; nicoli, 2017).
our paper addresses theseproblems by drawing on large volumes of longitu-dinal social media data that reﬂect potential frag-mentation of political opinion (sunstein, 2018) andits change over time.
our approach will thus un-cover how contested european solidarity is andhow it developed since the onset of covid-19..emotion and sentiment classiﬁcation in nlp.
in nlp, annotating and classifying text (in so-cial media) for sentiment or emotions is a well-established task (demszky et al., 2020; ding et al.,2020; haider et al., 2020; hutto and gilbert,2014; oberl¨ander and klinger, 2018).
impor-tantly, our approach focuses on expressions of (anti-)solidarity: for example, texts containing a positivesentiment towards persons, groups or organizationswhich are at their core anti-european, nationalisticand excluding reﬂect anti-solidarity and are anno-tated as such.
our annotations therefore go beyondsuperﬁcial assessment of sentiment.
in fact, thecorrelation between sentiment labels—e.g., as ob-tained from vader (hutto and gilbert, 2014)—andour annotations in §3 is only ∼0.2.
speciﬁcally,many tweets labeled as solidarity use negativelyconnoted emotion words..3 data and annotations.
we use the unforeseen onset of the covid-19 cri-sis, beginning with the ﬁrst european lockdown,enacted late february to early march 2020, to ana-lyze and compare social solidarity data before andduring the covid-19 crisis as if it were a naturalexperiment (creighton et al., 2015; kuntz et al.,2017).
in order to utilize this strategy and keep thebaseline solidarity debate comparable before andafter the onset of the covid-19 crisis, we conﬁnedour sample to tweets with hashtags predominantlyrelating to two previous european crises whose ef-fects continue to concern europe, its member statesand citizens: (i) migration and the distribution ofrefugees among european member states, and (ii)financial solidarity, i.e.
ﬁnancial support for in-debted eu countries.
the former solidarity debatepredominantly refers to the refugee crisis since2015 and the living situation of migrants, the lattermostly relates to the financial crisis, followed bythe euro crisis, and concerns the excessive indebt-edness of some eu countries since 2010.1.
1further analyses (not shown) revealed that around 20 per-cent of the tweets in our sample relate to solidarity regardingother issues..data.
we crawled 271,930 tweets between01.09.2019 and 31.12.2020, written in english orgerman and geographically restricted to europe, toobtain setups comparable to the survey-based socialscience literature on european solidarity.
we onlycrawled tweets that contained speciﬁc hashtags, toﬁlter for our two topics, i.e.
refugee and ﬁnancialsolidarity.
we started with an initial list of hash-tags (e.g., “#refugeecrisis”, “#eurobonds”), whichwe then expanded via co-occurrence statistics.
wemanually evaluated 456 co-occurring hashtags withat least 100 occurrences to see if they representedthe topics we are interested in.
ultimately, we se-lected 45 hashtags (see appendix) to capture a widerange of the discourse on migration and ﬁnancialsolidarity.
importantly, we keep the hashtag list as-sociated with our 270k tweets constant over time.2.
deﬁnition of social solidarity.
in line with so-cial scientiﬁc concepts of social solidarity, we de-ﬁne social solidarity as expressed and/or called forin online media as “the preparedness to share one’sown resources with others, be that directly by donat-ing money or time in support of others or indirectlyby supporting the state to reallocate and redistributesome of the funds gathered through taxes or con-tributions” (lahusen and grasso, 2018, p. 4).
wedeﬁne anti-solidarity as expressions that contestthis type of social solidarity and/or deny solidaritytowards vulnerable social groups and other euro-pean states, e.g.
by promoting nationalism or theclosure of national borders (burgoon and rooduijn,2021; cinalli et al., 2020; finseraas, 2008; wal-laschek, 2017)..expert annotations.
after crawling and prepar-ing the data, we set up guidelines for annotatingtweets.
overall, we set four categories to anno-tate, with solidarity and anti-solidarity being themost important ones.
a tweet indicating supportfor people in need, the willingness and/or gratitudetowards others to share resources and/or help themis considered expressing solidarity.
the sameapplies to tweets criticizing the eu in terms of notdoing enough to share resources and/or help so-cially vulnerable groups as well as advocating forthe eu as a solidarity union.
a tweet is consideredto be expressing anti-solidarity statements.
2we follow a purposeful sampling frame, but this nec-essarily introduces a bias in our data.
while we took careof including a variety of hashtags, we do not claim to havecaptured the full extent of discourse concerning the topicsmigration and ﬁnancial solidarity..1625if the above-mentioned criteria are reversed, and/or,the tweet contains tendencies of nationalism or ad-vocates for closed borders.
not all tweets ﬁt intothese classes, thus we introduce two additional cat-egories: ambivalent and not applicable.
while the ambivalent category refers to tweets thatcould be interpreted as both expressing solidarityand anti-solidarity statements, the second categoryis reserved for tweets that do not contain the topicof (anti-)solidarity at all or refer to topics that arenot concerned with discourses on refugee or ﬁnan-cial solidarity.
table 1 contains example tweets forall categories.
full guidelines for the annotation oftweets are given in the appendix..we divided the annotation process into six work-ing stages (i-vi) to reﬁne our data set and anno-tation standards over time and strengthen inter-annotator reliability through subsequent discus-sions among annotators and social science experts.
our annotators included four university studentsmajoring in computer science, one computer sci-ence faculty member as well as two social scienceexperts (one phd student and one professor).
westarted the training of seven annotators with a smalldataset that they annotated independently and re-ﬁned the guidelines during the annotation process.
in the training period, which lasted three iterations(i-iii), we achieved cohen’s kappa values of 0.51among seven annotators.
in working stage iv, twogroups of two annotators annotated 339 tweets withhashtags not included before.
across the four anno-tators, cohen’s kappa values of 0.49 were reached.
in working stages v and vi, one group of two stu-dents annotated overall 588 tweets, with a resultingkappa value of 0.79 and 0.77 respectively..while the kappa value was low in the ﬁrst stages,we managed to raise the inter-annotator reliabil-ity over time through discussions with the socialscience experts and extension of the guidelines.
we also introduced a gold-standard for annotationsfrom stage ii onward which served as orientation.
this was determined by majority voting and dis-cussions among the annotators.
for cases wherea decision on the gold-standard label could notbe reached, a social science expert decided on thegold-standard label; some hard cases were left un-decided (not included in the dataset)..the gold-standard additionally served as hu-man reference performance which we comparedthe model against.
on average across all stages,our kappa agreement is 0.64 for four and 0.69 for.
three classes (collapsing ambivalent and notapplicable), while the macro f1-score is 69%for four and 78.5% for three classes.
however,in the ﬁnal stages, the agreement is considerablyhigher: above 80% macro-f1 for four and between85.4% and 89.7% macro-f1 for three classes..crowd annotations.
we also conducted a‘crowd experiment’ with students in an introduc-tory course to nlp.
we provided students with theguidelines and 100 expert annotated tweets as il-lustrations.
we trained crowd annotators in threeiterations.
1) they were assigned reading the guide-lines and looking at 30 random expert annotations.
then they were asked to annotate 20 tweets them-selves and self-report their kappa agreement withthe experts (we provided the labels separately sothat they could further use the 20 tweets to under-stand the annotation task).
2) we repeated thiswith another 30 tweets for annotator training and20 tweets for annotator testing.
3) they received30 expert-annotated tweets for which we did notgive them access to expert labels, and 30 entirelynovel tweets, that had not been annotated before.
these 60 ﬁnal tweets were presented in randomorder to each student.
50% of the 30 novel tweetswere taken from before september 2020 and theother 50% were taken from after september 2020.
125 students participated in the annotation task.
the annotation experiment was part of a bonusthe students could achieve for the course (counted12.5% of the overall bonus for the class).
eachnovel tweet was annotated by up to 3 students (2.7on average).
to obtain a unique label for eachcrowd-annotated tweet, we used the following sim-ple strategy: we either chose the majority labelamong the three annotators or the annotation of themost reliable annotator in case there was no uniquemajority label.
the annotator that had the highestagreement with the expert annotators was taken asmost reliable annotator..figure 1: distribution of kappa agreements of crowdworkers with expert annotated gold-standard, 3 classes..16260102030400.000.100.200.300.400.500.600.700.800.901.00kappa with gold-standard, 3 classeschildren caught up in the moria camp ﬁre face unimaginable horrors#safepassage #refugeeswelcome.
solidarity.
most people supporting #refugeeswelcome are racists or psychopaths.
anti-solidarity.
does this rule apply to every uk citizen as well as every #asylumseeker?
ambivalent.
let’s make #vaccineswork for everyone #leavenoonebehind.
not applicable.
table 1: paraphrased (and translated) sample of annotated tweets in our dataset, together with labels..s.a.amb na total.
expertscrowds.
386768.
246209.
113186.
174217.
9191380.table 2: number of annotated tweets (after ge-oﬁltering) for the four classes solidarity (s),anti-solidarity (a), ambivalent (amb),and not applicable (na)..kappa agreements of students with the expertsare shown in figure 1. the majority of studentshas a kappa agreement with the gold-standard ofbetween 0.6-0.7 when three classes are taken intoaccount and between 0.5-0.6 for four classes..in table 2, we further show statistics onour annotated datasets: we have 2299 anno-tated tweets in total, about 60% of which havebeen annotated by crowd-workers.
about 50%of all tweets are annotated as solidarity,20% as anti-solidarity, and 30% as eithernot-applicable or ambivalent.
in our an-notations, 1196 tweets are english and 1103 aregerman.3 finally, we note that the distribution oflabels for expert and crowd annotations are differ-ent, i.e., the crowd annotations cover more soli-darity tweets.
the reason is twofold: (a) for theexperts, we oversampled hashtags that we believedto be associated more often with anti-solidaritytweets as the initial annotations indicated that thesewould be in the minority, which we feared to beproblematic for the automatic classiﬁers.
(b) thetime periods in which the tweets for the experts andcrowd annotators fall differ..4 methods.
we use multilingual bert (devlin et al.,to2019).
/ xlm-r (conneau et al., 2020).
3in our automatically labeled data, the majority of tweetsis german.
we assumed all german tweets to come fromwithin the eu, while the english tweets would be geoﬁlteredmore aggressively..classify our tweets in a 3-way classiﬁcationproblem (solidarity, anti-solidarity,other), not differentiating between the classesambivalent and non-applicable since ourmain focus is on the analysis of changes in (anti-)solidarity.
we use the baseline mbert model:bert-base-multilingual-cased and the base xlm-rmodel: xlm-roberta-base.
we implemented severaldata augmentation/transfer learning techniques toimprove model performance:.
• oversampling of minority classes: we ran-domly duplicate (expert and crowd annotated)tweets from minority classes until all classes havethe same number of tweets as the majority classsolidarity..• back-translation: we use the google translateapi to translate english tweets into a pivot lan-guage (we used german), and pivot languagetweets back into english (for expert and crowd-annotated tweets)..• fine-tuning: we ﬁne-tune mbert / xlm-rwith masked language model and next sentenceprediction tasks on domain-speciﬁc data, i.e., ourcrawled unlabeled tweets..• auto-labeled data: as a form of self-learning,we train 9 different models (including oversam-pling, back-translation, etc.)
on the expert andcrowd-annotated data, then apply them to our fulldataset (of 270k tweets, see below).
we only re-tain tweets where 7 of 9 models agree and select35k such tweets for each label (solidarity,anti-solidarity, other) into an aug-mented training set, thus increasing training databy 105k auto-labeled tweets..• ensembling: we take the majority vote of 15different models to leverage heterogeneous in-formation.
the k = 15 models, like the k =9 models above, were determined as the top-kmodels by their dev set performance..we also experimented with re-mapping multilin-.
1627gual bert and xlm-r (cao et al., 2020; zhaoet al., 2020a,b) as they have not seen parallel dataduring training, but found only minor effects ininitial experiments..5 experiments.
in §5.1, we describe our experimental setup.
in§5.2, we show the classiﬁcation results of our base-line models on the annotated data and the effects ofour various data augmentation and transfer learningstrategies.
in §5.3, we analyze performances of ourbest-performing models.
in §5.4, we automaticallylabel our whole dataset of 270k tweets and analyzechanges in solidarity over time..5.1 experimental setup.
to examine the effects of various factors, we designseveral experimental conditions.
these involve (i)using only hashtags for classiﬁcation, ignoring theactual tweet text, (ii) using only text, without thehashtags, (iii) combining expert and crowd annota-tions for training, (iv) examining the augmentationand transfer learning strategies, (v) ensembling var-ious models using majority voting..all models are evaluated on randomly sampledtest and dev sets of size 170 each.
both dev andtest set are taken from the expert annotations.
weuse the dev set for early stopping.
to make sure ourresults are not an artefact of unlucky choices of testand dev sets, we report averages of 3 random splitswhere test and dev set contain 170 instances in eachcase (for reasons of computational costs, we do soonly for selected experimental conditions)..we report the macro-f1 score to evaluate theperformance of different models.
hyperparametersof our models can be found in our github..5.2 results.
the main results are reported in table 3. using onlyhashtags and expert annotated data yields a macro-f1 score of below 50% for mbert and xlm-r. including the full texts improves this by over8 points (almost 20 points for xlm-r).
addingcrowd-annotations yields another substantial boostof more than 6 points for mbert.
removing hash-tags in this situation decreases the performance be-tween 5 and 6 points.
this means that the hashtagsindeed contain import information, but the texts aremore important than the hashtags: with hashtagsonly, we observe macro-f1 scores between 42 and49%, whereas with text only the performances are.
substantially higher, between 58 and 60%.
whileusing hashtags only means less data since not allof our tweets have hashtags, the performance withonly hashtags on the test sets stays below 50%, bothwith 572 and more than 1500 tweets for training.
next, we analyze the data augmentation andtransfer learning techniques.
including auto-labeled data drastically increases the train set, frombelow 2k instances to over 100k.
even though theseinstances are self-labeled, performance increasesby over 13 points to about 78% macro-f1.
addi-tionally oversampling or backtranslating the datadoes not yield further beneﬁts, but pretraining onunlabeled tweets is effective even here and boostsperformance to over 78%.
combining all strategiesyields scores of up to almost 80%.
finally, whenwe consider our ensemble of 15 models, we achievea best performance of 84.5% macro-f1 on the testset, close to the human macro-f1 agreement for theexperts in the last rounds of annotation..to sum up, we note: (i) adding crowd anno-tated data clearly helps, despite the crowd anno-tated data having a different label distribution; (ii)including text is important for classiﬁcation as theclassiﬁcation with hashtags only performs consid-erably worse; (iii) data augmentation (especiallyself-labeling), combining models and transfer learn-ing strategies has a further clearly positive effect..5.3 model analysis.
our most accurate ensemble models perform bestfor the majority class solidarity with an f1-score of almost 90%, about 10 points better thanfor anti-solidarity and over 5 points betterthan for the other class.
a confusion matrixfor this best performing model is shown in table4. here, anti-solidarity is disproportion-ately misclassiﬁed as either solidarity or theother class..table 5 shows selected misclassiﬁcations for ourensemble model with performance of about 84.5%macro-f1.
this reveals that the models sometimesleverage superﬁcial lexical cues (e.g., the germanpolitical party ‘afd’ is typically associated withanti-solidarity towards eu and refugees), includinghashtags (‘remigration’); see figure 2, where weused lime (ribeiro et al., 2016) to highlight wordsthe model pays attention to.
to further gain insightinto the misclassiﬁcations, we had one social sci-ence expert reannotate all misclassiﬁcations.
from.
1628condition.
train size.
dev.
test.
dev.
test.
mbert.
xlm-r.e, hashtag onlyee+ce+c, no hashtagse+c, hashtag onlye+c+auto labele+c+auto label+oversamplee+c+auto label+backtranslatione+c+auto label+pretraininge+c+all.
572579195919591567106959108048108918106959110007.
51.7±0.564.2±1.266.4±0.564.0±0.355.8±2.076.476.476.078.478.8±1.3.
49.0±1.157.7±0.464.0±1.558.0±0.549.5±2.178.376.377.178.878.6±0.8.
48.0±0.964.065.062.047.877.577.477.578.678.9.
44.0±0.863.364.860.042.278.476.978.779.079.7.table 3: macro-f1 scores (in %) for different conditions.
entries with ± give averages and standard deviationsover 3 different runs with different test and dev sets.
‘e’ stands for experts, ‘c’ for crowds.
‘all’ refers to alldata augmentation and transfer learning techniques..the 25 errors that our best model makes in the testset of 170 instances, the expert thinks that 12 timesthe gold standard is correct, 7 times the model pre-diction is correct, and in further 6 cases neitherthe model nor the gold standard are correct.
thishints at some level of errors in our annotated data;it further supports the conclusion that our model isclose to the human upper bound..predicteds.a o.s.63gold a 5o 5.
3376.
2445.table 4: confusion matrix for best ensemble withmacro-f1 score of 84.5% on the test set (for one spe-ciﬁc train, dev, test split)..5.4 temporal analysis.
throughout the period observed in our data, dis-courses relating to migration were much morefrequent than ﬁnancial solidarity discourses.
wecrawled an average of 2526 tweets per week relat-ing to migration (anti-)solidarity and an average of174 ﬁnancial (anti-)solidarity tweets, judging fromthe associated hashtags..we used our best performing model to automati-cally label all our 270k tweets between september2019 and december 2020. solidarity tweets wereabout twice as frequent compared to anti-solidaritytweets, reﬂecting a polarized discourse in whichsolidarity statements clearly dominated.
figure.
3 shows the frequency curves for solidarity,anti-solidarity and other tweets overtime in our sample.
the ﬁgure also gives the ratio.
s/a :=.
#solidarity tweets#anti-solidarity tweets.
that shows the frequency of solidarity tweets rel-ative to anti-solidarity tweets.
values above oneindicate that more solidarity than anti-solidaritystatements were tweeted that day..figure 3 displays several short-term increases insolidarity statements in our window of observation.
further analysis shows that these peaks have beenimmediate responses to drastic politically relevantevents in europe, which were also prominently cov-ered by mainstream media, i.e.
covid-19-relatednews, natural disasters, ﬁres, major policy changes.
we illustrate this in the following..on march 11th 2020, the world health organi-zation (who) declared the covid-19 outbreaka global pandemic.
shortly before and after, eu-ropean countries started to take a variety of coun-termeasures, including stay-at-home orders for thegeneral population, private gathering restrictions,and the closure of educational and childcare institu-tions (ecdc, 2020a).
with the onset of these inter-ventions, both solidarity and anti-solidarity state-ments relating to refugees and ﬁnancial solidarityincreased dramatically.
at its peak at the beginningof march, anti-solidarity statements markedly out-numbered solidarity statements (we recorded 2189solidarity tweets vs. 2569 anti-solidarity tweets onmarch 3rd).
in fact, the period in early march 2020is the only extended period in our data where anti-.
1629figure 2: our best-performing model (macro-f1 of 84.5%) predicts anti-solidarity for the current examplebecause of the hashtag #remigration (according to lime).
the tweet, also given as translation in table 5 (2) below,is overall classiﬁed as other in the gold standard, as it may be considered as expressing no determinate stance.
here, we hide identity revealing information in the tweet, but our classiﬁer sees it..text.
(1) you can drink a toast with the afd misanthropists #seenotrettung #niemehrcdu(2) why is an open discussion about #remigration (not) yet possible?.
(3) raped and beaten, lesbian #asylumseeker faces #deportation.
gold pred..so.a.aa.o.table 5: selected misclassiﬁcations of best performing ensemble model.
we consider the bottom tweet misclassi-ﬁed in the expert annotated data (correct would be solidarity).
tweets are paraphrased and/or translated..figure 3: frequency of solidarity (s), anti-solidarity (a) and other (o) tweets over time as well asthe ratio s/a.
the constant line 1 indicates where s = a. y-axis is in log-scale..solidarity statements outweighed solidarity state-ments.
the dominance of solidarity statements wasreestablished after two weeks.
over the followingmonths, anti-solidarity statements decreased againto pre-covid-19 levels, whereas solidarity state-ments remained comparatively high, with severalpeaks between march and september 2020..solidarity and anti-solidarity statements shotup again early-september 2020, with an unprece-dented climax on september 9th.
introspectionof our data shows that the trigger for this wasthe precarious situation of refugees after a ﬁre de-stroyed the m´oria refugee camp on the greekisland of lesbos on the night of september 8th.
human rights watch had compared the camp to an.
open-air prison in which refugees lived under inhu-mane conditions, and the disaster spurred debatesabout the responsibilities of eu countries towardsrefugees and the countries hosting refugee hot spots(i.e.
greece and italy).
at that time, covid-19infection rates in the eu were increasing but stilllow, and national measures to prevent the spread ofinfections relaxed in some and tightened in othereu countries (ecdc, 2020a,b).
further analyses(not displayed) show that the dominance of soli-darity over anti-solidarity statements at the timewas driven by tweets using hashtags relating tomigration.
the contemporaneous discourse on ﬁ-nancial solidarity between eu countries was muchless pronounced.
from september 2020 to decem-.
16302019-092019-102019-112019-122020-012020-022020-032020-042020-052020-062020-072020-082020-092020-102020-112020-122021-01100101102103frequencydevelopment of tweets reflecting (anti-) solidarity discourses over timesolidaryanti-solidarityothers/a ber 2020, solidarity and (anti-)solidarity statementswere about equal in frequency, which means thatanti-solidarity was on average on a higher levelcompared to the earlier time points in our timeframe.
this period also corresponds to the highestcovid-19 infection rates witnessed in the eu,on average, during the year 2020.in fact, thespearman correlation between the number of anti-solidarity tweets in our data and infection rates is0.45 and 0.47, respectively (infection rates withingermany and the eu); see figure 4 in the appendix.
correlation with the number of solidarity tweets is,in contrast, non-signiﬁcant..discussion late february to mid-march 2020,eu governments began enacting lockdowns andother measures to contain covid-19 infectionrates, turning people’s everyday lives upside down.
during this time frame, anti-solidarity statementspeaked in our data, but solidarity statementsquickly dominated thereafter again.
during thesummer of 2020, anti-solidarity tweets decreasedwhereas solidarity tweets continued to prevail onhigher levels than before.
a major peak on septem-ber 9th, in the aftermath of the destruction of them´oria refugee camp, signiﬁes an intensiﬁcation ofthe polarized solidarity discourse.
from septemberto december 2020, anti-solidarity and solidaritystatements were almost equal in number.
thus,the onset of the covid-19 crisis as well as timesof high infection rates concurred with dispropor-tionately high levels of anti-solidarity, despite adominance of solidarity overall.
whether the re-lationship between anti-solidarity and intensiﬁedstrains during crises is indeed causal will be thescope of our future research.4.
6 conclusion.
in this paper, we contributed the ﬁrst large-scalehuman and automatically annotated dataset labeledfor solidarity and its contestation, anti-solidarity.
the dataset uses the textual material in social me-dia posts to determine whether a post shows (anti-)solidarity with respect to relevant target groups.
our annotations, conducted by both trained expertsand student crowd-workers, show overall goodagreement levels for a challenging novel nlp task.
we further trained augmented bert models whose.
4we made sure that the substantial ﬁndings reported hereare not driven by inherently german (anti-)solidarity dis-courses.
still, our results are bound to the opinions of peopleposting tweets in the english and german language..performance is close to the agreement levels of theexperts and which we used for large-scale trendanalysis of over 270k media posts before and afterthe onset of the covid-19 pandemic.
our ﬁnd-ings show that (anti-)solidarity statements climaxedmomentarily with the ﬁrst lockdown, but the pre-dominance of solidarity expressions was quicklyrestored at higher levels than before.
solidarity andanti-solidarity statements were balanced by the endof the year 2020, when infection rates were rising.
the covid-19 pandemic constitutes a world-wide crisis, with profound economic and socialconsequences for contemporary societies.
it mani-fests yet another challenge for european solidarity,by putting a severe strain on available resources, i.e.
national economies, health systems, and individ-ual freedom.
while the eu, its member countriesand residents continued to struggle with the conse-quences of the financial crisis and its aftermath,as well as migration, the covid-19 pandemic hasaccelerated the problems related to these formercrises.
our data suggests that the covid-19 pan-demic has not severely negatively impacted thewillingness of european twitter users to take re-sponsibility for refugees, while ﬁnancial solidar-ity with other eu countries remained low on theagenda.
over time, however, this form of expressedsolidarity became more controversial.
on one hand,these ﬁndings are in line with survey-based, quan-titative research and its rather optimistic overallpicture regarding social solidarity in the eu dur-ing earlier crises (baglioni et al., 2019; gerhardset al., 2019; koos and seibel, 2019; lahusen andgrasso, 2018); on the other hand, results from ourcorrelation analysis suggests that severe strains dur-ing crises coincide with increased levels of anti-solidarity statements.
we conclude that a conver-gence of opinion (santhanam et al., 2019) amongthe european twitter-using public regarding thetarget audiences of solidarity, and the limits of eu-ropean solidarity vs. national interests, is not insight.
instead, our widened analytic focus has al-lowed us to examine pro-social online behaviorduring crises and its opposition, revealing that eu-ropean twitter users remain divided on issues ofeuropean solidarity..acknowledgments.
we thank the anonymous reviewers whose com-ments greatly improved the ﬁnal version of thepaper..1631ethical considerations.
we will release onlytweet ids in our ﬁnal dataset.
the presented tweetsin our paper were paraphrased and/or translated andtherefore cannot be traced back to the users.
nouser identities of any annotator (neither expert norcrowd worker) will ever be revealed or can be in-ferred from the dataset.
crowd workers were madeaware that the annotations are going to be used infurther downstream applications and they were freeto choose to submit their annotations.
while ourtrained model could potentially be misused, we donot foresee greater risks than with established nlpapplications such as sentiment or emotion classiﬁ-cation..references.
simone baglioni, olga biosca, and tom montgomery.
2019. brexit, division, and individual solidarity:what future for europe?
evidence from eight eu-ropean countries.
american behavioral scientist,63(4):538–550..scott r baker, aniket baksy, nicholas bloom, steven jdavis, and jonathan a rodden.
2020. elections,political polarization, and economic uncertainty.
working paper 27961, national bureau of economicresearch..alessandra bazo vienrich and mathew j. creighton.
2017. what’s left unsaid?
in-group solidarity andethnic and racial differences in opposition to immi-gration in the united states.
journal of ethnic andmigration studies, 44(13):2240–2255..kristina binner.
and karin scherschel,editors.
fluchtmigration und gesellschaft: von2019.nutzenkalk¨ulen, solidarit¨at und exklusion.
arbeits-gesellschaft im wandel.
beltz juventa..‘im-brian burgoon and matthijs rooduijn.
2021.migrationization’ of welfare politics?
anti-immigration and welfare attitudes in context.
westeuropean politics, 44(2):177–203..steven cao, nikita kitaev, and dan klein.
2020. mul-tilingual alignment of contextual word representa-tions.
iclr..manlio cinalli, olga eisele, verena k. br¨andle, andhans-j¨org trenz.
2020. solidarity contestation inthe public domain during the ‘refugee crisis’.
inchristian lahusen, editor, citizens’ solidarity in eu-rope, pages 120–148..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedincross-lingual representation learning at scale..proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..mathew j. creighton, amaney jamal, and natalia c.malancu.
2015. has opposition to immigration in-creased in the united states after the economic crisis?
an experimental approach.
international migrationreview, 49(3):727–756..dorottya demszky, dana movshovitz-attias, jeong-woo ko, alan cowen, gaurav nemade, and sujithravi.
2020. goemotions: a dataset of ﬁne-grainedemotions.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4040–4054, online.
association for computa-tional linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..keyang ding, jing li, and yuji zhang.
2020. hash-tags, emotions, and comments: a large-scale datasetto understand ﬁne-grained social emotions to onlinein proceedings of the 2020 conference ontopics.
empirical methods in natural language process-ing (emnlp), pages 1376–1382, online.
associa-tion for computational linguistics..georgi dragolov, zs´oﬁa s. ign´acz, jan lorenz, jan del-hey, klaus boehnke, and kai unzicker.
2016. socialcohesion in the western world: what holds societiestogether: insights from the social cohesion radar.
springer..ecdc.
2020a.
european centre for disease preventionand control, data on country response measures tocovid-19..ecdc.
2020b.
european centre for disease preventionand control, download historical data (to 14 decem-ber 2020) on the daily number of new reported covid-19 cases and deaths worldwide..natalie fenton.
2008. mediating solidarity.
global.
media and communication, 4(1):37–57..henning finseraas.
2008. immigration and preferencesfor redistribution: an empirical analysis of euro-pean survey data.
comparative european politics,6(4):407–431..michela franceschelli.
2019. global migration, localcommunities and the absent state: resentment andresignation on the italian island of lampedusa.
soci-ology, 54(3):591–608..1632markus gangl and carlotta giustozzi.
2018. the ero-sion of political trust in the great recession.
cor-rode working paper, (5)..j¨urgen gerhards, holger lengfeld, zs´oﬁa s. ign´acz,florian k. kley, and maximilian priem.
2019. eu-ropean solidarity in times of crisis: insights from athirteen-country survey.
routledge..mar´ıa g´omez garrido, m. ant`onia carbonerogamund´ı, and anah´ı viladrich.
2018. the role ofgrassroots food banks in building political solidar-ity with vulnerable people.
european societies,21(5):753–773..thomas haider, steffen eger, evgeny kim, romanklinger, and winfried menninghaus.
2020.po-emo: conceptualization, annotation, and modelingof aesthetic emotions in german and english poetry.
in proceedings of the 12th language resourcesand evaluation conference, pages 1652–1663, mar-seille, france.
european language resources asso-ciation..d. heerwegh.
2009. mode differences between face-to-face and web surveys: an experimental investi-gation of data quality and social desirability effects.
international journal of public opinion research,21(1):111–121..christiane heimann, sandra m¨uller, hannes scham-mann, and janina st¨urner.
2019. challenging thenation-state from within: the emergence of trans-municipal solidarity in the course of the eu refugeecontroversy.
social inclusion, 7(2):208–218..c. hutto and eric gilbert.
2014. vader: a parsimo-nious rule-based model for sentiment analysis of so-cial media text.
proceedings of the internationalaaai conference on web and social media, 8(1)..alexander l. janus.
2010. the inﬂuence of social de-sirability pressures on expressed immigration atti-tudes*.
social science quarterly, 91(4):928–946..sebastian koos and verena seibel.
2019. solidaritywith refugees across europe.
a comparative analysisof public support for helping forced migrants.
euro-pean societies, 21(5):704–728..anabel kuntz, eldad davidov, and moshe semyonov.
2017. the dynamic relations between economicconditions and anti-immigrant sentiment: a natu-ral experiment in times of the european economiccrisis.
international journal of comparative sociol-ogy, 58(5):392–415..christian lahusen and maria t. grasso, editors.
2018.solidarity in europe: citizens’ responses in timesof crisis.
palgrave studies in european political so-ciology.
springer international publishing..drew margolin and wang liao.
2018. the emotionalantecedents of solidarity in social media crowds.
new media & society, 20(10):3700–3719..adina nerghes and ju-sung lee.
2019. narrativesof the refugee crisis: a comparative study ofmainstream-media and twitter.
media and commu-nication, 7(2 refugee crises disclosed):275–288..francesco nicoli.
2017. hard-line euroscepticism andthe eurocrisis: evidence from a panel study of 108elections across europe.
jcms: journal of commonmarket studies, 55(2):312–331..laura ana maria oberl¨ander and roman klinger.
2018.an analysis of annotated corpora for emotion clas-siﬁcation in text.
in proceedings of the 27th inter-national conference on computational linguistics,pages 2104–2119..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
”why should i trust you?”: explain-ing the predictions of any classiﬁer.
in proceedingsof the 22nd acm sigkdd international conferenceon knowledge discovery and data mining, pages1135–1144.
association for computing machinery..sashank santhanam, vidhushini srinivasan, shainaglass, and samira shaikh.
2019. i stand with you:using emojis to study solidarity in crisis events.
arxiv preprint..hilary silver.
1994. social exclusion and social solidar-ity: three paradigms.
international labour review,133(5-6):531–578..cass r. sunstein.
2018.
#republic: divided democ-racy in the age of social media.
princeton universitypress..zeynep tufekci.
2014. social movements and govern-ments in the digital age: evaluating a complex land-scape.
journal of international affairs, 68(1):1–18..stefan wallaschek.
2017. notions of solidarity in eu-rope’s migration crisis: the case of germany’s me-dia discourse..stefan wallaschek.
2019..solidarity in europe injournal of european integration,.
times of crisis.
41(2):257–263..stefan wallaschek.
2020a.
contested solidarity in theeuro crisis and europe’s migration crisis: a dis-course network analysis.
journal of european pub-lic policy, 27(7):1034–1053..stefan wallaschek.
2020b.
the discursive constructionof solidarity: analysing public claims in europe’smigration crisis.
political studies, 68(1):74–92..wei zhao, steffen eger, johannes bjerva, and is-inducing language-arxiv.
abelle augenstein.
2020a.
agnostic multilingualpreprint arxiv:2008.09112..representations..wei zhao, goran glavaˇs, maxime peyrard, yang gao,robert west, and steffen eger.
2020b.
on thelimitations of cross-lingual encoders as exposed byinreference-free machine translation evaluation..1633proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1656–1671, online.
association for computational lin-guistics..1634a appendices.
guidelines.
read the guidelines for annotating solidarity care-fully..• deﬁnition of solidarity:.
the preparedness to share one’sown resources with others, be thatdirectly by donating money or timein support of others or indirectlyby supporting the state to reallocateand redistribute some of the fundsgathered through taxes or contribu-tions (lahusen and grasso, 2018).
• general rules.
1. do not take links (urls) into account.
when annotating..2. hashtags should be taken into account,especially if a tweet is otherwise neutral.
3. emojis, if easily interpretable, can be.
taken into account..4. if solidarity and anti-solidarity hashtags.
are used, code anti-solidarity..5. when annotating use the scheme: soli-darity: 0, anti-solidarity: 1, ambivalent:2, not applicable: 3..• detailed rules for annotation..1. a tweet is annotated as showing solidar-.
ity, when:(a) it clearly indicates support peopleand the willingness to share re-sources and/or help..(b) positive attitude and gratitude tothose sharing resources and/or help-ing..(c) advocacy of the european union as.
a solidarity union..(d) criticism of the eu in terms ofnot doing enough to share resourcesand/or help others..(e) hashtags can be to be takeninto account asto whether atweet qualiﬁes as showing soli-darity (e.g.
using hashtags like#refugeeswelcome)..(f) hashtags.
should be taken intoaccount if the tweet points neither.
towards solidarity or anti-solidarityitself..2. a tweet is annotated as showing anti-.
solidarity, when:(a) it clearly indicates no willingness tosupport people and an unwillingnessto share resources and/or help.
(b) it suggests to exclude our targetgroups from resources they currentlyhave access to..(c) tendencies of nationalism and clos-.
ing borders..(d) irony/sarcasm in tweets need to be.
taken into account..(e) hashtags can be taken into accountas to whether a tweet qualiﬁes as anti-solidarity (e.g.
using hashtags like#grexit)..(f) hashtags.
should be taken intoaccount if the tweet points neithertowards solidarity or anti-solidarityitself..3. a tweet is annotated ambivalent, when:(a) the tweet shows solidarity or anti-solidarity sentiment, but it cannot bedetermined whether the tweet showssolidarity or anti-solidarity as thereis additional info missing..(b) even if taking hashtags into account,there is no clear indication as towhether the author shows solidarityor anti-solidarity..(c) if solidarity and anti-solidarity hash-tags are used, code anti-solidarity..4. a tweet is annotated not applicable,.
when:(a) there is no indication of solidarity oranti-solidarity sentiment in the tweet.
(b) even when hashtags that usuallypointtowards solidarity or anti-solidarity are taken into account thetweet does not indicate any connec-tion to solidarity or anti-solidarity.
(c) the tweet concerns completely dif-ferent topics than solidarity or anti-solidarity..(d) the tweet is not understandable (e.g..1635contains only links)..hashtags.
refugee crisis hashtags#asylumseeker#asylumseekers#asylkrise#asylrecht#asylverfahren#ﬂ¨uchtling#ﬂ¨uchtlinge#ﬂ¨uchtlingskrise#ﬂ¨uchtlingswelle#leavenoonebehind#migrationskrise#niewieder2015#opentheborders#refugee#refugees#refugeecrisis#refugeesnotwelcome#refugeeswelcome#rightofasylum#remigration#seenotrettung#standwithrefugees#wirhabenplatz#wirschaffendas.
finance crisis hashtags#austerity + #eu#austerity + #euro#austerity + #eurobonds#austerity + #europe#austerity + #eurozone#austerit¨at + #eu#austerit¨at + #euro#austerit¨at + #eurobonds#austerit¨at + #europa#austerit¨at + #eurozone#debtunion#eurobonds#eurocrisis#eurokrise#eusolidarity#eusolidarit¨at#exiteu#ﬁscalunion#ﬁskalunion#schuldenunion#transferunion.
table 6: hashtags used in our experiments..infection numbers vs. anti-solidaritytweets.
1636scatter plot between infection ratesthe.
figure 4:eu.
bottom:basedondownload-todays-data-geographic-distribution-covid-19-cases-worldwide,28 eu countries..top:frame underconsideration is 01.03.2020 to 14.12.2020https://www.ecdc.europa.eu/en/publications-data/to.
anti-solidarity tweets..germany.
the.
and number of.
restricted.
from.
time.
data.
1637