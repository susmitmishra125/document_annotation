defense against synonym substitution-based adversarial attacksvia dirichlet neighborhood ensemble.
yi zhou1,2, xiaoqing zheng*1,2,cho-jui hsieh3, kai-wei chang3, xuanjing huang1,21school of computer science, fudan university, shanghai, china2shanghai key laboratory of intelligent information processing3department of computer science, university of california, los angeles, usa@fudan.edu.cn, chohsieh@cs.ucla.edu,yizhou17, zhengxq}.
kwchang@cs.ucla.edu, xjhuang@fudan.edu.cn.
{.
abstract.
although deep neural networks have achievedprominent performance on many nlp tasks,they are vulnerable to adversarial examples.
we propose dirichlet neighborhood ensemble(dne), a randomized method for training a ro-bust model to defense synonym substitution-based attacks.
during training, dne formsvirtual sentences by sampling embedding vec-tors for each word in an input sentence from aconvex hull spanned by the word and its syn-onyms, and it augments them with the trainingdata.
in such a way, the model is robust to ad-versarial attacks while maintaining the perfor-mance on the original clean data.
dne is ag-nostic to the network architectures and scalesto large models (e.g., bert) for nlp appli-cations.
through extensive experimentation,we demonstrate that our method consistentlyoutperforms recently proposed defense meth-ods by a signiﬁcant margin across different net-work architectures and multiple data sets..1.introduction.
deep neural networks are powerful but vulnera-ble to adversarial examples that are intentionallycrafted to fool the networks.
recent studies haveshown the vulnerability of deep neural networks inmany nlp tasks, including reading comprehension(jia and liang, 2017), text classiﬁcation (samantaand mehta, 2017; wong, 2017; liang et al., 2018;alzantot et al., 2018), machine translation (zhaoet al., 2018; ebrahimi et al., 2018; cheng et al.,2018), dialogue systems (cheng et al., 2019), anddependency parsing (zheng et al., 2020).
thesemethods attack an nlp model by replacing, scram-bling, and erasing characters or words under certainsemantic and syntactic constraints.
in particular,most of them craft adversarial examples by substi-tuting words with their synonyms in an input textto maximally increase the prediction error whilemaintaining the adversarial examples’ ﬂuency and.
naturalness.
in this paper, we focus on these wordsubstitution-based threat models and discuss thestrategy to defend against such attacks..the goal of adversarial defenses is to learn amodel capable of achieving high test accuracy onboth clean and adversarial examples.
adversar-ial training is one of the most successful defensemethods for nlp models (miyato et al., 2017; satoet al., 2019; zhu et al., 2019).
during the trainingtime, they replace a word with one of its synonymsthat maximizes the prediction loss.
by augmentingthese adversarial examples with the original train-ing data, the model is robust to such perturbations.
however, it is infeasible to explore all possiblecombinations where each word in a sentence canbe replaced with any of its synonyms.
also, whenupdating word embeddings during training, the dis-tance between a word and its synonyms in the em-bedding space change dynamically.
therefore, thepoint-wise guarantee becomes insufﬁcient, and theresulting models have shown to be vulnerable tostrong attacks (alzantot et al., 2018)..on the other hand, several certiﬁed defense meth-ods have recently been proposed to ensure that themodel predictions are unchanged when input wordembeddings are perturbed within the convex hullformed by the embeddings of a word and its syn-onyms (jia et al., 2019; huang et al., 2019).
how-ever, due to the difﬁculty of propagating convexhull through deep neural networks, they computea loose outer bound using interval bound prop-agation (ibp).
as a result, the convex hull maycontain irrelevant words and lead to a signiﬁcantperformance drop on the clean data..in this paper, we propose dirichlet neighbor-hood ensemble (dne) to create virtual sentencesby mixing the embedding of the original word inthe input sentence with its synonyms.
by trainingon these virtual sentences, the model can enhancethe robustness against word substitution-based per-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5482–5492august1–6,2021.©2021associationforcomputationallinguistics5482turbations.
speciﬁcally, our method samples anembedding vector in the convex hull formed by aword and its synonyms to ensure the robustnesswithin such a region.
in contrast to ibp, our ap-proach better represents the synonyms’ subspaceby creating virtual sentences.
to deal with com-plex error surface (e.g., surfaces containing multi-ple hills and valleys), a gradient-guided optimizeris applied to search for the most vulnerable pointswithin the convex hull.
by minimizing the errorwith these vulnerable points, we can guarantee withhigh probability that the resulting model is robustat any point within the convex hull (i.e., a set ofsynonyms).
the framework can be extended tohigher-order neighbors (synonyms) to boost the ro-bustness further.
in the inference time, the samedirichlet sampling technique is used, and the pre-diction scores on the virtual sentences are ensem-bled to get a robust output..through extensive experiments with variousmodel architectures on multiple data sets, we showthat dne consistently achieves better performanceon clean and adversarial samples than existing de-fense methods.
by conducting a detailed analysis,we found that dne enables the embeddings of aset of similar words to be updated together in a co-ordinated way.
in contrast, prior approaches eitherﬁx the word vectors during training (e.g., in thecertiﬁed defenses) or update individual word vec-tors independently (e.g., in the adversarial training).
we believe it is the crucial property why dne leadsto a more robust nlp model.
furthermore, unlikemost certiﬁed defenses, the proposed method iseasy to implement and can be integrated into anyexisting neural network including those with largearchitecture such as bert (devlin et al., 2019)..2 related workin the text domain, adversarial training is one ofthe most successful defenses (miyato et al., 2017;sato et al., 2019; zhu et al., 2019).
a family offast-gradient sign methods (fgsm) was introducedby goodfellow et al.
(2015) to generate adversar-ial examples in the image domain.
they showedthat the robustness and generalization of machinelearning models could be improved by includinghigh-quality adversarial examples in the trainingdata.
miyato et al.
(2017) proposed an fgsm-like adversarial training method to the text domainby applying perturbations to the word embeddingsrather than to the original input itself.
sato et al.
(2019) extended the work of miyato et al.
(2017) to.
improve the interpretability by constraining the di-rections of perturbations toward the existing wordsin the word embedding space..zhang and yang (2018) applied several typesof noises to perturb the input word embeddings,such as gaussian, bernoulli, and adversarial noises,to mitigate the overﬁtting problem of nlp mod-els.
zhu et al.
(2019) proposed a novel adversar-ial training algorithm, called freelb (free large-batch), which adds adversarial perturbations toword embeddings and minimizes the resultant ad-versarial loss inside different regions around inputsamples.
they add norm-bounded adversarial per-turbations to the input sentences’ embeddings us-ing a gradient-based method and enlarge the batchsize with diversiﬁed adversarial samples under suchnorm constraints.
however, they focus on the ef-fects on generalization rather than the robustnessagainst adversarial attacks..recently a set of certiﬁed defenses has been in-troduced, which guarantees robustness to some spe-ciﬁc types of attacks.
for example, jia et al.
(2019)and huang et al.
(2019) use a bounding technique,interval bound propagation (ibp) to formally ver-ify a model’s robustness against word substitution-based perturbations.
shi et al.
(2020) and xu etal.
(2020) proposed the robustness veriﬁcation andtraining method for transformers based on linearrelaxation-based perturbation analysis.
however,these defenses often lead to loose upper boundsfor arbitrary networks and result in a higher costof clean accuracy.
furthermore, due to the difﬁ-culty of veriﬁcation, certiﬁed defense methods areusually not scalable and remain hard to scale tocomplex prediction pipelines.
to achieve certiﬁedrobustness on large architectures, ye et al.
(2020)proposed a certiﬁed robust method called saferwhich is structure-free.
however, the base classiﬁerof safer is trained by the adversarial data aug-mentation.
as shown in our experiments, randomlyperturbing a word to its synonyms performs poorlyin practice..in the image domain, randomization has beenshown to overcome many of these obstacles in theibp-based defense.
empirically, xie et al.
(2017)showed that random resizing and padding in theinput domain could improve the robustness.
liu etal.
(2018) proposed to add gaussian noise in boththe input layer and intermediate layers of cnn inboth training and inference time to improve the ro-bustness.
lecuyer et al.
(2019) provided a certiﬁed.
5483guarantee of this method, and later on, the bound issigniﬁcantly improved in cohen et al.
(2019).
theresulting algorithm, called randomized smoothing,has become widely used in certifying `2 robustnessfor image classiﬁers.
these random smoothingmethods are very much under-explored in nlpmodels.
the main reason is that the adversarialexamples in texts are usually generated by wordsubstitution-based perturbations instead of small `pnorm.
in this paper, we show that random smooth-ing can be integrating with adversarial training toboost the empirical robust accuracy..3 method.
we here consider a word substitution-based threatmodel, where every word in an input sentence canbe replaced with one of its synonyms.
given asentence and synonym sets, we would like to en-sure that the prediction of a model trained with ourmethod cannot be altered by any word substitution-based perturbation to the sentence.
however, thenumber of possible perturbations scales exponen-tially with sentence length, so data augmentationcannot cover all perturbations of an input sentence.
we use a convex hull formed by a word and its syn-onyms to capture word substitutions, which allowsus to search for the worse-case over the convex hull.
by minimizing the error with the worst-case, wecan guarantee with high probability that the modelis robust at any point within the convex hull (i.e., aset of synonyms)..the proposed method can be viewed as a kindof randomized defense on nlp models, where ourmain contribution is to show that it is essential toensure the model works well in a region within theconvex hull formed by the embeddings of a wordand its synonyms instead of only ensuring model isgood under discrete perturbation.
although dnedoes not provide certiﬁed lower bounds like ibp, itachieves much better accuracy on both clean andadversarial data on different models, datasets, andattacks compared with ibp.
dne also can be easilyintegrated into any neural networks, including largearchitecture such as bert..2x.
let f be a base classiﬁer which maps an inputsentence xto a class label y.
we considerthe setting where for each word xi in the sentence(xi) in-x, we are given a set of its synonymscluding xi itself, where we know replacing xi with(xi) is unlikely to change the semanticany of.
2y.
s.s.meaning of the sentence1.
we relax the set of dis-crete points (a word and its synonyms) to a convexhull spanned by the word embeddings of all these(xi).
we assume any perturba-points, denoted bytion within this convex hull will keep the semanticmeaning unchanged, and deﬁne a smoothed clas-siﬁer g(x) based on random sampling within theconvex hull as follows..c.g(x) = arg max.
pˆx(f (ˆx) = y).
(1).
y.
2y.
c.where ˆx is generated by replacing the embeddingof each word xi in the sentence x with a point ran-domly sampled from xi’s convex hull(xi).
in thetraining time, the base classiﬁer is trained with “vir-tual” data augmentation sampled in the embeddingspace, where each word xi is replaced with a point(xi) by the proposedin the convex hull containingsampling algorithm described section 3.1. a newadversarial training algorithm is also designed toenable nlp models to defense against the strongattacks that search for the worst-case over all com-binations of word substitutions.
a similar samplingstrategy is conducted in the inference time..c.note that it is impossible to precisely calculatethe probabilities with which f classiﬁes x as eachclass, so we use a monte carlo algorithm for evalu-ating g(x).
as shown in fig.
1 (a), for a sentencex, we draw k samples of ˆx by running k noise-corrupted copies of x through the base classiﬁerf (ˆx), where ˆx is generated by replacing the embed-ding of every word xj in the sentence x with a point(xj) (the pentagon withrandomly sampled fromcyellow dashed borders).
if the class y appearedwith maximal weight in the categorical distributionˆx, the smoothed classiﬁer g(x) returns y. the de-cision regions of the base classiﬁer are drawn indifferent colors if we evaluate the smoothed classi-ﬁer at an input xj, where the regions with differentcolors represent different classes..c.assuming that the word xi is replaced with xjby an adversary, we need to sample the points from(xj) in the inference time.
how-the convex hullever, some of xj’s synonyms (indicated by yellowcircles) are outside the region formed by xi andits synonyms (indicated by blue circles).
we thusshould expand this region to the polygon with greendashed borders to make sure that the model makesthe same prediction for any point sampled fromthe expanded region.
we ensure that the smoothed.
1follow jia et al.
(2019), we base our sets of word substi-.
tutions s(xi) on the method of alzantot et al.
(2018)..5484c(xj)xj.
c(xi).
xi.
(a).
c(xj)xj.
q3.
v1.
v2.
margin > 0.y.q1.
c(xi).
xi.
v3.
q2.
(b).
c.figure 1: consider a word (sentence of length one) xi and its convex hull(xi) (projected to 2d for illustration)spanned by the set of its synonyms (blue circles).
we assume that an adversary replaces xi with one of its synonymsxj.
(a) evaluating the smoothed classiﬁer at the input xj.
the decision regions of the base classiﬁer f are drawn in(xi) to the polygon with green dashedblue, green, and pink colors, representing different classes.
if we expandborders when training the base classiﬁer f , the size of the intersection of this polygon and(xj) is large enoughto ensure that the smoothed classiﬁer g labels xj as f (xi).
here, the region where g labels xj as f (xi) is “blue.”(xj)(b) an example convex hull used to train the base classiﬁer.
since the size of the intersection of(xi) to the convex hull spanned by xi’s neighbors and “neighbors of neighbors” in theiris small, we expandembedding space when training the base classiﬁer f .
starting from three points v1, v2 and v3 sampled from theexpanded convex hull (the largest polygon with green dashed borders), q1, q2 and q3 are the local “worst-case”points found by searching over the entire convex hull with the gradient-guided optimization method..(xi) and.
c.c.c.c.c.classiﬁer label xj as f (xi) by training the baseclassiﬁer to label the instances sampled from theexpanded region as f (xi) so that the blue region isalways larger than green, yellow and pink ones..3.1 dirichlet neighborhood samplingthe random perturbations of x are combinatorial,and thus training the base classiﬁer f that consis-tently labels any perturbation of x as y requireschecking an exponential number of predictions.
tobetter reﬂect those discrete word substitution-basedperturbations, we sample the points from a convexhull using the dirichlet distribution.
this allows usto control how far we can expect the points are fromany vertex of the convex hull.
if a sampled point isvery close to a vertex (i.e., a word), it simulates aword substitution-based perturbation in which thevertex is chosen to replace the original one.
any(xi) can be represented as apoint sampled fromc(xi):convex combination of the embeddings of.
⌫(xi) =.
 j.
xj,.
·.
xxj 2s.
(xi).
s.(2).
0, ⌃j j = 1, and xj (in bold type)where  j  denotes the embedding of xj.
a vector   containsthe weights drawn from the dirichlet distributionas follows:.
 1, .
.
.
,  m.dir(↵1, .
.
.
,↵ m),.
⇠where m is the size of(xi), and the dirichletsdistribution is parameterized by a vector of ↵ used(xi)to control the degree in which the words incontribute to generate the vector ⌫(xi)..s.(3).
3.2 training the base classiﬁer with.
two-hop neighbors.
for the smoothed classiﬁer g to classify an adver-sarial example of x correctly and robustly, f needsto consistently classify ˆx as the gold label of x.therefore, we train the base classiﬁer with virtualdata augmentation ˆx for each training example x.in fig.
1 (b), we illustrate the process by consider-ing a sentence with one word xi and the set of itssynonyms (shown as blue circles).
the input per-(xi) around theturbations span a convex hull ofword xi (the pentagon with blue borders, projectedto 2d here).
assuming that the word xi is replacedwith xj by an adversary, noise-corrupted samples(xj) (the pentagon with yel-will be drawn from(xi).
if the sizelow dashed borders) instead ofc(xj) is small, we(xi) andof the intersection ofccannot expect f will consistently classify xj as thesame label as xi.
therefore, we expand(xi) tothe convex hull spanned by the word embeddings(xi),of the union ofnamely xi’s 1-hop neighbors and 2-hop neighborsin their embedding space, denoted by.
c(xj), xj 2s(xi)..(xi) and all of.
s.s.c.c.c.e.we use.
x to denote a virtual example created byreplacing the embedding of every word xi in an in-put sentence x with a point randomly sampled from(xi) by the dirichlet distribution.
the expandedsuch expansions will slightly hurt the performanceon the clean data.
recall that different values of ↵can be used to control the degree in which the 1-x.hop and 2-hop neighbors contribute to generating.
b.b.e.5485in our implementation, we let the expected weightsof the 2-hop neighbors are less than one-half ofx asthose of the 1-hop neighbors when computingeq.
(2) to reduce the impact on the clean accuracy.
the base classiﬁer is trained by minimizing thecross-entropy error with virtual data augmentationby gradient descent.
we assume the base classiﬁertakes form f (x) = arg maxcsc(x), where eachsc(x) is the scoring function for the class c. thatis, the outputs of the neural networks before thesoftmax layer.
our objective is to maximize thesum of the log-probabilities that f will classifybe a training set ofeachn instances, and each of them is a pair of (x, y):.
x as the label of x. let.
d.2y.
e.e.=.
log p.x(f (.
x) = y).
(x,y)x8.
2d.
elog e.x1.
(x,y)x8.
2d.
e.earg max.
.
c.2y.
sc(.
x) = y.,.
 .
(4).
x is a virtual example randomly created forwherean input example x. the softmax function can beviewed as a continuous, differentiable approxima-etion of argmax:.
e.1.arg max.
sc(.
x) = y.exp(sy(.
x))exp(sc(.
..x)).
(5).
⇡.
c..
2y.
 by the concavity of log and jensen’s inequality, theobjective is approximately lower-bounded by:.
p.2y.
e.e.e.c.e.x.log.
(x,y)x8.
2d.
.
e.exp(sy(.
x))exp(sc(.
c.2y.
..x)).
 .
e.(6).
pthis is the negative cross-entropy loss with virtualdata augmentation.
maximizing eq.
(6) approxi-mately maximizes eq.
(4)..e.s.since the virtual data point deﬁned in eq.
(2) is(xi), thea linear combination of embeddings ofback-propagation will propagate the gradient to allthese embeddings with nonzero coefﬁcients, thusallowing updating all these embeddings togetherin a coordinated way when performing parameterupdates.
as illustrated in fig.
1, the whole convexhull will be shifted together at each iteration.
incontrast, traditional adversarial training only up-dates the embedding of one synonym (a vertex ofthe convex hull), which will distort the relative po-sition of those embeddings and thus become slowerand less stable.
it is probably why the word em-beddings are ﬁxed during training in the certiﬁeddefenses (huang et al., 2019; jia et al., 2019).
eventhough the word embeddings can be pre-trained,holding embeddings ﬁxed makes them impossibleto be ﬁne-tuned for the tasks of interest, which mayhurt the performance..3.3 adversarial training.
b.to promote higher robustness and invariance toany region within the convex hull, we further pro-pose combining dirichlet sampling with adversar-ial training to better explore different regions inside(xi).
any point sampled fromthe convex hull(xi) is represented as the convex combination ofbthe embeddings of its vertices, which ensures thata series of points keep staying inside of the same(xi) while searching for the worst-case over thebentire convex hull by any optimization method.
as-x is generated for ansuming that a virtual exampleinput sentence x, we search for the next adversarialexample to maximize the model’s prediction errorby updating every vector of weights   = exp(⌘)by the following formula, each of them is used to(xi) as eq.
(2):represent a point sampled from.
e.⌘.
⌘.
✏.
.
 .
p(.
x, y) =.
bx, y).
@ log p(@⌘.
    .
ex))exp(sy(exp(sc(.
c.,.
,.
2.
    x)).
(7).
e.e.e.p.2ywhere ✏ is the step size.
in order to ensure that0 and ⌃j j = 1 asthe updated   satisfy  j  before, we sequentially apply logarithmic and soft-max functions to   after it is randomly drawn fromdir(↵).
note that softmax(log( )) =  , and ⌘will be updated instead of   in our implementation.
by updating ⌘ only, the representation deﬁned ineq.
(2) also ensures that a series of points keep stay-ing inside of the same convex hull while searchingfor the worst-case over.
(xi).
as shown in fig.
1 (b), we apply this update mul-tiple times with a small step size (arrow-linked redcircles represent data points generated after eachupdate by adding gradient-guided perturbations totheir preceding ones).
when training the base clas-siﬁer, we add all of the virtual examples generatedat every search step (i.e., all of the points indicatedby the red circles in fig.
1 (b)) into the training setto better explore different regions around x..b.
3.4 ensemble method.
as mentioned above, we use a monte carlo algo-rithm for evaluating g(x).
given an input sentencex, we draw k monte carlo samples of ˆx by runningk noise-corrupted copies of x through the base clas-siﬁer f (ˆx), where each ˆx is created by replacingthe embedding of every word xi in the sentencex with a point randomly sampled with the dirich-(xi) (not from the expandedlet distribution fromconvex hull.
(xi) in the inference time)..c.b.
5486we combine predictions by taking a weightedaverage of the softmax probability vectors of allthe randomly created ˆx, and take the argmax of thisaverage vector as the ﬁnal prediction.
we use cbw-d (dubey et al., 2019) to compute those weights.
the idea behind it is to give more weight to the pre-dictions that have more conﬁdence in their results.
cbw-d calculates the weights w as a function ofthe differences between the maximum value of thesoftmax distribution and the other values as follows:.
w =.
(p(ˆx, y).
p(ˆx, c))r,.
(8).
,cxc2y.
=y.
 .
where y is the class having the maximum proba-bility in a prediction, r is a hyperparameter tunedusing cross-validation in preliminary experiments..4 experimentswe conducted experiments on multiple data sets fortext classiﬁcation and natural language inferencetasks.
various model architectures (bag-of-words,cnn, lstm, and attention-based) were used toevaluate our dne and other defense methods undertwo recently proposed attacks.
ren et al.
(2019)described a greedy algorithm, called probabilityweighted word saliency (pwws), for adversar-ial text attacks based on word substitutions withsynonyms.
the word replacement order is deter-mined by taking both word saliency and predictionprobability into account.
alzantot et al.
(2018) de-veloped a generic algorithm-based attack, denotedby ga, to generate semantically and syntacticallysimilar adversarial examples.
they use a languagemodel (lm) (chelba et al., 2018) to rule out can-didate substitute words that do not ﬁt within thecontext.
however, unlike pwws, ruling out somecandidates by the lm will signiﬁcantly reduce thenumber of candidate substitute words (65% off onaverage).
for a fair comparison, we report therobust accuracy under ga attack both with andwithout using the lm.
we measure adversarial ac-curacy on perturbations found by the two attacks(pwws and ga) on 1, 000 randomly selected testexamples for each data set..we primarily compare with recently proposeddefense methods, including adversarial training(adv) (michel et al., 2019) and the interval boundpropagation (ibp) based methods (huang et al.,2019; jia et al., 2019).
the former can improve themodel’s robustness without suffering many dropson the clean input data by adding adversarial exam-ples in the training stage.
the latter was shown to.
be more robust to word substitution-based pertur-bations than ones trained with data augmentation.
to demonstrate that mixing the embedding of theoriginal word with its synonyms performs betterthan naively replacing the word with its synonyms,we designed a new baseline, called ran.
the mod-els trained by ran will take the corrupted copiesof each input sentence as inputs, in which everyword of the sentence is randomly replaced with oneof its synonyms.
the same random replacementis used in the inference time, and the predictionscores are ensembled to get an output.
ran canbe viewed as a variant of safer (ye et al., 2020),where during the training safer’s perturbation setis replaced with the synonym set used by the adver-saries and the number of ensembles is reduced to16 (instead of 5, 000) at the inference time, whichmake it feasible to be evaluated empirically underthe attacks..4.1 text classiﬁcationwe experimented on two text classiﬁcation datasets: internet movie database (imdb) (maas et al.,2011) and ag news corpus (agnews) (zhanget al., 2015).
we implemented three models forthese text classiﬁcation tasks like (jia et al., 2019).
the bag-of-words model (bow) averages the wordembeddings for each word in the input, then passesthis through a one-layer feedforward network with100-dimensional hidden state to get a ﬁnal logit.
the other two models are similar, except they runeither a cnn or a two-layer lstm on the word em-beddings.
all models are trained on cross-entropyloss, and their hyperparameters are tuned on thevalidation set (see appendix a.1 for details)..table 1 reports both clean accuracy (cln) andaccuracy under two attack algorithms (pwws andga) on imdb with three different model architec-tures (bow, cnn, and lstm).
we use ga-lmto denote the ga-based attack that rules out candi-date substitute words that may not ﬁt well with thecontext by the lm (chelba et al., 2018).
we useorig to the testing and adversarial accuracy of themodels trained without using any defense method.
as we can see from table 1, dne (k = 16)outperforms adv and ibp on the clean input data,and consistently performs better than the competi-tors across the three different architectures underall the attack algorithms.
for the text classiﬁca-tion, lstms seem more vulnerable to adversarialattacks than bows and cnns.
under the strongestattack ga, while the accuracy of lstms trained by.
54876table 1: text classiﬁcation on imdb dataset..imdb.
bow.
cnncln pwws ga-lm ga cln pwws ga-lm ga cln pwws ga-lm ga0.232.064.335.582.8.
0.0 89.977.2 87.066.9 79.633.7 87.779.0 84.4.
0.1 89.672.0 85.670.9 76.839.9 88.577.8 87.5.
4.880.070.556.181.3.
4.576.075.058.979.9.
14.656.664.756.084.3.
1.177.475.474.281.1.
2.535.472.271.584.0.
2.672.176.375.079.6.lstm.
orig 90.2adv 86.479.6ibpran 87.884.5dne.
table 2: text classiﬁcation on agnews dataset..bow.
agnews cln pwws ga-lm ga cln pwws ga-lm ga cln pwws ga-lm ga11.935.3orig 90.678.880.288.8adv77.986.287.4ibp51.978.289.0ran89.490.389.5dne.
25.7 91.582.5 88.481.3 87.851.3 88.788.7 91.3.
12.5 92.275.3 92.482.7 84.051.7 92.189.1 92.0.
55.082.586.774.489.6.
58.487.182.981.491.0.
68.885.786.875.289.1.
63.884.585.178.189.1.
48.885.482.381.491.2.lstm.
cnn.
orig, adv, ibp, and ran dropped to 0.2%, 32%,64.3%, and 8.1% respectively, the lstm trainedby dne still achieved 82.2% accuracy.
the re-sults on agnews are reported in table 2, andwe found similar trends as those on imdb.
anymodel performed on agnews shows to be morerobust than the same one on imdb.
it is proba-bly because the average length of the sentences inimdb (255 words on average) is much longer thanthat in agnews (43 words on average).
longersentences allow the adversaries to apply more wordsubstitution-based perturbations to the examples.
generally, dne performs better than ibp and com-parable to adv on the clean data, while it out-performs the others in all other cases.
the resultsfor both datasets show that our dne consistentlyachieves better clean and robust accuracy..4.2 natural language inferencewe conducted the experiments of natural languageinference on stanford natural language inference(snli) (bowman et al., 2015) corpus.
we also im-plemented three models for this task.
the bag-of-words model (bow) encodes the premise and hy-pothesis separately by summing their word vectors,then feeds the concatenation of these encodings toa two-layer feedforward network.
the other twomodels are similar, except they run either a decom-posable attention (decomatt) (parikh et al., 2016)or bert (devlin et al., 2019) on the word em-beddings to generate the sentence representations,which uses attention between the premise and hy-pothesis to compute richer representations of eachword in both sentences.
all models are trained withcross-entropy loss, and their hyperparameters aretuned on the validation set (see appendix a.2)..as reported in table 3, dne generally performs.
better than the others on the robust accuracy whilesuffering little performance drop on the clean dataon snli.
although our proposed baseline ran(k = 16) achieves a slightly higher accuracy (just1.2% difference) with bert under pwws attack,its accuracy rapidly drops to 27% under the moresophisticated attack ga, while dne still yields62.7% in accuracy.
the results on snli showthat dne can be applied to attention-based mod-els like decomatt and scales well to large archi-tectures such as bert.
we leave the results ofibp with bert as unknown since it is still a ques-tion whether ibp-based methods can be applied tobert..c.b.
(xi) instead of the expanded.
4.3 ablation studywe conducted an ablation study over imdb valida-tion set on dne with cnns to analyze the robust-ness and generalization strength of different vari-ants.
the “w/o expansion” in the second rowof table 4 indicates that given any word xi in a sen-tence, we generate virtual examples by sampling(xi) duringfromthe training.
the variant of dne trained withoutusing the adversarial training algorithm describedin section 3.3 is indicated by “w/o adv-train”.
if the single-point update strategy is applied totrain dne, we still use the same gradient-guidedoptimization method to ﬁnd adversarial examples(xi), but the found adversarial example xjoveris represented as xi +  , where   is the distancebetween xi and xj.
by such representation, onlyxi will be updated during the training instead of theembeddings of all its synonyms, and this variant isindicated by “w/o coord-upd”.
in the last row,we also report the results predicted without usingthe ensemble method (i.e., k = 1)..b.
5488table 3: natural language inference on snli dataset..snli.
orig 80.9adv 80.479.3ibpran 79.079.8dne.
bow.
decomattcln pwws ga-lm ga cln pwws ga-lm ga cln pwws ga-lm ga19.958.2.
42.668.2.
56.779.0.bert.
24.467.974.965.776.3.
41.671.075.044.475.3.
8.26 81.259.5 81.971.0 77.327.8 80.371.5 80.2.
23.171.772.867.277.4.
40.873.873.751.176.7.
8.1 90.565.2 89.470.5  30.6 89.974.6 90.1.
  72.771.5.
  42.780.1.
  27.062.7.table 4: ablation study on imdb..cln pwws ga-lm gamodel75.486.2dne45.00.1w/o expansion34.6w/o adv-train +1.612.80.0w/o coord-upd9.40.4w/o ensemble.
79.424.019.89.07.0.
81.414.27.84.21.8.
 .
    .
    .
    .
  .
as we can see from table 4, the differences inaccuracy among the variants of dne are negligibleon the clean data.
the key components to improvethe robustness of the models in descending orderby their importance are the following: samplingfrom the expanded convex hull, combining with ad-versarial training, updating the word embeddingstogether, and using the ensemble to get the predic-tion.
we also observed that the stronger the attackalgorithm is, the more effective these componentswill be.
when both “expansion” and “adversarial”are removed, the resulting accuracies on the vali-dation set of imdb dataset with the cnn-basedmodel drop to 48.6% (pwws) and 17.0% (ga).
in all the above experiments, we simply set thevalue of ↵ for 1-hop neighbors to 1.0, and that for2-hop neighbors to 0.5. we also conducted twoexperiments to investigate whether the dirichletdistribution is essential.
in the ﬁrst one, we uni-formly sample the weights (by setting the value of↵ for both 1-hop and 2-hop neighbors to 1.0) anddo an adversarial training step.
the clean accuracyis 82.4%, and the accuracies under the pwws andga attacks are 79.8% and 78.21% respectively.
inthe second experiment, we randomly sample a ver-tex from 2-hop neighbors and then do the sameadversarial training.
the resulting accuracies are85% (clean), 75.8% (pwws), and 54.6% (ga).
we found there is a trade-off between the cleanaccuracy and the accuracy under the attack.
gener-ally, the greater the value of ↵ is, the more robustthe models will be, but the worse they perform onthe clean data.
we also used different values of ↵in the dirichlet distribution to control the degreein which 1-hop and 2-hop neighbors contribute togenerating adversarial examples.
if we treat 2-hopneighbors equally as 1-hop ones, it will signiﬁ-.
cantly reduce the model’s accuracy on the cleandata, although it may lead to more robust models.
although this study mainly focuses on the set-ting speciﬁed by (jia et al., 2019), we also con-ducted experiments in which the defenders do notknow how the attackers generate synonyms.
weused the synonyms suggested by (alzantot et al.,2018) for training and evaluated the resulting mod-els with cnns and lstms on imdb and ag-news datasets under a new attack system, calledtextfooler (jin et al., 2020).
we strictly followedthe method proposed in (jin et al., 2020) to generatesynonyms during the attacking phase.
the experi-mental results show that dne achieved 30.6% and13.4% higher in average accuracy than adv onagnews and imdb respectively..5 conclusionin this study, we develop a novel defense algorithmfor nlp models to substantially improve the ro-bust accuracy without sacriﬁcing their performancetoo much on clean data.
this method is broadlyapplicable, generic, scalable, and can be incorpo-rated with little effort in any neural network, andscales to large architectures.
a novel adversarialtraining algorithm is also proposed, enabling nlpmodels to defend against the strong attacks thatsearch for the worst-case over all combinationsof word substitutions.
we demonstrated throughextensive experimentation that our adversariallytrained smooth classiﬁers consistently outperformall existing empirical and certiﬁed defenses by asigniﬁcant margin on three datasets across differentnetwork architectures, establishing state-of-the-artfor defenses against adversarial text attacks..we choose to focus on synonym swapping be-cause it is one of the most inﬂuential and widely-used attack methods.
there is still no effectivemethod to defend against existing attack algorithmsfrom this kind, such as hotﬂip (ebrahimi et al.,2018), pwws (2019), ga (2018), textfooler (jinet al., 2020) etc.
a general method to defend moredifferent attacks is worth exploring, but we chooseto leave this as future work..5489acknowledgements.
this work was supported by shanghai munici-pal science and technology major project (no.
2021shzdzx0103), national science foundationof china (no.
62076068) and zhangjiang lab..references.
moustafa alzantot, yash sharma, ahmed elgohary,bo-jhang ho, mani srivastava, and kai-wei chang.
2018. generating natural language adversarial ex-amples.
in proceedings of the conference on empir-ical methods in natural language processing..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large an-notated corpus for learning natural language infer-ence.
in proceedings of the conference on empiri-cal methods in natural language processing..ciprian chelba, tomas mikolov, mike schuster, qi ge,thorsten brants, phillipp koehn, and tony robin-son.
2018. one billion word benchmark for measur-ing progress in statistical language modeling.
com-puting research repository, arxiv: 1312.3005..minhao cheng, wei wei, and cho-jui hsieh.
2019.evaluating and enhancing the robustness of dialoguesystems: a case study on a negotiation agent.
inconference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies..minhao cheng, jinfeng yi, huan zhang, pin-yu chen,and cho-jui hsieh.
2018. seq2sick: evaluatingthe robustness of sequence-to-sequence models withadversarial examples.
computing research reposi-tory, arxiv: 1803.01128..jeremy cohen, elan rosenfeld, and zico kolter.
2019. certiﬁed adversarial robustness via random-ized smoothing.
in proceedings of the internationalconference on machine learning..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies..abhimanyu dubey, laurens van der maaten, zeki yal-niz, yixuan li, and dhruv mahajan.
2019. defenseagainst adversarial images using web-scale nearest-neighbor search.
in proceedings of the conferenceon computer vision and pattern recognition..javid ebrahimi, anyi rao, daniel lowd, and dejingdou.
2018. hotflip: white-box adversarial exam-ples for text classiﬁcation.
in proceedings of the an-nual meeting of the association for computationallinguistics..ian j. goodfellow, jonathon shlens, and christianszegedy.
2015. explaining and harnessing adversar-in proceedings of the internationalial examples.
conference on learning representations..jeremy howard and sebastian ruder.
2018. universallanguage model ﬁne-tuning for text classiﬁcation.
inproceedings of the annual meeting of the associa-tion for computational linguistics..po-sen huang, robert stanforth, johannes welbl,chris dyer, dani yogatama, sven gowal, krish-namurthy dvijotham, and pushmeet kohli.
2019.achieving veriﬁed robustness to symbol substitu-in proceed-tions via interval bound propagation.
ings of the conference on empirical methods in nat-ural language processing..robin jia and percy liang.
2017. adversarial ex-amples for evaluating reading comprehension sys-tems.
in proceedings of the conference on empir-ical methods in natural language processing..robin jia, aditi raghunathan, kerem g¨oksel, andpercy liang.
2019. certiﬁed robustness to adversar-in proceedings of the con-ial word substitutions.
ference on empirical methods in natural languageprocessing..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
2020.is bert really robust?
a strongbaseline for natural language attack on text classiﬁ-cation and entailment.
in proceedings of the aaaiconference on artiﬁcial intelligence..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the international conference on learning repre-sentations..mathias lecuyer, vaggelis atlidakis, roxana geam-basu, daniel hsu, and suman jana.
2019. certiﬁedrobustness to adversarial examples with differentialprivacy.
in 2019 ieee symposium on security andprivacy (sp), pages 656–672.
ieee..bin liang, hongcheng li, miaoqiang su, pan bian,xirong li, and wenchang shi.
2018. deep text clas-siﬁcation can be fooled.
in proceedings of the inter-national joint conference on artiﬁcial intelligence..xuanqing liu, minhao cheng, huan zhang, and cho-jui hsieh.
2018. towards robust neural networksin proceedings of thevia random self-ensemble.
european conference on computer vision (eccv),pages 369–385..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analysis.
in proceedings of the annual meeting of the associ-ation for computational linguistics..paul michel, xian li, graham neubig,.
andjuan miguel pino.
2019. on evaluation of ad-versarial perturbations for sequence-to-sequence.
5490models.
north american chapter ofcomputational linguistics:technologies..in proceedings of the conference of thethe association forhuman language.
takeru miyato, andrew m dai, and ian goodfel-low.
2017. adversarial training methods for semi-supervised text classiﬁcation.
in proceedings of theinternational conference on learning representa-tions..ankur parikh, oscar t¨ackstr¨om, dipanjan das, andjakob uszkoreit.
2016. a decomposable attentionin proceed-model for natural language inference.
ings of the conference on empirical methods in nat-ural language processing..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordin proceedings of the conferencerepresentation.
on empirical methods in natural language process-ing..shuhuai ren, yihe deng, kun he, and wanxiang che.
2019. generating natural language adversarial ex-amples through probability weighted word saliency.
in proceedings of the annual meeting of the associ-ation for computational linguistics..dongxu zhang and zhichao yang.
2018. word embed-ding perturbation for sentence classiﬁcation.
com-puting research repository, arxiv: 1804.08166..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation.
in proceedings of the conference on neu-ral information processing systems..zhengli zhao, dheeru dua, and sameer singh.
2018.in pro-generating natural adversarial examples.
ceedings of the international conference on learn-ing representations..xiaoqing zheng, jiehang zeng, yi zhou, cho-juihsieh, minhao cheng, and xuanjing huang.
2020.evaluating and enhancing the robustness of neuralnetwork-based dependency parsing models with ad-in proceedings of the annualversarial examples.
meeting of the association for computational lin-guistics..chen zhu, yu cheng, zhe gan, siqi sun, tom gold-stein, and jingjing liu.
2019. freelb: enhancedadversarial training for language understanding.
inproceedings ofthe international conference onlearning representations..suranjana samanta and sameep mehta.
2017. towardscrafting text adversarial samples.
computing re-search repository, arxiv: 1707.02812..appendix.
motoki sato, jun suzuki, shindo, and yuji matsumoto.
2019.interpretable adversarial perturbation in in-put embedding space for text.
in proceedings of theinternational joint conference on artiﬁcial intelli-gence..zhouxing shi, kai-wei chang huan zhang, minliehuang, and cho-jui hsieh.
2020. robustness veriﬁ-cation for transformers.
in proceedings of the inter-national conference on learning representations..catherine wong.
2017. dancin seq2seq: foolingtext classiﬁers with adversarial text example gen-eration.
computing research repository, arxiv:1712.05419..cihang xie, jianyu wang, zhishuai zhang, zhouren, and alan yuille.
2017. mitigating adversar-ial effects through randomization.
arxiv preprintarxiv:1711.01991..kaidi xu, zhouxing shi, huan zhang, yihan wang,kai-wei chang, minlie huang, bhavya kailkhura,xue lin, and cho-jui hsieh.
2020. automatic per-turbation analysis for scalable certiﬁed robustnessand beyond.
in advances in neural information pro-cessing systems..mao ye, chengyue gong, and qiang liu.
2020.safer: a structure-free approach for certiﬁed ro-in pro-bustness to adversarial word substitutions.
ceedings of the annual meeting of the associationfor computational linguistics.
association for com-putational linguistics..a.1 experimental details for textclassiﬁcation.
we report in table 5 and 6 the values of hyperpa-rameters used to train the text classiﬁcation models.
the values of hyperparameters in dirichlet neigh-borhood ensemble (dne) are listed in table 7.all the models were trained with the cross-entropyloss, and their hyper-parameters were tuned on thevalidation sets..table 5: hyperparameters for training the text classiﬁ-cation models..embedding hidden.
layer kernel.
modelbow 300, glove300, glovecnnlstm 300, glove.
100100100.
  12.
  3.
  .
a.2 experimental details for naturallanguage inference.
all the models were initialized by the pre-trainedglove word embeddings and trained with the cross-entropy loss.
their hyper-parameters were tunedon the validation sets..bag of words (bow): we use a bag-of-wordmodel with the same hyperparameters as shownin table 5 to encode the premise and hypothesis.
5491table 6: training hyperparameters for the text classi-ﬁcation (bow, cnn, and lstm) models.
the samevalues were used for all the settings (plain, data aug-mentation, and robust training)..hyperparameteroptimizerlearning ratedropout (embedding)weight decaybatch sizegradient clipepochs.
valueadam (kingma and ba, 2015).
0.5.
3.
10 .
1.
4.
⇥0.310 321, 1)20.
⇥.
(.
 .
table 7: hyperparameters of dne for text classiﬁca-tion and natural language inference tasks..hyperparameterdirichlet distribution ↵ (1-hop neighbors)dirichlet distribution ↵ (2-hop neighbors)step size ✏ (adversarial training)number of steps (adversarial training)parameter r (ensemble method).
value1.00.51033.
⇥.
separately by summing their word vectors, and thenfeeds the concatenation of these encodings to a two-layer feedforward network with a 300-dimensionalhidden state.
we used the adam optimizer (with3), and set the dropout10 a learning rate 0.5rate on word embedding to 0.3, the weight decay4, the batch size to 128, the maximumto 1number of epochs to 20, and the gradient clip to(.
1, 1) for the training.
decomposable attention (decomatt): we im-plemented the decomposable attention model asdescribed in (parikh et al., 2016) except for a fewdifferences listed as follows:.
10 .
⇥.
 .
• we did not normalize glove vectors (penning-.
ton et al., 2014)..• we used the adam optimizer (with a learning.
rate of 0.5.
10 .
3) instead of adagrad..• we used a dropout rate of 0.3 on word embed-.
⇥.
dings..(.
1, 1).. .
• we used a batch size of 128 instead of 4.
• we clipped the value of gradients to be within.
• we set the value of weight decay to 1• the intra-sentence attention was not used..⇥.
10 .
4..bidirectional encoder representations fromtransformers (bert): we implemented bertas described in (devlin et al., 2019) except for afew differences listed below:.
• we applied a “bert-base-uncased” architec-ture (12-layer, 768-hidden, 12-heads, 110mparameters)..• we use the adam optimizer with a learning.
rate of 0.4.
10 .
4..⇥• we used a batch size of 8.
• we set the number of epochs to 3.
• we clipped the value of gradients to be within.
(.
1, 1).. .
4.
• we set the value of weight decay to 1• we used slanted triangular learning rates de-.
10 .
⇥.
scribed in (howard and ruder, 2018)..we report in table 7 the hyperparameter valuesof dirichlet neighborhood ensemble (dne) usedfor snli benchmark, and they were tuned on thevalidation set of snli..table 8: effect of parameter ↵ on imdb..↵,  0.1, 0.020.1, 0.10.1, 0.51.0, 0.021.0, 0.11.0, 0.5.cln86.286.284.885.685.181.6.pwws79.081.482.278.880.478.6.ga-lm ga68.275.476.475.677.878.2.
76.079.479.880.480.879.4.c.b.
2.a.3 effect of parameters of dirichletdistributiongiven a word xi, different values of ↵ are usedto control how much its 1-hop and 2-hop neigh-bors contribute to generating virtual adversarialexamples.
the value also determines the size of(xi).
in order to(xi) tothe expansion fromreduce the impact on the clean accuracy, we letthe expected weights of the 2-hop neighbors are (0, 0.5] times of those of the (1-hop) nearestneighbors.
we tried a few different values of ↵ and  on imdb to understand how the choice of themimpact upon the performance.
as shown in table 8,we found that if the value of ↵ is ﬁxed, the greaterthe value of  , the more robust the models willbecome, but the worse they perform on the cleaninput data.
a small value of ↵ seems to be prefer-able, which allows us to simulate the discrete wordsubstitution-based perturbations better.
we foundthat 1-hop and 2-hop neighbors cannot be treatedequally; otherwise, it will signiﬁcantly reduce themodel’s accuracy on the clean data.
for example,if we uniformly sample the weights of 1-hop and 2-hop neighbors, the clean accuracy drops to 82.4%3.8%) on the validation set of the imdb dataset.
( .
5492