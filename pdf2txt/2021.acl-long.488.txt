joint biomedical entity and relation extraction withknowledge-enhanced collective inference.
tuan lai 1, heng ji 1, chengxiang zhai 1, quan hung tran 21university of illinois at urbana-champaign 2adobe research{tuanml2, hengji, czhai}@illinois.eduqtran@adobe.com.
abstract.
compared to the general news domain, infor-mation extraction (ie) from biomedical text re-quires much broader domain knowledge.
how-ever, many previous ie methods do not utilizeany external knowledge during inference.
dueto the exponential growth of biomedical pub-lications, models that do not go beyond theirﬁxed set of parameters will likely fall behind.
inspired by how humans look up relevant in-formation to comprehend a scientiﬁc text, wepresent a novel framework that utilizes exter-nal knowledge for joint entity and relation ex-traction named keci (knowledge-enhancedcollective inference).
given an input text,keci ﬁrst constructs an initial span graph rep-resenting its initial understanding of the text.
itthen uses an entity linker to form a knowledgegraph containing relevant background knowl-edge for the the entity mentions in the text.
tomake the ﬁnal predictions, keci fuses the ini-tial span graph and the knowledge graph intoa more reﬁned graph using an attention mecha-nism.
keci takes a collective approach to linkmention spans to entities by integrating globalrelational information into local representa-tions using graph convolutional networks.
ourexperimental results show that the frameworkis highly effective, achieving new state-of-the-art results in two different benchmark datasets:biorelex (binding interaction detection) andade (adverse drug event extraction).
for ex-ample, keci achieves absolute improvementsof 4.59% and 4.91% in f1 scores over the state-of-the-art on the biorelex entity and relationextraction tasks 1..1.introduction.
with the accelerating growth of biomedical publi-cations, it has become increasingly challenging tomanually keep up with all the latest articles.
as.
1the code is publicly available at https://github.com/.
laituan245/bio_relex.
figure 1: an example in the biorelex dataset.
uim isan abbreviation of “ubiquitin-interacting motif”.
ourbaseline scibert model incorrectly predicts the men-tion as a “dna” instead of a “protein motif”..a result, developing methods for automatic extrac-tion of biomedical entities and their relations hasattracted much research attention recently (li et al.,2017; fei et al., 2020; luo et al., 2020).
many re-lated tasks and datasets have been introduced, rang-ing from binding interaction detection (biorelex)(khachatrian et al., 2019) to adverse drug eventextraction (ade) (gurulingappa et al., 2012)..many recent joint models for entity and relationextraction rely mainly on distributional represen-tations and do not utilize any external knowledgesource (eberts and ulges, 2020; ji et al., 2020;zhao et al., 2020).
however, different from thegeneral news domain, information extraction forthe biomedical domain typically requires muchbroader domain-speciﬁc knowledge.
biomedicaldocuments, either formal (e.g., scientiﬁc papers)or informal ones (e.g., clinical notes), are writtenfor domain experts.
as such, they contain manyhighly specialized terms, acronyms, and abbrevia-tions.
in the biorelex dataset, we ﬁnd that about65% of the annotated entity mentions are abbre-viations of biological entities, and an example isshown in figure 1. these unique characteristicsbring great challenges to general-domain systemsand even to existing scientiﬁc language models that.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6248–6260august1–6,2021.©2021associationforcomputationallinguistics6248figure 2: keci operates in three main steps: (1) initial span graph construction (2) background knowledge graphconstruction (3) fusion of these two graphs into a ﬁnal span graph.
keci takes a collective approach to linkmultiple mentions simultaneously to entities by incorporating global relational information using gcns..do not use any external knowledge base duringinference (beltagy et al., 2019; lee et al., 2019).
for example, even though scibert (beltagy et al.,2019) was pretrained on 1.14m scientiﬁc papers,our baseline scibert model still incorrectly pre-dicts the type of the term uim in figure 1 to be“dna”, which should be a “protein motif” instead.
since the biomedical literature is expanding at anexponential rate, models that do not go beyondtheir ﬁxed set of parameters will likely fall behind..in this paper, we introduce keci (knowledge-enhanced collective inference), a novel end-to-endframework that utilizes external domain knowledgefor joint entity and relation extraction.
inspired byhow humans comprehend a complex piece of sci-entiﬁc text, the framework operates in three mainsteps (figure 2).
keci ﬁrst reads the input text andconstructs an initial span graph representing its ini-tial understanding of the text.
in a span graph, eachnode represents a (predicted) entity mention, andeach edge represents a (predicted) relation betweentwo entity mentions.
keci then uses an entitylinker to form a background knowledge graph con-taining all potentially relevant biomedical entitiesfrom an external knowledge base (kb).
for each en-tity, we extract its semantic types, its deﬁnition sen-tence, and its relational information from the exter-nal kb.
finally, keci uses an attention mechanismto fuse the initial span graph and the backgroundknowledge graph into a more reﬁned graph repre-.
senting the ﬁnal output.
different from previousmethods that link mentions to entities based solelyon local contexts (li et al., 2020b), our frameworktakes a more collective approach to link multiplesemantically related mentions simultaneously byleveraging global topical coherence.
our hypoth-esis is that if multiple mentions co-occur in thesame discourse and they are probably semanticallyrelated, their reference entities should also be con-nected in the external kb.
keci integrates globalrelational information into mention and entity rep-resentations using graph convolutional networks(gcns) before linking..the beneﬁt of collective inference can be illus-trated by the example shown in figure 2. the entitylinker proposes two candidate entities for the men-tion fkbp12; one is of semantic type “aa, peptide,or protein” and the other is of semantic type “geneor genome”.
it can be tricky to select the correctcandidate as fkbp12 is already tagged with thewrong type in the initial span graph (i.e., it is pre-dicted to be a “chemical” instead of a “protein”).
however, because of the structural resemblancebetween the mention-pair (cid:104)fk506, fkbp12(cid:105) andthe pair (cid:104)“organic chemical”, “aa, peptide, orprotein”(cid:105), keci will link fkbp12 to the entityof semantic type “aa, peptide, or protein”.
as aresult, the ﬁnal predicted type of fkbp12 will alsobe corrected to “protein” in the ﬁnal span graph..our extensive experimental results show that the.
6249proposed framework is highly effective, achiev-ing new state-of-the-art biomedical entity and re-lation extraction performance on two benchmarkdatasets: biorelex (khachatrian et al., 2019) andade (gurulingappa et al., 2012).
for example,keci achieves absolute improvements of 4.59%and 4.91% in f1 scores over the state-of-the-art onthe biorelex entity and relation extraction tasks.
our analysis also shows that keci can automati-cally learn to select relevant candidate entities with-out any explicit entity linking supervision duringtraining.
furthermore, because keci considerstext spans as the basic units for prediction, it canextract nested entity mentions..2 methods.
2.1 overview.
keci considers text spans as the basic units for fea-ture extraction and prediction.
this design choiceallows us to handle nested entity mentions (sohraband miwa, 2018).
also, joint entity and relationextraction can be naturally formulated as the taskof extracting a span graph from an input document(luan et al., 2019).
in a span graph, each noderepresents a (predicted) entity mention, and eachedge represents a (predicted) relation between twoentity mentions..given an input document d, keci ﬁrst enu-merates all the spans (up to a certain length) andembeds them into feature vectors (sec.
2.2).
withthese feature vectors, keci predicts an initial spangraph and applies a gcn to integrate initial rela-tional information into each span representation(sec.
2.3).
keci then uses an entity linker tobuild a background knowledge graph and appliesanother gcn to encode each node of the graph(sec.
2.4).
finally, keci aligns the nodes of theinitial span graph and the background knowledgegraph to make the ﬁnal predictions (sec.
2.5).
wetrain keci in an end-to-end manner without usingany additional entity linking supervision (sec.
2.6)..overall, the design of keci is partly inspiredby previous research in educational psychology.
students’ background knowledge plays a vital rolein guiding their understanding and comprehensionof scientiﬁc texts (alvermann et al., 1985; braaschand goldman, 2010).
“activating” relevant andaccurate prior knowledge will aid students’ readingcomprehension..2.2 span encoder.
our model ﬁrst constructs a contextualized rep-resentation for each input token using scibert(beltagy et al., 2019).
let x = (x1, ..., xn) be theoutput of the token-level encoder, where n denotesthe number of tokens in d. then, for each span siwhose length is not more than l, we compute itsspan representation si ∈ rd as:.
si = ffnng.
(cid:0)(cid:2)xstart(i), xend(i), ˆxi, φ(si)(cid:3)(cid:1) (1).
where start(i) and end(i) denote the start andend indices of si respectively.
xstart(i) and xend(i)are the boundary token representations.
ˆxi is anattention-weighted sum of the token representa-tions in the span (lee et al., 2017).
φ(si) is a fea-ture vector denoting the span length.
ffnng is afeedforward network with relu activations..2.3.initial span graph construction.
with the extracted span representations, we predictthe type of each span and also the relation betweeneach span pair jointly.
let e denote the set of entitytypes (including non-entity), and r denote the setof relation types (including non-relation).
we ﬁrstclassify each span si:.
ei = softmax(cid:0)ffnne(si)(cid:1).
(2).
where ffnne is a feedforward network mappingfrom rd → r|e|.
we then employ another networkto classify the relation of each span pair (cid:104)si, sj(cid:105):.
rij = softmax(cid:0)ffnnr.
(cid:0)(cid:2)si, sj, si ◦ sj.
(cid:3)(cid:1)(cid:1).
(3).
where ◦ denotes the element-wise multiplication,ffnnr is a mapping from r3×d → r|r|.
wewill use the notation rij[k] to refer to the predictedprobability of si and sj having the relation k..at this point, one can already obtain a valid out-put for the task from the predicted entity and rela-tion scores.
however, these predictions are basedsolely on the local document context, which can bedifﬁcult to understand without any external domainknowledge.
therefore, our framework uses thesepredictions only to construct an initial span graphthat will be reﬁned later based on information ex-tracted from an external knowledge source..to maintain computational efﬁciency, we ﬁrstprune out spans of text that are unlikely to be entitymentions.
we only keep up to λn spans with thelowest probability scores of being a non-entity.
thevalue of λ is selected empirically and set to be.
62500.5. spans that pass the ﬁlter are represented asnodes in the initial span graph.
for every span pair(cid:104)si, sj(cid:105), we create |r| directed edges from the noderepresenting si to the node representing sj.
eachedge represents one relation type and is weightedby the corresponding probability score in rij..let gs = {vs, es} denote the initial span graph.
we use a bidirectional gcn (marcheggiani andtitov, 2017; fu et al., 2019) to recursively updateeach span representation:.
(cid:88).
(cid:88).
(cid:18) (cid:126).
w(l).
k hl.
j +.
(cid:19).
(cid:126)b(l)k.rij[k].
(cid:126)hli =.
(cid:126)hli =.
sj ∈vs\{si}.
k∈r.
(cid:88).
(cid:88).
sj ∈vs\{si}.
k∈r.
(cid:18) (cid:126)w(l).
k hl.
j +.
(cid:126)b(l)k.(cid:19).
rji[k].
hl+1i = hl.
i + ffnn(l)a.relu.
(cid:32).
(cid:19)(cid:33).
(cid:18)(cid:2) (cid:126)hl.
i, (cid:126)hl.
i.
(cid:3).
(4).
where hli is the hidden feature vector of span si ati to be si (eq.
1).
ffnn(l)layer l. we initialize h0ais a feedforward network whose output dimensionis the same as the dimension of hli..after multiple iterations of message passing,each span representation will contain the globalrelational information of gs.
let hi denote the fea-ture vector at the ﬁnal layer of the gcn.
note thatthe dimension of hi is the same as the dimensionof si (i.e., hi ∈ rd)..2.4 background knowledge graph.
construction.
in this work, we utilize external knowledge fromthe uniﬁed medical language system (umls)(bodenreider, 2004).
umls consists of three maincomponents: metathesaurus, semantic network,and specialist lexicon and lexical tools.
themetathesaurus provides information about millionsof ﬁne-grained biomedical concepts and relationsbetween them.
to be consistent with the existingliterature on knowledge graphs, we will refer toumls concepts as entities.
each entity is anno-tated with one or more higher-level semantic types,such as anatomical structure, cell, or virus.
inaddition to relations between entities, there are alsosemantic relations between semantic types.
forexample, there is an affects relation from acquiredabnormality to physiologic function.
this infor-mation is provided by the semantic network..we ﬁrst extract umls biomedical entities fromthe input document d using metamap, an entity.
mapping tool for umls (aronson and lang, 2010).
we then construct a background knowledge graph(kg) from the extracted information.
more specif-ically, we ﬁrst create a node for every extractedbiomedical entity.
the semantic types of each en-tity node are also modeled as type nodes that arelinked with associated entity nodes.
finally, wecreate an edge for every relevant relation found inthe metathesaurus and the semantic network.
anexample kg is in the grey shaded region of figure2. circles represent entity nodes, and rectanglesrepresent nodes that correspond to semantic types.
note that we simply run metamap with the de-fault options and do not tune it.
in our experiments,we found that metamap typically returns many can-didate entities unrelated to the input text.
however,as to be discussed in section 3.4, we show thatkeci can learn to ignore the irrelevant entities..let gk = {vk, ek} denote the constructed back-ground kg, where vk and ek are the node andedge sets, respectively.
we use a set of umls em-beddings pretrained by maldonado et al.
(2019) toinitialize the representation of each node in vk.
wealso use scibert to encode the umls deﬁnitionsentence of each node into a vector and concatenateit to the initial representation.
after that, since gkis a heterogeneous relational graph, we use a rela-tional gcn (schlichtkrull et al., 2018) to updatethe representation of each node vi:.
(cid:32).
vl+1i = relu.
u(l)vl.
i +.
(cid:19)(cid:33).
(cid:18) 1ci,k.
u(l)k vl.
j.
(cid:88).
(cid:88).
k∈r.
vj ∈n ki.
(5)i is the feature vector of vi at layer l. n kwhere vliis the set of neighbors of vi under relation k ∈ r.ci,k is a normalization constant and set to be |n ki |.
after multiple iterations of message passing areperformed, the global relational information of thekg will be integrated into each node’s representa-tion.
let vi denote the feature vector at the ﬁnallayer of the relational gcn.
we further project eachvector vi to another vector ni using a simple feed-forward network, so that ni has the same dimensionas the span representations (i.e., ni ∈ rd)..2.5 final span graph prediction.
at this point, we have two graphs: the initial spangraph gs = {vs, es} (sec.
2.3) and the back-ground knowledge graph gk = {vk, ek} (sec.
2.4).
we have also obtained a structure-aware rep-resentation for each node in each graph (i.e., hi for.
6251(9).
similar to eq.
2 and eq.
3:(cid:98)ei = softmax(cid:0)ffnn(cid:99)rij = softmax(cid:0)ffnn.
(cid:98)e(fi)(cid:1)(cid:98)r((cid:2)fi, fj, fi ◦ fj(cid:98)e is a mapping from rd → r|e|, andwhere ffnn(cid:98)r is a mapping from r3×d → r|r|.
(cid:98)ei is theffnnﬁnal predicted probability distribution over possi-ble entity types for span si.
(cid:99)rij is the ﬁnal predictedprobability distribution over possible relation typesfor span pair (cid:104)si, sj(cid:105)..(cid:3))(cid:1).
2.6 training.
the total loss is computed as:.
ltotal = (le.
1 + lr.
1) + 2(le.
2 + lr2).
(10).
1 and lr.
where le* denotes the cross-entropy loss of spanclassiﬁcation.
lr* denotes the binary cross-entropyloss of relation classiﬁcation.
le1 are lossterms for the initial span graph prediction (eq.
2and eq.
3 of section 2.3).
le2 are lossterms for the ﬁnal span graph prediction (eq.
9of section 2.5).
we apply a larger weight score tothe loss terms le2. we train the frameworkusing only ground-truth labels of the entity andrelation extraction tasks.
we do not make use ofany entity linking supervision in this work..2 and lr.
2 and lr.
3 experiments and results.
3.1 data and experiments setup.
datasets and evaluation metrics we evaluatekeci on two benchmark datasets: biorelex andade.
the biorelex dataset (khachatrian et al.,2019) consists of 2,010 sentences from biomedi-cal literature that capture binding interactions be-tween proteins and/or biomolecules.
biorelex hasannotations for 33 types of entities and 3 typesof relations for binding interactions.
the train-ing, development, and test splits contain 1,405,201, and 404 sentences, respectively.
the train-ing and development sets are publicly available.
the test set is unreleased and can only be evaluatedagainst using codalab 2. for biorelex, we reportmicro-f1 scores.
the ade dataset (gurulingappaet al., 2012) consists of 4,272 sentences extractedfrom medical reports that describe drug-related ad-verse effects.
two entity types (adverse-effect anddrug) and a single relation type (adverse-effect)are pre-deﬁned.
similar to previous work (eberts.
2 https://competitions.codalab.org/.
competitions/20468.
figure 3: an illustration of the attention mechanism..each span si ∈ vs and nj for each entity vj ∈ vk).
the next step is to soft-align the mentions andthe candidate entities using an attention mechanism(figure 3).
let c(si) denote the set of candidateentities for a span si ∈ vs. for example, in figure2, the mention fkbp12 has two candidate entities,while fk506 has only one candidate.
for eachcandidate entity vj ∈ c(si), we calculate a scalarscore αij indicating how relevant vj is to si:.
αij = ffnnc.
(cid:0)(cid:2)hi, nj.
(cid:3)(cid:1).
(6).
where ffnnc is a feedforward network mappingfrom r2×d → r. then we compute an additionalsentinel vector ci (yang and mitchell, 2017; heet al., 2020) and also compute a score αi for it:.
ci = ffnnsαi = ffnnc.
(cid:0)hi(cid:1)(cid:0)(cid:2)hi, ci.
(cid:3)(cid:1).
(7).
where ffnns is another feedforward network map-ping from rd → rd.
intuitively, ci records theinformation of the local context of si, and αi mea-sures the importance of such information.
afterthat, we compute a ﬁnal knowledge-aware repre-sentation fi for each span si as follows:.
z = exp (αi) +.
exp (αiz).
(cid:88).
vz∈c(si)βi = exp (αi)/z and βij = exp (αij)/z.
(8).
fi = βi ci +.
(cid:88).
βijnj.
vj ∈c(si).
the attention mechanism is illustrated in figure 3.with the extracted knowledge-aware span repre-sentations, we predict the ﬁnal span graph in a way.
6252model.
sciie (2018)dygiepp + elmo (2020)dygiepp + bioelmo (2020)sentcontextonlyflatattentionknowbertattentionfull model (keci).
entity(micro-f1)77.9081.1082.8083.9884.3285.6987.42.relation(micro-f1)49.6055.6054.8063.9064.2365.1366.09.table 1: overall results (%) on the development set ofbiorelex..model.
relation-metric (2019)spert (2020)spanmulti-head (2020)sentcontextonlyflatattentionknowbertattentionfull model (keci).
entity(macro-f1)87.1189.2890.5988.1389.1690.0890.67.relation(macro-f1)77.2978.8480.7377.2378.8179.9581.74.table 3: overall results (%) on the ade dataset..model.
sciie (2018)second best modelfull model (keci).
entity(micro-f1)73.5682.7687.35.relation(micro-f1)50.1562.1867.09.table 2: overall results (%) on the test set of biorelex(from the leaderboard as of january 20th, 2021)..ablation setting.
full model (keci)• w/o external knowledge• w/o collective inference• w/o the bidirectional gcn• w/o the relational gcn• w/o the pretrained umls vectors• w/o the umls deﬁnition vectors.
entity(micro-f1)87.4283.98*84.32*84.76*85.14*86.25†86.76†.
relation(micro-f1)66.0963.90*64.23*64.25*65.32*65.29*65.45†.
and ulges, 2020; ji et al., 2020), we conduct 10-fold cross-validation and report averaged macro-f1scores.
all the reported results take overlappingentities into consideration..table 4: results (%) of ablation experiments on the de-velopment set of biorelex.
we use the symbols * and† to indicate statistical signiﬁcance with 95% and 90%conﬁdence levels respectively (compared to keci)..implementation details we implement keciusing pytorch (paszke et al., 2019) and hugging-face’s transformers (wolf et al., 2020).
keciuses scibert as the transformer encoder (beltagyet al., 2019).
all details about hyperparameters andreproducibility information are in the appendix..baselines for comparison in addition to com-paring our method with state-of-the-art methods onthe above two datasets, we implement the follow-ing baselines for further comparison and analysis:1. sentcontextonly: this baseline does notuse any external knowledge.
it uses only thelocal sentence context for prediction.
it ex-tracts the ﬁnal output directly from the predic-tions obtained using eq.
2 and eq.
3..2. flatattention: this baseline does not rely oncollective inference.
it does not integrate anyglobal relational information into mention andentity representations.
each hi mentioned insec.
2.3 is set to be si (eq.
1), and each vimentioned in sec.
2.4 is set to be v0i .
then,the prediction of the ﬁnal span graph is thesame as described in sec.
2.5..3. knowbertattention: this baseline uses theknowledge attention and recontextualization(kar) mechanism of knowbert (peters et al.,2019), a state-of-the-art knowledge-enhanced.
language model.
the baseline ﬁrst uses scib-ert to construct initial token-level represen-tations.
it then uses the kar mechanism toinject external knowledge from umls intothe token-level vectors.
finally, it embeds textspans into feature vectors (eq.
1) and usesthe span representations to extract entities andrelations in one pass (similar to eq.
9)..for fair comparison, all the baselines use scibertas the transformer encoder..a major difference between keci and know-bertattention (peters et al., 2019) is that keciexplicitly builds and extracts information from amulti-relational graph structure of the candidate en-tity mentions before the knowledge fusion process.
in contrast, knowbertattention only uses scibertto extract features from the candidate entity men-tions.
therefore, knowbertattention only takesadvantage of the entity-entity co-occurrence infor-mation.
on the other hand, keci integrates moreﬁne-grained global relational information (e.g., thebinding interactions shown in figure 2) into themention representations.
this difference makeskeci achieve better overall performance, as to bediscussed next..62533.2 overall results.
table 1 and table 2 show the overall results onthe development and test sets of biorelex, re-spectively.
compared to sentcontextonly, keciachieves much higher performance.
this demon-strates the importance of incorporating externalknowledge for biomedical information extraction.
keci also outperforms the baseline flatattentionby a large margin, which shows the beneﬁt of col-lective inference.
in addition, we see that our modelperforms better than the baseline knowbertatten-tion.
finally, at the time of writing, keci achievesthe ﬁrst position on the biorelex leaderboard 3..table 3 shows the overall results on ade.
keciagain outperforms all the baselines and state-of-the-art models such as spert (eberts and ulges, 2020)and spanmulti-head (ji et al., 2020).
this furtherconﬁrms the effectiveness of our framework..overall, the two datasets used in this work focuson two very different subareas of the biomedicaldomain, and keci was able to push the state-of-the-art results of both datasets.
this indicates thatour proposed approach is highly generalizable..3.3 ablation study.
table 4 shows the results of ablation studies we didon the development set of the biorelex benchmark.
we compare our full model against several partialvariants.
the variant [w/o external knowledge] isthe same as the baseline sentcontextonly, and thevariant [w/o collective inference] is the same as thebaseline flatattention (section 3.1).
for the variant[w/o the bidirectional gcn], we simply set eachhi mentioned in section 2.3 to be si.
similarly, forthe variant [w/o the relational gcn], we set eachvi in section 2.4 to be v0i .
the last two variants arerelated to the initialization of each vector v0i ..we see that all the partial variants perform worsethan our full model.
this shows that each compo-nent of keci plays an important role..3.4 attention pattern analysis.
there is no gold-standard set of correspondencesbetween the entity mentions in the datasets andthe umls entities.
therefore, we cannot directlyevaluate the entity linking performance of keci.
however, for each umls semantic type, we com-pute the average attention weight that an entity ofthat type gets assigned (table 5).
overall, we see.
3 https://competitions.codalab.org/.
competitions/20468.
that keci typically pays the most attention to therelevant informative entities while ignoring the ir-relevant ones..3.5 qualitative analysis.
table 6 shows some examples from the adedataset that illustrate how incorporating externalknowledge can improve the performance of jointbiomedical entity and relation extraction..in the ﬁrst example, initially, there is no edgebetween the node “bleeding symptoms” and thenode “warfarin”, probably because of the distancebetween their corresponding spans in the originalinput sentence.
however, keci can link the term“warfarin” to a umls entity (cui: c0043031),and the deﬁnition in umls says that warfarin is atype of anticoagulant that prevents the formationof blood clots.
as the initial feature vector of eachentity contains the representation of its deﬁnition(sec.
2.4), keci can recover the missing edge..in the second example, the initial span graph ispredicted to have three entities of type adverse-effect, which correspond to three different overlap-ping text spans.
among these three, only “retroperi-toneal ﬁbrosis” can be linked to a umls entity.
itis also evident from the input sentence that oneof these spans is related to “methysergide”.
as aresult, keci successfully removes the other twounlinked span nodes to create the ﬁnal span graph.
in the third example, probably because of thephrase “due to”, the node “endometriosis” is ini-tially predicted to be of type drug, and the node“acute abdomen” is predicted to be its adverse-effect.
however, keci can link the term “en-dometriosis” to a umls entity of semantic typedisease or syndrome.
as a result, the system cancorrect the term’s type and also predict the rightedges for the ﬁnal span graph..finally, we also examined the errors made bykeci.
one major issue is that metamap sometimesfails to return any candidate entity from umls foran entity mention.
we leave the extension of thiswork to using multiple kbs as future work..4 related work.
traditional pipelined methods typically treat entityextraction and relation extraction as two separatetasks (zelenko et al., 2002; zhou et al., 2005; chanand roth, 2011).
such approaches ignore the closeinteraction between named entities and their rela-tion information and typically suffer from the error.
6254datasets.
top 3 types with the lowest avg.
attention scores.
top 3 types with the highest avg.
attention scores.
biorelex.
diagnostic procedure (0.04); activity (0.05); plant(0.05).
amino acid, peptide, or protein (0.32); enzyme(0.32); molecular function (0.36).
ade.
intellectual product (0.15); idea or concept (0.19);temporal concept (0.19).
antibiotic (0.78); organic chemical (0.79); nucleicacid, nucleoside, or nucleotide (0.87).
table 5: average attention scores of different umls semantic types..input sentence.
initial span graph.
final span graph.
#1: despite the low dosage of warfarin, interna-tional normalized ratio (inr) was markedly el-evated from 1.15 to 11.28 for only 4 days, andbleeding symptoms concurrently developed..#2: a 25-year-old woman sought medical attentionbecause of iliocaval manifestations of retroperi-toneal ﬁbrosis while she was taking methysergide..#3: title: acute abdomen due to endometriosisin a premenopausal woman taking tamoxifen..table 6: examples showing how external knowledge improves the quality of extracted span graphs.
edges repre-sent relations of type adverse-effect.
only relations with predicted probabilities of at least 0.5 are shown..propagation problem.
to overcome these limita-tions, many studies have proposed joint modelsthat perform entity extraction and relation extrac-tion simultaneously (roth and yih, 2007; li andji, 2014; li et al., 2017; zheng et al., 2017; bek-oulis et al., 2018a,b; wadden et al., 2019; fu et al.,2019; luan et al., 2019; zhao et al., 2020; wangand lu, 2020; li et al., 2020b; lin et al., 2020).
particularly, span-based joint extraction methodshave gained much popularity lately because of theirability to detect overlapping entities.
for example,eberts and ulges (2020) propose spert, a simplebut effective span-based model that utilizes bertas its core.
the recent work of ji et al.
(2020) alsoclosely follows the overall architecture of spertbut differs in span-speciﬁc and contextual semanticrepresentations.
despite their impressive perfor-mance, these methods are not designed speciﬁcallyfor the biomedical domain, and they do not utilizeany external knowledge base.
to the best of ourknowledge, our work is the ﬁrst span-based frame-.
work that utilizes external knowledge for joint en-tity and relation extraction from biomedical text..biomedical event extraction is a closely relatedtask that has also received a lot of attention fromthe research community (poon and vanderwende,2010; kim et al., 2013; v s s patchigolla et al.,2017; rao et al., 2017; espinosa et al., 2019; liet al., 2019; wang et al., 2020; huang et al., 2020;ramponi et al., 2020; yadav et al., 2020).
sev-eral studies have proposed to incorporate externalknowledge from domain-speciﬁc kbs into neuralmodels for biomedical event extraction.
for ex-ample, li et al.
(2019) incorporate entity informa-tion from gene ontology into tree-lstm mod-els.
however, their approach does not explicitlyuse any external relational information.
recently,huang et al.
(2020) introduce a framework thatuses a novel graph edge conditioned attentionnetwork (geanet) to utilize domain knowledgefrom umls.
in the framework, a global kg forthe entire corpus is ﬁrst constructed, and then a.
6255sentence-level kg is created for each individualsentence in the corpus.
our method of kg con-struction is more ﬂexible as we directly create akg for each input text.
furthermore, the work ofhuang et al.
(2020) only deals with event extractionand assumes that gold-standard entity mentions areprovided at inference time..some previous work has focused on integrat-ing external knowledge into neural architecturesfor other tasks, such as reading comprehension(mihaylov and frank, 2018), question answer-ing (pan et al., 2019), natural language inference(sharma et al., 2019), and conversational modeling(parthasarathi and pineau, 2018).
different fromthese studies, our work explicitly emphasizes thebeneﬁt of collective inference using global rela-tional information..many previous studies have also used gnns forvarious ie tasks (nguyen and grishman, 2018; liuet al., 2018; subburathinam et al., 2019; zeng et al.,2021; zhang and ji, 2021).
many of these meth-ods use a dependency parser or a semantic parserto construct a graph capturing global interactionsbetween tokens/spans.
however, parsers for spe-cialized biomedical domains are expensive to build.
keci does not rely on such expensive resources..5 conclusions and future work.
in this work, we propose a novel span-based frame-work named keci that utilizes external domainknowledge for joint entity and relation extractionfrom biomedical text.
experimental results showthat keci is highly effective, achieving new state-of-the-art results on two datasets: biorelex andade.
theoretically, keci can take an entire docu-ment as input; however, the tested datasets are onlysentence-level datasets.
in the future, we plan toevaluate our framework on more document-leveldatasets.
we also plan to explore a broader rangeof properties and information that can be extractedfrom external kbs to facilitate biomedical ie tasks.
finally, we also plan to apply keci to other infor-mation extraction tasks (li et al., 2020a; lai et al.,2021; wen et al., 2021)..acknowledgement.
we thank the three reviewers and the area chair fortheir insightful comments and suggestions.
thisresearch is based upon work supported by themolecule maker lab institute: an ai researchinstitutes program supported by nsf under award.
no.
2019897, nsf no.
2034562, u.s. darpakairos program no.
fa8750-19-2-1004, theofﬁce of the director of national intelligence(odni), intelligence advanced research projectsactivity (iarpa), via contract no.
fa8650-17-c-9116. any opinions, ﬁndings and conclusions orrecommendations expressed in this document arethose of the authors and should not be interpreted asrepresenting the ofﬁcial policies, either expressedor implied, of the u.s. government.
the u.s. gov-ernment is authorized to reproduce and distributereprints for government purposes notwithstandingany copyright notation here on..references.
d. alvermann, l. smith, and j. readence.
1985.prior knowledge activation and the comprehensionof compatible and incompatible text.
reading re-search quarterly, 20:420..a. aronson and f. lang.
2010. an overview ofmetamap:historical perspective and recent ad-vances.
journal of the american medical informat-ics association : jamia, 17 3:229–36..giannis bekoulis, j. deleu, thomas demeester, andchris develder.
2018a.
joint entity recognition andrelation extraction as a multi-head selection problem.
arxiv, abs/1804.07847..giannis bekoulis, johannes deleu, thomas demeester,and chris develder.
2018b.
adversarial trainingfor multi-context joint entity and relation extrac-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 2830–2836, brussels, belgium.
associationfor computational linguistics..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: pretrained language model for scientiﬁc text.
inemnlp..abhinav bhatt and kaustubh d. dhole.
2020. bench-marking biorelex for entity tagging and relation ex-traction.
arxiv, abs/2006.00533..o. bodenreider.
2004. the uniﬁed medical languagesystem (umls): integrating biomedical terminology.
nucleic acids research, 32 database issue:d267–70..jason braasch and s. goldman.
2010. the role ofprior knowledge in learning from analogies in sci-ence texts.
discourse processes, 47:447 – 479..yee seng chan and dan roth.
2011..exploitingsyntactico-semantic structures for relation extrac-tion.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 551–560, port-land, oregon, usa.
association for computationallinguistics..6256markus eberts and a. ulges.
2020. span-based jointentity and relation extraction with transformer pre-training.
in european conference on artiﬁcial intel-ligence..kurt junshean espinosa, makoto miwa, and sophiaananiadou.
2019. a search-based neural model forbiomedical nested and overlapping event detection.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3679–3686, hong kong, china.
association for computa-tional linguistics..hao fei, yue zhang, yafeng ren, and donghong ji.
2020. a span-graph neural model for overlappingentity relation extraction in biomedical texts.
bioin-formatics.
btaa993..tsu-jui fu, peng-hsuan li, and wei-yun ma.
2019.graphrel: modeling text as relational graphs forjoint entity and relation extraction.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1409–1418, flo-rence, italy.
association for computational linguis-tics..harsha gurulingappa, abdul mateen rajput, angusroberts, juliane fluck, martin hofmann-apitius,and luca toldo.
2012. development of a benchmarkcorpus to support the automatic extraction of drug-related adverse effects from medical case reports.
journal of biomedical informatics, 45(5):885–892..c. harris, k. j. millman, s. walt, ralf gommers,p. virtanen, d. cournapeau, e. wieser, j. taylor,s. berg, nathaniel j. smith, r. kern, matti pi-cus, s. hoyer, m. kerkwijk, matthew brett, allanhaldane, jaime fern’andez del r’io, mark wiebe,p. peterson, pierre g’erard-marchant, k. sheppard,t. reddy, w. weckesser, h. abbasi, christophgohlke, and t. e. oliphant.
2020. array program-ming with numpy.
nature, 585 7825:357–362..keqing he, yuanmeng yan, and weiran xu.
2020.learning to tag oov tokens by integrating contex-tual representation and background knowledge.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, pages 619–624, online.
association for computational linguis-tics..kung-hsiang huang, mu yang, and nanyun peng.
2020. biomedical event extraction with hierarchi-in findings of the associa-cal knowledge graphs.
tion for computational linguistics: emnlp 2020,pages 1277–1285, online.
association for compu-tational linguistics..bin ji, jie yu, shasha li, jun ma, qingbo wu, yusongtan, and huijun liu.
2020. span-based joint entityand relation extraction with attention-based span-speciﬁc and contextual semantic representations.
inproceedings of the 28th international conference on.
computational linguistics, pages 88–99, barcelona,spain (online).
international committee on compu-tational linguistics..hrant khachatrian, lilit nersisyan, karen ham-bardzumyan, tigran galstyan, anna hakobyan, ar-sen arakelyan, andrey rzhetsky, and aram gal-styan.
2019. biorelex 1.0: biological relation ex-in proceedings of the 18thtraction benchmark.
bionlp workshop and shared task, pages 176–190,florence, italy.
association for computational lin-guistics..jin-dong kim, yue wang, and yamamoto yasunori.
2013. the genia event extraction shared task, 2013in proceedings of the bionlpedition - overview.
shared task 2013 workshop, pages 8–15, soﬁa, bul-garia.
association for computational linguistics..tuan lai, heng ji, trung bui, quan hung tran, franckdernoncourt, and walter chang.
2021. a context-dependent gated module for incorporating symbolicsemantics into event coreference resolution.
in pro-ceedings of the 2021 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages3491–3499, online.
association for computationallinguistics..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2019.biobert: a pre-trained biomedical language representation modelbioinformatics,text mining.
for biomedical36(4):1234–1240..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017. end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing,pages 188–197, copenhagen, denmark.
associationfor computational linguistics..diya li, lifu huang, heng ji, and jiawei han.
2019.biomedical event extraction based on knowledge-driven tree-lstm.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1421–1430, minneapolis, minnesota.
association for computational linguistics..fei li, meishan zhang, g. fu, and d. ji.
2017. a neu-ral joint model for entity and relation extraction frombiomedical text.
bmc bioinformatics, 18..manling li, ying lin, tuan manh lai, xiaoman pan,haoyang wen, sha li, zhenhailong wang, pengfeiyu, lifu huang, di lu, qingyun wang, haoranzhang, qi zeng, chi han, zixuan zhang, yujia qin,xiaodan hu, nikolaus parulian, daniel campos,heng ji, brian chen, xudong lin, alireza zareian,amith ananthram, emily allaway, shih-fu chang,kathleen mckeown, yixiang yao, michael spec-tor, mitchell dehaven, daniel napierski, marjorie.
6257freedman, pedro szekely, haidong zhu, ram neva-tia, yang bai, yifan wang, ali sadeghian, haodima, and daisy zhe wang.
2020a.
gaia at sm-kbp 2020 - a dockerlized multi-media multi-lingualknowledge extraction, clustering, temporal trackingin proceedingsand hypothesis generation system.
of thirteenth text analysis conference (tac 2020)..diego marcheggiani and ivan titov.
2017. encodingsentences with graph convolutional networks for se-in proceedings of the 2017mantic role labeling.
conference on empirical methods in natural lan-guage processing, pages 1506–1515, copenhagen,denmark.
association for computational linguis-tics..qi li and heng ji.
2014. incremental joint extractionin proceedingsof entity mentions and relations.
of the 52nd annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 402–412, baltimore, maryland.
associationfor computational linguistics..zhijing li, yuchen lian, xiaoyong ma, xiangrongzhang, and chen li.
2020b.
bio-semantic relationextraction with attention-based external knowledgereinforcement.
bmc bioinformatics, 21..todor mihaylov and anette frank.
2018. knowledge-able reader: enhancing cloze-style reading compre-hension with external commonsense knowledge.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 821–832, melbourne, australia.
association for computational linguistics..t. nguyen and r. grishman.
2018. graph convo-lutional networks with argument-aware pooling forevent detection.
in aaai..ying lin, heng ji, fei huang, and lingfei wu.
2020.a joint neural model for information extraction withglobal features.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 7999–8009, online.
association forcomputational linguistics..xiao liu, zhunchen luo, and heyan huang.
2018.jointly multiple events extraction via attention-in proceed-based graph information aggregation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 1247–1256,brussels, belgium.
association for computationallinguistics..yi luan, luheng he, mari ostendorf, and hannanehhajishirzi.
2018. multi-task identiﬁcation of enti-ties, relations, and coreference for scientiﬁc knowl-edge graph construction.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3219–3232, brussels, bel-gium.
association for computational linguistics..xiaoman pan, kai sun, dian yu, jianshu chen, hengimprovingji, claire cardie, and dong yu.
2019.question answering with external knowledge.
inproc.
emnlp2019 workshop on machine readingfor question answering..prasanna parthasarathi and joelle pineau.
2018. ex-tending neural generative conversational model us-ing external knowledge sources.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 690–695, brus-sels, belgium.
association for computational lin-guistics..adam paszke, s. gross, francisco massa, a. lerer,j. bradbury, g. chanan, t. killeen, z. lin,n. gimelshein, l. antiga, alban desmaison, an-dreas köpf, e. yang, zach devito, martin raison,alykhan tejani, sasank chilamkurthy, b. steiner,lu fang, junjie bai, and soumith chintala.
2019.pytorch: an imperative style, high-performancedeep learning library.
in neurips..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-in proceedings of the 2019namic span graphs.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3036–3046, minneapolis, minnesota.
association for computational linguistics..matthew e. peters, mark neumann, robert logan, royschwartz, vidur joshi, sameer singh, and noah a.smith.
2019. knowledge enhanced contextual wordin proceedings of the 2019 con-representations.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 43–54, hong kong, china.
associ-ation for computational linguistics..ling luo, zhihao yang, m. cao, lei wang, y. zhang,and hongfei lin.
2020. a neural network-basedjoint learning approach for biomedical entity and re-lation extraction from biomedical literature.
journalof biomedical informatics, page 103384..r. maldonado, meliha yetisgen, and sanda m.harabagiu.
2019. adversarial learning of knowl-edge embeddings for the uniﬁed medical languagesystem.
amia joint summits on translational sci-ence proceedings.
amia joint summits on transla-tional science, 2019:543–552..hoifung poon and lucy vanderwende.
2010..jointinference for knowledge extraction from biomedi-in human language technologies:cal literature.
the 2010 annual conference of the north ameri-can chapter of the association for computationallinguistics, pages 813–821, los angeles, california.
association for computational linguistics..alan ramponi, rob van der goot, rosario lombardo,and barbara plank.
2020. biomedical event extrac-in proceedings of thetion as sequence labeling.
2020 conference on empirical methods in natural.
6258language processing (emnlp), pages 5357–5367,online.
association for computational linguistics..sudha rao, daniel marcu, kevin knight, and haldaumé iii.
2017. biomedical event extraction us-in bionlping abstract meaning representation.
2017, pages 126–135, vancouver, canada,.
associa-tion for computational linguistics..dan roth and wen-tau yih.
2007. global inferencefor entity and relation identiﬁcation via a linear pro-gramming formulation.
introduction to statistical re-lational learning, pages 553–580..m. schlichtkrull, thomas kipf, p. bloem, r. v. berg,ivan titov, and m. welling.
2018. modeling re-lational data with graph convolutional networks.
arxiv, abs/1703.06103..soumya sharma, bishal santra, abhik jana, santoshtokala, niloy ganguly, and pawan goyal.
2019. in-corporating domain knowledge into medical nliin proceedings of theusing knowledge graphs.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 6092–6097, hong kong,china.
association for computational linguistics..mohammad golam sohrab and makoto miwa.
2018.deep exhaustive model for nested named entityrecognition.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, pages 2843–2849, brussels, belgium.
associa-tion for computational linguistics..ananya subburathinam, di lu, heng ji, jonathanmay, shih-fu chang, avirup sil, and clare voss.
2019. cross-lingual structure transfer for relationand event extraction.
in proc.
2019 conference onempirical methods in natural language processingand 9th international joint conference on naturallanguage processing (emnlp-ijcnlp2019)..t. tran and ramakanth kavuluru.
2019. neural met-ric learning for fast end-to-end relation extraction.
arxiv, abs/1905.07458..rahul v s s patchigolla, sunil sahu, and ashishanand.
2017. biomedical event trigger identiﬁ-cation using bidirectional recurrent neural networkin bionlp 2017, pages 316–321,based models.
vancouver, canada,.
association for computationallinguistics..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5784–5789, hong kong, china.
association for computa-tional linguistics..jue wang and wei lu.
2020. two are better thanone: joint entity and relation extraction with table-sequence encoders.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1706–1721, online.
as-sociation for computational linguistics..xing david wang, leon weber, and ulf leser.
2020.biomedical event extraction as multi-turn questionanswering.
in proceedings of the 11th internationalworkshop on health text mining and informationanalysis, pages 88–96, online.
association for com-putational linguistics..haoyang wen, ying lin, tuan lai, xiaoman pan, shali, xudong lin, ben zhou, manling li, haoyuwang, hongming zhang, xiaodong yu, alexanderdong, zhenhailong wang, yi fung, piyush mishra,qing lyu, dídac surís, brian chen, susan windischbrown, martha palmer, chris callison-burch, carlvondrick, jiawei han, dan roth, shih-fu chang,and heng ji.
2021. resin: a dockerized schema-guided cross-document cross-lingual cross-media in-formation extraction and event tracking system.
inproceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies:demonstrations, pages 133–143, online.
associa-tion for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..shweta yadav, pralay ramteke, asif ekbal, sriparnasaha, and pushpak bhattacharyya.
2020. exploringdisorder-aware attention for clinical event extraction.
16(1s)..bishan yang and tom mitchell.
2017. leveragingknowledge bases in lstms for improving machinein proceedings of the 55th annual meet-reading.
ing of the association for computational linguistics(volume 1: long papers), pages 1436–1446, van-couver, canada.
association for computational lin-guistics..dmitry zelenko, chinatsu aone,.
and anthonyrichardella.
2002. kernel methods for relation ex-traction.
in proceedings of the 2002 conference onempirical methods in natural language processing(emnlp 2002), pages 71–78.
association for com-putational linguistics..qi zeng, manling li, tuan lai, heng ji, mohit bansal,and hanghang tong.
2021. gene: global event.
6259in proceedings of the fif-network embedding.
teenth workshop on graph-based methods for nat-ural language processing (textgraphs-15), pages42–53, mexico city, mexico.
association for com-putational linguistics..zixuan zhang and heng ji.
2021. abstract meaningrepresentation guided graph encoding and decodingfor joint information extraction.
in proceedings ofthe 2021 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 39–49, on-line.
association for computational linguistics..shan zhao, minghao hu, zhiping cai, and fang liu.
2020. modeling dense cross-modal interactions forin proceedings ofjoint entity-relation extraction.
the twenty-ninth international joint conference onartiﬁcial intelligence, ijcai-20, pages 4032–4038.
international joint conferences on artiﬁcial intelli-gence organization.
main track..suncong zheng, feng wang, hongyun bao, yuexinghao, peng zhou, and bo xu.
2017.joint extrac-tion of entities and relations based on a novel tag-in proceedings of the 55th annualging scheme.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1227–1236,vancouver, canada.
association for computationallinguistics..guodong zhou, jian su, jie zhang, and min zhang.
2005. exploring various knowledge in relation ex-in proceedings of the 43rd annual meet-traction.
ing of the association for computational linguis-tics (acl’05), pages 427–434, ann arbor, michi-gan.
association for computational linguistics..a reproducibility checklist.
in this section, we present the reproducibility infor-mation of the paper.
we are planning to make thecode publicly available after the paper is reviewed..implementation dependencies libraries py-torch 1.6.0 (paszke et al., 2019), transformers 4.0.0(wolf et al., 2020), dgl 0.5.34, numpy 1.19.1(harris et al., 2020), cuda 10.2..computing infrastructure the experimentswere conducted on a server with intel(r) xeon(r)gold 5120 cpu @ 2.20ghz and nvidia teslav100 gpus.
the allocated ram is 187g.
gpumemory is 16g..datasets the biorelex dataset (khachatrianet al., 2019) is available at https://github.com/yerevann/biorelex.
the ade dataset (gurulin-gappa et al., 2012) can be downloaded by using thescript at https://github.com/markus-eberts/spert..4https://www.dgl.ai/.
average runtime table 7 shows the estimatedaverage run time of our full model..number of model parameters the number ofparameters in a full model trained on biorelexis about 121.0m parameters.
the number of pa-rameters in a full model trained on ade is about119.9m parameters..hyperparameters of best-performing modelsthe span length limit l is set to be 20 tokens.
notethat the choice of l only has some noticeable ef-fects on the training time of keci during the ﬁrstepoch.
keci with randomly initialized parame-ters may include many non-relevant spans in theinitial span graph.
however, after a few trainingiterations, keci typically can ﬁlter out most non-relevant spans.
the pruning parameter λ is set tobe 0.5. all of our models use scibert as thetransformer encoder (beltagy et al., 2019).
weuse two different learning rates, one for the lowerpretrained transformer encoder and one for the up-per layers.
table 8 summarizes the hyperparameterconﬁgurations of best-performing models..expected validation performance the mainpaper has the results on the dev set of biorelex.
for ade, as in previous work, we conduct a 10-fold cross validation..hyperparameter tuning process we experi-mented with the following range of possible values:{16, 32} for batch size, {2e-5, 3e-5, 4e-5, 5e-5} forlower learning rate, {1e-4, 2e-4, 5e-4} for upperlearning rate, and {50, 100} for number of trainingepochs.
for each particular set of hyperparame-ters, we repeat training for 3 times and compute theaverage performance..dataset.
one training.
evaluation.
epoch.
(dev set).
biorelex.
337.51 seconds.
35.38 seconds.
ade.
712.89 seconds.
52.39 seconds.
table 7: estimated average runtime of our full model..hyperparameters.
biorelex ade.
lower learning rate.
upper learning rate.
batch size.
number epochs.
5e-05.
2e-04.
32.
50.
5e-05.
1e-04.
32.
50.table 8: hyperparameters for best-performing models..6260