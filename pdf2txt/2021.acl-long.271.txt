gtm: a generative triple-wise model forconversational question generation.
jinchao zhang3 yang feng1,2∗.
jie zhou3lei shen1,2 fandong meng31institute of computing technology, chinese academy of sciences, beijing, china2university of chinese academy of sciences, beijing, china3pattern recognition center, wechat ai, tencent inc, chinashenlei17z@ict.ac.cn, {fandongmeng,dayerzhang}@tencent.comfengyang@ict.ac.cn, withtomzhou@tencent.com.
abstract.
generating some appealing questions in open-domain conversations is an effective way toimprove human-machine interactions and leadthe topic to a broader or deeper direction.
to avoid dull or deviated questions, somethe “fu-researchers tried to utilize answer,ture” information, to guide question genera-tion.
however, they separate a post-question-answer (pqa) triple into two parts: post-question (pq) and question-answer (qa) pairs,which may hurt the overall coherence.
besides,the qa relationship is modeled as a one-to-onemapping that is not reasonable in open-domainconversations.
to tackle these problems, wepropose a generative triple-wise model withhierarchical variations for open-domain con-versational question generation (cqg).
latentvariables in three hierarchies are used to rep-resent the shared background of a triple andone-to-many semantic mappings in both pqand qa pairs.
experimental results on a large-scale cqg dataset show that our method sig-niﬁcantly improves the quality of questions interms of ﬂuency, coherence and diversity overcompetitive baselines..1.introduction.
questioning in open-domain dialogue systems isindispensable since a good system should have theability to well interact with users by not only re-sponding but also asking (li et al., 2017).
besides,raising questions is a proactive way to guide usersto go deeper and further into conversations (yuet al., 2016).
therefore, the ultimate goal of open-domain conversational question generation (cqg)is to enhance the interactiveness and maintain thecontinuity of a conversation (wang et al., 2018)..joint work with pattern recognition center, wechat ai,tencent inc, china.
∗yang feng is the corresponding author..post:i ate out with my friends this evening..question candidates:q1.1: which restaurant did you go?
q1.2: where did you eat?
q2.1: what food did you eat?
q2.2: did you eat something special?
q3: what do you mean?
q4: how about drinking together?.
answer candidates:a1: we went to an insta-famous cafeteria.
a2: we ate steak and pasta..table 1: an example of cqg task which is talkingabout a person’s eating activity.
there are one-to-manymappings in both pq and qa pairs.
the content ofeach meaningful and relevant question (q1.1 to q2.2)is decided by its post and answer.
q3 (dull) and q4(deviated) are generated given only the post..cqg differs fundamentally from traditional ques-tion generation (tqg) (zhou et al., 2019; kimet al., 2019; li et al., 2019) that generates a ques-tion given a sentence/paragraph/passage and a spec-iﬁed answer within it.
while in cqg, an answeralways follows the to-be-generated question, andis unavailable during inference (wang et al., 2019).
at the same time, each utterance in open-domainscenario is casual and can be followed by severalappropriate sentences, i.e., one-to-many mapping(gao et al., 2019; chen et al., 2019)..at ﬁrst, the input information of cqg wasmainly a given post (wang et al., 2018; hu et al.,2018), and the generated questions were usuallydull or deviated (q3 and q4 in table 1).
based onthe observation that an answer has strong relevanceto its question and post, wang et al.
(2019) tried tointegrate answer into the question generation pro-cess.
they applied a reinforcement learning frame-work that ﬁrstly generated a question given thepost, and then used a pre-trained matching modelto estimate the relevance score (reward) between.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3495–3506august1–6,2021.©2021associationforcomputationallinguistics3495answer and generated question.
this method sep-arates a post-question-answer (pqa) triple intopost-question (pq) and question-answer (qa) pairsrather than considering the triple as a whole andmodeling the overall coherence.
furthermore, thetraining process of the matching model only utilizesone-to-one relation of each qa pair and neglectsthe one-to-many mapping feature..an open-domain pqa often takes place under abackground that can be inferred from all utterancesin the triple and help enhance the overall coher-ence.
when it comes to the semantic relationshipin each triple, the content of a speciﬁc questionis under the control of its post and answer (leeet al., 2020).
meanwhile, either a post or an answercould correspond to several meaningful questions.
as shown in table 1, the triple is about a person’seating activity (the background of the entire conver-sation).
there are one-to-many mappings in bothpq and qa pairs that construct different meaning-ful combinations, such as p-q1.1-a1, p-q1.2-a1,p-q2.1-a2 and p-q2.2-a2.
an answer connectstightly to both its post and question, and in turnhelps decide the expression of a question..on these grounds, we propose a generative triple-wise model (gtm) for cqg.
speciﬁcally, weﬁrstly introduce a triple-level variable to capturethe shared background among pqa.
then, twoseparate variables conditioned on the triple-levelvariable are used to represent the latent space forquestion and answer, and the question variable isalso dependent on the answer one.
during training,the latent variables are constrained to reconstructboth the original question and answer according tothe hierarchical structure we deﬁne, making surethe triple-wise relationship ﬂows through the la-tent variables without any loss.
for the questiongeneration process, we sample the triple-level andanswer variable given a post, then obtain the ques-tion variable conditioned on them, and ﬁnally gen-erate a question based on the post, triple-level andquestion variables.
experimental results on a large-scale cqg dataset show that gtm can generatemore ﬂuent, coherent, and intriguing questions foropen-domain conversations..the main contribution is threefold:.
• to generate coherent and informative ques-tions in the cqg task, we propose a genera-tive triple-wise model that models the seman-tic relationship of a triple in three levels: pqa,pq, and qa..figure 1: the graphical representation of gtm fortraining process.
zt is used to capture the shared back-ground among pqa, while zq and za are used to modelthe diversity in pq and qa pairs.
solid arrows illus-trate the generation of q, a (not used in inference), andqt, while dashed arrows are for posterior distributionsof latent variables..• our variational hierarchical structure can notonly utilize the “future” information (answer),but also capture one-to-many mappings in pqand qa, which matches the open-domain sce-nario well..• experimental results on a large-scale cqgcorpus show that our method signiﬁcantly out-performs the state-of-the-art baselines in bothautomatic and human evaluations..2 proposed model.
given a post as the input, the goal of cqg is togenerate the corresponding question.
followingthe work of zhao et al.
(2017) and wang et al.
(2019), we leverage the question type qt to con-trol the generated question, and take advantage ofthe answer information a to improve coherence.
in training set, each conversation is representedas {p, q, qt, a}, consisting of post p = {pi}|p|i=1,question q = {qi}|q|i=1 with its question type qt,and answer a = {ai}|a|i=1..2.1 overview.
the graphical model of gtm for training processis shown in figure 1. θ, ϕ, and φ are used to denoteparameters of generation, prior, and recognitionnetwork, respectively.
we integrate answer genera-tion to assist question generation with hierarchicallatent variables.
firstly, a triple-level variable ztis imported to capture the shared background and.
3496figure 2: the architecture of gtm.
⊕ denotes the concatenation operation.
in training process, latent variablesobtained from recognition networks and the real question type qt are used for decoding.
red dashed arrows refer toinference process, in which we get latent variables from prior networks, and the predicted question type qt(cid:48) is fedinto the question decoder.
the answer decoder is only utilized during training to assist the triple-wise modeling..is inferred from pqa utterances.
then answer la-tent variable za and question latent variable zq aresampled from gaussian distributions conditionedon both post and zt.
to ensure that the question iscontrolled by answer, zq is also dependent on za..2.2.input representation.
we use a bidirectional gru (cho et al., 2014) asencoder to capture the semantic representation ofeach utterance.
take post p as an example.
eachword in p is ﬁrstly encoded into its embedding vec-tor.
the gru then computes forward hidden states−→h i}|p|{.
←−h i}|p|i=1:.
i=1 and backward hidden states {−−→gru(epi,←−−gru(epi,.
−→h i−1),←−h i+1),.
−→h i =←−h i =.
where epi is employed to represent the embeddingvector of word pi.
we ﬁnally get the post represen-tation by concatenating the last hidden states of two←−directions hench 1].
similarly, we canobtain representations of question q and answer a,denoted as henc.
a , respectively..−→h |p|;.
and henc.
p = [.
q.the question type qt is represented by a real-valued, low dimensional vector vqt which is up-dated during training and is regarded as a linguisticfeature that beneﬁts the training of latent variables(zhao et al., 2017).
we use the actual questiontype qt during training to provide the informationof interrogative words that is the most importantfeature to distinguish question types..2.3 triple-level latent variable.
to capture the shared background of entire triple,we introduce a triple-level latent variable zt that.
is inferred from pqa utterances and is in turn re-sponsible for generating the whole triple.
inspiredby park et al.
(2018), we use a standard gaussiandistribution as the prior distribution of zt:.
pϕ(zt) = n (z|0, i),.
where i represents the identity matrix..for the inference of zt in training set, we con-sider three utterance representations hencandhencas a sequence, and use a bidirectional gruato take individual representation as the input ofeach time step.
the triple representation ht is ob-tained by concatenating the last hidden states ofboth directions.
then, zt is sampled from:.
, hencq.p.qφ(zt|p, q, a) = n (z|µt, σti),φ(ht),µt = mlptσt = softplus(mlpt.
φ(ht)),.
where mlp(·) is a feed-forward network, and soft-plus function is a smooth approximation to reluand can be used to ensure positiveness (park et al.,2018; serban et al., 2017)..2.4 one-to-many mappingsafter obtaining zt, we use a gru f to get a vectorhctxis then trans-pformed to hctxthat are used in prior andrecognition networks for zq and za:.
for connecting p and q/a.
hctxand hctx.
a.p.q.p = f (zt, henchctx),pθ (hctxq = mlptr1hctxθ (hctxa = mlptr2hctx.
p ),p )..3497prior network(a)recognitionnetwork (a)recognitionnetwork (q)prior network (q)mlp𝑡𝑟1mlp𝑡𝑟2𝐳𝑡𝐳𝑞𝐳𝑞𝐳𝑎𝐳𝑎𝐳𝑡𝜇′𝑞𝜎′𝑞𝜇𝑞𝜎𝑞𝜇𝑎𝜎′𝑎𝜇′𝑎𝜎𝑎𝑞𝑡𝐡𝑒𝑛𝑐𝑞𝐡𝑒𝑛𝑐𝑝𝐡𝑒𝑛𝑐𝑎𝐡𝑐𝑡𝑥𝑝𝐡𝑐𝑡𝑥𝑞𝑞𝑡𝑞𝑡′𝐡𝑑𝑒𝑐𝑎,0𝐡𝑑𝑒𝑐𝑞,0encoder & prior/recognition networkanswer decoderquestion decoderklkl𝐡𝑐𝑡𝑥𝑎questionpostanswermlp𝑞𝑡𝑞𝑡′question type predictionbi-gru encoderbi-gru encoderbi-gru encodergru decodergru decoderi ate out with my friends this evening.what did you eat?we ate steak and pasta.what did you eat?we ate steak and pasta.
[what]𝐳𝑎to model one-to-many mappings in pq andqa pairs under the control of zt, we design twoutterance-level variables, zq and za, to representlatent spaces of question and answer.
we deﬁne theprior and posterior distributions of za as follows:.
pϕ(za|p, zt) = n (z|µa, σai),qφ(za|p, zt, a) = n (z|µa, σ.
(cid:48).
(cid:48).
ai),.
where µa, σa, µ(cid:48)a, and σ(cid:48)gaussian distributions, are calculated as:.
a, the parameters of two.
ϕ([hctx.
µa = mlpaσa = softplus(mlpa.
a ; zt]),.
ϕ([hctx.
a ; zt])),.
(cid:48)µ.φ([hctx.
a = mlpaa = softplus(mlpaσ.a ; zt; hencφ([hctx.
a ]),a ; zt; henc.
(cid:48).
a ]))..to make sure the content of question is alsodecided by answer and improve their relatedness,we import za into zq space.
the prior and posteriordistributions of zq are computed as follows:.
pϕ(zq|p, zt, za) = n (z|µq, σqi),qφ(zq|p, zt, q, qt, za) = n (z|µ.
(cid:48).
(cid:48).
q, σ.qi),.
where µq, σq, µ(cid:48).
q, and σ(cid:48).
q are calculated as:.
ϕ([hctxq.µq = mlpqσq = softplus(mlpq(cid:48)q = mlpqµq = softplus(mlpqσ.; zt; za]),ϕ([hctxq; zt; hencφ([hctx.
φ([hctx.
q.q.q.
(cid:48).
; zt; za])),; vqt; za]),; zt; henc.
q.; vqt, za]))..2.5 question generation network.
following the work of zhao et al.
(2017) andwang et al.
(2019), a question type predictionnetwork mlpqtis introduced to approximatepθ(qt|zq, zt, p) in training process and producesquestion type qt(cid:48) during inference..as shown in figure 2, there are two decodersin our model, one is for answer generation thatis an auxiliary task and only exists in the train-ing process, and the other is for desired ques-tion generation.
the question decoder employsa variant of gru that takes the concatenation re-sult of zq, zt, hctx, and qt as initial state s0, i.e.,qs0 = [zq; zt, hctx, qt].
for each time step j, it cal-culates the context vector cj following bahdanau.
q.et al.
(2015), and computes the probability dis-tribution pθ(q|zq, zt, p, qt) over all words in thevocabulary:.
sj = gru(ej−1, sj−1, cj)˜sj = mlp([ej−1; cj; sj]),pθ(qj|q<j, zq, zt, p, qt) = softmax(wo˜sj),.
where ej−1 represents the embedding vector of the(j − 1)-th question word.
similarly, the answerdecoder receives the concatenation result of za, zt,and hctxas initial state to approximate the proba-bility pθ(a|za, zt, p)..a.
2.6 training and inference.
importantly, our model gtm is trained to max-imize the log-likelihood of the joint probabilityp(p, q, a, qt):.
logp(p, q, a, qt) = log.
p(p, q, a, qt, zt)..(cid:90).
zt.
however, the optimization function is not di-rectly tractable.
inspired by serban et al.
(2017)and park et al.
(2018), we convert it to the followingobjective that is based on the evidence lower boundand needs to be maximized in training process:.
lgtm =− kl(qφ(zt|p, q, a)||pϕ(zt))− kl(qφ(za|p, zt, a)||pϕ(za|p, zt))− kl(qφ(zq|p, zt, q, qt, za)||pϕ(zq|p, zt, za))+ eza,zt∼qφ[log pθ(a|za, zt, p)]+ ezq,zt∼qφ[log pθ(q|zq, zt, p, qt)]+ ezq,zt∼qφ[log pθ(qt|zq, zt, p)]..the objective consists of two parts: the varia-tional lower bound (the ﬁrst ﬁve lines) and questiontype prediction accuracy (the last line).
meanwhile,the variational lower bound includes the reconstruc-tion terms and kl divergence terms based on threehierarchical latent variables.
the gradients to theprior and recognition networks can be estimatedusing the reparameterization trick (kingma andwelling, 2014)..during inference, latent variables obtained viaprior networks and predicted question type qt(cid:48) arefed to the question decoder, which correspondsto red dashed arrows in figure 2. the inferenceprocess is as follows:.
3498(1) sample triple-level lv: zt ∼ qφ(zt|p)1.
(2) sample answer lv: za ∼ pϕ(za|p, zt).
(3) sample question lv: zq ∼ pϕ(zq|p, zt, za).
(4) predict question type: qt ∼ pθ(qt|zq, zt, p).
(5) generate question: q ∼ pθ(zq, zt, p, qt)..3 experiments.
in this section, we conduct experiments to eval-uate our proposed method.
we ﬁrst introducesome empirical settings, including dataset, hyper-parameters, baselines, and evaluation measures.
then we illustrate our results under both automaticand human evaluations.
finally, we give out somecases generated by different models and do furtheranalyses over our method..3.1 dataset.
we apply our model on a large-scale cqg cor-pus2 extracted from reddit3 by wang et al.
(2019).
there are over 1.2 million pqa triples whichhave been divided into training/validation/test setwith the number of 1,164,345/30,000/30,000.
thedataset has been tokenized into words using thenltk tokenizer (bird et al., 2009).
the aver-age number of words in post/question/answer is18.84/19.03/19.30, respectively.
following fanet al.
(2018) and wang et al.
(2019), we categorizequestions in training and validation set into 9 typesbased on interrogative words, i.e., “what”, “when”,“where”, “who”, “why”, “how”, “can (could)”, “do(did, does)”, “is (am, are, was, were)”.
3.2 hyper-parameter settings.
we keep the top 40,000 frequent words as the vo-cabulary and the sentence padding length is set to30. the dimension of gru layer, word embeddingand latent variables is 300, 300, and 100. the priornetworks and mlps have one hidden layer withsize 300 and tanh non-linearity, while the numberof hidden layers in recognition networks for bothtriple-level and utterance-level variables is 2. weapply dropout ratio of 0.2 during training.
themini-batch size is 64. for optimization, we useadam (kingma and ba, 2015) with a learning rateof 1e-4.
in order to alleviate degeneration problemof variational framework (park et al., 2018), we.
apply kl annealing, word drop (bowman et al.,2016) and bag-of-word (bow) loss (zhao et al.,2017)4. the kl multiplier λ gradually increasesfrom 0 to 1, and the word drop probability is 0.25.we use pytorch to implement our model, and themodel is trained on titan xp gpus..3.3 baselines.
we compare our methods with four groups of repre-sentative models: (1) s2s-attn: a simple seq2seqmodel with attention mechanism (shang et al.,2015).
(2) cvae&kgcvae: the cvae modelintegrates an extra bow loss to generate diversequestions.
the kgcvae is a knowledge-guidedcvae that utilizes some linguistic cues (questiontypes in our experiments) to learn meaningful latentvariables (zhao et al., 2017).
(3) std&htd: thestd uses soft typed decoder that estimates a typedistribution over word types, and the htd useshard typed decoder that speciﬁes the type of eachword explicitly with gumbel-softmax (wang et al.,2018).
(4) rl-cvae: a reinforcement learningmethod that regards the coherence score (computedby a one-to-one matching network) of a pair of gen-erated question and answer as the reward function(wang et al., 2019).
rl-cvae is the ﬁrst work toutilize the future information, i.e., answer, and isalso the state-of-the-art model for cqg5..additionally, we also conduct ablation study tobetter analyze our method as follows: (5) gtm-zt: gtm without the triple-level latent variable,which means zt is not included in the prior andposterior distributions of both zp and za.
(6) gtm-a: the variant of gtm that does not take answerinto account.
that is, answer decoder and za areremoved from the loss function and the prior andposterior distributions of zq.
besides, zt here doesnot capture the semantics from answer.
(7) gtm-zq/za: gtm variant in which distributions of zqare not conditioned on za, i.e., the fact that thecontent of question is also controlled by answer isnot modelled explicitly by latent variables..in our model, we use an mlp to predict ques-tion types during inference, which is different fromthe conditional training (ct) methods (li et al.,2016b; zhou et al., 2018; shen and feng, 2020).
1inspired by park et al.
(2018), using zt inferred from postwith the posterior distribution is better than sampling it fromthe prior one, i.e., a standard gaussian distribution.
2https://drive.google.com/drive/.
folder/1wng30yphimc_znye3bh5wa1uvtr8l1pg.
3http://www.reddit.com.
4the total bow loss is calculated as the sum of all bowlosses between each latent variable and q/a.
please refer topark et al.
(2018) for more details..5for those methods with open-source codes, we run theoriginal codes; otherwise, we re-implement them based on thecorresponding paper..3499model.
s2s-attncvaekgcvaestdhtdrl-cvaegtm-ztgtm-agtm-zq/zagtm.
embedding metrics.
diversity.
average0.6340.6460.6470.6370.6480.6620.6720.6530.6870.697.extrema greedy0.4130.4210.4250.4180.4230.4370.4480.4280.4490.454.
0.3220.3370.3320.3260.3300.3430.3510.3380.3600.365.dist-10.01320.01600.01530.01440.01540.01610.01650.01580.01700.0176.dist-20.08300.15990.15870.13250.15820.17850.18720.16790.19340.2028.bleu scoresbleu-1 bleu-20.02980.09360.03060.14220.03100.14910.03020.13270.03140.14750.03200.15030.03320.15210.03170.14820.03290.15280.15370.0331.ruber scoresrubg ruba0.6220.5840.6870.6490.6820.6500.6630.6330.6890.6530.7010.6600.7100.6610.6920.6570.7130.6690.7200.671.table 2: automatic evaluation results for different models based on four types of metrics..that provide the controllable feature, i.e., questiontypes, in advance for inference.
therefore, we donot consider ct-based models as comparable ones..3.4 evaluation measures.
to better evaluate our results, we use both quantita-tive metrics and human judgements in our experi-ments..automatic metrics.
for automatic evaluation, we mainly choose fourkinds of metrics: (1) bleu scores: bleu (pa-pineni et al., 2002) calculates the n-gram overlapscore of generated questions against ground-truthquestions.
we use bleu-1 and bleu-2 here andnormalize them to 0 to 1 scale.
(2) embeddingmetrics: average, greedy and extrema metricsare embedding-based and measure the semanticsimilarity between the words in generated ques-tions and ground-truth questions (serban et al.,2017; liu et al., 2016).
we use word2vec embed-dings trained on the google news corpus6 in thispart.
please refer to serban et al.
(2017) for moredetails.
(3) dist-1& dist-2: following the workof li et al.
(2016a), we apply distinct to report thedegree of diversity.
dist-1/2 is deﬁned as the ratioof unique uni/bi-grams over all uni/bi-grams in gen-erated questions.
(4) ruber scores: referencedmetric and unreferenced metric blended evalua-tion routine (tao et al., 2018) has shown a highcorrelation with human annotation in open-domainconversation evaluation.
there are two versions,one is rubg based on geometric averaging and theother is ruba based on arithmetic averaging..embedding metrics and bleu scores are usedto measure the similarity between generated andground-truth questions.
rubg/a reﬂects the se-.
6https://code.google.com/archive/p/.
word2vec/.
mantic coherence of pq pairs (wang et al., 2019),while dist-1/2 evaluates the diversity of questions..human evaluation settingsinspired by wang et al.
(2019), shen et al.
(2019),and wang et al.
(2018), we use following threecriteria for human evaluation: (1) fluency mea-sures whether the generated question is reasonablein logic and grammatically correct.
(2) coherencedenotes whether the generated question is seman-tically consistent with the given post.
incoherentquestions include dull cases.
(3) willingness mea-sures whether a user is willing to answer the ques-tion.
this criterion is to justify how likely thegenerated questions can elicit further interactions.
we randomly sample 500 examples from test set,and generate questions using models mentionedabove.
then, we send each post and corresponding10 generated responses to three human annotatorswithout order, and require them to evaluate whethereach question satisﬁes criteria deﬁned above.
allannotators are postgraduate students and not in-volved in other parts of our experiments..3.5 experimental results.
now we demonstrate our experimental results onboth automatic evaluation and human evaluation..automatic evaluation resultsnow we demonstrate our experimental results onboth automatic evaluation and human evaluation.
the automatic results are shown in table 2. thetop part is the results of all baseline models, andwe can see that gtm outperforms other methodson all metrics (signiﬁcance tests (koehn, 2004),p-value < 0.05), which indicates that our proposedmodel can improve the overall quality of gener-ated questions.
speciﬁcally, dist-2 and ruba havebeen improved by 2.43% and 1.90%, respectively,compared to the state-of-the-art rl-cvae model..3500first, higher embedding metrics and bleu scoresshow that questions generated by our model aresimilar to ground truths in both topics and contents.
second, taking answer into account and using itto decide the expression of question can improvethe consistency of pq pairs evaluated by ruberscores.
third, higher distinct values illustrate thatone-to-many mappings in pq and qa pairs makethe generated responses more diverse..the bottom part of table 2 shows the results ofour ablation study, which demonstrates that tak-ing advantage of answer information, modelingthe shared background in entire triple, and consid-ering one-to-many mappings in both pq and qapairs can help enhance the performance of our hi-erarchical variational model in terms of relevance,coherence and diversity..human evaluation resultsas shown in table 3, gtm can alleviate the prob-lem of generating dull and deviated questions com-pared with other models (signiﬁcance tests (koehn,2004), p-value < 0.05).
both our proposed modeland the state-of-the-art model rl-cvae utilize theanswer information and the results of them couldprove that answers assist the question generationprocess.
besides, gtm can produce more relevantand intriguing questions, which indicates the effec-tiveness of modeling the shared background andone-to-many mappings in cqg task.
the inter-annotator agreement is calculated with the fleiss’kappa (fleiss and cohen, 1973).
fleiss’ kappafor fluency, coherence and willingness is 0.493,0.446 and 0.512, respectively, indicating “moder-ate agreement” for all three criteria..and answers.
(2) matching score: we use the gru-matchpyramid (wang et al., 2019) model that addsthe matchpyramid network (pang et al., 2016) ontop of a bidirectional gru to calculate the semanticcoherence.
as shown in table 4, questions gener-ated by gtm are more coherent to answers.
at-tributing to the design of triple-level latent variablethat captures the shared background, one-to-many.
models2s-attncvaekgcvaestdhtdrl-cvaegtm-ztgtm-agtm-zq/zagtm.
fluency coherence willingness0.4820.4620.4740.4880.5260.5340.5380.5320.5420.548.
0.2160.4840.5360.3560.5040.5780.5800.5700.5860.608.
0.1860.4280.4760.2860.4140.5080.5160.5120.5200.526.table 3: results for human evaluation..cosine similarity matching score.
models2s-attncvaekgcvaestdhtdrl-cvaegtm-ztgtm-agtm-zq/zagtm.
0.4980.5640.5780.5420.5830.6070.6130.6050.6180.629.
5.3068.0478.0546.8798.0598.4238.4278.4248.4728.517.table 4: evaluation results for qa coherence..mappings in pq and qa pairs, and relationshipmodeling for zq and za, gtm can improve therelevance in qa pairs..3.6 question-answer coherence evaluation.
3.7 case study.
automatic metrics in section “automatic metrics”are designed to compare generated questions withground-truth ones (ruber also takes the post in-formation into consideration), but ignore answersin the evaluation process.
to measure the seman-tic coherence between generated questions and an-swers, we apply two methods (wang et al., 2019):(1) cosine similarity: we use the pre-trained in-fersent model7 (conneau et al., 2017) to obtainsentence embeddings and calculate cosine similar-ity between the embeddings of generated responses.
7the infersent model is trained to predict the meaning ofsentences based on natural language inference, and the cosinesimilarity computed with it is more consistent with human’sjudgements, which performs better than the pre-trained trans-former/bert model in our experiments..in table 5, we list the generated results of twoposts from the test set to compare the performanceof different models..in the ﬁrst case, both the post and answer men-tion two topics, “donation” and “song”, so the ques-tion is better to consider their relations.
besides,the answer here begins with “because”, then “why”and “what (reason)” questions are reasonable.
forthe second case, the post only talks about “pen”,while the answer refers to “ink”, which means thereis a topic transition the question needs to cover.
thesecond case shows the effectiveness of an answerthat not only decides the expression of questionbut also improves the entire coherence of a tripe.
questions generated by gtm are more relevant to.
3501post.
and it’s a donation to foodbanks, even if you dislikethe song..it’s a very blue pen.
i’mmore into brighter colours,but this is still pretty nice..questions2s-attn: what does that mean?
cvae: what’s the best way to do that?
kgcvae: is there anything else to buy?
std: is it a good donation?
htd: where are those food banks?
rl-cvae: why do we need to do the stupid thing?
gtm: why not just donate money to food banks?
s2s-attn: what colors do you have?
cvae: are you colorblind?
kgcvae: it has nothing to complain.
where’s the pen?
std: what color are you using?
htd: what’s the colour scheme?
rl-cvae: what kind of ink should i buy?
gtm: i’ll take it though.
do you also sell the ink?.
answer.
because i like the aspect ofsong criticising mayreaching to the charts aswell as the fact it goes tocharity..i have some coming,hopefully it will be hereearly next week.
right nowit’s got green ink and it’sreally messing me up..table 5: two cases comparison among gtm and other baselines..both posts and answers, and could attract peopleto give an answer to them.
however, other base-lines may generate dull or deviated responses, eventhe rl-cvae model that considers the answer in-formation would only contain the topic words inanswers (e.g., the question in case two), but fail toensure the pqa coherence..eration problem does not exist in our model andlatent variables can play their corresponding roles..4 related work.
the researches on open-domain dialogue systemshave developed rapidly (majumder et al., 2020;zhan et al., 2021; shen et al., 2021), and our workmainly touches two ﬁelds: open-domain conver-sational question generation (cqg), and contextmodeling in dialogue systems.
we introduce thesetwo ﬁelds as follows and point out the main differ-ences between our method and previous ones..4.1 cqg.
traditional question generation (tqg) has beenwidely studied and can be seen in reading compre-hension (zhou et al., 2019; kim et al., 2019), sen-tence transformation (vanderwende, 2008), ques-tion answering (li et al., 2019; nema et al., 2019),visual question generation (fan et al., 2018) andtask-oriented dialogues (li et al., 2017).
in suchtasks, ﬁnding information via a generated questionis the major goal and the answer is usually partof the input.
different from tqg, cqg aims toenhance the interactiveness and persistence of con-versations (wang et al., 2018).
meanwhile, theanswer is the “future” information which meansit is unavailable in the inference process.
wanget al.
(2018) ﬁrst studied on cqg, and they usedsoft and hard typed decoders to capture the distri-bution of different word types in a question.
huet al.
(2018) added a target aspect in the input andproposed an extended seq2seq model to generateaspect-speciﬁc questions.
wang et al.
(2019) de-vised two methods based on either reinforcementlearning or generative adversarial network (gan).
figure 3: total kl divergence (per word) of all latentvariables in gtm and gtm-a model (ﬁrst 30 epochsof validation set)..3.8 further analysis of gtm.
variational models suffer from the notorious degen-eration problem, where the decoders ignore latentvariables and reduce to vanilla seq2seq models(zhao et al., 2017; park et al., 2018; wang et al.,2019).
generally, kl divergence measures theamount of information encoded in a latent variable.
in the extreme case where the kl divergence of la-tent variable z equals to zero, the model completelyignores z, i.e., it degenerates.
figure 3 shows thatthe total kl divergence of gtm model maintainsaround 2 after 18 epochs indicating that the degen-.
3502to further enhance semantic coherence betweenposts and questions under the guidance of answers..4.2 context modeling in dialogue systems.
existing methods mainly focus on the historicalcontext in multi-turn conversations, and hierarchi-cal models occupy a vital position in this ﬁeld.
ser-ban et al.
(2016) proposed the hierarchical recur-rent encoder-decoder (hred) model with a con-text rnn to integrate historical information fromutterance rnns.
to capture utterance-level vari-ations, serban et al.
(2017) raised a new modelvariational hred (vhred) that augments hredwith cvaes.
after that, vhcr (park et al., 2018)added a conversation-level latent variable on top ofthe vhred, while csrr (shen et al., 2019) usedthree-hierarchy latent variables to model the com-plex dependency among utterances.
in order to de-tect relative utterances in context, tian et al.
(2017)and zhang et al.
(2018) applied cosine similar-ity and attention mechanism, respectively.
hran(xing et al., 2018) combined the attention resultson both word-level and utterance-level.
besides,the future information has also been consideredfor context modeling.
shen et al.
(2018) separatedthe context into history and future parts, and as-sumed that each of them conditioned on a latentvariable is under a gaussian distribution.
feng et al.
(2020) used future utterances in the discriminatorof a gan, which is similar to wang et al.
(2019).
the differences between our method and afore-mentioned ones in section 4.1 and 4.2 are: (1)rather than dividing pqa triples into two parts,i.e., pq (history and current utterances) and qa(current and future utterances) pairs, we model theentire coherence by utilizing a latent variable tocapture the share background in a triple.
(2) insteadof regarding the relationship between question andanswer as a text matching task that lacks the consid-eration of diversity, we incorporate utterance-levellatent variables to help model one-to-many map-pings in both pq and qa pairs..5 conclusion.
we propose a generative triple-wise model for gen-erating appropriate questions in open-domain con-versations, named gtm.
gtm models the entirebackground in a triple and one-to-many mappingsin pq and qa pairs simultaneously with latentvariables in three hierarchies.
it is trained in a one-stage end-to-end framework without pre-training.
like the previous state-of-the-art model that alsotakes answer into consideration.
experimental re-sults on a large-scale cqg dataset show that gtmcan generate ﬂuent, coherent, informative as wellas intriguing questions..acknowledgements.
we would like to thank all the reviewers for theirinsightful and valuable comments and suggestions..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015..steven bird, ewan klein, and edward loper.
2009.natural language processing with python: analyz-ing text with the natural language toolkit.
” o’reillymedia, inc.”..samuel r bowman, luke vilnis, oriol vinyals, an-drew dai, rafal jozefowicz, and samy bengio.
2016. generating sentences from a continuousspace.
in proceedings of the 20th signll confer-ence on computational natural language learning,pages 10–21..chaotao chen, jinhua peng, fan wang, jun xu, andhua wu.
2019. generating multiple diverse re-sponses with multi-mapping and posterior mappingselection.
in proceedings of the 28th internationaljoint conference on artiﬁcial intelligence, pages4918–4924.
aaai press..kyunghyun cho, bart van merri¨enboer caglar gul-cehre, dzmitry bahdanau, fethi bougares holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in natu-ral language processing, pages 1724–1734..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in proceedings ofthe 2017 conference on empirical methods in nat-ural language processing, pages 670–680, copen-hagen, denmark.
association for computationallinguistics..zhihao fan, zhongyu wei, piji li, yanyan lan, andxuanjing huang.
2018. a question type drivenframework to diversify visual question generation.
in proceedings of the 27th international joint con-ference on artiﬁcial intelligence, pages 4048–4054.
aaai press..3503shaoxiong feng, hongshen chen, kan li, and daweiyin.
2020. posterior-gan: towards informative andcoherent response generation with posterior genera-tive adversarial network.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 34,pages 7708–7715..joseph l fleiss and jacob cohen.
1973. the equiv-alence of weighted kappa and the intraclass corre-lation coefﬁcient as measures of reliability.
educa-tional and psychological measurement, 33(3):613–619..xiang gao, sungjin lee, yizhe zhang, chris brockett,michel galley, jianfeng gao, and bill dolan.
2019.jointly optimizing diversity and relevance in neuralin proceedings of the 2019response generation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1229–1238, minneapolis, minnesota.
association for computational linguistics..wenpeng hu, bing liu, jinwen ma, dongyan zhao,and rui yan.
2018. aspect-based question genera-tion..yanghoon kim, hwanhee lee, joongbo shin, and ky-omin jung.
2019.improving neural question gen-eration using answer separation.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 33, pages 6602–6609..diederick p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in the 3rd inter-national conference on learning representations..diederik p kingma and max welling.
2014. auto-encoding variational bayes.
in the 2nd internationalconference on learning representations..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..dong bok lee, seanie lee, woo tae jeong, dongh-wan kim, and sung ju hwang.
2020. gener-ating diverse and consistent qa pairs from con-texts with information-maximizing hierarchical con-in proceedings of the 58th annualditional vaes.
meeting of the association for computational lin-guistics, pages 208–224, online.
association forcomputational linguistics..jingjing li, yifan gao, lidong bing, irwin king, andimproving question gener-michael r. lyu.
2019.in proceedings ofation with to the point context.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3216–3226, hong kong,china.
association for computational linguistics..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016a.
a diversity-promoting ob-jective function for neural conversation models.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110–119, san diego, california.
associationfor computational linguistics..jiwei li, michel galley, chris brockett, georgios sp-ithourakis, jianfeng gao, and bill dolan.
2016b.
ain pro-persona-based neural conversation model.
ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 994–1003, berlin, germany.
associ-ation for computational linguistics..jiwei li, alexander h miller, sumit chopra,marc’aurelio ranzato, and jason weston.
2017.learning through dialogue interactions by askingquestions.
iclr..chia-wei liu, ryan lowe, iulian serban, mike nose-worthy, laurent charlin, and joelle pineau.
2016.how not to evaluate your dialogue system: anempirical study of unsupervised evaluation metricsfor dialogue response generation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2122–2132, austin,texas.
association for computational linguistics..bodhisattwa prasad majumder, harsh jhamtani, tay-lor berg-kirkpatrick, and julian mcauley.
2020.like hiking?
you probably enjoy nature: persona-grounded dialog with commonsense expansions.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9194–9206, online.
association for computa-tional linguistics..preksha nema, akash kumar mohankumar, mitesh m.khapra, balaji vasan srinivasan, and balaramanravindran.
2019. let’s ask again: reﬁne networkin proceedingsfor automatic question generation.
of the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 3314–3323, hongkong, china.
association for computational lin-guistics..liang pang, yanyan lan, jiafeng guo, jun xu, shengx-ian wan, and xueqi cheng.
2016. text matching asimage recognition.
in thirtieth aaai conference onartiﬁcial intelligence..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..3504yookoon park, jaemin cho, and gunhee kim.
2018.a hierarchical latent structure for variational conver-in proceedings of the 2018 con-sation modeling.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages1792–1801, new orleans, louisiana.
associationfor computational linguistics..zhiliang tian, rui yan, lili mou, yiping song, yan-song feng, and dongyan zhao.
2017. how to makecontext more useful?
an empirical study on context-in proceed-aware neural conversational models.
ings of the 55th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 231–236, vancouver, canada.
associa-tion for computational linguistics..iulian v serban, alessandro sordoni, yoshua bengio,aaron courville, and joelle pineau.
2016. buildingend-to-end dialogue systems using generative hierar-chical neural network models.
in proceedings of thethirtieth aaai conference on artiﬁcial intelligence,pages 3776–3783..iulian vlad serban, alessandro sordoni, ryan lowe,laurent charlin, joelle pineau, aaron courville, andyoshua bengio.
2017. a hierarchical latent variableencoder-decoder model for generating dialogues.
inproceedings of the thirty-first aaai conference onartiﬁcial intelligence, pages 3295–3301..lifeng shang, zhengdong lu, and hang li.
2015. neu-ral responding machine for short-text conversation.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages1577–1586, beijing, china.
association for compu-tational linguistics..lei shen and yang feng.
2020. cdl: curriculumdual learning for emotion-controllable response gen-in proceedings of the 58th annual meet-eration.
ing of the association for computational linguis-tics, pages 556–566, online.
association for com-putational linguistics..lei shen, yang feng, and haolan zhan.
2019. model-ing semantic relationship in multi-turn conversationswith hierarchical latent variables.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 5497–5502, florence,italy.
association for computational linguistics..lei shen, haolan zhan, xin shen, and yang feng.
2021. learning to select context in a hierarchi-cal and global perspective for open-domain dialoguein icassp 2021-2021 ieee interna-generation.
tional conference on acoustics, speech and signalprocessing (icassp), pages 7438–7442.
ieee..xiaoyu shen, hui su, wenjie li, and dietrich klakow.
2018. nexus network: connecting the precedingin pro-and the following in dialogue generation.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4316–4327, brussels, belgium.
association for computa-tional linguistics..lucy vanderwende.
2008. the importance of being im-portant: question generation.
in proceedings of the1st workshop on the question generation sharedtask evaluation challenge, arlington, va..weichao wang, shi feng, daling wang, and yifeizhang.
2019. answer-guided and semantic coherentquestion generation in open-domain conversation.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5066–5076, hong kong, china.
association for computa-tional linguistics..yansen wang, chenyi liu, minlie huang, and liqianglearning to ask questions in open-nie.
2018.domain conversational systems with typed decoders.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2193–2203, melbourne, aus-tralia.
association for computational linguistics..chen xing, yu wu, wei wu, yalou huang, and mingzhou.
2018. hierarchical recurrent attention net-work for response generation.
in proceedings of thethirty-second aaai conference on artiﬁcial intelli-gence, pages 5610–5617..zhou yu, ziyu xu, alan w black, and alexander rud-nicky.
2016. strategy and policy learning for non-in proceed-task-oriented conversational systems.
ings of the 17th annual meeting of the special in-terest group on discourse and dialogue, pages 404–412..haolan zhan, hainan zhang, hongshen chen, zhuoyeding, yongjun bao, and yanyan lan.
2021. aug-menting knowledge-grounded conversations with se-quential knowledge transition.
in proceedings of the2021 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 5621–5630, on-line.
association for computational linguistics..weinan zhang, yiming cui, yifa wang, qingfu zhu,lingzhi li, lianqiang zhou, and ting liu.
2018.context-sensitive generation of open-domain con-in proceedings of the 27thversational responses.
international conference on computational linguis-tics, pages 2437–2447..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in thirty-second aaai conference on artiﬁcial in-telligence..tiancheng zhao, ran zhao, and maxine eskenazi.
2017. learning discourse-level diversity for neuraldialog models using conditional variational autoen-in proceedings of the 55th annual meet-coders.
ing of the association for computational linguistics.
3505(volume 1: long papers), pages 654–664, vancou-ver, canada.
association for computational linguis-tics..hao zhou, minlie huang, tianyang zhang, xiaoyanzhu, and bing liu.
2018. emotional chatting ma-chine: emotional conversation generation with in-in proceedings ofternal and external memory.
the aaai conference on artiﬁcial intelligence, vol-ume 32..wenjie zhou, minghua zhang, and yunfang wu.
2019.in pro-question-type driven question generation.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 6032–6037,hong kong, china.
association for computationallinguistics..3506