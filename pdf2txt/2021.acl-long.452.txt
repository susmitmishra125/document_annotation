an in-depth study on internal structure of chinese words.
chen gong1, saihao huang1∗, houquan zhou1, zhenghua li1, min zhang1,zhefeng wang2, baoxing huai2, nicholas jing yuan21institute of artiﬁcial intelligence, school of computer science and technology,.
soochow university, china;.
2 huawei cloud, china.
1{cgong,shhuang1999,hqzhou}@stu.suda.edu.cn1{zhli13,minzhang}@suda.edu.cn2{wangzhefeng, huaibaoxing, nicholas.yuan}@huawei.com.
abstract.
unlike english letters, chinese charactershave rich and speciﬁc meanings.
usually, themeaning of a word can be derived from its con-stituent characters in some way.
several previ-ous works on syntactic parsing propose to an-notate shallow word-internal structures for bet-ter utilizing character-level information.
thiswork proposes to model the deep internal struc-tures of chinese words as dependency treeswith 11 labels for distinguishing syntactic re-lationships.
first, based on newly compiledannotation guidelines, we manually annotate aword-internal structure treebank (wist) con-sisting of over 30k multi-char words fromchinese penn treebank.
to guarantee qual-ity, each word is independently annotated bytwo annotators and inconsistencies are han-dled by a third senior annotator.
second, wepresent detailed and interesting analysis onwist to reveal insights on chinese word for-mation.
third, we propose word-internal struc-ture parsing as a new task, and conduct bench-mark experiments using a competitive depen-dency parser.
finally, we present two simpleways to encode word-internal structures, lead-ing to promising gains on the sentence-levelsyntactic parsing task..coordinate.
right.
coordinate.
root.
left.
left.
coordinate.
coordinate.
方想planthink.
法设design method.
姻婚marriage marriage.
法law.
老法pharaoh.
(a) zhang et al.
(2014): labels mark head positions..root.
v-v-v.v-v-n.v-v-n.n-n-n.n-n-n.n-n-n.root.
root.
法设方想design methodthink.
plan.
法姻婚lawmarriage marriage.
老法pharaoh.
(b) li et al.
(2018): labels correspond to pos tag triples..root.
cooobj.
obj.
root.
root.
frag.
attcoo.
法设方想design methodthink.
plan.
法姻婚lawmarriage marriage.
老法pharaoh.
(c) ours: ﬁne-grained structure with 11 labels..figure 1: three example words with internal struc-ture under different annotation paradigms.
“想(thinkof) 方(plan) 设(design) 法(method)” is a verb and“婚(marriage)means “ﬁnd ways or means to do”.
姻(marriage) 法(law)” is a noun.
“法老” is phonetictransliteration of “pharaoh”.
the three words all con-tain the character “法” under different meanings..1.introduction.
unlike english, chinese adopts a logographic writ-ing system and contains tens of thousands of dis-tinct characters.
many characters, especially fre-quently used ones, have rich and speciﬁc meanings.
however, words, instead of characters, are oftenconsidered as the basic unit in processing chinesetexts.
we believe the reason may be two-fold.
first,usually a character may have many meanings andusages.
word formation process greatly reducessuch char-level ambiguity.
second, by deﬁnition,.
∗chen gong and saihao huang make equal contributions.
to this work.
zhenghua is the corresponding author..words are the minimal units that express a com-plete semantic concept or play a grammatical roleindependently (xia, 2009; yu et al., 2003).1.roles played by characters in word formationcan be divided into three types.
(1) there is a stableand important set of single-char words, such as“你” (you)”, “的” (of), and most punctuation marks.
(2) a character having no speciﬁc meaning actsas a part of a single-morpheme word, such as “仿.
1there is still a dispute on the word granularity issue (gonget al., 2017; lai et al., 2021).
words are deﬁned as a charactersequence that is in tight and steady combination.
however, thecombination intensity is usually yet vaguely qualiﬁed accord-ing to co-occurrence frequency.
we believe this work mayalso be potentially useful to this direction..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5823–5833august1–6,2021.©2021associationforcomputationallinguistics5823佛” (like) and “法(fˇa)老(lˇao)” (pharaoh, translitera-tion of foreign words).
(3) a character correspondsto a morpheme, the smallest meaningful unit ina language, and composes a polysyllabic wordwith other characters.
this work targets multi-charwords, and is particularly interested in the thirdtype which most characters belong to..intuitively, modeling how multiple charactersform a word, i.e., the word-formation process, al-lows us to more effectively represent the meaningof a word via composing the meanings of charac-ters.
this is especially helpful for handling rarewords, considering that the vocabulary size of char-acters is much smaller than that of words.
in fact,many nlp researchers have tried to utilize char-level word-internal structures for better chineseunderstanding.
most related to ours, previous stud-ies on syntactic parsing have proposed to annotateword-internal structures to alleviate the data sparse-ness problem (zhang et al., 2014; li et al., 2018).
however, their annotations mainly consider ﬂat andshallow word-internal structure, as shown in figure1-(a) and (b).
meanwhile, researchers try to makeuse of character information to learn better wordembeddings (chen et al., 2015; xu et al., 2016).
without explicitly capturing word-internal struc-tures, these studies have to treat a word as a bag ofcharacters.
see section 2 for more discussion..this paper presents an in-depth study on char-level internal structure of chinese words.
we en-deavour to address three questions.
(1) what arethe word-formation patterns for chinese words?
(2)can we train a model to predict deep word-internalstructures?
(3) is modeling word-internal structuresbeneﬁcial for word representation learning?.
for the ﬁrst question, we propose to use labeleddependency trees to represent word-internal struc-tures, and employ 11 labels to distinguish syntacticroles in word formation.
we compile annotationguidelines following the famous textbook of zhu(1982) on chinese syntax, and annotate a high-quality word-internal structure treebank (wist),consisting of 30k words from penn chinese tree-bank (ctb) (xia, 2009).
we conduct detailed anal-ysis on wist to gain insights on chinese word-formation patterns..for the second question, we propose word-internal structure parsing as a new task, and presentbenchmark experimental results using a competi-tive open-source dependency parser..for the third question, we investigate two sim-.
ple ways to encode word-internal structure, i.e.,labelcharlstm and labelgcn, and show thatusing the resulting word representation leads topromising gains on the dependency parsing task..we release wist at https://github.com/suda-la/acl2021-wist, and also provide a demoto parse the internal structure of any input word..2 related work.
annotating word-internal structure.
in thedeep learning (dl) era, pretraining techniques areextremely powerful in handling large-scale unla-beled data, including skip-gram or cbow mod-els (mikolov et al., 2013) for learning context-independent word embedding in the beginning, andthe recent elmo (peters et al., 2018) or bert(devlin et al., 2019) for learning context-awareword representations.
conversely, in the pre-dlera, there exist few (if any) effective methods forutilizing unlabeled data, and statistical models relyon discrete one-hot features, leading to severe datasparseness for many nlp tasks.
this directly mo-tivates annotation of word-internal structure, espe-cially for dealing with rare words..annotation of shallow internal structure of chi-nese words was ﬁrst mentioned in zhao (2009),largely based on heuristic rules.
li (2011); li andzhou (2012) found that many multi-char wordscould be divided into two subwords, i.e., root andafﬁx.
they annotated structures of about 19kwords (35% of 54,214) in ctb6.
their experi-ments showed that subword-level syntactic parsingis superior to word-level parsing.
for the threewords in figure 1, their approach is only applicableto the second word, i.e., “婚姻/法”.
as an exten-sion to li and zhou (2012), zhang et al.
(2013,2014) proposed char-level syntactic parsing by fur-ther dividing subwords into chars.
as shown infigure 1-(a), for each word, they annotated a binaryhierarchical tree, using constituent labels to markwhich child constituent is more syntactically impor-tant, i.e., left, right, or coordinate.
in such way, theycould convert a word-level constituent/dependencytree into a char-level one.
similar to li and zhou(2012), cheng et al.
(2014) annotated internal struc-ture of synthesis (multi-morpheme) words withfour relations, i.e., branching, coordinate, begin-ning and other parts of a single-morpheme word..in the dl era, three works have studied word-internal structure.
similarly to our work, li et al.
(2018) employed dependency trees to encode word-.
5824label.
root.
subj.
objattadvcmpcoo.
pobj.
adjct.
frag.
repet.
obj−−→ 场 (stage).
subject.
meaning.
word root.
annotation.
example登场 (come on stage) $ root−−→ 登 (come)年轻 (young)下雨 (rain)object大衣 (overcoat)attribute modiﬁeradverbial modiﬁer 不同 (different)complement modiﬁer 放下 (put down)上下文 (context)coordinationpreposition object 到期 (expire)走过 (pass by)adjunct沙发 (sofa)常常 (often).
subj年 (age)←−− 轻 (small)obj−−→ 雨 (rain)下 (drop)大 (large) att←− 衣 (coat)不 (not) adv←−− 同 (same)cmp放 (put)−−→下 (down)上 (above) coo−−→ 下(below)pobj到 (reach)−−→期 (deadline)adjct走 (walk)−−−→过 (by)frag−−→发 (send)沙 (sand)repet−−−→常 (often)常 (often).
repetition.
fragment.
table 1: the 11 labels adopted in our guidelines for distinguishing syntactic roles in word formation..internal structure.
as shown in figure 1-(b), foreach multi-char word, they ﬁrst annotate the part-of-speech (pos) tag of each character, and then de-termine an unlabeled dependency tree, and ﬁnallyuse a pos tag triple as arc label, correspondingto the pos tags of the modiﬁer/head charactersand the whole word.
however, we argue pos tagtriples are only loosely related with word-formationpatterns, not to mention the severe difﬁculty of an-notating char-level pos tags in each word..recently, lin et al.
(2020) extended zhang et al.
(2014) by using an extra label for marking single-morpheme words, and annotated hierarchical inter-nal structure of 53k words from a chinese-englishmachine translation (mt) dataset.
li et al.
(2019a)annotated the internal structure of words with 4dependency relations..in summary, we can see that most previous stud-ies adopted quite shallow hierarchical structure.
incontrast, this work presents a more in-depth inves-tigation on internal structure of chinese words andemploys 11 labels to distinguish different syntacticroles in word formation, as shown in figure 1-(c)..leveraging character information for betterword representation.
it has already become astandard way in many nlp tasks to obtain char-aware word representation by applying lstm orcnn to the character sequence of a word, and con-catenate it with word embedding as input, such asnamed entity recognition (chiu and nichols, 2016),dependency parsing (zhang et al., 2020), and con-stituent parsing (gaddy et al., 2018)..another research direction is to leverage charac-.
ter information to obtain better word embeddings.
chen et al.
(2015) extended the cbow model andproposed to jointly learn character and word embed-dings.
based on chen et al.
(2015), yu et al.
(2017)proposed to jointly learn embeddings of words,characters, and sub-characters.2 however, bothstudies assume that characters contribute equallyto the meaning of a word and directly average em-beddings of all characters.
to address this, xuet al.
(2016) extended chen et al.
(2015) and pro-posed a cross-lingual approach to distinguish con-tribution of characters for a word.
the idea is totranslate chinese words and characters into englishwords, and use similarities between correspondingenglish word embeddings for contribution mea-surement.
instead of treating a word as a bag ofcharacters, we experiment with two simple ways toobtain structure-aware word representations.
mean-while, enhancing their approach with explicit word-internal structure could be also very interesting..utilizing word-internalstructure.
word-internal structure have been explored in variousnlp tasks.
several works propose to learn word-internal structure, word segmentation, pos taggingand parsing jointly (zhang et al., 2013, 2014; liet al., 2018), demonstrating the effectiveness ofword-internal structure in helping downstreamtasks.
cheng et al.
(2015) attempt to convertwords into ﬁne-grained subwords according to the.
2following this direction, studies tried to explore morecharacter information for better chinese word representation,such as strokes (cao et al., 2018) and ideographic shape (sunet al., 2019)..5825internal structure of words for better dealing withunknown words during word segmentation.
linet al.
(2020) propose to integrate the representationof word-internal structure into the input of neuralmachine translation model, leading to improvedtranslation performance..3 word-internal structure annotation.
in this section, we describe in detail the anno-tation process of wist.
as shown in figure 1-(c), we adopt dependency trees for representingword-internal structure.
the reason is two-fold.
first, word-formation process correlates with syn-tax in different ways depending on language type(aikhenvald, 2007).
such correlation is especiallyclose for chinese due to its lack of morphologi-cal inﬂections.
in particular, zhu (1982) presentedthorough investigation on chinese word formationmainly from a syntactic view.
second, as a gram-mar formalism, dependency tree structure has beenwidely adopted for capturing sentence-level syntaxdue to its simplicity and ﬂexibility in representingrelations.
meanwhile, its computational modelingis also developed quite well..annotation guidelines.
after several months’survey, we have compiled systematic and detailedguidelines for word-internal structure annotation.
our guidelines are mainly based on the famous text-book on chinese grammar of zhu (1982).
we inten-sively studied all previous works on word-internalstructure annotation, which are discussed in sec-tion 2. we also ﬁnd that it is quite beneﬁcial tobe familiar with guidelines developed by previousannotation projects for chinese word segmentation(xia, 2009; yu et al., 2003)..our guidelines contain 11 relations speciﬁcallydesigned to capture the internal dependency syntaxfor chinese words, as shown in table 1. we derivemost of the dependency relations by referring toguidelines of three popular chinese dependencytreebanks, i.e., ud, harbin institute technologychinese dependency treebank (hit-cdt) (liuet al., 2006), and chinese open dependency tree-bank (codt) (li et al., 2019b).
we give verydetailed illustrations with examples in our 30-pageguidelines to ensure annotation consistency andquality.
our guidelines are also gradually improvedaccording to the feedback from the annotators..quality control.
we employ 18 undergraduatestudents as part-time annotators who are familiar.
total #word type37,449word token 508,764 48.0 44.1.
3 ≥45.6 58.3 22.8 13.31.9.
6.0.
1.
2.table 2: word distr.
regarding char number in ctb5..with chinese syntax, and select 6 capable anno-tators with a lot of data annotation experience asexpert annotators to handle inconsistent submis-sions.
all the annotators (including expert anno-tators) were paid for their work .
the salary isdetermined by both quantity and quality.
besides,we give extra bonus to the annotators with highaccuracy.
the average salary of the annotators is30 rmb per hour.
all annotators are trained forseveral hours to be familiar with our guidelines andthe usage of annotation tool..we apply strict double annotation in order toguarantee quality.
each word is randomly assignedto two annotators.
two identical submissions aredirectly used as the ﬁnal answer.
otherwise, a thirdexpert annotator is asked to decide the ﬁnal answerafter analyzing the two inconsistent annotations..annotation tool.
we build a browser-based an-notation tool to support the annotation workﬂowand facilitate project management..given an annotation task, all its pos tags 3 ofthe focused word in ctb5 are presented to the an-notator, in order to explore multiple internal struc-tures for one word.
in that case, the annotator canclick a checkbox to inform us for further process.
please note that the manually annotated pos tagsin ctb5 are converted into universal dependen-cies (ud) 4 pos tags based on predeﬁned mappingrules, since the original ctb5 pos tags are tooﬁne-grained (33 tags) and difﬁcult for annotators tounderstand.
the interface also presents several ex-ample sentences to improve annotation efﬁciency.
we strongly encourage annotators to look up difﬁ-cult words or characters in electronic dictionaries.5.
3in ctb5, a word may be annotated with different postags under different contexts.
for example, “发展 (develop-ment)” is annotated as nn (noun) in the context “促进经济发展 (boost the economic development )”, whereas “发展(develop)” is annotated as (vv) verb in the context “稳定地发展 (develop steadily)”.
therefore, when annotating theword “发展 (develop/development )”, we present both “noun”and “verb” to the annotators for reference.”.
4universaldependencies.org/u/pos/5eg., hanyu.baidu.com; xh.5156edu.com/.
5826data selection.
following previous works, weselect multi-char words from ctb5 for annotation.
table 2 shows word distribution regarding characternumbers.
we can see that only 5.6% of words inthe vocabulary contain one char, but they accountfor nearly half (48%) token occurrences in the text.
the percent of words with two characters is high inboth vocabulary (58.3) and text (44.1).
we discardwords containing special symbols such as englishletters.
finally, we have annotated 32,954 multi-char words with their internal structure, containing83,999 dependencies (2.5 characters per word)..4 analysis on annotated wist.
in this section, we analyze the annotated wistfrom different aspects in order to gain more insightson chinese word-formation patterns..inter-annotator consistency.
as discussed ear-lier, each word is labeled by two annotators, andinconsistent submissions are handled by a third se-nior annotator for obtaining a ﬁnal answer.
theaveraged inter-annotator consistency ratio is 83.0dependency-wise, i.e., the percent of charactersreceiving the same head and label from two an-notators, and 75.8 word-wise, i.e., the percent ofwords receiving the same whole trees.
if we donot consider labels, the unlabeled consistency ra-tios increase to 87.5 dependency-wise and 85.1word-wise.
although it may be a factor that mostannotators are inexperienced in this new annotationtask, such low consistency ratios indicate that anno-tating word-internal structure is quite challenging,especially when it comes to distinguishing syntac-tic roles.
meanwhile, this also demonstrates theimportance of strict double annotation, consideringthat nearly a quarter of words are inconsistent andrequire handling by senior annotators..annotation accuracy.
we calculate annotationaccuracy by comparing all submissions (as denom-inator) from annotators against the ﬁnal answers inwist.
please note that each word is double anno-tated.
the overall dependency-wise accuracy forall annotators is 90.9, and word-wise is 86.9. ifnot considering labels, the overall unlabeled accu-racy increases to 93.4 and 92.1, dependency- andword-wise respectively..the ﬁrst major row in table 3 shows the label-wise annotation accuracy.
we divide charactersin wist into 11 groups according to their ﬁnal-answer labels, and then calculate the percent of.
correct submissions for each group.
the highestaccuracy is obtained on “repet”, since its pattern isquite regular.
determining the root character alsoseems relatively easy.
the lowest accuracy is 62.0on “subj” and 48.2 on “pobj”..comparing unlabeled versus labeled accuracy,the gap is quite large.
the extreme case is “pobj”.
annotators usually can correctly decide the head(84.5%), but very unlikely choose its true label“pobj” (48.2%).
similarly, accuracy drops by 24.9for “subj”.
we give more discussions on annotationdifﬁculties below..label distribution.
the third major row in ta-ble 3 shows distribution of different labels in wist.
from the percentage of “root” (39.2%), we caninfer that one word contains 2.5 characters on av-erage.
the overall percent for “att” is 29.1, almosthalf of the remaining labels, meaning that “att” ap-pears once every 1.45 words.
this reveals that at-tribute modiﬁcation is the most dominated patternin word formation.
coordination structure (“coo”)takes the second place with 10.2%.
the third mostused pattern is fragment (“frag”) with 5.7%.
wegive more discussion on “frag” below..besides the overall distribution, the third majorrow in table 3 gives label distribution per pos tag.
for clarity, we give the full name of each pos tag(ud, converted from the ﬁne-grained ctb tags) intable 3, and it means the pos tag of the focusedword.
if a word has multiple pos tags, then thesame word-internal structure is used for each tag.
coo−−→ 展 (ex-for example, if a word “发 (expand)pand)” has two tags, i.e., noun and verb, then thenumber of “coo” is added by one for both nounand verb.
moreover, a label is repeatedly countedif it appears several times in the same word.
due tospace limitation, we only present high-frequencypos tags, with percentage shown in parenthesis.
please note that we adopt a coarse-grained pos tagset for clarity..we can see that nouns are mostly formed with“att” (33.8%) and “coo” (11.5%), whereas verbsare with “coo/obj/adv/cmp” in the descending or-der.
proper nouns are evenly dominated by “frag”(29.6%) and “att” (28.4%).
it is also obvious thatproper nouns tend to be longer, consisting of 2.7characters according to its “root” percentage.
nu-merals are mainly composed via “att” (75.7%) andconsist of 5.0 character on average..5827adv cmp adjct.
att.
obj.
coo.
annotation accuracy.
unlabeledparsing accuracyunlabeledoverall distributionnoun (47.2%)verb (24.1%)proper noun (13.1%) 36.6 28.4adjective (7.1%)adverb (3.9%)numeral (3.7%)others (0.9%).
fragroot93.9 93.1 88.6 89.3 82.6 80.6 85.393.8 94.2 92.3 93.3 92.7 88.1 97.989.0 89.5 75.8 80.6 77.4 68.0 84.089.0 90.6 85.4 84.1 88.2 80.7 93.539.2 29.1 10.24.32.35.45.742.3 33.8 11.52.60.42.54.40.4 12.73.8 17.99.67.942.20.60.10.80.67.58.26.4 12.11.800.20.14.87.71.4.
44.4 16.5 17.745.5 12.1 10.320.0 75.70.48.747.6 15.2.
2.3 29.60.70.60.12.1.subj83.5 62.092.2 86.976.8 64.280.5 80.71.51.13.10.61.91.000.3.
1.51.11.20.90.75.33.68.2.repet pobj96.0 48.299.4 84.581.1 58.197.3 83.90.20.60.10.20.40.900.30.21.62.32.800.10.13.9.table 3: label-wise accuracy and distribution.
the ﬁrst major row presents annotation accuracy of wist and“unlabeled” means not considering labels.
the second major row gives parsing accuracy on wist-test, discussedin section 5. the third major row gives distribution of different labels for words of different pos tags..for one word?
manymultiple structureswords have multiple meanings.
then the questionis: how many words really have multiple internalstructures?
as illustrated in section 3, we showall pos tags to annotators in order to obtain allinternal structures of an ambiguous word.
how-ever, in annotated wist, we ﬁnd there are only103 such words with multiple internal structures,accounting for about 0.3% of all annotated words,and 2.7% of those having multiple pos tags.
as atypical example, “制服” have two structures.
as acmpverb, it means “subdue” and has “制(control)−−→服(tamely)”.
as a noun, it means “uniform” andatt←−− 服(cloth)”.
this low per-has “制(regulated)centage reveals that most chinese words actuallyhave very steady internal structure.
they have mul-tiple pos tags, mainly because they are used fordifferent syntactic functions without morphologi-cal inﬂections, such as “发展” as verb (“develop”)or noun (“development”)..more on “frag”.
the “frag” label is designedto handle all words that have no internal structuredue to the lack of semantic composition.
fromtable 3, we can see that “frag” accounts for 5.7%of all labels.
in order to gain more insights, wecollect all 3,528 words containing “frag” in wist,and randomly sample 100 words for investigation.
following the brief discussion in section 1, wedivide these words into three types, and ﬁnd that81 words are proper nouns (such as person name);16 correspond to transliteration of foreign words;and 3 are single-morpheme words..high-order structure distribution.
to gainmore insights on complex word-formation struc-ture, we focus on all three-char words.
we ﬁnd thatthe root usually lies in the third character by 74.6%,and the percentage for the second and ﬁrst charac-ters is only 15.3 and 10.1 respectively.
lookingmore closely, we ﬁnd the following four dominatedstructures..1 ← 2 ← 31 ← 2 → 3.
34.7% (1 → 2) ← 315.3% 1 → 2 → 3.
34.2%7.0%.
difﬁculties in annotation.
since it is difﬁcult tocapture the patterns on unlabeled-dependency in-consistencies, we focus on confusion patterns inlabel annotation.
among all characters receivingthe same head but different labels from two annota-tors, 20.1% correspond to “{att, adv}” confusiondue to the ambiguity of the head character beinga verb or a noun.
the second confusion pattern is“{coo,frag}”, with a proportion of 18.6, which aremainly from proper nouns.
according to our guide-lines, if the meaning of a proper noun is compound-ing, annotators have to annotate its real internalstructures rather than using “frag”.
it is also verydifﬁcult to distinguish “obj” and “pobj”, since theboundary between prepositions and verbs is vaguein chinese..5 word-internal structure parsing.
with annotated wist, we try to address the secondquestion: can we train a model to predict word-internal structure?
we adapt the biafﬁne parserproposed by dozat and manning (2017), a widely.
5828score(i → j).
score(i l−→ j).
dev.
test.
biaﬃne.
biaﬃnes.
rhi.rdj.rh(cid:48)i.rd(cid:48)j.mlph.
mlpd.
mlph(cid:48).
mlpd(cid:48).
hi.
hj.
bilstm × 3..
.
.
xi.
.
.
.
xj.
.
.
..figure 2: the basic architecture of biafﬁne parser..used sentence-level dependency parser, for this pur-pose, and present results and analysis..5.1 biafﬁne parser.
we adopt the supar implementation released byzhang et al.
(2020).6 as a graph-based parser, bi-afﬁne parser casts a tree parsing task as searchingfor a maximum-scoring tree from a fully-connectedgraph, with nodes corresponding to characters inour case.
as shown in figure 2, it adopts stan-dard encoder-decoder architecture, consisting ofthe following components..input layer.
given an input sequence, each itemis represented as a dense vector xi.
for word-internal structure parsing, an item corresponds to acharacter, and we use char embedding..xi = emb(ci).
(1).
bilstm encoder.
then, a three-layer bilstmis applied to obtain context-aware representations.
we denote the hidden vector of the top-layer bil-stm for the i-th position as hi..i (as head) and rd.
biafﬁne scorer.
two separate mlps are appliedto each hi, resulting in two lower-dimensional vec-tors rhi (as dependent).
then thescore of a dependency i → j is obtained via a bi-afﬁne attention over rhi and rdj .
scoring of labeleddependencies such as i l−→ j is analogous..decoder.
with the scores of all dependencies,we adopt the ﬁrst-order algorithm of eisner (2000)to ﬁnd the optimal unlabeled dependency tree, andthen independently decide the highest-scoring labelfor each arc..cmuas las uas las65.13random 81.18 76.15 80.63 75.5867.0982.42 77.30 81.64 76.98+1.96+1.24 +1.15 +1.01 +1.4088.27 85.18 88.33 84.9877.72+5.85 +7.88 +6.69 +8.00 +10.63.
pretrained.
bert.
table 4: results of word-internal structure parsing us-ing different character representations..training loss.
during training, the parser com-putes two independent cross-entropy losses foreach position, i.e., maximizing the probability ofits correct head and the correct label between them..5.2 settings.
data.
we randomly split all words in wist intothree parts, 2,500/5,000 as development/test dataand remaining as training data..hyperparameters.
we set the dimension ofchar embeddings to 100. we obtain pre-trainedcharacter embeddings by training word2vec on chi-nese gigaword third edition.
in order to see effectof contextualized character representations, we ap-ply bert (devlin et al., 2019) 7 to each word asa char sequence.
the output vectors of the topfour layers are concatenated and reduced into adimension of 100 via an mlp.
for other hyper-parameters, we keep the default conﬁguration insupar..evaluation metrics.
we adopt the standard un-labeled and labeled attachment score (uas/las),i.e., the percent of characters that receives the cor-rect head (and label).
the complete match (cm) isthe percent of words having correct whole trees..5.3 results.
table 4 shows the main results under different charrepresentations.
it is obvious that using randomlyinitialized char embeddings, the parser can onlyreach about 76 in las.
this shows that parsingword-internal structure is very challenging with-out using extra resources.
when we pretrain charembeddings on large-scale labeled data, the perfor-mance can be consistently improved by over 1 pointin both uas/las, and nearly 2 points in cm.
fi-nally, employing the contextualized character rep-.
7bert-base-chinese：https://github.com/.
6https://github.com/yzhangcs/parser.
google-research/bert.
5829resentations dramatically improves performancefurther by about 6/8/10 points in uas/las/cm..however, even with bert, model performancestill lags behind averaged human performance(90.9 in las) by large margin.
our experiencedannotators can even reach more than 94. our ex-perience in manual annotation points out two pos-sible directions to enhance the model: 1) makinguse of sentence-level contextual information; 2)leveraging the meanings in dictionaries, usually inthe form of explanation or example sentences.
weleave them for future exploration..analysis on label-wise accuracy.
the secondmajor row in table 3 reports accuracy regarding dif-ferent labels for the model with bert.
the modelachieves the highest accuracy on “att” and “root”,possibly because the two labels take very largeproportion in the data for sufﬁcient model train-ing.
by contrast, “pobj” and “subj” have the lowestaccuracy, and are difﬁcult for models as well asdiscussed in section 3. this leads to another ob-servation that model accuracy is roughly correlatedwith annotation accuracy, implying the difﬁcultiesfor human and model are usually consistent..6 utilizing word-internal structure.
this section presents a preliminary study on utiliz-ing word-internal structure, aiming to address thethird question: is modeling word-internal structuresbeneﬁcial for word representation learning?.
we use sentence-level dependency parsing as thefocusing task (k¨ubler et al., 2009), mainly consid-ering resemblance in tree structure representationand close relatedness between the two tasks.
givenan input sentence w0w1...wm, the goal of depen-dency parsing is to ﬁnd an optimal dependency treefor the sentence.
again, we adopt supar (zhanget al., 2020) for implementation of biafﬁne parser(dozat and manning, 2017) as our basic parser..6.1 methods.
the basic parser applys a bilstm over charactersequence to obtain word representation.
in thispart, we propose two simple alternative methods toencode internal structure shown in figure 1-(c)..sent each word in the input layer:.
xi = emb(wi) ⊕ charlstm(wi)charlstm(wi) ← bilstm(..., zk, ...)zk = emb(ci,k).
(2).
where ci,k is the k-th character of wi.
the ﬁnalword representation from charlstm(wi) is ob-tained by concatenating two last-timestamp hiddenoutput vectors of a one-layer bilstm..labelcharlstm method.
considering thatthe word is usually very short and a bare label itselfprovides rich syntax information, we propose astraightforward extension to charlstm, named aslabelcharlstm, via minor modiﬁcation..zk = emb(ci,k) ⊕ emb(li,k).
(3).
where li,k represents the label between ci,k and itshead in the word-internal structure..labelgcn method.
previous work show thatgcn is very effective in encoding syntactic trees(marcheggiani and titov, 2017; zhang et al., 2018).
we follow the implementation of zhang et al.
(2018) and use a two-layer gcn as a more so-phisticated way.
in order to utilize labels, we ex-tend vanilla gcn to have the same input with la-belcharlstm, i.e., zk.
we obtain the ﬁnal wordrepresentation by performing average pooling overthe output vectors of the top-layer gcn..6.2 experiments.
settings.
following chen and manning (2014),we conduct experiments on ctb5 with thesame data split(16,091/803/1,910 sentences)and constituent-to-dependency conversion.
bothchar/label embeddings are randomly initialized andhave the same dimension of 50. for the parsersusing gold-standard pos tags, we randomly ini-tialized the pos tagging embeddings and set thedimension to 50. for other hyperparameters, weadopt the default conﬁguration of supar, includingthe pre-trained word embeddings..for multi-char words without annotated internalstructure, we use the automatic outputs from thetrained parser with bert in section 5, so that everyword corresponds to a single structure..we use word-wise uas/las/cm for evaluation,.
and punctuation is excluded in all metrics..basic charlstm method.
for each word, thebasic biafﬁne parser uses the concatenation ofword embeddings and charlstm outputs to repre-.
main results.
table 5 shows the parsing perfor-mance.
we can see that both labelcharlstmand labelgcn substantially outperform the basic.
5830uasbasic charlstm 88.31labelcharlstm 88.7889.02labelgcn88.66.w/o label.
las85.9686.5186.7686.28.cm32.0433.1932.9332.20.table 5: parsing performance on ctb5-test..uas lasma and hovy (2017)89.05 87.74dozat and manning (2017) 89.30 88.2390.59 89.29ma et al.
(2018)91.11 89.91basic charlstm91.31 90.15labelcharlstm91.31 90.16labelgcn.
all > 2 ≤ 2.unk.
basic charlstm 85.96 86.42 82.03 81.7386.76 87.10 83.79 84.30+0.80 +0.68 +1.76 +2.57.
labelgcn.
table 6: parsing las regarding to word frequency..charlstm method.
labelgcn achieves the bestperformance on uas and las, with a gain of 0.71and 0.80 respectively..the fourth row reports performance of label-gcn without using label embedding, leading toconsistent accuracy drop, demonstrating the useful-ness of rich labels, which is a key contribution ofthis work, despite the extra annotation effort..analysis on rare words.
to gain more insightson how word-internal structure helps word repre-sentation learning, we divide the words in ctb5-test into several groups according to their frequencyin ctb5-train, and report ﬁne-grained accuracy intable 6. we can see that the overall performancegain is mostly contributed by improvement overrare words with low frequency or totally unknown.
this veriﬁes that word-internal structures can helpthe model to better represent rare words..results with gold-standard pos tags.
as sug-gested by a reviewer, we train our parser with gold-standard pos tags by concatenating the originalinput (i.e., xi in equation 2) with gold-standardpos tag embeddings, in order to compare with pre-vious works.
table 7 shows the results.
comparedwith the basic charlstm results in table 5, usinggold-standard pos tags as extra features for the ba-sic charlstm leads to substantial improvementsby 2.80 and 3.95 in uas and las respectively,and outperforms the previous works as presentedin table 7, showing that the basic charlstm canbe served as a strong baseline model..compared with the basic charlstm, utiliz-ing word-internal structure with labelcharlstmor labelgcn achieves consistently better perfor-mance by 0.24 and 0.25 respectively in las in.
table 7: parsing performance with gold-standard postags on ctb5-test..the scenario of using gold-standard pos tags.
be-sides the strong baseline, another reason that theimprovement brings by the internal-word structureis slight when using gold-standard pos tags is thata part of linguistic information in the pos tags andthe word-internal structures may be overlapping..7 conclusions.
this paper presents a thorough study on internalstructures of chinese words.
first, we annotate ahigh-quality word-internal structure treebank cov-ering over 30k words in ctb5, named as wist.
second, we perform analysis on wist from dif-ferent perspectives and draw many interesting ﬁnd-ings on chinese word-formation patterns.
third,we propose word-internal structure as a new task,and present benchmark results using a popular de-pendency parser.
finally, we conduct preliminaryexperiments with two simple methods, i.e., la-belcharlstm and labelgcn, to encode word-internal structure as extra word representation, andﬁnd promising performance gains on the sentence-level dependency parsing task.
analysis showsthat the rich dependency labels adopted in wistplay a key role, and word-internal structure is mostbeneﬁcial for rare word representation..acknowledgments.
the authors would like to thank the anonymousreviewers for the helpful comments.
we are verygrateful to guodong zhou for the inspiring discus-sions and suggestions on chinese word-internalstructures.
we thank kaihua lu for building theannotation system, and mingyue zhou, haopingyang, and yahui liu for their help in compilingannotation guidelines, and all the annotators fortheir hard work in data annotation.
this workis supported by the national key research anddevelopment program of china under grant no.
2017yfb1002104..5831references.
alexandra y. aikhenvald.
2007. typological distinc-tions in word-formation, 2 edition, volume 3, page1–65.
cambridge university press..shaosheng cao, wei lu, jun zhou, and xiaolong li.
2018. cw2vec: learning chinese word embeddingswith stroke n-gram information.
in proceedings ofaaai, pages 5053–5061..danqi chen and christopher d. manning.
2014. afast and accurate dependency parser using neural net-works.
in proceedings of emnlp, pages 740–750..xinxiong chen, lei xu, zhiyuan liu, maosong sun,and huan-bo luan.
2015. joint learning of charac-ter and word embeddings.
in proceedings of ijcai,pages 1236–1242..fei cheng, kevin duh, and yuji matsumoto.
2014.parsing chinese synthetic words with a character-based dependency model.
in proceedings of lrec,pages 67–72..fei cheng, kevin duh, and yuji matsumoto.
2015.synthetic word parsing improves chinese word seg-mentation.
in proceedings of ijcai, pages 262–267..jason p.c.
chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
tacl,4:357–370..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of naacl-hlt, pagesstanding.
4171–4186..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in proceedings of iclr..jason eisner.
2000. bilexical grammars and theircubic-time parsing algorithms.
in advances in prob-abilistic and other parsing technologies, pages 29–62. kluwer academic publishers..david gaddy, mitchell stern, and dan klein.
2018.what’s going on in neural constituency parsers?
anin proceedings of naacl-hlt, pagesanalysis.
999–1010..chen gong, zhenghua li, min zhang, and xinzhoujiang.
2017. multi-grained chinese word segmenta-tion.
in proceedings of emnlp, pages 692–703..sandra k¨ubler, ryan mcdonald, and joakim nivre.
2009. dependency parsing.
morgan and claypool..yuxuan lai, yijia liu, yansong feng, songfang huang,and ongyan zhao.
2021. lattice-bert: leverag-ing multi-granularity representations in chinesepre-trained language models.
in proceedings of naacl..haonan li, zhisong zhang, yuqi ju, and hai zhao.
2018. neural character-level dependency parsingfor chinese.
in proceedings of aaai, pages 5205–5212..yixuan li, kim gerdes, and dong chuanming.
character-level annotation for chinesein pro-.
2019a.
surface-syntactic universal dependencies.
ceedings of depling, pages 216–226..zhenghua li, xue peng, min zhang, rui wang, andluo si.
2019b.
semi-supervised domain adaptationin proceedings of acl,for dependency parsing.
pages 2386–2395..zhongguo li.
2011. parsing the internal structure ofwords: a new paradigm for chinese word segmenta-tion.
in proceedings of acl, pages 1405–1414..zhongguo li and guodong zhou.
2012. uniﬁed de-pendency parsing of chinese morphological and syn-tactic structures.
in proceedings of emnlp, pages1445–1454..qian lin, huating wen, jing yang, xin liu, huan lin,hongji wang, and jinsong su.
2020. establishmentof corpus of internal hierarchical structure for chi-nese words (in chinese).
journal of xiamen univer-sity (natural science), 59:83–88..ting liu, jinshan ma, and sheng li.
2006. build-ing a dependency treebank for improving chineseparser.
journal of chinese languige and comput-ing, 16(4):207–224..xuezhe ma and eduard hovy.
2017. neural probabilis-tic model for non-projective mst parsing.
in pro-ceedings of ijcnlp, pages 59–69..xuezhe ma, zecong hu, jingzhou liu, nanyun peng,graham neubig, and eduard h. hovy.
2018. stack-in pro-pointer networks for dependency parsing.
ceedings of acl, pages 1403–1414..diego marcheggiani and ivan titov.
2017. encodingsentences with graph convolutional networks for se-in proceedings of emnlp,mantic role labeling.
pages 1506–1515..tom´as mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space.
in proceedings of workshopon iclr..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in proceedings of naacl-hlt, page2227–2237..chi sun, xipeng qiu, and xuanjing huang.
2019.vcwe: visual character-enhanced word embed-dings.
in proceedings of naacl, pages 2710–2719..fei xia.
2009. the segmentation guidelines for thepenn chinese treebank (3.0).
university of penn-sylvania..5832jian xu, jiawei liu, liangang zhang, zhengyu li, andhuanhuan chen.
2016. improve chinese word em-in pro-beddings by exploiting internal structure.
ceedings of naacl-hlt, pages 1041–1050..jinxing yu, xun jian, hao xin, and yangqiu song.
2017. joint embeddings of chinese words, charac-ters, and ﬁne-grained subcharacter components.
inproceedings of emnlp, pages 286–291..shiwen yu, huiming duan, xuefeng zhu, bin swen,and baobao chang.
2003. speciﬁcation for cor-pus processing at peking university: word segmen-tation, pos tagging and phonetic notation (in chi-nese).
journal of chinese language and comput-ing, 13(2):121–158..meishan zhang, yue zhang, wanxiang che, and tingliu.
2013. chinese parsing exploiting characters.
inproceedings of acl, pages 125–134..meishan zhang, yue zhang, wanxiang che, and tingliu.
2014. character-level chinese dependencyparsing.
in proceedings of acl, pages 1326–1336..yu zhang, zhenghua li, and zhang min.
2020. efﬁ-cient second-order treecrf for neural dependencyparsing.
in proceedings of acl, pages 3295–3305..yuhao zhang, peng qi, and christopher d. manning.
2018. graph convolution over pruned dependencyin proceedingstrees improves relation extraction.
of emnlp, pages 2205–2215..hai zhao.
2009. character-level dependencies in chi-in proceedings of.
nese: usefulness and learning.
eacl, pages 879–887..dexi zhu.
1982. lexical notes on chinese grammar.
(in chinese).
the commercial press..5833