bootstrapped unsupervised sentence representation learning.
yan zhang∗1 ruidan he∗2 zuozhu liu†3 lidong bing2 haizhou li1,4.
1national university of singapore, singapore.
2damo academy, alibaba group.
3zju-uiuc institute.
4kriston ai lab, china.
eleyanz@nus.edu.sg, ruidan.he@alibaba-inc.comzuozhuliu@intl.zju.edu.cn, l.bing@alibaba-inc.comhaizhou.li@nus.edu.sg.
abstract.
as high-quality labeled data is scarce, unsu-pervised sentence representation learning hasin this paper, weattracted much attention.
propose a new framework with a two-branchsiamese network which maximizes the simi-larity between two augmented views of eachsentence.
speciﬁcally, given one augmentedthe online net-view of the input sentence,work branch is trained by predicting the rep-resentation yielded by the target network ofthe same sentence under another augmentedview.
meanwhile, the target network branchis bootstrapped with a moving average of theonline network.
the proposed method signif-icantly outperforms other state-of-the-art un-supervised methods on semantic textual sim-ilarity (sts) and classiﬁcation tasks.
it canbe adopted as a post-training procedure toboost the performance of the supervised meth-ods.
we further extend our method for learn-ing multilingual sentence representations anddemonstrate its effectiveness on cross-lingualsts tasks.
our code is available at https://github.com/yanzhangnlp/bsl..1.introduction.
sentence representation learning aims to map sen-tences into vectors that capture rich semantic in-formation.
among previous approaches, super-vised methods achieve state-of-the-art performanceby leveraging quality sentence labels.
for exam-ple, the recently proposed model sentence-bert(sbert) (reimers and gurevych, 2019) ﬁne-tunesa siamese bert network on natural language in-ference (nli) tasks with labeled sentence pairs.
itachieves state-of-the-art results on multiple seman-tic textual similarity (sts) tasks.
however, suchperformance is mostly induced by high-quality su-pervision, while labeled data are difﬁcult and ex-.
∗∗ equally contributed.
† corresponding author..pensive to obtain in practice.
zhang et al.
(2020)showed that sbert generalizes poorly on targettasks that differ signiﬁcantly from nli on whichsbert is ﬁne-tuned..many unsupervised methods learn sentencerepresentations by optimizing over various self-supervised learning (ssl) objectives on a large-scale unlabeled corpus.
early works often useauto-encoders (socher et al., 2011; hill et al.,2016) or next-sentence prediction (kiros et al.,2015) for sentence representation learning.
re-cently, more efforts have been devoted to represen-tation learning with transformer-based networks us-ing masked language modeling (mlm).
however,transformer-based methods do not directly producemeaningful sentence representations.
instead, sig-niﬁcant supervised ﬁne-tuning steps with labeleddata are commonly required to form good represen-tations (reimers and gurevych, 2019).
recently,giorgi et al.
(2020) and zhang et al.
(2020) pro-posed novel transformer-based frameworks to di-rectly learn sentence representations from an unla-beled corpus, which even exhibited competitive per-formance to the supervised counterparts on sometasks.
however, giorgi et al.
(2020) required longtext during training while the contrastive learningstrategy employed by zhang et al.
(2020) need acareful treatment of negative pairs.
more important,there is still great room for improvement in termsof the quality of learned sentence representations.
in this paper, we introduce bootstrappedsentence representation learning (bsl), a sim-ple and lightweight framework that directly learnssentence representations without supervised ﬁne-tuning.
our work is inspired by the recent successof siamese networks (bromley et al., 1994) forunsupervised visual representation learning (chenet al., 2020; grill et al., 2020; caron et al., 2020;chen and he, 2020), especially the byol frame-work (grill et al., 2020).
these models employed.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5168–5180august1–6,2021.©2021associationforcomputationallinguistics5168various kinds of unsupervised learning objectivesto maximize the similarity between two augmentedviews of each image, yielding performance onpar with supervised methods.
unlike contrastivelearning-based methods, which demand a carefullynegative sampling process and large batch sizes,byol could achieve great performance withoutnegative pairs..the proposed bsl works as follows.
given aninput sentence, we ﬁrst construct two augmentedviews through back-translation.
these two viewsare simultaneously fed into the two branches ofthe siamese network, i.e., an online network and atarget network following the terminology in (grillet al., 2020).
in particular, the online and targetnetworks use two pre-trained transformer networkswith the same structure, e.g., bert, to encode thetwo views separately.
during learning, the onlinenetwork is trained to predict the representation ofthe other augmented view generated by the targetnetwork, and its parameters are updated by min-imizing a predeﬁned prediction loss.
as for thetarget network, we apply a stop-gradient strategy(chen and he, 2020) and update it with a weightedmoving average of the online network.
hence, theoutputs of the target network are iteratively boot-strapped to serve as targets, enabling enhanced rep-resentation learning of the online network whileavoiding trivial solutions..our method is evaluated through extensive ex-periments.
empirical results show that bsl signiﬁ-cantly outperforms strong unsupervised baselineson a standard suite of sts and classiﬁcation tasksfrom the senteval benchmark (conneau and kiela,2018).
we also demonstrate that bsl can serve asan effective post-training approach to boost the per-formance of the state-of-the-art supervised sbertmodel.
we further extend our method for learningmultilingual sentence representations and demon-strate that it is able to outperform strong multilin-gual baselines on cross-lingual sts tasks underboth unsupervised and supervised settings.
de-tailed analysis of a few factors that could affect themodel performance is provided as well to motivatefuture research..2 related work.
2.1 sentence representation learning.
prior approaches for sentence representation learn-ing include two main categories – supervisedand unsupervised methods, while a few works.
might leverage on both of them.
most of thesupervised methods are trained on labeled natu-ral language inference (nli) datasets includingstanford nli (snli) (bowman et al., 2015) andmultinli (williams et al., 2018).
early methodsdemonstrate good performance on a wide rangeof tasks (conneau et al., 2017; cer et al., 2018).
recently, sbert (reimers and gurevych, 2019)ﬁne-tuned a pre-trained siamese bert networkon nli and demonstrated the state-of-the-art per-formance.
though effective, those methods highlyrely on labeled data and could be problematic toport to new domains.
zhang et al.
(2020) showedthat sbert generalizes poorly on target tasks witha data distribution signiﬁcantly different from thenli data..there are also fruitful outcomes for unsuper-vised methods.
some early studies attempt tolearn from the internal structures within each sen-tence (socher et al., 2011; hill et al., 2016; le andmikolov, 2014) or utilize a distributional hypothe-sis to encode contextual information with genera-tive (kiros et al., 2015; hill et al., 2016) or discrim-inative objectives (jernite et al., 2017; logeswaranand lee, 2018).
recently, transformer-based net-works attract more attentions (devlin et al., 2019;liu et al., 2019), however, they do not yield mean-ingful sentence representations directly withoutsupervised ﬁne-tuning.
reimers and gurevych(2019) show that sentence embeddings obtainedfrom bert without ﬁne-tuning even underperformthe glove embeddings (pennington et al., 2014) interms of semantic textual similarity..more recently, a few unsupervised methods wereproposed to learn sentence representations fromtransformer-based networks without supervisedﬁne-tuning.
li et al.
(2020) proposes to transformthe representation obtained by a pre-trained lan-guage model to an isotropic gaussian distribution.
giorgi et al.
(2020) minimizes the distance betweendifferent spans sampled from the same document.
however, it requires an extremely long documentof 2,048 tokens as input, which limits its applica-tions to domains with only short documents.
zhanget al.
(2020) proposed is-bert to maximize themutual information between the global embeddingand local n-gram embeddings of a given sentence.
however, is-bert requires careful negative sam-pling and the n-gram embeddings may be subopti-mal in capturing sentence-level semantics..5169siamese network.
the architecture of the proposedbsl is illustrated in figure 1. given a sentence x,we ﬁrst obtain two augmented views x1 (cid:44) t (x)and x2 (cid:44) t (cid:48)(x), where t and t (cid:48) are augmentationtransformations..the two views are fed into the siamese net-work separately.
the online network contains anencoder network fθ(·) and a predictor networkpθ(·).
the target network contains an encodernetwork fξ(·) without a predictor, leading to anasymmetric framework.
for the ﬁrst augmentedview x1, the online network outputs a representa-tion z1 (cid:44) pθ(fθ(x1)).
for the second augmentedview, the target network outputs a representationh2 (cid:44) fξ(x2).
afterwards, we deﬁne a meansquared loss between the two normalized represen-tations from the online and target networks, whichcan be simpliﬁed as minimizing their negative co-sine similarity:.
dθ,ξ(z1, h2) = − <.
>,.
(1).
z1(cid:107)z1(cid:107).
,.
h2(cid:107)h2(cid:107).
where (cid:107) · (cid:107) denotes the l2-norm and <, > denotesthe dot product between two vectors.
as the loss isasymmetric over the two views, we also feed x2 tothe online network and x1 to the target network toget ˜z2 (cid:44) pθ(fθ(x2)) and ˜h1 (cid:44) fξ(x1), leading tothe ﬁnal objective:.
lθ,ξ =.
dθ,ξ(z1, h2) +.
dθ,ξ(˜z2, ˜h1)..(2).
12.
12.though we deﬁne the loss with parameters{θ, ξ}, we only update θ during training, as shownin the stop-gradient operation fig 1. this stop-gradient operation is empirically demonstrated ef-fective for siamese network (grill et al., 2020;chen and he, 2020).
fξ is detached from the op-timization graph of lθ,ξ and will be updated witha weighted moving average of fθ.
the updatingdynamics becomes:.
θt ← θt−1 + (cid:53)θlθ,ξ,ξt ← δξt−1 + (1 − δ)θt..(3).
(4).
figure 1: the proposed framework bsl.
two aug-mented views x1 and x2 of sentence x are encoded bythe online network fθ and the target network fξ, respec-tively.
both networks are initialised from the same pre-trained language models but ξ are an exponential mov-ing average (ema) of θ during training.
p denotes thepredictor, which is a multi-layer perceptron and onlyapplied on the online side.
a stop-gradient operationis applied on the target side.
the loss lθ,ξ maximisethe similarity between online prediction z1 and targetrepresentation h2..2.2 unsupervised representation learning.
with siamese networks.
siamese networks have been increasingly used invarious models (chen and he, 2020; grill et al.,2020; caron et al., 2020) for unsupervised visualrepresentation learning.
these models typicallymaximize the similarity between two augmentedviews of an image encoded by the siamese net-work.
the main difference among these models ishow they prevent undesired trivial solutions.
mostworks rely on contrastive learning with negativesampling (chen et al., 2020; tian et al., 2020) toavoid collapsing.
our method bsl is mainly in-spired by byol (grill et al., 2020), which showsthat one can learn transferable visual representa-tions via bootstrapping representations without neg-ative sampling.
we transfer this learning strategyfrom images to texts with different network archi-tectures and augmenting methods..3 bsl.
3.1 model description.
given a sentence x sampled from the dataset dwithout label information, our goal is to learn ameaningful representation h (cid:44) f (x).
in our frame-work, we adopt the idea from byol for unsu-pervised sentence representation learning with a.here δ is the momentum.
when it is set to 1, thetarget network is never updated.
when it is set to 0,the target network is instantaneously synchronizedto the online network at each training step.
at theinference stage, we obtain the representation of asentence with the online encoder fθ..5170stop-gradoriginal sentenceema3.2 architecture details.
augmentation we use back-translation to obtaintwo augmented views x1 and x2.
in this work, weonly consider input sentence x in english.
we usean english-to-german machine translation (mt)system to translate x to y1, and subsequently use agerman-to-english mt system to translate y1 backto x1 to obtain one augmented view.
similarly,we use english-to-french and french-to-englishmt systems to obtain another augmented view x2.1besides back-translation, we also discuss other textaugmentation approaches in § 4.4..architecture the online network fθ and the tar-get network fξ take x1 and x2 as inputs and outputh1 and h2.
we use pre-trained language models toinitialize the weights in fθ and fξ such that theybeneﬁt from the knowledge obtained at the pre-training stage.
we apply average-pooling over out-puts from the pre-trained language models to obtainh1 and h2.
a multi-layer perceptron (mlp) pθ isstacked on top of fθ as the predictor to transformh1 to predictions z1 such as z1 matches the targetrepresentation h2..4 experiment.
design we conduct various experiments to eval-uate the effectiveness of the proposed method.
fol-lowing prior works (reimers and gurevych, 2019;zhang et al., 2020), our major evaluations are con-ducted on the semantic textual similarity (sts)tasks and the classiﬁcation tasks with the sentevaltoolkit (conneau and kiela, 2018).
to demonstratethe ﬂexibility of the proposed method, we furtherextend it for learning multilingual sentence rep-resentations and evaluate it on cross-lingual ststasks..implementation the mlp contains three linearlayers.
given an input vector of dimension d, theoutput dimensions of the three layers are kd →kd → d, where k is a hyperparameter controllingthe hidden size.
batch normalization and rectiﬁedlinear units (relu) are applied to the intermediatelinear layers.
we use bert-base or roberta-base to initialize the online and target networks inmonolingual settings..hyperparameter we tune learning rate, batchsize, momentum δ, and the hyperparameter k on.
the development set of sts-b (cer et al., 2017).
for all unsupervised experiments, we set learn-ing rate to 5e-4, momentum to 0.999, and k to8. adam (kingma and ba, 2015) is used as theoptimizer.
2.baselines under a unsupervised learning setting,we compare to the unigram-tfidf model, thesequential denoising auto-encoder (sdae) (hillet al., 2016), the skipthought (kiros et al., 2015)and fastsent (hill et al., 2016).
those models areall trained on the toronto book corpus with 70msentences (zhu et al., 2015).
we also compare withsentence representations obtained with the averageof glove embeddings (glove avg.
), the average ofbert embeddings (bert avg.
), and the [cls]representation of bert (bert [cls]), as thoseare common ways to get sentence-level represen-tations.
we compare with bert-ﬂow (li et al.,2020), a recent method that transforms the represen-tation obtained by bert to an isotropic gaussiandistribution.
in addition, we compare with two un-supervised bert ﬁne-tuning methods.
the ﬁrst isto ﬁnetune bert with masked language modeling(mlm) objective (bert-mlm) (gururangan et al.,2020).
the second is is-bert (zhang et al., 2020)which employs a mutual information maximiza-tion objective for ﬁne-tuning bert.
we denote ourmodel initialized by bert-base (roberta-base)as bsl-bert (bsl-roberta)..(use).
sentence encoder.
under a supervised learning setting, wecompared to infersent (conneau et al., 2017),universal(ceret al., 2018), and sentence bert/roberta(sbert/sroberta) (reimers and gurevych,trained on the snli2019), which are allto adapt bsl to aand multinli datasets.
supervised learning setting, we ﬁrsttrain asbert (sroberta) model and then use thelearned weights to initialize the online and targetnetworks of bsl and perform bsl training.
we denote this model variant as bsl-sbert(bsl-sroberta)..4.1 semantic textual similarity (sts).
senteval contains a suite of sts datasets includ-ing the sts tasks 2012-2016 (agirre et al., 2012,2013, 2014, 2015, 2016), the sts benchmark (sts-b) (cer et al., 2017), and the sick-relatednessdataset (marelli et al., 2014).
these datasets con-.
1we use google translation engine.
the datasets are re-.
2hyperparameters and implementation details are attached.
leased..in appendix a.
5171model.
sts-12 sts-13 sts-14 sts-15 sts-16 sts-b sick-r avg..unigram-tfidf†sdae†skipthought†fastsent†glove avg.‡bert avg.‡bert [cls]‡bert-mlmis-bert∗bert-ﬂow◦.
ours: bsl-bertours: bsl-roberta.
infersent‡use‡sbert‡sroberta‡bert-ﬂow◦.
ours: bsl-sbertours: bsl-sroberta.
----55.1438.7820.1648.8656.7759.54.
67.8368.47.
52.8664.4970.9771.5467.75.
71.4875.44.unsupervised methods.
----70.6657.9830.0164.7669.2464.69.
71.4072.41.
66.7567.8076.5372.4976.73.
81.2080.25.
58.0012.0027.0063.0059.7357.9820.0956.9761.2164.66.
66.8868.48.
62.1564.6173.1970.8075.53.
73.7876.14.
----68.2563.1536.8870.8675.2372.92.
79.9778.50.
72.7776.8379.0978.7480.63.
79.0881.62.
----63.6661.0638.0864.6570.1671.84.
73.9772.77.
66.8773.1874.3073.6977.58.
79.2380.00.
----58.0246.3516.5064.3369.2158.56.
73.7478.77.
68.0374.9277.0377.7779.10.
80.6781.90.supervised methods.
52.0046.0057.0061.0053.7658.4042.6367.7664.2565.44.
70.4069.97.
65.6576.6972.9174.4678.03.
76.9577.02.
----61.3254.8129.1962.6066.5865.38.
72.0372.76.
65.0171.2274.8974.2176.48.
77.4978.91.table 1: spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels.
ρ ∗ 100 is reported.
all bert/roberta-based models use bert/roberta-base as the transformer encoder.
results of baselines marked with † are obtained from (hill et al., 2016) (with a different number of decimal places).
results of baselines marked with ‡, ∗ and ◦ are obtained from (reimers and gurevych, 2019), (zhang et al., 2020)and (li et al., 2020), respectively..sist of sentence pairs with scores from 0 to 5, wherea larger score indicates higher semantic relatednessof the two sentences.
we use spearman’s rankcorrelation between the cosine-similarities of thesentence pairs and the gold scores as an evalua-tion metric, following prior works (reimers andgurevych, 2019; zhang et al., 2020)..most of the prior unsupervised methods weretrained on the toronto book corpus (zhu et al.,2015), while the most recent and the best per-formed unsupervised method is-bert was trainedon unlabeled texts from snli and multi-genre nli(multinli) datasets.
to have a fair comparisonwith is-bert, we follow its setting to train bslon unlabeled texts from the snli and multinlidatasets.
the bert-mlm baseline is also trainedwith the same setting for a fair comparison.
weillustrate the effect of corpus choice in § 4.4. snlicontains 570k sentence pairs and multinli con-tains 430k sentence pairs from a wider range ofgenres of spoken and written texts.
in both datasets,each sentence pair is labeled with contradiction,entailment, and neutral.
note that the labels are.
excluded when training bsl in unsupervised set-tings..table 1 presents the comparison results.
mod-els are divided into two sets: trained on unlabeleddata, or trained on labeled data.
for unsupervisedmodels, unigram-tfidf, sdae, skipthought andfastsent are trained on the toronto book corpuswhile bert-mlm, is-bert, bert-ﬂow and ourproposed method are trained on nli.
in the super-vised setting, bsl-sbert and bsl-srobertaonly take labeled entailment pairs as the inputs tothe online and target networks..we make the following observations.
first, bsloutperforms all prior unsupervised methods bylarge margins.
on average, it outperforms is-bertand bert-ﬂow trained with the same encoder andtraining corpus by 5.45%, and 6.65%, respectively.
it even outperforms supervised baselines infersentand use.
second, unsupervised bsl still under-performs sbert since the latter was ﬁne-tunedon labeled nli data.
we show that by using bslas a post-training approach, bsl-sbert ( bsl-sroberta) can further increase the average result.
5172model.
mr.cr.
subj mpqa sst trec mrpc avg..unigram-tfidf†sdae†skipthought†fastsent†glove avg.‡bert avg.‡bert [cls]‡bert-mlmis-bert∗.
ours: bsl-bertours: bsl-roberta.
infersent‡use‡sbert‡.
ours: bsl-sbertours: bsl-sroberta.
unsupervised methods.
73.774.676.570.877.2578.6678.6879.9281.09.
81.4280.92.
81.5780.0983.64.
83.3483.50.
79.278.080.178.478.3086.2584.8585.7887.18.
86.8990.41.
86.5485.1989.43.
89.6789.17.
90.390.893.688.791.1794.3794.2194.8294.96.
95.2093.80.
92.5093.9894.39.
95.6594.57.
82.486.987.180.687.8588.6688.2385.9788.75.
89.6089.96.
90.3886.7089.86.
89.9789.31.
--82.0-80.1884.4084.1386.0085.96.
87.7091.10.
84.1886.3888.96.
88.5891.60.
85.078.492.276.883.092.891.4092.4088.64.
93.0088.40.
88.293.289.6.
88.6092.40.supervised methods.
73.673.773.072.272.8769.5471.1374.1474.24.
74.0975.07.
75.7770.1476.00.
76.9377.1.
--83.50-81.5284.9484.6685.5785.91.
86.8487.09.
85.5985.1087.41.
87.5388.24.table 2: evaluation accuracies (%) on senteval classiﬁcation tasks.
scores are based on a 10-fold cross-validation.
results of baselines marked with † are obtained from (hill et al., 2016) (with a different number of decimal places).
results of baselines marked with ‡ and ∗ are obtained from (reimers and gurevych, 2019) and (zhang et al., 2020),respectively..by 2.6% (4.7%) from sbert.
this suggests thatbsl can also be used as an effective post-trainingapproach after supervised ﬁne-tuning..4.2 senteval classiﬁcation tasks.
following prior works (reimers and gurevych,2019; zhang et al., 2020), we evaluate sentencerepresentations on a set of classiﬁcation tasks fromsenteval.
the evaluation is done by the sentevaltoolkit.
it takes sentence representations as ﬁxed in-put features to a logistic regression classiﬁer, whichis trained in a 10-fold cross-validation setup and theprediction results is computed on the test-fold.
thesentence encoder is not ﬁne-tuned in the trainingprocess.
this set of tasks is the common bechmarkused to evaluate the transferability of sentence rep-resentations on downstream tasks..table 2 presents the comparison results.
on aver-age, bsl outperforms all prior unsupervised base-lines.
it also outperforms supervised baselines in-fersent and use, and only slightly underperformssbert.
bsl-sbert can marginally improve theresults of sbert.
bsl-sroberta achieves thebest performance..4.3 multilingual sts.
in this subsection, we show that bsl can be easilyextended for learning multilingual sentence rep-resentations.
following (reimers and gurevych,2020), we conduct evaluation on the multilingualsts 2017 dataset (cer et al., 2017) which containsannotated pairs for en-en, ar-ar, es-es, en-ar, en-es, en-tr, en-de, and en-fr..to learn multilingual representations under theunsupervised setting, we process the nli data asfollows.
we translate the english nli sentencesto ar, es, tr, de and fr using google transla-tion engine and pair the original english sentenceto each of its translations.
we obtain 5 pairs (en-ar/es/tr/de/fr) from one sentence and treatthe english sentence as one view and its translationas the other view.
we concatenate all pairs as thetraining data.
we use multilingual bert (mbert)to initialize fθ and fξ, such that the token-levelrepresentations between the different languages arealigned.
the remaining training procedure is thesame as described in § 3. we denote our unsu-pervised model as bsl-uns.
we compare withsentence representations obtained with mean pool-ing of mbert and xlm-r (conneau et al., 2020)embeddings under the unsupervised setting..for supervised learning, we compare with meth-.
5173model.
mbertxlm-r.ours: bsl-uns.
mbert-nli-stsbxlm-r-nli-stsbmbert ← sbert-nli-stsbxlm-r ← sbert-nli-stsbmuselabse.
ours: bsl-sup.
en-en es-es ar-ar en-ar en-de.
en-tr en-es.
en-fr.
unsupervised methods.
supervised methods.
54.450.7.
76.9.
80.278.282.582.586.479.4.
83.3.
56.751.8.
81.2.
83.983.183.083.586.980.8.
86.1.
50.925.7.
68.3.
65.364.478.879.976.469.1.
79.3.
16.717.4.
71.6.
30.944.077.277.879.374.5.
80.6.
33.921.3.
71.5.
62.259.578.978.982.173.8.
81.2.
16.09.2.
72.7.
23.942.473.274.075.572.0.
78.9.
21.510.9.
69.5.
45.554.779.279.779.665.5.
82.0.
33.016.6.
75.6.
57.863.478.878.582.677.0.
83.5.table 3: spearman’s rank correlation ρ between the cosine similarity of sentence representations and the goldlabels.
ρ*100 is reported.
results of baselines are obtained from (reimers and gurevych, 2020)..ods from (reimers and gurevych, 2020): mbert-/ xlm-r-nli-stsb denotes the setting where weﬁne-tune xlm-r and mbert on the english nliand the english training set of the sts benchmark(sts-b); mbert- /xlm-r ← sbert-nli-stsbis the knowledge-distillation method proposed intheir paper where we learn mbert and xlm-r toimitate the output of the english sbert trained onnli and sts-b with multilingual parallel sentencepairs.
we also compared to results of muse (chi-dambaram et al., 2019) and labse (feng et al.,2020), which use dual encoder transformer archi-tectures.
muse was trained on question-answerpairs, snli, translated snli data, and parallel cor-pora over 16 languages.
labse was trained on6 billion translation pairs for 109 languages.
forbsl, we initialize our online and target networkswith the learned weights from xlm-r ← sbert-nli-stsb3 and then perform bsl training in a sameway as described above.
we denote our model inthis setting as bsl-sup..table 3 presents the results.
under the unsuper-vised setting, averaging the multilingual token rep-resentations yields poor results.
bsl-uns achievespromising results with scores higher than 70. forthe supervised methods, we observe that directlyﬁne-tuning multilingual pre-trained models on en-glish nli and sts-b datasets does not general-ize well in a cross-lingual setting.
knowledgedistillation-based models are strong baselines.
ap-plying bsl as a post-training approach can boostthe results of the distilled models by large margins.
these observations demonstrate that bsl has the.
3downloaded.
from https://www.sbert.net/.
docs/pretrained_models.html.
ﬂexibility to be applied to learning multilingualsentence representations..4.4 analysis.
in this subsection, we discuss a few factors thatcould affect the model performance.
we use bert-base as the encoder for analysis..choice of corpus previous works (hill et al.,2016; cer et al., 2018) indicated that the datasetused for learning sentence representations in a su-pervised setting signiﬁcantly impacts their perfor-mance on sts tasks.
they found learning withnli datasets is particularly useful and yields goodresults on common sts benchmarks.
we havesimilar observations with the proposed unsuper-vised method.
in table 4, we show the results oftraining our model with a subset of 5 million sen-tences from the toronto book corpus.
this settingachieves an average result of 69.65 on sts tasks,still outperforming prior best unsupervised modelis-bert by 3.07%, which again demonstrates theeffectiveness of the proposed framework..however, we observe that the average resultobtained from training with the book corpus is2.38% lower than the result of training with the nlidatasets even the number of training pairs of the lat-ter is only 1 million.
training on both of them stillunderperforms training on nli alone.
this ﬁndingindicates that the choice of training corpus is a keyfactor that affects model performance.
when eval-uating the common sts benchmarks as used in ourexperiments, the nli datasets are better choices asthey are semantically related to the sts data.
wealso conduct an evaluation on an argument facetsimilarity task, which is more domain-speciﬁc and.
5174model.
sts-12.
sts-13.
sts-14.
sts-15.
sts-16.
sts-b sick-r.avg..ours: nli5m book5m book + nli.
ours: back-translationsynonymmlmnlientailback-translation + nlientail.
choice of training corpus.
effect of data augmentation.
67.8364.9068.93.
67.8362.3161.4765.8872.01.
71.4068.3368.23.
71.4069.7369.5872.6272.62.
66.8865.1866.13.
66.8863.3766.9165.6770.16.
79.9777.4879.72.
79.9777.7878.8678.3981.65.
73.9773.1272.54.
73.9767.9469.6274.1776.03.
73.7470.5574.94.
73.7470.0470.5573.4277.65.
70.4068.0569.76.
70.4066.3668.7870.6774.48.
72.0369.6571.46.
72.0368.2169.3971.5474.94.table 4: results with 1) different training corpora; and 2) different augmentation techniques.
spearman rankcorrelation ρ between the cosine similarity of sentence representations and the gold labels.
ρ ∗ 100 is reported..original.
the cats used to love plopping on the newspapers..synonymmlmback-translation.
the cats use to have sex ﬂump on the newspapers.
the cats used to love plucking in the newspapers.
cats loved to play in the newspapers..entailment.
oh when i had the uh cats at my place as soon as i took out thenewspaper to read it they would plop right down on top of it andjust not move and just stay there forever..table 5: an example of augmentations generated by different approaches..dissimilar to the nli tasks.
the results are pro-vided in appendix b. we ﬁnd that in this scenario,training with nli data yields poor generalizationresults on the target test set while training on the tar-get raw text yields a much better performance.
theresults indicate that semantically related corpus tothe target task should be adopted as the training set..augmentation techniquesit has been shownthat data augmentation plays a crucial role in unsu-pervised visual representation learning (he et al.,2020; chen et al., 2020; grill et al., 2020).
theimages can be augmented easily by rotating, re-sizing, or cropping (chen et al., 2020).
however,less work has been done on augmentation tech-niques for texts (fang et al., 2020; giorgi et al.,2020).
here, we study how different augmentationtechniques would affect the model performance.
we present the results of another two augmenta-tion approaches besides back-translation in table 4.synonym denotes the setting where we randomlyreplace a few words with their synonyms.
mlmdenotes the setting where we ﬁrst randomly maska few tokens and then use a pre-trained maskedlanguage model to generate the masked tokens.
speciﬁcally, for both methods, given a sentencex, we make x1 = x and obtain x2 with the re-spective augmentation technique.
we found that.
using one augmented view performs slightly bet-ter than using two augmented views for synonym-and mlm-based methods.
one possible reason isthat these methods may generate augmented sen-tences with semantics totally different from theoriginal sentences as we will show in this subsec-tion.
such kind of augmentation may bring in toomuch randomness and noise.
therefore using twoaugmented views might instead harm the modelperformance..for synonym, we select 30% of words and sub-stitute them with similar words according to word-net (miller, 1995).
for mlm, we mask 20% oftokens and use roberta-base for token gener-ation.
in addition, we show results of a settingwhere we treat the sentence pairs labeled with en-tailment from the nli datasets as the two views(nlientail) for our model, as well as a setting usingthe combination of nli unlabeled text with back-translations and the entailment pairs as the train-ing corpus(back-translation+nlientail).
the pur-pose is to illustrate how our model would performwith high quality augmented data..the results in table 4 show that our proposedframework can work with both synonym and mlm,as they still outperform is-bert on the averageresult by 1.63% and 2.81%, respectively.
how-ever, they are less effective compared to backt-.
5175momen..0.5.
0.9.
0.99.
0.999.
1.
33.18.
69.58.
72.51.
73.74.
68.19.table 6: performance w.r.t.
momentum on sts-b.
spearman rank correlation ρ (∗100) is reported..methods.
16.
32.
64.
128.bslcontrastive.
69.2168.18.
71.0870.06.
72.0271.04.
72.0171.81.table 7: performance under different batch sizes.
theaverage spearman rank correlation across sts12-16,sts-b, and sick-r is reported..translation.
we observe that training with entail-ment pairs yields good results, with only 300ktraining pairs, nlientail is comparable to the modeltrained on all data from the nli datasets augmentedwith back-translation (1 million training pairs).
inaddition, when training on both (back-translation +nlientail), a 2.91% improvement on the average re-sult over back-translation is observed.
the resultsindicate that the quality of the augmented pairsdirectly affects the performance of the proposedframework..table 5 presents an example of augmentationsgenerated to the same sentence.4 we observe thatsynonym substitutes words without consideringthe context while mlm generates words based onthe context but losing the original word semantics.
back-translation yields a relatively better sentence,however, the drawback of which is that it relies onexternal machine translation systems.
the entail-ment refers to the sentence in the nli datasets towhich the original sentence has an entailment rela-tion.
it can be regarded as an ideal augmentation ofthe original sentence.
how to automatically gener-ate such augmentations remains an open question,and we leave it to future research..momentum the momentum δ in equation (4) isan important hyperparameter.
when it is set to 1,the target network is never updated and remainsthe same to its initialization.
when it is set to 0,the target network is updated to the online networkat each training step.
table 6 shows the results ofour method with different values of momentum.
we observe that our proposed method works bet-ter with larger momentum near but not equals to 1.a similar phenomenon has also been observed inbyol (grill et al., 2020).
in addition, we ﬁnd that.
4more examples are provided in appendix c.although directly averaging the token embeddingsfrom bert yields poor sentence representationsas shown in table 1, initializing the target networkusing bert and keeping it unchanged (set momen-tum to 1) during the learning procedure helps theonline network learn much better representations,yielding a 21.84% improvement on sts-b..batch size & contrastive learning lastly, weanalyze the effect of batch size.
table 7 showshow the proposed model performs with batch sizesin {16, 32, 64, 128}.
we also compare to a set-ting where contrastive learning is used as the self-supervised learning objective since it is more com-monly used in visual representation learning (chenet al., 2020).
speciﬁcally, in this setting, givena batch of n augmented sentence pairs (2n sen-tences), each of them is treated as a positive pair.
for each positive pair, we treat the other 2(n − 1)augmented examples within the minibatch as nega-tive examples..the results in table 7 show that for bsl, settingthe batch size to 64 yields the best result.
over-all bsl is less sensitive to changes in batch sizewhile contrastive learning tends to perform betterwith a larger batch size such that sufﬁcient negativesamples can be obtained.
contrastive learning mayachieve better performance with a larger batch sizewhile we leave it for future investigation due to itslarge memory consumption..5 conclusion.
in this paper, we propose bsl for unsupervisedsentence representation learning.
the experimentalresults demonstrate that our method could signiﬁ-cantly outperform the state-of-the-art unsupervisedmethods and it can be further extended for learn-ing multilingual sentence representations.
in fu-ture work, we expect both theoretically advance ofsiamese networks for representation learning, e.g.,why stop-gradient works so well and how to furtherimprove the updating dynamics, as well as speciﬁ-cally designated ideas for nlp, e.g., augmentationor learning objectives..acknowledgements.
this work is partly supported by human-robotinteraction phase 1 (grant no.
19225 00054),national research foundation (nrf) singaporeunder the national robotics programme; hu-man robot collaborative ai for ame (grant no.
a18a2b0046), nrf singapore..5176references.
eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, i˜nigo lopez-gazpio, montse maritxalar, radamihalcea, german rigau, larraitz uria, and janycewiebe.
2015. semeval-2015 task 2: semantic tex-tual similarity, english, spanish and pilot on inter-pretability.
in proc.
of semeval@acl..eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, rada mihalcea, german rigau, and janycesemeval-2014 task 10: multilin-wiebe.
2014.in proc.
of se-gual semantic textual similarity.
meval@acl..eneko agirre, carmen banea, daniel cer, monadiab, aitor gonzalez-agirre, rada mihalcea, ger-man rigau, and janyce wiebe.
2016.semeval-2016 task 1: semantic textual similarity, monolin-in proc.
of se-gual and cross-lingual evaluation.
meval@acl..eneko agirre, daniel cer, mona diab, and aitorgonzalez-agirre.
2012. semeval-2012 task 6: apilot on semantic textual similarity.
in proc.
of se-meval@acl..eneko agirre, daniel cer, mona diab, aitor gonzalez-agirre, and weiwei guo.
2013.
*sem 2013 sharedin the secondtask: semantic textual similarity.
joint conference on lexical and computational se-mantics..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proc.
of emnlp..j. bromley, j. bentz, l. bottou, i. guyon, y. lecun,c. moore, eduard s¨ackinger, and r. shah.
1994.signature veriﬁcation using a ”siamese” time delayneural network.
in proc.
of neurips..m. caron, i. misra, j. mairal, priya goyal, p. bo-janowski, and armand joulin.
2020. unsupervisedlearning of visual features by contrasting cluster as-signments.
in proc.
of neurips..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017.semeval-2017task 1: semantic textual similarity multilingual andin proc.
of se-cross-lingual focused evaluation.
meval@acl..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st. john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,brian strope, and ray kurzweil.
2018. universalsentence encoder for english.
in proc.
of emnlp..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproc.
of icml..xinlei chen and kaiming he.
2020. exploring simple.
siamese representation learning..muthu chidambaram, yinfei yang, daniel cer, steveyuan, yunhsuan sung, brian strope, and raykurzweil.
2019. learning cross-lingual sentencerepresentations via a multi-task dual-encoder model.
in proceedings of the 4th workshop on representa-tion learning for nlp (repl4nlp-2019)..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproc.
of acl..alexis conneau and douwe kiela.
2018. senteval: anevaluation toolkit for universal sentence representa-tions.
in proc.
of lrec..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromin proc.
ofnaturalemnlp..language inference data..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proc.
of naacl-hlt..hongchao fang, sicheng wang, meng zhou, jiayuanding, and pengtao xie.
2020. cert: contrastiveself-supervised learning for language understanding.
corr..fangxiaoyu feng, yinfei yang, daniel cer, naveenlanguage-corr,.
arivazhagan, and wei wang.
2020.agnostic bert sentence embedding.
abs/2007.01852..john m. giorgi, osvald nitski, gary d. bader, andbo wang.
2020. declutr: deep contrastive learningfor unsupervised textual representations.
corr..jean-bastien grill, florian strub, florent altch´e,corentin tallec, pierre h. richemond, elenabuchatskaya, carl doersch, bernardo avila pires,zhaohan daniel guo, mohammad gheshlaghi azar,bilal piot, koray kavukcuoglu, r´emi munos, andmichal valko.
2020. bootstrap your own latent: anew approach to self-supervised learning.
in proc.
of neurips..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
inproc.
of acl..kaiming he, haoqi fan, yuxin wu, saining xie, andross b. girshick.
2020. momentum contrast for un-supervised visual representation learning..5177yonglong tian, chen sun, ben poole, dilip krishnan,cordelia schmid, and phillip isola.
2020. whatmakes for good views for contrastive learning?
inproc.
of neurips..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proc.
ofnaacl-hlt..yan zhang, ruidan he, zuozhu liu, kwan hui lim,and lidong bing.
2020. an unsupervised sentenceembedding method by mutual information maxi-mization.
in proc.
of emnlp..yukun zhu, ryan kiros, s. richard zemel, ruslansalakhutdinov, raquel urtasun, antonio torralba,and sanja fidler.
2015. aligning books and movies:towards story-like visual explanations by watchingmovies and reading books.
in proc.
of iccv..felix hill, kyunghyun cho, and anna korhonen.
2016.learning distributed representations of sentencesfrom unlabelled data.
in proc.
of naacl-hlt..yacine jernite, samuel r. bowman, and david a. son-tag.
2017. discourse-based objectives for fast un-supervised sentence representation learning.
arxivpreprint arxiv:1705.00557..diederik p. kingma and jimmy ba.
2015. adam:in proc.
of.
a method for stochastic optimization.
iclr..ryan kiros, yukun zhu, russ r salakhutdinov,richard zemel, raquel urtasun, antonio torralba,and sanja fidler.
2015. skip-thought vectors.
inproc.
of neurips..quoc v. le and tomas mikolov.
2014. distributed rep-in proc..resentations of sentences and documents.
of icml..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020. on the sentenceembeddings from pre-trained language models.
inproc.
of emnlp..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..lajanugen logeswaran and honglak lee.
2018. anefﬁcient framework for learning sentence represen-tations.
in proc.
of iclr..marco marelli, stefano menini, marco baroni, luisabentivogli, raffaella bernardi, and roberto zampar-elli.
2014. a sick cure for the evaluation of compo-sitional distributional semantic models.
in proc.
oflrec..g. miller.
1995. wordnet: a lexical database for en-.
glish.
commun.
acm, 38:39–41..amita misra, brian ecker, and marilyn walker.
2016.measuring the similarity of sentential arguments indialogue.
in proc.
of the 17th annual meeting of thespecial interest group on discourse and dialogue..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proc.
of emnlp..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proc.
of emnlp-ijcnlp..nils reimers and iryna gurevych.
2020. makingmonolingual sentence embeddings multilingual us-ing knowledge distillation.
in proc.
of emnlp..richard socher, eric h. huang,.
jeffrey pennin,christopher d manning, and andrew y. ng.
2011.dynamic pooling and unfolding recursive autoen-in proc.
ofcoders for paraphrase detection.
neurips..5178a implementation details.
our implementation is based on python 3.6 andpytorch 1.6.0. all experiments were conducted ona rtx 8000 gpu (cuda version 10.2) conﬁg-ured on a standard workstation.
the workstation isconﬁgured with 2 intel xeon gold 6248r, 256gbram, and ubuntu 18.04 operating system.
weprovide main hyperparameters of our model train-ing on the nli datasets in the table 8. for cross-lingual experiments, we use bert-base-multilingual-cased and the other hyperparameters are the same.
the nli and related datasets can be downloadedfrom https://huggingface.co/datasets.
thedevelopment results of bsl on the nli dataset areshown in table 9..hyperparameter.
size/type.
batch sizelearning rateweight decayepsilonoptimizerbert typebert embedding sizekpooling strategyepoch num.
{16, 32, 64, 128}{1e-4, 2e-4, 5e-4}{0.1, 0.01, 0.001 }1e-6adambert-base-uncased768{8, 4}mean1.table 8: hyperparameters for training on the nlidataset..method.
sts-b-dev.
bslbsl-sbert.
79.4281.67.table 9: performance on sts-b development set.
spearman rank correlation ρ (∗100) is reported..b argument facet similarity.
we have demonstrated that the proposed methodsigniﬁcantly outperforms other unsupervised base-lines on a suite of sts and classiﬁcation tasksthat are commonly used in previous works.
how-ever, those tasks are less domain or task speciﬁc.
here, we further investigate the effectiveness ofbsl in a domain-speciﬁc scenario.
following priorworks (reimers and gurevych, 2019; zhang et al.,2020), we conduct evaluations on an argumentfacet similarity (afs) (misra et al., 2016) dataset..model.
r.ρ.unigram-tfidf†glove avg.†bert avg.†bert-mlmis-bert†infersent†sbert†.
46.7732.4035.3947.0449.1427.0816.27.
42.9534.0035.0745.9245.2526.6315.84.ours: bsl.
51.56.
50.47.table 10: average pearson correlation r and averagespearman’s rank correlation ρ over three topics onthe argument facet similarity (afs) corpus.
resultsmarked with † are obtained from (zhang et al., 2020)..the dataset consists of 6k argument pairs on threecontroversial topics: gun control, gay marriage,and death penalty.
each pair was annotated ona scale from 0 (different) to 5 (equivalent).
thisdataset is more challenging compared to the stsbenchmarks: the lexical gap between the sentencesin afs is larger and to be consider similar, a pairof arguments must not only make similar claims,but also provide a similar reasoning..we compare models in a setting where task- ordomain-speciﬁc labeled data is not available.
inthis setting, supervised method such as sbertand infersent need to be trained on nli data andperform cross-domain predictions on the afs sen-tence pairs.
unsupervised methods such as bert-mlm, is-bert and our proposed bsl can be di-rectly trained on the task-speciﬁc raw texts..table 10 shows the comparison results.
wepresent both pearson correlation and spearman’srank correlation.
the results show that the pro-posed method still outperforms other methods.
itis interesting to ﬁnd that the two supervised meth-ods infersent and sbert perform the worst in thissetting.
this is due to the fact that afs data dif-feres signiﬁcantly from nli data.
this suggeststhat the domain-relatedness between the trainingset and the target test set has a huge impact on themodel performance, and the models learned withsupervised methods are problematic to port to otherdistant domains..c more examples.
more examples of augmentations generated by dif-ferent approaches are provided in the table 11..5179original.
i realize she had written a new will ..i realize she had drop a new will.
i realize she had bought a new will.
i realize that she had made a new will..synonymmlmback-translation.
entailment.
original.
synonymmlmback-translation.
entailment.
original.
synonymmlmback-translation.
entailment.
original.
synonym.
mlmback-translation.
entailment.
original.
synonym.
mlm.
back-translation.
i was now quite convinced that she had made a fresh will, and 67 had called thetwo gardeners in to witness her signature..there are people who believe that the interest on the national debt is a problem ..there are the great unwashed who believe that the stake on the interior debt is a problem.
there are some who believe that compound interest on the national debt is a problem.
there are people who believe that national debt interest rates are a problem..but if congress opts for debt over taxation, you can count on thoughtless commentatorsto denounce the interest payments on that debt as a second, and separate, outrage..according to numerous studies, music and suicide have little to no correlation ..harmonise to various survey, music and suicide have little to no correlation.
according to other studies, music and suicide have little to no correlation...according to many studies, music and suicide have little or no correlation..numerous studies show that there is no association between music and suicide..the earliest human remains found on crete date back to the seventh millennium b.c..the earliest human remains found on crete date backward to the 7th millenaryb. degree celsius.
the earliest human remains found in the planet date back to the seventh millennium b.c.
the ﬁrst human remains discovered in crete date back to the seventh millennium bc..crete has ancient human remains..it’s a commitment to general education–a sequence of courses intended to developcritical thinking in a wide variety of disciplines–in opposition to early specialization..it ’ s a commitment to general education – a sequence of course specify to educatecritical thought in a wide variety of ﬁeld – in opposition to other specialism.
it’s a commitment to general education – a sequence of courses intended to developcritical thinking in a wide variety of disciplines – in opposition to early specialization.
it is a commitment to general education - a sequence of courses designed to developcritical thinking in a wide variety of disciplines - as opposed to early specialization..entailment.
general education’s focus is to develop students’ critical thinking skills..table 11: more examples of augmentations generated by different approaches..5180