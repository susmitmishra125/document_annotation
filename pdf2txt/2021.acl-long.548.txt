cross-replication reliability -an empirical approach to interpreting inter-rater reliability.
ka wonggoogle researchdanicky@gmail.com.
praveen paritoshgoogle researchpkp@google.com.
lora aroyogoogle researchl.m.aroyo@gmail.com.
abstract.
when collecting annotations and labeled datafrom humans, a standard practice is to useinter-rater reliability (irr) as a measure ofdata goodness (hallgren, 2012).
metrics suchas krippendorff’s alpha or cohen’s kappa aretypically required to be above a threshold of0.6 (landis and koch, 1977).
these abso-lute thresholds are unreasonable for crowd-sourced data from annotators with high cul-tural and training variances, especially on sub-jective topics.
we present a new alternativeto interpreting irr that is more empirical andcontextualized.
it is based upon benchmarkingirr against baseline measures in a replication,one of which is a novel cross-replication relia-bility (xrr) measure based on cohen’s (1960)kappa.
we call this approach the xrr frame-work.
we opensource a replication dataset of4 million human judgements of facial expres-sions and analyze it with the proposed frame-work.
we argue this framework can be used tomeasure the quality of crowdsourced datasets..1.introduction.
much content analysis and linguistics research isbased on data generated by human beings (hence-forth, annotators or raters) asked to make some kindof judgment.
these judgments involve systematicinterpretation of textual, visual, or audible mat-ter (e.g.
newspaper articles, television programs,advertisements, public speeches, and other multi-modal data).
when relying on human observers,researchers must worry about the quality of thedata — speciﬁcally, their reliability (krippendorff,2004).
are the annotations collected reproducible,or are they the result of human idiosyncrasies?.
respectable scholarly journals typically requirereporting quantitative evidence for the inter-raterreliability (irr) of the data (hallgren, 2012).
co-hen’s kappa (cohen, 1960) or krippendorff’s alpha(hayes and krippendorff, 2007) is expected to be.
figure 1: agreement measures for categorical data (landisand koch, 1977).
above a certain threshold to be worthy of publica-tion, typically 0.6 (landis and koch, 1977).
simi-lar irr requirements for human annotations datahave been followed across many ﬁelds.
in this pa-per we refer to this absolute interpretation of irras the landis-koch approach (fig.
1)..this approach has been foundational in guid-ing the development of widely used and shareddatasets and resources.
meanwhile, the landscapeof human annotations collection has witnessed atectonic shift in recent years.
driven by the data-hungry success of machine learning (lecun et al.,2015; schaekermann et al., 2020), there has beenan explosive growth in the use of crowdsourcingfor building datasets and benchmarks (snow et al.,2008; kochhar et al., 2010).
we identify threeparadigm shifts in the scope of and methodologiesfor data collection that make the landis-koch ap-proach not as useful in today’s settings..a rise in annotator diversity in the pre-crowdsourcing era lab settings, data were typicallyannotated by two graduate students following de-tailed guidelines and working with balanced cor-pora.
over the past two decades, however, the bulkof data are annotated by crowd workers with highcultural and training variances..a rise in task diversity there has been an in-creasing amount of subjective tasks with genuineambiguity: judging toxicity of online discussions(aroyo et al., 2019), in which the irr values range.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7053–7065august1–6,2021.©2021associationforcomputationallinguistics7053between 0.2 and 0.4; judging emotions expressedby faces (cowen and keltner, 2017), in which morethan 80% of the irr values are below 0.6; and a/btesting of user satisfaction or preference evalua-tions (kohavi and longbotham, 2017), where irrvalues are typically between 0.3 and 0.5..a rise in imbalanced datasets datasets are nolonger balanced intentionally.
many high-stakeshuman judgements concern rare events with sub-stantial tail risks: event security, disease diagnos-tics, ﬁnancial fraud, etc.
in all of these cases, asingle rare event can be the source of considerablecost.
high class imbalance has led to many com-plaints of irr interpretability (byrt et al., 1993;feinstein and cicchetti, 1990; cicchetti and fein-stein, 1990)..each of these changes individually has a pro-found impact on data reliability.
together, theyhave caused a shift from data-from-the-lab to data-from-the-wild, for which the landis-koch approachto interpreting irr is admittedly too rigid and toostringent.
meanwhile, we have seen a drop in thereliance on reliability.
machine learning, crowd-sourcing, and data research papers and tracks haveabandoned the use and reporting of irr for humanlabeled data, despite calls for it (paritosh, 2012).
the most cited recent datasets and benchmarksused by the community such as squad (rajpurkaret al., 2016), imagenet (deng et al., 2009), free-base (bollacker et al., 2008), have never reportedirr values.
this would have been unthinkabletwenty years ago.
more importantly, this is happen-ing against the backdrop of a reproducibility crisisin artiﬁcial intelligence (hutson, 2018)..with the decline of the usage of irr, we haveseen a rise of ad hoc, misguided quality metricsthat took its place, including 1) agreement-%, 2)accuracy relative to consensus, 3) accuracy relativeto “ground truth.” this is dangerous, as irr is stillour best bet for ensuring data reliability.
how canwe ensure its continued importance in this new eraof data collection?.
this paper is an attempt to address this problemby proposing an empirical alternative to interpret-ing irr.
instead of relying on an absolute scale, webenchmark an experiment’s irr against two base-line measures, to be found in a replication.
repli-cation here is deﬁned as re-annotating the same setof items with a slight change in the experimentalsetup, e.g., annotator population, annotation guide-lines, etc.
by ﬁxing the underlying corpus, we can.
ensure the baseline measures are sensitive to theexperiment on hand.
the ﬁrst baseline measureis the annotator reliability in the replication.
thesecond measure is the annotator reliability betweenthe replications.
in section 3, we present a novelway of measuring this.
we call it cross-kappa (κx).
it is an extension of cohen’s (1960) kappa and isdesigned to measure annotator agreement betweentwo replications in a chance-corrected manner..we present in appendix a the internationalreplication (irep) dataset,1 a large-scale crowd-sourced dataset of four million judgements of hu-man facial expressions in videos.
the dataset con-sists of three replications in mexico city, budapest,and kuala lumpur.2 our analysis in section 4shows this empirical approach enables meaning-ful interpretation of irr.
in section 5, we arguexrr is a sensible way of measuring the goodnessof crowdsourced datasets, where high reliability isunattainable.
while we only illustrate comparingannotator populations in this paper, the method-ology behind the xrr framework is general andcan apply to similarly replicated datasets, e.g., viachange of annotation guidelines..2 related work.
to position our research, we present a brief sum-mary of the literature in two areas: metrics formeasuring annotator agreement and their shortcom-ings (section 2.1), comparing replications of anexperiment (section 2.2)..2.1 annotator agreement.
artstein and poesio (2008) present a comprehen-sive survey of the literature on irr metrics used inlinguistics.
popping (1988) compare an astounding43 measures for nominal data (mostly applicable toreliability of data generated by only two observers).
since then, cohen’s (1960) kappa and its variants(carletta et al., 1997; cohen, 1968) have becomethe de facto standard for measuring agreement incomputational linguistics..one of the strongest criticisms of kappa is itslack of interpretability when facing class imbal-ance.
this problem is known as the kappa paradox(feinstein and cicchetti, 1990; byrt et al., 1993;.
1https://github.com/google-research-datasets/replication-.
dataset.
2on this task, raters received average hourly wages of$12, $20, and $14 usd in mexico city, budapest, and kualalumpur respectively.
see appendix a for annotation setup..7054warrens, 2010), or the ‘base rates’ problem (ue-bersax, 1987).
bruckner and yoder (2006) showclass imbalance imposes practical limits on kappaand suggest one to interpret kappa in relation tothe class imbalance of the underyling data.
oth-ers have proposed measures that are more robustagainst class imbalance (gwet, 2008; spitznageland helzer, 1985; stewart and rey, 1988).
pon-tius jr and millones (2011) even suggest abandon-ing the use of kappa altogether..2.2 agreement between replications.
replications are often being compared, but it isdone at the level of per-item mean scores.
cowenand keltner (2017) measure the correlation be-tween the mean scores of two geographical raterpools.
they use spearman’s (1904) correction forattenuation (discussed later in this paper) with split-half reliability.
snow et al.
(2008) measure thepearson correlations between the score of a sin-gle expert and the mean score of a group of non-experts, and vice versa.
in this comparison theauthors do not correct for correlation attenuation,hence the reported correlations may be strongly bi-ased.
bias aside, correlation is not suitable for taskswith non-interval data or task with missing data.
inthis paper, we propose a general methodology formeasuring rater agreement between replicationswith the same kind of generality, ﬂexibility, andease of use as irr..3 cross-replication reliability (xrr).
data reliability can be assessed when a set of itemsare annotated multiple times.
when this is done bya single rater, intra-rater reliability assesses a per-son’s agreement with oneself.
when this is doneby two or more raters, inter-rater reliability (irr)assesses the agreement between raters in an exper-iment.
we propose to extend irr to measure asimilar notion of rater-rater agreement, but wherethe raters are taken from two different experiments.
we call it cross-replication reliability (xrr).
thesereplications can be a result of re-labeling the sameitems with a different rater pool, annotation tem-plate, or on a different platform, etc..we begin with a general deﬁnition of cohen’s(1960) kappa.
we extend it to cross-kappa (κx) tomeasure cross-replication reliability.
we then usethis foundation to deﬁne normalized κx to measuresimilarity between two replications..3.1 kappa and its generalizations.
the class of irr measures is quite diverse, cover-ing many different experimental scenarios, e.g., dif-ferent numbers of raters, rating scales, agreementdeﬁnitions, assumptions about rater interchange-ability, etc.
out of all such coefﬁcients, cohen’s(1960) kappa has a distinct property that makes itmost suitable for the task on hand.
unlike scott’spi (scott, 1955), fleiss’s kappa (fleiss, 1971), krip-pendorf’s alpha (krippendorff, 2004), and manyothers, cohen’s (1960) kappa allows for two dif-ferent marginal distributions.
this stems from co-hen’s belief that two raters do not necessarily sharethe same marginal distribution, hence they shouldnot be treated interchangeably.
when we comparereplications, e.g., two rater populations, we aredeliberately changing some underlying conditionsof the experiment, hence it is safer to assume themarginal distributions will not be the same.
withineither replication, however, we rely on the raterinterchangeability assumption.
we think this moreaccurately reﬂects the current practice in crowd-sourcing, where each rater contributes a limitednumber of responses in an experiment, and henceraters are operationally interchangeable..cohen’s (1960) kappa was invented to comparetwo raters classifying n items into a ﬁxed numberof categories.
since its publication, it has beengeneralized to accommodate multiple raters (light,1971; berry and mielke jr, 1988), and to coverdifferent types of annotation scales: ordinal (co-hen, 1968), interval (berry and mielke jr, 1988;janson and olsson, 2001), multivariate (berry andmielke jr, 1988), and any arbitrary distance func-tion (artstein and poesio, 2008).
in this paper wefocus on janson and olsson’s (2001) generaliza-tion, which the authors denote with the lowercasegreek letter iota (ι).
it extends kappa to accom-modate interval data with multiple raters, and isexpressed in terms of pairwise disagreement:.
ι = 1 −.
dode.
..(1).
do in this formula represents the observed portionof disagreement and is deﬁned as:.
do =.
n.(cid:34).
(cid:18)b2.
(cid:19)(cid:35)−1.
(cid:88).
n(cid:88).
r<s.
i.d(xri, xsi),.
(2).
where n is the number of items, b the number ofannotators, i the item index, r and s the annotator.
7055indexes; (cid:80)r<s is the sum over all r and s such that1 <= r < s <= b. d() is a pairwise disagreementdeﬁned as:.
where.
d(xri, xsi) = (xri − xsi)2.
(3).
and.
for interval data, and.
d(xri, xsi) =.
(4).
(cid:40).
0, xri = xsi1, otherwise.
for categorical data.
note we are dropping jan-son and olsson’s multivariate reference in d() andfocusing on the univariate case.
de in the denomina-tor represents the expected portion of disagreementand is deﬁned as:.
(cid:34).
de =.
n2.
(cid:18)b2.
(cid:19)(cid:35)−1.
(cid:88).
n(cid:88).
n(cid:88).
r<s.
i.j.d(xri, xsj)..(5).
janson and olsson’s expression in eq.
1 is basedon berry and mielke jr (1988).
while the latter useabsolute distance for interval data, the former usesquared distance instead.
we follow janson andolsson’s approach because squared distance leadsto desirable properties and familiar interpretationof coefﬁcients (fleiss and cohen, 1973; krippen-dorff, 1970).
squared distance is also used in alpha(krippendorff, 2004).
berry and mielke jr (1988)show if b = 2 and the scale is categorical, ι in eq.
1reduces to cohen’s (1960) kappa.
for other ratingscales such as ratio, rank, readers should refer tokrippendorff (2004) for additional distance func-tions.
the equations for do and de are unaffectedby the choice of d()..3.2 deﬁnition of κx.
here we present κx as a novel reliability coefﬁ-cient for measuring the rater agreement betweentwo replications.
in janson and olsson’s general-ized kappa above, the disagreement is measuredwithin pairs of annotations taken from the sameexperiment.
in order to extend it to measure cross-replication agreement, we construct annotationpairs such that the two annotations are taken fromdifferent replications.
we do not consider anno-tation pairs from the same replication.
we deﬁnecross-kappa, κx(x, y ), as a reliability coefﬁcientbetween replications x and y :.
κx(x, y ) = 1 −.
(6).
do(x, y )de(x, y ).
,.
do(x, y ) =.
d(xri, ysi),.
(7).
1nrs.
n(cid:88).
r(cid:88).
s(cid:88).
i=1.
r=1.
s=1.
de(x, y ) =.
d(xri, ysj),.
1n2rs.
n(cid:88).
n(cid:88).
r(cid:88).
s(cid:88).
i=1.
j=1.
r=1.
s=1.
(8)where x and y denote annotations from replicationsx and y respectively, n is the number of items,r and s the numbers of annotations per item inreplications x and y respectively.
in this deﬁ-nition, the observed disagreement is obtained byaveraging disagreement observed in nrs pairs ofannotations, where each pair contains two annota-tions on the same item taken from two differentreplications.
expected disagreement is obtained byaveraging over all possible n2rs cross-replicationannotation pairs.
when each replication has only 1annotation per item, and the data is categorical, it iseasy to show κx reduces to cohen’s (1960) kappa.
κx is a kappa-like measure, and will have prop-erties similar to kappa’s.
κx is bounded between0 and 1 in theory, though in practice it may beslightly negative for small sample sizes.
κx = 0means there is no discernible agreement betweenraters from two replications, beyond what wouldbe expected by chance.
κx = 1 means all ratersbetween two replications are in perfect agreementwith each other, which also implies perfect agree-ment within either replication..3.3 κx with missing dataas presented, the two replications can have dif-ferent numbers of annotations per item.
however,within either replication, the number of annotationsper item is assumed to be ﬁxed.
we recognize thismay not always be the case.
in practice, itemswithin an experiment can receive varying numbersof annotations (i.e., missing data).
we now showhow to calculate κx with missing data..when computing irr with missing data,weights can be used to account for varying numbersof annotations within each item.
janson and ols-son (2004) propose a weighting scheme for iota ineq.
1. instead, we follow the tradition of krippen-dorff (2004) in weighting each annotation equallyin computing do and de.
that amounts to the fol-lowing scheme.
in do, we ﬁrst normalize withineach item, then we take a weighted average over.
7056all items, with weights proportional to the com-bined numbers of annotations per item.
in de, noweighting is required..since r and s can now vary from item to item,we index them using r(∗) and s(∗) to denote thatthey are functions of the underlying items.
werewrite do and de as:.
do(x, y ) =.
n(cid:88).
i=1.
r(i) + s(i)r + s.r(i)(cid:88).
s(i)(cid:88).
r=1.
s=1.
d(xri, ysi)r(i) · s(i)(9).
and.
with.
de(x, y ) =.
1r · s.n(cid:88).
n(cid:88).
r(i)(cid:88).
s(j)(cid:88).
i=1.
j=1.
r=1.
s=1.
d(xri, ysj),.
(10).
r =.
r(i), s =.
s(j),.
(11).
n(cid:88).
i.n(cid:88).
j.where r is the total number of annotations in repli-cations x, r(i) the number annotations on itemi in replication x, r = 1, 2, .
.
.
, r(i) (on item iin replication x); and similarly for s, s(j), and swith respect to replication y .
(cid:80)r(i)s=1 ineq.
9 and 10 are inner summations, where i and jare indexes from the outer summations.
withoutmissing data, r(i) = r for all i, and s(j) = s forall j, then r = nr, s = ns, reducing eq.
9 and10 to eq.
7 and 8..r=1 and (cid:80)s(j).
3.4 normalization of κxxrr is modeled closely after irr in order to serveas its baseline.
as irr measures the agreementbetween raters, so does xrr.
in other words, κx isreally a measure of rater agreement, not a measureof experimental similarity per se.
this distinctionis important.
if we want to measure how well wereplicate an experiment, we need to measure itsdisagreement with the replication in relationshipto their own internal disagreements.
the departurebetween inter-experiment and intra-experiment dis-agreements is important in measuring experimentalsimilarity..this calls for a normalization that considers κxin relation to irr.
first, we take inspirations fromspearman’s correction for attenuation (spearman,1904):.
where rxy is the observed pearson product-momentcorrelation between x and y (variables observedwith measurement errors), ρxy is an estimateof their true, unobserved correlation (in the ab-sence of measurement errors), and reliabilityx andreliabilityy are the reliabilities of x and y respec-tively.
eq.
12 is spearman’s attempt to correct forthe negative bias in rxy caused by the observationerrors in x and y.3.
eq.
12 is relevant here because of the closeconnection between cohen’s (1960) kappa and thepearson correlation, rxy.
in the dichotomous case,if the two marginal distributions are the same, co-hen’s (1960) kappa is equivalent to the pearsoncorrelation (cohen, 1960, 1968).
in the multi-category case, cohen (1968) generalizes this equiv-alence to weighted kappa, under the conditions ofequal marginals and a speciﬁc quadratic weightingscheme..based on this strong connection, we proposereplacing rxy in eq.
12 with κx and deﬁne normal-ized κx as:.
normalized κx =.
√.
κx(x, y )√irrx.
irry.
..(13).
deﬁned this way, one would expect normalizedκx to behave like ρxy.
that is indeed the case.
when we apply both measures to the irep dataset,we obtain a pearson correlation of 0.99 betweenthem (see section 4.5).
this leads to two insights.
first, we can interpret normalized κx like a disat-tenuated correlation, ρxy (see (muchinsky, 1996)for a rigorous interpretation).
second, normalizedκx approximates the true correlation between twoexperiments’ item-level mean scores..despite their afﬁnity, ρxy is not a substitute fornormalized κx for measuring experimental simi-larity.
normalized κx is more general as it canaccommodate non-interval scales and missing data..3.5 connection between xrr and irr.
by connecting normalized κx to ρxy, we can alsolearn a lot about κx itself.
to the extent that nor-malized κx approximates ρxy, we can rewrite eq.
13 as:.
κx(x, y ) ≈ ρxy.
irrx.
irry ..(cid:112).
(cid:112).
(14).
this formulation shows κx behaves like a prod-uct of ρxy and the geometric mean of the two irrs..ρxy =.
√.
rxy(cid:112)reliabilityy.
,.
reliabilityx.
(12).
3spearman relied on the assumption that the errors are.
uncorrelated with each other and with x and y..7057this has important consequences, as we can deducethe following.
1) holding constant the mean scores,and hence ρxy, the lower the irrs, the lower theκx.
intra-experiment disagreement inﬂates inter-experiment disagreement.
2) in theory ρxy ≤ 1.0,4hence κx is capped by the greater of the two irrs.
i.e., intra-experiment agreement presents a ceilingto inter-experiment agreement.
3) if x and y areidentically distributed, e.g., in a perfect replication,ρxy = 1 and κx(x, y ) = irrx = irry .
thus,when a low reliability experiment is replicated per-fectly, κx will be as low, whereas normalized κxwill be 1. this explains why normalized κx is moresuitable for measuring experimental similarity..in this section, we propose κx as a measure ofrater agreement between two replications, and nor-malized κx is as an experimental similarity metric.
in the next section, we apply them in conjunctionwith irr to illustrate how we can gain deeper in-sights into experiment reliabilities by triangulatingthese measures..4 applying xrr to the irep dataset.
as a standalone measure, irr captures the reliabil-ity of an experiment by encapsulating many of itsfacets: class imbalance, item difﬁculty, guidelineclarity, rater qualiﬁcation, task ambiguity, etc.
assuch, it is difﬁcult to compare the irr of differentexperiments, or to interpret their individual values,because irr is tangled with all the aforementioneddesign parameters.
for example, we cannot at-tribute a low irr to rater qualiﬁcation without ﬁrstisolating other design parameters.
this is the prob-lem we try to solve with xrr by contextualizingirr with meaningful baselines via a replication.
we will demonstrate this by applying this tech-nique to the irep dataset (appendix a).
we focuson a subset of 5 emotions for illustration purposes,with the rest of the reliability values provided inappendix b. in our analysis, irr is measured withcohen’s (1960) kappa and xrr with κx.
we willrefer to them interchangeably..4.1.irr variability across emotions.
first we illustrate in fig.
2 that different emotionswithin the same city can have very different irr.
for instance, the labels awe and love in mexicocity have an irr of 0.1208 and 0.597 respectively(table 1).
awe and love are completely different.
4spearman’s correction can occasionally produce a corre-.
lation above 1.0 (muchinsky, 1996)..emotions with different levels of class imbalanceand ambiguity, and without controlling for thesedifferences, the gap in their reliabilities is not unex-pected.
that is exactly the problem about compar-ing irrs – such comparisons are not meaningful.
we need something directly comparable to awe inorder to interpret its low irr.
if we do not compareemotions, and just consider awe using the landis-koch scale, that would not be helpful either.
wewould not be able to tell if its low irr is a resultof poor guidelines, general ambiguity in emotiondetection, or ambiguity speciﬁc to awe.
it’s moremeaningful to compare replications of awe itself..figure 2: histograms of 31 emotion labels’ irr in 3 cities.
the x-axis denotes buckets of irr values.
the y-axis denotesthe number of emotion labels in each of those buckets.
thereis a lot of variation between emotion labels within each city..table 1: irr values of 5 emotion labels in 3 cities..4.2.irr variability across replications.
while the aforementioned variation in irr betweenemotions is expected, irr of the same emotion canvary greatly between replications as well.
fig.
3shows two contrasting examples.
on the one hand,the irr of love is consistent across replications.
onthe other hand, the irr of contemplation varies alot.
we know the irr variation in contemplation isstrictly attributed to rater pool differences becausethe samples, platforms and annotation templatesare the same across experiments.
such variationin irr will be missed entirely by sampling basedapproaches for error-bars (e.g.
standard error, boot-strap), which assume a ﬁxed rater population..4.3 cross-replication rater agreement.
as shown, replication can facilitate comparisons ofirr by producing meaningful baselines.
however,irr is an internal property of a dataset, it doesnot allow us to compare two datasets directly.
to.
7058figure 3: irr values for label love (left) and contemplation(right) across the 3 cities.
there are different degrees of irrvariability in the two emotion labels..that end, we can apply κx to quantify the rateragreement between two datasets, as irr quantiﬁesthe rater agreement within a dataset.
interestingly,not only is κx useful for comparing two datasets,but it also serves as another baseline for interpretingtheir irrs..irr is a step toward ensuring reproducibility, sonaturally we wonder how much of the observedirr is tied to the speciﬁc experiment and howmuch of it generalizes?
this is of particular con-cern when raters are sampled in a clustered manner,e.g., crowd workers from the same geographicalregion, grad students sharing the same ofﬁce.
werarely make sure raters are diverse and represen-tative of the larger population.
high irr can bethe result of a homogeneous rater group, limitingthe generality of the results.
in the context of theirep dataset, that two cities having similar irrsdoes not imply their raters agree with each other ata comparable level, or at all.
we will demonstratethis with two contrasting examples..mexico city and budapest both have a moder-ate irr for sadness, 0.5147 and 0.5175 respec-tively, and their κx is nearly the same at 0.4709(fig.
4).
this gives us conﬁdence that the highirr of sadness generalizes beyond the speciﬁcrater pools.
in contrast, on contentment mexicocity and kuala lumpur have comparable levels ofirr, 0.4494 and 0.6363 respectively, but their κxis an abysmal -0.0344 5 (fig.
5).
in other words,the rater agreement on contentment is limited towithin-pool observations only.
this serves as an im-portant reminder that irr is a property of a speciﬁcexperimental setup and may or may not generalizebeyond that.
κx allows us to ensure the internalagreement has external validity..4.4 replication similarity.
κx is a step towards comparing two replications,but it is not a good standalone measure of replica-tion similarity.
to do that, we must also accountfor both replications’ internal agreements, e.g., vianormalized κx in eq.
13. fig.
6 shows an example.
mexico city and budapest have a low κx of 0.0817on awe.
on the surface, this low agreement mayseem attributable to differences between the raterpools.
however, there is a similarly low irr ineither city: 0.1208 in mexico city, and 0.117 inbudapest.
after accounting for irr, normalizedκx is much higher at 0.6872 (table 2), indicating adecent replication similarity between the two cities..figure 4: irr values of sadness in mexico city and bu-dapest and their κx value.
both cities have as much internalagreement as cross-replication agreement..figure 6: irr of awe in mexico city and budapest and theirxrr.
the low xrr is primarily a reﬂection of their low irrs..figure 5: irr of contentment in kuala lumpur and mexicocity and their κx.
both cities have high internal agreement,but no discernible cross-replication agreement..table 2: κx and normalized κx (in parentheses) of 5 emotionlabels in 3 replication pairs..5negative xrr value due to estimation error..70594.5 connection to ρxywe apply spearman’s correction for attenuation ineq.
12 to all 31 emotion labels in 3 replicationpairs.
the resulting ρxy is plotted against the corre-sponding normalized κx in fig.
7. both measuresare strongly correlated with a pearson correlationof 0.99. this justiﬁes interpreting normalized κxas a disattenuated correlation like ρxy..figure 7: scatter plot of ρxy (y-axis) and normalized κx(x-axis).
each dot is an emotion label in a pair of replications..5 measuring the quality of acrowdsourced dataset.
the irep dataset is replicated and is conducive toxrr analysis.
however, in practice most datasetsare not replicated.
is xrr still useful?
we presenta speciﬁc use case of xrr in this section and arguethat it is worth replicating a crowdsourced datasetin order to evaluate its quality..5.1 data target.
given a set of items, it is possible that annotationsof the highest attainable quality still fail to meet thelandis-koch requirements.
task subjectivity andclass imbalance together impose a practical limiton kappa (bruckner and yoder, 2006).
in thesesituations, the experimenter can forgo a data collec-tion effort for reliability reasons.
alternatively, theexperimenter may believe that data of sufﬁcientlyhigh quality can still have scientiﬁc merits, regard-less of reliability.
if so, what guidance can we useto ensure the highest quality data, especially whencollecting data via crowdsourcing?
this paper isheavily motivated by this question..xrr allows us to interpret irr not on an abso-lute scale, but against a replication, a reference ofsorts.
by judging a crowdsourced dataset against areference, we can decide if its meets a certain qual-ity bar, albeit a relative one.
in the irep dataset,.
all replications are of equal importance.
however,in practice, we can often deﬁne a trusted sourceas our target.
this trusted source can consist oflinguists, medical experts, calibrated crowd work-ers, or the experimenters themselves.
they shouldhave enough expertise knowledge and an adequateunderstanding of the task.
the critical criterionin choosing a target is its ability to remove com-mon quality concerns such as rater qualiﬁcationand guideline effectiveness..5.2.internal agreements.
by replicating a random subset of a crowdsourceddataset with trusted annotators,6 one can comparethe two irrs and make sure they are at a simi-lar level.
if the crowd irr is much higher, thatmay be an indication of collusion, or a set of overlysimplistic guidelines that have deviated from the ex-periment ﬁdelity (sameki et al., 2015).
if the crowdirr is much lower, it may just be a reﬂection ofannotator diversity, or it can mean under-deﬁnedguidelines, unequal annotator qualiﬁcations, etc.
further investigation is needed to ensure the dis-crepancy is reasonable and appropriate..5.3 mutual agreement.
suppose the two irrs are similar, that is not to saythat both datasets are similar.
both groups of an-notators can have high internal agreement amongstthemselves, but the two groups can agree on differ-ent sets of items.
if our goal is to collect crowd-sourced data that closely mirror the target, then wehave to measure their mutual agreement, in addi-tion to comparing their internal agreements.
recallfrom section 3.5 that if an experiment is replicatedperfectly, κx should be identical to the two irrs.
or more concisely, normalized κx should be equalto 1. thus a high normalized κx can assure us thatthe crowdsourced annotators are functioning as anextension of the trusted annotators, based on whichwe form our expectations..5.4 relation to gold data.
at a glance, this approach seems similar to thecommon practice of measuring the accuracy ofcrowdsourced data against the ground truth (resniket al., 2006; hripcsak and wilcox, 2002).
how-ever, they are actually fundamentally different ap-proaches.
κx is rooted in the reliability literaturethat does not rely on the existence of a correct.
62 or more ratings per item are needed to measure the irr..7060answer.
the authors argue this is an unrealisticassumption for many crowdsourcing tasks, wherethe input involves some subjective judgement.
ac-curacy itself is also a ﬂawed metric for annotationsdata due to its inability to handle data uncertainty.
for instance, when the reliability of the gold datais less than perfect, accuracy can never reach 1.0.furthermore, accuracy is not chance-corrected, soit tends to inﬂate with class imbalance..5.5 extending an existing dataset.
the aforementioned technique can also measure thequality of a dataset extension.
the main challengein extending an existing dataset is to ensure the newdata is consistent with the old.
the state-of-the-artmethod in computer vision is frequency matching– ensuring the same proportion of yes/no votes ineach image class.
recht et al.
(2019) extendedimagenet7 using this technique, concluding there isa 11% - 14% drop in accuracy across a broad rangeof models.
while frequency matching controlsthe distribution of some statistics, the impact ofthe new platform is uncontrolled for.
engstromet al.
(2020) pointed out a bias in this samplingtechnique.
overall, it is difﬁcult to assess how wellwe are extending a dataset.
to that end, xrr can beof help.
a high normalized κx and a comparableirr in the new data can give us conﬁdence in theuniformity and continuity in the data collection..6 discussion.
there has been a tectonic shift in the scope of andmethodologies for annotations data collection dueto the rise of crowdsourcing and machine learning.
in many of these tasks, a high reliability is often dif-ﬁcult to attain, even under favorable circumstances.
the rigid landis-koch scale has resulted in a de-crease in the usage and reporting of irr in mostwidely used datasets and benchmarks.
instead ofabandoning irr, we should adapt it to new waysof measuring data quality.
the xrr frameworkpresents a ﬁrst-principled way of doing so.
it is amore empirical approach that utilizes a replicationas a reference point.
it is based on two metrics –κx for measuring cross-replication rater agreement– and normalized κx for measuring replication sim-ilarity..we opensource a large-scale replication datasetof facial expression judgements analyzed with theproposed framework.
we show this framework can.
7http://www.image-net.org/.
be used to guide our crowdsourcing data collectionefforts.
this is the beginning of a long line ofinquiry.
we outline future work and limitationsbelow:.
conﬁdence intervals for κx conﬁdence inter-vals for κx and normalized κx are required for hy-pothesis testing.
though one can use the block-bootstrap for an empirical estimate, large samplebehavior of these metrics needs to be studied..sensitivity of κx with high class-imbalancethe xrr framework sidesteps the effect of class-imbalance by comparing replications on the sameitem set.
further analysis needs to conﬁrm thesensitivity of κx metrics in high class-imbalance..optimization of κx computation our methodrequires constructing many pairs of observations:n2rs.
this may get prohibitively expensive, whenthe number of items is large.
using algebraic sim-pliﬁcation and dynamic programming, this can bemade much more efﬁcient..alternative normalizations of κx we providedone particular normalization technique, but it maynot suit all applications.
for example, when com-paring crowd annotations to expert annotations, onecan consider, κx/irrexpert..alternative xrr coefﬁcients our proposedxrr coefﬁcient, κx,is based on cohen’s(1960) kappa for its assumption about rater non-interchangeability.
it may be useful to considerkrippendorff’s alpha and other agreement statisticsas alternatives for other assumptions and statisticalcharacteristics..we hope this paper and dataset will spark re-search on these questions and increase reporting ofreliability measures for human annotated data..acknowledgments.
we like to thank gautam prasad and alan cowenfor their work on collecting and sharing the irepdataset and opensourcing it..references.
lora aroyo, lucas dixon, nithum thain, olivia red-ﬁeld, and rachel rosen.
2019. crowdsourcing sub-jective tasks: the case study of understanding tox-in companion pro-icity in online discussions.
ceedings of the 2019 world wide web conference,www ’19, page 1100–1105, new york, ny, usa.
association for computing machinery..7061ron artstein and massimo poesio.
2008. inter-coderagreement for computational linguistics.
computa-tional linguistics, 34(4):555–596..j.l.
fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378–382..kenneth j. berry and paul w. mielke jr. 1988. a gen-eralization of cohen’s kappa agreement measure tointerval measurement and multiple raters.
educa-tional and psychological measurement, 48(4):921–933..joseph l fleiss and jacob cohen.
1973. the equiv-alence of weighted kappa and the intraclass corre-lation coefﬁcient as measures of reliability.
educa-tional and psychological measurement, 33(3):613–619..kurt bollacker, colin evans, praveen paritosh, timsturge, and jamie taylor.
2008. freebase: a collab-oratively created graph database for structuring hu-man knowledge.
in proceedings of the 2008 acmsigmod international conference on managementof data, sigmod ’08, page 1247–1250, new york,ny, usa.
association for computing machinery..cornelia taylor bruckner and paul yoder.
2006..in-terpreting kappa in observational research: baseratematters.
american journal on mental retardation,111(6):433–441..ted byrt, bishop janet, and john carlin.
1993. bias,prevalence and kappa.
j clin epidemiol, 46(5):423–9..jean carletta, amy isard, stephen isard, jacqueline ckowtko, gwyneth doherty-sneddon, and anne h.anderson.
1997. the reliability of a dialogue struc-ture coding scheme..domenic v cicchetti and alvan r feinstein.
1990.resolvingjournal of clinical epidemiology,.
low kappa:.
high agreement butthe paradoxes.
43(6):551–558..ii..jacob cohen.
1960. a coefﬁcient of agreement fornominal scales.
educational and psychological mea-surement, 20(1):37–46..jacob cohen.
1968. weighted kappa: nominal scaleagreement provision for scaled disagreement or par-tial credit.
psychological bulletin, 70(4):213..alan cowen and dacher keltner.
2017. self-report cap-tures 27 distinct categories of emotion bridged bycontinuous gradients.
proceedings of the nationalacademy of sciences of the united states of amer-ica, 114..j. deng, w. dong, r. socher, l. li, kai li, and lifei-fei.
2009. imagenet: a large-scale hierarchicalimage database.
in 2009 ieee conference on com-puter vision and pattern recognition, pages 248–255..logan engstrom, andrew ilyas, shibani santurkar,dimitris tsipras, jacob steinhardt, and aleksandermadry.
2020. identifying statistical bias in datasetin international conference on ma-replication.
chine learning, pages 2922–2932.
pmlr..alvan r feinstein and domenic v cicchetti.
1990.high agreement but low kappa: i. the problems ofjournal of clinical epidemiology,two paradoxes.
43(6):543–549..kilem li gwet.
2008. computing inter-rater reliabilityand its variance in the presence of high agreement.
british journal of mathematical and statistical psy-chology, 61(1):29–48..kevin a hallgren.
2012. computing inter-rater relia-bility for observational data: an overview and tuto-rial.
tutorials in quantitative methods for psychol-ogy, 8(1):23–34..andrew f hayes and klaus krippendorff.
2007. an-swering the call for a standard reliability measurefor coding data.
communication methods and mea-sures, 1(1):77–89..george hripcsak and adam wilcox.
2002. referencestandards, judges, and comparison subjects: rolesfor experts in evaluating system performance.
jour-nal of the american medical informatics associa-tion, 9(1):1–15..matthew hutson.
2018. artiﬁcial intelligence faces re-.
producibility crisis..harald janson and ulf olsson.
2001. a measure ofagreement for interval or nominal multivariate obser-vations.
educational and psychological measure-ment, 61(2):277–289..harald janson and ulf olsson.
2004. a measure ofagreement for interval or nominal multivariate obser-vations by different sets of judges.
educational andpsychological measurement, 64(1):62–70..shailesh kochhar, stefano mazzocchi, and praveenparitosh.
2010. the anatomy of a large-scale humanin proceedings of the acmcomputation engine.
sigkdd workshop on human computation, pages10–17..ron kohavi and roger longbotham.
2017. online con-trolled experiments and a/b testing.
encyclopedia ofmachine learning and data mining, 7(8):922–929..klaus krippendorff.
1970. bivariate agreement coefﬁ-cients for reliability of data.
sociological methodol-ogy, 2:139–150..klaus krippendorff.
2004. content analysis: an intro-.
duction to its methodology.
sage publications..j. richard landis and gary g. koch.
1977. the mea-surement of observer agreement for categorical data.
biometrics, 33(1):159–74..yann lecun, yoshua bengio, and geoffrey hinton.
2015. deep learning.
nature, 521(7553):436–444..7062richard j. light.
1971. measures of response agree-ment for qualitative data: some generalizationsand alternatives.
psychological bulletin, 76(5):365–377..paul m muchinsky.
1996. the correction for attenua-tion.
educational and psychological measurement,56(1):63–75..praveen paritosh.
2012. human computation must be.
reproducible.
in www 2012, lyon..robert gilmore pontius jr and marco millones.
2011.death to kappa: birth of quantity disagreementand allocation disagreement for accuracy assess-international journal of remote sensing,ment.
32(15):4407–4429..c. spearman.
1904. the proof and measurement ofassociation between two things.
the american jour-nal of psychology, 15(1)..edward l. spitznagel and john e. helzer.
1985. a pro-posed solution to the base rate problem in the kappastatistic.
archives of general psychiatry, 42(7):725–728..gavin w. stewart and joseph m. rey.
1988. a partialsolution to the base rate problem of the k statistic.
archives of general psychiatry, 45(5):504–505..john s uebersax.
1987. diversity of decision-makingmodels and the measurement of interrater agreement.
psychological bulletin, 101(1):140..roel popping.
1988. on agreement indices for nom-in sociometric research, pages 90–105..inal data.
springer..matthijs j warrens.
2010. a formal proof of a paradoxassociated with cohen’s kappa.
journal of classiﬁ-cation, 27(3):322–332..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..benjamin recht, rebecca roelofs, ludwig schmidt,and vaishaal shankar.
2019. do imagenet clas-in internationalsiﬁers generalize to imagenet?
conference on machine learning, pages 5389–5400.
pmlr..philip resnik, michael niv, michael nossal, gregoryschnitzer, jean stoner, andrew kapit, and richardtoren.
2006. using intrinsic and extrinsic metricsto evaluate accuracy and facilitation in computer-assisted coding.
in perspectives in health informa-tion management computer assisted coding con-ference proceedings, pages 2006–2006..mehrnoosh sameki, aditya barua, and praveen par-itosh.
2015. rigorously collecting commonsensejudgments for complex question-answer content.
inproceedings of the aaai conference on humancomputation and crowdsourcing, volume 3..mike schaekermann, christopher homan, lora aroyo,praveen paritosh, kurt bollacker, and chris welty.
2020. the ai bookie bet: will machine learningoutgrow human labeling?
ai magazine, 41(4):123–126..w scott.
1955. reliability of content analysis: thecase of nominal scale coding.
public opinion quar-terly, (19):321—-325..rion snow, brendan o’connor, daniel jurafsky, andandrew ng.
2008. cheap and fast – but is it good?
evaluating non-expert annotations for natural lan-guage tasks.
in proceedings of the 2008 conferenceon empirical methods in natural language process-ing, pages 254–263, honolulu, hawaii.
associationfor computational linguistics..appendices.
a the irep dataset: facial expressions.
replication dataset.
the irep dataset is a human annotated datasetwith a list of 30 emotion labels from a set of emo-tion classes deﬁned in cowen and keltner (2017)plus one additional label ‘unsure’.
during the datacollection process, raters used the set of 30 facialexpression labels to annotate their perception of thefacial expression present in a video.
as the purposeof this dataset is to illustrate how replications of hu-man labeling experiments can be used to determinethe overall quality of the resulting annotations, wehave omitted the reference to the actual video con-tent.
the annotated videos are thus referred to as‘items’ with a set of indices (item ids), e.g.
item 1,item 2, etc, stored in a csv format.
raters arereferred to as rater 1 or rater 2 across all raterpools.
they are just placeholders and do not implyparticular individuals (table 3)..the.
dataset.
contains.
3,939,418.is 15mb, and the dataset.
annota-the sizeishttps://github..tions on 38,499 unique items.
of the datasetreleased on github:com/google-research-datasets/replication-dataset.
to produce the repli-cations for this labeling experiment, we used raterpools in three different cities - budapest, kualalumpur and mexico city - on the same labelingplatform.
in table 4 we show the distribution ofitems and ratings across the different rater pools..7063b irr, xrr, and normalized xrr values.
for the irep dataset.
in table 5 we report the irr, κx, and normalizedκx obtained from the entire irep dataset..7064table 3: schema of the csv ﬁle: each line in the irep csv ﬁle corresponds to one item id annotated by rater 1 and rater 2with some of the emotion labels (label 1 .
.
.
label 30) annotated on the corresponding.
there is one additional column for“unsure” indicating when it was not possible to determine which expression was expressed..table 4: distribution of items and ratings across different rater pools, where every item is annotated by a maximum of 2 ratersfrom each pool.
here we show what fraction of the unique items were annotated by one or two raters in each rating pool..table 5: the ﬁst column shows the 30 emotion labels + “unsure” in the irep dataset.
the next 3 columns are their irr measuredby cohen’s (1960) kappa in mexico city (mc), kuala lumpur (kl), and budapest (bud).
the next 3 columns are the κx in the3 pairs of cities, and the last 3 columns are the corresponding normalized κx..7065