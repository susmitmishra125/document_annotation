pens: a dataset and generic framework for personalized newsheadline generationxiang ao♠♦∗, xiting wang♥, ling luo♠♦, ying qiao♣, qing he♠♦, xing xie♥†♠key lab of intelligent information processing of chinese academy of sciences (cas),institute of computing technology, cas, beijing 100190, china♥microsoft research asia♣microsoft♦university of chinese academy of sciences, beijing 100049, china.
aoxiang,luoling18s,heqing}xitwan,yiqia,xing.xie}.
@ict.ac.cn@microsoft.com.
{{.
abstract.
in this paper, we formulate the personalizednews headline generation problem whose goalis to output a user-speciﬁc title based on botha user’s reading interests and a candidate newsbody to be exposed to her.
to build up abenchmark for this problem, we publicize alarge-scale dataset named pens (personal-ized news headlines).
the training set is col-lected from user impressions logs of microsoftnews, and the test set is manually createdby hundreds of native speakers to enable afair testbed for evaluating models in an ofﬂinemode.
we propose a generic framework asa preparatory solution to our problem.
at itsheart, user preference is learned by leveragingthe user behavioral data, and three kinds ofuser preference injections are proposed to per-sonalize a text generator and establish person-alized headlines.
we investigate our datasetby implementing several state-of-the-art usermodeling methods in our framework to demon-strate a benchmark score for the proposeddataset.
the dataset is available at https://msnews.github.io/pens.html..1.introduction.
news headline generation (dorr et al., 2003; lopy-rev, 2015; alfonseca et al., 2013; tan et al., 2017;see et al., 2017; zhang et al., 2018; xu et al., 2019;murao et al., 2019; gavrilov et al., 2019; gu et al.,2020; song et al., 2020), conventionally consideredas a paradigm of challenging text summarizationtask, has been extensively explored for decades.
their intuitive intention is to empower the modelto output a condensed generalization, e.g., one sen-tence, of a news article..the recent year escalation of online contentvendors such as google news, topbuzz, and.
∗ this work was done when xiang was visiting msrasupported by the msra young visiting researcher program..† corresponding author..etc (larocque, 2003) propels a new research di-rection that how to decorate the headline as an irre-sistible invitation to users for reading through thearticle (xu et al., 2019) since more readings may ac-quaint more revenue of these platforms.
to this end,speciﬁed stylized headline generation techniqueswere proposed, such as question headline (zhanget al., 2018), sensational headline (xu et al., 2019)generation, and so on (shu et al., 2018; gu et al.,2020).
however, the over-decorate headlines mightbring negative effects as click-baits begin to be-come notorious in ubiquitous online services1..hence, the question is now changing to howto construct a title that catches on reader curios-ity without entering into click-bait territory.
in-spired by the tremendous success of personalizednews recommendation (an et al., 2019; wang et al.,2018; li et al., 2010; zheng et al., 2018) where theultimate goal is to learn users’ reading interestsand deliver the right news to them, a plausible solu-tion to this question could be producing headlinessatisfying the personalized interests of readers..it thus motivates the study of the personalizednews headline generation whose goal is to output auser-speciﬁc title based on both a user’s reading in-terests and a candidate news body to be exposed toher.
analogous to personalized news recommenda-tions, user preference can be learned by leveragingthe behavioral data of readers on content vendors,and the representation could personalize text gen-erators and establish distinct headlines, even withthe same news body, for different readers..however, it might be difﬁcult to evaluate the ap-proaches of personalized headline generation dueto the lack of large-scale available datasets.
first,there are few available benchmarks that simultane-ously contain user behavior and news content totrain models.
for example, most available news rec-.
1https://www.vizion.com/blog/do-clickbait-titles-still-work/.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages82–92august1–6,2021.©2021associationforcomputationallinguistics82ommendation datasets may predominately containuser-side interaction data, e.g., exposure impres-sions and click behaviors, but the textual featuresusually have already been overly pre-processed (liet al., 2010; zheng et al., 2018).
as a result, ad-vanced nlp techniques that extract useful featuresfrom textual data are limited.
news headline gen-eration datasets, on the other hand, usually consistof news bodies as well as their headlines, which allcome from the news-side (tan et al., 2017; zhanget al., 2018) rather than the user-side.
though themind dataset (wu et al., 2020), which was pre-sented by microsoft, simultaneously contains theuser-side behavioral data and the news-side origi-nal textual data, it was constructed for personalizednews recommendations rather than our problem.
the more challenging issue for evaluating person-alized headline generation approaches is the severecost during the test phase.
it could be intractableand infeasible to do an a/b test for every model inonline environments.
an efﬁcient and fair testbedto evaluate the models in an ofﬂine mode is inurgent demand to make the effectiveness and repro-ducibility of proposed models comparable..to this end, we publicize a dataset namedpens (personalized news headlines) in this pa-per as a benchmark to testify the performance ofpersonalized news headline generation approaches.
the training set of pens is collected from theuser impression logs of microsoft news2, in which500, 000 impressions over 445, 765 users on morethan one hundred thousand english news articlesare provided.
in addition, we collected 103 englishnative speakers’ click behaviors as well as theirmore than 20, 000 manually-crafted personalizedheadlines of news articles on the same news corpusfor testing.
these manually-written headlines areregarded as the gold standard of the user-preferredtitles.
then, proposed methods can take prevailingmatching metrics, e.g., rouge, bleu and etc.,to verify the performance..moreover, we propose a generic framework toinject personalized interests into a proposed neuralheadline generator to enable a beacon for this area,considering there are few existing works that cangenerate personalized news headlines.
in more de-tail, we devise three kinds of incorporation methodsto inject user interest representation into a proposedneural headline generator with a transformer-basedencoder and a pointer network-based (see et al.,.
2https://microsoftnews.msn.com.
figure 1: personalized news recommendation frame-work..2017) decoder.
we implement six state-of-the-artspersonalized news recommendation approaches tomodel user preferences and provide a horizontalstandard for the pens dataset.
the experimen-tal results show effective personalization modelingand comprehensive injection of user interests canunderpin an improvement in the quality of person-alized news headline generation.
we expect penscan serve as a benchmark for personalized headlinegeneration and bolster the research in this area..2 problem formulation and discussion.
in this section, we formulate the problem of person-alized news headline generation and differentiate itfrom personalized news recommendations..2.1 problem formulation.
1 , cu.
the problem of personalized news headline gen-eration is formulated as follows.
given a user uon an online content vendor, we denote his past2 , .
.
.
, cuclick history as [cun ] where each c rep-resents the headline of user u’s clicked news andeach headline is composed of a sequence of wordsc = [wc1, .
.
.
, wct ] with the maximum length oft .
then, given the news body of a piece of newsv = [wv1, .
.
.
, wvn] to be exposed to user u, ourproblem is to generate a personalized news head-line h uvt ] based on the clickednews [cu.
v = [yu1 , cu.
v1, .
.
.
, yu.
2 , .
.
.
, cu.
n ] and v..2.2 difference to personalized news.
recommendation.
here we differentiate our problem from personal-ized news recommendation whose general frame-work is shown as fig.
1..recall that the aim of personalized news rec-ommendation is computing and matching betweenthe candidate news and the user’s interests.
hence,.
83newsencodernewsencodernewsencodernewsencoderuser encoder…candidate newsclicked news…dotclick probabilityclick predictorrurv3 pens dataset.
in this section, we detail our pens dataset.
thedataset was randomly sampled impression logs ofmicrosoft news from june 14 to july 12, 2019.both user behaviors and news contents are involved,and each user was de-linked from the productionsystem when securely hashed into an anonymousid to reserve the data privacy issues..3.1 news corpus.
the pens dataset contains 113, 762 pieces of newsarticles whose topics are distributed into 15 cate-gories.
the topical distribution is demonstrated infig.
2 (c).
each news article in the pens datasetincludes a news id, a title, a body and a categorylabel.
the average length of news title and newsbody is 10.5 and 549.0, individually.
moreover, weextract entities from each news title and body andlink them to the entities in wikidata3.
it could betaken as an auxiliary source to facilitate knowledge-aware personalization modeling and headline gener-ation.
the key statistical information of the pensdataset is exhibited in fig.
2 (a)–(e)..3.2 training set.
the training set of pens consists of impressionlogs.
an impression log records the news arti-cles displayed to a user as well as the click be-haviors on these news articles when he/she visitsthe news website homepage at a speciﬁc time.
wefollow the mind dataset (wu et al., 2020) thatwe add the news click histories of every individ-ual user to his/her impression log to offer labeledsamples for learning user preferences.
hence, theformat of each labeled sample in our training set is[uid, tmp, clknews, uclknews, clkedhis], whereuid indicates the anonymous id of a user, tmpdenotes the timestamp of this impression record.
clknews and uclknews are the clicked news andun-clicked news in this impression, respectively.
clkedhis represents the news articles previouslyclicked by this user.
all the samples in clknews,uclknews and clkedhis are news ids, and they allsort by the user’s click time.
the histogram of thenumber of news in the clicked history per user isshown in fig.
2 (f)..3.3 test set.
to provide an ofﬂine testbed, we invited 103 en-glish native speakers (all are college students) man-.
3https://www.wikidata.org/wiki/wikidata:mainpage.
figure 2: the statistics of news corpus and training setof the pens dataset..learning accurate news and user representations iscritical for this problem.
under the neural frame-work, the news representation is usually modeledby a news encoder that encodes news title, newsbody or other attributes via various neural struc-tures (okura et al., 2017; wang et al., 2018; wuet al., 2019a; an et al., 2019; wu et al., 2019a).
the user representation is generated by engrav-ing the high-level aspects over their clicked newssequences using sequential (okura et al., 2017;an et al., 2019) or attentive modules (wu et al.,2019b,a), in which every news is encoded by thenews encoder in advance.
finally, the two repre-sentations are matched by the click predictor, andthe whole model is trained by the supervision ofclick signals..different from personalized news recommenda-tions, our personalized news headline generationcould be regarded as an nlp task than a user mod-eling and matching problem.
although it similarlyneeds to model preferences for the individual usersas what personalized news recommendations do,the output of our problem is a natural languagesequence that the target user might be interestedin, i.e., user-preferred news title, rather than a clickprobability score..84<66789101112131415>15title length0.000.020.040.060.080.100.120.14(a) distribution of title length<100100-500500-1k1k-1.5k1.5k-2k>2knews body length0.00.10.20.30.4(b) distribution news body length01234>4entity number0.000.050.100.150.200.25(c) distribution of entity number  in news title<55-1010-1515-2020-2525-30>30entity number0.000.050.100.150.20(d) distribution of entity number  in news bodysportsnewsf&dfinancemusiclifestyleweatherhealthvideoautosmoviestvjoytravelkids0.000.050.100.150.200.25(e) distribution of news topic<2020-4040-6060-80>80click number0.000.050.100.150.200.250.300.35(f) distribution of the length  of click history per usertable 1: the statistics of the training and test set inpens.
“wd.” in the table means word..traintest.
traintest.
#impression500,000naavg.
click/user74.8107.6.
#news111,76260,000avg.
wd./title10.510.8.
#user445,765103avg.
wd./body549.0548.2.ually create a test set by two stages.
at the ﬁrststage, each person browses 1, 000 news headlinesand marks at least 50 pieces he/she is interested in.
these exhibited news headlines were randomly se-lected from our news corpus and were arranged bytheir ﬁrst exposure time.
at the second stage, every-one is asked to write down their preferred headlinesfor another 200 news articles from our corpus, with-out exhibiting them the original news titles.
notethat these news articles are excluded from the ﬁrststage, and only news bodies were exhibited to theseannotators in this stage.
these news articles areevenly sampled, and we redundantly assign themto make sure each news is exhibited to four peopleon average.
the quality of these manually-writtenheadlines was checked by professional editors fromthe perspective of the factual aspect of the mediaframe (wagner and gruszczynski, 2016).
low-quality headlines, e.g.
containing wrong factualinformation, inconsistent with the news body, too-short or overlong, etc., are removed.
the rest areregarded as the personalized reading focuses ofthese annotators on the articles and are taken asgold-standard headlines in our dataset.
the statis-tics of the training and test sets of the pens areshown in table 1..4 our framework.
in this section, we illustrate our generic frameworkfor resolving personalized news headline genera-tion, and its key issue is how to inject the userpreference into a news headline generator.
wedevise a headline generator with a transformer en-coder and a pointer network decoder as our basemodel and propose three kinds of manners of in-jecting the user interests to generate personalizedheadlines.
the user interests can be derived fol-lowing the approaches in news recommendationscommunity, and we omit its details due to the spacelimitation.
the architecture of our proposed frame-work is shown as figure 3..figure 3: the generic framework of personalized newsheadline generation.
three kinds of user embeddinginjections are devised.
1 : utilizing user embedding toinitialize the decoder’s hidden state of the headline gen-erator.
2 : personalizing the attentive values on wordsin the news body by the user embedding.
3 : perturb-ing the choice between generation and copying via theuser embedding..4.1 headline generator.
the pin-point of our proposed headline generatoris a variant of transformer encoder and pointer net-work decoder.
during the encoding, given the newsbody of a candidate news v = [wv1, .
.
.
, wvn], itsrdw are ﬁrstword embeddings [ev1, .
.
.
, evn]fed to a two-layer positional encoder.
the ﬁrstlayer aims to enhance the word structure within thewhole news body sequence following vaswani et al.
(2017), and we add the positional encoding to eachembedding vector with,.
∈.
pe (pos,2i) = sin(pos/100002i/dw ).
pe (pos,2i+1) = cos(pos/100002i/dw ).
where pos is the word position and i is the dimen-sion.
we also apply a sentence-layer positional en-coding to discover structural relations from higherrl×ds represents thelevel.
suppose the wposposition embedding matrix of sentence level wherel is the sentence length and ds is the embeddingsize, the l-th row of wpos represents the positionalembedding of all the words in the l-th sentence.
thus, each word embedding e(cid:48)pos with positional.
∈.
(1).
(2).
85candidate news bodyclicked news…vnewsencodernewsencodernewsencoderuser encoder…cuncu2cu1user embeddingtransformerencoderattention distributionptgen!encoder hidden states!xtxt−1!decoder hidden statesf1f2context vectorvocabulary distributionpvocab(yt) final distribution×ptgen×(1−ptgen)p(yt)!
"#wv1wvnwv2!hv2hv1hvninformation can be represented as:.
e(cid:48)pos = (epos + pe pos) ⊕ wpos[l]..(3).
⊕.
wheremeans concatenation.
furthermore, multi-head self-attention mechanism (vaswani et al.,2017) is adopted to capture the word and sentenceinteractions by,.
where pvocab(wt) is zero when wt is out of vocab-ulary while (cid:80)j:wj =wt at,j = 0 when the wt is notin the news body.
ptgen is calculated based on thecontext vector ct, decoder state st and the decoderinput xt:.
ptgen = tθ(ct, st, xt),.
(9).
where.
θ is a function template as eq.
(6)..(4).
4.2 personalization by injecting user.
t.interests.
hi = softmax(.
e(cid:48)w q.i )(cid:62).
i (e(cid:48)w kdk.
√.
)e(cid:48)w vi.ki , w k, w vi.where dk = ds+dwand i = 1, .
.
.
, k given kheads.
w qr(ds+dw)×dk .
e(cid:48) rep-i ∈resents the word sequence embeddings in candi-date news v. thus, the encoder hidden statesh = h1.
h2, .
.
.
, hk can be derived..during the process of decoding, the decodedhidden state st at time step t can be derived aftergiven the input xt, and an attention distribution atover the encoder hidden states h is calculated as,.
⊕.
at = fθ(h, st).
(5).
fθ(h, st) = softmax(v (cid:62).
atttanh(whh + wsst + batt)) (6).
f.whereθ represents a function template parame-terized by θ to combine the linear transformationof the encoder and the decoder states, i.e., h andst. next, the context vector ct, which can be seenas a ﬁxed-size representation read from the newsbody at time step t, is computed by a weightedsum of the encoder hidden states over the attentiondistribution.
then the vocabulary distribution isproduced by,.
pvocab(wt) = tanh(vp[st; ct] + bv),.
(7).
where vp and bv are learnable parameters whilepvocab(wt) represents the probability distributionover all the words in the vocabulary to predict theword at time step t..inspired by pointer-generator network (see et al.,2017), which exhibits desirable performance on ei-ther dealing with out-of-vocabulary (oov) wordsor improving the reproducing factual details withcopy mechanism, we adopt a pointer ptgen at de-coding step t as a soft switch to choose betweengenerating a word from the vocabulary with a prob-ability of pvocab(wt) or copying a word from thenews body sampling from the attention distribu-tion at.
thus, the probability distribution over theextended vocabulary is computed by,.
so far, the imperative issue is to personalize theheadline generator by injecting the user’s prefer-ence.
recall that we can obtain user embeddingindicating user’s reading interests based on his/herhistorical clicked news sequences, and we denotesuch representation as u. as the user embedding uis usually not aligned with the word embeddings,it remains challenges to incorporate the user in-terests to inﬂuence the headline generation withpersonalized information..in our framework, based on our headline gener-ator, we propose three different manners to injectuser interests, considering different intuitions, andthey are exhibited in fig.
3. first, the most simpleand intuitive choice is to utilize the user embed-ding u to initialize the decoder hidden state of theheadline generator.
second, under the empiricalassumption that users may attend on different para-graphs and words in news articles correspondingto their individual preference, we inject u to affectthe attention distribution at in order to personal-ize the attentive values on the different words inthe news body.
that is, we modify eq.
(5) andderive at =θ(h, st, u).
lastly, we incorporatethe personalized information to perturb the choicebetween generating a word from vocabulary orcopying a word from the news body, and deriveptθ(ct, st, xt, u).
compared with eq.
(9), ugen =is taken as an auxiliary parameter, whereθ is alsoa function template as eq.
(6)..f.t.t.4.3 training.
in this subsection, we present the training processof our framework.
the headline generation canbe considered as a sequential decision-making pro-cess, hence we optimize a θ parametrized policy forthe generator by maximizing the expected rewardof generated headline y1:t :.
ey1:t ∼gθ [r(y1:t )]..(10).
p (wt) = pt.
genpvocab(wt) + (1 − pt.
gen).
(cid:88).
at,j.
(8).
j:wj =wt.
for the generator, policy gradient methods are ap-plied to maximize the objective function in eq.
(10),.
86whose gradient can be derived as,.
∇θj(θ) (cid:39) eyt∼gθ (yt|y1:t−1).
[∇θ log gθ(yt|y1:t−1) · r(y1:t−1, yt)].
(11).
where the reward r is estimated by the degree ofpersonalization, ﬂuency and factualness as we aimto generate a user-speciﬁc and coherent headline tocover the main theme of news articles and arousepersonalized reading curiosity.
the implementedrewards in our framework contain: (1) the person-alization of the generated headline is measured bythe dot product between the user embedding andthe generated headline representation.
such a scoremight imply a matching degree of personalization.
(2) the ﬂuency of a generated headline is assessedby a language model.
we adopt a two-layer lstmpre-trained by maximizing the likelihood of newsbody and consider the probability estimation of agenerated headline as the ﬂuency reward.
(3) wemeasure the degree of factual consistency and thecoverage by calculating the mean of rouge (lin,2004)-1, -2 and -l f-scores between each sentencein the news body and the generated headline, andthen take the average of the top 3 scores as thereward.
we average all three rewards as the ﬁ-nal signal.
as all the above reward functions onlyproduce an end reward after the whole headline isgenerated, we apply a monte carlo tree search toestimate the intermediate rewards..5 experimental evaluation.
in this section, we investigate our proposed pensdataset and conduct several comparisons to givebenchmark scores of personalized headline genera-tion on this dataset.
in the following part, we willintroduce the compared methods ﬁrst, and then de-tail the experimental setup, and ﬁnally present theresults and analysis..5.1 compared methods.
we mainly compare two groups of approaches.
theﬁrst group consists of various user modeling meth-ods, which are all sota neural-based news rec-ommendation methods: (1) ebnr (okura et al.,2017) learns user representations by aggregatingtheir browsed news with gru.
(2) dkn (wanget al., 2018) is a deep knowledge-aware networkfor news recommendation.
(3) npa (wu et al.,2019b) proposes personalized attention module inboth news and user encoder.
(4) nrms (wu et al.,2019c) conducts neural news recommendation with.
multi-head self-attention.
(5) lstur (an et al.,2019) models long- and shor-term user represen-tations based on user id embedding and sequen-tial encoding, individually.
(6) naml (wu et al.,2019a) proposes multi-view learning in user repre-sentation..to the best of our knowledge, there are noexclusive methods for personalized news head-line generation.
hence we take several headlinegeneration methods for comparison.
(1) pointer-gen (see et al., 2017) proposes an explicit proba-bilistic switch to choose between copying fromsource text and generating word from vocabu-lary.
(2) pg+rl-rouge (xu et al., 2019) ex-tends pointer-gen with as a reinforcement learningframework which generates sensational headlinesby considering rouge-l score as rewards..5.2 experiment setup.
we perform the following preprocessings.
for eachimpression, we empirically keep at most 50 clickednews to learn user preferences, and set the length ofnews headline and news body to 30 and 500, respec-tively.
word embeddings are 300-dimension andinitialized by the glove (pennington et al., 2014)while the size of position embeddings at sentencelevel is 100. the multi-head attention networkshave 8 heads..first of all, we conduct news recommendationtasks to pretrain a user encoder with a learning rateof 10−4 on the ﬁrst three weeks, i.e., from june14 to july 4, 2019, on the training set, and test onthe rest.
notice that the parameters of the user en-coder are not updated thereafter.
meanwhile, theheadline generator is also pretrained with a learn-ing rate of 0.001 by maximizing the likelihood oforiginal headlines based on a random but ﬁxed userembedding which can be considered as a globaluser without personalized information.
next, wetrain each individual model for 2 epochs follow-ing eq.
10, and adam (kingma and ba, 2014) isused for model optimization where we sample 16sequences for monte carlo search..5.3 evaluation metrics.
for news recommendation evaluation, we re-port the average results in terms of auc, mrr,ndcg@5 and ndcg@10. for personalized head-line generation, we evaluate the generation qualityusing f1 rouge (lin, 2004) 4 including unigram.
4werameters.
compute“-a.
all rouge.
scores with.
-c 95 -m -n 4 -w 1.2.” refer.
pa-to.
87table 2: the overall performance of compared methods.
“r-1, -2, -l” indicate f scores of rouge-1, -2, and -l,and “na” denotes “not available”.
“im” means injection methods, c.f.
1 , 2 , and 3 in fig.
3 for details..methods.
auc mrr ndcg@5 ndcg@10.im rouge-1 rouge-2 rouge-l.metrics.
pointer-genpg+rl-rouge.
nana.
nana.
nana.
nana.
nana.
ebnr.
63.97.
22.52.
26.45.
32.81.dkn.
65.25.
24.07.
26.97.
34.24.npa.
64.91.
23.65.
26.72.
33.96.nrms.
64.27.
23.28.
26.60.
33.58.lstur.
62.49.
22.69.
24.71.
32.28.naml.
66.18.
25.51.
27.56.
35.17.
19.8620.56.
25.1325.4924.6225.9727.4825.0225.4926.1126.3524.9226.1525.4123.7124.1023.1127.4928.0127.25.
123123123123123123.
7.768.42.
9.039.148.959.2310.078.989.149.589.719.019.379.128.738.828.4210.1410.7210.01.
18.8320.03.
20.7320.8220.4020.9221.8120.3420.8221.4021.8220.7521.0320.9121.1320.7320.3821.6222.2421.40.and bigram overlap (rouge-1 and rouge-2) toassess informativeness, and the longest commonsubsequence (rouge-l) to measure ﬂuency.
herewe adopt rouge because we care more aboutevaluating the recall of the generated results.
allthe reported values are the averaged results of 10independently repeated runs..5.4 experimental results.
since we include six kinds of user modeling meth-ods from personalized news recommendations andpropose three ways of injecting user interests inour framework, we can derive 18 variants of ap-proaches that can generate personalized news head-lines.
meanwhile, there are two headline genera-tion baselines, hence we totally have 20 methodsfor evaluation.
the overall performance is illus-trated in table 2, and we have the following obser-vations..first, we can see that every personalized newsheadline generation method can outperform non-personalized methods like pg.
it might be thatour proposed framework can generate personal-ized news headlines by incorporating user inter-ests.
such personalized headlines are more similarto the manually-written ones, which are taken asgold-standard in our evaluation.
second, we ﬁnd.
https://pypi.python.org/pypi/pyrouge/0.1.3.
that user modeling makes a difference in generat-ing personalized headlines.
for instance, namlachieves the best performance in news recommen-dation by learning news and user representationsfrom multiple views, i.e., obtaining 66.18, 25.51,27.56 and 35.17 on auc, mrr ndcg@5 andndcg@10. then injecting the user preferenceslearned by naml to the proposed headline genera-tor also gets the highest rouge scores with eitherway of the incorporation.
we conjecture it is be-cause better user modeling methods can learn morerich personalized information from click behaviors,and well-learned user embeddings could strive togenerate better-personalized headlines.
third, itis reported that the second way of injecting userinterests gets the best performance on most of theuser modeling methods, e.g., ebnr, dkn andnaml.
it is probably because the differentiationof the attention distribution is intensiﬁed after theuser embedding perturbation, which then impactsthe word generation in the decoding process.
how-ever, it still remains a large room for explorationson better injecting user representations into the gen-eration process since the second way seems to bedefective at some time..88table 3: a case study on personalized headline generation for two different users by personalized (naml+hg)and non-personalized (pointer-gen).
underlined words and colored words represent the correlated words in themanually-written headlines, clicked news, and the generated headlines, respectively..case 1. original headline: venezuelans rush to peru before new requirements take effectpointer-gen:user a written headline:naml+hg for user a:clicked news of user a:.
venezuelans rush to perunew requirements set to take effect causes venezuelans to rush to peruperu has stricter entry requirements for escaping venezuelans on that inﬂux.
1. peru and venezuela fans react after match ends in a draw2. uruguay v. peru, copa america and gold cup, game threads and how to watchvenezuelan migrants to peru face danger and discriminationstricter entry requirements on venezuelan migrants and refugees.
1. countries accepting the most refugees (and where they’re coming from)2. venezuelan mothers, children in tow, rush to migrate.
user b written headline:naml+hg for user b:clicked news of user b:.
5.5 case study.
to further comprehend our task and the pro-posed framework, we demonstrate interesting casesfrom two representative methods, namely one non-personalized method pointer-gen (pg) and onepersonalized method naml+hg which utilizesthe second user interests injection (c.f.
fig.
3).
wealso exhibit the manually-written headlines by theusers and the original news headline as references.
from the results shown in table 3, we can ob-serve that generated headline by non-personalizedmethod might omit some detailed but importantinformation.
we believe the reason is that pg istrained via supervised learning to maximize the log-likelihood of ground-truth news headlines.
whileour framework is trained via rl technique wherecoverage score is considered as an indicator to en-courage the generation to be more complete.
inaddition, the exhibited cases show that our frame-work can produce user-speciﬁc news headlines inaccordance with their individual interests reﬂectedby historical click behaviors.
meanwhile, some keyphrases in the personalized-written titles success-fully appeared in the machine-generated headlines..6 related work.
headline generation has been considered as spe-cialized text summarization (luo et al., 2019; jiaet al., 2020), from which both extractive (dorret al., 2003; alfonseca et al., 2013) and abstrac-tive summarization (sun et al., 2015; takase et al.,2016; tan et al., 2017; gavrilov et al., 2019; seeet al., 2017) approaches prevailed for decades.
extractive methods select a subset of actual sen-tences in original article, which may derive inco-herent summary (alfonseca et al., 2013).
whileabstractive models, basically falling in an encoder-decoder (shen et al., 2017a; murao et al., 2019).
framework, can generate more condensed outputbased on the latent representation of news content.
however, the nature of text summarization methodswithout considering interactions between news andusers renders them ineffective in our personalizedheadline generation..recently, stylized headlines generation were pro-posed to output eye-catching headlines by implicitstyle transfer (shen et al., 2017b; fu et al., 2018;prabhumoye et al., 2018) or style-oriented super-visions (shu et al., 2018; zhang et al., 2018; xuet al., 2019).
however, either training a uniﬁed textstyle transfer model or constructing a personalizedtext style transfer model for every user is infeasi-ble due to the complex personalized style-relatedpatterns and the limited personalized-oriented ex-amples.
meanwhile, these methods might sufferfrom the risk of entering into click-bait territory..personalized news recommendation is alsorelated to our problem.
among them, content-based recommendations (okura et al., 2017; liuet al., 2010; li et al., 2011; lian et al., 2018; wanget al., 2018; wu et al., 2019a,b) perform user andnews matching on a learned hidden space, and userrepresentation is learned based on historical clickednews contents.
it inspires us to personalize head-line generator by incorporating user embeddings.
deep models (lian et al., 2018; wang et al., 2018;wu et al., 2019b,a), recently, demonstrated signif-icant improvements because of their capabilitiesin representation learning on both user-side andnews-side data.
different from the efforts on per-sonalized news recommendation, our work focuseson generating fascinating headlines for differentusers, which is orthogonal to existing work..7 conclusion and future work.
in this paper, we formulated the problem of per-sonalized news headline generation.
to provide an.
89ofﬂine testbed for this problem, we constructed adataset named pens from microsoft news.
thenews corpus of this dataset contains more than 100thousand news articles over 15 topic categories.
the training set constitutes of 500, 000 impressionsof 445, 765 users to learn user interests and con-struct personalized news headline generator by dis-tant supervisions.
the test set was constructed by103 annotators with their clicked behaviors andmanually-written personalized news headlines.
wepropose a generic framework that injects user inter-ests into an encoder-decoder headline generator inthree different manners to resolve our problem.
wecompared both sota user modeling and headlinegenerating approaches to present benchmark scoreson the proposed dataset..for future work, we ﬁrst believe designing morecomplex and reﬁned approaches to generated morediversiﬁed personalized news headlines will be in-teresting.
more importantly, how to improve per-sonalization while keeping factualness will be an-other interesting work, and it will propel the meth-ods deployable in practical scenarios.
third, newsheadline personalization might burgeon the newscontent personalization, which is a more challeng-ing but interesting open problem..acknowledgments.
the research work is supported by the nationalkey research and development program of chinaunder grant no.
2017yfb1002104, the nationalnatural science foundation of china under grantno.
92046003, 61976204, u1811461.
xiang aois also supported by the project of youth innova-tion promotion association cas and beijing novaprogram z201100006820062..references.
enrique alfonseca, daniele pighin, and guillermo gar-rido.
2013. heady: news headline abstractionin proceedingsthrough event pattern clustering.
of the 51st annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1243–1253, soﬁa, bulgaria.
association forcomputational linguistics..mingxiao an, fangzhao wu, chuhan wu, kun zhang,zheng liu, and xing xie.
2019. neural news rec-ommendation with long- and short-term user repre-sentations.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 336–345, florence, italy.
association forcomputational linguistics..bonnie dorr, david zajic, and richard schwartz.
2003.hedge trimmer: a parse-and-trim approach to head-line generation.
in proceedings of the hlt-naacl03 on text summarization workshop - volume 5,hlt-naacl-duc ’03, page 1–8, usa.
associa-tion for computational linguistics..zhenxin fu, xiaoye tan, nanyun peng, dongyan zhao,and rui yan.
2018. style transfer in text: explo-ration and evaluation.
in aaai, pages 663–670..daniil gavrilov, pavel kalaidin, and valentin malykh.
2019. self-attentive model for headline generation.
in ecir, pages 87–93..xiaotao gu, yuning mao, jiawei han, jialu liu, youwu, cong yu, daniel finnie, hongkun yu, jiaqizhai, and nicholas zukoski.
2020. generating rep-in www,resentative headlines for news stories.
pages 1773–1784..ruipeng jia, yanan cao, hengzhu tang, fang fang,cong cao, and shi wang.
2020. neural extractivesummarization with hierarchical attentive heteroge-in proceedings of the 2020neous graph network.
conference on empirical methods in natural lan-guage processing (emnlp), pages 3622–3631, on-line.
association for computational linguistics..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..paul larocque.
2003. heads you win: an easy guideto better headline and caption writing.
marionstreet press, inc..lei li, dingding wang, tao li, daniel knox, and bal-aji padmanabhan.
2011. scene: a scalable two-stagein si-personalized news recommendation system.
gir, pages 125–134..lihong li, wei chu, john langford, and robert eschapire.
2010. a contextual-bandit approach topersonalized news article recommendation.
inwww, pages 661–670..jianxun lian, fuzheng zhang, xing xie,.
andguangzhong sun.
2018. towards better represen-tation learning for personalized news recommenda-in ij-tion: a multi-channel deep fusion approach.
cai, pages 3805–3811..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..jiahui liu, peter dolan, and elin rønby pedersen.
2010. personalized news recommendation based onclick behavior.
in iui, pages 31–40..konstantin lopyrev.
2015. generating news head-lines with recurrent neural networks.
arxiv preprintarxiv:1512.01712..90ling luo, xiang ao, yan song, feiyang pan, minyang, and qing he.
2019. reading like her: hu-man reading inspired extractive summarization.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3033–3043, hong kong, china.
association for computa-tional linguistics..kazuma murao, ken kobayashi, hayato kobayashi,taichi yatsuka, takeshi masuyama, tatsuru hig-urashi, and yoshimune tabuchi.
2019. a case studyon neural headline generation for editing support.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 2 (industry papers), pages 73–82, minneapo-lis, minnesota.
association for computational lin-guistics..shumpei okura, yukihiro tagami, shingo ono, andakira tajima.
2017. embedding-based news rec-ommendation for millions of users.
in kdd, pages1933–1942..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..shrimai prabhumoye, yulia tsvetkov, ruslan salakhut-style transferdinov, and alan w black.
2018.in proceedings of thethrough back-translation.
56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 866–876, melbourne, australia.
associationfor computational linguistics..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..shi-qi shen, yan-kai lin, cun-chao tu, yu zhao, zhi-yuan liu, mao-song sun, et al.
2017a.
recent ad-vances on neural headline generation.
journal ofcomputer science and technology, 32(4):768–784..tianxiao shen, tao lei, regina barzilay, and tommijaakkola.
2017b.
style transfer from non-paralleltext by cross-alignment.
in nips, pages 6830–6841..kai shu, suhang wang, thai le, dongwon lee, andhuan liu.
2018. deep headline generation for click-bait detection.
in icdm, pages 467–476..yun-zhu song, hong-han shuai, sung-lin yeh, yi-lun wu, lun-wei ku, and wen-chih peng.
2020.attractive or faithful?
popularity-reinforced learn-ing for inspired headline generation.
in proceedings.
of the aaai conference on artiﬁcial intelligence,volume 34, pages 8910–8917..rui sun, yue zhang, meishan zhang, and donghongji.
2015. event-driven headline generation.
in pro-ceedings of the 53rd annual meeting of the associa-tion for computational linguistics and the 7th inter-national joint conference on natural language pro-cessing (volume 1: long papers), pages 462–472,beijing, china.
association for computational lin-guistics..sho takase, jun suzuki, naoaki okazaki, tsutomuhirao, and masaaki nagata.
2016. neural head-line generation on abstract meaning representation.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages1054–1059, austin, texas.
association for compu-tational linguistics..jiwei tan, xiaojun wan, and jianguo xiao.
2017.from neural sentence summarization to headlinein ijcai,generation: a coarse-to-ﬁne approach.
pages 4109–4115..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..michael w wagner and mike gruszczynski.
2016.when framing matters: how partisan and journalis-tic frames affect individual opinions and party iden-journalism & communication mono-tiﬁcation.
graphs, 18(1):5–48..hongwei wang, fuzheng zhang, xing xie, and minyiguo.
2018. dkn: deep knowledge-aware networkin www, pages 1835–for news recommendation.
1844..chuhan wu, fangzhao wu, mingxiao an, jianqianghuang, yongfeng huang, and xing xie.
2019a.
neural news recommendation with attentive multi-view learning.
in ijcai, pages 3863–3869..chuhan wu, fangzhao wu, mingxiao an, jianqianghuang, yongfeng huang, and xing xie.
2019b.
npa: neural news recommendation with personal-ized attention.
in kdd, pages 2576–2584..chuhan wu, fangzhao wu, suyu ge, tao qi,yongfeng huang, and xing xie.
2019c.
neu-ral news recommendation with multi-head self-attention.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages6389–6394, hong kong, china.
association forcomputational linguistics..fangzhao wu, ying qiao, jiun-hung chen, chuhanwu, tao qi, jianxun lian, danyang liu, xing xie,jianfeng gao, winnie wu, and ming zhou.
2020..91mind: a large-scale dataset for news recommen-in proceedings of the 58th annual meet-dation.
ing of the association for computational linguistics,pages 3597–3606, online.
association for computa-tional linguistics..peng xu, chien-sheng wu, andrea madotto, and pas-cale fung.
2019. clickbait?
sensational headlinegeneration with auto-tuned reinforcement learning.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3065–3075, hong kong, china.
association for computa-tional linguistics..ruqing zhang, jiafeng guo, yixing fan, yanyan lan,jun xu, huanhuan cao, and xueqi cheng.
2018.question headline generation for news articles.
incikm, pages 617–626..guanjie zheng, fuzheng zhang, zihan zheng, yangxiang, nicholas jing yuan, xing xie, and zhen-hui li.
2018. drn: a deep reinforcement learn-ing framework for news recommendation.
in www,pages 167–176..92