do context-aware translation models pay the right attention?.
kayo yin1.
patrick fernandes1,2,3 danish pruthi1 aditi chaudhary1.
andr´e f. t. martins2,3,4 graham neubig11language technologies institute, carnegie mellon university, pittsburgh, pa2instituto superior t´ecnico & lumlis (lisbon ellis unit), lisbon, portugal3instituto de telecomunicac¸ ˜oes, lisbon, portugal4unbabel, lisbon, portugal{kayoy, pfernand, ddanish, aschaudh, gneubig}@cs.cmu.eduandre.t.martins@tecnico.ulisboa.pt.
abstract.
human.
context-aware machine translation models aredesigned to leverage contextual information,but often fail to do so.
as a result, they in-accurately disambiguate pronouns and polyse-mous words that require context for resolu-tion.
in this paper, we ask several questions:what contexts do human translators use to re-solve ambiguous words?
are models payinglarge amounts of attention to the same context?
what if we explicitly train them to do so?
toanswer these questions, we introduce scat(supporting context for ambiguous transla-tions), a new english-french dataset compris-ing supporting context words for 14k trans-lations that professional translators found use-ful for pronoun disambiguation.
using scat,we perform an in-depth analysis of the contextused to disambiguate, examining positionaland lexical characteristics of the supportingwords.
furthermore, we measure the degreeof alignment between the model’s attentionscores and the supporting context from scat,and apply a guided attention strategy to encour-age agreement between the two.1.
1.introduction.
there is a growing consensus in machine trans-lation research that it is necessary to move be-yond sentence-level translation and incorporatedocument-level context (guillou et al., 2018;l¨aubli et al., 2018; toral et al., 2018).
whilevarious methods to incorporate context in neuralmachine translation (nmt) have been proposed(tiedemann and scherrer (2017); miculicich et al.
(2018); maruf and haffari (2018), inter alia), itis unclear whether models rely on the “right” con-text that is actually sufﬁcient to disambiguate dif-ﬁcult translations.
even when additional context.
1our scat data and code for experiments are available at.
https://github.com/neulab/contextual-mt..en.
fr.
en.
fr.
en.
fr.
look after her a lot.
okay.
any questions?
have wegot her report?
yes, it’s in the inﬁrmary alreadydorlotez-la.
d’accord.
vous avez des questions ?
ondispose de son rapport.
oui, il est `a l’inﬁrmerie..context-aware baseline.
look after her a lot.
okay.
any questions?
have wegot her report?
yes, it’s in the inﬁrmary already.
dorlotez-la.
d’accord.
vous avez des questions ?
ondispose de son rapport ?
oui, elle est d´ej`a `a l’inﬁrmerie..model w/ attention regularization.
look after her a lot.
okay.
any questions?
have wegot her report?
yes it’s in the inﬁrmary already.
dorlotez-la.
d’accord.
vous avez des questions ?
ondispose de son rapport ?
oui, il est d´ej`a `a l’hˆopital.
table 1: translation of the ambiguous pronoun “it”.
infrench, if the referent of “it” is masculine (e.g., report)then “il” is used, otherwise “elle”.
the model withregularized attention translates the pronoun correctly,with the largest attention on the referent “report”.
top3 words with the highest attention are highlighted..is provided, models often perform poorly on eval-uation of relatively simple discourse phenomena(m¨uller et al., 2018; bawden et al., 2018; voitaet al., 2019b,a; lopes et al., 2020) and rely onspurious word co-occurences during translation ofpolysemous words (emelin et al., 2020).
some evi-dence suggests that models attend to uninformativetokens (voita et al., 2018) and do not use contextualinformation adequately (kim et al., 2019)..to understand plausibly why current nmt mod-els are unable to fully leverage the disambiguatingcontext they are provided, and how we can developmodels that use context more effectively, we posethe following research questions: (i) in contextaware translation, what context is intrinsically use-ful to disambiguate hard translation phenomenasuch as ambiguous pronouns or word senses?
; (ii)are context-aware mt models paying attention tothe relevant context or not?
; and (iii) if not, can we.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages788–801august1–6,2021.©2021associationforcomputationallinguistics788encourage them to do so?.
to answer the ﬁrst question, we collect anno-tations of context that human translators founduseful in choosing between ambiguous translationoptions (§3).
speciﬁcally, we ask 20 professionaltranslators to choose the correct french translationbetween two contrastive translations of an ambigu-ous word, given an english source sentence andthe previous source- and target-side sentences.
thetranslators additionally highlight the words theyfound the most useful to make their decision, giv-ing an idea of the context useful in making thesedecisions.
we collect 14k such annotations andrelease scat (“supporting context for ambigu-ous translations”), the ﬁrst dataset of human ra-tionales for resolving ambiguity in document-leveltranslation.
analysis reveals that inter-sententialtarget context is important for pronoun translation,whereas intra-sentential source context is often suf-ﬁcient for word sense disambiguation..to answer the second question, we quantify thesimilarity of the attention distribution of context-aware models and the human annotations in scat(§4).
we measure alignment between the base-line context-aware model’s attention and humanrationales across various model attention heads andlayers.
we observe a relatively high alignment be-tween self attention scores from the top encoder lay-ers and the source-side supporting context markedby translators, however, the model’s attention ispoorly aligned with target-side supporting context.
for the third question, we explore a method toregularize attention towards human-annotated dis-ambiguating context (§5).
we ﬁnd that attentionregularization is an effective technique to encour-age models to pay more attention to words humansﬁnd useful to resolve ambiguity in translations.
ourmodels with regularized attention outperform previ-ous context-aware baselines, improving translationquality by 0.54 bleu, and yielding a relative im-provement of 14.7% in contrastive evaluation.
anexample of translations from a baseline and ourmodel, along with the supporting rationale by aprofessional translator is illustrated in table 1..2 document-level translation.
neural machine translation.
current nmtmodels employ encoder-decoder architectures(bahdanau et al., 2015; vaswani et al., 2017).
first, the encoder maps a source sequence x =(x1, x2, ..., xs) to a continuous representation z =.
(z1, z2, ..., zs).
then, given z, the decoder gen-erates the corresponding target sequence y =(y1, y2, ..., yt ), one token at a time.
sentence-levelnmt models take one source sentence and gener-ate one target sentence at a time.
these models per-form reasonably well, but given that they only haveintra-sentential context, they fail to handle somephenomena that require inter-sentential contextto accurately translate.
well-known examples ofthese phenomena include gender-marked anaphoricpronouns (guillou et al., 2018) and maintenance oflexical coherence (l¨aubli et al., 2018)..document-level translation.
document-leveltranslation models learn to maximize the proba-bility of a target document y given the sourcedocument x: pθ(y |x) = (cid:81)jj=1 pθ(yj|xj, cj),where yj and xj are the j-th target and sourcesentences, and cj is the collection of contextualsentences for the j-th sentence pair.
there are manymethods for incorporating context (§6), but evensimple concatenation (tiedemann and scherrer,2017), which prepends the previous source or tar-get sentences to the current sentence separated bya (cid:104)brk(cid:105) tag, achieves comparable performance tomore sophisticated approaches, especially in high-resource scenarios (lopes et al., 2020)..evaluation.
bleu (papineni et al., 2002) ismost widely used to evaluate mt, but it can bepoorly correlated with human evaluation (callison-burch et al., 2006; reiter, 2018).
recently, a num-ber of neural evaluation methods, such as comet(rei et al., 2020), have shown better correlationwith human judgement.
nevertheless, commonautomatic metrics have limited ability to evaluatediscourse in mt (hardmeier, 2012).
as a remedyto this, researchers often use contrastive test setsfor a targeted discourse phenomenon (m¨uller et al.,2018), such as pronoun anaphora resolution andword sense disambiguation, to verify if the modelranks the correct translation of an ambiguous sen-tence higher than the incorrect translation..3 what context do human translators.
pay attention to?.
we ﬁrst conduct a user study to collect supportingcontext that translators use in disambiguation, andanalyze characteristics of the supporting words..789pronoun anaphora resolution.
we annotateexamples from the contrastive test set by lopeset al.
(2020).
this set includes 14k examples fromthe opensubtitles2018 dataset (lison et al., 2018)with occurrences of the english pronouns “it” and“they” that correspond to the french translations“il” or “elle” and “ils” or “elles”, with 3.5k exam-ples for each french pronoun type.
through ourannotation effort, we obtain 14k examples of sup-porting context for pronoun anaphora resolutionin ambiguous translations selected by professionalhuman translators.
statistics on this dataset, scat:supporting context for ambiguous translations,are provided in appendix a..word sense disambiguation.
there are no ex-isting contrastive datasets for wsd with a con-text window larger than 1 sentence, therefore, weautomatically generate contrastive examples withcontext window of 5 sentences from opensubti-tles2018 by identifying polysemous english wordsand possible french translations.
we describe ourmethodology in appendix b..quality.
for quality control, we asked 8 inter-nal speakers of english and french, with native orbilingual proﬁciency in both languages, to carefullyannotate the same 100 examples given to all profes-sional translators.
we compared both the answeraccuracies and the selected words for each hiredtranslator against this control set and discardedsubmissions that either had several incorrect an-swers while the internal bilinguals were able tochoose the correct answer on the same example,or that highlighted contextual words that the in-ternal annotators did not select and that had littlerelevance to the ambiguous word.
furthermore,among the 400 examples given to each annotator,the ﬁrst hundred are identical, allowing us to mea-sure the inter-annotator agreement for both answerand supporting context selection..first, for answer selection on par, we ﬁnd91.0% overall agreement, with fleiss’free-marginal kappa κ = 0.82. for wsd, we ﬁnd85.9% overall agreement with κ = 0.72. this in-dicates a substantial inter-annotator agreement forthe selected answer.
in addition, we measure theinter-annotator agreement for the selected wordsby calculating the f1 between the word selectionsfor each pair of annotators given identical contextsettings.
for par, we obtain an average f1 of 0.52across all possible pairs, and a standard deviation of.
figure 1: the annotation page shown to translators..3.1 recruitment and annotation setup.
we recruited 20 freelance english-french transla-tors on upwork.2 the translators are native speak-ers of at least one of the two languages and havea job success rate of over 90%.
each translatoris given 400 examples with an english source sen-tence and two possible french translations, and oneout of 5 possible context levels: no context (0+0),only the previous source sentence as context (1+0),only the previous target sentence (0+1), the previ-ous source sentence and target sentence (1+1), andthe 5 previous source and target sentences (5+5).
we vary the context level in each example to mea-sure how human translation quality changes..translators provide annotations using the inter-face shown in figure 1. they are ﬁrst asked to se-lect the correct translation out of the two contrastivetranslations, and then highlight word(s) they founduseful to arrive at their answer.
in cases where mul-tiple words are sufﬁcient to disambiguate, transla-tors were asked to mark only the most salient wordsrather than all of them.
further, translators also re-ported their conﬁdence in their answers, choosingfrom “not at all”, “somewhat”, and “very”..3.2 tasks and data quality.
we perform this study for two tasks: pronounanaphora resolution (par), where the translatorsare tasked with choosing the correct french gen-dered pronoun associated to a neutral englishpronoun, and word sense disambiguation (wsd),where the translators pick the correct translationof a polysemous word.
par, and wsd to a lesserextent, have been commonly studied to evaluatecontext-aware nmt models (voita et al., 2018;lopes et al., 2020; m¨uller et al., 2018; huo et al.,2020; nagata and morishita, 2020)..2https://www.upwork.com.
790par.
wsd.
context correct not conﬁdent correct not conﬁdent.
0 + 01 + 00 + 11 + 15 + 5.no antehas ante.
78.490.693.093.695.9.
75.496.0.
27.013.29.26.72.8.
33.83.3.
88.788.787.587.188.7.
––.
7.06.56.76.55.9.
––.
table 2: percentage of correct and zero-conﬁdenceanswers by varying context level.
n+m: n previoussource and m previous target sentences given as con-text.
values in bold are signiﬁcantly different from val-ues above it (p < 0.05)..0.12. for wsd, we ﬁnd an average f1 of 0.46 anda standard deviation of 0.12. there is a high agree-ment between annotators for the selected words aswell..3.3 answer accuracy and conﬁdence.
table 2 shows the accuracy of answers and thepercentage of answers being reported as not at allconﬁdent for each of the 5 different context levels.
for par, there is a large increase in accuracy andconﬁdence when just one previous sentence in ei-ther language is provided as context compared tono context at all.
target-side context also seemsmore useful than source: only target-side contextgives higher answer accuracy than only source-sidecontext, while the accuracy does not increase sig-niﬁcantly by having both previous sentences..for wsd, we do not observe signiﬁcant differ-ences in answer accuracy and conﬁdence betweenthe different context levels (figure 2).the high an-swer accuracy with 0+0 context and the low rate ofzero-conﬁdence answers across all settings suggestthat the necessary disambiguating information isoften present in the intra-sentential context.
alter-natively, this may be partially due to characteristicsof the automatically generated dataset itself: wefound that some examples are misaligned so theprevious sentences given as context do not actu-ally correspond to the context of the current sen-tences, and therefore do not add useful information.
we also observe that translators tend to report ahigh conﬁdence and high agreement in incorrectanswers as well.
this can be explained by thetendency to select the masculine pronoun in par(figure 3) or the prevailing word sense in wsd..to properly translate an anaphoric pronoun, thetranslator must identify its antecedent and deter-.
figure 2: distribution of conﬁdence in answers per con-text level for par (left) and wsd (right)..figure 3: distribution of gender of selected pronounsper conﬁdence level for par..mine its gender, so we hypothesize that the an-tecedent is of high importance for disambiguation.
in our study, 72.4% of the examples shown to an-notators contain the antecedent in the context orcurrent sentences.
we calculate how answer accu-racy and conﬁdence vary between examples thatdo or do not contain the pronoun antecedent.
weﬁnd that the presence of the antecedent in the con-text leads to larger variations in answer accuracythan the level of context given, demonstrating theimportance of antecedents for resolution..3.4 analysis of the highlighted words.
next, we examine the words that were selected asrationales from several angles..distance.
figure 4 shows, for each context level,the number of highlighted words at a given distance(in sentences) from the ambiguous word.
for par,when no previous sentences are provided, there areas many selected words from the source as the tar-get context.
with inter-sentential context, expertsselected more supporting context from the targetside.
one possible reason is that the source and tar-get sentences on their own are equally descriptiveto perform par, but one may look for the corefer-ence chain of the anaphoric pronoun in the targetcontext to determine its gender, whereas the samecoreference chain in the source context would notnecessarily contain gender information.
moreover,the antecedent in the target side is more reliable.
7910+01+00+11+15+5020406080100% of answerspar0+01+00+11+15+5wsdverysomewhatnot at allcontext level2952405638985949813631917263398.par.
385315772725848.
937304204367109185226481924400.wsd.
–4075014593.
4277496323773189116351776417546261.
–90475912997.pos.
source target total source target total.
par.
wsd.
nounproper nounpronounverbdeterminerauxiliaryadjectiveadpositionconjunctionnumeralparticle.
1550136267624749931010565713737.
45024192306511069974464242371347645.
334019211940680078291283832261.
(a) part-of-speech.
dep.
source target total source target total.
antecedentdeterminermodiﬁerconjunctcase marking.
1297304168227.
515018814408055.
–497258844.
(b) dependency relation.
table 3: most frequent part-of-speech and dependencyrelation of highlighted words..par listen, these big celebrities, they do it different than anybody else?
jesus, you know if they knew you had hidden cameras in thatbedroom....detces.
ante nounvedettes.
pronelles.
dis-moi,le font diff´eremment desautres?
bon dieu, tu te rends compte que si elles/ils savaient quecette chambre cache des cam´eras....,.
wsd right this way.
your charm is only exceeded.
.
by your franknesssuivez-moi.
ton charme/portebonheur n’a d’´egal que ta franchise..verb.
noun.
table 4: examples of supporting context..information about the ambiguous word..the main difference between par and wsd isthat for par, the key supporting information isgender.
the source side does not contain explicitinformation about the gender of the ambiguous pro-noun whereas the target side may contain othergendered pronouns and determiners referring to theambiguous pronoun.
for wsd however, the keysupporting information is word sense.
while thesource and target sides contain around the sameamount of semantic information, humans may pre-fer to attend to source sentences that express howthe ambiguous word is used in the sentence..4 do models pay the right attention?.
next, we study nmt models and quantify the de-gree to which the model’s attention is aligned withthe supporting context from professional transla-tors..figure 4: sentence distance of the highlighted wordsfor each context level for par and wsd..than the source antecedent, since the antecedentcan have multiple possible translations with differ-ent genders.
for wsd, we ﬁnd that inter-sententialcontext is seldom highlighted, which reinforces ourprevious claim that most supporting context forwsd can be found in the current sentences..part-of-speech and dependency.
we usespacy (honnibal and montani, 2017) to predictpart-of-speech (pos) tags of selected words andsyntactic dependencies between selected wordsand the ambiguous word.
in table 3a, we ﬁndthat nominals are the most useful for par, whichsuggests that human translators look for otherreferents of the ambiguous pronoun to determineits gender.
this is reinforced by table 3b, wherethe antecedent of the pronoun is selected the mostoften..for wsd, proper nouns and pronouns are notas important as nouns, probably because they donot carry as much semantic load that indicates thesense of the ambiguous word.
determiners, verbsand adpositions are relatively important since theyoffer clues on the syntactic dependencies of theambiguous word on other words as well as its rolein the sentence, and modiﬁers provide additional.
7920+00+11+01+15+51+00+10+01+00+11+15+5metric.
attnreg-randenc self dec cross dec self enc self dec cross dec self enc self dec cross dec self.
attnreg-pre.
baseline.
dot product (↑, uniform=0.04)kl divergence (↓, uniform=3.6)probes needed (↓, uniform=12.6).
0.613.28.5.
0.133.611.3.
0.103.613.5.
0.503.27.6.
0.283.29.0.
0.203.55.8.
0.533.26.9.
0.633.26.0.
0.313.510.0.table 5: alignment between model attention and scat..4.1 model.
we incorporate the 5 previous source and target sen-tences as context to the base transformer (vaswaniet al., 2017) by prepending the previous sentencesto the current sentence, separated by a (cid:104)brk(cid:105) tag,as proposed by tiedemann and scherrer (2017)..4.2 similarity metrics.
to calculate similarity between model attentionand highlighted context, we ﬁrst construct a hu-man attention vector αhuman, where 1 correspondsto tokens marked by the human annotators, and0 otherwise.
we compare this vector against themodel’s attention for the ambiguous pronoun for agiven layer and head, αmodel, across three metrics:.
dot product.
the dot product αhuman · αmodelmeasures the total attention mass the model assignsto words highlighted by humans..kl divergence.
we compute the kl divergencebetween the model attention and the normalized hu-man attention vector kl(αhuman-norm||αmodel(θ)),where the normalized distribution αhuman-norm isuniform over all tokens selected by humans and avery small constant (cid:15) elsewhere such that the sumof values in αhuman-norm is equal to 1..probes needed.
we adapt the “probes needed”metric by zhong et al.
(2019) to measure the num-ber of tokens we need to probe, based on the modelattention, to ﬁnd a token highlighted by humans.
this corresponds to the ranking of the ﬁrst high-lighted token after sorting all tokens by descendingmodel attention.
the intuition is that the more at-tention the model assigns to supporting context, thefewer probes are needed to ﬁnd a supporting token..4.3 results.
we compute the similarity between the model at-tention distribution for the ambiguous pronoun andthe supporting context from 1,000 scat samples.
in table 5, for each attention type we report thebest score across layers and attention heads.
wealso report the alignment score between a uniform.
distribution and supporting context for compari-son.
we ﬁnd that although there is a reasonablyhigh alignment between encoder self attention andscat, decoder attentions have very low alignmentwith scat..5 making models pay the right.
attention.
5.1 attention regularization.
we hypothesize that by encouraging models to in-crease attention on words that humans use to re-solve ambiguity, translation quality may improve.
we apply attention regularization to guide modelattention to increase alignment with the support-ing context from scat.
to do so, we append thetranslation loss with an attention regularizationloss between the normalized human attention vec-tor αhuman-norm and the model attention vector forthe corresponding ambiguous pronoun αmodel:.
r(θ) = −λkl(αhuman-norm||αmodel(θ)).
where λ is a scalar weight parameter for the loss.
during training, we randomly sample batchesfrom scat with p = 0.2. we train with the stan-dard mt objective on the full dataset, and on ex-amples from scat, we additionally compute theattention regularization loss..5.2 data.
for document translation, we use the english andfrench data from opensubtitles2018 (lison et al.,2018), which we clean then split into 16m training,10,036 development, and 9,740 testing samples.
for attention regularization, we retain examplesfrom scat where 5+5 context was given to theannotator.
we use 11,471 examples for trainingand 1,000 for testing..5.3 models.
we ﬁrst train a baseline model, where the 5 previ-ous source and target sentences serve as context andare incorporated via concatenation.
this baselinemodel is trained without attention regularization..793we explore two models with attention regulariza-tion: (1) attnreg-rand, where we jointly train onthe mt objective and regularize attention on a ran-domly initialized model; (2) attnreg-pre, where weﬁrst pre-train the model solely on the mt objec-tive, then we jointly train on the mt objective andregularize attention.
we describe the full setup inappendix c..5.4 evaluation.
as described in section 2, we evaluate translationoutputs with bleu and comet.
in addition, toevaluate the direct translation quality of speciﬁcphenomena, we translate the 4,015 examples fromlopes et al.
(2020) containing ambiguous pronounsthat were not used for attention regularization, andwe compute the mean word f-measure of transla-tions of the ambiguous pronouns and other words,with respect to reference texts..we also perform contrastive evaluation on thesame subset of lopes et al.
(2020) with a contextwindow of 5 sentences (big-par) and the con-trastive test sets by bawden et al.
(2018), which in-clude 200 examples on anaphoric pronoun transla-tion and 200 examples on lexical consistency/wordsense disambiguation.
the latter test sets werecrafted manually, have a context window of 1 sen-tence, and either the previous source or target sen-tence is necessary to disambiguate..context-aware models often suffer from errorpropagation when using previously decoded out-put tokens as the target context (li et al., 2020a).
therefore, during inference, we experiment withboth using the gold target context (gold) as wellas using previous output tokens (non-gold)..5.5 overall performance.
before delving into the main results, we note thatwe explored regularizing different attention vec-tors in the model (appendix c.3) and obtain thebest bleu and comet scores for attnreg-randwhen regularizing the self-attention of the top en-coder layer, cross-attention of the top decoder layerand self-attention of the bottom decoder layer.
forattnreg-pre, regularizing self-attention in the topdecoder layer gives the best scores.
thus, we usethese as the default regularization methods below.
moving on to the main results in table 6, weobserve that attnreg-rand improves on all metrics,which demonstrates that attention regularization isan effective method to improve translation qual-ity.
although attnreg-pre does not improve gen-.
eral translation scores signiﬁcantly, it yields con-siderable gains in word f-measure on ambiguouspronouns and achieves some improvement overthe baseline on contrastive evaluation on big-parand par.
attention regularization with support-ing context for par seems to especially improvemodels on similar tasks.
the disparity betweenbleu/comet scores and targeted evaluationssuch as word f-measure and contrastive evalua-tion further suggests that general mt metrics aresomewhat insensitive to improvements on speciﬁcdiscourse phenomena.
for both models with atten-tion regularization, there are no signiﬁcant gains inwsd.
as discussed in §3.4, wsd and par requiredifferent types of supporting context, so it is naturalthat regularizing attention using supporting contextextracted from only one task does not always leadto improvement on the other..5.6 analysis.
we now investigate how models trained with at-tention regularization handle context differentlycompared to the baseline model..how does attention regularization inﬂuencealignment with human rationales?
we revisitthe similarity metrics from §4.2 to measure align-ment with scat.
in table 5, the dot product align-ment over attention in the decoder increases withattention regularization, suggesting that attentionregularization guides different parts of the modelto pay attention to useful context.
interestingly, al-though only the encoder self-attention was explic-itly regularized for attnreg-pre, the model seems toalso have learned better alignment for attention inthe decoder.
moreover, attnreg-pre generally hasbetter alignment than attnreg-rand, suggesting thatmodels respond more to attention regularizationonce it has been trained to perform translation..which attention is the most useful?
for eachof attnreg-rand and attnreg-pre, we perform at-tention regularization on either the encoder self-attention, decoder cross-attention or decoder self-attention only.
in table 7, encoder self-attentionseems to contribute the most to both translationperformance and contrastive evaluation.
althoughattnreg-rand models achieve higher bleu andcomet scores, attnreg-pre obtain higher scoreson metrics targeted to pronoun translation.
atten-tion regularization seems to have limited effect onwsd performance, the scores vary little betweenattention types..794model.
gold.
contrastive evaluationbleu comet bleu comet pronouns other big-par par wsd.
f-measure.
non-gold.
baselineattnreg-randattnreg-pre.
33.533.932.3.
41.742.336.9.
29.030.229.3.
37.139.334.6.
0.360.420.48.
0.440.440.44.
90.091.190.6.
60.069.062.5.
55.056.052.5.table 6: overall results.
scores signiﬁcantly better than baseline with p < 0.05 are bolded.
comet scores aremultiplied by 100..attention.
b.c.f-meas.
contrastive evaluationpron.
other big-par par wsd.
arenc selfar dec crossdec selfarapenc selfap dec crossdec selfap.
33.833.833.132.332.332.3.
42.541.741.637.437.436.9.
0.470.470.410.480.470.48.
0.440.430.450.440.430.44.
91.389.888.991.592.190.6.
59.565.066.066.563.062.5.
57.055.553.552.555.052.5.table 7: performance of models with various regular-ized attention.
ar: attnreg-rand, ap: attnreg-pre, b:bleu, c:comet.
fect.
another interesting ﬁnding is that both base-line and attnreg-rand seem to rely more on thesource context than the target context, in contrastto human translators.
this result corroborates priorresults where models have better alignment withsupporting context on attention that attends to thesource (encoder self-attention and decoder cross-attention), and regularizing these attention vectorscontributes more to translation quality than regular-izing the decoder self-attention..mask.
baseline.
attnreg-rand.
attnreg-pre.
6 related work.
no masksupportingrandomsourcetargetall.
82.575.176.065.570.665.3.
84.469.477.067.375.367.1.
86.755.080.473.067.668.7.table 8: contrastive performance with various maskson the context.
the lowest score for each model is un-derlined..how much do models rely on supporting con-text?
we compare model performance on con-trastive evaluation on scat when it is given fullcontext, and when we mask either the supportingcontext, random context words with p = 0.1, thesource context, the target context, or all of the con-text.
in table 8, we ﬁnd that baseline varies lit-tle when the supporting context is masked, whichagain suggests that context-aware baselines do notuse the relevant context, although they do observea drop in contrastive performance when the sourceand all context are masked.
models with atten-tion regularization, especially attnreg-pre observea large drop in contrastive performance when sup-porting context is masked, which indicates thatthey learned to rely more on supporting context.
furthermore, for attnreg-pre, the score after mask-ing supporting context is signiﬁcantly lower thanwhen masking all context, which may indicate thathaving irrelevant context can have an adverse ef-.
6.1 context-aware machine translation.
most current context-aware nmt approaches en-hance nmt by including source- and/or target-side surrounding sentences as context to the model.
tiedemann and scherrer (2017) concatenate theprevious sentences to the input; jean et al.
(2017);bawden et al.
(2018); zhang et al.
(2018) use anadditional encoder to extract contextual features;wang et al.
(2017) use a hierarchical rnn to en-code the global context from all previous sentences;maruf and haffari (2018); tu et al.
(2018) usecache-based memories to encode context; miculi-cich et al.
(2018); maruf et al.
(2019) use hierar-chical attention networks; chen et al.
(2020) adddocument-level discourse structure information tothe input.
while maruf et al.
(2019); voita et al.
(2018) also ﬁnd higher attention mass attributed torelevant tokens in selected examples, our work isthe ﬁrst to guide model attention in context-awarenmt using human supervision and analyze its at-tention distribution in a quantitative manner..however, recent studies suggest that currentcontext-aware nmt models often do not use con-text meaningfully.
kim et al.
(2019) claim thatimprovements by context-aware models are mostlyfrom regularization by reserving parameters forcontext inputs, and li et al.
(2020b) show that re-placing the context in multi-encoder models withrandom signals leads to similar accuracy as usingthe actual context.
our work addresses the above.
795disparities by collecting human supporting contextto regularize model attention heads during training..6.2 attention mechanisms.
though attention is usually learned in an unsuper-vised manner, recent work supervises attention withword alignments (mi et al., 2016; liu et al., 2016),event arguments and trigger words (liu et al., 2017;zhao et al., 2018), syntactic dependencies (strubellet al., 2018) or word lexicons (zou et al., 2018).
our work is closely related to a large body of workthat supervises attention using human rationales fortext classiﬁcation (barrett et al., 2018; bao et al.,2018; zhong et al., 2019; choi et al., 2020; pruthiet al., 2020).
our work, however, is the ﬁrst to col-lect human evidence for document translation anduse it to regularize the attention of nmt models..7.implications and future work.
in this work, we collected a corpus of support-ing context for translating ambiguous words.
weexamined how baseline context-aware translationmodels use context, and demonstrated how contextannotations can improve context-aware translationaccuracy.
while we obtain promising results forcontext-aware translation by testing one methodfor attention regularization, our publicly availablescat dataset could enable future research on alter-native attention regularizers.
moreover, our analy-ses demonstrate that humans rely on different typesof context for par and wsd in english-frenchtranslation, similar user studies can be conducted tobetter understand the usage of context in other am-biguous discourse phenomena, such as ellipsis, orother language pairs.
we also ﬁnd that regularizingattention using scat for par especially improvesanaphoric pronoun translation, suggesting that su-pervising attention using supporting context fromdifferent tasks may help models resolve other typesof ambiguities..one caveat regarding our method for collectingsupporting context from humans is the differencebetween translation, translating text from the input,and disambiguation, choosing between translationcandidates.
during translation, humans might paymore attention to the source sentences to under-stand the source material, but during disambigua-tion, we have shown that human translators relymore often on the target sentences.
one reason whythe model beneﬁts more from increased attentionon source may be because the model is trained and.
evaluated to perform translation, not disambigua-tion.
a future step would be to explore alternativemethods for extracting supporting context, such aseye-tracking during translation (o’brien, 2009)..8 acknowledgements.
we would like to thank emma landry, guillaumedidier, wilson jallet, baptiste moreau-pernet,pierre gianferrara, and duy-anh alexandre forhelping with a preliminary english-french trans-lation study.
we would also like to thank niko-lai vogler for the original interface for data an-notation, and the anonymous reviewers for theirhelpful feedback.
this work was supported bythe european research council (erc stg deep-spin 758969), by the p2020 programs maia andunbabel4eu (lisboa-01-0247-feder-045909and lisboa-01-0247-feder-042671), and bythe fundac¸ ˜ao para a ciˆencia e tecnologia throughcontract uidb/50008/2020..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..yujia bao, shiyu chang, mo yu, and regina barzilay.
2018. deriving machine attention from human ra-tionales.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 1903–1913, brussels, belgium.
associationfor computational linguistics..maria barrett, joachim bingel, nora hollenstein,marek rei, and anders søgaard.
2018. sequenceclassiﬁcation with human attention.
in proceedingsof the 22nd conference on computational naturallanguage learning, pages 302–312, brussels, bel-gium.
association for computational linguistics..rachel bawden, rico sennrich, alexandra birch, andbarry haddow.
2018. evaluating discourse phenom-ena in neural machine translation.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 1304–1313, new orleans, louisiana.
association for computational linguistics..chris callison-burch, miles osborne, and philippkoehn.
2006. re-evaluating the role of bleu in ma-in 11th conference ofchine translation research.
the european chapter of the association for com-putational linguistics, trento, italy.
association forcomputational linguistics..796junxuan chen, xiang li, jiarui zhang, chulun zhou,jianwei cui, bin wang, and jinsong su.
2020. mod-eling discourse structure for document-level neuralin proceedings of the firstmachine translation.
workshop on automatic simultaneous translation,pages 30–36, seattle, washington.
association forcomputational linguistics..seungtaek choi, haeju park, jinyoung yeo, and seung-won hwang.
2020. less is more: attention supervi-sion with counterfactuals for text classiﬁcation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 6695–6704, online.
association for computa-tional linguistics..zi-yi dou and graham neubig.
2021. word alignmentby ﬁne-tuning embeddings on parallel corpora.
con-ference of the european chapter of the associationfor computational linguistics (eacl)..denis emelin, ivan titov, and rico sennrich.
2020.detecting word sense disambiguation biases in ma-chine translation for model-agnostic adversarial at-tacks.
arxiv preprint arxiv:2011.01846..liane guillou, christian hardmeier, ekaterinalapshinova-koltunski, and sharid lo´aiciga.
2018.a pronoun test suite evaluation of the english–german mt systems at wmt 2018. in proceedingsof the third conference on machine translation:shared task papers, pages 570–577, belgium, brus-sels.
association for computational linguistics..christian hardmeier.
2012. discourse in statistical ma-chine translation.
a survey and a case study.
dis-cours.
revue de linguistique, psycholinguistique etinformatique.
a journal of linguistics, psycholinguis-tics and computational linguistics..matthew honnibal and ines montani.
2017. spacy 2:natural language understanding with bloom embed-dings, convolutional neural networks and incremen-tal parsing.
to appear..jingjing huo, christian herold, yingbo gao, leonarddahlmann, shahram khadivi, and hermann ney.
2020. diving deep into context-aware neural ma-chine translation.
in proceedings of the fifth confer-ence on machine translation, pages 604–616, on-line.
association for computational linguistics..sebastien jean, stanislas lauly, orhan firat, andkyunghyun cho.
2017. does neural machine trans-lation beneﬁt from larger context?
arxiv preprintarxiv:1704.05135..yunsu kim, duc thanh tran, and hermann ney.
2019.when and why is document-level context useful inneural machine translation?
in proceedings of thefourth workshop on discourse in machine trans-lation (discomt 2019), pages 24–34, hong kong,china.
association for computational linguistics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..samuel l¨aubli, rico sennrich, and martin volk.
2018.has machine translation achieved human parity?
ain proceed-case for document-level evaluation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 4791–4796,brussels, belgium.
association for computationallinguistics..bei li, hui liu, ziyang wang, yufan jiang, tong xiao,jingbo zhu, tongran liu, and changliang li.
2020a.
does multi-encoder help?
a case study on context-aware neural machine translation.
arxiv preprintarxiv:2005.03393..bei li, hui liu, ziyang wang, yufan jiang, tong xiao,jingbo zhu, tongran liu, and changliang li.
2020b.
does multi-encoder help?
a case study on context-in proceedingsaware neural machine translation.
of the 58th annual meeting of the association forcomputational linguistics, pages 3512–3518, on-line.
association for computational linguistics..pierre lison, j¨org tiedemann, and milen kouylekov.
2018. opensubtitles2018: statistical rescoring ofsentence alignments in large, noisy parallel corpora.
in proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), miyazaki, japan.
european language re-sources association (elra)..lemao liu, masao utiyama, andrew finch, and ei-ichiro sumita.
2016. neural machine translationwith supervised attention.
in proceedings of col-ing 2016, the 26th international conference oncomputational linguistics: technical papers, pages3093–3102, osaka, japan.
the coling 2016 orga-nizing committee..shulin liu, yubo chen, kang liu, and jun zhao.
2017. exploiting argument information to improveevent detection via supervised attention mechanisms.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1789–1798, vancouver,canada.
association for computational linguistics..797ant´onio lopes, m. amin farajian, rachel bawden,michael zhang, and andr´e f. t. martins.
2020.document-level neural mt: a systematic compari-son.
in proceedings of the 22nd annual conferenceof the european association for machine transla-tion, pages 225–234, lisboa, portugal.
european as-sociation for machine translation..sameen maruf and gholamreza haffari.
2018. docu-ment context neural machine translation with mem-in proceedings of the 56th annualory networks.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1275–1284, melbourne, australia.
association for compu-tational linguistics..sameen maruf, andr´e f. t. martins, and gholamrezahaffari.
2019. selective attention for context-awarein proceedings of theneural machine translation.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 3092–3102, minneapolis, min-nesota.
association for computational linguistics..haitao mi, zhiguo wang, and abe ittycheriah.
2016.supervised attentions for neural machine translation.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages2283–2288, austin, texas.
association for compu-tational linguistics..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neu-ral machine translation with hierarchical attentionin proceedings of the 2018 conferencenetworks.
on empirical methods in natural language process-ing, pages 2947–2954, brussels, belgium.
associa-tion for computational linguistics..mathias m¨uller, annette rios, elena voita, and ricosennrich.
2018. a large-scale test set for the eval-uation of context-aware pronoun translation in neu-ral machine translation.
in proceedings of the thirdconference on machine translation: research pa-pers, pages 61–72, brussels, belgium.
associationfor computational linguistics..masaaki nagata and makoto morishita.
2020. a testset for discourse translation from japanese to en-in proceedings of the 12th language re-glish.
sources and evaluation conference, pages 3704–3709, marseille, france.
european language re-sources association..sharon o’brien.
2009. eye tracking in translation pro-cess research: methodological challenges and solu-tions.
methodology, technology and innovation intranslation process research, 38:251–266..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,.
pennsylvania, usa.
association for computationallinguistics..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..danish pruthi, bhuwan dhingra, livio baldini soares,michael collins, zachary c. lipton, graham neu-big, and william w. cohen.
2020. evaluating expla-nations: how much do explanations from the teacheraid students?.
ricardo rei, craig stewart, ana c farinha, and alonlavie.
2020. comet: a neural framework for mtevaluation.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2685–2702, online.
associa-tion for computational linguistics..ehud reiter.
2018. a structured review of the validityof bleu.
computational linguistics, 44(3):393–401..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..emma strubell,patrick verga, daniel andor,and andrew mccallum.
2018.david weiss,linguistically-informed self-attention for semanticin proceedings of the 2018 confer-role labeling.
ence on empirical methods in natural languageprocessing, pages 5027–5038, brussels, belgium.
association for computational linguistics..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the third workshop on discourse in machinetranslation, pages 82–92, copenhagen, denmark.
association for computational linguistics..antonio toral, sheila castilho, ke hu, and andyway.
2018. attaining the unattainable?
reassessingclaims of human parity in neural machine translation.
in proceedings of the third conference on machinetranslation: research papers, pages 113–123, brus-sels, belgium.
association for computational lin-guistics..zhaopeng tu, y. liu, shuming shi, and t. zhang.
2018.learning to remember translation history with a con-tinuous cache.
transactions of the association forcomputational linguistics, 6:407–420..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..798elena voita, rico sennrich, and ivan titov.
2019a.
context-aware monolingual repair for neural ma-chine translation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 877–886, hong kong, china.
as-sociation for computational linguistics..elena voita, rico sennrich, and ivan titov.
2019b.
when a good translation is wrong in context:context-aware machine translation improves ondeixis, ellipsis, and lexical cohesion.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1198–1212, flo-rence, italy.
association for computational linguis-tics..elena voita, pavel serdyukov, rico sennrich, and ivantitov.
2018. context-aware neural machine trans-in proceedingslation learns anaphora resolution.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1264–1274, melbourne, australia.
associa-tion for computational linguistics..longyue wang, zhaopeng tu, andy way, and qun liu.
2017. exploiting cross-sentence context for neuralin proceedings of the 2017machine translation.
conference on empirical methods in natural lan-guage processing, pages 2826–2831, copenhagen,denmark.
association for computational linguis-tics..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 533–542, brussels, bel-gium.
association for computational linguistics..yue zhao, xiaolong jin, yuanzhuo wang, and xueqicheng.
2018. document embedding enhanced eventdetection with hierarchical and supervised attention.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume2: short papers), pages 414–419, melbourne, aus-tralia.
association for computational linguistics..ruiqi zhong, steven shao, and kathleen r. mckeown.
2019. fine-grained sentiment analysis with faithfulattention.
corr, abs/1908.06870..yicheng zou, tao gui, qi zhang, and xuanjing huang.
2018. a lexicon-based supervised attention modelin proceedings offor neural sentiment analysis.
the 27th international conference on computationallinguistics, pages 868–877, santa fe, new mexico,usa.
association for computational linguistics..7990+0.
1+0.
0+1.
1+1.
5+5.
# examplesanswer accuracy (%)not at all conﬁdent (%)highlighted current sourcehighlighted current targethighlighted context sourcehighlighted context target.
1,70978.427.69531,019––.
1,61690.713.9636913500–.
1,68993.319.4574941–619.
1,74293.76.9533965226709.
12,61697.22.81,1696,1987116,011.table 9: statistics on scat for various context levels.
source.
target.
class.
nail.
fork.
mistakes.
heater.
clou, onglemetal nail, ﬁngernail.
non-synonymous.
diapason, fourche, fourchettetuning fork, pitchfork, kitchen fork.
non-synonymous.
erreurs, fauteserrors, faults.
chauffage, radiateurheating, radiator.
synonymous.
synonymous.
table 10: examples of ambiguous word groups..a scat: supporting context forambiguous translations.
the dataset contains 19,372 annotations in total on14,000 unique examples of ambiguous anaphoricpronoun translations.
in table 9, for each contextlevel, we report the total number of examples, theoverall answer accuracy, the percentage of not atall conﬁdent answers, and the number of examplesthat contain a highlighted word the current/contextsource/target sentences..disambiguation.
to automatically generate contrastive examples ofwsd, we identify english words that have multi-ple french translations.
to do so, we ﬁrst extractword alignments from opensubtitles2018 usingawesome-align (dou and neubig, 2021) andobtain:.
am = {(cid:104)xi, yj(cid:105) : xi ∈ xm, yj ∈ ym},.
where for each word pair (cid:104)xi, yj(cid:105), xi and yj aresemantically similar to each other in context..for each pair (cid:104)xi, yj(cid:105) ∈ am, we computethe number of times the lemmatized source wordtype (vx = lemma(xi)) along with its pos tag(tx = tag(xi)) is aligned to the lemmatized tar-get word type (vy = lemma(yj)): c(vx, tx, vy).
then, we extract tuples of source types with itspos tags (cid:104)vx, tx(cid:105) that have at least two targetwords that have been aligned at least 50 times(|{vy|c(vx, tx, vy) ≥ 50}| ≥ 2).
finally, we ﬁl-ter out the source tuples which have an entropy.
h(vx, tx) less than a pre-selected threshold z. thisentropy is computed using the conditional proba-bility of a target translation given the source wordtype and its pos tag as follows:.
p := p(vy|vx, tx) =.
c(vx, tx, vy)c(vx, tx).
h(vx, tx) =.
(cid:88).
−p loge p.vy∈trans(vx,tx).
where trans(vx, tx) is the set of target translationsfor the source tuple (cid:104)vx, tx(cid:105) and p(vy|vx, tx) is theconditional probability of a given target translationvy for the source word type vx and its pos tag txout of the 394 extracted word groups, we man-ually validate and retain 201 groups and thenclassify them into 64 synonymous and 137 non-synonymous word groups (table 10).
we createcontrastive translations by extracting sentence pairscontaining an ambiguous word pair, and replacingthe translation of the polysemous english word by adifferent french word in the same group.
for wordgroups with synonymous french words, we onlyretain examples where the french word appearswithin the previous 5 sentences to enforce lexi-cal consistency, as otherwise the different frenchwords may be interchangeable..c.1 data preprocessing.
we use the english and french data from the pub-licly available opensubtitles2018 dataset (lisonet al., 2018).
we ﬁrst clean the data by selectingsentence pairs with a relative time overlap betweensource and target language subtitle frames of atleast 0.9 to reduce noise.
each data is then encodedwith byte-pair encoding (sennrich et al., 2016) us-ing sentencepiece (kudo and richardson, 2018),with source and target vocabularies of 32k tokens..c.2 training conﬁguration.
we follow the transformer base (vaswani et al.,2017) conﬁguration in all our experiments, withn = 6 encoder and decoder layers, h = 8 atten-tion heads, hidden size dmodel = 512 and feedfor-ward size dff = 2048. we use the learning rateschedule and regularization described in vaswaniet al.
(2017).
we train using the adam optimizer(kingma and ba, 2015) with β1 = 0.9, β2 = 0.98..b generating data for word sense.
c experimental setup.
800model.
attention and layer.
gold.
contrastive evaluationbleu comet bleu comet pronouns other big-par par wsd.
f-measure.
non-gold.
arararar.
apapapap.
enc self l1dec cross l1dec self l1enc self l1 + dec cross l1 + dec self l6.
enc self l1dec cross l1dec self l1enc self l1 + dec cross l1 + dec self l6.
33.833.833.133.9.
32.332.332.332.3.
42.541.741.642.3.
37.437.436.936.9.
29.930.529.730.2.
28.828.729.329.3.
39.039.338.639.3.
34.434.234.634.6.
0.470.470.410.42.
0.480.470.4848.0.
0.440.430.450.44.
0.440.430.4444.1.
91.389.888.991.1.
91.592.190.690.6.
59.565.066.069.0.
66.563.062.562.5.
57.055.553.556.0.
52.555.052.552.5.table 11: results of all models with regularized attention.
ar: attnreg-rand, ap: attnreg-pre.
c.3 attention regularization setups.
for both attnreg-rand and attnreg-pre, we experi-ment performing regularization on different modelattentions at different layers.
for the attention reg-ularization loss, in all experiments, we computethe attention regularization loss on the ﬁrst atten-tion head with λ = 10, and we divide the loss bythe length of the input.
we give the results for allsetups in table 11..c.4 evaluation.
to calculate bleu scores we use sacrebleubleu+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.14 (post, 2018) and for cometwe use wmt-large-da-estimator-1719 3. we testfor statistical signiﬁcance with p < 0.05 usingbootstrap sampling on a single run (koehn, 2004)..3https://github.com/unbabel/comet.
801