unsupervised out-of-domain detection via pre-trained transformers.
keyang xu1, tongzheng ren2, shikun zhang3, yihao feng2, caiming xiong41 columbia university 2 university of texas at austin3 carnegie mellon university 4 salesforce researchkx2155@columbia.edu shikunz@cs.cmu.edu{tzren, yihao}@cs.utexas.edu cxiong@salesforce.com.
abstract.
deployed real-world machine learning appli-cations are often subject to uncontrolled andeven potentially malicious inputs.
those out-of-domain inputs can lead to unpredictableoutputs and sometimes catastrophic safety is-sues.
prior studies on out-of-domain detec-tion require in-domain task labels and are lim-ited to supervised classiﬁcation scenarios.
ourwork tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data.
we utilize the latent represen-tations of pre-trained transformers and pro-pose a simple yet effective method to trans-form features across all layers to construct out-of-domain detectors efﬁciently.
two domain-speciﬁc ﬁne-tuning approaches are further pro-posed to boost detection accuracy.
our em-pirical evaluations of related methods on twodatasets validate that our method greatly im-proves out-of-domain detection ability in amore general scenario.1.
1.introduction.
deep neural networks, despite achieving good per-formance on many challenging tasks, can makeoverconﬁdent predictions for completely irrelevantand out-of-domain (ood) inputs, leading to sig-niﬁcant ai safety issues (hendrycks and gimpel,2017).
detecting out-of-domain inputs is a funda-mental task for trustworthy ai applications in real-world use cases, because those applications are of-ten subject to ill-deﬁned queries or even potentiallymalicious inputs.
prior work on out-of-domain de-tection (e.g., hendrycks and gimpel, 2017; leeet al., 2018; liang et al., 2018; hendrycks et al.,2019, 2020; xu et al., 2020) mostly requires in-domain task labels, limiting its usage to super-vised classiﬁcation.
however, deployed applica-.
1code.
isbert-unsupervised-ood..available.
at https://github.com/rivercold/.
tions rarely receive controlled inputs and are sus-ceptible to an ever-evolving set of user inputs thatare scarcely labeled.
for example, for many non-classiﬁcation tasks, such as summarization or topicmodeling, there are no available classiﬁers or tasklabels, which limits the practical usage of recentlyproposed out-of-domain detection methods.
there-fore, it is natural to ask the following question:.
can we detect out-of-domain samples using onlyunsupervised data without any in-domain labels?
we regard the out-of-domain detection problemas checking whether the given test samples aredrawn from the same distribution that generatesthe in-domain samples, which requires a weakerassumption than prior work (e.g., lee et al., 2018;hendrycks et al., 2020).
we suppose that there areonly in-domain samples, which allows us to under-stand the properties of data itself regardless of tasks.
therefore, methods developed for this problem aremore applicable than task-speciﬁc ones and can befurther adapted to tasks where no classiﬁcation la-bels are present, such as active learning or transferlearning..to solve the problem, we utilize the latent em-beddings of pre-trained transformers (e.g., vaswaniet al., 2017; devlin et al., 2019; liu et al., 2019) torepresent the input data, which allow us to applyclassical ood detection methods such as one-classsupport vector machines (schölkopf et al., 2001)or support vector data description (tax and duin,2004) on them..however, the best practice on how to extractfeatures from bert is usually task-speciﬁc.
forsupervised classiﬁcation, we can represent the textsequence using the hidden state of [cls] tokenfrom the top layer.
meanwhile bert’s interme-diate layers also capture rich linguistic informa-tion that may outperform the top layer for speciﬁcnlp tasks.
by performing probing tasks on eachlayer, jawahar et al.
(2019) suggest bottom layers.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1052–1061august1–6,2021.©2021associationforcomputationallinguistics1052of bert capture more surface features, middle lay-ers focus more on syntax and semantic features arewell represented by top ones..as no prior knowledge about ood samples isusually provided in practice, deciding which layerof features is the most effective for ood detec-tion is itself non-trivial.
some ood samples mayjust contain a few out-of-vocabulary words; whileothers are ood due to their syntax or semantics..based on the observations above, this paper stud-ies how to leverage all-layer features from a pre-trained transformer for ood detection in an unsu-pervised manner.
our contributions are three-fold:.
• by analyzing all layers of (ro)bert(a) mod-els, we empirically validate that it is hard to extractfeatures from a certain layer that work well for anyood datasets..• we propose a computationally efﬁcient way totransform all-layer features of a pre-trained trans-former into a low-dimension one.
we empiricallyvalidate that the proposed method outperformsbaselines that use one-layer features or by simpleaggregations of all layers..• we propose two different techniques for ﬁne-tuning a pre-trained transformer to further improveits capability of detecting ood data..2 problem setup.
assume that we have a collection of text inputsdn := {xi}ni=1, we want to construct an out-of-domain detector that takes an unseen new input uand determines whether uuu comes from the samedistribution that generates dn.
we adopt a morepractical setting where we have no prior knowl-edge of what out-of-domain inputs look like.
inthis case, training a domain classiﬁer directly isnot feasible.
the out-of-domain detector can bedescribed mathematically as:.
(cid:40).
g(uuu, (cid:15)) =.
truefalse.
if i(uuu) ≤ (cid:15) ,if i(uuu) > (cid:15) ,.
there are two different scenarios, consideringif we have any in-domain labels for data xi ∈ dn.
here we deﬁne in-domain labels as any speciﬁcsupervised task labels, such as sentiments, intentsor topics of the text..with in-domain labels suppose that we havemulti-class label yi ∈ [k] and dn = {(xxxi, yi)}ni=1.
given a classiﬁer h trained with dn, we can usemaximum calibrated softmax probability with tem-perature scaling as the anomaly score (liang et al.,2018; hinton et al., 2015):.
i(xxx) := − maxi∈[k].
exp (hi(xxx)/t )j=1 exp (hj(xxx)/t ).
(cid:80)k.,.
where hi(xxx) is the output logits of the multi-classclassiﬁer, and t is the temperature that is selectedsuch that the true positive rate is at a given rate(e.g., 95% in liang et al.
(2018)).
this method isknown as maximum softmax probability (msp),which requires multi-class labels to train a classiﬁerand thus limits its application in practice.
we arguethat requiring in-domain labels is a less practicalscenario for ood detection and will not be furtherdiscussed it in this paper..without in-domain labels the setting of no in-domain labels is our major focus.
under this as-sumptin, the models we can obtain in hand are usu-ally not classiﬁers, but feature extractors instead.
then it is natural to resort to classic outlier detec-tion methods like one-class support vector machine(schölkopf et al., 2001), support vector data de-scription (tax and duin, 2004) or kernel densityestimation (kde) for estimating the support or thedensity of the in-domain data distribution..when applying such methods to text data, themajor focus of prior work is to design a goodnetwork structure or learning objectives (ruffet al., 2018).
instead, in this paper we mainly fo-cus on how to obtain good representations frompre-trained transformers and design new anomalyscores without modifying its structure, while stillobtaining good ood detection performance..where i(·) denotes the anomaly score function, and(cid:15) is a chosen threshold to ensure that the true posi-tive rate is at a certain level (e.g., 95%) (hendrycksand gimpel, 2017; liang et al., 2018; lee et al.,2018).
the ood detection problem boils down todesigning i(·) such that it assigns in-domain inputslower scores than out-of-domain inputs..3 model and feature learning.
bert and its variants such as roberta (e.g., de-vlin et al., 2019; liu et al., 2019) are pre-trainedon large-scale public data (denoted as dpub) us-ing self-supervised tasks, such as language modeland next sentence prediction.
these models show.
1053figure 1: an overview of using mahalanobis distance features (mdf) extracted from a pre-trained transformer fto detect out-of-domain data.
we estimate mean ˆcl and covariance matrix ˆσl for each layer of f by samples froman unsupervised training set dn; and then extract mdf of dn to optimize a oc-svm.
given an unseen test sample,its feature m is extracted using ˆcl and ˆσl and then fed into oc-svm for an anomaly score.
two domain-speciﬁcﬁne-tuning methods, imlm and bcad, can be further applied to bert to boost detection accuracy..promising results when transferred to tasks in otherdomains.
we aim to leverage features obtainedfrom pre-trained transformers to construct ooddetectors in lieu of in-domain labels in dn..3.1 bert features for ood detection.
after pretraining, we can obtain a bert/robertamodel f with l layers.
we denote f(cid:96)(x) ∈ rd asthe d-dimensional feature embeddings correspond-ing to the (cid:96)-th layer for input x, and f (xxx) is theoverall representation using all layers of f .
weexplore the following methods to extract bertfeatures to construct ood detectors..features from the (cid:96)-th layer f(cid:96) options to ex-tract fl(xxx) include using the hidden states of [cls]token or averaging all contextualized token embed-dings at the (cid:96)-layer.
then we can directly constructan ood detector based on features from f(cid:96) of eachinput xxx in dn using existing pure sample basedmethods, such as one-class support vector machine(oc-svm).2.features from all layers using bert featuresfrom only one layer might not be sufﬁcient, asprior work (jawahar et al., 2019) has explored thatdifferent layers of bert capture distinct linguis-tic properties, e.g., lower-level features capturinglexical properties, middle layers representing syn-tactic properties, and semantic properties surfacingin higher layers.
the effects of bert featuresfrom different layers on detecting ood data are.
2it is also possible to use other related one-class classiﬁca-tion methods, such as isolation forest.
however, in practicewe ﬁnd oc-svm works the best and we use it in our empiricalevaluations..yet to be investigated.
one straightforward way thatleverages all l layers is to concatenate all layer-wise features f(cid:96)(xxx), which has no information loss.
however, this solution is computationally expen-sive and thus hard to optimize oc-svm or kernelbased methods.
another solution is to perform ag-gregation likes max- or mean-pooling along the fea-ture dimension across all layers, sacriﬁcing someinformation in exchange for efﬁciency..in this paper, we propose a simple yet effectivemethod (described below) to use latent representa-tions from all layers of a pre-trained transformerand can automatically decide features from whichlayers are important.
besides, this method is com-putationally efﬁcient, only requiring us to solve alow-dimensional constrained convex optimization..mahalanobis distance as features (mdf) forlayers support vector data descriptionall(svdd) (tax and duin, 2004) is a technique re-lated to oc-svm where a hypersphere is used toseparate the data instead of a hyperplane.
however,the features provided by deep models may not beseparable by hyperspheres.
we focus on a general-ization of the hypersphere called hyper-ellipsoid toaccount for such surface shapes..suppose that we use the concatenated featuresfrom all layers φ(xxx) = [f1(xxx), .
.
.
, fl(xxx)](cid:62) ∈rd·l and consider the following optimizationproblem to ﬁnd the hyper-ellipsoid, which issimilar to the optimization formula of svdd:.
minr,c,σ,ξ.
12.
(cid:32).
(cid:107)σ(cid:107)2.fr +.
r2 +.
(cid:88).
(cid:33).
ξi.
,.
1νn.
s.t.
(cid:107)φ(xi) − c(cid:107)2.iσ−1 ≤ r2 + ξi , ξi ≥ 0 , ∀i ,.
(1).
1054move 100 dollars from my savings to my checkinglet me know how to make a vacation requestis rice ok after 3 days in the refrigeratorcan you tell me a joke about politicianstell the miles it will take to get to las vegasfrom san diego…training setdntest samplehow are my sports teams doing…x1[cls]x2tellx14sanx15diegofeedforwardself-attentionz1z2z14z15……𝑓1(ĉ1,∑1)feedforwardself-attentionz1z2z13z14…𝑓2(ĉ2,∑2)feedforwardself-attentionz1z2z13z14…𝑓l(ĉl,∑l)feedforwardself-attentionz1z2z13z14…feedforwardself-attentionz1z2z13z14……x1[cls]x2howx6teamsx7doingfeedforwardself-attentionz1z2z6z7…m1m2mloc-svmanomaly score…t1[cls]t2t14t15bcadfine-tuningimlm fine-tuning…ˆˆˆwhere φ is the feature map, c is the centerof the hyper-ellipsoid, and σ is a symmetricpositive deﬁnite matrix that reﬂects the shapeof the ellipsoid.
and r reﬂects the volume ofthe hyper-ellipsoid.3 here we also introducea regularization term 1fr to constrain thecomplexity of σ. if σ = i, then the optimizationproblem is identical to one-class svdd..2 (cid:107)σ(cid:107)2.solving eq (1) exactly can be difﬁcult, since it in-volves ﬁnding the optimal σ of shape d×d, whered = d · l is the dimension of the features.
for theconcatenated features φ(x), d can be tens of thou-sands or even hundreds of thousands, which makesthe exact solution computationally intractable.
totackle the problem, we consider a simple and com-putationally efﬁcient approximation of the solu-tion, which can be useful in practice..first, we decompose the feature space into sev-eral subspaces, based on the features from differentlayers, i.e., assume σ is a block diagonal matrix,and σ(cid:96) reﬂects the shape of feature distribution atlayer (cid:96).
by a straightforward calculation, we have:.
(cid:107)φ(x) − c(cid:107)2.σ−1 =.
(cid:107)f(cid:96)(x) − c(cid:96)(cid:107)2.,.
σ−1(cid:96).
l(cid:88).
(cid:96)=1.
where we decompose the center c to be thecenter of each layer c = [c1, .
.
.
, cl](cid:62).
still,optimizing c(cid:96) and σ(cid:96) can be difﬁcult sincethe dimension of f(cid:96)(x) can be high.
based onthe intuition that c(cid:96) and σ(cid:96) should not deviatefrom the empirical mean and covariance estima-tion (cid:98)c(cid:96) and (cid:98)σ(cid:96) from the training data, we canreplace c and σ(cid:96) with the following approximation:.
1n.n(cid:88).
i=1.
c(cid:96) ≈(cid:98)c(cid:96) =.
[f(cid:96)(xi)] ,.
σ(cid:96) ≈.
=.
(cid:98)σ(cid:96)w(cid:96).
1(n − 1)w(cid:96).
n(cid:88).
i=1.
(f(cid:96)(xi) − (cid:98)c(cid:96))(f(cid:96)(xi) − (cid:98)c(cid:96))(cid:62),.
where w(cid:96) is a layer-dependent constant.
nowwe only need to ﬁnd proper {w(cid:96)}l(cid:96)=1 as wellas the corresponding r and {ξi}ni=1, which is alow-dimension optimization problem that onlyscales linearly with the number of layer l. wefurther deﬁne:.
m(cid:96)(xi) = (f(cid:96)(xi) − (cid:98)c(cid:96))(cid:62) (cid:98)σ−1.
(cid:96) (f(cid:96)(xi) − (cid:98)c(cid:96)) ,.
3we can further assume (cid:107)σ(cid:107) = 1, where the norm canbe the operator norm or frobenius norm, which can give thedeﬁnition of the hyper-ellipsoid with unique σ and r..where the square root of m(cid:96)(xxxi) is also referred toas the mahalanobis distance of the features of dataxi from layer (cid:96).
assume w = [w1, .
.
.
, wl](cid:62) ∈rl and m (x) = [m1(x), .
.
.
, ml(x)](cid:62) ∈ rl,then we have:.
(cid:107)φ(x) − c(cid:107)2.σ−1 = (cid:104)w, m (x)(cid:105) ..(cid:96)=1.
is not convex w.r.t w,.
(cid:107) (cid:98)σl(cid:107)2fr = (cid:80)las (cid:107)σ(cid:107)2frw2(cid:96)we instead minimize − 12 (cid:107)w(cid:107)22, which has a similarregularization effect on σ (as we don’t want (cid:107)w(cid:107)2to be small, which can make (cid:107)σ(cid:107)fr very large).
sothe ﬁnal optimization problem to solve is:.
minr,w,ξ.
−.
12.
(cid:107)w(cid:107)2.
2 + r2 +.
1νn.
(cid:88).
ξi,.
i.s.t..(cid:104)w, m (xi)(cid:105) ≤ r2 + ξi, ξi ≥ 0 , ∀i , (2).
which in fact is a one-class svm with a linearkernel, with mahalanobis distance of each layersas features (mdf), and it can be simply solved withthe standard convex optimization.
we illustrate ourproposed algorithm in figure 1..remark note that the optimization in eq (2) isnot identical as that in eq (1), since we are usingempirical sample mean {(cid:98)c(cid:96)}l(cid:96)=1 and covariance{ (cid:98)σ(cid:96)/w(cid:96)}l(cid:96)=1 to replace the original parameters cand σ in eq (1), which are hard to optimize whenthe dimension of the concatenated features φ(x)is high.
also, our approximation from eq (1) toeq (2) is different from the known result that whenφ(x) is the inﬁnite-dimensional feature map of thewidely used gaussian rbf kernels, oc-svm andsvdd are equivalent and asymptotically consis-tent density estimators (tsybakov et al., 1997; vertet al., 2006).
in our case, φ(x) is the concatenatedfeatures from all layers of pre-trained transform-ers, which makes our approximation fundamentallydifferent from prior work..3.2 feature ﬁne-tuning.
we can also ﬁne-tune the pre-trained transformerf on the unsupervised in-domain dataset dn sothat f (xxx) can better represent the distribution ofdn.
we explore two domain-speciﬁc ﬁne-tuningapproaches..in-domain masked language modeling (imlm)gururangan et al.
(2020) ﬁnd that domain-adaptivemasked language modeling (devlin et al., 2019)would improve supervised classiﬁcation capabil-ity of bert when it is transferred to that domain..1055sourcetypesstin-domainin-domainsstout-of-domain rteout-of-domain snliout-of-domain multi30k.
cross-corpus examples (sst)textif you love reading and or poetry , then by all means check it outthere ’s no disguising this as one of the worst ﬁlms of the summercapital punishment is a deterrent to crimea crowd of people are sitting in seats in a sports ground bleachersa trailer drives down a red brick road.
cross-intent examples (clinic150).
intenttransferpto requestfood lasttell joke.
typein-domainin-domainin-domainin-domainout-of-domain —out-of-domain —out-of-domain —.
textmove 100 dollars from my savings to my checkinglet me know how to make a vacation requestis rice ok after 3 days in the refrigeratorcan you tell me a joke about politicianshow are my sports teams doingcreate a contact labeled momwhat’s the extended zipcode for my address.
table 1: examples of in-domain/out-of-domain samples for sst and clinic150.
the source labels for sst andthe intent labels for clinic150 are here just for illustration and are not included in dn.
none of the above oodsamples are provided in training as well..similarly, we can do mlm on dn and argue thiswould make the features of dn concentrate, bring-ing beneﬁts to downstream ood detection..binary classiﬁcation with auxiliary dataset(bcad) another way of ﬁne-tuning the modelf is to use the public dataset dpub that pretrains it.
we consider the training data in dn as in-domainpositive samples and data in the public datasetdpub as ood negative samples.
we add a newclassiﬁcation layer on top of f and update this layertogether with all parameters of f by performing abinary classiﬁcation task.
in practice, we only needa small subset of dpub, denoted as ˜dpub, for ﬁne-tuning.
since ˜dpub is publicly available and has nolabels, we do not violate the unsupervised setting.
˜dpub does not provide any information about theood samples at test time as well..besides, the added classiﬁcation layer can actu-ally be applied for ood detection using the mspmethod, and this is exactly the setting of zero-shotclassiﬁcation, which we use as a baseline for com-parison in our experiments..4 experiments.
datasets we consider two distinct datasets forexperiments, where one is to regard text from un-seen corpora as ood, and the other one is to detectclass-level ood samples within the same corpus..• cross-corpus dataset (sst) we follow theexperimental setting in hendrycks et al.
(2020),.
by providing in-domain dn with the original train-ing set of sst dataset (socher et al., 2013) andconsidering samples from four other datasets (i.e.,20 newsgroups (lang, 1995), english-germanmulti30k (elliott et al., 2016), rte (dagan et al.,2005) and snli (bowman et al., 2015)) as ooddata.
for evaluation, we use the original test dataof sst as in-domain positives and randomly pick500 samples from each of the four datasets as oodnegatives.
we do not include any sentiment labelsfrom sst to dn for training..• cross-intent dataset (clinic150)this isa crowdsourced dialog dataset (larson et al., 2019),including in-domain queries covering 150 intentsand out-of-domain queries that do not fall withinany of the 150 intents.
we use all 15,000 queriesthat are originally in its training data as in-domainsamples but discard their intent labels.
for eval-uation, we mix the 4,500 unseen in-domain testqueries with 1,000 out-of-domain queries and wishto separate two sets by their anomaly scores..examples taken from the two datasets can befound in table 1. note that for both datasets, onlythe in-domain samples are used for training, andthe source/intent labels are not used in our experi-ments..evaluation metrics we rank all test samples bytheir anomaly scores and follow liang et al.
(2018)to report four different metrics, namely, area un-der the receiver operating characteristic curve(auroc), detection accuracy (dtacc), and.
1056area under the precision-recall curve (aupr) forin-domain and out-of-domain testing sentences re-spectively, denoted by auin and auout..model conﬁgurations we evaluate all methodswith both bert and roberta (base models with768 latent dimensions and 12 layers)..choice of ˜dpub for bcad we adoptthebookscorpus (zhu et al., 2015) and englishwikipedia, which are the sources used in commonby bert and roberta for pre-training.
we splitparagraphs into sentences and sample ˜dpub to havethe same size as dn for bcad..baselines to examine the effectiveness of ournewly proposed anomaly score based on mdf thatutilizes the representations of all layers, we com-pare it with the following baselines..• (ro)bert(a)-single layer: it uses f(cid:96)(xxx) men-tioned above.
we iterate all 12 layers and detailedresults of each layer are discussed in section 5.1..• (ro)bert(a)-mean pooling: we construct all-layer representation by averaging all f(cid:96)(xxx), whichhas 768 dimensions..• (ro)bert(a)-max pooling: we aggregate alllayers by picking largest values along each featuredimension and get a 768-dimension vector..• (ro)bert(a)-euclidean distance as features(edf): we replace mahalanobis distance with eu-clidean distance and still obtain a 12-dimensionvector..• tf-idf: we extract tf-idf features and adoptsvd to reduce high-dimensional features to 100dimensions for computational efﬁciency..all of the above methods extract features as the.
input to oc-svm to compute anomaly scores..• bcad + msp: it performs zero-shot classiﬁ-cation after bcad ﬁne-tuning, as discussed in sec-tion 3. the temperature scaling is tuned to achievethe best result.
this method is not applicable whenno ˜dpub is provided..5 results and discussions.
in this section, we present the results for our exper-iments and summarize our ﬁndings..5.1 using single-layer feature f(cid:96)(x).
table 2 shows results obtained from using the[cls] embedding or averaging token embeddings.
layer.
sst.
clinic150.
bert.
roberta.
bert.
roberta.
cls avg cls avg cls avg cls avg92.7 81.7 89.8 87.8 61.5 60.2 53.4 51.688.8 66.3 88.8 68.8 57.3 59.0 51.6 55.587.7 52.1 79.6 68.4 56.6 55.4 53.8 56.285.5 50.7 84.2 67.2 56.8 56.5 58.3 56.582.9 57.6 78.7 67.7 61.6 55.8 58.9 56.085.8 59.2 83.6 67.5 62.3 63.0 57.5 56.476.4 61.9 73.0 67.8 58.2 62.3 55.5 56.774.2 58.2 63.5 67.2 56.3 62.8 56.2 57.166.7 67.4 70.0 69.8 61.9 60.9 52.7 57.865.8 67.5 62.9 69.3 54.3 59.4 51.0 58.562.6 63.2 75.7 68.8 60.4 58.6 55.6 59.968.1 63.5 70.0 71.0 60.9 64.6 55.6 58.5.
121110987654321.table 2: the auroc scores of ood detectionon the sst/clinic150 dataset for each layer ofbert/roberta.
cls denotes using the hidden stateof the [cls] token and avg represents averaging all to-ken embeddings in the same layer.
layer 12 indicatesthe top layer and layer 1 is the bottom layer right af-ter the word embedding layer.
the best result for eachcolumn is marked in bold..(avg) at each layer of (ro)bert(a) models in thecross-corpus and the cross-intent dataset..we observe that detecting cross-intent ood sam-ples in clinic150 is more challenging than that ofcross-dataset ood data in sst.
this is mainly be-cause the ood samples in clinic150 are sortedby humans and the differences between intents canbe subtle.
we will further compare the performanceof these two settings in figure 2..the best f(cid:96)(xxx) for ood is dataset-speciﬁc forthe cross-corpus dataset (sst), we ﬁnd that thebest results come from the top layer of both(ro)bert(a).
however, for the cross-intent dataset(clinic150), the middle layers perform the bestwhen using [cls], while the bottom layers achievethe best results with avg.
this indicates that ooddistributions are not simply based on certain typesof linguistic features and the strategy of choos-ing f(cid:96)(xxx) is dataset-speciﬁc; for some dataset, se-mantic features play a more important role, whilesometimes we need to focus on syntactic or lexicalfeatures.
this validates the assumption that it isbeneﬁcial to fully utilize all layers of the hiddenrepresentations from pre-trained transformers todetect ood instances..we ﬁnd using f(cid:96)(xxx) of bert is generally better.
1057#feats auroc dtacc auin auout auroc dtacc auin auout.
sst.
clinic150.
bert-single layer (best)roberta-single layer (best).
bert + mean-poolingbert + max-poolingroberta + mean-poolingroberta + max-pooling.
bert + edfbert + mdfbert + imlm + mdfbert + bcad + mdfbert + imlm + bcad + mdfroberta + edfroberta + mdfroberta + imlm + mdfroberta + bcad + mdfroberta + imlm + bcad + mdf.
tf-idf + svdbert + bcad + msproberta + bcad + msp.
768768.
768768768768.
12121212121212121212.
100--.
92.789.8.
81.867.291.093.2.
90.193.393.697.098.199.599.899.999.299.9.
78.068.573.7.
85.891.5.
76.566.192.391.9.
84.887.588.194.595.495.897.797.896.698.6.
72.069.069.3.
93.479.2.
77.264.280.989.3.
92.894.997.598.098.799.599.899.899.499.9.
78.261.569.0.
91.793.8.
82.859.494.595.1.
84.289.189.494.895.999.499.899.898.799.9.
73.265.475.3.
64.659.9.
62.963.057.154.9.
55.376.777.881.282.156.978.680.180.584.4.
58.568.362.1.
60.957.6.
59.960.056.254.4.
55.271.172.274.575.656.971.973.172.976.7.
56.563.559.6.
88.486.8.
87.088.085.784.8.
84.393.493.894.695.086.393.894.594.395.4.
86.289.785.9.
26.722.7.
27.925.820.519.4.
20.338.239.147.447.619.642.644.949.459.9.
21.834.127.8.table 3: ood detection performance on sst and clinic 150 for all models.
oc-svm is used for computinganomaly scores except msp, and its parameters size is #feats.
for (ro)bert(a)+single-layer, the best results intable 2 are reported.
for all mdf-based model, we only report results of avg as sequence representation at eachlayer due to space limit.
larger values of all four metrics indicate better performances.
the best result for eachmetric is marked in bold..than roberta, especially with [cls].
we guessnext sentence prediction may cause this, which pre-trains on [cls] and is exclusive for bert..in later sections, (ro)bert(a)-single layer will.
refer to the best one in table 2..5.2 overall ood detection performance.
we report the empirical results of ood detectionin table 3 and the following observations..pre-trained transformers produce good featurerepresentations methods using single-layer fea-ture f(cid:96) outperforms frequency-based features (tf-idf) and zero-shot classiﬁcation (msp), which val-idates the strong representation capability grantedby self-supervised pre-training..simple aggregations of all layers are not so ef-fective the results of max-pooling and mean-polling are not very promising.
even though weobserve an absolute 0.5% boost in sst using max-pooling, using the best single layer actually outper-forms those simple aggregations in clinic150..mdf is more effective mdf consistently out-performs methods that directly use features f(cid:96)(xxx),simple aggregations of f(cid:96)(xxx), or tf-idf featureson all four metrics.
in terms of auroc, mdf out-performs the best single-layer of (ro)bert(a) byabsolute 7.1% on sst and 14.0% on clinic150..mdf also performs better than edf.
note thateuclidean distance is a special case of mahalanobisdistance when the covariance is an identity matrix.
empirically, the features generated by neural mod-els are not invariant across all dimensions; andthe comparison between mdf and edf validatessvdd with a hyper-ellipsoid is better than a hyper-sphere..mdf is more efﬁcient in training oc-svmnotice that our approach is also more computa-tionally efﬁcient when obtaining optimal www andrrr since the optimization is performed on a newtransformed low dimensional data space (d = 12is number of layers in f ).
see column #feats intable 3 for detailed comparisons..1058(a) roc curve on sst.
(b) i(x) on sst.
(c) roc curve on clinic150.
(d) i(x) on clinic150.
figure 2: (a): roc curves on the sst dataset.
(b): distribution of anomaly scores generated by imlm + bcad + mdf.
bothﬁgures are based on the bert model.
(c): roc curves on the clinic150 dataset.
(d): distribution of anomaly scores generatedby imlm + bcad + mdf..sentenceis a visa necessary for traveling to south africacan you tell me who sells dixie paper plates.
gt tf-idfinoutcan you tell me how to solve simple algebraic equations with one variable outout.
what oil is best for chicken.
ininoutin.
(a)(b)(c)(d).
single mdf.
inoutinin.
inoutoutin.
table 4: examples of clinic150 with predictions from three models, which is “in” when sample’s anomaly score is lowerthan 25th percentile and “out” when larger than 75th percentile.
gt is the ground truth and single stands for bert-single..fine-tuning techniques improve performancefrom table 3, we can see both milm and bcadimprove ood detection performance when incor-porated with mdf separately.
the overall best de-tecting performance is achieved by milm + bcad+ mdf, combining both proposed ﬁne-tuning meth-ods with mdf..we also ﬁnd that roberta outperforms bertwhen using mdf, even though features from a sin-gle layer prefers bert in table 2..5.3 visualizations.
we plot the roc curves of four different anomalyscores on sst in figure 2 (a) and on clinic150in figure 2 (c), conﬁrming that our proposed mdfand two ﬁne-tuning techniques improve the abilityin detecting ood samples.
we also present thedistributions of anomaly scores i(x) generated byour best method in figure 2 (b) for sst and infigure 2 (d) for clinic150.
for sst, the ood de-tector can clearly separate i(x) of in-domain andout-domain samples, and the in-domain scores aredensely concentrated on the low-score region.
al-though for clinic150, we do observe some oodsamples mixing with in-domain ones, accountingfor the gap of metric scores between two datasets..5.4 case studies.
we present some examples from clinic150 to-gether with their corresponding predictions by tf-idf, bert-single layer and mdf methods in ta-ble 4. tf-idf predicts false positives for examples(b) and (d) because most of the words in the exam-.
ple test query are seen in the training set, like “iwould like you to buy me some paper plates” (in-tent: order), “i need to know how long to cookchicken for” (intent: cooking time) and etc.
bert-single layer learns the syntax of “can you tell mehow to ...”, which is frequently seen in the train-ing data, but it fails to discern that the semanticmeaning is out-of-domain.
for example (d), allmodels make the mistake, potentially associating itwith the intent: recipe (“i need to ﬁnd a good wayto make chicken soup” or “what’s the best way tomake chicken stir fry”)..6 related work.
out-of-domain detection is essentially an importantcomponent for trustworthy machine learning appli-cations.
there are two lines of work proposed toperform out-of-domain detection.
one is to tacklethe problem in speciﬁc multi-class classiﬁcationtasks, where well-trained classiﬁers are utilized todesign anomaly scores (e.g., hendrycks and gim-pel, 2017; liang et al., 2018; lee et al., 2018; cardet al., 2019; hendrycks et al., 2020; xu et al., 2020),those methods can only be useful when multi-classlabels are available, which limits their applicationin more general domains.
our proposed work goesbeyond this limitation and can utilize large amountsof unsupervised data..another line of work is based on support esti-mation or density estimation, which assumes thatthe in-domain data is in speciﬁc support or fromthe high density region (schölkopf et al., 2001; tax.
10590.00.20.40.60.81.0false positive rate0.00.20.40.60.81.0true positive ratemspsingle layermdfmdf+imlm+bcad01020304050anomaly scorein-domainout-of-domain0.00.20.40.60.81.0false positive rate0.00.20.40.60.81.0true positive ratemspsingle layermdfmdf+imlm+bcad510152025anomaly scorein-domainout-of-domainand duin, 2004).
in principle, our work is closelyrelated to this line of work.
besides, zhai et al.
(2016); ruff et al.
(2018); zong et al.
(2018) alsoleverage the features of neural networks, thoughthese methods require designing speciﬁc networkstructures for different data.
our work circumventsthe issues of prior work by designing a computa-tionally efﬁcient method that leverages the power-ful representations of pre-trained transformers..finally, the ﬁne-tuning techniques we use to im-prove the representation of data are closely relatedto unsupervised pre-training for transformers (de-vlin et al., 2019; yang et al., 2019), and recentlyproposed contrastive learning (e.g., he et al., 2020;chen et al., 2020).
lately, gururangan et al.
(2020)discover that performing pre-training (mlm) onthe target domain with unlabeled data can also helpto improve downstream classiﬁcation performance.
to the best of our knowledge, our method is theﬁrst to incorporate transformers and pre-trainingtechniques to improve out-of-domain detection..7 conclusion.
we study the problem of detecting out-of-domainsamples with unsupervised in-domain data, whichis a more general setting for out-of-domain detec-tion.
we propose a simple yet effective method us-ing mahalanobis distance as features, which signif-icantly improves the detection ability and reducescomputational cost in learning the detector.
twodomain-adaptive ﬁne-tuning techniques are furtherexplored to boost the detection performance..in the future, we are interested in deploying ourood method to real-world applications, such asdetecting unseen new classes for incremental few-shot learning (zhang et al., 2020; xia et al., 2021)or ﬁltering ood samples in data augmentations..acknowledgments.
we would like to thank the anonymous reviewersfor their valuable feedback and comments..references.
samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiricalmethods in natural language processing, emnlp2015, pages 632–642..dallas card, michael zhang, and noah a smith.
2019.deep weighted averaging classiﬁers.
in proceedings.
of the conference on fairness, accountability, andtransparency, fat* 2019, pages 369–378..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproceedings of the 37th international conference onmachine learning, icml 2020, pages 1597–1607..i. dagan, oren glickman, and b. magnini.
2005. thepascal recognising textual entailment challenge.
inmachine learning challenges, evaluating predic-tive uncertainty, visual object classiﬁcation andrecognizing textual entailment, first pascal ma-chine learning challenges workshop, mlcw 2005..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, naacl-hlt 2019, pages 4171–4186..desmond elliott, s. frank, k. sima’an, and lucia spe-cia.
2016. multi30k: multilingual english-germanimage descriptions.
in proceedings of the 5th work-shop on vision and language..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a smith.
2020. don’t stop pretraining:language models to domains and tasks.
adapttheproceedings ofassociation for computational linguistics, acl2020, pages 8342–8360..the 58th annual meeting of.
kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for un-in 2020supervised visual representation learning.
ieee/cvf conference on computer vision and pat-tern recognition, cvpr 2020, pages 9729–9738..dan hendrycks and kevin gimpel.
2017. a baselinefor detecting misclassiﬁed and out-of-distributionin 5th internationalexamples in neural networks.
conference on learning representations,iclr2017..dan hendrycks, xiaoyuan liu, eric wallace, adamdziedzic, rishabh krishnan, and dawn song.
2020.pretrained transformers improve out-of-distributionrobustness.
proceedings of the 58th annual meet-ing of the association for computational linguistics,acl 2020, pages 2744–2751..dan hendrycks, mantas mazeika, and thomas diet-terich.
2019. deep anomaly detection with outlierexposure.
in 7th international conference on learn-ing representations, iclr 2019..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
arxivpreprint arxiv:1503.02531..1060ganesh jawahar, benoît sagot, and djamé seddah.
2019. what does bert learn about the structure oflanguage?
in proceedings of the 57th conference ofthe association for computational linguistics, acl2019, pages 3651–3657..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..k. lang.
1995. newsweeder: learning to ﬁlter net-in machine learning, proceedings of thenews.
twelfth international conference on machine learn-ing, pages 331–339..régis vert,.
jean-philippe vert,.
and bernhardschölkopf.
2006. consistency and convergencerates of one-class svms and related algorithms.
journal of machine learning research, 7(5)..stefan larson, anish mahendran, joseph j. peper,christopher clarke, andrew lee, parker hill,jonathan k. kummerfeld, kevin leach, michael a.laurenzano, lingjia tang, and jason mars.
2019.an evaluation dataset for intent classiﬁcation andout-of-scope prediction.
in proceedings of the 2019conference on empirical methods in natural lan-guage processing, emnlp 2019, pages 1311–1316..kimin lee, kibok lee, honglak lee, and jinwoo shin.
2018. a simple uniﬁed framework for detecting out-of-distribution samples and adversarial attacks.
inadvances in neural information processing systems,neurips 2018, pages 7167–7177..shiyu liang, yixuan li, and rayadurgam srikant.
2018.enhancing the reliability of out-of-distribution image detection in neural networks.
in6th international conference on learning represen-tations, iclr 2018..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..lukas ruff, robert vandermeulen, nico goernitz, lu-cas deecke, shoaib ahmed siddiqui, alexanderbinder, emmanuel müller, and marius kloft.
2018.deep one-class classiﬁcation.
in proceedings of the35th international conference on machine learning,icml 2018, pages 4393–4402..bernhard schölkopf, john c platt, john shawe-taylor,alex j smola, and robert c williamson.
2001. es-timating the support of a high-dimensional distribu-tion.
neural computation, 13(7):1443–1471..r. socher, alex perelygin, j. wu, jason chuang,christopher d. manning, a. ng, and christopherpotts.
2013. recursive deep models for semanticcompositionality over a sentiment treebank.
in pro-ceedings of the 2013 conference on empirical meth-ods in natural language processing, emnlp 2013,pages 1631–1642..david mj tax and robert pw duin.
2004..sup-port vector data description.
machine learning,54(1):45–66..alexandre b tsybakov et al.
1997. on nonparametricestimation of density level sets.
the annals of statis-tics, 25(3):948–969..congying xia, wenpeng yin, yihao feng, and philipyu.
2021.incremental few-shot text classiﬁcationwith multi-round new classes: formulation, datasetand system.
in proceedings of the 2021 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2021, pages 1351–1360..hong xu, keqing he, yuanmeng yan, sihong liu, zi-jun liu, and weiran xu.
2020. a deep generativedistance-based classiﬁer for out-of-domain detectionwith mahalanobis space.
in proceedings of the 28thinternational conference on computational linguis-tics, pages 1452–1460..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5754–5764..shuangfei zhai, yu cheng, weining lu, and zhongfeizhang.
2016. deep structured energy based mod-in proceedings of theels for anomaly detection.
33rd international conference on machine learn-ing, pages 1100–1109..jian-guo zhang, kazuma hashimoto, wenhao liu,chien-sheng wu, yao wan, philip s yu, richardsocher, and caiming xiong.
2020. discriminativenearest neighbor few-shot intent detection by trans-ferring natural language inference.
in proceedingsof the 2020 conference on empirical methods innatural language processing, emnlp 2020, pages5064–5082..y. zhu, ryan kiros, r. zemel, r. salakhutdinov, r. ur-tasun, a. torralba, and s. fidler.
2015. aligningbooks and movies: towards story-like visual ex-planations by watching movies and reading books.
2015 ieee international conference on computervision, iccv 2015, pages 19–27..bo zong, qi song, martin renqiang min, wei cheng,cristian lumezanu, daeki cho, and haifeng chen.
2018. deep autoencoding gaussian mixture modelfor unsupervised anomaly detection.
in 6th interna-tional conference on learning representations..1061