multi-perspective coherent reasoning for helpfulness prediction ofmultimodal reviews.
junhao liu1,2,3∗ zhen hai3 min yang1† lidong bing31shenzhen key laboratory for high performance data mining,shenzhen institute of advanced technology, chinese academy of sciences2university of chinese academy of sciences3damo academy, alibaba group{jh.liu, min.yang}@siat.ac.cn{zhen.hai, l.bing}@alibaba-inc.com.
abstract.
as more and more product reviews are postedin both text and images, multimodal reviewanalysis (mra) becomes an attractive re-search topic.
among the existing review analy-sis tasks, helpfulness prediction on review texthas become predominant due to its importancefor e-commerce platforms and online shops,i.e.
helping customers quickly acquire use-ful product information.
this paper proposesa new task multimodal review helpfulnessprediction (mrhp) aiming to analyze the re-view helpfulness from text and visual modal-ities.
meanwhile, a novel multi-perspectivecoherent reasoning method (mcr) is pro-posed to solve the mrhp task, which con-ducts joint reasoning over texts and imagesfrom both the product and the review, and ag-gregates the signals to predict the review help-fulness.
concretely, we ﬁrst propose a product-review coherent reasoning module to measurethe intra- and inter-modal coherence betweenthe target product and the review.
in addi-tion, we also devise an intra-review coherentreasoning module to identify the coherencebetween the text content and images of thereview, which is a piece of strong evidencefor review helpfulness prediction.
to evalu-ate the effectiveness of mcr, we present twonewly collected multimodal review datasetsas benchmark evaluation resources for themrhp task.
experimental results show thatour mcr method can lead to a performanceincrease of up to 8.5% as compared to the bestperforming text-only model.
the source codeand datasets can be obtained from https://github.com/jhliu17/mcr..1.introduction.
product reviews are essential information sourcesfor consumers to acquire useful information and.
∗this work was conducted when junhao liu was an intern.
at damo academy, alibaba group..† min yang is the corresponding author..make purchase decisions.
many e-commerce sitessuch as amazon.com offer reviewing functions thatencourage consumers to share their opinions andexperiences.
however, the user-generated reviewsvary a lot in their qualities, and we are continuouslybombarded with ever-growing, noise information.
therefore, it is critical to examine the quality ofreviews and present consumers with useful reviews.
motivated by the demand of gleaning insightsfrom such valuable data, review helpfulness pre-diction has gained increasing interest from bothacademia and industry communities.
earlier re-view helpfulness prediction methods rely on a widerange of handcrafted features, such as semantic fea-tures (yang et al., 2015), lexical features (martinand pu, 2014), and argument based features (liuet al., 2017), to train a classiﬁer.
the success ofthese methods generally relies heavily on featureengineering which is labor-intensive and highlightsthe weakness of conventional machine learningmethods.
in recent years, deep neural networkssuch as cnn (chen et al., 2018, 2019) and lstm(fan et al., 2019) have become dominant in theliterature due to their powerful performance forhelpfulness prediction by learning text representa-tion automatically.
note that these existing workson review helpfulness prediction mainly focus onthe pure textual data..as multimodal data become increasingly popu-lar in online reviews, multimodal review analysis(mra) has become a valuable research direction.
in this paper, we propose the multimodal reviewhelpfulness prediction (mrhp) task which aims atexploring multimodal clues that often convey com-prehensive information for review helpfulness pre-diction.
in particular, for the multimodal reviews,the helpfulness of reviews is not only determined bythe textual content but rather the combined expres-sion (e.g., coherence) of multimodality data (e.g.,texts and images).
taking the reviews in table 1.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5927–5936august1–6,2021.©2021associationforcomputationallinguistics5927as an example, we cannot identify the helpfulnessscore of review 3 solely from the text content untilreading the attached images that are totally irrele-vant to the product “teﬂon pans”.
the reviews thathave incoherent text content and images tend to beunhelpful, even be malicious reviews.
in contrast,a helpful review (e.g., review 2) should contain notonly concise and informative textual content butalso coherent text content and images..in this paper, we explore both text and imagesin product reviews to improve the performance ofreview helpfulness prediction.
we design a novelmulti-perspective coherent reasoning method (de-noted as mcr) to tackle the mrhp task.
con-cretely, we propose a product-review coherent rea-soning module to effectively capture the intra- andinter-modal coherence between the target productand the review.
in addition, we also devise an intra-review coherent reasoning module to capture thecoherence between the text content and images ofthe review, which is a piece of strong evidence forreview helpfulness prediction.
finally, we formu-late the helpfulness prediction as a ranking problemand employ a pairwise ranking objective to opti-mize the whole model..we summarize our main contributions as follows.
(1) to the best of our knowledge, this is the ﬁrstattempt to explore both text and images in reviewsfor helpfulness prediction, which is deﬁned as themrhp task.
(2) we propose a multi-perspectivecoherent reasoning method for the mrhp task toconduct joint reasoning over texts and images fromboth the product and the review, and aggregate thesignals to predict the helpfulness of multimodalreviews.
(3) we present two newly-collected mul-timodal review datasets for helpfulness predictionof multimodal reviews.
to facilitate research inthis area, we will release the datasets and sourcecode proposed in this paper, which would pushforward the research in this ﬁeld.
(4) extensiveexperiments on two collected datasets demonstratethat our mcr method signiﬁcantly outperformsother methods..2 related work.
most conventional approaches on review helpful-ness prediction focus solely on the text of reviews,which can be generally divided into two categoriesbased on the way of extracting predictive features:machine learning based methods with hand-craftedfeatures (kim et al., 2006; krishnamoorthy, 2015).
product informationteﬂon pans 1 set of 3 pcs 1042-non-stick set of 3.review 1 (helpfulness score: 2)overall, it is quite satisfactory.
thanks to the seller..review 2 (helpfulness score: 4)for that price, it is more than satisfactory, even thoughthere are a few scratches in the pan and the small fryingpan, the package is very neat, the frying pan has beenused as if it‘s a little burnt, it looks like it can’t stand theheat, but overall i like it..review 3 (helpfulness score: 0)recommend for the price.
yes, the package is neat butthe pan has scratched.
it is unfortunate for the delivery.
i ordered 4 items in this shop.
but the postage has to paydouble and quite very expensive..table 1: example of multimodal reviews under thesame product “teﬂon pan”.
review 1: the brief re-view text is insufﬁcient to predict its helpfulness to thecorresponding product, while the images provide a richsemantic supplement.
review 2: a helpful review witha good coherence between text and images.
review 3:an irrelevant image is attached to the review..and deep learning based methods (chen et al., 2019;fan et al., 2018; chen et al., 2018).
the machinelearning based methods employ domain-speciﬁcknowledge to extract a variety of hand-crafted fea-tures, such as structure features (kim et al., 2006),lexical features (krishnamoorthy, 2015), emotionalfeatures (martin and pu, 2014), and argument fea-tures (liu et al., 2017), from the textural reviews,which are then fed into conventional classiﬁerssuch as svm (kim et al., 2006) for helpfulnessprediction.
these methods rely heavily on featureengineering, which is time-consuming and laborintensive.
motivated by the remarkable progressof deep neural networks, several recent studies at-tempt to automatically learn deep features fromtextual reviews with deep neural networks.
chenet al.
(2019) employs a cnn model to capture the.
5928multi-granularity (character-level, word-level, andtopic-level) features for helpfulness prediction.
fanet al.
(2018) proposes a multi-task neural learningmodel to identify helpful reviews, in which theprimary task is helpfulness prediction and the aux-iliary task is star rating prediction..subsequently, several works have been proposedto explore not only the reviews but also the usersand target products for helpfulness prediction ofreviews.
fan et al.
(2019) argued that the helpful-ness of a review should be aware of the meta-data(e.g., title, brand, category, description) of the tar-get product besides the textual content of the reviewitself.
to this end, a deep neural architecture wasproposed to capture the intrinsic relationship be-tween the meta-data of a product and its numerousreviews.
qu et al.
(2020) proposed to leverage thereviews, the users, and items together for helpful-ness prediction of reviews and devised a category-aware graph neural networks with one shared andmany item-speciﬁc graph convolutions to learn thecommon features and each item’s speciﬁc criterionfor helpfulness prediction..different from the above methods, we take fulladvantage of the text content and images of reviewsby proposing a novel hierarchical coherent reason-ing method to learn the coherence between textcontent and images in a review and the coherencebetween the target product and the review..3 methodology.
the overall architecture of our mcr method isillustrated in figure 1. our multi-perspective co-herent reasoning consists of two perspectives ofcoherence: (i) the intra- and inter-modal coherencebetween a review and the target product and (ii)the intra-review coherence between the text con-tent and images in the review.
in the followingsections, we will provide the problem deﬁnition ofreview helpfulness prediction and introduce eachcomponent of our mcr model in detail..3.1 problem deﬁnition.
as mentioned by diaz and ng (2018), we formulatethe multimodal review helpfulness prediction prob-lem as a ranking task.
speciﬁcally, given a productitem pi consisting of product related information piand an associated review set ri = {ri,1, · · · , ri,n },where n is the number of reviews for pi.
eachreview has a scalar label si,j ∈ {0, · · · , s} indicat-ing the helpfulness score of the review ri,j.
the.
ground-truth ranking of ri is the descending sortorder determined by the helpfulness scores.
thegoal of review helpfulness prediction is to predicthelpfulness scores for ri which can rank the set ofreviews ri into the ground-truth result.
the pre-dicted helpfulness score ˆsi,j for the review ri,j isdeﬁned as follows:.
ˆsi,j = f (pi, ri,j),.
(1).
where f is the helpfulness prediction function tak-ing a product-review pair (cid:104)pi, ri,j(cid:105) as input.
inmultimodal review helpfulness prediction task, theproduct pi consists of associated description tpand pictures ip, while review ri,j consists of user-posted text tr and images ir..3.2 feature representation.
given a text (tp or tr) consisting of lt text tokens{w1, · · · , wlt } and an image set (ip or ir), weadopt a convolutional neural network to learn thecontextualized text representation.
meanwhile, weuse a self-attention mechanism on image regionfeatures to obtain the image representations.
toprevent conceptual confusion, we use the subscriptsp and r to indicate variables that are related to theproduct and the review, respectively..text representation inspired by the great suc-cess of convolutional neural network (cnn) in nat-ural language processing (kim, 2014; dai et al.,2018), we also apply cnn to learn the text rep-resentation.
first, we convert each token wi in areview into an embedding vector wi ∈ rd via anembedding layer.
then, we pass the learned wordembeddings to a one-dimensional cnn so as toextract multi-gram representations.
speciﬁcally,the k-gram cnn transforms the token embeddingvectors wi into k-gram representations hk:.
hk = cnnk({w1, · · · , wlt }),.
(2).
where k ∈ {1, · · · , kmax} represents the ker-nel size.
kmax represents the maximum kernelsize.
hk ∈ rlt ×dt is the k-gram representa-tion.
all the k-gram representations are stackedto form the ﬁnal text representation, denoted ash = [h1, · · · , hkmax].
here, we use hp and hrto represent the representations of text content ofthe product and the review, respectively..image representation we use pre-trainedfaster r-cnn to extract the region of interest (roi)pooling features (anderson et al., 2018) for the.
5929figure 1: model overview of our mcr method, which consists of two primary coherent reasoning components:product-review coherent reasoning and intra-review coherent reasoning..review and product images, obtaining the ﬁne-grained object-aware representations.
all the roifeatures vi extracted from image sets ip and ir arethen encoded by a self-attention module (vaswaniet al., 2017), resulting in a di -dimensional semanticspace with non-local understanding:.
v = selfattn({v1, · · · , vli }),.
(3).
where v ∈ rli ×di represents the visual semanticrepresentation and li is the number of extracted roifeatures.
here, we use vp and vr to represent theproduct and review image features, respectively..3.3 product-review coherent reasoning.
the helpfulness of a review should be fully awareof the product besides the review itself.
in thispaper, we propose a product-review coherent rea-soning module to effectively capture the intra- andinter-modal coherence between the target productand the review..intra-modal coherence we propose the intra-modal coherent reasoning to measure two kinds ofintra-modal coherence: (i) the semantic alignmentsbetween the product text and the review text, and(ii) the semantic alignments between product im-ages and review images.
the cosine similarity isutilized to derive the intra-modal coherence matrix.
for text representations hir, we computethe corresponding coherence matrix as follow:.
p and hj.
p, hji,j = cosine(hish∀i, j ∈ {1, .
.
.
, kmax},.
r),.
(4).
i,j has the shape of rltp ×ltr , ltp and ltr in-where shdicate the text length of the product and the review,respectively.
all the coherence matrices are stackedto form the whole coherence features sh.
with-out loss of generality, we also compute the imagecoherence matrix between vp and vr via cosinesimilarity.
in this way, we obtain the image coher-ence matrix sv with the shape of rlip ×lir , wherelip and lir indicate the number of roi features ofthe product and review images, respectively..subsequently, the text and image coherence ma-trix (i.e., sh and sv) are passed to a cnn, andthe top-k values in each feature map are selectedas the pooling features:.
ointram = topk(cnn([sh, sv])),(5)where ointram ∈ rk∗m is the intra-modal coher-ent reasoning features.
m is the number of ﬁltersused in the cnn module..inter-modal coherence the intra-modal coher-ence ignores the cross-modal relationship betweenthe product and the review.
in order to mitigatethis problem, we propose the inter-modal coher-ent reasoning to capture two kinds of inter-modalcoherence: (i) the coherence between the reviewtext and the product images, and (ii) the coherencebetween the review images and the product text.
since the text representation h and the image rep-resentation v lie in two different semantic spaces,we ﬁrst project them into a dc-dimensional com-mon latent space by:.
fh = tanh(w1h + b1),fv = tanh(w2v + b2),.
(6).
(7).
5930inter-modal coherenceintra-modal coherenceimage encoderimage encoderproduct imagereview imageintra-modal coherenceinter-modal coherencetext encoderteflon pans 1 set of 3 pcs 1042-non-stick set of 3product texttext encoderoverall, it is quite satisfactory.
thanks to …review textproduct-reviewcoherent reasoningproduct-review and intra-review coherent featureshelpful scorecoherentgraphlayer 0layer n…multimodal product representationmultimodal review representationintra-reviewcoherent reasoningwhere fh ∈ rlt ×dc and fv ∈ rli ×dc are textand image representations in the common latentspace, respectively..taking the coherence of review image and prod-uct text as an example, our inter-modal coherentreasoning aligns the features in review images fvrbased on the product text fhp .
speciﬁcally, we de-ﬁne the review images as the query qr = wqfvrand the product text as the key kp = wkfhp ,where wq, wk ∈ rdc×dc are learnable parame-ter matrices.
hence, the inter-modal relationshipivr can be formulated as follows:.
mr = softmax(qrktr + mrfhp ,.
r = fviv.
p ),.
(8).
(9).
where mr ∈ rli ×lt is the query attended mask.
amean-pooling operation is then conducted to getan aggregated vector of the inter-modal coherencefeatures between the review images and the producttext: ˜ivr :.
˜ivr = mean(iv.
r ) ∈ rdc..(10).
following equations 8-10, the same procedureis employed to learn the coherence features ˜ihrbetween the review text and the product images.
finally, we concatenate ˜ivto form theﬁnal inter-modal coherence features ointerm :.
r and ˜ihr.ointerm = [˜iv.
r , ˜ihr ],.
(11).
where [·] denotes the concatenate operation..3.4.intra-review coherent reasoning.
generally, consumers usually express their opin-ions in textual reviews and post images as a kindof evidence to support their opinions.
to capturethe coherence between the text content and imagesof the review, we should grasp sufﬁcient relationaland logical information between them.
to this end,we devise an intra-review coherent reasoning mod-ule to learn the coherence between the text contentand images of the review, which performs messagepropagation among semantic nodes of a reviewevidence graph and then obtains an intra-reviewcoherence score of the multimodal review..r and fv.
speciﬁcally, we construct a review evidencegraph gr by taking each feature (each row) offhr as a semantic node, and connectsall node pairs with edges, resulting in a fully-connected review evidence graph with lt + linodes.
in a similar manner, we can construct a.p = {gt.
r = {gt.
r,1, .
.
.
, gt.
product evidence graph gp with lt + li nodesp and fvfrom fhp .
the hidden states of nodes atlayer t are denoted as gtr,n} andgtp,1, .
.
.
, gtp,n} for the review and productevidence graphs respectively, where n = lt + liand t denotes the number of hops for graph rea-soning.
we compute the edge weights of semanticnode pairs with an adjacency matrix that can beautomatically learned through training.
taking thereview evidence graph gr as an example, we ini-tialize the i-th semantic node at the ﬁrst layer withg0i = [fhr,i], i ∈ {1, · · · , lt + li }.
then, theadjacency matrix at representing edge weights atlayer t is computed as follows:.
r,i, fv.
˜at.
i,j = mlpt−1([gt−1.
r,i , gt−1.
r,j ]),.
at = softmax( ˜at),.
(12).
(13).
where mlpt−1 is an mlp at layer t − 1.
˜ati,jrepresents semantic coefﬁcients between a node iwith its neighbor j ∈ ni.
softmax operation isused to normalize semantic coefﬁcients ˜at.
then,we can obtain the reasoning features at layer t by:.
gtr,i =.
at.
i,jgt−1r,j ..(14).
(cid:88).
j∈ni.
by stacking l graph reasoning layers, the seman-tic nodes can perform coherence relation reasoningby passing messages with each other.
we use glr,nand glp,n to denote the ﬁnal reasoning hidden statesof the review and product evidence graphs.
subse-quently, to obtain the product-related intra-reviewcoherent reasoning features, we adopt an attentionmechanism to ﬁlter the features that are irrelevantto the product:.
p = mean(hlp,∗),˜αi = mlp([p, gl.
r,i]),.
(15).
(16).
where a mean pooling operation is employed toderive the product coherent graph embedding p.mlp is an attention layer to calculate the product-related features and output the attention weight ˜αifor the i-th node.
after normalizing the attentionweight with a softmax function, we use a linearcombination to aggregate the intra-review coherentreasoning results oirc :.
α = softmax(˜α),(cid:88)αiglr,i..oirc =.
i.
(17).
(18).
59313.5 review helpfulness prediction.
we concatenate the intra-modal product-review co-herence features ointram , the inter-modal product-review coherence features ointerm , and the intra-review coherence features oirc to form the ﬁ-nal multi-perspective coherence features oﬁnal =[ointram , ointerm , oirc].
the ﬁnal helpfulnessprediction layer feeds oﬁnal into a linear layer tocalculate a ranking score:.
f (pi, ri,j) = wroﬁnal + br,.
(19).
where wr and br denote the projection parameterand bias term.
pi represents information of the i-thproduct and ri,j is the j-th review for pi..the standard pairwise ranking loss is adopted to.
train our model:.
(cid:88).
l =.
i.max(0, β−f (pi, r+)+f (pi, r−)) (20).
where r+, r− ∈ ri are an arbitrary pair of reviewsfor pi where r+ has a higher helpfulness score thanr−.
β is a scaling factor that magniﬁes the differ-ence between the score and the margin.
since ourmcr model is fully differentiable, it can be trainedby gradient descent in an end-to-end manner..4 experimental setup.
4.1 datasets.
to the best of our knowledge, there is no bench-mark dataset for the multimodal review help-fulness prediction task (mrhp).
hence, we con-struct two benchmark datasets (lazada-mrhp andamazon-mrhp) from popular e-commerce plat-forms to evaluate our method..lazada-mrhp in indonesian lazada.com is apopular platform in southeast asia, which is inthe indonesian language.
we construct the lazada-mrhp dataset by crawling the product information(title, description, and images) and user-generatedreviews (text content and images) from lazada.
tomake sure that the user feedback of helpfulnessvoting is reliable, we strictly extract the reviewswhich were published spanning from 2018 to 2019.we focus on three product categories, includingclothing, shoes & jewelry (cs&j), electronics(elec.
), and home & kitchen (h&k)..amazon-mrhp in english the amazon re-view dataset (ni et al., 2019) was collected fromamazon.com, containing meta-data of products.
dataset category.
lazada.
amazon.
cs&jelec.
h&k.
cs&jelec.
h&k.
instance number (#p/#r)train+dev.
test.
8,245/130,2324,811/52,3933,675/46,602.
2,062/32,2741,204/12,661920/12,551.
15,903/348,76613,205/324,90718,186/462,225.
3,966/87,4923,327/79,5704,529/111,193.
table 2: statistics of the two datasets.
#p and #r repre-sent the number of products and reviews, respectively..and customer reviews from 1996 to 2018. weextract the product information and associated re-views published from 2016 to 2018. since there areno review images in the original amazon dataset,we crawl the images for each product and re-view from the amazon.com platform.
similar tolazada-mrhp, the products and reviews also be-long to three categories: clothing, shoes & jewelry(cs&j), electronics (elec.
), and home & kitchen(h&k)..learning from user-feedback in review helpful-ness prediction has been revealed effective in (fanet al., 2019; chen et al., 2019).
speciﬁcally, thehelpfulness voting received by each review can betreated as the pseudo label indicating the helpful-ness level of the review.
following the same dataprocessing as in (fan et al., 2019), we ﬁlter the re-views that received 0 votes in that they are under anunknown user feedback state.
based on the votesreceived by a review, we leverage a logarithmicinterval to categorize reviews into ﬁve helpfulnesslevels.
speciﬁcally, we map the number of votesinto ﬁve intervals (i.e., [1,2), [2, 4), [4, 8), [8, 16),[16, ∞)) based on an exponential with base 2. theﬁve intervals correspond to ﬁve helpfulness scoressi,j ∈ {0, 1, 2, 3, 4}, where the higher the score,the more helpful the review.
finally, the statis-tics of the two datasets are shown by table 2. forboth lazada-mrhp and amazon-mrhp, we uti-lize 20% of the training set per category as thevalidation data..4.2.implementation details.
for a fair comparison, we adopt the same dataprocessing for all baselines.
we use the icu tok-enizer1 and nltk toolkit (loper and bird, 2002) toseparate text data in lazada-mrhp and amazon-mrhp, respectively.
each image is extracted asroi features with 2048 dimensions.
for the net-.
1http://site.icu-project.org.
5932type.
method.
clothing.
electronicsmap n@3 n@5 map n@3 n@5 map n@3 n@5.home.
text-only.
multi-modal.
60.0bimpmeg-cnn60.4conv-knrm 62.162.1prhnet.
sse-crossd&r netmcr (ours).
66.166.569.7.text-only.
multi-modal.
57.7bimpmeg-cnn56.4conv-knrm 57.258.3prhnet.
sse-crossd&r netmcr (ours).
65.065.267.0.
52.451.754.354.9.
59.760.763.8.
41.840.641.242.2.
56.056.158.1.
57.757.559.959.9.
64.865.368.3.
46.044.745.646.5.
59.159.261.1.
74.473.574.174.3.
76.076.177.4.
52.351.552.652.4.
53.753.956.0.
67.366.367.167.0.
68.969.271.3.
40.539.440.540.1.
43.844.246.5.
72.270.871.972.2.
73.874.075.9.
44.142.144.243.9.
47.247.549.7.
70.670.771.471.6.
72.272.474.0.
56.655.357.457.1.
60.861.263.2.
64.763.465.765.2.
66.066.367.8.
43.642.444.544.3.
51.051.854.2.
69.168.570.570.0.
71.071.472.5.
47.646.748.448.1.
54.054.657.3.table 3: helpfulness review prediction results on the lazada-mrhp dataset..type.
method.
clothing.
electronicsmap n@3 n@5 map n@3 n@5 map n@3 n@5.home.
table 4: helpfulness review prediction results on the amazon-mrhp dataset..work conﬁgurations, we initialize the word embed-ding layers with the pre-trained 300d glove wordembeddings2 for amazon-mrhp and the fasttextmultilingual word vectors3 for lazada-mrhp.
thetext n-gram kernels are set as 1, 3, and 5 with 128hidden dimensions.
for the image representations,we set the encoded size of feature dli as 128, andthe size of common latent space dc is set to 128.we stack two graph reasoning layers (i.e., l = 2)where the hidden dimension of each layer is set to128. we adopt the adam optimizer (kingma andba, 2014) to train our model, and the batch size isset to 32. the margin hyperparameter β is set to 1..4.3 compared methods.
we compare mcr with several state-of-the-artreview helpfulness methods.
first, we comparemcr with four strong methods that rely only onthe text content of reviews, including the bilat-eral multi-perspective matching (bimpm) model(wang et al., 2017), embedding-gated cnn (eg-cnn) (chen et al., 2018), convolutional kernel-based neural ranking model (conv-knrm) (daiet al., 2018), the product-aware helpfulness pre-diction network (prhnet) (fan et al., 2019)..we are the ﬁrst to leverage images in the re-.
2http://nlp.stanford.edu/data/glove.6b.zip3https://fasttext.cc/docs/en/crawl-vectors.html.
view for helpfulness prediction of multimodalreviews, thereby we compare our mcr modelwith two strong multimodal reasoning techniques:sse-cross (abavisani et al., 2020) that lever-ages stochastic shared embedding to fuse differentmodality representations and d&r net (xu et al.,2020) that adopts a decomposition and relation net-work to model both cross-modality contrast andsemantic association..4.4 evaluation metrics.
in this paper, we propose a pairwise ranking lossfunction for review helpfulness prediction, whichfully beneﬁts from the sampling of informativenegative examples.
since the output of mcris a list of reviews ranked by their helpfulnessscores, we adopt two authoritative ranking-basedmetrics to evaluate the model performance: meanaverage precision (map) and normalized dis-counted cumulative gain (ndcg@n) (j¨arvelinand kek¨al¨ainen, 2017).
here, the value of n is setto 3 and 5 in the experiments for ndcg@n. mapis a widely-used measure method evaluating thegeneral ranking performance on the whole candi-date review set, while ndcg@n merely takes intoaccount the top n reviews in the scenario that thecustomers only read a limited number of reviews..59335 experimental results.
5.1 main results.
since we adopt the pairwise ranking loss for re-view helpfulness prediction, we treat the producttext as the query, and the associated reviews areviewed as candidates for ranking.
table 3 and ta-ble 4 report the results of mcr and baselines onlazada-mrhp and amazon-mrhp, respectively.
from the results, we can make the following ob-servations.
first, eg-cnn performs worse thanother text-only baselines, because eg-cnn onlyconsiders the hidden features from the review text,while other text-only methods additionally utilizethe product information as a helpfulness signal.
second, the multimodal baselines (sse-cross andd&r net) perform signiﬁcantly better than text-only baselines.
this veriﬁes that multimodal infor-mation of reviews can help the models to discoverhelpful reviews.
third, mcr performs even betterthan strong multimodal competitors.
for example,on lazada-mrhp, map and ndcg@3 increaseby 2.9% and 3.5% respectively over the best base-line method (i.e., d&r net).
we can observe sim-ilar trends on amzaon-mrhp.
the advantage ofmcr comes from its capability of capturing theproduct-review and intra-review coherence..5.2 ablation study.
to analyze the effectiveness of different compo-nents of mcr, we conduct detailed ablation studiesin terms of removing intra-review coherence (de-noted as w/o intra-review), removing intra-modalcoherence between product and review images (de-noted as w/o intra-modal-i), removing intra-modalcoherence between product and review texts (de-noted as w/o intra-modal-ii), removing inter-modalcoherence between review text and product im-ages (denoted as w/o inter-modal-i), and remov-ing inter-modal coherence between review imagesand product text (denoted as w/o inter-modal-ii).
the ablation test results on the cs&j category oflazada and amazon datasets are summarized intable 5. we can observe that the intra-review co-herent reasoning has the largest impact on the per-formance of mcr.
this suggests that the imageswithin a review are informative evidence for reviewhelpfulness prediction.
the improvements of theintra-modal and inter-modal coherent reasoning inthe product-review coherent reasoning module arealso signiﬁcant.
however, intra-modal-i and intra-modal-ii have a smaller impact on mcr than the.
dataset model variant.
map n@3 n@5.lazada.
amazon.
mcr (ours)-w/o intra-review-w/o intra-modal-i-w/o intra-modal-ii-w/o inter-modal-i-w/o inter-modal-ii.
mcr (ours)-w/o intra-review-w/o intra-modal-i-w/o intra-modal-ii-w/o inter-modal-i-w/o inter-modal-ii.
69.768.469.169.268.968.9.
67.065.966.666.866.566.4.
63.862.063.063.262.762.5.
58.157.057.757.857.557.5.
68.366.967.567.767.367.2.
61.160.160.760.760.560.4.table 5: the ablation study on clothing, shoes& jew-elry category of lazada-mrhp and amazon-mrhp..other two variants.
this may be because most prod-uct images have been always beautiﬁed, and thereare signiﬁcant differences between the product im-ages and the images posted by the consumers.
it isno surprise that combining all components achievesthe best performance on both datasets..5.3 case study.
to gain more insight into the multimodal reviewhelpfulness prediction task, we use an exemplarycase that is selected from the test set of home& kitchen category of amazon-mrhp to empir-ically investigate the effectiveness of our model.
table 6 shows a product and two associated re-views with ground-truth helpfulness scores votedby consumers.
these two reviews are ranked cor-rectly by our mcr method while being wronglyranked by strong baselines (e.g., conv-knrm andprhnet).
the text content of both reviews con-tains negative emotion words (e.g., “disappointed”and “sad”) and expresses similar information “theproduct size does not meet my expectation”.
itis hard for text-only methods to discriminate thehelpfulness of these two reviews via solely consid-ering the text content of reviews.
after analyzingthe images within the reviews, we can reveal thatthe review 1 is helpful since it provides two ap-propriate bed images with a brought comforter asevidence that can well support his/her claim in thetext content.
however, review 2 provides an inap-propriate image with the product package, whichcannot well support the claim of product size.
thisveriﬁes that it is essential to capture the complexsemantic relationship between the images and textcontent within a review for helpfulness prediction..5934product informationbedding printed comforter set (king, grey) with 2 pillowshams - luxurious soft brushed microﬁber - goose downalternative comforter.
review 1 (helpfulness score: 4)though i like the color and look, i am very disappointedin the size.
the picture on amazon shows the comfortergoing all the way to the ﬂoor.
to be sure, i ordered theking size.
as you can see in the photos, i have a queenbed and the comforter still has 18” to the ﬂoor on eachside.
i will try to ﬁx it with a bed skirt..review 2 (helpfulness score: 1)this comforter is very ﬂuffy and does have a nice feel toit, but is far too small to actually cover much more thanthe top of the bed.
in the picture, it nearly touched theﬂoor on both visible sides.
likewise, it was describedas a printed comforter set (grey, queen) with 2 pillowshams - luxurious soft brushed microﬁber - goose downalternative comforter by utopia bedding but the itemitself said nothing of being a down alternative.
i’m sadthat this doesn’t meet my expectations..table 6: an example product and two associated re-views.
we use underlines to highlight main opinions..6 conclusion.
multimodal review analysis (mra) is extremelyimportant for helping businesses and consumersquickly acquire valuable information from user-generated reviews.
this paper is the ﬁrst attemptto explore the multimodal review helpfulness pre-diction (mrhp) task, which aims at analyzing thereview helpfulness from text and images.
we pro-pose a multi-perspective coherent reasoning (mcr)method to solve mrhp task, which fully exploresthe product-review coherence and intra-review co-herence from both textual and visual modalities.
in addition, we construct two multimodal reviewdatasets to evaluate the effectiveness of mcr,which may push forward the research in this ﬁeld.
extensive experimental results demonstrate thatmcr signiﬁcantly outperforms baselines by com-prehensively exploiting the images associated withthe reviews..acknowledgments.
this work was partially supported by na-tional natural science foundation of china(no.
61906185), natural science founda-tion of guangdong province of china (no.
2019a1515011705), youth innovation promotionassociation of cas china (no.
2020357),shenzhen science and technology innovationprogram (grant no.
kqtd20190929172835662),shenzhen basic research foundation (no.
jcyj20200109113441941)..references.
mahdi abavisani, liwei wu, shengli hu,.
joeltetreault, and alejandro jaimes.
2020. multimodalcategorization of crisis events in social media.
inproceedings of the ieee/cvf conference on com-puter vision and pattern recognition, pages 14679–14689..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..cen chen, minghui qiu, yinfei yang, jun zhou, junhuang, xiaolong li, and forrest sheng bao.
2019.multi-domain gated cnn for review helpfulness pre-diction.
in the world wide web conference, pages2630–2636..cen chen, yinfei yang, jun zhou, xiaolong li, andforrest bao.
2018. cross-domain review helpful-ness prediction based on convolutional neural net-works with auxiliary domain discriminators.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 2 (short papers), pages 602–607..zhuyun dai, chenyan xiong, jamie callan, andzhiyuan liu.
2018. convolutional neural networksfor soft-matching n-grams in ad-hoc search.
in pro-ceedings of the eleventh acm international confer-ence on web search and data mining, pages 126–134..gerardo ocampo diaz and vincent ng.
2018. model-ing and prediction of online product review helpful-ness: a survey.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 698–708..miao fan, chao feng, lin guo, mingming sun, andping li.
2019. product-aware helpfulness predictionof online reviews.
in the world wide web confer-ence, pages 2715–2721..5935zhiguo wang, wael hamza, and radu florian.
2017.bilateral multi-perspective matching for natural lan-guage sentences.
in proceedings of the 26th inter-national joint conference on artiﬁcial intelligence,pages 4144–4150..nan xu, zhixiong zeng, and wenji mao.
2020. rea-soning with multimodal sarcastic tweets via mod-eling cross-modality contrast and semantic associ-in proceedings of the 58th annual meet-ation.
ing of the association for computational linguistics,pages 3777–3786..yinfei yang, yaowei yan, minghui qiu, and forrestbao.
2015. semantic analysis and helpfulness pre-diction of text for online product reviews.
in acl,pages 38–44..miao fan, yue feng, mingming sun, ping li, haifengwang, and jianmin wang.
2018. multi-task neurallearning architecture for end-to-end identiﬁcation ofhelpful reviews.
in 2018 ieee/acm internationalconference on advances in social networks analy-sis and mining, pages 343–350.
ieee..kalervo j¨arvelin and jaana kek¨al¨ainen.
2017. ir eval-uation methods for retrieving highly relevant docu-in acm sigir forum, volume 51, pagesments.
243–250.
acm new york, ny, usa..soo-min kim, patrick pantel, timothy chklovski, andmarco pennacchiotti.
2006. automatically assess-ing review helpfulness.
in emnlp, pages 423–430..yoon kim.
2014. convolutional neural networks forsentence classiﬁcation.
in proceedings of the 2014conference on empirical methods in natural lan-guage processing, pages 1746–1751..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..srikumar krishnamoorthy.
2015. linguistic featuresfor review helpfulness prediction.
expert systemswith applications, 42(7):3751–3759..haijing liu, yang gao, pin lv, mengxue li, shiqianggeng, minglan li, and hao wang.
2017. usingargument-based features to predict and analyse re-view helpfulness.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 1358–1363..edward loper and steven bird.
2002. nltk: the nat-ural language toolkit.
arxiv preprint cs/0205028..lionel martin and pearl pu.
2014. prediction of helpfulreviews using emotions extraction.
in aaai, pages1551–1557..jianmo ni, jiacheng li, and julian mcauley.
2019.justifying recommendations using distantly-labeledreviews and ﬁne-grained aspects.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 188–197..xiaoru qu, zhao li, jialin wang, zhipeng zhang,pengcheng zou, junxiao jiang, jiaming huang,rong xiao, ji zhang, and jun gao.
2020. category-improving e-aware graph neural networks forin pro-commerce review helpfulness prediction.
ceedings of the 29th acm international conferenceon information & knowledge management, pages2693–2700..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..5936