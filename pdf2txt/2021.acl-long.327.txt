verb metaphor detection via contextual relation learningwei song1âˆ—, shuhui zhou1âˆ—, ruiji fu2,3, ting liu4, lizhen liu11college of information engineering and academy for multidisciplinary studies,capital normal university, beijing, china2state key laboratory of cognitive intelligence, iflytek research, china3iflytek ai research (hebei), langfang, china4research center for social computing and information retrieval,harbin institute of technology, harbin, china{wsong, shzhou, liz_liu7480}@cnu.edu.cn,rjfu@iflytek.com, tliu@ir.hit.edu.cn.
abstract.
correct naturallanguage understanding re-quires computers to distinguish the literal andmetaphorical senses of a word.
recent neu-ral models achieve progress on verb metaphordetection by viewing it as sequence labeling.
in this paper, we argue that it is appropri-ate to view this task as relation classiï¬ca-tion between a verb and its various contexts.
we propose the metaphor-relation bert (mr-bert) model, which explicitly models the re-lation between a verb and its grammatical, sen-tential and semantic contexts.
we evaluateour method on the vua, moh-x and trofidatasets.
our method gets competitive resultscompared with state-of-the-art approaches..1.introduction.
metaphor is ubiquitous in our daily life for effec-tive communication (lakoff and johnson, 1980).
metaphor processing has become an active researchtopic in natural language processing due to its im-portance in understanding implied meanings..this task is challenging, requiring contextualsemantic representation and reasoning.
variouscontexts and linguistic representation techniqueshave been explored in previous work..early methods focused on analyzing restrictedforms of linguistic context, such as subject-verb-object type grammatical relations, based onhand-crafted features (shutova and teufel, 2010b;tsvetkov et al., 2013; gutiÃ©rrez et al., 2016).
later,word embeddings and neural networks were in-troduced to alleviate the burden of feature engi-neering for relation-level metaphor detections (reiet al., 2017; mao et al., 2018).
however, althoughgrammatical relations provide the most direct clues,other contexts in running text are mostly ignored.
recently, token-level neural metaphor detectiondraws more attention.
several approaches discov-.
âˆ—these authors contributed equally to this work..ered that wider context can lead to better perfor-mance.
do dinh and gurevych (2016) considereda ï¬xed window surrounding each target token ascontext.
gao et al.
(2018) and mao et al.
(2018)argued that the full sentential context can providestrong clues for more accurate prediction.
somerecent work also attempted to design models moti-vated by metaphor theories (mao et al., 2019; choiet al., 2021)..despite the progress of exploiting sentential con-text, there are still issues to be addressed.
firstof all, a wordâ€™s local context, its sentential con-text and other contexts should be all important fordetecting metaphors; however, they are not wellcombined in previous work.
more importantly, asshown in figure 1, most token-level metaphor de-tection methods formulate metaphor detection aseither a single-word classiï¬cation or a sequencelabeling problem (gao et al., 2018).
the contextinformation is mainly used for learning contextualrepresentations of tokens, rather than modeling theinteractions between the target word and its con-texts (zayed et al., 2020)..in this paper, we focus on token-level verbmetaphor detection, since verb metaphors areof the most frequent type of metaphoric expres-sions (shutova and teufel, 2010a).
as shown infigure 1, we propose to formulate verb metaphordetection as a relation extraction problem, insteadof token classiï¬cation or sequence labeling formu-lations.
in analogy to identify the relations betweenentities, our method models the relations between atarget verb and its various contexts, and determinesthe verbâ€™s metaphoricity based on the relation rep-resentation rather than only the verbâ€™s (contextual)representation..we present a simple yet effective model â€”metaphor-relation bert (mrbert), which isadapted from a bert (devlin et al., 2019)based state-of-the-art relation learning model (bal-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4240â€“4251august1â€“6,2021.Â©2021associationforcomputationallinguistics4240figure 1: formulations of verb metaphor detection: (a) a single word classiï¬cation model; (b) a sequence labelingmodel; (c) the proposed relation extraction model, where hs, hi, hc and r(hi, hc) represent the representationsof a sentence, a token, the context and the relation between the target verb v and its context components..dini soares et al., 2019).
our model has three high-lights, as illustrated in figure 2. first, we explicitlyextract and represent context components, such asa verbâ€™s arguments as the local context, the wholesentence as the global context, and its basic mean-ing as a distant context.
so multiple contexts can bemodeled interactively and integrated together.
sec-ond, mrbert enables modeling the metaphoricalrelation between a verb and its context components,and uses the relation representation for determiningthe metaphoricity of the verb.
third, the model isï¬‚exible to incorporate sophisticated relation mod-eling methods and new types of contexts..we conduct experiments on the largest metaphordetection corpus vu amsterdam metaphor corpus(vua) (steen, 2010).
our method obtains com-petitive results on the large vua dataset.
detailanalysis demonstrates the beneï¬ts of integratingvarious types of contexts for relation classiï¬cation.
the results on relatively small datasets, such asmoh-x and trofi, also show good performanceand model transferability..2 formulating verb metaphor detection.
this section brieï¬‚y summarizes the common for-mulations of token-level verb metaphor detectionas a background, and discusses the relation betweenthis paper and previous work.
the task a given sentence contains a sequence ofn tokens x = x1, ..., xn, and a target verb in thissentence is xi.
verb metaphor detection is to judgewhether xi has a literal or a metaphorical sense.
basic formulations most neural networks basedapproaches cast the task as a classiï¬cation or se-quence labeling problem (do dinh and gurevych,2016; gao et al., 2018).
as shown in figure 1, theclassiï¬cation paradigm predicts a single binary la-.
bel to indicate the metaphoricity of the target verb,while the sequence labeling paradigm predicts a se-quence of binary labels to all tokens in a sentence.
based on the basic formulations, various ap-proaches have tried to enhance feature represen-tations by using globally trained contextual wordembeddings (gao et al., 2018) or incorporatingwider context with powerful encoders such as bil-stm (gao et al., 2018; mao et al., 2019) and trans-formers (dankers et al., 2019; su et al., 2020).
limitations and recent trends however,above two paradigms have some limitations..the.
first, contextual information is mostly used toenhance the representation of the target word, butthe interactions between the target word and its con-texts are not explicitly modeled (zayed et al., 2020;su et al., 2020).
to alleviate this, su et al.
(2020)proposed a new paradigm by viewing metaphor de-tection as a reading comprehension problem, whichuses the target word as a query and captures its in-teractions with the sentence and clause.
a concur-rent work to this work (choi et al., 2021) adopteda pre-trained contextualized model based late inter-action mechanism to compare the basic meaningand the contextual meaning of a word..second, exploiting wider context will bring inmore noise and may lose the focus.
fully de-pending on data-driven models to discover usefulcontexts is difï¬cult, given the scale of availabledatasets for metaphor detection is still limited.
thegrammar structures, such as verb arguments, are im-portant for metaphor processing (wilks, 1978), butis not well incorporated into neural models.
stoweet al.
(2019) showed that data augmentation basedon syntactic patterns can enhance a standard model.
le et al.
(2020) adopted graph convolutional net-works to incorporate dependency graphs, but did.
4241sentenceencoderğ‘ =ğ‘¥$,â€¦,ğ‘¥â€™=ğ‘£,â€¦,ğ‘¥)mğ‘œğ‘Ÿlsentenceencoderğ‘ =ğ‘¥$,â€¦,ğ‘¥â€™=ğ‘£,â€¦,ğ‘¥)mllâ€¦â€¦â„0ğ‘œğ‘Ÿâ„â€™â„$â„â€™â„)sentenceencoderğ‘ =ğ‘¥$,â€¦,ğ‘¥â€™=ğ‘£,â€¦,ğ‘¥)â€¦â€¦â„$â„â€™â„)relationencodermğ‘œğ‘Ÿlğ‘Ÿ(â„â€™,â„2)(a)classification(b)sequencelabeling(c)relationextractionfigure 2: an example shows mrbertâ€™s main architecture.
mrbert considers the representations of (1) the sen-tential global context, (2) the grammatical local context, and (3) the basic meaning of the verb as a distant context.
three context integration strategies for modeling contextual relations are adopted: (a) context concatenation, (b)context average, and (c) context maxout.
contextual relation r is modeled to indicate the probability of beingmetaphorical, where linear, bilinear and neural tensor models can be applied to capture interactions between theverb and its contexts.
the relation-level and sequence-level predictions are jointly optimized..not consider speciï¬c grammatical relations.
it is in-teresting to further explore how to integrate explicitlinguistic structures for contextual modeling..this paper presents a new paradigm for verbmetaphor detection to overcome these limitations,by viewing the task as a relation extraction task.
we assume a target verb and its multiple contextsare entities, and metaphor detection is to determinewhether a metaphorical relation holds between theverb and its contexts..we will introduce the proposed model in sec-tion 3. before diving into details, we argue thatviewing metaphor as a relation is reasonable andconsistent with existing metaphor theories.
ac-cording to wilks (1978), metaphors show a viola-tion of selectional preferences in a given context.
the conceptual metaphor theory views metaphorsas transferring knowledge from a familiar, orconcrete domain to an unfamiliar, or more ab-stract domain (lakoff and johnson, 1980; turneyet al., 2011).
the metaphor identiï¬cation proce-dure (mip) theory (group, 2007) aims to identifymetaphorically used words in discourse based oncomparing their use in particular context and theirbasic meanings.
all the theories care about a kindof relations between a target word and its contexts,which may help identify metaphors..3 metaphor-relation bert (mrbert).
we propose the metaphor-relation bert (mr-bert) model to realize verb metaphor detectionas a relation classiï¬cation task..figure 2 shows the architecture of mrbert.
weuse the pre-trained language model bert as thebackbone model.
there are three main procedures:(1) extract and represent contexts; (2) model thecontextual relations between the target verb and itscontexts; (3) manipulate the contextual relationsfor predicting the verbâ€™s metaphoricity..3.1 contexts and their representations.
3.1.1 types of contexts.
a metaphor can result when a target word interactswith a certain part in a sentence.
previous work of-ten explored individual context types, such as verbarguments through grammatical relations or thewhole sentence/clause.
little work has attemptedto summarize and combine different contexts..we summarize the following contexts, which.
would help determine verbsâ€™ metaphoricity:.
â€¢ global context: we view the whole sentenceas the global context.
a metaphorically usedword may seem divergent to the meaning ortopic of the sentence..4242[cls][subj]he    [/subj][verb]absorbed [/verb]the      [obj]costs     [/obj]for        the     accident [sep]deeptransformer(bert)relationrepresentationandpredictionğ‘€ğ‘’ğ‘¡ğ‘ğ‘â„ğ‘œğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘€ğ‘œğ‘Ÿğ‘™ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘™(ğ¿)?maxouta contextconcatenationc contextmaxoutâŠ•âŠ•ğ‘Ÿ()ï¼Œ++ğ‘Ÿ()ï¼Œğ‘Ÿ(ï¼Œ)ğ‘Ÿ(ï¼Œ)ğ‘Ÿ(ï¼Œ)ğ‘(ğ‘Ÿ=ğ‘€)ğ‘(ğ‘Ÿ=ğ‘€)b contextaverageğ‘(ğ‘Ÿ=ğ‘€)âŠ•+ğ‘Ÿ(ï¼Œ)sequenceprediction(1)(2)(3)(2)â€¢ local context: we view the words that have aclose grammatical relation to the target wordsas the local context, which is widely studiedto capture selectional preference violations..â€¢ distant context: motivated by the mip the-ory, the difference between the contextual us-age of a word and its basic meaning may in-dicate a metaphor so that we view the basicmeaning of the target verb as a distant context..then, we have to extract and represent these.
contexts..3.1.2 context extraction and representation.
we call the target verbâ€™s contexts as context com-ponents.
to get the contextual or basic meaningsof these components.
we use the deep transformermodels, such as bert..we ï¬rst use stanford dependency parser (chenand manning, 2014) to parse each sentence andextract verb-subject and verb-direct object relationswith vb head and nn dependent.
the nominalsubjects and objects are used as the local contextcomponents..motivated by (baldini soares et al., 2019), we[subj],introduce 6 component marker tokens,[/subj], [verb], [/verb], [obj] and [/obj], to ex-plicitly label the boundaries of the target verb, itssubject and object in each sentence.
we also use[cls] and [sep ] to mark the whole sentence.
forexample, the marker inserted token sequence forthe sentence he absorbed the costs for the accidentis shown in figure 2. the whole token sequence isfed into bertâ€™s tokenizer, and then the transformerlayers..to get the contextual representations, we use thehidden states of the ï¬nal transformer layer.
foreach marked component, we use the start marker(e.g., [subj]) or the averaged embedding betweenthe start and the end markers (e.g., [subj] and[/subj]) as the component representation..the contextual representation of the whole sen-tence is read from the ï¬nal hidden state of [cls].
to represent the basic meaning of the verb, weuse the output from the bert tokenizer to get thecontext independent verb representation.
if wordpieces exist, their averaged embedding is used..3.2 modeling the contextual relation.
the relation between the target verb and one ofits contexts is called a contextual relation.
our.
purpose is to utilize the contextual relation(s) todetermine the metaphoricity of the verb..the representations of the verb and a contextcomponent are denoted as v âˆˆ rd and c âˆˆ rk,respectively.
we adopt three ways to explicitlydeï¬ne the form of the relation r for capturing theinteractions between v and c..â€¢ linear model we use a parameter vectorvr âˆˆ rd+k and a bias br to represent the rela-tion r, and the probability of the relation beingmetaphorical is computed according to.
p(r|v, c) = Ïƒ(v (cid:62)r.+ br),.
(1).
(cid:19).
(cid:18)vc.where Ïƒ is the sigmoid function..â€¢ bilinear model we use a parameter matrixar âˆˆ rdÃ—k and a bias br to represent therelation r:.
p(r|v, c) = Ïƒ(v(cid:62)arc + br)..(2).
the components and the relation can interactmore sufï¬ciently with each other in this way..â€¢ neural tensor model we also exploit a sim-pliï¬ed neural tensor model for relation repre-sentation:.
p(r|v, c) = Ïƒ(v(cid:62)arc + v (cid:62)r.+ br).
(3).
(cid:19).
(cid:18)vc.3.3.integrating contextual relations forprediction.
we focus on 3 types of contextual relations:.
â€¢ verb-global relation the relation betweenthe contextual representations of the verb vand the whole sentence ccls..â€¢ verb-local relation the relation between thecontextual representations of the verb v andits subject csubj or object cobj..â€¢ verb-distant relation the relation between.
the verb v and its basic meaning vbsc..the representations of csubj, cobj, ccls and vbsccan be obtained as described in section 3.1.2. wetry three ways to integrate the contextual relations.
the ï¬rst two ways build a combined context c ï¬rst:.
â€¢ context concatenation we can concatenatethe representations of context components to-gether as the combined context, i.e., c =csubj âŠ• cobj âŠ• ccls âŠ• vbsc..4243â€¢ context average similarly, we can use theaveraged representation of all context com-ponents as the combined context, i.e., c =average(csubj, cobj, ccls, vbsc)..then we compute the probability that the relationis metaphorical, i.e., p(r|v, c), where either linear,bilinear or neutral tensor model can be applied..the other way is to choose the most conï¬dent.
single prediction, i.e.,.
â€¢ context maxout the prediction is basedon max{p(r|v, c)}, where c belongs to{ccls, csubj, cobj, vbsc}..to train the relation-level prediction model, we.
use binary cross-entropy as the loss function,.
l0 = âˆ’.
(Ë†yiyi + (1 âˆ’ Ë†yi)(1 âˆ’ yi)),.
(4).
1n.n(cid:88).
i=1.
where n is the number of training samples; Ë†yi isthe golden label of a verb with Ë†yi = 1 indicating ametaphorical usage and Ë†yi = 0 indicating a literalusage; yi is the probability of being metaphoricalpredicted by our model..we further combine relation-level and sequence-level metaphor detection via multi-task learning.
the sequence metaphor detection uses the hiddenstates of the ï¬nal layer and a softmax layer forpredicting the metaphoricity of each token.
we usecross-entropy as the loss function and denote theaverage loss over tokens in training samples as l1.
the ï¬nal loss of mrbert is l = l0 + l1..4 evaluation.
4.1 experimental settings.
4.1.1 datasets and evaluation metricsvua dataset we mainly conduct experiments onthe vua (steen, 2010) dataset.
it is the largestpublicly available metaphor detection dataset andhas been used in metaphor detection sharedtasks (leong et al., 2018, 2020).
this dataset hasa training set and a test set.
previous work uti-lized the training set in different ways (neidleinet al., 2020).
we use the preprocessed version ofthe vua dataset provided by gao et al.
(2018).
the ï¬rst reason is that this dataset has a ï¬xed de-velopment set so that different methods can adoptthe same model selection strategy.
the second rea-son is that several recent important methods usedthe same dataset (mao et al., 2018; dankers et al.,.
# tokens# unique sent.
% metaphor.
train116,6226,32311.2.dev38,6281,55011.6.test50,175 (5,873)2,69412.4.table 1: basic statistics of the preprocessed vuadataset provided by (gao et al., 2018).
50,175 and5,873 tokens are used for evaluating all-pos and verbtracks, respectively..2019; stowe et al., 2019; le et al., 2020).
there-fore it is convenient for us to compare the proposedmethod with previous work..there are two tracks: verb and all-posmetaphor detection.
some basic statistics of thedataset are shown in table 1. we focus on theverb track since we mainly model metaphorical re-lations for verbs.
we use mrbertâ€™s relation-levelpredictions for the verb track and use its sequencelabeling module to deal with the all-pos track.
moh-x and trofi datasets moh-x (moham-mad et al., 2016) and trofi (birke and sarkar,2006) are two relatively smaller datasets comparedwith vua.
only a single target verb is annotated ineach sentence.
we will report the results on moh-x and trofi in three settings: zero-shot transfer,re-training and ï¬ne-tuning.
metrics the evaluation metrics are accuracy (acc),precision (p), recall (r) and f1-score (f1), whichare most commonly used in previous work..4.1.2 baselineswe compare with the following approaches..â€¢ gao et al.
(2018) use contextual embeddingselmo to enhance word representations anduse bilstm as the encoder.
it has two set-tings: classiï¬cation (cls) and sequence label-ing (seq)..â€¢ mao et al.
(2019) exploit two linguistic the-ory motivated intuitions based on the basisof (gao et al., 2018).
this work motivates usto further explore contextual relation model-ing with pre-trained language models..â€¢ stowe et al.
(2019) exploit grammatical rela-tions for data augmentation to enhance (gaoet al., 2018)..â€¢ le et al.
(2020) propose a multi-task learningapproach with graph convolutional neural net-works and use word sense disambiguation asan auxiliary task..4244parameterlearning rateoptimizerbatch-sizedropoutweight decaylinear warmup.
value5e-5adam160.10.01used.
table 2: hyper-parameters for bert based systems..â€¢ neidlein et al.
(2020) (bert-seq) providea detail setting for a bert based sequencelabeling model.
this method is used as a mainpre-trained language model based baseline..the above methods all used gao et al.
(2018)â€™sdataset for evaluation so that their results can bedirectly read from their papers for comparison..â€¢ su et al.
(2020) (deepmet) view metaphor de-tection as a reading comprehension problemwith roberta as the backbone model.
it ob-tained the best performance on 2020 metaphordetection shared task..â€¢ choi et al.
(2021) (melbert) present a con-current work to ours.
the method shares simi-lar ideas and architecture with us, but it doesnot consider the grammatical relations..notice that the systems participating in the vuametaphor detection shared tasks (leong et al., 2018,2020) can use any way to manipulate the trainingset for model selection and ensemble learning sothat the reported results in the task report are notdirectly comparable to us.
the results of deep-met and melbert are based on the single modelevaluation in (choi et al., 2021)..the ï¬rst four baselines do not utilize pre-trainedlanguage models, while the last three baselinesuse bert or roberta.
these baselines supportcomprehensive comparisons from multiple aspects..4.1.3 parameter conï¬gurationduring context component extraction, if the targetverb does not have a subject or an object, we usea ï¬xed zero vector instead.
we use the bert-base-uncased model and the standard tokenizer.
thevalues of hyper-parameters are shown in table 2.for mrbert, we view the ways of componentrepresentation (start marker or averaged embed-ding, see section 3.1.2), relation modeling (lin-ear, bilinear, and neural tensor (nt)) models, seesection 3.2) and context integration (context con-catenation, average and maxout, see section 3.3).
strategies as hyper-parameters as well.
we runeach model for 10 epoches, and choose the bestcombination according to the performance on thedevelopment set.
the best combination uses theaveraged embeddings, the bilinear model and thecontext average strategy, and it will represent mr-bert for performance report in section 4.2..4.2 main results on vua dataset.
table 3 shows the results of the baselines and mr-bert.
except for (gao et al., 2018)-cls, all meth-ods use the annotation information of all tokens.
for the all-pos track, we report the performanceon either all pos tags or 4 main pos tags for com-parison with previous work..we can see that mrbert achieves superior orcompetitive performance compared with previouswork on verb metaphor detection.
the use of pre-trained language models improves the performancein general, compared with several lstm basedmethods.
recent proposed models, such as deep-met, melbert and mrbert, gain further improve-ments compared with bert-seq..mrbert outperforms (stowe et al., 2019)and (le et al., 2020) largely.
the two base-lines attempt to make use of grammar informa-tion, through data augmentation or graph neuralnetworks.
in contrast, mrbert provides a simpleyet effective way to incorporate verb arguments andnew contexts into a pre-trained language model..mrbert also has competitive performance com-pared with deepmet and melbert.
we share thesimilar idea to enhance interactions between thetarget verb and its contexts, but implement in differ-ent ways.
deepmet and melbert base on the pre-trained model roberta and use additional posor fgpos information.
moreover, these two mod-els are trained for every token so that the trainingmight be more sufï¬cient.
in contrast, we mainlymodel metaphorical relation for verbs.
this is per-haps also the reason that on the all-pos metaphordetection track, mrbert has slightly worse resultscompared with melbert.
however, our model isï¬‚exible and can be applied to tokens with otherpos tags as well.
we leave this as future work..4.3 analysis.
we further analyze the effects of modeling contex-tual relations from several aspects.
relation modeling and contextintegrationstrategies table 4 shows the results of different.
4245modelgao et al.
(2018)-clsgao et al.
(2018)-seqmao et al.
(2019)stowe et al.
(2019).
le et al.
(2020)neidlein et al.
(2020)deepmet (su et al., 2020)melbert (choi et al., 2021).
mrbert.
vua verbrp65.653.471.368.275.266.3â€“â€“.
72.578.079.578.7.
80.8.
70.969.070.972.9.
71.5.f158.969.770.569.5.
71.773.274.975.7.
75.9.vua all-pos.
pâ€“71.673.0â€“.
74.883.082.080.1.
82.7.râ€“73.675.7â€“.
75.571.971.376.9.
72.5.f1â€“72.674.373.5.
75.177.076.378.5.
77.2.accâ€“93.193.8â€“.
93.894.5â€“â€“.
94.7.vua all-pos (4 pos)f1â€“â€“â€“â€“.
accâ€“â€“â€“â€“.
râ€“â€“â€“â€“.
pâ€“â€“â€“â€“.
â€“91.8â€“â€“.
91.8.
â€“77.9â€“â€“.
78.4.
â€“64.6â€“â€“.
64.6.
â€“70.7â€“.
70.9.acc69.181.481.8â€“.
83.284.9â€“â€“.
86.4.table 3: results on the vua dataset.
mrbert uses the bilinear model for relation modeling and the context-average integration strategy.
vua all-pos (4 pos) indicates the performance on 4 main pos tags..vua-verbp77.5.modelbert-seq.
average-linearaverage-bilinearaverage-nt.
maxout-linearmaxout-bilinearmaxout-nt.
concat-linearconcat-bilinearconcat-nt.
acc85.1.
85.786.485.7.
85.285.385.6.
85.585.285.0.
79.880.877.4.
78.175.778.8.
80.377.676.4.r70.8.
70.271.573.8.
70.274.870.9.
68.671.272.3.f174.0.
74.775.975.6.
73.975.374.7.
74.074.374.3.table 4: the effects of the ways for modeling contex-tual relations and integrating multiple contexts..combinations of relation modeling and context in-tegration strategies..bert-seq here refers to the re-trained baselinewith model selection based on the performance onthe development set, and surpasses the reportedresults in (neidlein et al., 2020).
we can see thatmost combinations outperform bert-seq, andhave consistent performance.
the bilinear and neu-ral tensor models perform better than the linearmodel.
this means that sophisticated relation mod-elling techniques can beneï¬t the performance..context average and context maxout strategiesperform better than context concatenation.
thereason may be that context concatenation is moredifï¬cult to be trained due to more parameters.
effects of different contexts table 5 shows theperformance of mrbert when it considers theglobal context (mrbert-g), the global and the lo-cal contexts (mrbert-gl), and the full model withthe distant context (mrbert-gld).
each model istrained separately, with the same model selectionprocedure.
we can see that integrating multiplecontexts leads to better performance..modelaccmrbert-g85.285.5mrbert-glmrbert-gld 86.4.vua-verbp77.376.880.8.r71.973.971.5.f174.575.375.9.table 5: the performance of mrbert when consid-ering different types of contexts: g, l and d indicateglobal, local and distant contexts, respectively..mrbert explicitly incorporates verb argumentsthrough grammatical relations as the local context,which differs from other methods.
we are inter-ested in the effect of such information..we analyze mrbert-g and mrbert-gl.
ta-ble 6 shows the distribution of auto-extracted verb-subject and verb-direct object relations in the vuatest dataset.
âˆ†f1 values indicate the improvementsof mrbert-g compared with bert-seq in f1.
we can see that mrbert-g outperforms bert-seq mainly when verbâ€™s arguments are incom-plete.
for verbs with complete verb-subject andverb-direct object structures, little improvement isgained..table 7 shows the corresponding performance ofmrbert-gl.
better performance is obtained forverbs with all status of grammatical relations.
theimprovement on verbs in the lower right corneris obvious.
in these cases, the verbs are usuallyintransitive verbs or used as a noun or an adjective.
the beneï¬t of involving grammatical relations maybe that it helps keep a dynamic and balanced focusbetween the global and local contexts according tothe signals expressed by the grammatical structure.
intuitively, the effect of incorporating grammati-cal relations should be more obvious for metaphordetection in long sentences, since the local andglobal contexts are quite different.
to verify this,we divide sentences in the test dataset into bins.
4246verb-direct object.
yes.
no.
total.
yes.
1,324 (36%)âˆ†f1=0.0.
2,035 (23%)âˆ†f1= +0.57.
3,359.verb-subject.
no.
1,201 (38%)âˆ†f1=+0.05.
1,313 (27%)âˆ†f1= +1.51.
2,514.total.
2,525.
3,348.table 6: the distribution of available syntactic patternsin vua-verb test dataset and the improved f1 score ofmrbert-g compared with bert-seq.
the ï¬gures inbrackets are the percentage of metaphors..verb-direct object.
yes.
no.
total.
yes.
1,324 (36%)âˆ†f1=0.47.
2,035 (23%)âˆ†f1= +0.65.
3,359.verb-subject.
no.
1,201 (38%)âˆ†f1=0.93.
1,313 (27%)âˆ†f1= +4.29.
2,514.total.
2,525.
3,348.table 7: similar to table 6, this table shows the im-proved f1 score of mrbert-gl, instead of mrbert-g, compared with bert-seq..according to the number of clauses.
figure 3 con-ï¬rms our hypothesis that mrbert obtains largerimprovements on sentences with more clauses, in-dicating that incorporating grammatical relationscan help ï¬lter noisy information..finally, the use of distant context obtains a fur-ther improvement.
this observation is consistentwith the conclusion of (choi et al., 2021).
it alsoindicates that the bert tokenizerâ€™s embedding canbe used to approximate the representation of thetarget verbâ€™s basic meaning..4.4 results on moh-x and trofi datasets.
figure 3: the f1 scores of mrbert and bert-seqfor sentences with different number of clauses..modelgao et al.
(2018)mao et al.
(2019)le et al.
(2020)mrbertmrbert-ï¬netunedeepmetmelbertmrbert.
modelgao et al.
(2018)mao et al.
(2019)le et al.
(2020)mrbertmrbert-ï¬netunedeepmetmelbertmrbert.
moh-xacc78.579.879.981.984.9--79.3.p75.377.579.780.084.179.979.375.9.trofiacc74.675.276.475.176.7--61.1.p70.768.673.170.473.953.753.453.8.r84.383.180.585.185.676.579.784.1.r71.676.873.674.372.172.974.175.0.f179.180.079.682.184.277.979.279.8.f171.172.473.272.272.961.762.062.7.cv.
trans..cv.
trans..table 8: the experimental results on moh-x andtrofi, where cv indicates 10-fold cross-validation andtrans.
indicates transferring the trained mrbert onvua to the target datasets..2021).
the results means mrbert has good zero-shot transferability, although these datasets havequite different characteristics..in the 10-fold cross-validation setting, the re-trained mrbert can also obtain superior or com-petitive results compared with previous work.
ifwe continue to ï¬ne-tune the pre-trained mrberton the target datasets, better performance can beobtained, especially on the moh-x dataset..table 8 shows the results on the moh-x and trofidatasets..5 related work.
in the zero-shot transfer setting, mrbert ob-tains better performance compared with deepmetand melbert on both datasets.
the performanceof deepmet and melbert is read from (choi et al.,.
metaphor detection is a key task in metaphor pro-cessing (veale et al., 2016).
it is typically viewedas a classiï¬cation problem.
the early methodswere based on rules (fass, 1991; narayanan, 1997),.
424712344+number of clauses0.600.650.700.750.800.85f1bert-seqmrbertwhile most recent methods are data-driven.
next,we summarize data-driven methods from the per-spective of context types that have been explored.
grammatical relation-level detection thisline of work is to determine the metaphoricityof a given grammatical relation, such as verb-subject, verb-direct object or adjective-noun rela-tions (shutova et al., 2016).
the key to this cate-gory of work is to represent semantics and capturethe relation between the arguments..feature-based methods are based on hand-crafted linguistic features.
shutova and teufel(2010b) proposed to cluster nouns and verbs toconstruct semantic domains.
turney et al.
(2011)and shutova and sun (2013) considered the ab-stractness of concepts and context.
mohler et al.
(2013) exploited wikipedia and wordnet to builddomain signatures.
tsvetkov et al.
(2014) com-bined abstractness, imageability, supersenses, andcross-lingual features.
bulat et al.
(2017) exploitedattribute-based concept representations..the above handcrafted features heavily rely onlinguistic resources and expertise.
recently, dis-tributed representations are exploited for grammat-ical relation-level metaphor detection.
distributedword embeddings were used as features (tsvetkovet al., 2014) or to measure semantic related-ness (gutiÃ©rrez et al., 2016; mao et al., 2018).
vi-sual distributed representations were also provento be useful (shutova et al., 2016).
rei et al.
(2017) designed a supervised similarity networkto capture interactions between words.
song et al.
(2020) modeled metaphors as attribute-dependentdomain mappings and presented a knowledgegraph embedding approach for modeling nominalmetaphors.
zayed et al.
(2020) identiï¬ed verb-nounand adjective-noun phrasal metaphoric expressionsby modeling phrase representations as a context..token-level detection another line of work for-mulates metaphor detection as a single token clas-siï¬cation or sequence labeling problem (do dinhand gurevych, 2016; gao et al., 2018; mao et al.,2019).
these approaches are mostly based on neu-ral network architectures and learn representationsin an end-to-end fashion.
these approaches dependon token-level human annotated datasets, such asthe widely used vua dataset (steen, 2010)..bilstm plus pre-trained word embeddings isone of the popular architectures for this task (gaoet al., 2018; mao et al., 2019).
recently, trans-former based pre-trained language models become.
the most popular architecture in the metaphor de-tection shared task (leong et al., 2020).
multi-task learning (dankers et al., 2019; rohanian et al.,2020; le et al., 2020; chen et al., 2020) and dis-course context (dankers et al., 2020) have beenexploited as well.
discussion the grammatical relation-level andtoken-level metaphor detection consider differentaspects of information.
grammatical relations in-corporate syntactic structures, which are well stud-ied in selectional preferences (wilks, 1975, 1978)and provide important clues for metaphor detection.
however, sentential context is also useful but is ig-nored.
in contrast, token-level metaphor detectionexplores wider context and gains improvements,but syntactic information is neglected and as dis-cussed in (zayed et al., 2020), interactions betweenmetaphor components are not explicitly modeled.
this paper aims to combine the grammaticalrelation-level, token-level and semantic-level infor-mation through pre-trained language model basedcontextual relation modeling..6 conclusion.
this paper presented the metaphor-relation bert(mrbert) model for verb metaphor detection.
wepropose a new view to formulate the task as mod-eling the metaphorical relation between the targetverb and its multiple context components, i.e., con-textual relations.
we propose and evaluate variousways to extract, model and integrate contextual re-lations for metaphoricity prediction.
we conductcomprehensive experiments on the vua dataset.
the evaluation shows that mrbert achieves su-perior or competitive performance compared withprevious methods.
we also observe that incorpo-rating grammatical relations can help balance localand global contexts, and the basic meaning of theverb as a distant context is effective.
further exper-iments on small datasets moh-x and trofi alsoshow good model transferability of mrbert..acknowledgments.
this work is supported by the national naturalscience foundation of china (nos.
61876113,61876112), beijing natural science founda-tion (no.
4192017), support project of high-level teachers in beijing municipal univer-sities in the period of 13th five-year plan(cit&tcd20170322).
lizhen liu is the corre-sponding author..4248references.
livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-ing.
in proceedings of the 57th annual meetingof the association for computational linguistics,pages 2895â€“2905, florence, italy.
association forcomputational linguistics..julia birke and anoop sarkar.
2006..a clus-tering approach for nearly unsupervised recogni-tion of nonliteral language.
in 11th conferenceof the european chapter of the association forcomputational linguistics, trento, italy.
associa-tion for computational linguistics..luana bulat, stephen clark, and ekaterina shutova.
2017. modelling metaphor with attribute-based se-mantics.
in proceedings of the 15th conferenceof the european chapter of the association forcomputational linguistics: volume 2, short papers,pages 523â€“528, valencia, spain.
association forcomputational linguistics..danqi chen and christopher manning.
2014. a fastand accurate dependency parser using neural net-works.
in proceedings of the 2014 conference onempirical methods in natural language processing(emnlp), pages 740â€“750, doha, qatar.
associa-tion for computational linguistics..xianyang chen, chee wee (ben) leong, michaelflor, and beata beigman klebanov.
2020. goï¬gure!
multi-task transformer-based architecturefor metaphor detection using idioms: ets teamin 2020 metaphor shared task.
in proceedingsof the second workshop on figurative languageprocessing, pages 235â€“243, online.
association forcomputational linguistics..minjin choi, sunkyung lee, eunseong choi, heesoopark, junhyuk lee, dongwon lee, and jongwuklee.
2021. melbert: metaphor detection via con-textualized late interaction using metaphorical iden-tiï¬cation theories.
in proceedings of the 2021conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 1763â€“1773, online.
association for computational linguistics..natural language processing (emnlp-ijcnlp),pages 2218â€“2229, hong kong, china.
associationfor computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171â€“4186, minneapolis, minnesota.
associ-ation for computational linguistics..erik-lÃ¢n do dinh and iryna gurevych.
2016. token-level metaphor detection using neural networks.
inproceedings of the fourth workshop on metaphor innlp, pages 28â€“33, san diego, california.
associa-tion for computational linguistics..dan fass.
1991. met*: a method for discrim-inating metonymy and metaphor by computer.
computational linguistics, 17(1):49â€“90..ge gao, eunsol choi, yejin choi, and luke zettle-moyer.
2018. neural metaphor detection in con-text.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 607â€“613, brussels, belgium.
association forcomputational linguistics..pragglejaz group.
2007. mip: a method for iden-tifying metaphorically used words in discourse.
metaphor and symbol, 22(1):1â€“39..e. dario gutiÃ©rrez, ekaterina shutova, tyler marghetis,and benjamin bergen.
2016. literal and metaphor-ical senses in compositional distributional semanticmodels.
in proceedings of the 54th annual meetingof the association for computational linguistics(volume 1: long papers), pages 183â€“193, berlin,germany.
association for computational linguis-tics..george lakoff and mark johnson.
1980. metaphors.
we live by.
university of chicago press..duong le, my thai, and thien nguyen.
2020. multi-task learning for metaphor detection with graph con-volutional neural networks and word sense disam-biguation.
in aaai, pages 8139â€“8146..verna dankers, karan malhotra, gaurav kudva,volodymyr medentsiy, and ekaterina shutova.
2020.being neighbourly: neural metaphor identiï¬ca-tion in discourse.
in proceedings of the secondworkshop on figurative language processing,pages 227â€“234, online.
association for computa-tional linguistics..chee wee (ben) leong, beata beigman klebanov,chris hamill, egon stemle, rutuja ubale, and xi-anyang chen.
2020. a report on the 2020 vuaand toefl metaphor detection shared task.
inproceedings of the second workshop on figurativelanguage processing, pages 18â€“29, online.
associ-ation for computational linguistics..verna dankers, marek rei, martha lewis, and eka-terina shutova.
2019. modelling the interplayof metaphor and emotion through multitask learn-in proceedings of the 2019 conference oning.
empirical methods in natural language processingand the 9th international joint conference on.
chee wee (ben) leong, beata beigman klebanov, andekaterina shutova.
2018. a report on the 2018 vuametaphor detection shared task.
in proceedings ofthe workshop on figurative language processing,pages 56â€“66, new orleans, louisiana.
associationfor computational linguistics..4249rui mao, chenghua lin, and frank guerin.
2018.word embedding and wordnet based metaphoridentiï¬cation and interpretation.
in proceedingsof the 56th annual meeting of the associationfor computational linguistics (volume 1: longpapers), pages 1222â€“1231, melbourne, australia.
association for computational linguistics..ekaterina shutova and lin sun.
2013. unsupervisedmetaphor identiï¬cation using hierarchical graph fac-torization clustering.
in proceedings of the 2013conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 978â€“988, atlanta,georgia.
association for computational linguistics..rui mao, chenghua lin, and frank guerin.
2019.end-to-end sequential metaphor identiï¬cation in-in proceedings ofspired by linguistic theories.
the 57th annual meeting of the association forcomputational linguistics, pages 3888â€“3898, flo-rence, italy.
association for computational linguis-tics..saif mohammad, ekaterina shutova, and peter tur-ney.
2016. metaphor as a medium for emotion:an empirical study.
in proceedings of the fifthjoint conference on lexical and computationalsemantics, pages 23â€“33..michael mohler, david bracewell, marc tomlinson,and david hinote.
2013. semantic signatures forexample-based linguistic metaphor detection.
inproceedings of the first workshop on metaphor innlp, pages 27â€“35, atlanta, georgia.
associationfor computational linguistics..srini narayanan.
1997..knowledge-based actionrepresentations for metaphor and aspect (karma).
ph.d. thesis, ph.
d. thesis, university of californiaat berkeley..arthur neidlein, philip wiesenbach, and katja markert.
2020. an analysis of language models for metaphorrecognition.
in proceedings of the 28th internationalconference on computational linguistics, pages3722â€“3736..marek rei, luana bulat, douwe kiela, and ekate-rina shutova.
2017. grasping the ï¬ner point: asupervised similarity network for metaphor detec-tion.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 1537â€“1546, copenhagen, denmark.
associa-tion for computational linguistics..omid rohanian, marek rei, shiva taslimipoor, andle an ha.
2020. verbal multiword expressionsin proceedings offor identiï¬cation of metaphor.
the 58th annual meeting of the association forcomputational linguistics, pages 2890â€“2895, on-line.
association for computational linguistics..ekaterina shutova, douwe kiela, and jean maillard.
2016. black holes and white rabbits: metaphor iden-tiï¬cation with visual features.
in proceedings ofthe 2016 conference of the north american chapterof the association for computational linguistics:human language technologies, pages 160â€“170,san diego, california.
association for computa-tional linguistics..ekaterina shutova.
and simone teufel..2010a.
targetmetaphor corpus annotated for source -in proceedings of the seventhdomain mappings.
international conference on language resourcesand evaluation (lrecâ€™10), valletta, malta.
euro-pean language resources association (elra)..ekaterina shutova.
and simone teufel.
2010b.
metaphor corpus annotated for source-target do-main mappings.
in lrec, volume 2, pages 2â€“2.
citeseer..wei song, jingjin guo, ruiji fu, ting liu, andlizhen liu.
2020. a knowledge graph embed-ding approach for metaphor processing.
ieee/acmtransactions on audio, speech, and languageprocessing, 29:406â€“420..gerard steen.
2010. a method for linguistic metaphoridentiï¬cation: from mip to mipvu, volume 14.john benjamins publishing..kevin stowe, sarah moeller, laura michaelis, andlinguistic analysis im-martha palmer.
2019.proves neural metaphor detection.
in proceedingsof the 23rd conference on computational naturallanguage learning (conll), pages 362â€“371, hongkong, china.
association for computational lin-guistics..chuandong su, fumiyo fukumoto, xiaoxi huang,jiyi li, rongbo wang, and zhiqun chen.
2020.deepmet: a reading comprehension paradigm forin proceedingstoken-level metaphor detection.
of the second workshop on figurative languageprocessing, pages 30â€“39, online.
association forcomputational linguistics..yulia tsvetkov, leonid boytsov, anatole gershman,eric nyberg, and chris dyer.
2014. metaphordetection with cross-lingual modelinproceedings of the 52nd annual meeting of theassociation for computational linguistics (volume1: long papers), pages 248â€“258, baltimore, mary-land.
association for computational linguistics..transfer..yulia tsvetkov, elena mukomel, and anatole gersh-man.
2013. cross-lingual metaphor detection us-ing common semantic features.
in proceedings ofthe first workshop on metaphor in nlp, pages 45â€“51, atlanta, georgia.
association for computationallinguistics..peter turney, yair neuman, dan assaf, and yohai co-hen.
2011. literal and metaphorical sense identi-ï¬cation through concrete and abstract context.
inproceedings of the 2011 conference on empirical.
4250methods in natural language processing, pages680â€“690, edinburgh, scotland, uk.
association forcomputational linguistics..tony veale, ekaterina shutova, and beata beigmanklebanov.
2016. metaphor: a computational per-spective.
synthesis lectures on human languagetechnologies, 9(1):1â€“160..yorick wilks.
1975. a preferential, pattern-seeking,semantics for natural language inference.
artiï¬cialintelligence, 6(1):53â€“74..yorick wilks.
1978. making preferences more active..artiï¬cial intelligence, 11(3):197â€“223..omnia zayed, john p. mccrae, and paul buite-contextual modulation for relation-laar.
2020.level metaphor identiï¬cation.
in findings of theassociation for computational linguistics: emnlp2020, pages 388â€“406, online.
association for com-putational linguistics..4251