evaluating entity disambiguation and the role of popularityin retrieval-based nlp.
anthony chen∗.
pallavi gudipati.
shayne longpre.
xiao ling.
sameer singh.
appleuniversity of california, irvine{anthony.chen, sameer}@uci.edu{pgudipati, slongpre, xiaoling}@apple.com.
abstract.
retrieval is a core component for open-domainnlp tasks.
in open-domain tasks, multiple en-tities can share a name, making disambigua-tion an inherent yet under-explored problem.
we propose an evaluation benchmark for as-sessing the entity disambiguation capabilitiesof these retrievers, which we call ambiguousentity retrieval (amber) sets.
we deﬁne anamber set as a collection of entities that sharea name along with queries about those entities.
by covering the set of entities for polysemousnames, amber sets act as a challenging testof entity disambiguation.
we create ambersets for three popular open-domain tasks: factchecking, slot ﬁlling, and question answering,and evaluate a diverse set of retrievers.
we ﬁndthat the retrievers exhibit popularity bias, sig-niﬁcantly under-performing on rarer entitiesthat share a name, e.g., they are twice as likelyto retrieve erroneous documents on queries forthe less popular entity under the same name.
these experiments on amber sets show theirutility as an evaluation tool and highlight theweaknesses of popular retrieval systems.1.
1.introduction.
substantial progress in nlp has been made on“closed” tasks, where queries are paired with rele-vant documents (rajpurkar et al., 2016; dua et al.,2019).
however, there is growing interest in “open-domain” tasks, where relevant documents needto be retrieved from a knowledge source beforean nlp system can perform reasoning and pro-duce an answer (chen et al., 2017; petroni et al.,2021).
the open-domain setting better reﬂectsreal-world usage for tasks where relevant informa-tion is generally not provided (e.g., fact checking)..∗work started during an internship at apple.
1the amber sets used in this paper and the code togenerate them are available at https://github.com/anthonywchen/amber-sets..q: which battle did abe lincoln ﬁght in?
a: world war iiwikipedia documents ranked by blink:1.
2.
3.
4.
5.
6..abraham lincolnabraham lincoln in the black hawk warabraham lincoln (captain)benjamin lincolnlincoln nebraskalincoln england.
q: what musical instrument does abe lincoln play?
a: trombonewikipedia documents ranked by blink:1.
2.
3.
4.
5.
6..abraham lincolnjohn wilkes boothabe (musical)nebraskalincoln nebraskaabe lincoln (musician).
figure 1: queries for two entities (president & mu-sician) with the name “abe lincoln”.
retrieving thegold document involves disambiguating which “abelincoln” each query is asking about.
blink performssub-optimally on the second query, as it ranks the doc-ument of the president over the gold document..because success hinges on ﬁnding relevant docu-ments, open-domain progress has been closely tiedto improvements in retrieval systems2 (lee et al.,2019; karpukhin et al., 2020; lewis et al., 2020b).
a crucial challenge when interacting with a largeknowledge source (e.g., wikipedia) is entity ambi-guity, the phenomenon where a single name canmap to multiple entities.
resolving this ambiguityis referred to as entity disambiguation and is animportant step for effective retrieval.
for example,given the query “what musical instrument doesabe lincoln play?”, documents about the musicianshould rank higher than other entities with the samename (figure 1).
although entity disambiguationhas been extensively studied in entity linking (hof-fart et al., 2011; rao et al., 2013; sevgili et al.,.
2for example, replacing the bm25 retriever with dpr on.
natural questions increases exact match by 15 points..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4472–4485august1–6,2021.©2021associationforcomputationallinguistics44722020) and search (balog et al., 2010, 2011), inthe context of open-domain nlp, it is unclear howgood retrieval systems are when faced with querieswith ambiguous entities.
evaluating entity ambigu-ity is challenging because the popularity of entitiesfollows a long-tail (figure 2) and rare entities areseldom covered in naturally-occurring datasets..in this paper we introduce amber sets, a bench-mark for evaluating the entity disambiguation ca-pabilities of retrievers across multiple nlp tasks.
each amber set is a collection of wikidata entitiesthat share a name, and their corresponding queriesfor speciﬁc nlp tasks.
for each set, we deﬁne thehead entity as the most popular entity and tail en-tities as the less popular ones.
by creating queriesfor multiple entities that share a name, amber setsprovide an accurate test of entity disambiguationcapabilities of retrievers and help assess the roleof entity popularity in disambiguation.
we showexamples of amber sets for the question answer-ing task in table 1. we automatically create am-ber sets by mining the wikidata knowledge graph(vrandecic and kr¨otzsch, 2014) for relevant namesand entities, and leveraging task-speciﬁc templatesto generate inputs for three tasks: fact checking,slot ﬁlling, and question answering (figure 3).
intotal, our amber sets contain 80k task-speciﬁcqueries which we align to the wikipedia snapshotfrom kilt (petroni et al., 2021)..we use amber sets to conduct a systematicstudy of various retrieval systems that operate un-der different principles, such as token overlap anddense embedding similarity.
retrievers performvery differently on amber sets in terms of ab-solute retrieval numbers, with bootleg (orr et al.,2020), an entity-linking-based retriever, perform-ing best.
despite these differences, all retrieversexhibit a large degree of popularity bias, under-performing on inputs concerning tail entities.
tf-idf, a token-based retriever, performs about fourtimes worse on tail entity inputs compared to headentity inputs.
even with bootleg, the best perform-ing retriever, performance on tail entities is still1.5 times lower than on head entities.
our resultson amber sets demonstrate that there is signiﬁ-cant work to be done on making retrievers robustin handling entity disambiguation..2 amber sets.
retrieving relevant documents from large knowl-edge sources such as wikipedia is an important.
figure 2: the long tail of entity popularity: graphof the wikipedia pageviews (in october 2019) for eachwikidata entity, ranked by popularity.
gray are 100krandomly sampled entities, while red/blue are entitieswith the name “abe lincoln”..ﬁrst step in the open-domain pipeline.
an inher-ent problem in working with such sources is entitydisambiguation: resolving a name (mention) toan entity in the knowledge source.
entity disam-biguation can be challenging because many entitiesshare a name, and the popularity of entities followsa long-tail distribution (figure 2).
despite the im-portance of entity disambiguation, it remains anunderstudied problem for open-domain nlp.
weintroduce amber sets for evaluating entity disam-biguation capabilities of retrievers and analyze therole of entity popularity in disambiguation..2.1 what is an amber set?.
we ﬁrst provide an intuition for an amber set be-fore concretely deﬁning one.
consider two entities,a president and a musician, both of which have thename “abe lincoln” (figure 1).
now, considerthe query “which battle did abe lincoln ﬁght in?”and assume a retriever correctly returns the articleabout the president for this query.
simply becausethe correct document was retrieved does not meana retriever has the ability to disambiguate betweenthe president and the musician, as the president ismuch more popular.
we should only be conﬁdentin its ability to disambiguate entities if we alsopose a query about the less popular musician andthe retriever again returns the correct document (asopposed to the document about the president)..based on this intuition, we deﬁne an amber setas a collection of queries that satisfy the following:• criteria 1: polysemous name: the queries inan amber set are all about entities that share acommon name (e.g., abe lincoln)..4473qid.
input.
what wars did napoleon participate in?.
q517q3335909 what sport does napoleon play?
q3335909 which team does napoleon play for?.
answer.
gold document.
napoleon wars napoleonrugbyfiji national.
napolioni nalaganapolioni nalaga.
q117012q16264827 which sport does yoko ono participate in?.
what movement did yoko ono participate in?.
fluxusjudo.
amber-h.amber-n.q312q532100q7714007 who acted in apple?.
which industry is apple in?
what is the record label of apple?.
q788822q788822q28441308 who performed her?.
who is a cast member on her?
who is her’s screenwriter?.
yoko onoyoko ono (judoka).
apple inc.apple (band)the apple (1980 ﬁlm).
her (ﬁlm)her (ﬁlm)her (song).
electronicspage oneray shell.
steve zississpike jonzeaaron tippin.
table 1: examples of qa amber sets.
an amber set is a collection of entities that share a name, withinstantiated queries for each entity.
in this work, we use wikidata to collect entities (qid).
we also create queriesfor two more tasks, fact checking and slot ﬁlling (omitted from this table)..• criteria 2: disparity in popularity: an am-ber set contains queries about both the mostpopular entity for a name (the head entity), e.g.,the president, and the less popular entities (thetail entities), e.g., the musician..• criteria 3: resolvable ambiguity: the con-tent of the query should be sufﬁcient to resolveto the correct entity.
the query “which battle didabe lincoln ﬁght in?” satisﬁes this criteria, be-cause there is only one abe lincoln that foughtin a war, while “where was abe lincoln born?”does not since it applies to all abe lincolns.
we provide examples of amber sets for the taskof question answering in table 1..2.2 open-domain tasks.
in this work, we create amber sets for three tasks:fact checking, slot ﬁlling, and question answering(table 2).
we consider these three tasks for threereasons.
first, these three set of tasks are diversein nature.
in this work, slot ﬁlling is a generationtask, question answering is a span selection task,and fact checking is a classiﬁcation task.
second,the training sets available for each task are quitedisparate.
the largest fact checking training set,fever (thorne et al., 2018), has 80k instances,while the slot ﬁlling dataset, t-rex (elsahar et al.,2018), has over 2 million instances.
the ﬁnal rea-son we study these three tasks is that their inputsare short and easy to create..3 creating amber sets.
while amber sets can be manually created, doingso can be time-consuming, requiring a human tomanually scour a knowledge base for polysemous.
task.
input instance.
output.
fcsfqa.
john mayer plays music.
nike [sep] countrywhose face is on $100 bill?
benjamin franklin.
trueusa.
table 2: examples for each open-domain nlp task..names and related entities before manually writingqueries for those entities.
instead, we present apipeline for automatically creating amber sets us-ing the wikidata knowledge graph (vrandecic andkr¨otzsch, 2014).
in this section, we describe twodifferent collections of amber sets, and discussour automatic pipeline for creating amber sets..3.1 two collections of amber sets.
a natural question is “how do retrievers handleentity ambiguity when two entities have the sameentity type as opposed when they have differenttypes?”.
to answer this question, we create twocollections of amber sets.
the ﬁrst is amber-h, a collection of amber sets where all entitiesare humans.
the choice to restrict amber-h tohumans is motivated by the fact that humans haveproperties that help distinguish themselves fromother humans, generally based on occupation.
thesecond is amber-n, a collection of amber setswhere all entities contained are non-humans, anddisambiguation of a name is between non-humanentities with different entity types.
this is becausea non-human entity, like a movie, does not gener-ally have a single distinguishing property to distin-guish from other movies.
this makes it natural tocompare non-human entities to other non-humanentities with different types.
we specify the entitytypes in each collection in table 3..4474figure 3: automated creation of amber sets for three tasks.
we collect sets of entities from wikipedia thatshare a name, where the most popular entity is the head entity (in red) and others are tail entities (in blue), alongwith their properties and associated values.
we ﬁlter out properties that do not help distinguish entities in the set(gray-ed out), and remove entities that do not have any properties remaining.
from the remaining properties, weinstantiate queries via templates for three tasks: question answering (qa), slot ﬁlling (sf), and fact checking (fc)..3.2 automatic creation of amber sets.
entity type.
property (pid).
percent.
we now describe a pipeline to automatically createamber sets for three tasks: fact checking, slotﬁlling, and question answering.
we provide a visu-alization of the pipeline in figure 3..collecting names and entities we begin bycollecting all entity aliases3 in wikidata.
fromthese aliases, we ﬁlter for those that are shared bymultiple wikidata entities.
each entity in wikidatais represented by a unique qid.
the entities musthave an entity type from table 3 depending on thecollection we are collecting amber sets for.
eachalias and associated entities form the basis for anamber set.
within each set, we deﬁne the headand tail entities based on the number of wikipediapage views for the month of october 2019. weﬁlter out amber sets where the percentage gap inpopularity between the head entity and the mostpopular tail entity is less than 10% to account fornoise in the monthly page views..properties wecollecting distinguishinggather properties and associated values for eachentity from wikidata.
we only retain propertiesthat are in a speciﬁed list (table 3), as they areuseful for resolving ambiguity (criteria 3).
wealso ﬁlter a property if two entities within anamber set have that property, ensuring that theremaining properties can be used to disambiguatebetween entities with the same name.
theseproperties are used to instantiate the queries..aligning entities to wikipedia we use thekilt wikipedia snapshot (petroni et al., 2021) as.
3aliases are all possible names for an entity..instrument (p1303)movement (p135)appears in (p1441)killed by (p157)phd student (p185)military branch (p241)sports position (p413)sports team (p54)battles or wars (p607)sport (p641).
performer (p175)record label (p264)tracklist (p658)industry (p452)population (p1082)cast member (p161)screenwriter (p58)author (p50)record label (p264)performer (p175)record label (p264)cast member (p161)# seasons (p2437)screenwriter (p58)author (p50).
-.
hrebma.
-.
nrebma.human.
album.
businesscity.
film.
literary workmusical group.
song.
tv series.
written work.
17.012.040.080.190.4212.2212.8217.2512.2925.68.
16.577.110.210.650.2427.1418.2811.132.14.420.622.011.850.217.43.table 3: distinguishing properties selected to createqueries based on whether they are sufﬁcient to resolveambiguity.
we provide the percent breakdown of howoften each property occurs in each amber collection..the knowledge source for amber sets for betterreproducibility.
each wikipedia document in kilthas an associated qid.
for each entity, we ﬁndall wikipedia documents with that associated qid.
after this alignment, we apply a round of ﬁlteringon the tuples.
for each tuple, we check that thevalue of the tuple is within the ﬁrst 350 tokens ofthe aligned wikipedia article.
if not, we remove.
4475“davy jones”namedavid bowie*popularity: 4.09wikidata entitiesdavy jones(racing driver)popularity: 2.49davy jones(baseball)popularity: 1.93gender: malebirthplace: brixtongender: malesport: baseballgender: malesport: auto racingmovement: new wavewikidata propertiessports team: chicago white soxtask speciﬁc inputs qa: which movement is davy jones associated with?sf: davy jones [sep] movementfc: davy jones participated in the new wave movement.
true       davy jones participated in the baroque music movement.
falseqa: which team does davy jones play for?sf: davy jones [sep] member of sports teamfc: davy jones plays for the chicago white sox.
true       davy jones plays for the philadelphia phillies.
false*born davy jonesq5383q1178405q5242203amber-h amber-n.3.4 limitations.
# amber sets.
2,093.
5,237.averages per amber set.
.
.
# entities.
.
.
# entities w/ properties.
.
.
# properties.
# input queries.
.
.
question answering (qa).
.
.
slot filling (sf).
.
.
fact checking (fc).
2.982.032.84.
23,7685,9425,94211,884.
2.422.062.64.
55,21613,80413,80427,608.table 4: statistics of amber collections..the tuple.4 aligned wikipedia articles that containthe tuple value serve as gold documents..instantiating amber instances recallthatour goal was to create amber sets for three tasks:fact checking, slot ﬁlling, and question answering.
we are able to create queries for all three tasks si-multaneously using the collected wikidata tuples.
for question answering and fact checking, we usetemplates based on properties to instantiate inputs.
three of the authors wrote a template each for eachproperty for the two tasks.
duplicate templatesare removed, resulting in an average of 3 ques-tion answering templates per property and 2.7 factchecking templates per property.
see appendix bfor the complete list of templates..for slot ﬁlling, we create a single input fromeach wikidata tuple by concatenating the amberset name with the property name, and using thevalue of the tuple as the answer.
for question an-swering, we also create a single input for each tupleby ﬁlling in the template with the amber set nameand using the value of the tuple as the answer.
forfact checking, we create two inputs for each tuple,one claim that is true using the tuple value and oneclaim that is false.
the false claim is created byﬁnding the most popular value for the tuple prop-erty that does not match the tuple value5..3.3 dataset statistics.
we provide statistics for amber sets in table 4.on average, each amber set has about three en-tities that share the same name.
of these threeentities, on average, only two have properties afterﬁltering.
in total, our amber sets contain about80k task-speciﬁc input queries..4this reduces the number of tuples for amber-h from.
17,079 to 5,942 and for amber-n from 22,219 to 13,804..5 the most popular instrument in wikidata is piano.
there-fore, given the true claim “abe lincoln played the trombone.”,the false claim would be “abe lincoln played the piano.”..since our pipeline is automated and relies onwikipedia and wikidata, there are a few limitationsworth noting.
amber sets will be affected by in-completeness of the knowledge source, sometimesresulting ambiguous queries if a property is miss-ing from wikidata, but answerable from wikipediatext.
for this reason, we only select a few proper-ties for each type (table 3).
second, even thoughwe author multiple templates for each property, thereliance on these templates limits the syntactic di-versity in the queries (not a critical concern, sincewe are only evaluating existing models).
also, weuse wikipedia page views as a proxy for real-worldpopularity of entities.
deﬁning popularity in thisway may be problematic, as page views for anentity can ﬂuctuate, and may make our pipelinedifﬁcult to generalize to other knowledge sources,where this information may not be available..several design choices in creating amber setsare worth further investigation.
we limit ambersets to a pre-speciﬁed list of entity types and prop-erties to ensure that entities in an amber set aredistinguishable.
this precludes other propertiesthat may be useful in distinguishing entities, reduc-ing the diversity in amber sets.
another designchoice is we allow any alias in wikidata to form anamber sets, however, not all aliases are canonicalways to refer to the entity.
for instance, shaquilleo’neal has the unusual alias “the big cactus”,potentially leading to a somewhat unrealistic query“what sport did the big cactus play?”.
we plan torevisit the these design choices in future work..4 evaluation setup.
retrieval systems the primary focus of thiswork is to evaluate entity ambiguity of retrievalsystems.
we consider four retrievers based ondifferent retrieval paradigms.
the ﬁrst three aretf-idf, a token-based retriever using sparse em-beddings, dpr (karpukhin et al., 2020), a denseembedding based retriever, and blink (wu et al.,2020), a linker-based retriever which ranks docu-ments based on input entities.
these three retriev-ers have been thoroughly evaluated on a number ofopen-domain tasks in petroni et al.
(2021) with noobvious winner across tasks.
encouraged by thedisambiguation success on rare entities by orr et al.
(2020), we also evaluate a retriever based on boot-leg, another entity linker.
we provide additionaldetails about these retrievers in appendix d..4476collection.
retriever.
fact checking (fc).
slot filling (sf).
question answering (qa).
all head.
all head.
∀.
all head.
amber-h.amber-n.tf-idfdprblinkbootleg.
tf-idfdprblinkbootleg.
17.318.155.934.8.
9.436.911.73.5.
28.523.964.443.0.
13.648.013.94.6.tail.
8.213.349.028.2.
4.924.89.42.4.
∀.
0.00.15.60.7.
0.04.40.00.0.
18.88.038.256.5.
13.429.95.752.3.
31.911.657.063.9.
21.040.97.361.3.tail.
8.15.122.950.6.
5.218.03.942.5.
0.00.311.525.3.
0.26.00.722.4.
16.713.131.767.2.
13.936.235.259.8.
28.219.640.577.1.
21.749.244.769.5.tail.
7.37.924.659.1.
5.422.224.949.3.
∀.
0.11.16.636.1.
0.39.310.129.0.table 5: top-1 retrieval results on each collection of amber sets.
we report accuracy@1 results on all instancesas well as results on instances about head entities and instances about tail entities.
we also report a set-level metric,all correct (∀), the percentage of amber sets where all inputs had the correct document retrieved..fc.
sfhead tail head tail head tail.
qa.
h*.
tf-idfdprblinkbootleg.
19.5 67.51.2 10.09.8 32.26.2 24.7.
28.2 75.72.3 23.814.0 58.29.3 30.5.
27.9 76.12.6 27.04.4 27.63.7 28.7.n*.
tf-idfdprblinkbootleg.
23.0 76.88.7 44.05.5 31.97.8 31.6* h represents amber-h and n represents amber-n..22.0 76.99.1 48.35.1 32.216.1 36.2.
10.1 49.96.2 32.25.8 22.87.7 26.1.table 6: entity confusion measures the % of queriesthe gold document ranks worse (lower) than a docu-ment for another entity with the same name (i.e., an-other entity in the amber set).
retrievers are fourtimes as likely to exhibit this when dealing tail queries..downstream models the dominant approach toopen-domain tasks is a two-stage process where aretriever ﬁrst ﬁnds relevant documents, followedby a downstream model that processes these doc-uments to produce an answer.
we evaluate theend-to-end performance on amber sets by train-ing downstream nlp models on our tasks of in-terest.
for fact checking, we ﬁne-tune a bertclassiﬁer (devlin et al., 2019) on fever (thorneet al., 2018).
for question answering, we ﬁne-tunea roberta model (liu et al., 2019) on naturalquestions (kwiatkowski et al., 2019).
for slotﬁlling, a generation task, we ﬁne-tune a bartmodel (lewis et al., 2020a) on t-rex (elsahar et al.,2018).
we provide example training instances intable 2 and additional details on the models in ap-pendix e. we use the allennlp and huggingfacetransformers library to ﬁnetune our downstreammodels (gardner et al., 2018; wolf et al., 2020)..5 results.
in this section, we evaluate existing open-domainnlp pipelines using amber sets.
we also conduct.
figure 4: popularity gap vs retrieval gap.
we binqa queries of pairs of head and tail entities based onthe popularity gap between the entities.
for each bin,we calculate the retrieval accuracy@1 difference on thehead and tail queries.
larger popularity gaps tend tolead to a wider gaps in retrieval performance.
the redline is retrievers’ performance gaps between head andtail queries on the entire collection..a user study to evaluate the quality of the queriesin the amber sets..top document retrieval we report retrievalperformance in table 5 in terms of retriever ac-curacy@1 (the % of instances where the ﬁrst re-trieved document is the gold document).
for eachtask, we report values on the entire amber set(“all”), as well as instances corresponding onlyto “head” entities or to “tail” entities.
we alsoreport a metric we call all correct (∀), the fraction.
4477task.
system.
results.
all head.
h.n.fc.
sf.
qa.
fc.
sf.
qa.
bert (oracle)77.7bert + blink 59.8bart (oracle)83.9bart + blink 34.4bert (oracle)71.4bert + blink 27.5.bert (oracle)bert + dprbart (oracle)bart + dprbert (oracle)bert + dpr.
66.660.982.118.683.526.0.tail.
80.357.783.532.683.022.3.
69.560.484.318.681.820.4.
73.660.185.038.277.733.8.
63.961.480.118.685.131.3.table 7: end-to-end performance on amber sets.
we evaluate systems in an oracle setting, where thegold document is provided, and a retrieval setting,where 20 documents are provided from a retriever..of amber sets in which all queries had the cor-rect document retrieved.
all retrievers do betteron head entities compared to tail entities.
sinceblink, bootleg, and dpr are initialized usingpre-trained language models, they may have a pre-disposition towards being biased to more popularentities.
however, we ﬁnd tf-idf also does bet-ter on head entities, perhaps because more popularentities have longer wikipedia pages, possibly in-creasing term-frequency scores.
second, there arelarge discrepancies between a retriever’s perfor-mance on different tasks for an amber collection.
for instance, dpr does substantially worse on slotﬁlling compared to its performance on questionanswering.
this is surprising since queries for alltasks are created from the same set of wikidatatuples.
finally, we ﬁnd that retrievers are mostlyincorrect on getting all the queries in a set correct,with some receiving a ∀ score of 0 on some tasks.
overall, we ﬁnd that the bootleg retriever on av-erage does the best across tasks, however there issigniﬁcant scope for improvement..entity confusion to explicitly evaluate whetherretrievers get confused by entities in the same am-ber set, we compute entity confusion for retrieversdeﬁned as the percentage of queries where the re-triever ranks a document for an incorrect entityfrom the same amber set over the gold document(table 6).
we ﬁnd that across retrievers, tasks, andamber collections, entity confusion is twice ashigh for tail entity inputs.
this result indicates thatthe popularity of an entity for a given name plays asigniﬁcant role in retrieval performance..effect of popularity gap since the differencein popularity between the head and tail entities canvary considerably, these results obfuscate the effectof the size of the popularity gap.
we explore howthe gap in popularity between head and tail enti-ties translates to the gaps in performance on theirassociated queries.
for a head entity with popu-larity ph and a tail entity with popularity pt fromthe same amber set, we calculate popularity gap,ph−pt, and bin associated head/tail inputs based onptthe gap6.
for each bin, we calculate the differencein accuracy@1 between the head and tail entityqueries.
results for qa amber sets (figure 4)show that there is a strong correlation between thepopularity gap and the difference in performance..end to end results we evaluate end to end per-formance in several evaluation settings with allresults provided in table 7. the metrics used aref1 for slot ﬁlling and question answering and accu-racy for fact checking.
in the “oracle” setting, wedirectly provide the downstream nlp model thegold document, and ﬁnd that the gap between headentities and tail entities is fairly small.
this sug-gests that in closed nlp settings, where the golddocument is known, entity disambiguation is not amajor concern..in the regular retrieval setting, we provide themodel the top 20 documents as ranked by a re-trieval system (blink and dpr), and ﬁnd thatretrievers still perform better on head entity queries(see appendix a).
the downstream systems thatuse retrieved documents display a noticeable gapin end-to-end performance between head and tailentity inputs.
this is expected, as retrieval systemsperform worse on tail entities..user study amber sets are created in a largelyautomatic process, raising questions about dataquality.
to address these questions, we conducta small user study on amber sets to evaluatewhether the queries are resolvable by humans.
wepresent a query from a qa amber set along withthree documents for the entities from the same am-ber set, one of which is the gold document.
weﬁrst ask the user to select the relevant document,then we ask the user to select an answer span fromthe selected document.
in total, we asked 7 sub-jects to examine about 120 queries across amber-h and amber-n, and computed their accuracy in.
6bin width of 20%.
queries with a popularity gap higher.
than 100% are binned into the highest bin..4478system.
tf-idfdprblinkbootleg.
bert.
human.
amber-h.amber-n.doc acc..em doc acc..em.
43.369.169.179.6.
-.
100.
----.
71.8.
78.8.
50.368.374.173.1.
-.
97.9.
----.
75.5.
77.5.table 8: user study on amber qa.
humans arenearly perfect in identifying the correct document foreach query (doc acc), while existing retrievers fre-quently fail.
when the gold document is provided todownstream nlp models (bert), they do almost aswell as humans in answering the question (em)..selecting the correct document and answer (table8).
we also compare retrievers for this task, i.e.
select from 3 documents for the same queries, andﬁnd that humans perform very well on the docu-ment selection task compared to retrievers on bothsets.
we also compare the accuracy of answer se-lection, and see that the closed domain nlp model(ﬁne-tuned bert) is as almost accurate as humanson the same set of queries7.
this further conﬁrmsthat closed nlp models are not the source of biastowards head entities, but the retrievers are..6 related work.
entity ambiguity as previously mentioned, en-tity ambiguity is when a single name can matchmultiple entities in a knowledge source.
entityambiguity has been most studied in the context ofentity linking (rao et al., 2013).
to improve dis-ambiguation, entity linkers have included auxiliaryinformation such as entity types (onoe and durrett,2020) and entity descriptions (logeswaran et al.,2019).
a recent thread of work aims to study howlanguage models recall and leverage informationabout names and entities.
prabhakaran et al.
(2019)shows that names can have a measurable effecton the prediction of sentiment analysis systems.
shwartz et al.
(2020) demonstrates that pre-trainedlanguage models implicitly resolve entity ambigu-ity by grounding names to entities based on the pre-training corpus.
the problem of entity ambiguityalso appears implicitly in entity-centric tasks suchas determining the semantic relatedness betweenentities (hoffart et al., 2012) and entity-oriented.
7the relatively low answer score is due to artifacts inusing em for qa evaluation, and is consistent with humanperformance on span selection (rajpurkar et al., 2016))..search (balog et al., 2010, 2011).
we draw inspira-tion from these works by studying entity ambiguityin the context of open-domain nlp..popularity bias system’s that perform worse onthe long-tail suffer from what is known as popular-ity bias.
this problem has been studied extensivelyin the recommendation systems literature, whererecommendation systems are known to often ignorethe long-tail of products and instead recommendvery popular items (abdollahpouri et al., 2017;chen et al., 2020).
this has the effect of unfairlyhurting users who would prefer these less-popularitems (abdollahpouri et al., 2019; ciampaglia et al.,2018).
we explore popularity bias from the angleof retrieval as opposed to recommendation, andﬁnd popularity bias exists in retrieval systems..open-domain ambiguity ambiguity is an in-herent problem when it comes to open-domainreasoning.
min et al.
(2020) showed that half ofinstances sampled from natural questions are am-biguous, with multiple correct answers.
ambersets are similar in that the ambiguity is in termsof the entity in the query, however, in contrast tonatural questions, amber set inputs have beenconstructed such that the ambiguity is resolvable..challenge sets there have been many evalua-tion sets speciﬁcally designed to assess a model’sability to handle a speciﬁc phenomenon (naiket al., 2018; zhao et al., 2018; mccoy et al., 2019;warstadt et al., 2020; richardson et al., 2020;jeretic et al., 2020; ribeiro et al., 2019).
someof these challenge sets, similar to amber sets, usetemplates to generate a large amount of evalua-tion data quickly (richardson et al., 2020; mccoyet al., 2019; ribeiro et al., 2020).
amber sets canbe viewed as a challenge set for assessing open-domain systems’ ability to handle entity ambiguity..7 conclusion.
entity ambiguity is an inherent problem in retrieval,as many entities can share a name.
for evaluatingdisambiguation capabilities of retrievers, we intro-duce amber sets; an amber set is a collectionof task-speciﬁc queries about entities that share aname, but the queries have sufﬁcient content to re-solve the correct entity.
we create a broad range ofamber sets, covering many entity types, with in-put queries for three open-domain nlp tasks: factchecking, slot ﬁlling, and question answering.
ourexperiments demonstrate the struggles of current.
4479retrievers in handling entity ambiguity.
in partic-ular, we ﬁnd that the popularity of an entity inrelation to other entities that share a name playsa signiﬁcant role during disambiguation.
for in-stance, we ﬁnd that all tested retrievers are abouttwice as likely to retrieve erroneous documentswhen dealing with less popular entities than themost popular entity with the same name.
futuregoals include improving entity disambiguation ca-pabilities of retrievers, perhaps more directly in-corporating ideas from entity linking and corefer-ence resolution.
the amber sets and the codefor the generation pipeline is available at https://github.com/anthonywchen/amber-sets..acknowledgements.
we would like to thank jo daiber, michael tu,russ webb, matt gardner, robert logan, sherrytongshuang wu, and the anonymous reviewers forproviding valuable feedback for our work.
thiswork is funded in part by the darpa mcs pro-gram under contract no.
n660011924033 with theunited states ofﬁce of naval research..references.
himan abdollahpouri, robin burke, and bamshadmobasher.
2017. controlling popularity bias inin proceedingslearning-to-rank recommendation.
of the eleventh acm conference on recommendersystems, recsys 2017, como, italy, august 27-31,2017, pages 42–46.
acm..himan abdollahpouri, masoud mansoury, robinburke, and bamshad mobasher.
2019. the unfair-ness of popularity bias in recommendation.
arxivpreprint arxiv:1907.13286..k. balog, pavel serdyukov, and arjen p. de vries.
2010.overview of the trec 2010 entity track.
in trec..k. balog, pavel serdyukov, and arjen p. de vries.
2011.overview of the trec 2011 entity track.
in trec..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879, vancouver, canada.
association for computa-tional linguistics..j. chen, hande dong, xiao lei wang, fuli feng, ming-chieh wang, and x. he.
2020. bias and debiasin recommender system: a survey and future direc-tions.
arxiv preprint arxiv:2010.03240..giovanni luca ciampaglia, azadeh nematzadeh, fil-ippo menczer, and alessandro flammini.
2018.how algorithmic popularity bias hinders or pro-motes quality.
scientiﬁc reports, 8..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2368–2378, min-neapolis, minnesota.
association for computationallinguistics..hady elsahar, pavlos vougiouklis, arslen remaci,christophe gravier,jonathon hare, frederiquelaforest, and elena simperl.
2018. t-rex: a largescale alignment of natural language with knowledgebase triples.
in proceedings of the eleventh interna-tional conference on language resources and eval-uation (lrec 2018), miyazaki, japan.
europeanlanguage resources association (elra)..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthew pe-ters, michael schmitz, and luke zettlemoyer.
2018.allennlp: a deep semantic natural language pro-in proceedings of workshop forcessing platform.
nlp open source software (nlp-oss), pages 1–6, melbourne, australia.
association for computa-tional linguistics..johannes hoffart, stephan seufert, dat ba nguyen,martin theobald, and gerhard weikum.
2012.kore: keyphrase overlap relatedness for entity dis-in 21st acm international confer-ambiguation.
ence on information and knowledge management,cikm’12, maui, hi, usa, october 29 - november02, 2012, pages 545–554.
acm..johannes hoffart, mohamed amir yosef, ilaria bor-dino, hagen f¨urstenau, manfred pinkal, marc span-iol, bilyana taneva, stefan thater, and gerhardweikum.
2011. robust disambiguation of named en-tities in text.
in proceedings of the 2011 conferenceon empirical methods in natural language process-ing, pages 782–792, edinburgh, scotland, uk.
asso-ciation for computational linguistics..paloma jeretic, alex warstadt, suvrat bhooshan, andadina williams.
2020. are natural language infer-ence models imppressive?
learning implicature.
4480and presupposition.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8690–8705, online.
associationfor computational linguistics..vladimir karpukhin, barlas oguz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen, andwen-tau yih.
2020. dense passage retrieval foropen-domain question answering.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6769–6781, online.
association for computational lin-guistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:452–466..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 6086–6096, florence,italy.
association for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020a.
bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..patrick s. h. lewis, ethan perez, aleksandra pik-tus, fabio petroni, vladimir karpukhin, namangoyal, heinrich k¨uttler, mike lewis, wen-tau yih,tim rockt¨aschel, sebastian riedel, and douwekiela.
2020b.
retrieval-augmented generation forin advances inknowledge-intensive nlp tasks.
neural information processing systems 33: annualconference on neural information processing sys-tems 2020, neurips 2020, december 6-12, 2020,virtual..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..lajanugen logeswaran, ming-wei chang, kenton lee,kristina toutanova, jacob devlin, and honglak lee.
2019. zero-shot entity linking by reading entity de-scriptions.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 3449–3460, florence, italy.
association forcomputational linguistics..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticheuristics in natural language inference.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3428–3448,florence, italy.
association for computational lin-guistics..sewon min, julian michael, hannaneh hajishirzi, andluke zettlemoyer.
2020. ambigqa: answering am-biguous open-domain questions.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 5783–5797, online.
association for computational lin-guistics..aakanksha naik, abhilasha ravichander, normansadeh, carolyn rose, and graham neubig.
2018.stress test evaluation for natural language inference.
in proceedings of the 27th international conferenceon computational linguistics, pages 2340–2353,santa fe, new mexico, usa.
association for com-putational linguistics..yasumasa onoe and greg durrett.
2020. fine-grainedentity typing for domain independent entity linking.
in aaai..laurel orr, megan leszczynski, simran arora, senwu, neel guha, xiao ling, and christopher r´e.
2020. bootleg: chasing the tail with self-supervisedarxiv preprintnamed entity disambiguation.
arxiv:2010.10363..fabio petroni, aleksandra piktus, angela fan, patricklewis, majid yazdani, nicola de cao, jamesthorne, yacine jernite, vladimir karpukhin, jeanmaillard, vassilis plachouras, tim rockt¨aschel, andsebastian riedel.
2021. kilt: a benchmark forknowledge intensive language tasks.
in proceedingsof the 2021 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 2523–2544,online.
association for computational linguistics..vinodkumar prabhakaran, ben hutchinson, and mar-garet mitchell.
2019. perturbation sensitivity analy-sis to detect unintended model biases.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5740–5745, hongkong, china.
association for computational lin-guistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..delip rao, paul mcnamee, and mark dredze.
2013.entity linking: finding extracted entities in a knowl-in multi-source, multilingual informa-edge base.
tion extraction and summarization..4481ledell yu wu, f. petroni, martin josifoski, sebastianriedel, and luke zettlemoyer.
2020. zero-shot en-tity linking with dense entity retrieval.
in emnlp..jieyu zhao, tianlu wang, mark yatskar, vicente or-donez, and kai-wei chang.
2018. gender bias incoreference resolution: evaluation and debiasingin proceedings of the 2018 conferencemethods.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 2 (short papers), pages 15–20,new orleans, louisiana.
association for computa-tional linguistics..marco tulio ribeiro, carlos guestrin, and sameersingh.
2019. are red roses red?
evaluating con-sistency of question-answering models.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 6174–6184,florence, italy.
association for computational lin-guistics..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4902–4912, online.
association for computational lin-guistics..kyle richardson, h. hu, l. moss, and a. sabharwal.
2020. probing natural language inference modelsthrough semantic fragments.
in aaai..ozge sevgili, artem shelmanov, mikhail v. arkhipov,alexander panchenko, and christian biemann.
2020.neural entity linking: a survey of models based ondeep learning.
arxiv preprint arxiv:2006.00575..vered shwartz, rachel rudinger, and oyvind tafjord.
2020.
“you are grounded!”: latent name artifacts inpre-trained language models.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 6850–6861,online.
association for computational linguistics..james.
andreas vlachos,.
and arpit mittal..christosthorne,2018.christodoulopoulos,fever: a large-scale dataset for fact extractionin proceedings of the 2018and veriﬁcation.
conference ofthe north american chapter ofthe association for computational linguistics:human language technologies, volume 1 (longpapers), pages 809–819, new orleans, louisiana.
association for computational linguistics..denny vrandecic and m. kr¨otzsch.
2014. wikidata: afree collaborative knowledgebase.
commun.
acm,57:78–85..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of the as-sociation for computational linguistics, 8:377–392..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45.
associationfor computational linguistics..4482not “none”.
for fact checking, we train a three-way(i.e., supports, refutes, neutral) bert-base classiﬁer.
similar to slot ﬁlling, we train withthe top non-gold document retrieved by tf-idf asa negative document and train the model to classifythis negative document as neutral.
during infer-ence, we take the highest scoring prediction thatis not neutral.
when training baselines models,we do not tune over hyperparameters and train witha batch size of 32 for 3 epochs..appendix.
a top-20 retrieval results.
we provide results for top-20 retrieval in table 9.top-20 retrieval is used for providing documentsin the end-to-end evaluation setting.
in this set-ting, retrieval accuracy measures whether a golddocument appears in one of the top-20 retrieveddocuments.
similar to top-1 retrieval, retrieverscontinue to perform better on head queries..b task speciﬁc templates.
table 10 contains the templates used to instantiatethe task-speciﬁc inputs.
templates were writtenon a per-property basis.
we note that many of theproperties share templates that are very similar..c computational resources.
all experiments (e.g., training baselines, generatingamber sets, etc.)
were conducted on a machinewith 500 gb of ram, 64 cpus, and using annvidia titanrtx with 24 gb of ram.
retrievalon a collection of amber sets takes about 12 hoursfor the most time-consuming retriever, blink.
training a downstream model takes roughly 5hours and inference on a collection of amber setstakes less than 30 minutes..d retriever details.
for blink, dpr, and tf-idf, we use the retrievercode in the kilt repository released by facebook8.
for bootleg, we use the code provided by the hazyresearch group9..e downstream model details.
for question answering, we train a roberta-largemodel on natural questions.
we use the nega-tive documents in natural questions to train a “no-answer” classiﬁer using the [cls] token.
duringinference, we take the highest-scoring span wherethe answer is not classiﬁed as “no-answer”.
forslot ﬁlling, we train a bart-base model.
for eachslot ﬁlling instance, we train with the top non-golddocument retrieved by tf-idf as a negative doc-ument.
for this negative document, we train themodel to generate a “none” token, and during in-ference, we take the highest scoring answer that is.
8https://github.com/facebookresearch/.
kilt.
bootleg.
9https://github.com/hazyresearch/.
4483collection.
retriever.
fact checking.
slot filling.
question answering.
all head tail.
∀.
all head tail.
∀.
all head tail.
∀.
amber-h.amber-n.tf-idfdprblinkbootleg.
tf-idfdprblinkbootleg.
65.8 78.5 55.4 26.739.8 51.0 30.6 4.178.6 82.0 76.0 43.896.5 97.6 95.6 93.2.
72.0 83.5 62.5 55.626.6 37.0 18.16.873.3 73.9 72.8 64.696.6 97.7 95.7 93.6.
72.6 82.0 64.8 55.936.1 49.3 25.3 9.658.8 60.3 57.5 32.296.5 97.6 95.6 93.5.
50.8 57.0 44.1 12.062.3 75.8 47.7 27.833.5 38.7 27.9 1.379.3 80.2 78.4 61.5.
46.8 53.4 39.7 35.357.3 71.4 42.0 29.418.2 21.5 14.65.889.6 91.9 87.1 85.3.
52.0 59.1 44.4 40.763.4 77.9 47.8 37.274.7 80.6 68.3 53.083.8 83.6 84.1 71.1.table 9: top-20 retrieval results measuring retrieval accuracy and ∀..4484property.
instrument.
movement.
appears in.
question answering template.
fact checking template.
which musical instrument did $name play?
what musical instrument does $name play?
what instrument does $name play?.
$name plays the $object.
$name plays the musical instrument $object.
the $object is played by $name..what movement did $name participate in?
which movement is $name associated with?
what movement is $name associated with?.
what works does the ﬁctional entity $nameappear in?
what work is the character $name present in?
which work was the character $name in?.
doctoral student who were the doctoral students of $name?.
who are $name’s doctoral students?
who did $name advise?.
military branch what branch of the military does $name be-.
long to?
which military branch does $name belong to?
what military branch is $name afﬁliated with?.
sports position what is the position that $name plays?.
-.
hrebma.
$name was a member of the $object move-ment.
$name participated in the $object movement.
$name was a part of the $object movement..$name is a character in $object.
$name is a ﬁctional character in $object.
$object features the ﬁctional character $name..$name has a doctoral student named $object.
$name’s doctoral student is $object.
$name advised their student $object..$name is a member of the $object.
$name belongs to the military branch $object.
$name belongs to the $object branch of themilitary..$name plays the $object position.
$name plays as a $object..sports team.
battles or wars.
sport.
performer.
record label.
tracklist.
industry.
-.
nrebma.population.
cast member.
screenwriter.
# seasons.
author.
what position does $name play?
which position does $name play?.
$name plays for which team?
what team does $name play for?
which team does $name play for?.
what were the wars that $name participatedin?
which battle did $name ﬁght in?
which war did $name ﬁght?.
which sport does $name participate in?
which sport does $name play?
what sport does $name play?.
who performs $name?
who is the performer of $name?
who performed $name?.
what is the record label of $name.?
what is the record label for $name?
$name belongs to which record label?.
$name is a player on the $object.
$name plays for the $object team.
$name plays for the $object..$name fought in the $object.
$name fought in $object..$name plays $object.
$name plays the sport $object..$object performs in $name.
$object is the performer of $name .
$name was performed by $object..$object is the record label for $name.
$name’s record label is $object..what song appears in the album $name?
what song appears on $name?
what are the tracks in $name?.
$name belongs to $object tracklist.
$object is on the release of $name .
$object is a song in the $name tracklist..which industry is $name in?
in what industry is $name?
what is $name’s industry?.
what is the total population of $name?
what is the population of $name?
how many people live in $name?.
who acted in $name?
who is a cast member on $name?
who starred in $name?.
who was the screenwriter for $name?
who was screenwriter for $name?
who is $name’s screenwriter?.
how many seasons are there in $name?
how many seasons does $name have?
how many seasons were there in $name?.
who is the author of $name?
who wrote $name?
who authored $name?.
$name is in the industry of $object.
the company $name is in the $object industry.
$name’s industry is $object..the population of $name is $object.
$name’s population is $object.
$name has a population of $object..$object was a cast member in $name.
$object appeared in $name.
$object acted in $name..$name’s screenwriter is $object.
$object wrote the screenplay of $name.
$object screenwrote $name..there were $object seasons in $name.
$name has $object seasons..$name wrote $object.
$name is written by $object.
$object authored $name..table 10: templates used to instantiate the task-speciﬁc inputs..4485