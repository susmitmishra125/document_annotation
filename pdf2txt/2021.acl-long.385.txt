tail-to-tail non-autoregressive sequence prediction for chinesegrammatical error correction.
piji li shuming shitencent ai lab, shenzhen, china{pijili,shumingshi}@tencent.com.
abstract.
we investigate the problem of chinese gram-matical error correction (cgec) and presenta new framework named tail-to-tail (ttt)non-autoregressive sequence prediction to ad-dress the deep issues hidden in cgec.
con-sidering that mosttokens are correct andcan be conveyed directly from source to tar-get, and the error positions can be estimatedand corrected based on the bidirectional con-text information,thus we employ a bert-initialized transformer encoder as the back-bone model to conduct information modelingand conveying.
considering that only relyingon the same position substitution cannot han-dle the variable-length correction cases, vari-ous operations such substitution, deletion, in-sertion, and local paraphrasing are requiredjointly.
therefore, a conditional randomfields (crf) layer is stacked on the up tailto conduct non-autoregressive sequence pre-diction by modeling the token dependencies.
since most tokens are correct and easily tobe predicted/conveyed to the target, then themodels may suffer from a severe class imbal-ance issue.
to alleviate this problem, focalloss penalty strategies are integrated into theloss functions.
moreover, besides the typicalﬁx-length error correction datasets, we alsoconstruct a variable-length corpus to conductexperiments.
experimental results on stan-dard datasets, especially on the variable-lengthdatasets, demonstrate the effectiveness of tttin terms of sentence-level accuracy, precision,recall, and f1-measure on tasks of error de-tection and correction1..1.introduction.
grammatical error correction (gec) aims to au-tomatically detect and correct the grammatical er-rors that can be found in a sentence (wang et al.,2020c).
it is a crucial and essential application task.
1code: https://github.com/lipiji/ttt.
figure 1: illustration for the three types of operationsto correct the grammatical errors: type i-substitution;type ii-deletion and insertion; type iii-local paraphras-ing..in many natural language processing scenarios suchas writing assistant (ghufron and rosyida, 2018;napoles et al., 2017; omelianchuk et al., 2020),search engine (martins and silva, 2004; gao et al.,2010; duan and hsu, 2011), speech recognitionsystems (karat et al., 1999; wang et al., 2020a;kubis et al., 2020), etc.
grammatical errors mayappear in all languages (dale et al., 2012; xinget al., 2013; ng et al., 2014; rozovskaya et al.,2015; bryant et al., 2019), in this paper, we only fo-cus to tackle the problem of chinese grammaticalerror correction (cgec) (chang, 1995)..we investigate the problem of cgec and therelated corpora from sighan (tseng et al., 2015)and nlpcc (zhao et al., 2018) carefully, and weconclude that the grammatical error types as wellas the corresponding correction operations can becategorised into three folds, as shown in figure 1:(1) substitution.
in reality, pinyin is the most pop-ular input method used for chinese writings.
thus,the homophonous character confusion (for exam-ple, in the case of type i, the pronunciation of thewrong and correct words are both “feichang”) isthe fundamental reason which causes grammaticalerrors (or spelling errors) and can be corrected bysubstitution operations without changing the wholesequence structure (e.g., length).
thus, substitutionis a ﬁxed-length (fixlen) operation.
(2) deletion.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4973–4984august1–6,2021.©2021associationforcomputationallinguistics4973(cid:7625)(cid:10280)(cid:12560)(cid:12318)(cid:8495)(cid:26589)(cid:25726)(cid:27095)(cid:8307)(cid:28424)i feel fly long happy today!correct(cid:7625)(cid:10280)(cid:12560)(cid:8495)(cid:26205)(cid:11575)(cid:11575)(cid:27095)(cid:8307)(cid:28424)(cid:7625)(cid:10280)(cid:12560)(cid:12318)(cid:8495)(cid:26205)(cid:11575)(cid:27095)(cid:8307)(cid:28424)i feel very happy today!i am always happy wheni come to fei today!type itype iitype iii(cid:7625)(cid:10280)(cid:12560)(cid:26205)(cid:11575)(cid:12318)(cid:8495)(cid:27095)(cid:8307)(cid:28424)i very feel happy today!
(cid:26205)(cid:11575)(cid:12318)figure 2: illustration of the token information ﬂows from the bottom tail to the up tail..and insertion.
these two operations are used tohandle the cases of word redundancies and omis-sions respectively.
(3) local paraphrasing.
some-times, light operations such as substitution, dele-tion, and insertion cannot correct the errors directly,therefore, a slightly subsequence paraphrasing isrequired to reorder partial words of the sentence,the case is shown in type iii of figure 1. deletion,insertion, and local paraphrasing can be regarded asvariable-length (varlen) operations because theymay change the sentence length..however, over the past few years, although anumber of methods have been developed to dealwith the problem of cgec, some crucial and es-sential aspects are still uncovered.
generally, se-quence translation and sequence tagging are thetwo most typical technical paradigms to tacklethe problem of cgec.
beneﬁting from the devel-opment of neural machine translation (bahdanauet al., 2015; vaswani et al., 2017), attention-basedseq2seq encoder-decoder frameworks have beenintroduced to address the cgec problem in a se-quence translation manner (wang et al., 2018; geet al., 2018; wang et al., 2019, 2020b; kanekoet al., 2020).
seq2seq based translation modelsare easily to be trained and can handle all types ofcorrecting operations above mentioned.
however,considering the exposure bias issue (ranzato et al.,2016; zhang et al., 2019), the generated resultsusually suffer from the phenomenon of hallucina-tion (nie et al., 2019; maynez et al., 2020) andcannot be faithful to the source text, even thoughcopy mechanisms (gu et al., 2016) are incorpo-rated (wang et al., 2019).
therefore, omelianchuket al.
(2020) and liang et al.
(2020) propose topurely employ tagging to conduct the problem ofgec instead of generation.
all correcting opera-tions such as deletion, insertion, and substitutioncan be guided by the predicted tags.
neverthe-less, the pure tagging strategy requires to extend.
the vocabulary v to about three times by adding“insertion-” and “substitution-” preﬁxes to the orig-inal tokens (e.g., “insertion-good”, “substitution-paper”) which decrease the computing efﬁciencydramatically.
moreover, the pure tagging frame-work needs to conduct multi-pass prediction untilno more operations are predicted, which is inefﬁ-cient and less elegant.
recently, many researchersﬁne-tune the pre-trained language models such asbert on the task of cgec and obtain reason-able results (zhao et al., 2019; hong et al., 2019;zhang et al., 2020b).
however, limited by thebert framework, most of them can only addressthe ﬁxed-length correcting scenarios and cannotconduct deletion, insertion, and local paraphrasingoperations ﬂexibly..moreover, during the investigations, we alsoobserve an obvious but crucial phenomenon forcgec that most words in a sentence are correctand need not to be changed.
this phenomenon isdepicted in figure 2, where the operation ﬂow isfrom the bottom tail to the up tail.
grey dash linesrepresent the “keep” operations and the red solidlines indicate those three types of correcting oper-ations mentioned above.
on one side, intuitively,the target cgec model should have the ability ofdirectly moving the correct tokens from bottom tailto up tail, then transformer(vaswani et al., 2017)based encoder (say bert) seems to be a preference.
on the other side, considering that almost all typi-cal cgec models are built based on the paradigmsof sequence tagging or sequence translation, maxi-mum likelihood estimation (mle) (myung, 2003)is usually used as the parameter learning approach,which in the scenario of cgec, will suffer froma severe class/tag imbalance issue.
however, noprevious works investigate this problem thoroughlyon the task of cgec..to conquer all above-mentioned challenges, wepropose a new framework named tail-to-tail non-.
4974(cid:7625)(cid:10280)(cid:12560)(cid:12318)(cid:8495)(cid:26589)(cid:25726)(cid:27095)(cid:8307)(cid:28424)i feel fly long happy today!
(cid:7625)(cid:10280)(cid:12560)(cid:8495)(cid:26205)(cid:11575)(cid:11575)(cid:27095)(cid:8307)(cid:28424)(cid:7625)(cid:10280)(cid:12560)(cid:12318)(cid:8495)(cid:26205)(cid:11575)(cid:27095)(cid:8307)(cid:28424)i am always happy when icome to fei today!
(cid:7625)(cid:10280)(cid:12560)(cid:26205)(cid:11575)(cid:12318)(cid:8495)(cid:27095)(cid:8307)(cid:28424)i very feel happy today!
(cid:7625)(cid:10280)(cid:12560)(cid:12318)(cid:8495)(cid:26205)(cid:11575)(cid:27095)(cid:8307)(cid:28424)(cid:7625)(cid:10280)(cid:12560)(cid:12318)(cid:8495)(cid:26205)(cid:11575)(cid:27095)(cid:8307)(cid:28424)i feel very happy today!type itype iitype iiifigure 3: the proposed tail-to-tail non-autoregressive sequence prediction framework (ttt)..autoregressive sequence prediction, which abbrevi-ated as ttt, for the problem of cgec.
speciﬁcally,to directly move the token information from thebottom tail to the up tail, a bert based sequenceencoder is introduced to conduct bidirectional rep-resentation learning.
in order to conduct substi-tution, deletion, insertion, and local paraphrasingsimultaneously, inspired by (sun et al., 2019), aconditional random fields (crf) (lafferty et al.,2001) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modelingthe dependencies among neighbour tokens.
focalloss penalty strategy (lin et al., 2020) is adoptedto alleviate the class imbalance problem consider-ing that most of the tokens in a sentence are notchanged.
in summary, our contributions are as fol-lows:.
• a new framework named tail-to-tail non-autoregressive sequence prediction (ttt) isproposed to tackle the problem of cgec.
• bert encoder with a crf layer is employedas the backbone, which can conduct substitu-tion, deletion, insertion, and local paraphras-ing simultaneously..• focal loss penalty strategy is adopted to alle-viate the class imbalance problem consideringthat most of the tokens in a sentence are notchanged..• extensive experiments on several benchmarkdatasets, especially on the variable-lengthgrammatical correction datasets, demonstratethe effectiveness of the proposed approach..2 the proposed ttt framework.
2.1 overview.
figure 3 depicts the basic components of our pro-posed framework ttt.
input is an incorrect sen-.
tence x = (x1, x2, .
.
.
, xt ) which contains gram-matical errors, where xi denotes each token (chi-nese character) in the sentence, and t is the lengthof x. the objective of the task grammatical errorcorrection is to correct all errors in x and gener-ate a new sentence y = (y1, y2, .
.
.
, yt (cid:48)).
here, itis important to emphasize that t is not necessaryequal to t (cid:48).
therefore, t (cid:48) can be =, >, or < t .
bidirectional semantic modeling and bottom-to-updirectly token information conveying are conductedby several transformer (vaswani et al., 2017) lay-ers.
a conditional random fields (crf) (laffertyet al., 2001) layer is stacked on the up tail to con-duct the non-autoregressive sequence generation bymodeling the dependencies among neighboring to-kens.
low-rank decomposition and beamed viterbialgorithm are introduced to accelerate the computa-tions.
focal loss penalty strategy (lin et al., 2020)is adopted to alleviate the class imbalance problemduring the training stage..2.2 variable-length inputsince the length t (cid:48) of the target sentence y isnot necessary equal to the length t of the inputsequence x. then in the training and inferencestage, different length will affect the complete-ness of the predicted sentence, especially whent < t (cid:48).
to handle this issue, several simple tricksare designed to pre-process the samples.
assum-ing x = (x1, x2, x3, <eos>): (1) when t = t (cid:48),i.e., y = (y1, y2, y3, <eos>), then do nothing; (2)when t > t (cid:48), say y = (y1, y2, <eos>), whichmeans that some tokens in x will be deleted duringcorrecting.
then in the training stage, we can padt − t (cid:48) special tokens <pad> to the tail of y tomake t = t (cid:48), then.
y = (y1, y2, <eos>, <pad>);.
4975(cid:1876)(cid:2869)(cid:1876)(cid:2870)(cid:1876)(cid:2871)<mask><mask><eos>embedding layermulti-head self-attention layerfeed-forward layer(cid:1867)(cid:2869)(cid:1877)(cid:2869)<eos>(cid:1867)(cid:2870)(cid:1867)(cid:2871)(cid:1867)(cid:2872)(cid:1867)(cid:2873)(cid:1867)(cid:2874)(cid:1877)(cid:2870)(cid:1877)(cid:2871)(cid:1877)(cid:2872)(cid:1877)(cid:2873)inputbottom tailtransformer layersconditional random fields (crf)up tailoutputnx(3) when t < t (cid:48), say.
2.4 non-autoregressive sequence prediction.
y = (y1, y2, y3, y4, y5, <eos>),.
which means that more information should be in-serted into the original sentence x. then, we willpad the special symbol <mask> to the tail of x toindicate that these positions possibly can be trans-lated into some new real tokens:.
x = (x1, x2, x3, <eos>, <mask>, <mask>)..2.3 bidirectional semantic modeling.
transformer layers (vaswani et al., 2017) are par-ticularly well suited to be employed to conduct thebidirectional semantic modeling and bottom-to-upinformation conveying.
as shown in figure 3, afterpreparing the input samples, an embedding layerand a stack of transformer layers initialized with apre-trained chinese bert (devlin et al., 2019) arefollowed to conduct the semantic modeling..speciﬁcally, for the input, we ﬁrst obtain therepresentations by summing the word embeddingswith the positional embeddings:.
h0.
t = ewt + ept.
(1).
where 0 is the layer index and t is the state index.
ew and ep are the embedding vectors for tokensand positions, respectively..then the obtained embedding vectors h0 arefed into several transformer layers.
multi-headself-attention is used to conduct bidirectional rep-resentation learning:.
(cid:1).
t ) + h1t.t = ln (cid:0)ffn(h1t = ln (cid:0)slf-att(q0.
h1h1q0 = h0wq.
t , k0, v0) + h0t.(cid:1).
k0, v0 = h0wk, h0wv.
(2).
where slf-att(·), ln(·), and ffn(·) representself-attention mechanism, layer normalization, andfeed-forward network respectively (vaswani et al.,2017).
note that our model is a non-autoregressivesequence prediction framework, thus we use allthe sequence states k0 and v0 as the attentioncontext.
then each node will absorb the contextinformation bidirectionally.
after l transformerlayers, we obtain the ﬁnal output representationvectors hl ∈ rmax(t,t (cid:48))×d..direct prediction the objective of our model isto translate the input sentence x which containsgrammatical errors into a correct sentence y .
then,since we have obtained the sequence representationvectors hl, we can directly add a softmax layerto predict the results, just similar to the methodsused in non-autoregressive neural machine trans-lation (gu and kong, 2020) and bert-based ﬁne-tuning framework for the task of grammatical errorcorrection (zhao et al., 2019; hong et al., 2019;zhang et al., 2020b)..speciﬁcally, a linear transformation layer isplugged in and softmax operation is utilized togenerate a probability distribution pdp(yt) over thetarget vocabulary v:.
st = h(cid:62).
t ws + bs.
pdp(yt) = softmax(st).
(3).
where ht ∈ rd, ws ∈ rd×|v|, bs ∈ r|v|, andst ∈ r|v|.
then we obtain the result for each statebased on the predicted distribution:.
y(cid:48)t = argmax(pdp(yt)).
(4).
however, although this direct prediction methodis effective on the ﬁxed-length grammatical errorcorrection problem, it can only conduct the same-positional substitution operation.
for complex cor-recting cases which require deletion, insertion, andlocal paraphrasing, the performance is unaccept-able.
this inferior performance phenomenon isalso discussed in the tasks of non-autoregressiveneural machine translation (gu and kong, 2020).
one of the essential reasons causing the inferiorperformance is that the dependency informationamong the neighbour tokens are missed.
there-fore, dependency modeling should be called backto improve the performance of generation.
natu-rally, linear-chain crf (lafferty et al., 2001) isintroduced to ﬁx this issue, and luckily, sun et al.
(2019) also employ crf to address the problemof non-autoregressive sequence generation, whichinspired us a lot..dependency modeling via crf then given theinput sequence x, under the crf framework, thelikelihood of the target sequence y with length t (cid:48).
4976is constructed as:.
pcrf (y |x) =.
1z(x).
exp.
(cid:32) t (cid:48)(cid:88).
t=1.
t (cid:48)(cid:88).
t=2.
s(yt) +.
t(yt−1, yt).
(cid:33).
and the loss function lcrf for crf-based depen-dency modeling is:.
lcrf = − log pcrf (y |x).
(8).
(5).
then the ﬁnal optimization objective is:.
where z(x) is the normalizing factor and s(yt)represents the label score of y at position t, whichcan be obtained from the predicted logit vectorst ∈ r|v| from eq.
(3), i.e., st(v yt), where v ytis the vocabulary index of token yt.
the valuet(yt−1, yt) = myt−1,yt denotes the transition scorefrom token yt−1 to yt where m ∈ r|v|×|v| is thetransition matrix, which is the core term to conductdependency modeling.
usually, m can be learnt asneural network parameters during the end-to-endtraining procedure.
however, |v| is typically verylarge especially in the text generation scenarios(more than 32k), therefore it is infeasible to obtainm and z(x) efﬁciently in practice.
to overcomethis obstacle, as the method used in (sun et al.,2019), we introduce two low-rank neural parametermetrics e1, e2 ∈ r|v|×dm to approximate the full-rank transition matrix m by:.
m = e1e(cid:62)2.
(6).
where dm (cid:28) |v|.
to compute the normalizingfactor z(x), the original viterbi algorithm (for-ney, 1973; lafferty et al., 2001) need to searchall paths.
to improve the efﬁciency, here we onlyvisit the truncated top-k nodes at each time stepapproximately (sun et al., 2019)..2.5 training with focal penalty.
considering the characteristic of the directlybottom-to-up information conveying of the taskcgec, therefore, both tasks, direct prediction andcrf-based dependency modeling, can be incorpo-rated jointly into a uniﬁed framework during thetraining stage.
the reasons are that, intuitively,direct prediction will focus on the ﬁne-grained pre-dictions at each position, while crf-layer will paymore attention to the high-level quality of the wholeglobal sequence.
we employ maximum likelihoodestimation (mle) to conduct parameter learningand treat negative log-likelihood (nll) as the lossfunction.
thus, the optimization objective for di-rect prediction ldp is:.
ldp = −.
log pdp(yt|x).
(7).
t (cid:48)(cid:88).
t=1.
l = ldp + lcrf.
(9).
as mentioned in section 1, one obvious but cru-cial phenomenon for cgec is that most words ina sentence are correct and need not to be changed.
considering that maximum likelihood estimationis used as the parameter learning approach in thosetwo tasks, then a simple copy strategy can lead to asharp decline in terms of loss functions.
then, in-tuitively, the grammatical error tokens which needto be correctly ﬁxed in practice, unfortunately, at-tract less attention during the training procedure.
actually, these tokens, instead, should be regardedas the focal points and contribute more to the opti-mization objectives.
however, no previous worksinvestigate this problem thoroughly on the task ofcgec..to alleviate this issue, we introduce a usefultrick, focal loss (lin et al., 2020) , into our lossfunctions for direct prediction and crf:.
t (cid:48)(cid:88).
t=1.
lﬂ.
dp = −.
(1 − pdp(yt|x))γ log pdp(yt|x).
crf = −(1 − pcrf (y |x))γ log pcrf (y |x)lﬂ.
(10).
where γ is a hyperparameter to control the penaltyweight.
it is obvious that lﬂdp is penalized on thetoken level, while lﬂcrf is weighted on the sam-ple level and will work in the condition of batch-training.
the ﬁnal optimization objective with fo-cal penalty strategy is:.
lﬂ = lﬂ.
dp + lﬂcrf.
(11).
2.6.inference.
during the inference stage, for the input sourcesentence x, we can employ the original |v| nodesviterbi algorithm to obtain the target global opti-mal result.
we can also utilize the truncated top-kviterbi algorithm for high computing efﬁciency(sun et al., 2019)..4977corpussighan15hybirdsettttset.
#train2,339274,039539,268.
#dev-3,1625,662.type#testfixlen1,1003,162fixlen5,662 varlen.
table 1: statistics of the datasets..3 experimental setup.
3.1 settings.
the core technical components of our proposed tttis transformer (vaswani et al., 2017) and crf (laf-ferty et al., 2001).
the pre-trained chinese bert-base model (devlin et al., 2019) is employed toinitialize the model.
to approximate the transitionmatrix in the crf layer, we set the dimension dof matrices e1 and e2 as 32. for the normalizingfactor z(x), we set the predeﬁned beam size k as64. the hyperparameter γ which is used to weightthe focal penalty term is set to 0.5 after parametertuning.
training batch-size is 100, learning rateis 1e − 5, dropout rate is 0.1. adam optimizer(kingma and ba, 2015) is used to conduct the pa-rameter learning..3.2 datasets.
the overall statistic information of the datasetsused in our experiments are depicted in table 1.sighan15 (tseng et al., 2015)2 this is a bench-mark dataset for the evaluation of cgec and itcontains 2,339 samples for training and 1,100 sam-ples for testing.
as did in some typical previousworks (wang et al., 2019; zhang et al., 2020b), wealso use the sighan15 testset as the benchmarkdataset to evaluate the performance of our mod-els as well as the baseline methods in ﬁxed-length(fixlen) error correction settings.
hybirdset (wang et al., 2018)3 it is a newly re-leased dataset constructed according to a preparedconfusion set based on the results of asr (yu anddeng, 2014) and ocr (tong and evans, 1996).
this dataset contains about 270k paired samplesand it is also a fixlen dataset.
tttset considering that datasets of sighan15and hybirdset are all fixlen type datasets, inorder to demonstrate the capability of our modeltit on the scenario of variable-length (varlen)cgec, based on the corpus of hybirdset, we.
2http://ir.itc.ntnu.edu.tw/lre/.
sighan8csc.html.
3https://github.com/wdimmy/.
automatic-corpus-generation.
build a new varlen dataset.
speciﬁcally, opera-tions of deletion, insertion, and local shufﬂing areconducted on the original sentences to obtain theincorrect samples.
each operation covers one-thirdof samples, thus we get about 540k samples ﬁnally..3.3 comparison methods.
we compare the performance of ttt with sev-eral strong baseline methods on both fixlen andvarlen settings.
ntou employs n-gram language model with areranking strategy to conduct prediction (tsenget al., 2015).
nctu-ntut also uses crf to conduct label de-pendency modeling (tseng et al., 2015).
hanspeller++ employs hidden markov modelwith a reranking strategy to conduct the predic-tion (zhang et al., 2015).
hybrid utilizes lstm-based seq2seq frameworkto conduct generation (wang et al., 2018) andconfusionset introduces a copy mechanism intoseq2seq framework (wang et al., 2019).
faspell incorporates bert into the seq2seq forbetter performance (hong et al., 2019).
softmask-bert ﬁrstly conducts error detectionusing a gru-based model and then incorporatingthe predicted results with the bert model using asoft-masked strategy (zhang et al., 2020b).
notethat the best results of softmask-bert are ob-tained after pre-training on a large-scale datasetwith 500m paired samples.
spellgcn proposes to incorporate phonologicaland visual similarity knowledge into languagemodels via a specialized graph convolutional net-work (cheng et al., 2020).
chunk proposes a chunk-based decoding methodwith global optimization to correct single characterand multi-character word typos in a uniﬁed frame-work (bao et al., 2020)..we also implement some classical methods forcomparison and ablation analysis, especially forthe varlen correction problem.
transformer-s2sis the typical transformer-based seq2seq frame-work for sequence prediction (vaswani et al.,2017).
gpt2-ﬁnetune is also a sequence genera-tion framework ﬁne-tuned based on a pre-trainedchinese gpt2 model4 (radford et al., 2019; li,2020).
bert-ﬁnetune is just ﬁne-tune the chi-nese bert model on the cgec corpus directly.
beam search decoding strategy is employed to con-.
4https://github.com/lipiji/guyu.
4978model.
ntou (2015)nctu-ntut (2015)hanspeller++ (2015)hybird (2018)faspell (2019)confusionset (2019)softmask-bert (2020b)chunk (2020)spellgcn (2020)transformer-s2s (sec.3.3)gpt2-ﬁnetune (sec.3.3)bert-ﬁnetune (sec.3.3)ttt (sec.2).
model.
transformer-s2s (sec.3.3)gpt2-ﬁnetune (sec.3.3)bert-ﬁnetune (sec.3.3)ttt (sec.2).
acc.
42.260.170.1-74.2-80.976.8-67.065.175.482.7.acc.
25.651.346.855.6.detection.
prec.
42.271.780.356.667.666.873.788.174.873.170.084.185.4.rec.
41.833.653.369.460.073.173.262.080.752.251.961.578.1.f142.045.764.062.363.569.873.572.877.750.959.471.181.6.detection.
prec.
65.685.289.089.8.rec.
16.147.938.950.4.f125.961.354.164.6.acc.
39.056.469.2-73.7-77.474.6-66.264.671.681.5.acc.
24.645.136.960.6.correctionprec.
38.166.379.7-66.671.566.787.372.172.569.182.285.0.rec.
35.226.151.5-59.159.566.257.677.750.650.753.975.6.correctionprec.
63.682.884.888.5.rec.
14.840.226.744.2.f136.637.562.557.162.664.966.469.475.959.658.565.180.0.f124.054.140.758.9.table 2: detection and correction results evaluated on the sighan2015 testset (1100 samples)..table 3: detection and correction results evaluated on the tttset testset (5662 samples)..duct generation for transformer-s2s and gpt2-ﬁnetune, and beam-size is 5. note that some of theoriginal methods above mentioned can only workin the fixlen settings, such as softmask-bertand bert-ﬁnetune..3.4 evaluation metrics.
following the typical previous works (wang et al.,2019; hong et al., 2019; zhang et al., 2020b), weemploy sentence-level accuracy, precision, re-call, and f1-measure as the automatic metrics toevaluate the performance of all systems5.
we alsoreport the detailed results for error detection (alllocations of incorrect characters in a given sen-tence should be completely identical with the goldstandard) and correction (all locations and corre-sponding corrections of incorrect characters shouldbe completely identical with the gold standard) re-spectively (tseng et al., 2015)..4 results and discussions.
4.1 results in fixlen scenario.
table 2 depicts the main evaluation results of ourproposed framework ttt as well as the compar-ison baseline methods.
it should be emphasized.
5http://nlp.ee.ncu.edu.tw/resource/csc..html.
that softmask-bert is pre-trained on a 500m-size paired dataset.
our model ttt, as well asthe baseline methods such as transformer-s2s,gpt2-ﬁnetune, bert-ﬁnetune, and hybird areall trained on the 270k-size hybirdset.
neverthe-less, ttt obtains improvements on the tasks oferror detection (f1:77.7 → 81.6) and correction(f1:75.9 → 80.0) compared to all strong baselineson f1 metric, which indicates the superiority of ourproposed approach..4.2 results in varlen scenario.
beneﬁt from the crf-based dependency modelingcomponent, ttt can conduct deletion, insertion,local paraphrasing operations jointly to address thevariable-length (varlen) error correction problem.
the experimental results are described in table 3.considering that those sequence generation meth-ods such as transformer-s2s and gpt2-ﬁnetunecan also conduct varlen correction operation, thuswe report their results as well.
from the results, wecan observe that ttt can also achieve a superiorperformance in the varlen scenario.
the reasonsare clear: bert-ﬁnetune as well as the relatedmethods are not appropriate in varlen scenario,especially when the target is longer than the input.
the text generation models such as transformer-s2s and gpt2-ﬁnetune suffer from the problem ofhallucination (maynez et al., 2020) and repetition,.
4979trainset.
model.
sighan15.
transformer-s2sgpt2-ﬁnetunebert-ﬁnetunettttransformer-s2sgpt2-ﬁnetunebert-ﬁnetunettt.
acc.
46.545.235.851.367.065.175.482.7.detection.
prec.
42.242.334.150.673.170.084.185.4.rec.
23.630.832.838.052.251.961.578.1.f130.335.733.443.450.959.471.181.6.acc.
43.442.631.345.866.264.671.681.5.correctionprec.
34.937.727.141.972.569.182.285.0.rec.
17.325.523.626.750.650.753.975.6.hybirdset.
table 4: performance of models trained on different datasets..trainset.
model.
sighan15.
hybirdset.
ttt w/o lcrfttt w/o ldptttttt w/o lcrfttt w/o ldpttt.
acc.
35.835.542.675.481.282.7.detection.
prec.
34.132.039.484.183.485.6.rec.
32.828.031.561.577.177.9.f133.429.935.071.180.181.5.acc.
31.331.236.771.680.081.1.correctionprec.
27.124.928.982.283.085.0.rec.
23.619.323.653.974.774.7.f123.230.425.332.759.658.565.180.0.f125.321.626.065.178.679.5.trainset.
sighan15.
hybirdset.
table 5: ablation analysis of ldp and lcrf ..γ.
0.00.10.51.02.05.00.00.10.51.02.05.0.acc.
42.648.851.351.850.048.982.774.682.781.179.280.3.detection.
prec.
39.447.050.651.348.647.185.673.585.483.280.481.6.rec.
31.535.538.037.736.337.277.975.478.077.176.277.3.f135.040.343.443.541.547.681.574.481.680.078.279.4.acc.
36.743.845.846.344.442.881.173.281.580.078.278.7.correctionprec.
28.938.741.942.539.537.685.072.785.082.880.080.9.rec.
23.625.126.726.525.025.174.772.675.674.974.174.1.f126.030.432.632.630.630.679.572.780.078.676.977.4.table 6: tuning for focal loss hyperparameter γ..which are not steady on the problem of cgec..eling, can indeed improve the performance..4.3 ablation analysis.
different training dataset recall that we intro-duce several groups of training datasets in differentscales as depicted in table 1. it is also very inter-esting to investigate the performances on different-size datasets.
then we conduct training on thosetraining datasets and report the results still on thesighan2015 testset.
the results are shown intable 4. no matter what scale of the dataset is, tttalways obtains the best performance..impact of ldp and lcrf table 5 describes theperformance of our model ttt and the variantswithout ldp (ttt w/o ldp) and lcrf (ttt w/o lcrf ).
we can conclude that the fusion of these two tasks,direct prediction and crf-based dependency mod-.
parameter tuning for focal loss the focalloss penalty hyperparameter γ is crucial for the lossfunction l = ldp + lcrf and should be adjustedon the speciﬁc tasks (lin et al., 2020).
we conductgrid search for γ ∈ (0, 0.1, 0.5, 1, 2, 5) and the cor-responding results are provided in table 6. finally,we select γ = 0.5 for ttt for the cgec task..4.4 computing efﬁciency analysis.
practically, cgec is an essential and useful taskand the techniques can be used in many real appli-cations such as writing assistant, post-processingof asr and ocr, search engine, etc.
therefore,the time cost efﬁciency of models is a key pointwhich needs to be taken into account.
table 7 de-picts the time cost per sample of our model ttt and.
4980modeltransformer-s2sgpt2-ﬁnetunetttbert-ﬁnetune.
time (ms)815.40552.8239.2514.72.speedup1x1.47x20.77x55.35x.
table 7: comparisons of the computing efﬁciency..some baseline approaches.
the results demonstratethat ttt is a cost-effective method with superiorprediction performance and low computing timecomplexity, and can be deployed online directly..5 conclusion.
we propose a new framework named tail-to-tailnon-autoregressive sequence prediction, which ab-breviated as ttt, for the problem of cgec.
abert based sequence encoder is introduced toconduct bidirectional representation learning.
in or-der to conduct substitution, deletion, insertion, andlocal paraphrasing simultaneously, a crf layer isstacked on the up tail to conduct non-autoregressivesequence prediction by modeling the dependenciesamong neighbour tokens.
low-rank decompositionand a truncated viterbi algorithm are introducedto accelerate the computations.
focal loss penaltystrategy is adopted to alleviate the class imbalanceproblem considering that most of the tokens in asentence are not changed.
experimental resultson standard datasets demonstrate the effectivenessof ttt in terms of sentence-level accuracy, pre-cision, recall, and f1-measure on tasks of errordetection and correction.
ttt is of low computingcomplexity and can be deployed online directly..in the future, we plan to introduce more lexicalanalysis knowledge such as word segmentation andﬁne-grained named entity recognition (zhang et al.,2020a) to further improve the performance..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..zuyi bao, chen li, and rui wang.
2020. chunk-basedchinese spelling check with global optimization.
inproceedings of the 2020 conference on empiricalmethods in natural language processing: findings,emnlp 2020, online event, 16-20 november 2020,.pages 2031–2040.
association for computationallinguistics..christopher bryant, mariano felice, øistein e. an-dersen, and ted briscoe.
2019. the bea-2019shared task on grammatical error correction.
in pro-ceedings of the fourteenth workshop on innovativeuse of nlp for building educational applications,bea@acl 2019, florence, italy, august 2, 2019,pages 52–75.
association for computational lin-guistics..chao-huang chang.
1995. a new approach for auto-in proceedingsmatic chinese spelling correction.
of natural language processing paciﬁc rim sympo-sium, pages 278–283.
citeseer..spellgcn:.
xingyi cheng, weidi xu, kunlong chen, shaohuajiang, feng wang, taifeng wang, wei chu, andyuan qi.
2020.incorporating phono-logical and visual similarities into language modelsin proceedings of thefor chinese spelling check.
58th annual meeting of the association for com-putational linguistics, acl 2020, online, july 5-10, 2020, pages 871–881.
association for compu-tational linguistics..robert dale, ilya anisimoff, and george narroway.
2012. hoo 2012: a report on the preposition anddeterminer error correction shared task.
in proceed-ings of the seventh workshop on building educa-tional applications using nlp, bea@naacl-hlt2012, june 7, 2012, montr´eal, canada, pages 54–62.
the association for computer linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..huizhong duan and bo-june paul hsu.
2011. onlinein pro-spelling correction for query completion.
ceedings of the 20th international conference onworld wide web, www 2011, hyderabad, india,march 28 - april 1, 2011, pages 117–126.
acm..g david forney.
1973. the viterbi algorithm.
proceed-.
ings of the ieee, 61(3):268–278..jianfeng gao, xiaolong li, daniel micol, chris quirk,and xu sun.
2010. a large scale ranker-based sys-in col-tem for search query spelling correction.
ing 2010, 23rd international conference on com-putational linguistics, proceedings of the confer-ence, 23-27 august 2010, beijing, china, pages 358–366. tsinghua university press..tao ge, furu wei, and ming zhou.
2018. reach-ing human-level performance in automatic grammat-ical error correction: an empirical study.
corr,abs/1807.01270..4981m ali ghufron and fathia rosyida.
2018. the roleof grammarly in assessing english as a foreign lan-guage (eﬂ) writing.
lingua cultura, 12(4):395–403..jiatao gu and xiang kong.
2020..fully non-autoregressive neural machine translation: tricks ofthe trade.
corr, abs/2012.15833..jiatao gu, zhengdong lu, hang li, and victor o. k.incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association forcomputational linguistics, acl 2016, august 7-12,2016, berlin, germany, volume 1: long papers.
theassociation for computer linguistics..yuzhong hong, xianguo yu, neng he, nan liu,and junhui liu.
2019.faspell: a fast, adapt-able, simple, powerful chinese spell checker basedin proceedings of theon dae-decoder paradigm.
5th workshop on noisy user-generated text, w-nut@emnlp 2019, hong kong, china, november4, 2019, pages 160–169.
association for computa-tional linguistics..masahiro kaneko, masato mita, shun kiyono, junsuzuki, and kentaro inui.
2020. encoder-decodermodels can beneﬁt from pre-trained masked lan-guage models in grammatical error correction.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 4248–4254.
associa-tion for computational linguistics..clare-marie karat, christine halverson, daniel b.horn, and john karat.
1999. patterns of entry andcorrection in large vocabulary continuous speechin proceeding of the chi ’99recognition system.
conference on human factors in computing sys-tems: the chi is the limit, pittsburgh, pa, usa,may 15-20, 1999, pages 568–575.
acm..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..marek kubis, zygmunt vetulani, mikolaj wypych, andtomasz zietkiewicz.
2020. open challenge for cor-recting errors of speech recognition systems.
corr,abs/2001.03041..deng liang, chen zheng, lei guo, xin cui, xiuzhangxiong, hengqiao rong, and jinpeng dong.
2020.bert enhanced neural machine translation and se-quence tagging model for chinese grammatical er-ror diagnosis.
in proceedings of the 6th workshopon natural language processing techniques for ed-ucational applications, pages 57–66, suzhou, china.
association for computational linguistics..tsung-yi lin, priya goyal, ross b. girshick, kaim-ing he, and piotr doll´ar.
2020. focal loss for denseobject detection.
ieee trans.
pattern anal.
mach.
intell., 42(2):318–327..bruno martins and m´ario j. silva.
2004. spellingin advancescorrection for search engine queries.
in natural language processing, 4th internationalconference, estal 2004, alicante, spain, october20-22, 2004, proceedings, volume 3230 of lec-ture notes in computer science, pages 372–383.
springer..joshua maynez, shashi narayan, bernd bohnet, andryan t. mcdonald.
2020. on faithfulness and fac-tuality in abstractive summarization.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, acl 2020, online, july5-10, 2020, pages 1906–1919.
association for com-putational linguistics..in jae myung.
2003. tutorial on maximum likelihoodjournal of mathematical psychology,.
estimation.
47(1):90–100..courtney napoles, keisuke sakaguchi, and joel r.tetreault.
2017.jfleg: a ﬂuency corpus andbenchmark for grammatical error correction.
in pro-ceedings of the 15th conference of the europeanchapter of the association for computational lin-guistics, eacl 2017, valencia, spain, april 3-7,2017, volume 2: short papers, pages 229–234.
as-sociation for computational linguistics..hwee tou ng, siew mei wu, ted briscoe, christianhadiwinoto, raymond hendy susanto, and christo-pher bryant.
2014. the conll-2014 shared task ongrammatical error correction.
in proceedings of theeighteenth conference on computational naturallanguage learning: shared task, conll 2014, bal-timore, maryland, usa, june 26-27, 2014, pages 1–14. acl..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning (icml2001), williams college, williamstown, ma, usa,june 28 - july 1, 2001, pages 282–289.
morgankaufmann..feng nie, jin-ge yao, jinpeng wang, rong pan, andchin-yew lin.
2019. a simple recipe towards re-ducing hallucination in neural surface realisation.
in proceedings of the 57th conference of the as-sociation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume1: long papers, pages 2673–2679.
association forcomputational linguistics..piji li.
2020. an empirical investigation of pre-trainedtransformer language models for open-domain dia-logue generation.
corr, abs/2003.04195..kostiantyn omelianchuk, vitaliy atrasevych, artem n.chernodub, and oleksandr skurzhanskyi.
2020.gector - grammatical error correction: tag, not.
4982in proceedings of.
the fifteenth work-rewrite.
shop on innovative use of nlp for building educa-tional applications, bea@acl 2020, online, july10, 2020, pages 163–170.
association for computa-tional linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..marc’aurelio ranzato, sumit chopra, michael auli,and wojciech zaremba.
2016. sequence level train-in 4th inter-ing with recurrent neural networks.
national conference on learning representations,iclr 2016, san juan, puerto rico, may 2-4, 2016,conference track proceedings..alla rozovskaya, houda bouamor, nizar habash, wa-jdi zaghouani, ossama obeid, and behrang mohit.
2015. the second qalb shared task on automaticin proceedings of thetext correction for arabic.
second workshop on arabic natural language pro-cessing, anlp@acl 2015, beijing, china, july 30,2015, pages 26–35.
association for computationallinguistics..zhiqing sun, zhuohan li, haoqing wang, di he,zi lin, and zhi-hong deng.
2019. fast structureddecoding for sequence models.
in advances in neu-ral information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 3011–3020..xiang tong and david a. evans.
1996. a statisticalapproach to automatic ocr error correction in con-in fourth workshop on very large corpora,text.
vlc@coling 1996, copenhagen, denmark, au-gust 4, 1996..yuen-hsien tseng, lung-hao lee, li-ping chang, andhsin-hsi chen.
2015.introduction to sighanin2015 bake-off for chinese spelling check.
proceedings of the eighth sighan workshop onchinese language processing, sighan@ijcnlp2015, beijing, china, july 30-31, 2015, pages 32–37. association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..dingmin wang, yan song, jing li, jialong han, andhaisong zhang.
2018. a hybrid approach to auto-matic corpus generation for chinese spelling check.
in proceedings of the 2018 conference on empiricalmethods in natural language processing, brussels,belgium, october 31 - november 4, 2018, pages2517–2527.
association for computational linguis-tics..dingmin wang, yi tay,.
and li zhong.
2019.confusionset-guided pointer networks for chinesespelling check.
in proceedings of the 57th confer-ence of the association for computational linguis-tics, acl 2019, florence, italy, july 28- august 2,2019, volume 1: long papers, pages 5780–5785.
as-sociation for computational linguistics..haoyu wang, shuyan dong, yue liu, james logan,ashish kumar agrawal, and yang liu.
2020a.
asrerror correction with augmented transformer for en-tity retrieval.
in interspeech 2020, 21st annual con-ference of the international speech communicationassociation, virtual event, shanghai, china, 25-29october 2020, pages 1550–1554.
isca..hongfei wang, michiki kurosawa, satoru katsumata,and mamoru komachi.
2020b.
chinese grammat-ical correction using bert-based pre-trained model.
in proceedings of the 1st conference of the asia-the association for compu-paciﬁc chapter oftational linguistics and the 10th internationaljoint conference on natural language processing,aacl/ijcnlp 2020, suzhou, china, december 4-7, 2020, pages 163–168.
association for computa-tional linguistics..yu wang, yuelin wang, jie liu, and zhuo liu.
2020c.
a comprehensive survey of grammar error correc-tion.
corr, abs/2005.06600..junwen xing, longyue wang, derek f. wong, lidia s.chao, and xiaodong zeng.
2013. um-checker: ahybrid system for english grammatical error correc-in proceedings of the seventeenth confer-tion.
ence on computational natural language learning:shared task, conll 2013, soﬁa, bulgaria, august8-9, 2013, pages 34–42.
acl..dong yu and li deng.
2014. automatic speech recog-.
nition: a deep learning approach.
springer..haisong zhang, lemao liu, haiyun jiang, yangmingli, enbo zhao, kun xu, linfeng song, suncongzheng, botong zhou, jianchen zhu, xiao feng, taochen, tao yang, dong yu, feng zhang, zhanhuikang, and shuming shi.
2020a.
texsmart: a textunderstanding system for ﬁne-grained ner and en-hanced semantic analysis.
corr, abs/2012.15639..shaohua zhang, haoran huang, jicong liu, and hangli.
2020b.
spelling error correction with soft-masked bert.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages882–890.
association for computational linguis-tics..shuiyuan zhang, jinhua xiong, jianpeng hou, qiaozhang, and xueqi cheng.
2015. hanspeller++: auniﬁed framework for chinese spelling correction.
in proceedings of the eighth sighan workshop onchinese language processing, sighan@ijcnlp2015, beijing, china, july 30-31, 2015, pages 38–45. association for computational linguistics..4983wen zhang, yang feng, fandong meng, di you, andqun liu.
2019. bridging the gap between trainingand inference for neural machine translation.
in pro-ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 4334–4343.
association for computa-tional linguistics..wei zhao, liang wang, kewei shen, ruoyu jia, andimproving grammatical er-jingming liu.
2019.ror correction via pre-training a copy-augmented ar-in proceedings ofchitecture with unlabeled data.
the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, naacl-hlt 2019,minneapolis, mn, usa, june 2-7, 2019, volume 1(long and short papers), pages 156–165.
associa-tion for computational linguistics..yuanyuan zhao, nan jiang, weiwei sun, and xiao-jun wan.
2018. overview of the nlpcc 2018shared task: grammatical error correction.
in nat-ural language processing and chinese computing- 7th ccf international conference, nlpcc 2018,hohhot, china, august 26-30, 2018, proceedings,part ii, volume 11109 of lecture notes in computerscience, pages 439–445.
springer..4984