edited media understanding frames: reasoning about theintents and implications of visual disinformation.
jeﬀ da♠ maxwell forbes♠♥ rowan zellers♠♥ anthony zheng♣jena d. hwang♠ antoine bosselut♠♦ yejin choi♠♥♠allen institute for artiﬁcial intelligence ♣university of michigan♥paul g. allen school of computer science & engineering, university of washington♦stanford university.
jeffda.com/edited-media-understanding.
abstract.
understanding manipulated media, from auto-matically generated ‘deepfakes’ to manuallyedited ones, raises novel research challenges.
because the vast majority of edited or manipu-lated images are benign, such as photoshoppedimages for visual enhancements, the key chal-lenge is to understand the complex layers ofunderlying intents of media edits and their im-plications with respect to disinformation..in this paper, we study edited media under-standing frames, a new conceptual formal-ism to understand visual media manipulationas structured annotations with respect to the in-tents, emotional reactions, eﬀects on individu-als, and the overall implications of disinforma-tion.
we introduce a dataset for our task, emu,with 56k question-answer pairs written in richnatural language.
we evaluate a wide vari-ety of vision-and-language models for our task,and introduce a new model pelican, whichbuilds upon recent progress in pretrained mul-timodal representations.
our model obtainspromising results on our dataset, with humansrating its answers as accurate 48.2% of thetime.
at the same time, there is still muchwork to be done – and we provide analysis thathighlights areas for further progress..figure 1: edited media understanding frames.
given a manipulated image and its source, a modelmust generate natural language answers to a set ofopen-ended questions.
our questions test the under-standing of the what and why behind important changesin the image – like that subject1 appears to be on goodterms with subject2..1.introduction.
the modern ubiquity of powerful image-editingsoftware has led to a variety of new disinforma-tion threats.
from ai-enabled “deepfakes” to low-skilled “cheapfakes,” attackers edit media to en-gage in a variety of harmful behaviors, such asspreading disinformation, creating revenge porn,and committing fraud (paris and donovan, 2019;chesney and citron, 2019; kietzmann et al., 2020,c.f.).
accordingly, we argue that it is importantto develop systems to help spot harmful manipu-lated media.
the rapid growth and virality of social.
media requires as such, especially as social mediatrends towards visual content (gretzel, 2017)..identifying whether an image or video has beendigitally altered (i.e., “digital forgery detection”)has been a long-standing problem in the computervision and media forensics communities.
this hasenabled the development of a suite of detectionapproaches, such as analyzing pixel-level statisticsand compression artifacts (farid, 2009; bianchiand piva, 2012; bappy et al., 2017) or identifying.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2026–2039august1–6,2021.©2021associationforcomputationallinguistics2026i want to suggest that subject2has the support of subject1.subject1subject2i feel mad, because we’re deﬁnitely not friends...how subject1 might feelit seems like subject2 and subject1 are allies!originaleditedbackground changedperson introducedhorizontally ﬂippedprevious work:understanding the structure of edits(jhamtani et al., 2018; tan et al., 2019)this paper: understanding the intent and                     implications of editswhy did the editor create this?how the community might respondframe: emotionframe: disinformationframe: intent“what” the edit was (tan et al., 2019)..between the best machine and human accuracy..however, little work has been done on “why”an edit is made, which is necessary for identifyingharm.
darkening someone’s skin in a family photobecause background light made them seem quitepale is generally harmless.
while such color re-balancing is common, darkening barack obama’s(or rafael warnock’s) skin in campaign ads wasclearly meant as a harmful edit by the editor that didit.1 we choose to focus on the “why” – we deﬁne aschema for approaching the problem of intent andprovide a rich set of natural language responses.
we also make a signiﬁcant contribution towardsthe “what:” we include a physical-change question,provide rationales based in physical changes, andgive structured annotations (bounding boxes) onwhat was changed in the edit..we introduce edited media understandingframes (emu), a new conceptual formalism thatcaptures the notions of “why” and “what” in imageediting for language and vision systems (figure1).
following literature on pragmatic frames (sapet al., 2017, 2020; forbes et al., 2020)—derivedfrom frame semantics (baker et al., 1998)— weformalize emu frames along six dimensions thatcover a diverse range of inferences necessary tofully capture the scope of visual disinformation.
we delve into the concept of intention as discussedby the fake news literature (rashkin et al., 2017;shu et al., 2017; zhou and zafarani, 2020) to cap-ture editor’s intent such as motivation for edit andintent to deceive, as well as the resulting implica-tions of the edited content.
for every dimension wecollect both a classiﬁcation label and a free-formtext explanation.
for example, for frame intent,a model must classify the intent of the edit, anddescribe why this classiﬁcation is selected..we then introduce a new dataset for our task,emu, with 56k annotations over 8k image pairs.
to kickstart progress on our task, we introduce anew language and vision model, pelican, thatleverages recent progress in pretrained multimodalrepresentations of images and text (tan and bansal,2019; lu et al., 2019; li et al., 2019).
we compareour model to a suite of strong baselines, including astandard vlp model (zhou et al., 2019), and showkey improvement in terms of ability to reason aboutco-referent subjects in the edit.
nevertheless, ourtask is far from solved: a signiﬁcant gap remains.
1how georgia’s senate race pits the old south againstthe new south.
https://www.politico.com/news/2020/12/05/georgia-senate-old-new-south-442423.
our contributions are thus as follows.
first, weintroduce a new task of edited media understand-ing frames, which requires a deep understandingof why an image was edited, and a correspondingdataset, emu, with 56k captions that cover diverseinferences.
in addition, we introduce a new model,pelican, improving over competitive language-and-vision transformer baselines.
our empiricalstudy demonstrates promising results, but signiﬁ-cant headroom remains.
we release our dataset atjeffda.com/edited-media-understandingto encourage further study in discovering pragmaticmarkers of disinformation..2 deﬁning edited media understanding.
frames.
through an edit e on source image i (e.g.
“e = xis edited into a room full of drugs”), an editor cancause harm to the subject x’s mental state (mentalstate: “x is angry about e”) and eﬀect x’s image (ef-fect: “e makes x seem dishonest”) (rashkin et al.,2016).
the editor does this through the intention ofthe edit (intent: “e intends to harm x’s image”) andchanging the implications of the image (implica-tion: “e frames x as a drug cartel member”) (forbeset al., 2020; sap et al., 2020; paris and donovan,2019)..to this end, we collect edits e and sourceimages i from reddit’s r/photoshopbattles com-munity.
there is no readily available (large)central database of harmfulimage edits, butr/photoshopbattles is replete with suitable com-plex and culturally implicative edits (e.g., referenceto politics or pop culture).
this provides us withrelevant image edits at a reasonable cost withoutadvocation for dangerous training on real harmfulimage edits.
keeping the source image i in thetask allows us to sustain the tractability of the im-age edit problem (tan et al., 2019; jhamtani andberg-kirkpatrick, 2018)..2.1 edited media understanding frames:.
task summary.
given an edit e: is → ie, we deﬁne an editedmedia understanding frame f (∗) as a collectionof typed dimensions and their polarity assign-ments: (i) physical p(is → ie): the changes fromis → ie, (ii) intent n(e → ie): whether the ed-itor e implied malicious intent in is → ie, (iii)implication m(e → ie): how e might use ie to.
2027figure 2: an example from emu.
given a source image and its edit, and a list of main subjects in the image,we collect a label l and natural language responses (reponse to frame y and rationale r to applicable open-endedquestions q covering each of ﬁve frames f ∈ f .
we also collect structural annotations ai highlighting the editedsections of the image..mislead, (iv) mental state s (ie → si): whetherthe predicate ie impacts the emotion of a role si,(v) eﬀect e(ie → si): the eﬀect of ie on si.
weassume frames can be categorized as harmful ornot harmful with polarity l ∈ {+, −}.
each polarityl can be interpreted with reason y, and that eachreason can be supported with rationale r..technically, a model is given the following as.
input:• a source image is , and an edited image ie.
• a list of important subjects: expressed as bound-.
ing boxes bi for each subject..• an open-ended question q associated with f (∗);e.g., “how might subject3 feel upon seeing thisedit?”.
• a list of annotated boxes ai ∈ ie marking theobjects in the image that were introduced andmodiﬁed, and a true/false label denoting if thebackground was changed..a model must produce the polarity classiﬁcationl(cid:48) ∈ {+, −}, interpretation of the polarity (responsey(cid:48)) and rationale for interpolation r(cid:48).
(for the phys-ical frame, only y needs to be generated).
figure 2shows an example of our task conﬁguration.
thelexicon of the label is ﬁxed for each f(∗) (e.g.
forn(∗), − → harmful, + → harmless)..3 emu: a corpus of edited media.
understanding frames.
sourcing image edits we source our image ed-its from the r/photoshopbattles community onreddit which hosts regular photoshop competi-tions, where given a source photo, members submita comment with their own edited photo..we collect 8k image edit pairs (source andedited photo pairs) from this community by, ﬁrst,manually curating a list of more than 100 terms de-scribing people frequently appearing in photoshopbattles posts.
then, we screen over 100k posts fortitles that contain one or more of these search termsresulting in 20k collected image pairs.
addition-ally, we run an object detector (he et al., 2017) toensure that is at least one person present in eachimage as a means for ensuring that annotators donot see image pairs without any subjects..annotating image edits we ask a group of vet-ted crowd workers to identify the main subjects inan image edit and answer open-ended questions innatural language.
each image is annotated by 3independent crowd workers..crowd workers are ﬁrst presented with a num-bered set of people bounding boxes (produced bymask r-cnn (he et al., 2017)) over the edited.
2028the editor creates image edit from image sourceframe n(e → ie): the editor has intentframe m(e → ie): the image edit has potential implicationsframe s(ie → s1): the image edit impacts mental state of subject 1frame e(ie → s1): the image edit has effect on perceptions of subject 1p(e → ie): the physical changes between image source and image editsubject 1 is edited into a room full of illicit drugs         background changedstructural changesintent: to show subject 1 in illegal behaviorimplications: to frame subject 1 as a member of a drug cartelmental state: subject 1 would be angryeffect: makes subject 1 seem like a dishonest leadersubject 1 is shown next to illegal substanceslabel: intent is harmfullabel: implications is harmfullabel: mental state of s1 is negativelabel: effect on s1 is negativesubject 1 is next to a massive amount of drugssubject 1 would not want to be seen in front of drugsshows subject 1 is in charge of illegal drugssubject 1because…because…because…because…image sourceemu framesframephysical.
notationrelated questionp(is → ie) what changed in this image.
intent.
n(e → ie) why would someone create.
edit?.
this edit?.
implication.
m(e → ie) how might this edit be used to.
mental[of subjectx]effectsubjectx].
state.
s (ie → si).
[on.
e(ie → si).
this image edit.
mislead?
how mightmake subjectx feel?
how could this edit misleadpublic perception of subjectx?.
table 1: questions for each of the frames in edited me-dia understanding frames.
each frame is associatedwith a question that allows human annotators to addressthe frame, and models to generate l, y, r for the givenframe..image and are asked to select subjects that are sig-niﬁcant to the edits (as opposed, say, a crowd in thebackground).
once subjects are selected, the anno-tators are asked to assign classiﬁcation labels foreach of the ﬁve possible question types and providefree-form text answers for each question (whenapplicable).
for the classiﬁcation label, we retainthe majority vote (fleiss κ = 0.67).
in a separateand ﬁnal pass, we explicitly identify which portionsof the modiﬁed image is introduced or altered byasking the workers to to label the most importantsections of the modiﬁed image and selecting oneof the two labels.
the statistics of the dataset areshown in figure 3..4 modeling edited media understanding.
frames.
in this section, we present a new model for editedmedia understanding frames, with a goal of kick-starting research on this challenging problem.
asdescribed in section 2, our task diﬀers from manystandard vision-and-language tasks both in terms offormat and required reasoning: a model must takeas input two images (a source image and its edit),with a signiﬁcant change of implication added bythe editor.
a model must be able to answer ques-tions, grounded in the main subjects of the image,describing these changes.
the answers are eitherboolean labels, or open-ended natural language –including explainable rationales..4.1 our model: pelican.
figure 3: statistics for emu.
we consider ﬁve ques-tion types, which in aggregate require a strong under-standing of the image edit.
the ﬁrst three types aresubject agnostic, though annotations refer explicitly tosubjects through subject tags; two (with subjectx) aresubject-speciﬁc..the image edit that are introduced or altered.
wepropose to use the annotations that collected forthese regions as additional signal for the modelto highlight where to attend.2 not only should amodel likely attend to these important regions, itshould prioritize attending to regions nearby (suchas objects that an edited person is interacting with)..we propose to model the (likely) importance ofan image region through graph propagation.
wewill build a directed graph with all regions of theimage, rooted at a subject mentioned by the ques-tion (e.g.
subject1).
we will then topologicallysort this graph; each region is then given an embed-ding corresponding to its sorted position – similarto the position embedding in a transformer.
thiswill allow the model to selectively attend to im-portant image regions in the image edit.
we use adiﬀerent position embedding for the image source,and do not perform the graph propagation here (aswe do not have introduced or altered annotations);this separate embedding captures the inductive biasthat the edited is more important than the source..for edited media understanding frames, not allimage regions are created equal.
not only is thesubject referred to in the question (e.g.
subject1)likely important, so too are all of the regions in.
2these annotations are collected from workers, but in the-ory, it would be possible to train a model to annotate regionsas such.
to make our task as accessible and easy-to-study aspossible, however, we use the provided labels in place of aseparate model however..2029figure 4: overview of pelican.
our model takes as input all regions s from the source image and e fromthe edited image.
we order the regions in e using a topological sort of overlapping boxes, rooted at subject1.
the green regions marked with an asterisk are additional regions that were introduced, and were labeled throughannotators.
this ordering allows the model to selectively attend to important image regions in generating an answerto the visual question about subject1..4.2 model details and transformer.
integration.
in this section, we describe integrating our impor-tance embeddings with a multimodal transformer.
let the source image be is and ie.
we use thebackbone feature extractor φ ( faster-rcnn fea-ture extractor (ren et al., 2015; anderson et al.,2018) to extract n regions of interest for each re-gion:.
[s1, ... , sn] = φ(is ).
[e1, ... , en] = φ(ie).
(1).
we note that some of these regions in e1, ... , en areprovided to the model (as annotated regions in theimage); the rest are detected by φ. these, plus thelanguage representation of the question, are passedto the transformer backbone t :.
[z1 ... zn+l] = t ([s1 ... sn], [e1, ... , en] , [x1 ... xl])(2)important for emu, z2n+1, ... , z2n+l serve aslanguage representations.
training under a left-to-right language modeling objective, we can predictthe next next token xl+1 using the representationzn+l..4.2.1 prioritization embeddings from.
topological sort.
transformers require position embeddings to beadded to each image region and word – enablingit to distinguish which region is which.
we sup-plement the position embeddings of the regions.
{e1...en} in the edited image ie with the result of atopological sort..graph deﬁnition.
we deﬁne the graph over im-age regions in the edited image as follows.
webegin by sourcing a seed region s ∈ {e1...en}.
letg = (v, e), where each v ∈ v represents meta-data of some ri ∈ φ(ie), deﬁned as vi ∈ m(ie) forsimplicity, s.t.:.
vi = {x1, y1, x2, y2, si, li}.
(3).
where x1, y1, x2, y1 represents the bounding boxof ri, si ∈ {1, 0} denoting if ri is a subject of ie, andli ∈ {introduced, altered} denoting the label of ri.
we build the graph iteratively: for each iteration,.
we deﬁne an edge e = {v, u}; u ∈ v s.t.:.
∀v ∈ m(ie), ∀u ∈ v, e = e ∪ (u, v) ∈ e(cid:48).
(4).
we deﬁne e(cid:48) as the set of edges (u, v) in whichu and v are notationally similar.
we deﬁne threecases in which this is true: if si ∈ ui ∧ s j ∈ v j,if li ∈ ui = l j ∈ v j, and if x1, y1, x2, y2 ∈ ui andx3, y3, x4, y4 ∈ ui overlaps, in which the percentageoverlap is deﬁned by standard intersection-over-union:.
min{x4, x2} − max{x3, x1}min{y4, y2} − max{y3, y1}.
(5).
we cap the number of outgoing edges at 3, andprevent cycles by allowing edges only to unseenimage regions.
in cases where there are more than.
2030three possible edges, we add edges in the order de-ﬁned in the previous paragraph, and break overlapties via maximum overlap..to produce embeddings, we run topological sortover the directed graph to assign each image regionan embedding, then assign an embedding to eachimage region based on the ordered index.
theembedding is zeroed out for image regions that aremissing from the dag, and from the source image(which are unlabeled).
we include bounding boxand class labels.
to generate text and classiﬁcationlabels, we attach the embeddings onto the input foran encoder-decoder structure..5 experimental results on emu.
in this section, we evaluate a variety of strongvision-and-language generators on emu.
similarto past work on vqa, we rebalance our test setsplit ensuring a 50/50 split per question type of ma-liciously labeled captions.
we provide two humanevaluation metrics – head-to-head, in which gener-ated responses are compared to human responses,and accuracy, in which humans are asked to label ifgenerated responses are accurate in regards to thegiven edit..5.1 baselines.
in addition to evaluating pelican, we compareand evaluate the performance of various potentiallyhigh-performing baselines on our task..a. retrieval.
for a retrieval baseline, whichgenerally performs well for generation-based tasks,we use features from resnet-158 (he et al., 2016),deﬁned as φ, to generate vectors for each ie in thetest set.
we then ﬁnd the most similar edited imageit in the training set t via cosine similarity:.
argmaxit ∈t.
φ(ie) · φ(it )(cid:107)φ(ie)(cid:107) × (cid:107)φ(it )(cid:107).
(6).
we use the captions associated with the most.
similar image in the training set..b. gpt-2 (radford et al., 2019).
as a text-onlybaseline, we use the 117m parameter model fromgpt-2, ﬁne-tuned on the captions from our dataset.
since the images are not taken into consideration,we generate from the seeds associated with eachquestion type and use the same captions for allimages in the test set..c. cross-modality gpt-2.
we test a uniﬁedlanguage-and-vision model on our dataset.
simi-lar to (alberti et al., 2019), we append the visual.
features φ(is ) and φ(ie) to the beginning of thetoken embeddings from gpt-2 (117m).
for thequestions involving a subject, we append an ad-ditional vector φ(r), where r is the region deﬁnedby the bounding box for that subject..d. dynamic relational attention (tan et al.,2019).
we test the best model from previous workon image edits on our task, dynamic relationalattention.
we train the model from scratch on ourdataset, using the same procedure as (tan et al.,2019).
we seed each caption with the relevantquestion..e. vlp (zhou et al., 2019).
we test vlp, apre-trained vision-and-language transformer model.
for image captioning, vlp takes a single image asinput and uses an oﬀ-the-shelf object detector to ex-tract regions, generation a caption using sequence-to-sequence decoding and treating the regions as asequence of input tokens..to generate a caption for a particular questiontype, we ﬁx the ﬁrst few generated tokens to matchthe preﬁx for that question type.
we ﬁne-tune vlpstarting from weights pre-trained on conceptualcaptions (3.3m image-caption pairs) (sharma et al.,2018) and then further trained on coco captions(413k image-caption pairs) (lin et al., 2014)..5.2 quantitative results and ablation study.
we present our results in table 2. we calculategenerative metrics (e.g.
meteor) by appendingthe rationale to the response.
generations frompelican are preferred over human generations14.0% of the time, with a 0.86 drop in perplexitycompared to the next best model.
to investigatethe performance of the model, we run an ablationstudy on various modeling attributes, detailed intable 3. first, we investigate the eﬀect of pretrain-ing (on conceptual captions (sharma et al., 2018;zhou et al., 2019)).
we ﬁnd that performance dropswithout pretraining (53.47%), but surprisingly stillbeats other baselines.
this suggests that the task re-quires more pragmatic inferences than the semanticlearning typically gained from pre-training tasks.
second, we ablate the importance of including an-notated (ai) features from the dataset when creatingthe directed graph, relying on a seed from a randomr-cnn region (54.44%).
we also ablate our use oftopological sort and a directed graph by suggestinga simple (but consistent) order for image regions(54.91%).
finally, we ablate including the visualregions from the source image.
the performance is.
2031figure 5: generation examples from pelican, marked with results from human evaluation.
pelican is able tocorrectly reference marked ﬁgures and is able to infer intent accordingly across each question type..model.
humans.
retrieval baselinegpt-2 (radford et al., 2019)cross-modality gpt-2dynamic ra (tan et al., 2019)vlp (zhou et al., 2019)pelican real (ours)pelican (ours).
n/an/a26.622.123.112.311.611.0.automated metrics.
human evaluation.
perplexity ↓.
rouge-l ↑ meteor ↑ accuracy ↑.
accurate % ↑.
n/a.
11.510.312.013.218.519.522.1.n/a.
7.26.27.98.910.510.811.6.
89.8.
51.950.051.051.853.254.155.4.head-to-head ↑.
50.0.
4.40.04.15.39.311.314.6.
95.2.
20.63.010.412.420.325.548.2.table 2: experimental results on emu.
we compare our model, pelican, with several strong baseline ap-proaches.
we calculate generative metrics (e.g.
meteor) by appending the rationale to the response.
pelicanreal describes a version of pelican trained on emu without additional human annotation (6.1)..auto evalaccuracy ↓.
human evalaccuracy ↑.
similar (55.35%), suggesting that pelican wouldbe able to perform in real-world settings in whichonly the edited image is present (e.g.
social mediaposts)..model.
pelican.
physicalintentimplicationmental state [of subjectx]eﬀect [on subjectx].
− pretraining− annotated features− directed graph− source image.
55.40n/a55.260.154.653.7.
54.654.454.955.3.
48.2.
60.543.049.942.541.1.
44.040.145.247.5.table 3: ablation study for pelican.
we also explorethe performance of pelican across each frame type..5.3 qualitative results.
last, we present qualitative examples in figure5. pelican is able to correctly understand im-age pairs which require mostly surface level un-derstanding - for example, in the top example, itis able to identify that the gun and action impliesnegative context, but misunderstands the responsewith regards to the situation.
in the bottom ex-ample, we show that pelican is able to refer tosubject1 correctly, but misinterprets the situationto be non-negative..2032and human captions.
category details are in-cluded in the appendix.
figure 6 shows our results.
overall, current models primarily lack the com-monsense (event-based and social) to accuratelydescribe disinformation.
geographical (location-based) and political (e.g.
knowledge about the jobof a president) external knowledge is also a missingcomponent..inpelican also still makes mistakesdescription-related attributes: describing some-thing other than the important change and aninaccuracy (e.g.
wrong color) are the most com-mon.
speciﬁc information – such as informationrelating to a speciﬁc person in the image (i.e.
requiring a model to identify the person in theimage), and information about a past event – arethe least critical, suggesting that eﬀorts should befocused ﬁrst on general intelligence rather thannamed-entity lookup..7 related work.
language-and-vision datasets datasets involv-ing images and languages cover a variety of tasks,including visual question answering (agrawal et al.,2015; goyal et al., 2017), image caption generation(lin et al., 2014; young et al., 2014; krishna et al.,2016), visual storytelling (park and kim, 2015;bosselut et al., 2016), machine translation (elliottet al., 2016), visual reasoning (johnson et al., 2017;hudson and manning, 2019; suhr et al., 2019), andvisual common sense (zellers et al., 2019)..two-image tasks though most computer visiontasks involve single images, some work has beendone on exploring image pairs.
the nlvr2 dataset(suhr et al., 2019) involves yes-no question answer-ing over image pairs.
neural naturalist (forbeset al., 2019) tests ﬁne-grained captioning of birdpairs; (jhamtani and berg-kirkpatrick, 2018) iden-tiﬁes the diﬀerence between two similar images..image edits there has been some computer vi-sion research studying image edits.
unlike ouremu dataset, however, much of this work has fo-cused on modeling lower-level image edits whereinthe cultural implications do not change signiﬁ-cantly between images.
for example, (tan et al.,2019) predicts image editing requests (generate‘change the background to blue’ from a pair ofimages).
past work has also studied learning toperform image adjustments (like colorization andenhancement) from a language query (chen et al.,2017; wang et al., 2018).
hateful meme challenge.
figure 6: failure cases from pelican, trained onemu.
commonsense is the largest diﬀerentiator be-tween human understanding and model-based analysisof disinformation..6 future implications.
6.1 emu in the real world.
to study if emu is helpful in real-world settings,we train a model of pelican on emu with onlythe edited image.
in this setting, the model must hy-pothesize which parts of the image were edited anddiscern the main subjects in the image.
at test time,we generate captions for each of the 5 intention-based question types.
results of this version ofpelican is in table 2..while this evaluation scheme is crude, we ﬁndthat this version of pelican is still able to outper-form previous models without usage of the sourceimage.
this suggests potential for generations fromemu-trained models in human-assisted settings.
in an initial human study (given pelican realcaptions, classify the edit as disinformation – werethe captions helpful in your decision?)
we ﬁnd thatannotators label as helpful 71.5% of the time.
ad-ditionally, annotators tended more often to pick thegold label (89.1% → 95.2%)..6.2 failure cases in current models and.
avenues for future research.
emu also helps us understand what current vision-and-language models are missing for use on dis-information , by analyzing the reasons and ratio-nales generated.
we ask annotators to comparepelican-generated captions marked as “worse”.
2033d. annotator demographic: n/ae. speech situation: all frames were collectedand validated over a period of about 12 weeks, be-tween november and january 2020, through theamazon amt platform.
workers were given regu-lar, detailed feedback regarding the quality of theirsubmissions and were able to address any questionsor comments to the study’s main author via emailor slack..f. text characteristics: in line with the in-tended purpose of the dataset, the included editsdescribe social interactions related (but not limitedto) platonic and romantic relationships, politicalsituations, as well as cultural and social contexts..g. recording quality: n/ah. other: n/alastly, we want to emphasize that our work isstrictly scientiﬁc in nature, and serves the explo-ration of machine reasoning alone.
it was not de-veloped to oﬀer guidance on misinformation or totrain models to classify social posts as misinfor-mation.
consequently, the inclusion of maliciousimage edits could allow adversaries to train mali-cious agents to produce visual misinformation.
weare aware of this risk, but also want to emphasizethat the utility of these agents allow useful negativetraining signal for minimizing harm that may becased by agents operating in visual information.
itis, therefore, necessary for future work that usesour dataset to specify how the collected examplesof both negative and positive misinformation areused, and for what purpose..(kiela et al., 2020) is a recent work challengingmodels to classify a meme as hateful or not..8 conclusion.
we present edited media understanding frames–a language-and-vision task requiring models to an-swer open-ended questions that capture the intentand implications of an image edit.
our model, pel-ican, kickstarts progress on our dataset – beatingall previous models and with humans rating its an-swers as accurate 48.2% of the time.
at the sametime, there is still much work to be done – and weprovide analysis that highlights areas for furtherprogress..acknowledgements.
the authors would like to thank ryan qiu for helpwith analysis, and the amazon mechanical turkcommunity for help with annotation.
this ma-terial is based upon work supported by the na-tional science foundation graduate research fel-lowship under grant no.
dge1256082, and inpart by nsf (iis-1714566), darpa cwc througharo (w911nf15-1-0543), darpa mcs programthrough niwc paciﬁc (n66001-19-2-4031), nsf(iis-1714566), and the allen institute for ai..9 ethical considerations.
in constructed the emu dataset, great care wastaken to ensure that crowd-workers are compen-sated fairly for their eﬀorts.
to this end, wemonitored median hit completion times for eachpublished batch, adjusting the monetary rewardsuch that at least 80% of workers always received>$15/hour, which is roughly double the minimumwage in the united states (the country of residencefor most amazon mechanical turk workers).
thisincluded the qualiﬁcation and evaluation rounds.
the following data sheet summarized relevant as-pects of the data collection process (bender andfriedman, 2018):.
a. curation rationale: selection criteria for theedits included in the presented dataset are discussedsection 3. we selected the highest-rated posts onreddit, and collected metadata data from annota-tors marking if the edit is nsfw or oﬀensive..b. language variety: the dataset is availablein english, with mainstream us englishes beingthe dominant variety, as per the demographic ofamazon mechanical turk workers.
c. speaker demographic: n/a.
2034references.
aishwarya agrawal, jiasen lu, stanislaw antol, mar-garet mitchell, c. lawrence zitnick, devi parikh,and dhruv batra.
2015. vqa: visual question an-swering.
international journal of computer vision,123:4–31..chris alberti, jeﬀrey ling, michael collins, and davidreitter.
2019. fusion of detected objects in text forvisual question answering.
arxiv, abs/1908.05054..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..collin f. baker, c. fillmore, and j. lowe.
1998. the.
berkeley framenet project.
in coling-acl..jawadul h bappy, amit k roy-chowdhury, jasonbunk, lakshmanan nataraj, and bs manjunath.
2017. exploiting spatial structure for localizing ma-nipulated image regions.
in proceedings of the ieeeinternational conference on computer vision, pages4970–4979..emily m. bender and b. friedman.
2018. data state-ments for nlp: toward mitigating system bias andenabling better science..tiziano bianchi and alessandro piva.
2012..imageforgery localization via block-grained analysis ofieee transactions on informationjpeg artifacts.
forensics and security, 7(3):1003–1017..antoine bosselut, jianfu chen, david warren, han-naneh hajishirzi, and yejin choi.
2016. learningprototypical event structure from photo albums.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1769–1779..jianbo chen, yelong shen, jianfeng gao, jingjingliu, and xiaodong liu.
2017. language-based im-age editing with recurrent attentive models.
2018ieee/cvf conference on computer vision and pat-tern recognition, pages 8721–8729..bobby chesney and danielle citron.
2019. deep fakes:a looming challenge for privacy, democracy, and na-tional security.
calif. l.
rev., 107:1753..desmond elliott, stella frank, khalil sima’an, and lu-cia specia.
2016. multi30k: multilingual english-german image descriptions.
arxiv, abs/1605.00459..hany farid.
2009. a survey of image forgery detection.
ieee signal processing magazine, 26(2):16–25..maxwell forbes, christine kaeser-chen, piyushsharma, and serge j. belongie.
2019. neural nat-uralist: generating ﬁne-grained image comparisons.
in emnlp/ijcnlp..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image under-standing in visual question answering.
2017 ieeeconference on computer vision and pattern recog-nition (cvpr), pages 6325–6334..ulrike gretzel.
2017. the visual turn in social media.
marketing..kaiming he, georgia gkioxari, piotr doll´ar, andross b. girshick.
2017. mask r-cnn.
2017 ieee in-ternational conference on computer vision (iccv),pages 2980–2988..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
2016 ieee conference on computer visionand pattern recognition (cvpr), pages 770–778..drew a. hudson and christopher d. manning.
2019.gqa: a new dataset for real-world visual reason-ing and compositional question answering.
2019ieee/cvf conference on computer vision and pat-tern recognition (cvpr), pages 6693–6702..harsh jhamtani and taylor berg-kirkpatrick.
2018.learning to describe diﬀerences between pairs ofsimilar images.
in emnlp..johanna e. johnson, bharath hariharan, laurensvan der maaten, li fei-fei, c. lawrence zitnick,and ross b. girshick.
2017. clevr: a diagnosticdataset for compositional language and elementaryvisual reasoning.
2017 ieee conference on com-puter vision and pattern recognition (cvpr), pages1988–1997..douwe kiela, hamed firooz, aravind mohan, vedanujgoswami, amanpreet singh, pratik ringshia, anddavide testuggine.
2020. the hateful memes chal-lenge: detecting hate speech in multimodal memes.
arxiv, abs/2005.04790..jan kietzmann, linda w lee, ian p mccarthy, andtim c kietzmann.
2020. deepfakes: trick or treat?
business horizons, 63(2):135–146..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a. shamma,michael s. bernstein, and li fei-fei.
2016. vi-sual genome: connecting language and vision us-ing crowdsourced dense image annotations.
interna-tional journal of computer vision, 123:32–73..m. forbes, jena d. hwang, vered shwartz, maartensap, and yejin choi.
2020. social chemistry 101:learning to reason about social and moral norms.
inemnlp..liunian harold li, mark yatskar, da yin, cho-juihsieh, and kai-wei chang.
2019. visualbert: asimple and performant baseline for vision and lan-guage.
arxiv preprint arxiv:1908.03557..2035hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-formers.
in emnlp/ijcnlp..hao tan, franck dernoncourt, zhe lin, trung bui, andmohit bansal.
2019. expressing visual relationshipsvia language.
in acl..hai wang, jason d. williams, and singbing kang.
2018. learning to globally edit images with textualdescription.
arxiv, abs/1810.05786..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
transactions of theassociation for computational linguistics, 2:67–78..rowan zellers, yonatan bisk, ali farhadi, and yejinchoi.
2019. from recognition to cognition: visualcommonsense reasoning.
2019 ieee/cvf confer-ence on computer vision and pattern recognition(cvpr), pages 6713–6724..luowei zhou, hamid palangi, lefei zhang, houdonghu, jason j. corso, and jianfeng gao.
2019. uni-ﬁed vision-language pre-training for image caption-ing and vqa.
arxiv, abs/1909.11059..xinyi zhou and reza zafarani.
2020. a survey of fakenews: fundamental theories, detection methods, andopportunities.
acm computing surveys (csur),53(5):1–40..tsung-yi lin, michael maire, serge j. belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c. lawrence zitnick.
2014. microsoft coco:common objects in context.
arxiv, abs/1405.0312..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems, pages 13–23..britt s. paris and joan m. donovan.
2019. deepfakesand cheap fakes.
technical report, data and society..cesc c. park and gunhee kim.
2015. expressing animage stream with a sequence of natural sentences.
in nips..alec radford, jeﬀrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..hannah rashkin, eunsol choi, jin yea jang, svitlanavolkova, and yejin choi.
2017. truth of varyingshades: analyzing language in fake news and po-in proceedings of the 2017litical fact-checking.
conference on empirical methods in natural lan-guage processing, pages 2931–2937, copenhagen,denmark.
association for computational linguis-tics..hannah rashkin, sameer singh, and yejin choi.
2016.connotation frames: a data-driven investigation.
arxiv: computation and language..shaoqing ren, kaiming he, ross girshick, and jiansun.
2015. faster r-cnn: towards real-time ob-ject detection with region proposal networks.
inadvances in neural information processing systems,pages 91–99..maarten sap, saadia gabriel, lianhui qin, dan juraf-sky, noah a. smith, and yejin choi.
2020. socialbias frames: reasoning about social and power im-plications of language.
in acl..maarten sap, marcella cindy prasettio, ari holtzman,hannah rashkin, and yejin choi.
2017. connota-tion frames of power and agency in modern ﬁlms.
in emnlp..piyush sharma, nan ding, sebastian goodman, andconceptual captions: aradu soricut.
2018.cleaned, hypernymed, image alt-text dataset for au-tomatic image captioning.
in acl..kai shu, amy sliva, suhang wang, jiliang tang, andhuan liu.
2017. fake news detection on social me-dia: a data mining perspective.
acm sigkdd ex-plorations newsletter, 19(1):22–36..alane suhr, stephanie zhou, iris d. zhang, huajunbai, and yoav artzi.
2019. a corpus for reasoningabout natural language grounded in photographs.
inacl..2036a appendices.
a.1 reproducibility of experiments.
we provide downloadable source code of all scripts,and experiments, at to-be-provided.
we usetwo titan x gpus to train and evaluate all models,except dynamic relational attention (tan et al.,2019), which was trained on a single titan xpgpu.
for gpt-2 (radford et al., 2019), we usethe 117m parameter model, taking 5 hours to train.
our conﬁguration of vlp (zhou et al., 2019) has138,208,324 parameters, taking 6 hours to train.
our model, pelican, has 138,208,324 parame-ters, taking 6 hours to train.
our dynamic rela-tional attention model has 55,165,687 parameters,taking 10 hours to train..a.2 reproducibility of hyperparameters.
for models using gpt-2 as their underlying infras-tructure, we use a maximum sequence length of1024, 12 hidden layers, 12 heads for each attentionlayer, and 0.1 dropout in all fully connected lay-ers.
for dynamic relational attention (tan et al.,2019), we use a batch size of 95, hidden dimensionsize of 512, embedding dimension size of 256, 0.5dropout, adam optimizer, and a 1e-4 learning rate.
we used early stopping based on the bleu scoreon the validation set at the end of every epoch;the test scores reported are for a model trainedfor 63 epochs.
for all models relying on vlp astheir underlying infrastructure, we use 30 trainingepochs, 0.1 warmup proportion, 0.01 weight decay,64 batch size..a.3 reproducibility of datasets.
our dataset has 39338 examples in the training setand 4268 and 3992 examples in the developmentand test sets respectively.
all training on additionaldatasets (e.g.
(zhou et al., 2019)) matches theirimplementation exactly.
our train/val/test splitswere chosen at random, during the annotation pe-riod.
no data was excluded, and no additionalpre-processing was done.
a downloadable link isavailable at to-be-provided after publication..a.4 data collection.
for reference and reproducibility, we show the fulltemplate used to collect data in figure 9..we also show our human evaluation process in.
figure 10..figure 7: subject distribution.
to highlight our deci-sion for a 3 subject limit, we show that the majority ofimages contains 1-2 subjects..a.5 additional annotation details.
for an image pair (consisting of an image edit anda source image), we 1) ask the annotator to iden-tify and index the main subjects in the image, 2)prime the annotator by asking them to describe thephysical change in the image, 3) ask a series ofquestions for each main person they identiﬁed, and4) ask a series of questions about the image as awhole.
for each question we require annotatorsto provide both an answer to the question and arationale (e.g.
the physical change in the imageedit that alludes to their answer).
this is critical,as the rationales prevent models from guessing aresponse such as “would be harmful” without pro-viding the proper reasoning for their response.
weask annotators to explicitly separate the rationalefrom the response by using the word “because” or“since” (however, we ﬁnd that the vast majority ofannotators naturally do this, without being explic-itly prompted).
for the main subjects, we limit thenumber of subjects to 3. this also mitigates a largevariation in workload between image pairs, whichwas gathered as potentially problematic from an-notator feedback.
we limit the number of captionsper type to 3. we ﬁnd that a worker chooses toprovide more than one label for a type in only asmall proportion of cases, suggesting that usually,one caption is needed to convey all the informationabout the image edit relating to that type ..2037figure 8: language sentence length distribution,measured in words, across other language-and-visiondatasets.
the natural language answers in emu showa high degree of complexity, with an average sentencelength of 26.45 words..a.6 lexical analysis.
word-level statistics we analyze the lexicalstatistics of this dataset.
we remove stop wordsas words such as “him”.
we show that diﬀerenttypes require diﬀerent language in their response.
in addition, we highlight that many of the rationalesinvolve people, suggesting that understanding so-cial implications is critical to solving this task..a.7 motivation for emu task deﬁnition.
we begin by motivating and contextualising ourproblem.
a key insight is that we need to think intothe future – since the task is important but diﬃcult,we aim to structure emu such that it can help mod-els learn how to understand misinformation (byproviding the source image, grounding captions,and additional annotations) without oversimplify-ing the task..frames.
we ask models a series of questionsabout the what and why of the image edit.
wearrived on these questions by ﬁrst asking annota-tors to explain the image edits without prompting.
then, we bucketed the responses into similar cat-egories, motivating us to create questions basedon the parts of edits humans naturally focused on.
in our task, we consider six open-ended questiontypes – physical, intent, implication, emotion [ofsubjectx], attack [on subjectx], and disinforma-tion.
descriptions of each are in figure 2. eachtype focuses on a diﬀerent aspect of the image edit,and is related one-to-one with an open-ended ques-tion q. each question type may also reference aspeciﬁc entity b. in these cases, the answer to thequestion would diﬀer based on the main subject.
figure 9: example of our annotation process..figure 10: example of our evaluation process..referred..labels.
for each q, we ask models to provideboth a classiﬁcation label l and a generated answer(response y and rationale r) for a given image edit.
visual misinformation is not a closed form problem– the potential label-space and responses for anmalicious edit are ever-changing with recent events.
thus, we suggest that models need to produce agenerated answer.
however, we also want modelsto go beyond simple answering – we want them toanswer for the right reasons, in an explainable way.
thus, we require models to generate a rationaleexplaining why its answer is true.
for example,a good rationale explains that the perception ofsubject1 could be injured because a gun was addedto subject1’s hand.
our evaluation recruits humanraters to compare generated answers and rationalesy/r to those written by annotators.
to account forthe current diﬃculty of evaluating generation, we.
2038rationales.
intent.
implication.
disinformation.
emotion [of subjectx].
attack [on subjectx].
holdingfacewearingmanappears.
4.21% fun4.09% powerful3.17% funny2.64% hero2.41% movie.
4.83% public1.13% think1.09% man1.02% fun1.01% disgrace.
3.07% movie2.12% woman1.75% new1.68% game1.25% real.
2.93% confused2.12% amused1.92% embarrassed1.23% upset1.23% proud.
7.62% likes4.38% hates3.88% loves3.50% wants2.61% doesn’t.
3.00%2.21%1.36%1.35%1.31%.
responses.
table 4: lexical statistics.
statistics for each dimension represent omit the rationale, and statistics for the rationaleare reported separately..of these labels in our modeling section..figure 11: our template for human evaluations.
eachannotator is shown an edited image, the source image,and is asked to compare a human annotated captionsand a machine annotated caption..include a binary classiﬁcation label l for each ofthe “why” answers to allow for a simple checkpointevaluation metric of model progress..grounding.
each explanation is grounded tobounding boxes ai of the people in the edited im-age.
similar to past work in vision-and-language(zellers et al., 2019), annotators write captionsthat refer to the bounding box (for example,subject1would be angry).
this allows precise ref-erence in visually complex edits..additional annotations.
finally, we provideannotators for bounding boxes of introduced andmodiﬁed regions in edited images.
these boundingboxes provide the syntax of the change in a machinedigestible format (bounding boxes + labels).
weconduct initial exploration of the empirical beneﬁt.
2039