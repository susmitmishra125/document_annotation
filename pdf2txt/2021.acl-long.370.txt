gwlan: general word-level autocompletionfor computer-aided translation.
huayang li lemao liu guoping huang shuming shitencent ai lab{alanili,redmondliu,donkeyhuang,shumingshi}@tencent.com.
abstract.
computer-aided translation (cat), the use ofsoftware to assist a human translator in thetranslation process, has been proven to be use-ful in enhancing the productivity of humantranslators.
autocompletion, which suggeststranslation results according to the text piecesprovided by human translators, is a core func-tion of cat.
there are two limitations in pre-vious research in this line.
first, most researchworks on this topic focus on sentence-level au-tocompletion (i.e., generating the whole trans-lation as a sentence based on human input), butword-level autocompletion is under-exploredso far.
second, almost no public benchmarksare available for the autocompletion task ofcat.
this might be among the reasons whyresearch progress in cat is much slower com-pared to automatic mt.
in this paper, we pro-pose the task of general word-level autocom-pletion (gwlan) from a real-world cat sce-nario, and construct the ﬁrst public bench-mark1 to facilitate research in this topic.
inaddition, we propose an effective method forgwlan and compare it with several strongbaselines.
experiments demonstrate that ourproposed method can give signiﬁcantly moreaccurate predictions than the baseline methodson our benchmark datasets..1.introduction.
machine translation (mt) has witnessed great ad-vancements with the emergence of neural machinetranslation (nmt) (sutskever et al., 2014; bah-danau et al., 2015; wu et al., 2016; gehring et al.,2017; vaswani et al., 2017), which is able toproduce much higher quality translation resultsthan statistical machine translation (smt) mod-els (koehn et al., 2003; chiang, 2005; koehn,.
1the information of benchmark datasets is in https:.
//github.com/ghrua/gwlan.
figure 1:illustration of different autocompletionmethods.
the translation context is in red.
sub-ﬁgurein (a) is the sentence-level autocompletion, where thegray part is the completion generated by mt system.
both (b) and (c) are word-level autocompletion, under-lined text “sp” is the human typed characters and thewords in the rounded rectangles are word-level auto-completion candidates..2009).
in spite of this, mt systems cannot re-place human translators, especially in the scenar-ios with rigorous translation quality requirements(e.g., translating product manuals, patent docu-ments, government policies, and other ofﬁcial doc-uments).
therefore, how to leverage the pros ofmt systems to help human translators, namely,computer-aided translation (cat), attracts the at-tention of researchers (barrachina et al., 2009;green et al., 2014; knowles and koehn, 2016;santy et al., 2019).
among all cat technologies(such as translation memory, terminology manage-ment, sample sentence search, etc.
), autocomple-tion plays an important role in a cat system inenhancing translation efﬁciency.
autocompletionsuggests translation results according to the textpieces provided by human translators..we note two limitations in previous research onthe topic of autocompletion for cat.
first, mostof previous studies aim to save human efforts bysentence-level autocompletion (figure 1 a).
nev-ertheless, word-level autocompletion (figure 1 b.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4792–4802august1–6,2021.©2021associationforcomputationallinguistics4792we asked two sp wir haben die meinung von zwei fachärzten eingeholt.
we asked two experts for their opinion.
we    sp    their opinion.
abc1 specialists2 specific3 splitsource sentencetranslation1 specialists2 specific3 splitand c) has not been systematically studied.
sec-ond, almost no public benchmarks are available forthe autocompletion task of cat.
although someachievements have been made, research progressin cat is more sluggish than that in automatic mt.
the lack of benchmarks has hindered researchersfrom making continuous progress in this area..in this work, we propose a general word-levelautocompletion (gwlan) task, and construct abenchmark with automatic evaluation to facilitatefurther research progress in cat.
speciﬁcally, thegwlan task aims to complete the target wordfor human translators based on a source sentence,translation context as well as human typed charac-ters.
compared with previous work, gwlan con-siders four most general types of translation con-text: preﬁx, sufﬁx, zero context, and bidirectionalcontext.
besides, as in most real world scenarios,we only know the relative position between inputwords and the spans of translation context in thegwlan task.
we construct a benchmark for thetask, with the goal of supporting automatic evalua-tion and ensuring a convenient and fair comparisonamong different methods.
the benchmark is builtby extracting triples of source sentences, transla-tion contexts, and human typed characters fromstandard parallel datasets.
accuracy is adopted asthe evaluation metric in the benchmark..to address the variety of context types and weakposition information issue, we propose a neuralmodel to complete a word in different types of con-text as well as a joint training strategy to optimizeits parameters.
our model can learn the represen-tation of potential target words in translation andthen choose the most possible word based on thehuman input..our contributions are two-fold:.
• we propose the task of general word-levelautocompletion for cat, and construct theﬁrst public benchmark to facilitate research inthis topic..• we propose a joint training strategy to opti-mize the model parameters on different typesof contexts together.
2.
2 related work.
computer-aided translation (cat) is a widely usedpractice when using mt technology in the industry..2this approach has been implemented into a human-machine interactive translation system transmart (huanget al., 2021) at www.transmart.qq.com..as the the mt systems advanced and improved,various efﬁcient interaction ways of cat haveemerged (vasconcellos and le´on, 1985; greenet al., 2014; hokamp and liu, 2017; weng et al.,2019; wang et al., 2020).
among those differentmethods, the autocompletion is the most related toour work.
therefore, we will ﬁrst describe previousworks in both sentence-level and word-level auto-completion, then show the relation to other tasksand scenarios..sentence-level autocompletion most of previ-ous work in autocompletion for cat focus onsentence-level completion.
a common use casein this line is interactive machine translation (imt)(green et al., 2014; cheng et al., 2016; peris et al.,2017; knowles and koehn, 2016; santy et al.,2019).
imt systems utilize mt systems to com-plete the rest of a translation after human transla-tors editing a preﬁx translation (alabau et al., 2014;zhao et al., 2020).
for most imt systems, the coreto achieve this completion is preﬁx-constrained de-coding (wuebker et al., 2016)..another sentence-level autocompletion method,lexically constrained decoding (lcd) (hokampand liu, 2017; post and vilar, 2018), recently at-tracts lots of attention (hasler et al., 2018; susantoet al., 2020; kajiwara, 2019).
compared with imt,lcd relaxes the constraints provided by humantranslators from preﬁxes to general forms: lcdcompletes a translation based on some unorderedwords (i.e., lexical constraints), which are not nec-essary to be continuous (hokamp and liu, 2017;hu et al., 2019; dinu et al., 2019; song et al., 2019).
although it does not need additional training, itsinference is typically less efﬁcient compared withthe standard nmt.
therefore, other works proposeefﬁcient methods (li et al., 2020; song et al., 2019)by using lexical constraints in a soft manner ratherthan a hard manner as in lcd..word-level autocompletion word-level auto-completion for cat is less studied than sentence-level autocompletion.
langlais et al.
(2000); santyet al.
(2019) consider to complete a target wordbased on human typed characters and a transla-tion preﬁx.
but they require the target word tobe the next word of the translation preﬁx, whichlimits its application.
in contrast, in our work theproposed word-level autocompletion is more gen-eral and can be applied to real-world scenariossuch as post-editing (vasconcellos and le´on, 1985;.
4793green et al., 2013) and lcd, where human trans-lators need to input some words (corrections orconstraints).
huang et al.
(2015) propose a methodto predict a target word based on human typed char-acters, however, this method only uses the sourceside information and does not consider translationcontext, leading to limited performance comparedwith our work..others our work may also be related to previousworks in input method editors (ime) (huang et al.,2018; lee et al., 2007).
however, they are in themonolingual setting and not capable of using theuseful multilingual information..3 task and benchmark.
in this section, we ﬁrst describe why we need word-level autocompletion in real-world cat scenarios.
we then present the details of the gwlan taskand the construction of benchmark..why gwlan?
word level autocompletion isbeneﬁcial for improving input efﬁciency (langlaiset al., 2000).
previous works assume that the trans-lation context should be a preﬁx and the desiredword is next to the preﬁx as shown in figure 1(b), where the context is “we asked two” and thedesired word is “specialists”.
however, in somereal-world cat scenarios such as post-editing andlexically constrained decoding, translation contextmay be discontinuous and the input words (cor-rections or lexical constraints) are not necessarilyconjunct to the translation context.
as shown infigure 1 (c), the context is “we · · · their opinion”and the human typed characters “sp” is conjunct toneither “we” nor “their” in the context.
therefore,existing methods can not perform well on such ageneral scenario.
this motivates us to propose ageneral word-level autocompletion task for cat..3.1 task deﬁnition.
suppose x = (x1, x2, .
.
.
, xm) is a source se-quence, s = (s1, s2, .
.
.
, sk) is a sequence ofhuman typed characters, and a translation con-text is denoted by c = (cl, cr), where cl =(cl,1, cl,2, .
.
.
, cl,i), cr = (cr,1, cr,2, .
.
.
, cr,j).
thetranslation pieces cl and cr are on the left and righthand side of s, respectively.
formally, given asource sequence x, typed character sequence sand a context c, the general word-level autocom-pletion (gwlan) task aims to predict a targetword w which is to be placed in the middle be-.
tween cl and cr to constitute a partial translation.
note that in the partial translation consisting ofcl, w and cr, w is not necessary to be consec-utive to cl,i or cr,1.
for example, in figure 1(c), cl = (“we”, ), cr = (“their”, “option”, “.”),s = (“sp”, ), the gwlan task is expected to pre-dict w = “specialists” to constitute a partial transla-tion “we · · · specialists · · · their opinion.”, where“· · · ” represents zero, one, or more words (i.e., thetwo words before and after it are not necessarilyconsecutive)..to make our task more general in real-worldscenarios, we assume that the left context cl andright context cr can be empty, which leads to thefollowing four types of context:• zero-context: both cl and cr are empty;• sufﬁx: cl is empty;• preﬁx: cr is empty;• bi-context: neither cl nor cr is empty.
with the tuple (x, s, c), the gwlan task is topredict the human desired word w..relation to most similar tasks some similartechniques have been explored in cat.
green et al.
(2014) and knowles and koehn (2016) studied aautocompletion scenario called translation predic-tion (tp), which provides suggestions of the nextword (or phrase) given a preﬁx.
besides the strictassumption of translation context (i.e., preﬁx here),compared with gwlan, another difference is thatthe information of human typed characters is ig-nored in their setting.
there also exist some worksthat consider the human typed sequences (huanget al., 2015; santy et al., 2019), but they only con-sider a speciﬁc type of translation contexts.
huanget al.
(2015) propose to complete a target wordbased on the zero-context assumption.
despite itsﬂexibility, this method is unable to explore transla-tion contexts to improve the autocompletion perfor-mance.
the word-level autocompletion methods inlanglais et al.
(2000); santy et al.
(2019) have thesame assumption as tp, which impedes the use oftheir methods under the scenarios like post editingand lexically constrained decoding, where humaninputs are not necessarily conjunct to the variety oftranslation contexts..3.2 benchmark construction.
to set up a benchmark, ﬁrstly we should create alarge scale dataset including tuples of (x, s, c, w)for training and evaluating gwlan models.
ide-ally, we may hire professional translators to man-.
4794ually annotate such a dataset, but it is too costlyin practice.
therefore, in this work, we proposeto automatically construct the dataset from paral-lel datasets which is originally used in automaticmachine translation tasks.
the procedure for con-structing our data is the same for train, validation,and test sets.
and we construct a dataset for eachtype of translation context..assume we are given a parallel dataset{(xi, yi)}, where yi is the reference translationof xi.
then, we can automatically construct thedata ci and si by randomly sampling from yi.
weﬁrst sample a word w = yik and then demonstratehow to extract ci for different translation contexts:• zero-context: both cl and cr are empty;• sufﬁx: randomly sample a translation piece cr =ypr,1:pr,2 from y, where k < pr,1 < pr,2.
the cl isempty here;• preﬁx: randomly sample a translation piece cl =ypl,1:pl,2 from y, where pl,1 < pl,2 < k. the cr isempty here;• bi-context: sample cl as in preﬁx, and sample cras in sufﬁx..then we have to simulate the human typed char-acters s based on w. for languages like english andgerman, we sample a position p from the charactersequence and the human input s = w1:p, where1 ≤ p < lw.
for languages like chinese, thehuman input is the phonetic symbols of the word,since the word cannot be directly typed into thecomputer.
therefore, we have to convert w to pho-netic symbols that are characters in alphabet andsample s from phonetic symbols like we did onenglish..evaluation metric to evaluate the performanceof the well-trained models, we choose accuracy asthe evaluation metric:.
acc =.
nmatchnall.
,.
(1).
where nmatch is the number of words that are cor-rectly predicted and nall is the number of testingexamples..4 proposed approach.
given a tuple (x, c, s), our approach decomposesthe whole word autocompletion process into twoparts: model the distribution of the target word wbased on the source sequence x and the translationcontext c, and ﬁnd the most possible word w basedon the distribution and human typed sequence s..figure 2: cross-lingual encoder of the wpm..therefore, in the following subsections, we ﬁrstlypropose a word prediction model (wpm) to de-ﬁne the distribution p(w|x, c) of the target wordw (§4.1).
then we can treat the human input se-quence s as soft constraints or hard constraints tocomplete s and obtain the target word w (§4.2).
finally, we present two strategies for training andinference (§4.3)..4.1 word prediction model.
the purpose of wpm is to model the distributionp(w|x, c).
more concretely, we will use a singleplaceholder [mask] to represent the unknown tar-get word w, and use the representation of [mask]learned from wpm to predict it.
formally, giventhe source sequence x, and the translation contextc = (cl, cr), the possibility of the target word wis:.
p (w|x, cl, cr; θ) = softmax (φ(h)) [w].
(2).
where h is the representation of [mask], φ is a lin-ear network that projects the hidden representationh to a vector with dimension of target vocabularysize v , and softmax(d)[w] takes the componentregarding to w after the softmax operation over avector d ∈ rv ..inspired by the attention-based architectures(vaswani et al., 2017; devlin et al., 2019)3, we.
3because the use of attention-based models has becomeubiquitous recently, we omit an exhaustive background de-.
4795translation contextbidirectional masked attentionn×add & normcross-lingual attentionoutputs of source encoderadd & normfeed forwardadd & normsoftmaxoutput probabilitieslinear figure 3: the input representation of our model and architecture of bidirectional masked attention.
the inputembeddings are the sum of the token embeddings and position embeddings.
[mask] represents the potenial targetword in this translation context..use a dual-encoder architecture to learn the repre-sentation h based on source sequence x and trans-lation context c. our model has a source encoderand a cross-lingual encoder.
the source encoderof wpm is the same as the transformer encoder,which is used to encode the source sequence x. asshown in figure 2, the output of source encoderwill be passed to the cross-lingual encoder later.
the cross-lingual encoder is similar to the trans-former decoder, while the only difference is that wereplace the auto-regressive attention (ara) layerby a bidirectional masked attention (bma) module,due to that the ara layer cannot use the leftwardinformation ﬂow (i.e., cr)..speciﬁcally, the bma module is built by amultiple-layer self attention network.
as shownin figure 3, in each layer of bma, each tokenin the attention query can attend to all words intranslation context cl and cr.
in addition, the in-put consists of three parts, the [mask] token, andtranslation contexts cl and cr, as illustrated in fig-ure 3. note that its position embeddings e are onlyused to represent the relative position relationshipbetween tokens.
taking the sentence in figure 3as an example, e3 does not precisely specify theposition of the target word w but roughly indicatesthat w is on the right-hand-side of cl and on theleft-hand-side of cr.
finally, the representation of[mask] as learnt by bma will be passed to add& norm layer as shown in figure 2..scription of the model and refer readers to vaswani et al.
(2017) and devlin et al.
(2019)..4.2 human input autocompletionafter learning the representation h of the [mask]token, there are two ways to use the human inputsequence s to determinate the human desired word.
firstly, we can learn the representation of s anduse it as a soft constraint while predicting word w.taking the sentence in figure 3 as an example, sup-posing the human typed sequence is s = “des”,we can use an rnn network to learn the represen-tation of des and concatenate it with h to predictthe word descending.
alternatively, we can usedes as a hard constraint:(cid:40) p (w|x,c;θ)z.,.
if w starts with sotherwise..ps[w] =.
0,.where p (·|·) is the probability distribution deﬁnedin eq.
(2) and z is the normalization term indepen-dent on w. then we pick w∗ = arg maxw ps[w] asthe most possible word.
in our preliminary experi-ments, the performances of these two methods arecomparable, and there is no signiﬁcant gain whenwe use them together.
one main reason is that themodel can already learn the starts-with action pre-cisely in the soft constraint method.
therefore, wepropose to use the human inputs as hard constraintsin our later experiments, because of the method’sefﬁciency and simplicity..4.3 training and inference strategy.
suppose d denotes the training data for gwlan,i.e., a set of tuples (x, c, s, w).
since there arefour different types of context in d as presented in.
4796ethee1theeaircrafte2aircrafte[mask]e3[mask]erapidlye4rapidlye[eos]e5[eos]e[sos]e0[sos]<latexit sha1_base64="cxntnbinxcmwhkwkn8rhfmu8che=">aaab7xicbvdlssnafl3xweur6tlnybfcluquxrbdukxgh9cgmplo2rgtszi5europ7hxoyhb/8edf+okzujbdwwczrmhufceirqgxffbwvldw9/ylg2vt3d29/yrb4cte6ea8salzaw7atvccswbkfdytqi5jqlj28h4nvfbt1wbeashnctcj+hqivawilzq9qyxmnk/unvr7gxkmxgfqukbrr/yzymsjbhcjqkxxc9n0m+orsekn5z7qeejzwm65f1lfy248bpztlnyapubcwntn0iyu38nmhozm4kcoxlrhjlflxf/87ophtd+jlssilds/lgysoixyu8na6e5qzmxhdit7k6ejaimdg1beqne4snlphve8y5r7v1ftx5t1fgcyzibm/dgcupwbw1oaonheizxehni58v5dz7moytoktmcp3a+fwao847a</latexit>...aircraft[mask]<latexit sha1_base64="cxntnbinxcmwhkwkn8rhfmu8che=">aaab7xicbvdlssnafl3xweur6tlnybfcluquxrbdukxgh9cgmplo2rgtszi5europ7hxoyhb/8edf+okzujbdwwczrmhufceirqgxffbwvldw9/ylg2vt3d29/yrb4cte6ea8salzaw7atvccswbkfdytqi5jqlj28h4nvfbt1wbeashnctcj+hqivawilzq9qyxmnk/unvr7gxkmxgfqukbrr/yzymsjbhcjqkxxc9n0m+orsekn5z7qeejzwm65f1lfy248bpztlnyapubcwntn0iyu38nmhozm4kcoxlrhjlflxf/87ophtd+jlssilds/lgysoixyu8na6e5qzmxhdit7k6ejaimdg1beqne4snlphve8y5r7v1ftx5t1fgcyzibm/dgcupwbw1oaonheizxehni58v5dz7moytoktmcp3a+fwao847a</latexit>...positionembeddingstokenembeddingsquery / inputkey / valuetheaircraftrapidly[eos][sos]trans.
contextsimulationbidirectional masked attentionthe[sos]rapidly[eos]<latexit sha1_base64="d+tgssld733/boe2hyihb5fojzy=">aaab+xicbvdlsgmxfl1tx7w+rl26crbbvzkrrzdfny4r2ae0w5djpg1ojhmstkem/rm3lhrx65+482/mtlpq6ogqwzn3kpmtpzxp43lftmvtfwnzq7pd29nd2z9wd486wmak0darxkpehdxltnc2yybtxqootijou9hkrvc7u6o0k+lrzfiajhgk2jarbkwuuu4gkjzws8reozmhphtrxsnbap0lfknqukivup+dwjisociqjrxu+15qghwrwwin89og0ztfzijhtg+pwanvqb5ipkdnvonrucp7heel9edgjhndhlotctzjveov4n9epzpdmybnis0mfwt50ddjyehu1ibipigxfgyjjorzriimscle2ljqtgr/9ct/seei4v81viflevo2rkmkj3ak5+ddntthhlrqbgjteiixehvy59l5c96xoxwn3dmgx3a+vge2jjqi</latexit>cl<latexit sha1_base64="7hsmdsvdzvyidscrqxyho4b4ryu=">aaab+xicbvdlsgmxfl1tx7w+rl26crbbvzkrrzdfny4r2ae0w5djpg1ojhmstkem/rm3lhrx65+482/mtlpq6ogqwzn3kpmtpzxp43lftmvtfwnzq7pd29nd2z9wd486wmak0darxkpehdxltnc2yybtxqootijou9hkrvc7u6o0k+lrzfiajhgk2jarbkwuuu4gkjzws8reozmhkntrxsnbap0lfknqukivup+dwjisociqjrxu+15qghwrwwin89og0ztfzijhtg+pwanvqb5ipkdnvonrucp7heel9edgjhndhlotctzjveov4n9epzpdmybnis0mfwt50ddjyehu1ibipigxfgyjjorzriimscle2ljqtgr/9ct/seei4v81viflevo2rkmkj3ak5+ddntthhlrqbgjteiixehvy59l5c96xoxwn3dmgx3a+vge/pjqo</latexit>cr§3, we can split d into four subsets dzero, dpreﬁx,dsufﬁx and dbi.
to yield good performances onthose four types of translation context, we also pro-pose two training strategies.
the inference strategydiffers accordingly..strategy 1: one context type one model forthis strategy, we will train a model for each trans-lation context, respectively.
speciﬁcally, for eachtype of context t ∈ {zero, preﬁx, sufﬁx, bi}, we in-dependently train one model θt by minimizing thefollowing loss l(dt, θ):.
l(dt; θ) =.
log p (w|x, c; θ),.
(cid:88).
1|dt|.
(x,c,s,w)∈dt.
(3)where p (w|x, c; θ) is the wpm model deﬁned ineq.
2, |dt| is the size of training dataset dt, andt can be any type of translation context.
in thisway, we actually obtain four models in total aftertraining.
in the inference process, for each testinginstance (x, cl, cr, s), we decide its context type tin terms of cl and cr and then use ˆθt to predict theword w..strategy 2: joint model the separate trainingstrategy is straightforward.
however, it may alsomake the models struck in the local optimal.
toaddress these issues, we also propose a joint train-ing strategy, which has the ability to stretch themodel out of the local optimal once the parametersis over-ﬁtting on one particular translation context.
therefore, using the joint training strategy, we traina single model for all types of translation contextby minimizing the following objective:.
l(d; ˆθ) = l(dzero; ˆθ) + l(dpreﬁx; ˆθ)+.
l(dsufﬁx; ˆθ) + l(dbi; ˆθ).
where each l(dt; θ) is as deﬁned in eq.
3.inthis way, we actually obtain a single model ˆθ aftertraining.
in the inference process, for each testinginstance (x, cl, cr, s) we always use ˆθ to predictthe target word w..5 experiments.
5.1 datasets.
datasets.
the training set for two directional chi-nese–english tasks consists of 1.25m bilingual sen-tence pairs from ldc corpora.
the toolkit we usedto convert chinese word w to phonetic symbolsis pypinyin4.
as discussed in (§3.2), the trainingdata for gwlan is extracted from 1.25m sentencepairs.
the validation data for gwlan is extractedfrom nist02 and the test datasets for gwlanare constructed from nist05 and nist06.
fortwo directional german–english tasks, we use thewmt14 dataset preprocessed by stanford5.
thevalidation and test sets for our tasks are based onnewstest13 and newstest14 respectively.
for eachdataset, the models are tuned and selected based onthe validation set..the main strategies we used to prepare ourbenchmarks are shown in §3.2.
however, lots oftrivial instances may be included if we directly usethe uniform distribution for sampling, e.g., predict-ing word “the” given “th”.
therefore, we applysome intuitive rules to reduce the probability oftrivial instances.
for example, we assign higherprobability for words with more than 4 charactersin english and 2 characters in chinese, and we re-quire that the lengths of input character sequence sand translation contexts c should not be too long..5.2 systems for comparison.
in the experiments, we evaluate and compare theperformance of our methods (wpm-sep and wpm-joint) and a few baselines.
they are illustratedbelow,.
wpm-sepis our approach with the “one con-text one model” training and inference strategy insection §4.3.
in other words, we train our modelfor each translation context separately..wpm-joint is our approach with the “jointmodel” strategy in section §4.3..transtable: we train an alignment model6on the training set and build a word-level transla-tion table.
while testing, we can ﬁnd the transla-tions of all source words based on this table, andselect out valid translations based on the humaninput.
the word with highest frequency amongall candidates is regarded as the prediction.
thisbaseline is inspired by huang et al.
(2015)..4https://github.com/mozillazg/.
python-pinyin.
5https://nlp.stanford.edu/projects/.
6https://github.com/clab/fast_align.
we carry out experiments on four gwlan tasksincluding bidirectional chinese–english tasks andgerman–english tasks.
the benchmarks for ourexperiments are based on the public translation.
nmt/.
4797# systems.
1 transtable2 trans-pe3 trans-npe4 wpm-sep5 wpm-joint.
zh⇒en.
en⇒zh.
de⇒en.
en⇒de.
nist05 nist06 nist05 nist06 nt13 nt14 nt13 nt1431.1226.9941.4030.6534.8834.5131.3036.1935.9751.4653.6754.1552.6854.2555.54.
28.0032.2334.3153.3053.64.
37.4334.4536.6956.9357.84.
32.9931.5133.2554.5456.91.
36.6433.0236.0155.6756.75.
39.7835.5036.7855.0455.85.table 1: the main results of different systems on chinese-english and german-english datasets.
the results inthis table are the averaged accuracy on four translation contexts (i.e., preﬁx, sufﬁx, zero-context, and bi-context)..# systems.
1 transtable2 trans-pe3 trans-npe4 wpm-sep5 wpm-joint.
preﬁx sufﬁx44.9941.9138.6129.8440.4337.3660.5958.4360.7159.91.zh⇒enzero44.1926.0829.5053.9955.35.bi43.2848.0644.4264.4662.30.avg.
43.5935.6437.9259.3659.56.preﬁx sufﬁx32.8029.7334.9730.6443.0536.1061.0560.0261.7361.39.en⇒zhzero29.7322.6732.0053.7653.87.bi29.6138.9545.7964.4663.78.avg.
30.4631.8039.2359.8260.19.table 2: the results of different systems on nist02.
we evaluate the performances of those systems on bothzh⇒en and en⇒zh tasks by accuracy..trans-pe: we train a vanilla nmt model us-ing the transformer-base model.
during the infer-ence process, we use the context on the left handside of human input as the model input, and returnthe most possible words based on the probability ofvalid words selected out by the human input.
thisbaseline is inspired by langlais et al.
(2000); santyet al.
(2019)..trans-npe: as another baseline, we also trainan nmt model based on transformer, but withoutposition encoding on the target side.
while testing,we use the averaged hidden vectors of all the targetwords outputted by the last decoder layer to predictthe potential candidates..5.3 main results.
table 1 shows the main results of our methods andthree baselines on the test sets of chinese-englishand german-english datasets.
it is clear from theresults that our methods wpm-sep and wpm-joint signiﬁcantly outperform the three baselinemethods.
results on row 4 and row 5 of table1 also show that the wpm-joint method, whichuses a joint training strategy to optimize a singlemodel, achieves better overall performance thanwpm-sep, which trains four models for differenttranslation contexts respectively.
in-depth analysisabout the two training strategies is presented in the.
next section..the method trans-pe, which assumes thehuman input is the next word of the given con-text, behaves poorly under the more general set-ting.
as the results of trans-npe show, whenwe use the same model as trans-pe and relaxthe constraint of position by removing the posi-tion encoding, the accuracy of the model improves.
one interesting ﬁnding is that the transtablemethod, which is only capable of leveraging thezero-context, achieves good results on the chinese-english task when the target language is english.
however, when the target language is chinese, theperformance of transtable drops signiﬁcantly..6 experimental analysis.
6.1 effects on different translation context.
in this section, we presents more detailed resultson the four translation contexts and analyze thefeatures of gwlan.
these analyses can help usto better understand the task and propose effectiveapproaches in the future..separate training vs. joint training com-pared with wpm-sep, wpm-joint shows twoadvantages.
on one hand, even there is only onemodel, wpm-joint yields better performancesthan wpm-sep, enabling simpler deployment.
this may be caused by that training on multiple.
4798related tasks can force the model learn more ex-pressive representations, avoiding over-ﬁtting.
onthe other hand, the variance of results on differ-ent translation contexts of wpm-joint is smaller,which can provide an more steady autocompletionservice.
from the viewpoint of joint training, thelower variance may be caused by that wpm-jointspends more efforts to minimize the one with max-imal risk (i.e., zero-context), although sometimesit may slightly sacriﬁce the task with minimal risk(i.e., bi-context)..the results of wpm-sep and wpm-joint alsohave some shared patterns.
firstly, the perfor-mances of the two methods on preﬁx and sufﬁxtranslation contexts are nearly the same.
althoughthe preﬁx and sufﬁx may play different roles in thesvo language structure, they have little impact onthe the autocompletion accuracy using our method.
moreover, among the results on four translationcontexts, the performances on bi-context are bet-ter than preﬁx and sufﬁx, and preﬁx and sufﬁx arebetter than zero-context.
this ﬁnding shows thatmore context information can help to reduce theuncertainty of human desired words..comparison with baselines the trans-pemethod in previous works is more sensitive to theposition of human input.
the statistical resultsshows that the averaged distances in the originalsentence between the prediction words and trans-lation contexts are various for different translationcontexts, which are 7.4, 6.5, 14.1, and 3.2 for pre-ﬁx, sufﬁx, zero-context, and bi-context, respec-tively.
when the desired words are much closer tothe context, trans-pe can achieve better perfor-mances.
moreover, trans-pe can achieve morethan 80 accuracy scores when the prediction wordis the next word of the given preﬁx, however, itsperformance drops signiﬁcantly when the word isnot necessarily conjunct to the preﬁx.
we can alsoﬁnd that trans-npe, which removes the positioninformation of target words, achieves better overallperformances compared with trans-pe..in contrast, the performance of transtableis less affected by the position of the predictionwords, which is demonstrated by the low varianceson both tasks in table 2. the results of transta-ble have also surprised us, which achieves morethan 41 accuracy scores on the zh⇒en task.
thisobservation shows the importance of alignment andthe potential of statistical models.
compared withthe results on the zh⇒en task, the overall accu-.
figure 4: robustness analysis.
the x-axis representsthe percentage of words that have been replaced bynoise tokens in nist02.
the model used for this analy-sis is the wpm-joint, which is trained on the zh⇒entask without noisy translation context..racy on en⇒zh task is much lower, likely due tothat the number of valid words after ﬁltered by thehuman input on chinese is much more than that onenglish.
therefore, it is easier for transtableto determine the human desired words in english..6.2 robustness on noisy contexts.
in this work, the translation contexts are simulatedusing the references.
however, in real-world sce-narios, translation contexts may not be perfect, i.e.,some words in the translation contexts may be in-correct.
in this section, we evaluate the robustnessof our model on noisy contexts.
we ﬁrst use thetranslation table constructed by transtable toﬁnd some target words that share the same sourcewords with the original target words, and then usethose found words as noise tokens..the robustness results are shown in figure 4.for all the translation context types except for zero-context, the performance drops slowly when thepercentage of noise tokens increases.
however,even with 80% words in the context, the perfor-mance of wpm-joint outperforms the case ofzero-context, which shows that our wpm-jointmethod is noise tolerant..6.3 discussion.
in this work, we formalize the task as a classiﬁ-cation problem.
however, the generation formal-ization also deserves to be explored in the future.
for example, the generation may happen in twocircumstances: word-level completion based on.
4799subwords, and phrase-level completion.
in the ﬁrstcase, although the autocompletion service providedfor human translators is word-level, in the internalsystem we can generate a sequence of subwords(sennrich et al., 2015) that satisfy the human typedcharacters, and provide human translators with themerged subwords.
this subword sequence gener-ation can signiﬁcantly alleviate the oov issue inthe word-level autocompletion.
in the phrase-levelautocompletion case, if we can predict more thanone desired words, the translation efﬁciency andexperience may be improved further.
we wouldlike to leave it as future work..it is also worth noting that we did not conducthuman studies in this work.
we think evidences inprevious work can already prove the effectivenessof word-level autocompletion when assisting hu-man translators.
for example, transtype (langlaiset al., 2000) is a simple rule-based tool that onlyconsiders the preﬁx context, but the majority oftranslators said that transtype improved their typ-ing speed a lot.
huang et al.
(2015) hired 12 profes-sional translators and systematically evaluate theirword autocompletion tool based on zero-context.
experiments show that the more keystrokes are re-duced, the more time can be saved for translators.
since the prediction accuracy is highly correlatedwith the keystrokes, we think higher accuracy willmake translators more productive.
that is the mainreason that we use accuracy to automatically evalu-ate the model performance.
besides, the automaticevaluation metric also makes the gwlan taskeasier to follow..7 conclusion.
we propose a general word-level autocomple-tion (gwlan) task for computer-aided translation(cat).
in our setting, we relax the strict constraintson the translation contexts in previous work, andabstract four most general translation contexts usedin real-world cat scenarios.
we propose two ap-proaches to address the variety of context types andweak position information issues in gwlan.
tosupport automatic evaluation and to ensure a conve-nient and fair comparison among different methods,we construct a benchmark for the task.
experi-ments on this benchmark show that our methodoutperforms baseline methods by a large margin onfour datasets.
we believe that this benchmark to bereleased will push forward future research in cat..acknowledgments.
we would like to thank three anonymous reviewersfor their invaluable discussions on this work.
thecorresponding is lemao liu..references.
vicent alabau, christian buck, michael carl, fran-cisco casacuberta, mercedes garc´ıa-mart´ınez, ul-rich germann, jes´us gonz´alez-rubio, robin hill,philipp koehn, luis a leiva, et al.
2014. casmacat:a computer-assisted translation workbench.
in pro-ceedings of the demonstrations at the conference ofthe european chapter of the association for compu-tational linguistics (eacl), pages 25–28..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin proceedings oflearning to align and translate.
the international conference on learning represen-tations (iclr)..sergio barrachina, oliver bender, francisco casacu-berta, jorge civera, elsa cubel, shahram khadivi,antonio lagarda, hermann ney, jes´us tom´as, en-rique vidal, et al.
2009. statistical approaches tocomputer-assisted translation.
computational lin-guistics (cl), 35(1):3–28..shanbo cheng, shujian huang, huadong chen, xinyudai, and jiajun chen.
2016.primt: a pick-revise framework for interactive machine transla-tion.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt), pages 1240–1249..david chiang.
2005. a hierarchical phrase-basedin pro-model for statistical machine translation.
ceedings of the annual meeting of the associationfor computational linguistics (acl), pages 263–270..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt), pages 4171–4186..georgiana dinu, prashant mathur, marcello federico,and yaser al-onaizan.
2019. training neural ma-chine translation to apply terminology constraints.
in proceedings of the annual meeting of the asso-ciation for computational linguistics (acl), pages3063–3068..jonas gehring, michael auli, david grangier, denisyarats, and yann n dauphin.
2017. convolutionalin proceedings ofsequence to sequence learning.
the international conference on machine learning(icml), pages 1243–1252..4800spence green, jeffrey heer, and christopher d man-ning.
2013. the efﬁcacy of human post-editing forlanguage translation.
in proceedings of the sigchiconference on human factors in computing sys-tems (chi), pages 439–448..spence green, sida i wang, jason chuang, jeffreyheer, sebastian schuster, and christopher d man-ning.
2014. human effort and machine learnabilityin computer aided translation.
in proceedings of theconference on empirical methods in natural lan-guage processing (emnlp), pages 1225–1236..eva hasler, adri`a de gispert, gonzalo iglesias, andbill byrne.
2018. neural machine translation decod-ing with terminology constraints.
in proceedings ofthe conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies (naacl-hlt), pages506–512..chris hokamp and qun liu.
2017. lexically con-strained decoding for sequence generation using gridin proceedings of the annual meet-beam search.
ing of the association for computational linguistics(acl), pages 1535–1546..j. edward hu, huda khayrallah, ryan culkin, patrickxia, tongfei chen, matt post, and benjaminimproved lexically constrainedvan durme.
2019.decoding for translation and monolingual rewriting.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt), pages 839–850..guoping huang, lemao liu, xing wang, longyuewang, huayang li, zhaopeng tu, chengyan huang,and shuming shi.
2021. transmart: a practical in-teractive machine translation system.
arxiv preprintarxiv..guoping huang,.
jiajun zhang, yu zhou,.
andchengqing zong.
2015. a new input method forhuman translators:integrating machine translationeffectively and imperceptibly.
in proceedings of theinternational joint conference on artiﬁcial intelli-gence (ijcai), pages 1163–1169..yafang huang, zuchao li, zhuosheng zhang, and haizhao.
2018. moon ime: neural-based chinese pinyinaided input method with customizable association.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics (acl): sys-tem demonstrations, pages 140–145..tomoyuki kajiwara.
2019. negative lexically con-strained decoding for paraphrase generation.
in pro-ceedings of the annual meeting of the associationfor computational linguistics (acl), pages 6047–6052..rebecca knowles and philipp koehn.
2016. neuralinteractive translation prediction.
in proceedings ofthe association for machine translation in the amer-icas (amta), pages 107–120..philipp koehn.
2009. statistical machine translation..cambridge university press..philipp koehn, franz j. och, and daniel marcu.
2003.in proceed-statistical phrase-based translation.
ings of the human language technology confer-ence of the north american chapter of the associa-tion for computational linguistics (naacl), pages127–133..philippe langlais, george foster, and guy lapalme.
2000. transtype: a computer-aided translation typ-ing system.
in anlp-naacl 2000 workshop: em-bedded machine translation systems..kai-fu lee, zheng chen, and jian han.
2007. lan-guage input architecture for converting one text formto another text form with modeless entry.
us patent7,165,019..huayang li, guoping huang, deng cai, and lemaoliu.
2020. neural machine translation with noisylexical constraints.
ieee/acm transactions on au-dio, speech, and language processing (taslp),28:1864–1874..´alvaro peris, miguel domingo, and francisco casacu-berta.
2017. interactive neural machine translation.
computer speech & language, 45:201–220..matt post and david vilar.
2018. fast lexically con-strained decoding with dynamic beam allocation forin proceedings of theneural machine translation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies (naacl-hlt), pages 1314–1324..sebastin santy, sandipan dandapat, monojit choud-hury, and kalika bali.
2019.inmt: interactivein proceed-neural machine translation prediction.
ings of the conference on empirical methods innatural language processing and the internationaljoint conference on natural language processing(emnlp-ijcnlp): system demonstrations, pages103–108..rico sennrich, barry haddow, and alexandra birch.
2015. neural machine translation of rare words withsubword units.
in proceedings of the annual meet-ing of the association for computational linguistics(acl), pages 1715–1725..kai song, yue zhang, heng yu, weihua luo, kunwang, and min zhang.
2019. code-switching forenhancing nmt with pre-speciﬁed translation.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt), pages 449–459..raymond hendy susanto, shamil chollampatt, andliling tan.
2020. lexically constrained neural ma-inchine translation with levenshtein transformer.
proceedings of the annual meeting of the associ-ation for computational linguistics (acl), pages3536–3543..4801ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems (neurips), pages 3104–3112..muriel vasconcellos.
and marjorie le´on.
1985.spanam and engspan: machine translation at thepan american health organization.
computationallinguistics (cl), 11(2-3):122–136..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems (neurips), pages 5998–6008..qian wang, jiajun zhang, lemao liu, guoping huang,and chengqing zong.
2020.touch editing: aﬂexible one-time interaction approach for transla-tion.
in proceedings of the conference of the asia-paciﬁc chapter of the association for computa-tional linguistics and the international joint con-ference on natural language processing (aacl-ijcnlp), pages 1–11..rongxiang weng, hao zhou, shujian huang, lei li,yifan xia, and jiajun chen.
2019. correct-and-memorize: learning to translate from interactive re-in proceedings of the international jointvisions.
conference on artiﬁcial intelligence (ijcai), pages5255–5263..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap between hu-arxiv preprintman and machine translation.
arxiv:1609.08144..joern wuebker, spence green, john denero, saˇsahasan, and minh-thang luong.
2016. models andinference for preﬁx-constrained machine translation.
in proceedings of the annual meeting of the asso-ciation for computational linguistics (acl), pages66–75..tianxiang zhao, lemao liu, guoping huang,zhaopeng tu, huayang li, yingling liu, guiquanliu, and shuming shi.
2020. balancing quality andhuman involvement: an effective approach to inter-in proceedingsactive neural machine translation.
of the aaai conference on artiﬁcial intelligence(aaai), pages 9660–9667..4802