ildc for cjpe: indian legal documents corpus for court judgmentprediction and explanation.
vijit malik1kripa ghosh2.
shubham kumar nigam1.
rishabh sanjay1shouvik kumar guha3ashutosh modi11indian institute of technology kanpur (iit-k)2indian institute of science education and research kolkata (iiser-k)3west bengal national university of juridical sciences (wbnujs){vijitvm,rsan,sknigam}@iitk.ac.in.
arnab bhattacharya1.
kripaghosh@iiserkol.ac.in.
shouvikkumarguha@nujs.edu.
{arnabb,ashutoshm}@cse.iitk.ac.in.
abstractan automated system that could assist a judgein predicting the outcome of a case would helpexpedite the judicial process.
for such a sys-tem to be practically useful, predictions by thesystem should be explainable.
to promote re-search in developing such a system, we intro-duce ildc (indian legal documents corpus).
ildc is a large corpus of 35k indian supremecourt cases annotated with original court de-cisions.
a portion of the corpus (a separatetest set) is annotated with gold standard ex-planations by legal experts.
based on ildc,we propose the task of court judgment pre-diction and explanation (cjpe).
the task re-quires an automated system to predict an ex-plainable outcome of a case.
we experimentwith a battery of baseline models for case pre-dictions and propose a hierarchical occlusionbased model for explainability.
our best pre-diction model has an accuracy of 78% versus94% for human legal experts, pointing towardsthe complexity of the prediction task.
theanalysis of explanations by the proposed al-gorithm reveals a signiﬁcant difference in thepoint of view of the algorithm and legal ex-perts for explaining the judgments, pointing to-wards scope for future research..1.introduction.
in many of the highly populated countries like in-dia, there is a vast number of pending backlog oflegal cases that impede the judicial process (katju,2019).
the backlog is due to multiple factors,including the unavailability of competent judges.
therefore, a system capable of assisting a judge bysuggesting the outcome of an ongoing court caseis likely to be useful for expediting the judicialprocess.
however, an automated decision systemis not tenable in law unless it is well explained in.
terms of how humans understand the legal process.
hence, it is necessary to explain the suggestion.
inother words, we would like such a system to pre-dict not only what should be the ﬁnal decision of acourt case but also how one arrives at that decision.
in this paper, we introduce indian legal doc-uments corpus (ildc) intending to promoteresearch in developing a system that could assistin legal case judgment prediction in an explainableway.
ildc is a corpus of case proceedings from thesupreme court of india (sci) that are annotatedwith original court decisions.
a portion of ildc(i.e., a separate test set) is additionally annotatedwith gold standard judgment decision explanationsby legal experts to evaluate how well the judgmentprediction algorithms explain themselves..based on ildc, we propose a new task: courtjudgment prediction and explanation(cjpe).
this task aims to predict the ﬁnal deci-sion given all the facts and arguments of the caseand provide an explanation for the predicted deci-sion.
the decision can be either allowed, whichindicates ruling in favor of the appellant/petitioner,or dismissed, which indicates a ruling in favor ofthe respondent.
the explanations in the cjpe taskrefer to sentences/phrases in the case descriptionthat best justify the ﬁnal decision.
since, we areaddressing mainly the sci cases, one might arguethat the usefulness of the task may be limited since,the legislative provisions can always change withtime.
however, the legal principles of how to applya given law to a given set of facts remain constantfor prolonged periods..judgment prediction and explanation in thecjpe task are far more challenging than a stan-dard text-classiﬁcation task for multiple reasons.
firstly, the legal court case documents (especially.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4046–4062august1–6,2021.©2021associationforcomputationallinguistics4046in indian context) are unstructured and are usuallyquite long, verbose, and noisy.
there is no easyway of extracting and directly using the facts andarguments.
secondly, the domain-speciﬁc lexiconused in court cases makes models pre-trained ongenerally available texts ineffective on such doc-uments.
consequently, the standard models needto be adapted to the legal domain for the proposedjudgment prediction on court cases.
thirdly, ex-plaining prediction in legal documents is consider-ably more challenging as it requires understandingthe facts, following the arguments and applyinglegal rules, and principles to arrive at the ﬁnal deci-sion.
our main contributions can be summarized as:1. we create a new corpus, indian legal doc-uments corpus (ildc), annotated with courtdecisions.
a portion of the corpus (i.e.
a separatetest set) is additionally annotated with explanationscorresponding to the court decisions.
we performdetailed case studies on the corpus to understanddifferences in prediction and explanation annota-tions by legal experts, indicative of the computa-tional challenges of modeling the data.
2. we introduce a new task, court judgmentprediction and explanation (cjpe), withthe two sub-tasks: (a) court judgment prediction(cjp) and (b) explanation of the prediction.
whilecjp is not a novel task per se; however, in combi-nation with the explanation part, the cjpe task isnew.
moreover, the requirement for explanationsalso puts restrictions on the type of techniques thatcould be tried for cjp.
in the cjpe task, gold ex-planations are not provided in the train set; the taskexpects that the trained algorithms should explainthe predictions without requiring additional infor-mation in the form of annotations during training.
3. we develop a battery of baseline models for thecjpe task.
we perform extensive experimentationwith state-of-the-art machine learning algorithmsfor the judgment prediction task.
we develop anew method for explaining machine predictionssince none of the existing methods could be readilyapplied in our setting.
we compare model explain-ability results with annotations by legal experts,showing signiﬁcant differences between the pointof view of algorithms and experts..munity needs to pursue more research in this regardto fully understand the unforeseen social implica-tions of such models.
this paper takes initial stepsby introducing the corpus and baseline models tothe community.
moreover, we plan to continue togrow, revise and upgrade ildc.
we release theildc and code for the prediction and explanationmodels via github1..2 related workthere has been extensive research on legal do-main text, and various corpora and tasks havebeen proposed e.g., prior case retrieval (jacksonet al., 2003), summarization (tran et al., 2019;bhattacharya et al., 2019a), catchphrase extraction(galgani et al., 2012), crime classiﬁcation (wanget al., 2019), and judgment prediction (zhong et al.,2020).
why ildc?
the task of legal judgment predic-tion (ljp) and its corresponding corpora (chalkidiset al., 2019; zhong et al., 2020; yang et al., 2019a;xiao et al., 2018) are related to our setting.
inthe ljp task, given the facts of a case, violations,charges (e.g., theft) and terms of penalty are pre-dicted.
however, the ildc and the cjpe task intro-duced in this paper differ from the existing ljp cor-pora and task in multiple ways.
firstly, we requireprediction algorithms to explain the decisions in thecjpe task, to evaluate the explanations we providea separate test set annotated with gold explanations.
secondly, in the ljp task, typically, the facts of acase are explicitly provided.
however, in our case,only unannotated unstructured documents are pro-vided.
ildc addresses a more realistic/practicalsetting, and consequently, cjpe is a much morechallenging task.
moreover, the bare facts do notform the judgment premise of a case since facts aresubject to interpretations.
a court case description,in practice, has other vital aspects like ruling bylower court, arguments, statutes, precedents, andratio of the decision (bhattacharya et al., 2019b)that are instrumental in decision making by thejudge(s).
unlike ljp, we consider (along with thefacts) the entire case (except the judgment), andwe predict the judgment only.
work by stricksonand de la iglesia (2020) comes close to our set-ting, where the authors prepared the test set on ukcourt cases by removing the ﬁnal decision fromrulings and employed classical machine learningmodels.
thirdly, to the best of our knowledge,1https://github.com/exploration-lab/.
ildc is introduced to promote the developmentof a system/models that will augment humans andnot replace them.
we have covered the ethicalconsiderations in the paper.
nevertheless, the com-.
cjpe.
4047we are the ﬁrst to create the largest legal corpus(34, 816 documents) for the indian setting.
it isimportant because india has roots in the commonlaw system and case decisions are not strictly asper the statute law, with the judiciary having thediscretion to interpret their version of the legal pro-visions as applicable to the case at hand; this cansometimes make the decision process subjective.
fourth, we do not focus on any particular classof cases (e.g., criminal, civil) but address publiclyavailable generic sci case documents..xiao et al.
(2018) released the chinese ai andlaw challenge dataset (cail2018) in chinese forjudgment prediction, that contains more than 2.68million criminal cases published by the supremepeople’s court of china.
chalkidis et al.
(2019) re-leased an english legal judgment prediction dataset,containing 11, 478 cases from the european courtof human rights (echr).
it contains facts, arti-cles violated (if any), and an importance score foreach case.
ildc contrasts with the existing ljpcorpora, where mainly the civil law system andcases are considered.
though the proposed corpusfocuses on indian cases, our analysis reveals (§ 4.2)that the language used in the cases is quite chal-lenging to process computationally and provides agood playground for developing realistic legal textunderstanding systems..several different approaches and corpora havebeen proposed for the ljp task.
chalkidis et al.
(2019) proposed a hierarchical version of bert(devlin et al., 2019) to alleviate bert’s input to-ken count limitation for the ljp task.
yang et al.
(2019a) applied multi-perspective bi-feedbacknetwork for predicting the relevant law articles,charges, and terms of penalty on chinese ai andlaw challenge (cail2018) datasets.
xu et al.
(2020) proposed a system for distinguishing con-fusing law articles in the ljp task.
zhong et al.
(2018) applied topological multi-task learning on adirected acyclic graph to predict charges like theft,trafﬁc violation, intentional homicide on three chi-nese datasets (cjo, pku, and cail).
luo et al.
(2017) proposed an attention-based model to pre-dict the charges given the facts of the case alongwith the relevant articles on a dataset of criminallaw of the people’s republic of china.
hu et al.
(2018) used an attribute-attentive model in a few-shot setup for charge prediction from facts of thecase.
long et al.
(2019) predicts the decision of thecase using a legal reading comprehension tech-.
corpus(avg.
tokens).
ildcmulti(3231)ildcsingle(3884)ildcexpert(2894).
number of docs(accepted class %)validation.
test.
1517(50.23%).
994(50%).
train32305(41.43%)5082(38.08%).
56 (51.78%).
table 1: ildc statistics.
nique on a chinese dataset.
chen et al.
(2019) useda deep gating network for prison term prediction,given the facts and charges on a dataset constructedfrom documents of the supreme people’s court ofchina.
aletras et al.
(2016) used linear svm topredict violations from facts on european courtof human rights cases.
s¸ ulea et al.
(2017) usedsvm in the ljp task on french supreme courtcases.
katz et al.
(2017) presented a random for-est model to predict the “reverse”, “afﬁrm”, and“other” decisions of us supreme court judges.
wealso experiment with some of these models as base-lines for the cjpe task (§ 5)..explainability in a system is of paramount im-portance in the legal domain.
zhong et al.
(2020)presented a qa based model using reinforcementlearning for explainable ljp task on three chinesedatasets (cjo, pku, and cail).
the model aimsto predict the appropriate crime by asking relevantquestions related to the facts of the case.
jiang et al.
(2018) used a rationale augmented classiﬁcationmodel for the charge prediction task.
the modelselects as rationale the relevant textual portions inthe fact description.
ye et al.
(2018) used label-conditioned seq2seq model for charge predictionon chinese legal documents, and the interpretationcomprise the selection of the relevant rationales inthe text for the charge.
we develop an explainabil-ity model based on the occlusion method (§ 5.2)..3.indian legal document corpus.
in this paper, we introduce the indian legaldocuments corpus (ildc), a collection ofcase proceedings (in the english language) fromthe supreme court of india (sci).
for a case ﬁledat the sci, a decision (“accepted” v/s “rejected”)is taken between the appellant/petitioner versus therespondent by a judge while taking into account thefacts of the case, ruling by lower court(s), if any,arguments, statutes, and precedents.
for every caseﬁled in the supreme court of india (sci), the judge.
4048(or a bench) decides on whether the claim(s) ﬁledby the appellant/petitioner against the respondentshould be “accepted” or “rejected”.
the decision isrelative to the appellant.
in ildc, each of the caseproceeding document is labeled with the originaldecision made by the judge(s) of the sci, whichserve as the gold labels.
in addition to the groundtruth decision, a separate test set documents areannotated (by legal experts) with explanations thatled to the decision.
the explanations annotationsare ranked in the order of importance..ildc creation.
we extracted all the publiclyavailable sci2 case proceedings from the year1947 to april 2020 from the website: https://indiankanoon.org.
case proceedings are un-structured documents and have different formatsand sizes, have spelling mistakes (since these aretyped during the court hearing), making it challeng-ing to (pre-)process.
we used regular expressionsto remove the noisy text and meta-information (e.g.,initial portions of the document containing casenumber, judge name, dates, and other meta informa-tion) from the proceedings.
in practice, as pointedby the legal experts, the judge deciding the caseand other meta information inﬂuence the ﬁnal de-cision.
in sci case proceedings, the decisions arewritten towards the end of the document.
these endsection(s) directly stating the decision have beendeleted from the documents in ildc since that iswhat we aim to predict.
each case’s actual decisionlabel has been extracted from the deleted end sec-tions of the proceeding using regular expressions.
another challenge with sci case proceedings is thepresence of cases with multiple petitions where, ina single case, multiple petitions have been ﬁled bythe appellant leading to multiple decisions.
con-sequently, we divided ildc documents into twosets.
the ﬁrst set, called ildcsingle, either havedocuments where there is a single petition (and,thus, a single decision) or multiple petitions, butthe decisions are the same across all those petitions.
the second set, called ildcmulti, is a superset ofildcsingle and has multiple appeals leading to dif-ferent decisions.
predicting multiple different deci-sions for cases with multiple appeals is signiﬁcantlychallenging.
in this paper, we do not develop anybaseline computational models for this setting; weplan to address this in future work.
for the com-2although indiankanoon includes lower court cases aswell, they do not have a common structural format and manyof the case documents in lower courts may be in a regionalindian language.
hence, for now we only use sci documents..putational models for the cjpe task, in the caseof ildcmulti, even if a single appeal was acceptedin the case having multiple appeals/petitions, weassigned it the label as accepted.
table 1 shows thecorpus statistics for ildc.
note that the validationand test sets are the same for both ildcmulti andildcsingle..temporal aspect.
the corpus is randomly dividedinto train, validation, and test sets, with the restric-tion that validation and test sets should be balancedw.r.t.
the decisions.
the division into train, devel-opment, and test set was not based on any tempo-ral consideration or stratiﬁcation because the sys-tem’s objective that may eventually emerge fromthe project is not meant to be limited to any partic-ular law(s), nor focused on any particular period oftime.
on the contrary, the aim is to identify stan-dard features of judgments pronounced in relationto various legislation by different judges and acrossdifferent temporal phases, to be able to use the saidfeatures to decipher the judicial decision-makingprocess and successfully predict the nature of theorder ﬁnally pronounced by the court given a setof facts and legal arguments.
while there would bea degree of subjectivity involved, given the differ-ence in the thoughts and interpretations adopted bydifferent judges, such differences are also found be-tween two judges who are contemporaries of eachother, as much as between two judges who havepronounced judgments on similar matters across agap of decades.
the focus is, therefore, to develop asystem that would be equally successful in predict-ing the outcome of a judgment given the law thathad been in vogue twenty years back, as it would inrelation to the law that is currently in practice.
thevalidity and efﬁcacy of the system can therefore beequally tested by applying it to cases from yearsback, as to cases from a more recent period.
in fact,if the system cannot be temporally independent,and remains limited to only successful predictionof contemporary judgments, then it is likely to failany test of application because by the time the ﬁnalversion of the system can be ready for practicalapplications on a large scale, the laws might getamended or replaced, and therefore, the judgmentsthat would subsequently be rendered by the courtmight be as different from one pronounced today,as the latter might differ from one pronounced inthe twentieth century.
not acknowledging time asa factor during data sample choice, therefore, ap-pears to be the prudent step in this case, especially.
4049given the exponential rate at which legislation isgetting amended today, as well as the fast-pacedgrowth of technological development..legal expert annotations.
in our case, the legalexpert team consisted of a law professor and hisstudents at a reputed national law school.
we tooka set of 56 documents (ildcexpert) from the testset, and these were given to 5 legal experts.
ex-perts were requested to (i) predict the judgment,and (ii) mark the sentences that they think are ex-planations for their judgment.
each document wasannotated by all the 5 experts (in isolation) usingthe webanno framework (de castilho et al., 2016).
the annotators could assign ranks to the sentencesselected as explanations; a higher rank indicatesmore importance for the ﬁnal judgment.
the ra-tionale for rank assignment to the sentences is asfollows.
rank 1 was given to sentences immedi-ately leading to the decision.
rank 2 was assignedto sentences that contributed to the decision.
rank3 was given to sentences indicative of the disagree-ment of the current court with a lower court/tribunaldecision.
sentences containing the facts of the case,not immediately, leading to decision making, butare essential for the case were assigned rank 4(or lower).
note in practice, only a small set ofsentences of a document were assigned a rank.
al-though documents were annotated with explana-tions in order of ranks, we did not have a similarmechanism in our automated explainability mod-els.
from the machine learning perspective, thisis a very challenging task, and to the best of ourknowledge, none of the state-of-the-art explainabil-ity models are capable of doing this.
annotation ofexplanations is a very specialized, time-consuming,and laborious effort.
in the current version of ildcwe provide explanation annotations to only a smallportion of the test set, this is for evaluating predic-tion algorithms for the explainability aspect.
eventhis small set of documents is enough to highlightthe difference between the ml-based explainabilitymethods and how a legal expert would explain adecision (§ 5.3).
nevertheless, we plan to continueto grow the corpus by adding more explainabilityannotations and other types of annotations.
more-over, we plan to include lower courts like indianhigh court cases and tribunal cases.
the corpusprovides new research avenues to be explored bythe community..might creep in.
we have not made any speciﬁcchoice with regard to any speciﬁc law or any cate-gory of cases, i.e., the sampling of cases was com-pletely random.
as explained earlier, we took careimportantly, the namesof the temporal aspect.
of the judge(s), appellants, petitioners, etc., wereanonymized in the documents so that no inherentbias regarding these creeps in.
the anonymizationwith respect to judge names is necessary as legalexperts pointed out that a judge’s identity can some-times be a strong indicator of the case outcome.
itis noteworthy that according to the legal expertsif we had not done the same, we could have hadhigher prediction accuracy.
the subjectivity asso-ciated with judicial decision-making may also becontrolled in this way since the system focuses onhow consideration of the facts and applicable laware supposed to determine the outcome of the cases,instead of any individual bias on the judge’s part.
we also address the ethical concerns in the end..4 annotation analysiswe performed a detailed analysis of case predic-tions and the explanations annotations.
with as-sistance from a legal expert, we also performeddetailed studies for some court cases to understandthe task’s complexity and possible reasons for de-viations between the annotators.
4.1 case judgment accuracy.
we computed the case judgment accuracy of the an-notators with respect to original decisions by judgesof sci.
the results are shown in table 2. thoughthe values are high, none of these are 100%.
theaccuracy indicates that no annotator agrees with theoriginal judgment in all the cases.
this possiblydepicts the subjectivity in the legal domain withregard to decision making.
the subjectivity aspecthas also been observed in other tasks that involvehuman decision-making, e.g., sentiment and emo-tion analysis.
we performed detailed case studieswith the help of experts to further probe into thisdifference in judgment.
due to space limitations,we are not able to present the studies here; pleaserefer to appendix a and github repository for de-tails.
to summarize, the study indicated that thesources of confusion are mainly due to differencesin linguistic interpretation (by the annotators) ofthe legal language given in the case document..4.2.inter-annotator agreements.
fairness and bias.
while creating the corpus, wetook all possible steps to mitigate any biases that.
agreement in the judgment prediction: for thequantitative evaluation, we calculate pair-wise.
4050expertexpert 1expert 2expert 3expert 4expert 5.accuracy (%)94.6491.0798.2189.2896.43.agreement (%) expert 1 expert 2 expert 3 expert 4 expert 5.expert 1expert 2expert 3expert 4expert 5.
100.087.594.685.789.3.
87.5100.092.987.591.1.
94.692.9100.091.194.6.
85.787.591.1100.089.3.
89.391.194.689.3100.0.table 2: annotators’ accuracy..table 3: pairwise inter-annotator agreement for judgment prediction..tic reasoning for the decision.
they look at bothsubstantive (sections applicable) and procedural(about the jurisdiction of a lower court) aspects ofthe case.
the differences among them are largelydue to consideration/non-consideration of the fac-tual sentences.
on the other hand, expert 2 and ex-pert 5 often use bare-minimum reasoning leading tothe ﬁnal judgment instead of looking at the exhaus-tive set of reasons and did not always cover bothsubstantive and procedural aspects of the case..analysis of annotations gives insights into theinherent complexity and subjectivity of the task.
legal proceedings are long, verbose, often chal-lenging to comprehend, and exhibit interesting (andcomputationally challenging) linguistic phenom-ena.
for example, in a case numbered “1962 47”(appendix a), sentence 17 of the case appears torefer to the supreme court having accepted a pre-vious appeal for which a review has been requested(i.e., the current appeal).
this amounted to the factthat the court actually rejected the present appealwhile accepting the previous one.
such intricaciescan confuse even legal experts..5 cjpe task.
given a case proceeding from the sci, the task ofcourt judgment prediction and expla-nation (cjpe) is to automatically predict thedecision for the case (with respect to the appel-lant) and provide the explanation for the decision.
we address the cjpe task via two sub-tasks in thefollowing sequence: prediction and explanation.
prediction: given a case proceeding d, the task isto predict the decision y ∈ {0, 1}, where the label 1corresponds to the acceptance of the appeal/petitionof the appellant/petitioner.
explanation: given the case proceeding and thepredicted decision for the case, the task is to explainthe decision by predicting important sentences thatlead to the decision.
annotated explanations arenot provided during training; the rationale is thata model learned for prediction should explain thedecision without explicit training on explanations,since explanation annotations are difﬁcult to obtain..figure 1: explanation agreement among the annotators.
agreement between the annotators as shown in ta-ble 3. the highest agreement (94.6%) is betweenexperts 1-3 and 3-5. we also calculate fleiss’kappa (fleiss, 1971) as 0.820, among all the ﬁveannotators, which indicates high agreement..agreement in the explanation: there are no stan-dard metrics for evaluating annotator agreementsfor textual annotations.
for quantitative evaluationof agreements among the annotators for explana-tions, we took inspiration from machine transla-tion community and used metrics like rouge-l,rouge-1, rouge-2 (lin, 2004), bleu (pap-ineni et al., 2002) (unigram and bigram averaging),meteor (lavie and agarwal, 2007), jaccard sim-ilarity, overlap maximum and overlap minimum3.
the result for rouge-l (averaged out over alldocuments)4 is shown in figure 1. the highestoverlap across all the metrics is observed betweenexpert 3 and expert 4. the highest value (0.9129)is between expert 2 and expert 4 for overlap-min.
we also performed a qualitative evaluation of theagreements in the explanations.
we observed thatexpert 1, expert 3, and expert 4 consider holis-3overlap max: size of the intersection divided by the max-imum size out of the two sample sets that are being compared.
overlap min: size of the intersection divided by the minimumsize out of the two sample sets that are being compared.
4due to space constraints we are not able to show heatmapscorresponding to other metrics but they showed similar trends.
for the heatmaps for other metrics please refer to our githubrepository..4051user 1user 2user 3user 4user 5user 1user 2user 3user 4user 51.00.63130.78690.80480.65090.63091.00.62230.62410.60830.78690.62241.00.86940.65930.80480.6240.86951.00.67650.65160.60970.65980.67631.0rouge-l0.640.720.800.880.965.1 case decision prediction.
ildc documents are long and have specializedvocabulary compared to typical corpora used fortraining text classiﬁcation models and languagemodels.
we initially experimented with non-neuralmodels based on text features (e.g., n-grams, tf-idf, word based features, and syntactic features)and existing pre-trained models (e.g., pre-trainedword embeddings based models, transformers), butnone of them were better than a random classiﬁer.
consequently, we retrained/ﬁne-tuned/developedneural models for our setting.
in particular, weran a battery of experiments and came up withfour different types of models: classical models,sequential models, transformer models, and hier-archical transformer models.
table 4 summarizesthe performance of different models.
due to spaceconstraints, we are not able to describe each of themodels here.
we give a very detailed descriptionof model implementations in appendix b..classical models: we considered classical mlmodels like word/sentence embedding based lo-gistic regression, svm, and random forest.
wealso tried prediction with summarized legal (bhat-tacharya et al., 2019a) documents; however, theseresulted in a classiﬁer no better than random clas-siﬁer.
as shown in table 4, classical models didnot perform so well.
however, model based ondoc2vec embeddings had similar performance assequential models..we extensively experimented with dividing doc-uments into chunks and training the model usingeach of the chunks separately.
we empiricallydetermined that sequential and transformer-basedmodels performed the best on the validation setusing the last 512 tokens5 of the document.
intu-itively, this makes sense since the last parts of caseproceedings usually contain the main informationabout the case and the rationale behind the judg-ment.
we also experimented with different sectionsof a document, and we observed last 512 tokensgave the best performance..sequence models: we experimented with stan-dard bigru (2 layers) with attention model.
wetried 3 different types of embeddings: (i) wordlevel trained glove embeddings (pennington et al.,2014), with last 512 tokens as input, (ii) sentencelevel embeddings (sent2vec), where last 150 sen-5length of 512 was partly inﬂuenced by the maximum.
input token limit of bert.
model.
macroprecision(%).
macrorecall(%).
macrof1(%).
accuracy(%).
60.13.
61.00.
63.0357.19.classical models on ildcmulti train setdoc2vec + lrsent2vec + lrsequential models on ildcmulti train setsent2vec + bigru + att.
doc2vec + bigru + att.
glove + bigru + att.
hansequential models on ildcsingle train setsent2vec + bigru+ att.
doc2vec + bigru + att.
glove + bigru + att.
hancatchphrases + sent2vec+ bigru + att..60.0558.0766.9257.64.
60.9857.1868.2659.96.
61.90.
61.0055.55.
58.4056.0360.8759.57.
55.857.4462.3055.56.transformer models on ildcmulti train set57.64bert base62.22bert base63.85bert base60.58bert base67.31bert base64.26distillbertroberta71.31xlnet70.07hierarchical models on ildcmulti train set70.42bert + bigru.
60.5667.5467.2466.1269.3365.2172.2572.09.
70.98.roberta + bigru.
75.13.
74.30.xlnet + bigru77.7870.17bert + cnn73.17roberta + cnnxlnet + cnn77.21hierarchical models on ildcsingle train set.
77.8071.6874.7477.84.
62.0056.36.
59.6656.6064.3559.77.
57.8557.7564.5356.58.
59.0664.7765.5063.2368.3164.7371.7771.07.
70.6974.71(±0.01)77.7970.9273.9577.53.
65.28.
63.95.bert + bigru.
roberta + bigru.
64.27(±0.0116)73.09(±0.0022)75.09(±0.0043)hierarchical models with attention on ildcmulti train set.
xlnet + bigru.
73.24.
72.93.
75.11.
75.06.
70.98.
71.31.bert + bigru + att..roberta + bigru + att..71.14(±0.0011)75.38(±0.0004)77.07(±0.0077)hierarchical models with attention on ildcsingle train set.
xlnet + bigru + att..74.88.
77.32.
76.82.
75.89.bert + bigru + att..68.30.
62.05.roberta + bigru + att..73.39.
72.66.xlnet + bigru + att..75.26.
75.22.
65.03(±0.0084)73.02(±0.0017)75.25(±0.0009).
60.9155.44.
58.3157.4460.7559.53.
55.6759.2362.255.44.
60.06.
57.6562.1063.7460.4567.2464.2171.2670.01.
70.3874.33(±1.99)77.7870.1273.2277.24.
63.89(±1.10)72.95(±0.25)75.06(±0.42).
71.26(±0.09)74.91(±0.11)77.01(±0.52).
61.93(±0.68)72.69(±0.29)75.22(±0.13).
transformers voting ensembleroberta68.2067.84xlnethierarchical concatenated model with attention on ildcsingle train76.32(±2.43).
76.55(±0.0140).
xlnet + bigru.
65.2663.72.
62.4359.92.
62.5560.07.
76.85.
76.31.table 4: prediction results using different models.
some of the transformer and hierarchical models varyin performance across runs, we average out perfor-mance across 3 runs (variance in the parenthesis)..tences were input6, and (iii) chunk level embed-dings (trained via doc2vec).
we also trained hi-erarchical attention network (han) (yang et al.,2016) model.
glove embeddings with bigru and6last 150 sentences covered around 90% of the documents.
4052ent complexity and motivates more research in thisdirection..5.2 case decision explanation.
we experimented with a variety explainability algo-rithms as a post-prediction step.
we experimentedwith the best judgment prediction model (hierar-chical transformer (xlnet + bigru)) for all theexplainable algorithms.
we explored three class ofexplainability methods (xie et al., 2020): attribu-tion based, model agnostic, and attention-based..in the class of attribution based methods, lay-erwise relevance propagation (lrp) (bach et al.,2015) and deeplift (shrikumar et al., 2017) meth-ods did not work in our case.
due to the long lengthof documents, model agnostic explainability meth-ods like lime (ribeiro et al., 2016) and anchors(ribeiro et al., 2018) were not applicable.
we alsoexperimented with attention-based methods, andintegrated gradients (sundararajan et al., 2017)method using the captum library (kokhlikyanet al., 2019).
however, these highlighted only a fewtokens or short phrases.
moreover, attention-basedscores are not necessarily indicative of explanations(jain and wallace, 2019)..to extract explanations, we propose a methodinspired from li et al.
(2016) and zeiler and fergus(2014).
the idea is to use the occlusion method atboth levels of the hierarchy.
for each document,for the bigru part of the model, we mask eachcomplete chunk embedding one at a time.
themasked input is passed through the trained bigru,and the output probability (masked probability) ofthe label obtained by the original unmasked modelis calculated.
the masked probability is comparedwith unmasked probability to calculate the chunkexplainability score.
formally, for a chunk c, if thesigmoid outputs (of the bigru) are σm (when thechunk was not masked) and σm(cid:48) (when the chunkwas masked) and the predicted label is y then theprobabilities and chunk score sc = pm − pm(cid:48) and.
pm(cid:48)/m =.
(cid:40).
y = 1σm(cid:48)/m,1 − σm(cid:48)/m, y = 0.we obtain sentences that explain the decisionfrom the transformer part of the model (xlnet)using the chunks that were assigned positive scores.
each chunk (length 512 tokens) is segmented intosentences using nltk sentence splitter (loper andbird, 2002).
similar to bigru, each sentence ismasked and the output of the transformer at theclassiﬁcation head (softmax logits) is compared.
figure 2: hierarchical xlnet architecture (xlnet +bigru).
attention model gave the best performance (64%f1) among the sequential models.
sequential mod-els trained on ildcmulti and ildcsingle have simi-lar performancestransformer models: we experimented withbert (devlin et al., 2019), distilbert (sanhet al., 2019), roberta (liu et al., 2019), and xl-net (yang et al., 2019b).
due to limitation on thenumber of input tokens to bert and other trans-former models, we experimented with differentsections (begin tokens, middle tokens, end tokens,combinations of these) of the documents and asshown in table 4, the last 512 tokens gave thebest performance.
in general, transformer mod-els outperform classical and sequential models.
roberta gave the best performance (72% f1) anddistilbert was the worst.
we did not experimentwith domain speciﬁc transformers like legal-bert (chalkidis et al., 2020), since these havebeen trained upon us/eu legal texts, hence, theydo not work well in the indian setting as the legalsystems are entirely different.
hierarchical transformer models: taking inspi-ration from hierarchical topic prediction model(chitkara et al., 2019), we developed hierarchicaltransformer model architecture (chalkidis et al.,2019).
we divided each document into chunks us-ing a moving window approach where each chunkwas of length 512 tokens, and there was an overlapof 100 tokens.
we obtained the [cls] represen-tation of these chunks, which were then used asinput to sequential models (bigru + attention) orfeed-forward model (cnn (kim, 2014)).
we alsotried an ensemble of individual transformer modelson each of the chunks..in general, all the hierarchical models outper-form transformer models.
the best performingmodel (78% f1) for predicting the case decision isxlnet with bigru on the top (figure 2).
com-paring best model accuracy with average annotatoraccuracy (78% vs. 94%) indicates the task’s inher-.
4053xlnetxlnetxlnet|   |   |   |   |   |   ||   |   |   |   |   |   ||   |   |   |   |   |   ||  |  |  ||  |  |  ||  |  |  |dense.
.
.
.
.
.
.. .
.
.
.
.
.. .
.
.
.
.
.chunk input ids[cls] embeddingbigrubigrubigrubigrubigrubigru.
.
.
.
.
.
.. .
.
.
.
.
.concatbigruhalfxlnethalf+metric.
explainability model vs expertsexpert3.
4.
2.
5.
1.
0.318.
0.324.
0.328.
0.333.
0.317.jaccardsimilarityoverlap-minoverlap-maxrouge-1rouge-2rouge-lbleumeteor.
0.6170.5890.4010.4140.5010.5170.2940.2950.4070.4070.2480.280.2790.3table 5: machine explanations v/s expert explanations.
0.7440.390.4440.3030.4390.160.22.
0.810.360.4010.2960.4230.0990.18.
0.8340.350.3910.2970.4440.0930.177.with logits of the label corresponding to originalhierarchical model.
the difference between thelogits normalized by the length of the sentence isthe explanation score of the sentence.
finally, top-ksentences (∼ 40%) in each chunk are selected..to understand and analyze which parts of thedocuments were contributing towards prediction,we examined the attention weights (scores) in thecase of the xlnet+bigru+attention model andthe occlusion scores of the xlnet+bigru model.
plots for some of the documents are shown in fig-ure 3. plots for different chunk sizes are providedin data/images folder in our github repository.
wealso provide the t-sne visualization on the test setusing the bert and doc2vec embeddings.
tokenvisualization heatmap using integrated gradientfor document name 1951 33.txt for bert model isalso provided in github.
plots of scores averagedout over the entire test set for each chunk size canbe visualized in appendix b.2.
two things can benoted: ﬁrstly, the largest attention and occlusionscores are assigned to chunks corresponding to theend of the document; this is in line with our hy-pothesis that most of the important information andrationale for judgment is mainly towards the end ofthe document.
secondly, although attention scoresare optimized (via loss minimization or accuracymaximization) to concentrate on the last chunks,this is not the case with occlusion scores.
there isno optimization of occlusion scores; yet they stillfocus on the chunks at the end, which afﬁrms ourhypothesis..5.3 model explainability versus annotators.
we compare the performance of occlusion methodexplanations with the expert annotators’ gold expla-nations by measuring the overlap between the two.
we used the same measures (§ 4.2) rouge-l,rouge-1, rouge-2, jaccard similarity, bleu,meteor, overlap maximum, and overlap mini-mum table 5 compares machine explanations with.
figure 3: averaged chunk scores for attention and oc-clusion.
the gold explanations.
the highest overlap value(0.8337) is observed for the measure overlap-minwith expert 4. the values for overlap-min depicthigh agreements of the explainability model withall the experts.
however, the values for the otherevaluation measures, e.g., rouge-l, are in thelow to medium range, the highest being 0.4445for rouge-l and expert 4. the results show thewide gap between how a machine would explain ajudgment and the way a legal expert would explainit.
the results motivate us for future research inthis direction of developing an explainable model.
6 conclusionthis paper introduces the ildc corpus and cor-responding cjpe task.
the corpus is annotatedwith case decisions and explanations for the deci-sions for a separate test set.
analysis of the corpusand modeling results shows the complexity of legaldocuments that pose challenges from a computa-tional perspective.
we hope that the corpus and thetask would provide a challenging and interestingresource for the legal nlp researchers.
for futurework, we would like to train a legal transformersimilar to legal-bert (chalkidis et al., 2020)on our indian legal case documents.
moreover, wewould also like to focus upon using rhetorical rolesbhattacharya et al.
(2019b) of the sentences to in-clude structural information of the documents forcjpe task as well..acknowledgementswe would like to thank anonymous reviewers fortheir insightful comments.
we would like to thankstudent research assistants abin thomas alex, am-rita ghosh, parmeet singh, and unnati jhunjhun-wala from west bengal national university of ju-ridical sciences (wbnujs) for annotating the doc-uments.
this work would not have been possiblewithout their help..4054ethical concerns.
the corpus is created from publicly available data:proceedings of supreme court of india (sci).
the data was scraped from the website: www.
indiankanoon.org.
the website allows scrappingof the data and no copyrights were infringed.
anno-tators were selected randomly and they participatedvoluntarily..the proposed corpus aims to promote the devel-opment of an explainable case judgment predictionsystem.
the system intends to assist legal profes-sionals in their research and decision-making andnot replace them.
therefore, ethical considerationssuch as allowing legal rights and obligations of hu-man beings to be decided and pronounced uponby non-human intelligence are not being breachedby the system.
the system proposes to providevaluable information that might be useful to a le-gal professional to make strategic decisions, butthe actual decision-making process is still going tobe carried out by the professional himself.
there-fore, the system is not intended to produce a hostof artiﬁcial lawyers and judges regulating humanbehavior.
at the same time, the ﬁnal expert hu-man analysis of the systemic output should ensurethat any existing ﬂaw, absurdity, or overt or latentbias gets subjected to an additional layer of ethicalscrutiny.
in this way, the usual ethical concernsassociated with the concept of case-law predictionalso get addressed to a considerable extent since thesystem is not performing any judicial role hereinnor deciding the legal rights or liabilities of humanbeings.
instead, the system is purported to be usedprimarily by legal professionals to make strategicdecisions of their own, said decisions being stillsubjected to legal and judicial scrutiny performedby human experts.
nevertheless, the communityneeds to pursue more research in this regard to fullyunderstand the unforeseen social implications ofsuch system.
this paper takes initial steps by in-troducing the corpus and baseline models to thecommunity..care has been taken to select cases in a com-pletely random manner, without any particular fo-cus on the type of law or the identities or socio-politico-economic background of the parties or thejudges involved.
speciﬁcally, the aforementionedidentities have been deliberately anonymized so asto minimize or eliminate any possible bias in thecourse of prediction.
the subjectivity that is associ-ated with the judicial decision-making may also be.
controlled in this way, since the system is focusingon how consideration of the facts and applicablelaw are supposed to determine the outcome of thecases, instead of any individual bias on the judge’spart; another judge might not share such bias, andtherefore the only common point of reference thatthe two judges would have would be the relevantfacts of the case and the laws involved.
this alsogets reﬂected in the objective methodology usedin the selection of annotators and by eliminatingany interaction between the annotators themselveswhile at the same time paying attention to the fac-tors or observations common to the output from thevarious annotators..the only speciﬁcation with regard to the forumhas been made by taking all the cases from the do-main of the supreme court of india, owing to thepropensity of the apex court of the land towards fo-cusing on the legalities of the issues involved ratherthan rendering mere fact-speciﬁc judgments, aswell as the binding nature of such decisions on thesubordinate courts of the land.
this would also al-low the results to be further generalized and appliedto a broader set of cases ﬁled before other forums,too, since the subordinate courts are supposed tofollow the reasoning of the supreme court’s judg-ments to the greatest possible extent.
as a result,the impact of the training and testing opportunitiesprovided to the system by a few supreme courtcases is likely to be much greater than the mereabsolute numbers would otherwise suggest..references.
nikolaos aletras, dimitrios tsarapatsanis, danielpreotiuc-pietro, and vasileios lampos.
2016. pre-dicting judicial decisions of the european court ofhuman rights:a natural language processing per-spective.
peerj computer science, 2:93..sebastian bach, alexander binder, gr´egoire mon-tavon, frederick klauschen, klaus-robert m¨uller,and wojciech samek.
2015. on pixel-wise explana-tions for non-linear classiﬁer decisions by layer-wiserelevance propagation.
plos one, 10(7):e0130140..iz beltagy, matthew e peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv preprint arxiv:2004.05150..paheli bhattacharya, kaustubh hiware, subham raj-garia, nilay pochhi, kripabandhu ghosh, and sap-tarshi ghosh.
2019a.
a comparative study of sum-marization algorithms applied to legal case judg-ments.
in european conference on information re-trieval, pages 413–428.
springer..4055paheli bhattacharya, shounak paul, kripabandhughosh, saptarshi ghosh, and adam wyner.
2019b.
identiﬁcation of rhetorical roles of sentences inin legal knowledge andindian legal judgments.
information systems - jurix 2019, volume 322 offrontiers in artiﬁcial intelligence and applications,pages 3–12.
ios press..richard eckart de castilho, eva mujdricza-maydt,seid muhie yimam, silvana hartmann,irynagurevych, anette frank, and chris biemann.
2016.a web-based tool for the integrated annotation of se-mantic and syntactic structures.
in proceedings ofthe workshop on language technology resourcesand tools for digital humanities (lt4dh), pages76–84..ilias chalkidis, ion androutsopoulos, and nikolaosaletras.
2019. neural legal judgment prediction inin proceedings of the 57th annual meet-english.
ing of the association for computational linguis-tics, pages 4317–4323, florence, italy.
associationfor computational linguistics..ilias chalkidis, manos fergadiotis, prodromos malaka-siotis, nikolaos aletras, and ion androutsopoulos.
2020. legal-bert: the muppets straight out oflaw school.
in findings of the association for com-putational linguistics: emnlp 2020, pages 2898–2904, online.
association for computational lin-guistics..huajie chen, deng cai, wei dai, zehui dai, andyadong ding.
2019. charge-based prison term pre-diction with deep gating network.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 6362–6367, hongkong, china.
association for computational lin-guistics..pooja chitkara, ashutosh modi, pravalika avvaru,sepehr janghorbani, and mubbasir kapadia.
2019.topic spotting using hierarchical networks withself attention.
in proceedings of the 2019 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3755–3761, minneapolis, minnesota.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..j. l. fleiss.
1971. measuring nominal scale agreementamong many raters.
psychological bulletin, 75(5)..filippo galgani, paul compton, and achim hoff-mann.
2012. towards automatic generation of catch-phrases for legal case reports.
in international con-ference on intelligent text processing and computa-tional linguistics, pages 414–425.
springer..zikun hu, xiang li, cunchao tu, zhiyuan liu, andmaosong sun.
2018. few-shot charge predictionin proceed-with discriminative legal attributes.
ings of the 27th international conference on compu-tational linguistics, pages 487–498, santa fe, newmexico, usa.
association for computational lin-guistics..peter jackson, khalid al-kofahi, alex tyrrell, andinformation extraction fromarun vachher.
2003.case law and retrieval of prior cases.
artiﬁcial intel-ligence, 150(1-2):239–290..sarthak jain and byron c. wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556, minneapolis, minnesota.
association for computational linguistics..xin jiang, hai ye, zhunchen luo, wenhan chao,and wenjia ma.
2018. interpretable rationale aug-mented charge prediction system.
in proceedingsof the 27th international conference on computa-tional linguistics: system demonstrations, pages146–151, santa fe, new mexico.
association forcomputational linguistics..justice markandey katju.
2019. backlog of caseshttps://tinyurl.com/.
crippling judiciary.
v4xu6mvk..daniel martin katz, michael j. bommarito, ii, andjosh blackman.
2017. a general approach for pre-dicting the behavior of the supreme court of theunited states.
plos one, 12:1–18..yoon kim.
2014. convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational lin-guistics..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
ininternational conference on learning representa-tions..narine kokhlikyan, vivek miglani, miguel mar-tin, edward wang, jonathan reynolds, alexan-der melnikov, natalia lunova, and orion reblitz-richardson.
2019. pytorch captum.
https://github.com/pytorch/captum..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231..4056quoc le and tomas mikolov.
2014. distributed repre-sentations of sentences and documents.
in interna-tional conference on machine learning, pages 1188–1196..jiwei li, will monroe, and dan jurafsky.
2016. un-derstanding neural networks through representationerasure.
arxiv preprint arxiv:1612.08220..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..shangbang long, cunchao tu, zhiyuan liu, andmaosong sun.
2019. automatic judgment predic-tion via legal reading comprehension.
in china na-tional conference on chinese computational lin-guistics, pages 558–572.
springer..edward loper and steven bird.
2002. nltk: the natu-ral language toolkit.
arxiv preprint cs/0205028..bingfeng luo, yansong feng, jianbo xu, xiang zhang,and dongyan zhao.
2017.learning to predictincharges for criminal cases with legal basis.
proceedings of the 2017 conference on empiricalmethods in natural language processing, pages2727–2736, copenhagen, denmark.
association forcomputational linguistics..arpan mandal, kripabandhu ghosh, arindam pal, andsaptarshi ghosh.
2017. automatic catchphrase iden-tiﬁcation from legal court case documents.
in pro-ceedings of the 2017 acm on conference on infor-mation and knowledge management, pages 2187–2190..matteo pagliardini, prakhar gupta, and martin jaggi.
2018. unsupervised learning of sentence embed-dings using compositional n-gram features.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long papers), pages 528–540, new orleans,louisiana.
association for computational linguis-tics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..f. pedregosa, g. varoquaux, a. gramfort, v. michel,b. thirion, o. grisel, m. blondel, p. prettenhofer,r. weiss, v. dubourg, j. vanderplas, a. passos,d. cournapeau, m. brucher, m. perrot, and e. duch-esnay.
2011. scikit-learn: machine learning injournal of machine learning research,python.
12:2825–2830..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for wordin proceedings of the 2014 confer-representation.
ence on empirical methods in natural language pro-cessing (emnlp), pages 1532–1543..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
”why should i trust you?” explain-in proceed-ing the predictions of any classiﬁer.
ings of the 22nd acm sigkdd international con-ference on knowledge discovery and data mining,pages 1135–1144..marco t´ulio ribeiro, sameer singh, and carlosguestrin.
2018. anchors: high-precision model-agnostic explanations.
in proceedings of the thirty-second aaai conference on artiﬁcial intelligence,(aaai-18), pages 1527–1535.
aaai press..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..avanti shrikumar, peyton greenside, and anshul kun-daje.
2017. learning important features throughpropagating activation differences.
in internationalconference on machine learning, pages 3145–3153.
pmlr..benjamin strickson and beatriz de la iglesia.
2020. le-icissgal judgement prediction for uk courts.
2020: the 3rd international conference on infor-mation science and system, cambridge, uk, march19-22, 2020, pages 204–209..octavia-maria s¸ ulea, marcos zampieri, mihaela vela,predicting theand josef van genabith.
2017.law area and decisions of french supreme courtcases.
in proceedings of the international confer-ence recent advances in natural language process-ing, ranlp 2017, pages 716–722, varna, bulgaria.
incoma ltd..mukund sundararajan, ankur taly, and qiqi yan.
2017.in inter-axiomatic attribution for deep networks.
national conference on machine learning, pages3319–3328.
pmlr..vu tran, minh le nguyen, and ken satoh.
2019.building legal case retrieval systems with lexicalmatching and summarization using a pre-trainedphrase scoring model.
in proceedings of the seven-teenth international conference on artiﬁcial intel-ligence and law, icail ’19, page 275–282, newyork, ny, usa.
association for computing machin-ery..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..4057fact descriptions.
in proceedings of the 2018 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages1854–1864, new orleans, louisiana.
associationfor computational linguistics..matthew d zeiler and rob fergus.
2014. visualizingand understanding convolutional networks.
in euro-pean conference on computer vision, pages 818–833.
springer..haoxi zhong, zhipeng guo, cunchao tu, chaojunxiao, zhiyuan liu, and maosong sun.
2018. le-gal judgment prediction via topological learning.
the 2018 conference on em-in proceedings ofpirical methods in natural language processing,pages 3540–3549, brussels, belgium.
associationfor computational linguistics..haoxi zhong, yuzhong wang, cunchao tu, tianyangzhang, zhiyuan liu, and maosong sun.
2020.iteratively questioning and answering for inter-pretable legal judgment prediction.
proceedingsof the aaai conference on artiﬁcial intelligence,34(01):1250–1257..pengfei wang, yu fan, shuzi niu, ze yang, yongfengzhang, and jiafeng guo.
2019. hierarchical match-in proceed-ing network for crime classiﬁcation.
ings of the 42nd international acm sigir confer-ence on research and development in informationretrieval, sigir 2019, paris, france, july 21-25,2019, pages 325–334..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..chaojun xiao, haoxi zhong, zhipeng guo, cunchaotu, zhiyuan liu, maosong sun, yansong feng,xianpei han, zhen hu, heng wang, et al.
2018.cail2018: a large-scale legal dataset for judgmentprediction.
arxiv preprint arxiv:1807.02478..ning xie, gabrielle ras, marcel van gerven, andderek doran.
2020. explainable deep learning:arxiv preprinta ﬁeld guide for the uninitiated.
arxiv:2004.14545..nuo xu, pinghui wang, long chen, li pan, xiaoyanwang, and junzhou zhao.
2020. distinguish con-fusing law articles for legal judgment prediction.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages3086–3095, online.
association for computationallinguistics..wenmian yang, weijia jia, xiaojie zhou, and yu-tao luo.
2019a.
legal judgment prediction viain pro-multi-perspective bi-feedback network.
ceedings of the twenty-eighth international jointconference on artiﬁcial intelligence, ijcai 2019,macao, china, august 10-16, 2019, pages 4085–4091..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019b.
xlnet: generalized autoregressive pretrain-in advances ining for language understanding.
neural information processing systems, pages 5753–5763..zichao yang, diyi yang, chris dyer, xiaodong he,alex smola, and eduard hovy.
2016. hierarchi-cal attention networks for document classiﬁcation.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1480–1489..hai ye, xin jiang, zhunchen luo, and wenhan chao.
2018.interpretable charge predictions for crimi-nal cases: learning to generate court views from.
4058appendix.
a annotations and case studies:.
agreement in judgment prediction forannotators.
annotation assignment 1954 13:in this case,although the original decision is that the appealhas been rejected, experts 1-4 have reached thedecision that it has been accepted, while expert5 has decided that it has been rejected.
thisdiscrepancy appears to owe its origin to the verynature of the case and the issues considered bythe court.
there had been more than one suchissue and separate arguments had been made byappellant in favour of each of such issue andassociated prayer.
the court appears to haveagreed to some of the arguments and disagreedwith the rest.
annotation assignment 1961 417: in this case,although the original decision is that the appealhas been rejected, experts 2 and 4 have decidedthat it has been accepted.
expert 2 appears tohave misconstrued certain positions of law andrelied unduly upon one of the other cases beingcited as precedent (but not considered relevant bythe supreme court), which might account for thedivergence.
in case of expert 4, however, the issueappears to be more of a linguistic matter.
expert4 has referred to a particular statement made bythe court, “the main question that arises in thisappeal is whether an illegitimate son of a sudravis-a-vis his self acquired property, after havingsucceeded to a half share of his putative fathersestate, will be entitled to succeed to the otherhalf share got by the widow, after the successionopened out to his putative father on the death ofthe said widow.” from this sentence, expert 4 hasdrawn the inference that the appellant was the oneasking to establish such entitlement.
since thecourt in subsequent comments agreed that suchentitlement does exist, expert 4 inferred that theappeal had been accepted.
however, in reality, theappellant had been contesting such entitlement.
annotation assignment 1962 47:in this case,although the original decision is that the appealhas been rejected, experts 2 and 5 have decidedthat it has been accepted.
this discrepancy appearsto owe its origin to both of them having beenmisled by sentence 17 of the case, which appearsto refer to the supreme court having acceptedan appeal and merely giving reasons for such.
order in the present case.
however, the case inpoint was actually arising from an application forreview of the court’s earlier judgment (acceptanceof the appeal), and therefore, when the court wasafﬁrming its earlier judgment and giving reasonsbehind it, it was in reality rejecting this presentapplication for review, that had been made by theparty (respondent in the original appeal) aggrievedby the acceptance of such appeal by the courtearlier.
experts 2 and 5 could not apparentlydistinguish the appeal from the review petition andthat appears to have led to such discrepancy..b models details.
table 6 summarizes hyperparameter settings forall the models.
all the experiments were run ongoogle colab7 and used the default single gputesla p100-pcie-16gb, provided by colab..b.1 case prediction model details.
classical models: we considered classical mlmodels like logistic regression, svm, and ran-dom forest.
we used sentence embeddings viasent2vec (pagliardini et al., 2018) and documentembeddings via doc2vec (le and mikolov, 2014)as input features.
both embeddings were trainedon ildcmulti as our data is domain-speciﬁc.
le-gal proceedings are typically long documents, wetried out extractive summarization methods (as de-scribed in bhattacharya et al.
(2019a)) for gleaningrelevant information from the documents and pass-ing these as input to neural models.
however, thisapproach also resulted in classiﬁers that were nobetter than random classiﬁer..we also experimented by using tf-idf vectorswith the classical models like logistic regression(lr), random forests (rf) and support vectormachines (svm) from the scikit-learn library inpython (pedregosa et al., 2011).
however, the re-sults were no better than a random classiﬁer, which,according to us, could be due to the huge length ofthe documents and they were not able to capturesuch long term dependencies well enough..results: classical models based on logisticregression and sent2vec embeddings performedmuch worse than the one based on doc2vec em-beddings.
it is interesting to see that doc2vec+lrhas performance competitive to sequential mod-els.
the simple word embedding based model has7https://colab.research.google.com/.
4059similar performance as the more complicated hi-erarchical attention network model (han).
thebest results are recorded in the table 4, each forsent2vec and doc2vec.
sequential models: we experimented with stan-dard bigru (2 layers) with attention model.
wetried 3 different types of embeddings: (i) wordlevel trained glove embeddings (pennington et al.,2014), with last 512 tokens as input, (ii) sentencelevel embeddings (sent2vec), where last 150 sen-tences were input8, and (iii) chunk level embed-dings (trained via doc2vec).
both sequential mod-els and han were trained on both ildcmulti andildcsingle.
all the models from here on weretrained on colab9..we extracted catchphrases (mandal et al., 2017)from the ildcsingle (we could not use this methodon ildcmulti due to requirement of huge computeresources).
after extracting these catchphraseswe ranked the sentences from the documentsaccordingly and used upto 200 sentences only10.
these top 200 sentences were then mapped to theirsent2vec embeddings and passed through bigruas above..results:.
trained onsequential modelsildcmulti and ildcsingle have similar perfor-mances.
we also experimented with extractingkey sentences from ildcsingle documents with thehelp of catchphrases and using these sentences asinput (via the sent2vec embeddings) to a sequencemodel.
extracting the key sentences performsbetter than the using all the sentences but theperformance is worse (61% versus 64% f1) thanusing glove embeddings on last 512 words.
gloveembeddings with bigru and attention modelgave the best performance (64% f1) among thesequential models.
the glove embeddings (last512 tokens) with bigru + attention gave the bestresults among the models mentioned above.
transformer models: recently, sota languagemodels have been developed using transformer ar-chitectures (vaswani et al., 2017).
a number oftransformer architectures have been introduced re-cently.
we experimented with bert (devlin et al.,2019), distilbert (sanh et al., 2019), roberta(liu et al., 2019), and xlnet (yang et al., 2019b).
we used huggingface library (wolf et al., 2020)to ﬁne tune base models of above transformers8last 150 sentences covered around 90% of the documents9https://colab.research.google.com/10these covered more than 90% of the ildcsingle..from huggingface (wolf et al., 2020) on the last11. due to high compute re-512 tokens of ildcmultiquirements we could not utilize longformer (belt-agy et al., 2020) and reformer (kitaev et al., 2020)models developed especially for long documents.
for the other transformer models we used only.
the last 512 tokens as input..results: among the combinations of input to-kens, the best performance was obtained by usinglast 512 tokens as input to the bert base model.
we can observe the trend that the more the tokensfrom the ﬁnal parts of the document are taken asinput, the better is the prediction performance.
thisobservation agrees with the fact that there are moreclues towards the correct prediction in the ﬁnalparts of the document (since arguments, ratio ofthe decision etc.
bhattacharya et al.
(2019b) mostaligned to the judgment are expected to appearmore towards the end, closer to the judgment).
asfor the comparison between different transformers,unsurprisingly, roberta and xlnet perform bet-ter than bert in the prediction sub-task.
similarly,among distilbert and bert, the latter outper-forms the other.
hierarchical models: in order to use transform-ers hierarchically, it was ﬁrst necessary to ﬁne-tunethese models on the downstream task of classiﬁca-tion.
we use two different strategies to ﬁne-tunethese:.
• on ildcmulti: using last 512 tokens only.
from the documents..• on ildcsingle: we ﬁne-tune the transformerby dividing each document into chunks of 512with an overlap of 100 tokens, the label foreach chunk is given as the whole documentlabel..then we extracted the 768 dimension, [cls] tokenembeddings from the transformers for each chunkin all the documents.
this was done on ildcmulticorpus irrespective of whether it was ﬁne-tuned onildcmulti or ildcsingle.
as mentioned in (devlinet al., 2019) we also experimented with concatenat-ing the last 4 hidden layers of the [cls] token andtaking that as the chunk embedding.
after getting the chunk embeddings we used twotypes of neural networks: bigru and cnn..for some models, the results varied over mul-tiple runs.
for these we recorded their mean andvariance on f1 and accuracy in the table 4..11as shown in table 4, we also experimented with differentsections of documents and we observed last 512 tokens gavethe best performance.
4060results: information is lost in considering onlythe last portion of the case proceeding for predic-tion and this is reﬂected in the performance of hi-erarchical models.
in general, all the hierarchicalmodels outperform transformer models.
adding at-tention on top of bigru in the hierarchical modeldoes not boost the performance signiﬁcantly.
how-ever, adding a cnn (instead of bigru + attention)on top gives a competitive performance.
as for thecomparison between the strategies of ﬁne-tuningbetween ildcmulti and ildcsingle, the later seemedto perform worse on prediction.
for the hierarchi-cal concatenated model ﬁne tuned on ildcsingle,there was a slight boost in performance..b.2 explanability models and results details.
to extract explanations from our best model (xl-net + bigru), we propose a method inspired fromli et al.
(2016) and zeiler and fergus (2014).
the.
idea is to use occlusion method at both levels of thehierarchy.
for the bigru part of the model, foreach document we mask each complete chunk em-bedding one at a time.
the masked input is passedthrough the trained bigru and output probability(masked probability) of the label obtained by orig-inal unmasked model is calculated.
the maskedprobability is compared with unmasked probabilityto calculate chunk explainability score.
formally,for a chunk c, if the sigmoid outputs (of the bigru)are σm (when the chunk was not masked) and σm(cid:48)(when the chunk was masked) and the predictedlabel is y then the probabilities and chunk scoreσm(cid:48)/m,y = 11 − σm(cid:48)/m, y = 0we obtain sentences that explain the decisionfrom the transformer part of the model (xlnet)using the chunks that were assigned positive scores.
each chunk (length 512 tokens) is segmented into.
sc = pm − pm(cid:48) and pm(cid:48)/m =.
(cid:40).
figure 4: visualization of occlusion scores accross full test set..figure 5: visualization of attention scores accross full test set..4061sentences using nltk sentence splitter (loper andbird, 2002).
similar to bigru, each sentence ismasked and the output of the transformer at theclassiﬁcation head (softmax logits) is comparedwith logits of the label corresponding to originalhierarchical model.
the difference between thelogits normalized by the length of the sentence isthe explanation score of the sentence.
finally, top-ksentences (∼ 40%) in each chunk are selected..in figure 4 and figure 5 we visualize the meanchunk importance scores.
out of the 1517 test doc-uments we average out chunk scores of the docu-ments having same number of chunks.
as shown infigure 5, the attention weights are biased towardsthe last chunks, thus giving negligible attention tothe chunks before.
however, in figure 4, in someof the graphs, the last chunk is given the second-highest score and in 7 out of 10 graphs, it has thehighest score.
due to space limitation, we are notproviding the graphs for occlusion and attentionscores for chunks 1 to 15. but we observed that forthese chunks pattern matches for occlusion scoreswith attention scores.
from these observations, webelieve it is safe to say that both the methods ofvisualization afﬁrm our hypothesis that the mostrelevant syntactic and semantic information liestowards the end of the case.
although attentionscores are optimized (via loss minimization or ac-curacy maximization) to concentrate on last chunks,this is not the case with occlusion scores.
there isno optimization of occlusion scores, yet they stillfocus on the chunks at the end which afﬁrms ourhypothesis.
one might argue that this observationmight be due to the transformer being trained onlast 512 tokens only.
to check this, we also vi-sualized the hierarchical transformers trained onildcsingle, but the results were similar as to whatwe have observed in this case..model.
hyper-parameters (e = epochs),(dim = embedding dimension),(l = layers), (att.
= attention),(default setting= 512 tokens withoverlapping 100 tokens).
classical models on ildcmulti train setdoc2vec + lrsent2vec + lrsequential models on ildcmulti train setsent2vec + bigru + att.
doc2vec + bigru + att.
glove + bigru + att..dim = 1000 , e = 20dim=500, e = 20, avg pool.
dim = 200, e = 1, l = 2dim = 1000, e = 2, l = 2dim = 180, e = 3, l = 2word dim = 100, sent dim = 100,e = 10.sequential models on ildcsingle train setsent2vec + bigru+ att.
doc2vec + bigru + att.
glove + bigru + att..dim = 200, e = 1, l = 2dim = 1000, e = 2, l = 2dim = 180, e = 10, l = 2word dim = 100, sent dim = 100,e = 10.catchphrases + sent2vec+ bigru + att..dim =180, e =5, l = 2.han.
han.
default setting, e = 5, l = 3default setting, e = 2, l = 3, runs = 3default setting, e = 5, l = 2default setting, e = 3, l = 3 (conv1d)default setting, e = 3, l = 3 (conv1d)default setting, e = 3, l = 3 (conv1d).
512 begin tokens, e = 3256 begin, 256 end tokens, e = 3256 mid, 256 end tokens, e = 3128 begin, 256 mid, 128 end, e = 3512 end tokens, e = 3512 end tokens, e = 5512 end tokens, e = 5512 end tokens, e = 3.transformer models on ildcmulti train setbert basebert basebert basebert basebert basedistillbertrobertaxlnethierarchical models on ildcmulti train setbert + bigruroberta + bigruxlnet + bigrubert + cnnroberta + cnnxlnet + cnnhierarchical models on ildcsingle train setbert + bigruroberta + bigruxlnet + bigruhierarchical models with attention on ildcmulti train setbert + bigru + att.
roberta + bigru + att.
xlnet + bigru + att.
hierarchical models with attention on ildcsingle train setbert + bigru + att.
roberta + bigru + att.
xlnet + bigru + att.
transformers voting ensemblerobertaﬁne tuned on last 512 tokens, votingﬁne tuned on last 512 tokens, votingxlnethierarchical concatenated model with att on ildcsingle train.
default setting, e = 1, l = 2, 3 runsdefault setting, e = 1, l = 3, 3 runsdefault setting, e = 1, l = 2, 3 runs.
default setting, e = 2, l = 2, 3 runsdefault setting, e = 2, l = 3, 3 runsdefault setting, e = 3, l = 2, 3 runs.
default setting, e = 1, l = 2, 3 runsdefault setting, e = 1, l = 2, 3 runsdefault setting, e = 2, l = 2, 3 runs.
xlnet + bigru.
last 4 layers concat, e = 1,l = 2, 3 runs.
table 6: hyper-parameters corresponding to everymodel..4062