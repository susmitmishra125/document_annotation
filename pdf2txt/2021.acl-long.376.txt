knowledge-enriched event causality identiﬁcation via latent structureinduction networks.
pengfei cao1,2, xinyu zuo1,2, yubo chen1,2, kang liu1,2, jun zhao1,2,yuguang chen3 and weihua peng31national laboratory of pattern recognition, institute of automation, cas, beijing, china2school of artiﬁcial intelligence, university of chinese academy of sciences, beijing, china3beijing baidu netcom science technology co., ltd{pengfei.cao, xinyu.zuo, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn,{chenyuguang, pengweihua}@baidu.com.
abstract.
identifying causal relations of events is an im-portant task in natural language processingarea.
however, the task is very challenging,because event causality is usually expressed indiverse forms that often lack explicit causalclues.
existing methods cannot handle wellthe problem, especially in the condition oflacking training data.
nonetheless, humanscan make a correct judgement based on theirbackground knowledge, including descriptiveknowledge and relational knowledge.
inspiredby it, we propose a novel latent structureinduction network (lsin) to incorporate theexternal structural knowledge into this task.
speciﬁcally, to make use of the descriptiveknowledge, we devise a descriptive graphinduction module to obtain and encode thegraph-structured descriptive knowledge.
toleverage the relational knowledge, we proposea relational graph induction module which isable to automatically learn a reasoning struc-ture for event causality reasoning.
experi-mental results on two widely used datasets in-dicate that our approach signiﬁcantly outper-forms previous state-of-the-art methods..1.introduction.
event causality identiﬁcation (eci) aims to iden-tify causal relation of events in texts.
for exam-ple, in the sentence “the earthquake generated atsunami.”, an eci model should be able to identifya causal relationship that holds between the twomentioned events, i.e., earthquake cause−−−→ tsunami.
eci is an important task in natural language pro-cessing (nlp) area and can support many nlp ap-plications, such as machine reading comprehension(berant et al., 2014), process extraction (thalap-pillil scaria et al., 2013) and future event prediction(radinsky et al., 2012; hashimoto et al., 2014)..figure 1: an example of leveraging the external struc-tural knowledge for eci task.
the dashed arrow indi-cates a missing link in the knowledge base..challenging, because event causality is usually ex-pressed in diverse forms that often lack explicitclues indicating its existence.
for example in fig-ure 1, the sentence has no explicit clue indicat-ing the causal relation between “global warming”and “tsunami”.
in this scenario, models can re-sort to a large amount of labeled data to learn di-verse causal expressions.
however, existing ecidatasets are very small.
for example, the largestdataset eventstoryline (caselli and vossen, 2017)only contains 258 documents, which is not sufﬁ-cient to train neural network models (liu et al.,2020).
consequently, models cannot thoroughlyunderstand the text and possibly make a wrong pre-diction.
nonetheless, humans could make a correctjudgement, because humans have the backgroundknowledge about the two events.
to be more spe-ciﬁc, humans not only know what the two eventsare, but also know the connection between them.
fortunately, existing knowledge bases (kbs) usu-ally contain the descriptive knowledge of eventsand relational knowledge between events, whichcan be regarded as the background knowledge to en-hance eci models.
in this paper, we focus on howto incorporate these two kinds of external knowl-edge into the task..identifying event causal relation is inherently.
descriptive knowledge: the external knowl-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4862–4872august1–6,2021.©2021associationforcomputationallinguistics4862global warming worsened, and tsunamistrengthened.sea-level risingatlocationcapableofleveraging external structural knowledgeglobal warmingisaheatingglacier meltingtemperature changeisagreenhouse gascreatedbycausescapableofisadeathcausestsunamioceandestroy housetidal waveatlocationedge base contains the descriptive or explanatoryinformation about events, which can be called thedescriptive knowledge of events.
it usually consistsof one-hop neighbors of events.
this kind of knowl-edge is able to help the model better understandwhat the mentioned event is.
for example in fig-ure 1, the descriptive knowledge associated with“global warming” includes (global warming, isa,temperature change), (global warming, createdby,greenhouse gas) and so on.
if the model can makeuse of such knowledge, it is obvious that the modelcan better understand the meaning of the event it-self than using only the given text.
therefore, incor-porating the descriptive knowledge is very helpfulfor this task.
however, when leveraging this kindof knowledge, we ﬁnd two critical challenges: (1)as shown in figure 1, the descriptive knowledgeforms a sub-graph.
how to effectively encode thegraph-structured knowledge is a very challengingproblem; (2) the knowledge base is incomplete(wang et al., 2020), which will inevitably cause thedescriptive knowledge of some events cannot beobtained from the kb.
thus, the model should havethe ability to obtain and encode such knowledge,even if it does not exist in the kb..relational knowledge: the external knowl-edge base contains connections between events,which can be referred as the relational knowl-edge between events.
it is usually deﬁned by themulti-hop path between two events.
this kind ofknowledge can provide useful information for eventcausality reasoning, especially when the text lackscausal clues.
for example in figure 1, the relationalknowledge between the two events is “global warm-.
causes−−−−→ “glacier melting”atlocation−−−−−−→ “ocean”.
capableof−−−−−−→ “sea-leveling”atlocationrising”←−−−−−− “tsunami”.
apparently, compared with only using text informa-tion, utilizing the relational knowledge can provideample evidence for the model to judge the causalitybetween “global warming” and “tsunami”.
how-ever, two challenges exist when using the relationalknowledge: (1) the multi-hop path may miss somepotentially useful relations.
for example in figure1, the fact (sea-level rising, causes, tsunami) is de-scribed in the wikipedia page of “sea-level rising”1,while it is not annotated in the kb; (2) not all theknowledge on the path is related to causality, suchas (sea-level rising, atlocation, ocean).
therefore,directly reasoning along the multi-hop path struc-.
1https://en.wikipedia.org/wiki/sea_.
level_rise.
ture may not be optimal.
the model should beable to learn a more reasonable structure for cap-turing potentially useful information and reducingthe impact of irrelevant knowledge..in this paper, we propose a novel method termedas latent structure induction network (lsin) toovercome aforementioned challenges.
speciﬁcally,we devise a descriptive graph induction module tomake use of the descriptive knowledge.
the mod-ule ﬁrst adopts a hybrid method of retrieval andgeneration to obtain the descriptive knowledge, andthen utilizes the information aggregation techniqueto encode the graph-structured knowledge.
mean-while, we propose a relational graph inductionmodule to leverage the relational knowledge.
themodule ﬁrst treats the reasoning structure as a la-tent variable and learns it in an end-to-end fashion.
then, the module performs event causality reason-ing based on the induced structure.
experimentalresults on two widely used datasets demonstratethat our model substantially outperforms previousstate-of-the-art methods..our contributions are summarized as follows:.
• we propose a novel latent structure inductionnetwork (lsin) to leverage the external struc-tural knowledge.
to our knowledge, we arethe ﬁrst to use both the descriptive knowledgeand relational knowledge for this task..• to exploit the descriptive knowledge, we de-vise a descriptive graph induction module.
toutilize the relational knowledge, we propose arelational graph induction module..• experimental results on two widely useddatasets indicate that our proposed approachsigniﬁcantly outperforms previous state-of-the-art methods..2 related work.
event causality identiﬁcation (eci) is a very impor-tant task in natural language processing area, whichhas attracted extensive attention in the past fewyears.
early studies for the task are feature-basedmethods which utilize lexical and syntactic features(riaz and girju, 2013; gao et al., 2019), explicitcausal patterns (beamer and girju, 2009; do et al.,2011; hu et al., 2017), and statistical causal associa-tions (riaz and girju, 2014; hashimoto et al., 2014;hu and walker, 2017; hashimoto, 2019) for thetask.
with the development of deep learning, neural.
4863figure 2: the architecture of our proposed latent structure induction network for event causality identiﬁcation..network-based methods have been proposed for thetask and achieved the state-of-the-art performance(kruengkrai et al., 2017; kadowaki et al., 2019;liu et al., 2020; zuo et al., 2020).
liu et al.
(2020)propose a mention masking generalization methodand also consider the external structural knowledge.
the very recent work (zuo et al., 2020) proposea data augmentation method to alleviate the datalacking problem for the task.
regarding datasetsconstruction, mirza (2014) annotates the causal-timebank dataset about event causal relations inthe tempeval-3 corpus.
caselli and vossen (2017)construct a dataset called eventstoryline for eventcausality identiﬁcation.
despite many efforts forthis task, most existing methods typically train themodels on manually labeled data solely, rarely con-sidering the external structural knowledge.
as aresult, these methods cannot handle well the caseswhere there is no explicit causal clue..although liu et al.
(2020) leverage the descrip-tive knowledge to enrich event representations, theydirectly retrieve the descriptive knowledge fromthe kb.
therefore, their method cannot handlethe cases where there is no knowledge about theevent in the kb.
in addition, they ignore the re-lational knowledge between events.
by contrast,our method can not only generate the descriptiveknowledge when it cannot be retrieved from thekb, but also leverage the relational knowledge.
toour knowledge, we are the ﬁrst to simultaneouslymake use of the descriptive knowledge and rela-tional knowledge for this task..3 methodology.
siﬁcation problem.
for every pair of events ina sentence, we predict whether a causal relationholds.
figure 2 schematically visualizes our ap-proach, which consists of three major components:(1) context encoding (§3.1), which encodes theinput sentence and outputs contextualized repre-sentations; (2) descriptive graph induction (§3.2),which ﬁrst obtains the corresponding descriptiveknowledge for each event, and then encodes thegraph-structured knowledge; (3) relational graphinduction (§3.3), which automatically induces areasoning structure and performs causality reason-ing on the induced structure.
we will illustrate eachcomponent in detail..3.1 context encoding.
given a sentence with a pair of events (denoted ase1 and e2), the context encoding module aims toextract context features, which takes the sentenceas input and outputs the context representations.
our context encoder is based on the transformerarchitecture (vaswani et al., 2017).
we adopt thebert (devlin et al., 2019) to encode the inputsentence,2 which has achieved the state-of-the-artperformance for eci task (liu et al., 2020; zuoet al., 2020).
after using bert encoder to com-pute the contextual representations of the entiresentence, we concatenate representations of [cls],e1 and e2 as the context representation regardingto the event pair (e1, e2), namely.
f (e1,e2)c.= h[cls] ⊕ he1 ⊕ he2,.
(1).
following previous works (ning et al., 2018; liuet al., 2020), we formulate eci as a binary clas-.
2note that the encoder is not our focus in this paper.
infact, other models like convolutional neural networks and longshort-term memory networks can also be as encoders..4864global warming worsened, and tsunamistrengthened.conceptnetglobal warmingisaheatingglacier meltingtemperature changeisagreenhouse gascreatedbycausescapableoftsunamidestroy housetidal waveoceanatlocationisadeathcausesknowledgeobtainingknowledgeencodingdescriptive graphglobal warmingglacier meltingcausessea-level risingcapableofoceanatlocationtsunamiatlocationrelational pathstructureinductioniterativerefinementrelational graphdeeptransformerglobal warmingtsunamicausescontextrepresentationdescriptive knowledgerepresentationrelational knowledgerepresentationcontextencodingdescriptivegraph inductionrelationalgraphinductionwhere ⊕ indicates the concatenation operation.
h[cls] ∈ rd, he1 ∈ rd and he2 ∈ rd are rep-resentations of [cls], e1 and e2, respectively.
d isthe output hidden size of bert model..3.2 descriptive graph induction.
3.2.1 knowledge obtaininggiven e1 and e2, we adopt a hybrid method of re-trieval and generation to obtain their descriptiveknowledge, respectively.
the descriptive knowl-edge forms a sub-graph which is called descriptivegraph (denoted as gd).
for this paper, we preferconceptnet (speer et al., 2017) as the externalkb, which contains abundant semantic knowledgeof concepts.
we take e1 as an example to illustratethe knowledge obtaining procedure:.
(1) if the descriptive knowledge can be retrievedfrom the kb, we adopt the retrieval method.
ourmethod ﬁrst grounds e1 to a concept via match-ing the event mention with the tokens of conceptsin conceptnet.
we enhance the matching ap-proach with some rules, such as soft matching withlemmatization and ﬁltering of stop words.
thegrounded concept is called zero-hop concept.
then,our method grows zero-hop concept with one-hopconcepts.
the zero-hop concept, one-hop conceptsand all relations between them form the descriptivegraph for e1 (denoted as gd1)..(2) if the descriptive knowledge cannot be re-trieved from the kb, we adopt the generationmethod.
our method employs the pre-trainedmodel, comet (bosselut et al., 2019), which isoriginally proposed for the knowledge base com-pletion.
speciﬁcally, comet is obtained by ﬁne-tuning gpt (radford et al., 2018) on concept-net.
the input of comet is the head event andcandidate relation, and the output is the tail event.
the relation types are the same as the ones used inbosselut et al.
(2019).
by leveraging comet, wecan generate the descriptive graph gd1 for e1..in the same way, we can also construct the de-.
scriptive graph gd2 for e2..3.2.2 knowledge encodinggraph neural networks have been widely used to en-code graph-structured data (lin et al., 2019; yanget al., 2019), as they are able to effectively col-lect relevant evidence based on an information ag-gregation scheme.
in addition, many works showthat relational graph convolutional networks (r-gcns) (schlichtkrull et al., 2018) usually over-parameterize the model and cannot effectively uti-.
lize multi-hop relational information (zhang et al.,2018; lin et al., 2019).
we thus apply gcns (kipfand welling, 2017) to encode the related descrip-tive knowledge of e1 and e2..formally, given a descriptive graph gd (i.e., gd1or gd2) with nd nodes (i.e., concepts), which canbe represented with an nd × nd adjacency matrixad.
if there is a connection between node i andnode j, the adij is set to 1. for the node i at the l-thlayer, the convolution computation can be deﬁnedas follows:.
u(l).
i = ρ(.
ad.
ijw (l).
u u(l−1).
j.
+ b(l).
u ),.
(2).
nd(cid:88).
j=1.
u and b(l).
where w (l)u are the weight matrix and biasvector for the l-th layer, respectively.
ρ is an activa-tion function (e.g., relu).
u(0)i ∈ rd is the initialrepresentation of the i-th node obtained by the pre-trained model (i.e., bert).
to consider contextinformation when encoding descriptive knowledge,we use the he1 and he2 obtained in section 3.1 asthe initial representations of events..after the knowledge encoding, the representa-tions of e1 and e2 in descriptive graphs are denotedas ue1 and ue2, respectively.
we concatenate themas the descriptive knowledge representation:.
f (e1,e2)d.= ue1 ⊕ ue2..(3).
3.3 relational graph induction.
3.3.1 multi-hop path obtaininggiven e1 and e2, our model ﬁrst retrieves the multi-hop path between the two events from concept-net.
we refer to the multi-hop path as relationalpath.
since shorter connections between two con-cepts could mean stronger relevance (lin et al.,2019), our model exploits the shortest path betweenthe two events as the relational path.
we representthe conceptnet as a graph, and then use net-workx toolkit3 to get the shortest path betweenthe two events.
when there are multiple shortestpaths, we randomly select one path for avoidinginformation redundancy..3.3.2 structure induction.
to capture potentially useful information and re-duce the impact of irrelevant knowledge on the re-lational path, our model treats the reasoning struc-ture as a latent variable and induces it with the.
3https://networkx.org.
4865input of the relational path, which can be shownin figure 2. we call the induced reasoning struc-ture as relational graph (denoted as gr).
thestructure induction module is built based on thestructured attention (kim et al., 2017).
we use avariant of kirchhoff’s matrix-tree theorem (kooet al., 2007; nan et al., 2020) to learn the graphstructure..formally, the nodes of relational graph are theconcepts on the relational path.
the initializedrepresentation of each node is obtained via the pre-trained model (i.e., bert).
the representation ofthe i-th node is denoted as mi ∈ rd.
we ﬁrst cal-culate the pair-wise unnormalized attention scoresij between the i-th node and the j-th node:.
sij = (tanh(wpmi))t wb(tanh(wcmj)), (4).
where wp and wc are weights matrixes.
wb arethe weights for the bilinear transformation.
next,we compute the root score sri which represents theunnormalized probability of the i-th node to beselected as the root node of the structure:.
sri = wrmi,.
(5).
where wr ∈ r1×d is the weight for linear transfor-mation.
suppose the graph gr has nr nodes, weﬁrst assign non-negative weights p ∈ rnr×nr tothe edges of the induced relational graph:.
(cid:40).
pij =.
0,exp(sij),.
if i = jotherwise,.
(6).
where pij is the weight of the edge between thei-th and the j-th node.
then, following kooet al.
(2007), we deﬁne the laplacian matrixl ∈ rnr×nr of gr, and its variant ˆl ∈ rnr×nr ,respectively:.
lij =.
(cid:40) (cid:80)nr.
k=1 pkj,.
−pij,.
if i = jotherwise,.
(cid:40).
ˆlij =.
i ),.
exp(srlij,.
if i = 1otherwise..we use arij to denote the marginal probability ofthe edge between the i-th node and the j-th node,which can be computed as follows:.
ar.
ij = (1 − δ1,j)pij[ ˆl−1]ij− (1 − δi,1)pij[ ˆl−1]ji,.
(7).
(8).
(9).
where δ is the kronecker delta (koo et al., 2007)and ·−1 denotes matrix inversion.
ar can be re-garded as a weighted adjacency matrix of the graphgr.
finally, ar is fed into the iterative reﬁnementfor event causality reasoning..iterative reﬁnement.
3.3.3after obtaining the relational graph structure, weperform event causality reasoning on the inducedstructure.
to better capture potential reasoningclues, we adopt the densely connected graph con-volutional networks (dcgcns) (guo et al., 2019),which allows training a deeper reasoning model.
the convolution computation of each layer is:.
v(l)i = ρ(.
ar.
ijw (l).
v g(l).
j + b(l)v ),.
(10).
nr(cid:88).
j=1.
j.where g(l)is the concatenation of the initial nodejrepresentation and the node representations pro-duced in layers 1, .
.
.
, l − 1, namely g(l)j = mj ⊕j ⊕ · · · ⊕ v(l−1)v(1).
the induced structure at once is relatively shal-low (liu et al., 2019; nan et al., 2020) and maynot be optimal for causality reasoning.
therefore,we iteratively reﬁne the induced structure to learna more informative structure.
we stack n blocks(each block is structure induction and dcgcnsreasoning) of this module to induce the structuren times.
intuitively, as the structure gets morereﬁned, the structure is more reasonable..after the iterative reﬁnement, the representa-tions of e1 and e2 are denoted as ve1 and ve2, re-spectively.
we concatenate them as the relationalknowledge representation:.
f (e1,e2)r.= ve1 ⊕ ve2..(11).
3.4 model prediction and training.
we concatenate the context representation, descrip-tive knowledge representation and relational knowl-edge representation as the ﬁnal representation:.
fe1,e2 = f (e1,e2).
c.⊕ f (e1,e2)d.⊕ f (e1,e2)r...(12).
to make the ﬁnal prediction, we perform a binaryclassiﬁcation by taking fe1,e2 as input:.
pe1,e2 = softmax(wsfe1,e2 + bs)..(13).
for training, we adopt cross entropy as the lossfunction:.
j(θ) = −.
(cid:88).
(cid:88).
yei,ej log(pei,ej ),.
(14).
s∈d.
ei,ej ∈esei(cid:54)=ej.
4866where θ denotes the model parameters.
s denotesa sentence in the training set d. es is the set ofevents in sentence s. yei,ej is a one-hot vectorrepresenting the gold label between ei and ej..methods.
bert.
4 experiments.
4.1 datasets and evaluation metrics.
we evaluate our proposed method on two widelyused datasets, including eventstoryline (caselliand vossen, 2017) and causal-timebank (mirzaet al., 2014).
for eventstoryline, the dataset con-tains 258 documents, 5,334 events in total, and1,770 of 7,805 event pairs are causally related.
forcausal-timebank, the dataset contains 184 docu-ments, 6,813 events, and 318 of 7,608 event pairsare causally related.
we conduct the 5-fold and 10-fold cross-validation on the eventstoryline datasetand causal-timebank dataset respectively, sameas previous methods to ensure fairness.
followingprevious works (choubey and huang, 2017; gaoet al., 2019), we adopt precision (p), recall (r) andf1-score (f1) as evaluation metrics..4.2 parameter settings.
in our implementations, our method uses the hug-gingface’s transformers library4 to implement theuncased bert base model, which has 12-layers,768-hidden, and 12-heads.
the learning rate is ini-tialized as 2e-5 with a linear decay.
we use theadam algorithm (kingma and ba, 2015) to opti-mize model parameters.
the batch size is set to 20.the number of induction blocks (i.e., n ) is set to2. the dropout of gcn is set to 0.3. due to thesparseness of positive examples, we adopt a neg-ative sampling strategy for training.
the negativesampling rate is 0.6 and 0.7 for the eventstorylineand causal-timebank, respectively.
we utilizeconceptnet 5.0 as the external knowledge base..4.3 baselines.
we compare the proposed approach lsin with pre-vious state-of-the-art methods:.
feature-based methods: (1) mirza and tonelli(2014), which proposes a data driven method withcausal signals for the task; (2) mirza (2014), whichemploys a verb rule based model with data ﬁlter-ing and causal signals enhancement; (3) choubeyand huang (2017), which proposes a sequencemodel exploring complex handcrafted features for.
4https://github.com/huggingface/.
transformers.
cheng and miyao (2017)choubey and huang (2017)gao et al.
(2019)knowdis (zuo et al., 2020)kmmg (liu et al., 2020).
lsin (ours).
p(%) r(%) f1(%).
36.9.
34.032.737.439.741.9.
47.9.
56.0.
41.544.955.866.562.5.
58.1.
44.5.
37.437.844.749.750.152.5∗.
table 1: experimental results on the eventstorylinedataset.
bold denotes best results.
* denotes a signiﬁ-cance test with p=0.05..methods.
bert.
mirza and tonelli (2014)mirza (2014)kmmg (liu et al., 2020)knowdis (zuo et al., 2020).
lsin (ours).
p(%) r(%) f1(%).
38.8.
67.369.036.642.3.
51.5.
44.1.
22.631.555.660.5.
56.2.
41.3.
33.943.244.149.852.9∗.
table 2: experimental results on the causal-timebankdataset.
bold denotes best results.
* denotes a signiﬁ-cance test with p=0.05..the task; (4) gao et al.
(2019), which utilizes alogistic regression classiﬁer with the integer linearprogramming to model causal structure for the task.
neural network-based methods: (1) chengand miyao (2017), which proposes a dependencypath based bidirectional long short-term memorynetwork (bilstm) that models the context be-tween two event mentions for causal relation iden-tiﬁcation; (2) kmmg (liu et al., 2020), which pro-poses a mention masking generalization methodand also utilizes the external knowledge;(3)knowdis (zuo et al., 2020), which proposes aknowledge enhanced distant data augmentationmethod to alleviate data lacking problem..4.4 overall results.
since some baselines are evaluated either on theeventstoryline dataset or the causal-timebankdataset, the baselines used for the two datasets aredifferent.
table 1 and table 2 show the results onthe eventstoryline and causal-timebank, respec-tively.
from the tables, we can observe that:.
(1) our method outperforms all the baselinesby a large margin on the two datasets.
for ex-ample, compared with the state-of-the-art modelknowdis (zuo et al., 2020), our method lsin.
4867methods.
p(%) r(%) f1(%).
methods.
p(%) r(%) f1(%).
36.9bertbert+dk41.8bert+rk46.1bert+dk+rk 47.9.
56.051.955.458.1.
44.546.350.352.5.liu et al.
(2020)dgi-retrievaldgi-generationdgi-hybrid.
44.540.039.341.8.
39.346.151.351.9.
41.842.844.546.3.table 3: experimental results by using different kindsof knowledge on the eventstoryline dataset.
“dk”and “rk” refer to “descriptive knowledge” and “rela-tional knowledge”, respectively..table 4: comparison between the different methods forusing the descriptive knowledge on the eventstorylinedataset.
“dgi” refer to “descriptive graph induction”..achieves 2.8% and 3.1% improvements of f1-scoreon the eventstoryline and causal-timebank, re-spectively.
it indicates that our proposed method isvery effective for this task..(2) compared with the state-of-the-art modelkmmg (liu et al., 2020), our method achieves6.0% improvements in terms of precision score onthe eventstoryline.
the reason may be that ourmethod utilizes the relational knowledge betweenevents for causality reasoning, which can improvethe conﬁdence of event causality prediction..(3) our method improves upon the bert modelby 8.0% and 11.6% in terms of f1-score on thetwo datasets, respectively.
this suggests that onlyusing the annotated training data is not enough totackle the task.
moreover, it also indicates that ourmethod is able to effectively leverage the externalstructural knowledge for eci task..(4) the bert model achieves comparable per-formance with complex feature-based methodssuch as gao et al.
(2019) on the eventstorylinedataset, which indicates that the bert is able toextract useful text features for the task..4.5 effectiveness of external structural.
knowledge.
we validate the effectiveness of external structuralknowledge for this task.
based on the bert model,we leverage the descriptive knowledge via descrip-tive graph induction module, and the relationalknowledge via relational graph induction module.
the results are shown in table 3. we have twoimportant observations:.
(1) based on the bert model, incorporatingthese two kinds of knowledge can both improve per-formance.
moreover, simultaneously using thesetwo kinds of knowledge can further improve theperformance.
it indicates that the external struc-tural knowledge is very effective for this task..(2) the performance improvement of using the.
relational knowledge is more obvious than that ofusing the descriptive knowledge, achieving 4.0%improvements in terms of f1-score.
we guess thatthe relational knowledge can provide more cluesfor event causality reasoning..4.6 effectiveness of descriptive graph.
induction.
to verify the effectiveness of descriptive graphinduction module, we compare our method withthe state-of-the-art model (liu et al., 2020).
liuet al.
(2020) ﬁrst retrieve the descriptive knowl-edge, and then transfer the knowledge into a se-quence.
finally, they adopt the bert to encodethe knowledge.
the results are listed in table 4.in the table, “dgi-retrieval”, “dgi-generation”and “dgi-hybrid” denote obtaining the descrip-tive knowledge via retrieval, generation and hybridmethod, respectively.
overall, we can observe that:(1) the dgi-hybrid model signiﬁcantly outper-forms liu et al.
(2020), achieving 4.5% improve-ments of f1-score.
moreover, even if we use thesame retrieval method as liu et al.
(2020), ourmodel still achieves better result.
it indicates thedescriptive graph induction module can better takeadvantage of the descriptive knowledge..(2) compared with liu et al.
(2020), the dgi-hybrid model achieves great improvements interms of recall score (i.e., improving 12.6%).
thereason is that our method can automatically gener-ate the descriptive knowledge, when the knowledgecannot be retrieved from the kb..4.7 effectiveness of relational graph.
induction.
to validate the effectiveness of the relational graphinduction module, we compare our method withother three baselines.
the three baselines are illus-trated as follows:.
(1) lstm-based reasoning, which regards therelational path as a sequence and employs lstm.
4868methods.
p(%) r(%) f1(%).
examples.
lstm-basedfixed graph-basedattention-based.
lsin (ours).
43.043.146.3.
47.9.
54.556.555.0.
58.1.
48.148.950.3.
52.5.table 5: comparison between the different methods forleveraging the relational knowledge on the eventstory-line dataset..figure 3: f1-score for different number of reﬁnements(i.e., n ) on the eventstoryline dataset and causal-timebank dataset, respectively.
the number of reﬁne-ments is ranging from 1 to 5..to encode it; (2) fixed graph-based reasoning,which regards the relational path as a graph.
itsnodes are concepts on the path and edges only existbetween adjacent concepts; (3) attention-basedreasoning, which uses the self-attention to encodethe relational path for modeling the dependenciesbetween arbitrary two concepts..the results are shown in table 5. from the re-.
sults, we can observe that:.
(1) our method lsin outperforms the threemethods by a large margin.
for example, com-pared with lstm-based reasoning method, ourmethod achieves 4.4% improvements of f1-score.
this empirically conﬁrms using induced relationalgraph structure is more effective than directly usingthe relational path for causality reasoning..(2) compared with fixed graph-based reasoningmethod, our method achieves 3.6% improvementsof f1-score.
it indicates that our method is able toeffectively capture the potentially useful informa-tion and reduce the impact of irrelevant knowledgeon the relational path..a) indonesia earthquake: over 200injured in aceh province .
.
..b) the ﬁghts erupted in flatbush, and46 were arrested at wednesday .
.
..bert lsin.
(cid:55).
(cid:55).
(cid:51).
(cid:51).
table 6: results of case study where bold denotes thetwo event pair.
(cid:51) and (cid:55) denote a correct and incorrectprediction, respectively..4.8.impact of the number of reﬁnements.
we investigate the effect of the reﬁnement on theoverall performance.
we plot the overall f1-scorevarying with the number of reﬁnements in figure3. from the ﬁgure, we can observe that:.
(1) our method lsin yields the best perfor-mance in the second reﬁnement.
compared withthe ﬁrst induction, the second reﬁnement achieves1.1% improvements of f1-score on the eventsto-ryline dataset.
this indicates that the proposedlsin is able to induce more reasonable reasoningstructures by iterative reﬁnement..(2) when the number of reﬁnements is too large,the performance on the two datasets stops increas-ing or even decreases due to over-ﬁtting..4.9 case study.
we conduct case study to further verify the effec-tiveness of our method.
table 6 shows severalcases showing the outputs of bert and our methodlsin.
from the results, we can observe that thebert model cannot handle the cases where there isno causal clue.
by contrast, our method can makecorrect predictions by leveraging the external struc-tural knowledge.
for the second example in table6, although the text has no clue indicating the exis-tence of causality between “ﬁghts” and “arrested”,there is the relational knowledge between the twohassubeventevents in the kb, namely “ﬁght”−−−−−−−−→ “hurthassubeventsomeone else”−−−−−−−−→ “get arrested”.
ourmethod can make use of the relational knowledge tomake a correct prediction.
the two examples qual-itatively demonstrate our method can effectivelyleverage the external knowledge for eci task..5 conclusion.
in this paper, we propose a novel latent structureinduction network (lsin) to leverage the externalstructural knowledge for eci task.
to make useof the descriptive knowledge, we devise a descrip-.
486912345number of refinements48495051525354f1-score (%)51.452.551.650.349.552.052.952.151.249.9eventstorylinecausal-timebanktive graph induction module to obtain and encodethe graph-structured descriptive knowledge.
toutilize the relational knowledge, we propose a re-lational graph induction module to induce a morereasonable reasoning structure for causality rea-soning.
experimental results on two widely useddatasets indicate that our approach substantiallyoutperforms previous state-of-the-art methods..acknowledgments.
we thank anonymous reviewers for their insight-ful comments and suggestions.
this work is sup-ported by the national key research and develop-ment program of china (no.
2020aaa0106400),and the national natural science foundation ofchina (no.
61806201).
this work is also sup-ported by beijing academy of artiﬁcial intelli-gence (baai2019qn0301) and the fund of thejoint project with beijing baidu netcom sciencetechnology co., ltd..references.
brandon beamer and roxana girju.
2009. using a bi-gram event model to predict causal potential.
in in-ternational conference on intelligent text process-ing and computational linguistics, pages 430–441..jonathan berant, vivek srikumar, pei-chun chen,abby vander linden, brittany harding, brad huang,peter clark, and christopher d. manning.
2014.modeling biological processes for reading compre-hension.
in proceedings of the 2014 conference onempirical methods in natural language processing,pages 1499–1510.
association for computationallinguistics..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, asli celikyilmaz, and yejin choi.
2019. comet: commonsense transformers forin pro-automatic knowledge graph construction.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4762–4779. association for computational linguistics..tommaso caselli and piek vossen.
2017. the eventstoryline corpus: a new benchmark for causal andtemporal relation extraction.
in proceedings of theevents and stories in the news workshop, pages 77–86. association for computational linguistics..fei cheng and yusuke miyao.
2017. classifying tem-poral relations by bidirectional lstm over depen-in proceedings of the 55th annualdency paths.
meeting of the association for computational lin-guistics, pages 1–6.
association for computationallinguistics..prafulla kumar choubey and ruihong huang.
2017. asequential model for classifying temporal relationsin proceedings ofbetween intra-sentence events.
the 2017 conference on empirical methods in nat-ural language processing, pages 1796–1802.
asso-ciation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 4171–4186.
association for compu-tational linguistics..quang do, yee seng chan, and dan roth.
2011. min-imally supervised event causality identiﬁcation.
inproceedings of the 2011 conference on empiricalmethods in natural language processing, pages294–303.
association for computational linguis-tics..lei gao, prafulla kumar choubey, and ruihonghuang.
2019. modeling document-level causalstructures for event causal relation identiﬁcation.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1808–1817.
association for computationallinguistics..zhijiang guo, yan zhang, zhiyang teng, and weilu.
2019. densely connected graph convolutionalnetworks for graph-to-sequence learning.
transac-tions of the association for computational linguis-tics, 7:297–312..chikara hashimoto.
2019. weakly supervised multi-lingual causality extraction from wikipedia.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing, pages 2988–2999.
association for com-putational linguistics..chikara hashimoto, kentaro torisawa, julien kloetzer,motoki sano, istv´an varga, jong-hoon oh, and yu-taka kidawara.
2014. toward future scenario gener-ation: extracting event causality exploiting semanticin pro-relation, context, and association features.
ceedings of the 52nd annual meeting of the associa-tion for computational linguistics, pages 987–997.
association for computational linguistics..zhichao hu, elahe rahimtoroghi, and marilyn walker.
2017. inference of ﬁne-grained event causality fromblogs and ﬁlms.
in proceedings of the events andstories in the news workshop, pages 52–58.
associ-ation for computational linguistics..zhichao hu and marilyn walker.
2017. inferring nar-rative causality between event pairs in ﬁlms.
in pro-ceedings of the 18th annual sigdial meeting on dis-course and dialogue, pages 342–351.
associationfor computational linguistics..4870kazuma kadowaki, ryu iida, kentaro torisawa, jong-hoon oh, and julien kloetzer.
2019. event causal-ity recognition exploiting multiple annotators’ judg-ments and background knowledge.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing, pages 5816–5822.
association for compu-tational linguistics..yoon kim, carl denton, luong hoang, and alexan-der m. rush.
2017. structured attention networks.
in 5th international conference on learning rep-resentations, 2017, conference track proceedings.
openreview.net..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,2015, conference track proceedings..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in 5th international conference on learn-ing representations, 2017, conference track pro-ceedings.
openreview.net..terry koo, amir globerson, xavier carreras, andmichael collins.
2007. structured prediction mod-els via the matrix-tree theorem.
in proceedings ofthe 2007 joint conference on empirical methodsin natural language processing and computationalnatural language learning, pages 141–150.
asso-ciation for computational linguistics..canasai kruengkrai, kentaro torisawa, chikarahashimoto, julien kloetzer, jong-hoon oh, andmasahiro tanaka.
2017.improving event causal-ity recognition with multiple background knowl-edge sources using multi-column convolutional neu-in proceedings of the thirty-firstral networks.
aaai conference on artiﬁcial intelligence, pages3466–3473.
aaai press..bill yuchen lin, xinyue chen, jamin chen, and xi-ang ren.
2019. kagnet: knowledge-aware graphnetworks for commonsense reasoning.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing, pages 2829–2839.
association for compu-tational linguistics..1745–1755.
association for computational linguis-tics..paramita mirza.
2014. extracting temporal and causalrelations between events.
in proceedings of the acl2014 student research workshop, pages 10–17.
as-sociation for computational linguistics..paramita mirza, rachele sprugnoli, sara tonelli, andmanuela speranza.
2014. annotating causalityin proceedings ofin the tempeval-3 corpus.
the eacl 2014 workshop on computational ap-proaches to causality in language (catocl), pages10–19.
association for computational linguistics..paramita mirza and sara tonelli.
2014. an analy-sis of causality between events and its relation toin proceedings of colingtemporal information.
2014, the 25th international conference on compu-tational linguistics: technical papers, pages 2097–2106. dublin city university and association forcomputational linguistics..guoshun nan, zhijiang guo, ivan sekulic, and wei lu.
2020. reasoning with latent structure reﬁnement fordocument-level relation extraction.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1546–1557.
asso-ciation for computational linguistics..qiang ning, zhili feng, hao wu, and dan roth.
2018.joint reasoning for temporal and causal relations.
inproceedings of the 56th annual meeting of the asso-ciation for computational linguistics, pages 2278–2288. association for computational linguistics..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..kira radinsky,.
sagie davidovich,.
and shaulmarkovitch.
2012.learning causality for newsevents prediction.
in proceedings of the 21st worldwide web conference, 2012, pages 909–918.
acm..mehwish riaz and roxana girju.
2013. toward a bet-ter understanding of causality between verbal events:extraction and analysis of the causal power of verb-in proceedings of the sigdialverb associations.
meeting on discourse and dialogue 2013 confer-ence, pages 21–30.
association for computationallinguistics..jian liu, yubo chen, and jun zhao.
2020. knowl-edge enhanced event causality identiﬁcation within proceedingsmention masking generalizations.
of the twenty-ninth international joint conferenceon artiﬁcial intelligence, 2020, pages 3608–3614.
ij-cai.org..mehwish riaz and roxana girju.
2014..in-depth ex-ploitation of noun and verb semantics to identify cau-sation in verb-noun pairs.
in proceedings of the 15thannual meeting of the special interest group on dis-course and dialogue, pages 161–170.
associationfor computational linguistics..yang liu, ivan titov, and mirella lapata.
2019. singledocument summarization as tree induction.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages.
michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in european semantic web confer-ence, pages 593–607..4871robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-eral knowledge.
in proceedings of the thirty-firstaaai conference on artiﬁcial intelligence, pages4444–4451.
aaai press..aju thalappillil scaria, jonathan berant, mengqiuwang, peter clark, justin lewis, brittany harding,and christopher d. manning.
2013. learning bio-in pro-logical processes with global constraints.
ceedings of the 2013 conference on empirical meth-ods in natural language processing, pages 1710–1720. association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, pages 5998–6008..peifeng wang, nanyun peng, filip ilievski, pedroszekely, and xiang ren.
2020. connecting the dots:a knowledgeable path generator for commonsensequestion answering.
in findings of the associationfor computational linguistics: emnlp 2020, pages4129–4140.
association for computational linguis-tics..hsiu-wei yang, yanyan zou, peng shi, wei lu, jimmylin, and xu sun.
2019. aligning cross-lingual enti-ties with multi-aspect information.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing, pages 4431–4441.
association for compu-tational linguistics..yuhao zhang, peng qi, and christopher d. manning.
2018. graph convolution over pruned dependencytrees improves relation extraction.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 2205–2215.
asso-ciation for computational linguistics..xinyu zuo, yubo chen, kang liu, and jun zhao.
2020.knowdis: knowledge enhanced data augmentationfor event causality detection via distant supervision.
in proceedings of the 28th international conferenceon computational linguistics, pages 1544–1550.
in-ternational committee on computational linguis-tics..4872