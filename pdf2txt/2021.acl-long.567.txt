neural machine translation with monolingual translation memory∗.
deng cai♥, yan wang♠, huayang li♠, wai lam♥, and lemao liu♠.
♥the chinese university of hong kongthisisjcykcd@gmail.comwlam@se.cuhk.edu.hk♠tencent ai lab{brandenwang,alanili,redmondliu}@tencent.com.
abstract.
prior work has proved that translation mem-ory (tm) can boost the performance of neuralmachine translation (nmt).
in contrast to ex-isting work that uses bilingual corpus as tmand employs source-side similarity search formemory retrieval, we propose a new frame-work that uses monolingual memory and per-forms learnable memory retrieval in a cross-lingual manner.
our framework has unique ad-vantages.
first, the cross-lingual memory re-triever allows abundant monolingual data to betm.
second, the memory retriever and nmtmodel can be jointly optimized for the ulti-mate translation goal.
experiments show thatthe proposed method obtains substantial im-provements.
remarkably, it even outperformsstrong tm-augmented nmt baselines usingbilingual tm.
owning to the ability to lever-age monolingual data, our model also demon-strates effectiveness in low-resource and do-main adaptation scenarios..1.introduction.
augmenting parametric neural network modelswith non-parametric memory (khandelwal et al.,2019; guu et al., 2020; lewis et al., 2020a,b) hasrecently emerged as a promising direction to re-lieve the demand for ever-larger model size (de-vlin et al., 2019; radford et al., 2019; brown et al.,2020).
for the task of machine translation (mt),inspired by the computer-aided translation (cat)tools by professional human translators for increas-ing productivity for decades (yamada, 2011), theusefulness of translation memory (tm) has longbeen recognized (huang et al., 2021).
in general,tm is a database that stores pairs of source textand its corresponding translations.
like for human.
∗the work described in this paper is partially supportedby a grant from the research grant council of the hongkong special administrative region, china (project code:14200719).
correspondence to yan wang..translation, early work (koehn and senellart, 2010;he et al., 2010; utiyama et al., 2011; wang et al.,2013, inter alia) presents translations for similarsource input to statistical translation models as ad-ditional cues..recent work has conﬁrmed that tm can helpneural machine translation (nmt) models as well.
in a similar spirit to prior work, tm-augmentednmt models do not discard the training corpusafter training but keep exploiting it in the test time.
these models perform translation in two stages: inthe retrieval stage, a retriever searches for nearestneighbors (i.e., source-target pairs) from the train-ing corpus based on source-side similarity suchas lexical overlaps (gu et al., 2018; zhang et al.,2018; xia et al., 2019), embedding-based matches(cao and xiong, 2018), or a hybrid (bulte andtezcan, 2019; xu et al., 2020); in the generationstage, the retrieved translations are injected into astandard nmt model by attending over them withsophisticated memory networks (gu et al., 2018;cao and xiong, 2018; xia et al., 2019; he et al.,2021) or directly concatenating them to the sourceinput (bulte and tezcan, 2019; xu et al., 2020),or biasing the word distribution during decoding(zhang et al., 2018).
most recently, khandelwalet al.
(2020) propose a token-level nearest neighborsearch using complete translation context, i.e., boththe source-side input and target-side preﬁx..despite their differences, we identify two majorlimitations in previous research.
first, the transla-tion memory has to be a bilingual corpus consistingof aligned source-target pairs.
this requirementlimits the memory bank to bilingual pairs and pre-cludes the use of abundant monolingual data, whichcan be especially helpful for low-resource scenar-ios.
second, the memory retriever is non-learnable,not end-to-end optimized, and lacks for the abilityto adapt to speciﬁc downstream nmt models.
con-cretely, current retrieval mechanisms (e.g., bm25).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7307–7318august1–6,2021.©2021associationforcomputationallinguistics7307are generic similarity search, adopting a simpleheuristic.
that is, the more a source sentence over-laps with the input sentence, the more likely itstarget-side translation pieces will appear in the cor-rect translation.
although this observation is true,the most similar one does not necessarily serve thebest for nmt models.
ideally, the retrieval metricwould be learned from the data in a task-dependentway: we wish to consider a memory only if it canindeed boost the quality of ﬁnal translation..in this work, we propose to augment nmt mod-els with monolingual tm and a learnable cross-lingual memory retriever.
speciﬁcally, we alignsource-side sentences and the corresponding target-side translations in a latent vector space using asimple dual-encoder framework (bromley et al.,1993), such that the distance in the latent spaceyields a score function for retrieval.
as a result,our memory retriever directly connects the dotsbetween the source-side input and target-side trans-lations, enabling monolingual data in the targetlanguage to be used alone as tm.
before runningeach translation, the memory retriever selects thehighest-scored memories from a large collection ofmonolingual sentences (tm), which may includebut are not limited to the target side of training cor-pus, and then the downstream nmt model attendsover those memories to help inform its translation.
we design the memory retriever with differentiableneural networks.
to unify the memory retriever andits downstream nmt model into a learnable whole,the retrieval scores are used to bias the attentionscores to the most useful retrieved memories.
inthis way, our memory retrieval can be end-to-endoptimized for the translation objective: a retrievalthat improves the golden translation’s likelihood ishelpful and should be rewarded, while an uninfor-mative retrieval should be penalized..one challenge for training our proposed frame-work is that, when starting from random initializa-tion, the retrieved memories will likely be totallyunrelated to the input.
since the memory retrieverdoes not exert positive inﬂuence on nmt model’sperformance, it cannot receive a meaningful gradi-ent and improve.
this causes the nmt model tolearn to ignore all retrieved memories.
to avoid thiscold-start problem, we propose to warm-start theretrieval model using two cross-alignment tasks..experiments show that (1) our model leadsto signiﬁcant improvements over non-tm base-line nmt model, even outperforming strong tm-.
augmented baselines.
this is remarkable given thatprevious tm-augmented models rely on bilingualtm while our model only exploits the target side.
(2) our model can substantially boost translationquality in low-resource scenarios by utilizing extramonolingual tm that is not present in training pairs.
(3) our model gains a strong cross-domain trans-ferability by hot-swapping domain-speciﬁc mono-lingual memory..2 related work.
tm-augmented nmt this work contributesprimarily to the research line of translation mem-ory (tm) augmented neural machine translation(nmt).
feng et al.
(2017) augmented nmt with abilingual dictionary to tackle infrequent word trans-lation.
gu et al.
(2018) proposed a model that re-trieves examples similar to the test source sentenceand encodes retrieved source-target pairs with key-value memory networks.
cao and xiong (2018);cao et al.
(2019) used a gating mechanism to bal-ance the impact of the translation memory.
zhanget al.
(2018) proposed guiding models by retriev-ing n-grams and up-weighting the probabilities ofretrieved n-grams.
bulte and tezcan (2019) andxu et al.
(2020) used fuzzy-matching with transla-tion memories and augment source sequences withretrieved source-target pairs.
xia et al.
(2019) di-rectly ignored the source side of a tm and packedthe target side into a compact graph.
khandelwalet al.
(2020) ran existing translation model on largebi-text corpora and recorded all hidden states forlater nearest neighbor search at each decoding step,which is very compute-intensive.
the distinctionsbetween our work and prior work are obvious: (1)the tm in our framework is a collection of mono-lingual sentences rather than bilingual sentencepairs; (2) we use learnable task-speciﬁc retrievalrather than generic retrieval mechanisms..retrievalfor text generation discrete re-trieval as an intermediate step has been shownbeneﬁcial to a variety of natural language process-ing tasks.
one typical use is to retrieve support-ing evidence for open-domain question answering(e.g., chen et al., 2017; lee et al., 2019; karpukhinet al., 2020).
recently, retrieval-guided genera-tion has gained increasing interest in a wide rangeof text generation tasks such as language model-ing (guu et al., 2018; khandelwal et al., 2019;guu et al., 2020), dialogue response generation(weston et al., 2018; wu et al., 2019; cai et al.,.
7308figure 1: overall framework.
for an input sentence x in the source language, the retrieval model uses maximuminner product search (mips) to ﬁnd the top-m tm sentences {zi}mi=1 in the target language.
the translationmodel takes {zi}m.i=1 and corresponding relevance scores {f (x, zi)}m.i=1 as input and generate the translation y..2019a,b), code generation (hashimoto et al., 2018)and other knowledge-intensive generation (lewiset al., 2020b).
it can be observed that there is ashift from using off-the-shelf search engines tolearning task-speciﬁc retrievers.
our work drawsinspiration from this line of research.
however,retrieval-guided generation has so far been mainlyinvestigated for knowledge retrieval in the samelanguage.
the memory retrieval in this work ismore challenging due to the cross-lingual setting..nmt using monolingual data to our knowl-edge, the integration of monolingual data for nmtwas ﬁrst investigated by gulcehre et al.
(2015), whoseparately trained target-side language models us-ing monolingual data, and then integrated them dur-ing decoding either through re-scoring the beam, orby feeding the hidden state of the language modelto the nmt model.
jean et al.
(2015) also exploredre-ranking the nmt output with a n-gram languagemodel.
another successful method for leveragingmonolingual data in nmt is back-translation (sen-nrich et al., 2016; fadaee et al., 2017; edunov et al.,2018; he et al., 2016), where a reverse translationmodel is used to translate monolingual sentencesfrom the target language to the source language togenerate synthetic parallel sentences.
recent stud-ies (jiao et al., 2021; he et al., 2019) showed thatself-training, where the synthetic parallel sentencesare created by translating monolingual sentencesin the source language, is also helpful.
our methodis orthogonal to previous work and bears a uniquefeature: it can use more monolingual data withoutre-training (see §4.3)..3 proposed approach.
we start by formalizing the translation task as aretrieve-then-generate process in §3.1.
then in§3.2, we describe the model design for the cross-.
lingual memory retrieval model.
in §3.3, we de-scribe the model design for the memory-augmentedtranslation model.
lastly, we show how to optimizethe two components jointly using standard maxi-mum likelihood training in §3.4 and therein weaddress the cold-start problem via cross-alignmentpre-training..3.1 overview.
our approach decomposes the whole translationprocessing into two steps: retrieve, then generate.
the overall framework is illustrated in figure 1.the translation memory (tm) in our approachis a collection of sentences in the target languagez. given an input x in the source language, the re-trieval model ﬁrst selects a number of possibly help-ful sentences {zi}mi=1 from z, where m (cid:28) |z|, ac-cording to a relevance function f (x, zi).
then, thetranslation model conditions on both the retrievedset {(zi, f (x, zi)}mi=1 and the original input x togenerate the output y using a probabilistic modelp(y|x, z1, f (x, z1), .
.
.
, zm , f (x, zm )).
note thatthe relevance scores {f (x, zi)}mi=1 are also part ofthe input to the translation model, encouraging thetranslation model to focus more on more relevantsentences.
during training, maximizing the likeli-hood of the translation references improves boththe translation model and the retrieval model..3.2 retrieval model.
the retrieval model is responsible for selecting themost relevant sentences for a source sentence froma large monolingual tm.
this could involve mea-suring the relevance scores between the source sen-tence and millions of candidate target sentences,which poses a serious computational challenge.
toaddress this, we implement the retrieval model us-ing a simple dual-encoder framework (bromleyet al., 1993) such that the selection of the most.
7309source sentenceencoder 𝐸"#$(𝑥)target sentence encoder 𝐸()((𝑧)translation memory 𝒵…dense indexmipssourceencodermemoryencoder𝓏-,//0123decoderretrieval modeloutput ytranslation modelbias attentioninput x𝓏1𝓏5…𝑓(𝑥,𝓏5)𝑓(𝑥,𝓏1)…relevant tmrelevance scoresrelevant sentences can be reduced to maximum in-ner product search (mips).
with performant datastructures and search algorithms (e.g., shrivastavaand li, 2014; malkov and yashunin, 2018), theretrieval can be done efﬁciently..speciﬁcally, we deﬁne the relevance scoref (x, z) between the source sentence x and the can-didate sentence z as the dot product of their densevector representations:.
f (x, z) = esrc(x)tetgt(z).
where esrc and etgt are the source sentence encoderand the target sentence encoder that map x and z tod-dimensional vectors respectively.
we implementthe two sentence encoders using two independenttransformers (vaswani et al., 2017).
for an inputsentence, we prepend the [bos] token to its to-ken sequence and then feed it into a transformer.
we take the representation at the [bos] token asthe output (denoted trans{src,tgt}({x, z})), and per-form a linear projection (w{src,tgt}) to reduce thedimensionality of the vector.
finally, we normal-ize the vectors to regulate the range of relevancescores..esrc(x) = normalize(wsrctranssrc(x))etgt(z) = normalize(wtgttranstgt(z)).
the normalized vectors have zero means and unitlengths.
therefore, the relevance scores alwaysfall in the interval [−1, 1].
we let θ denote allparameters associated with the retrieval model..in practice, the dense representations of all sen-tences in tm can be pre-computed and indexed us-ing faiss (johnson et al., 2019), an open-sourcetoolkit for efﬁcient vector search.
given a sourcesentence x in hand, we compute the vector rep-resentation vx = esrc(x) and retrieve the top mtarget sentences with vectors closest to vx..3.3 translation model.
given a source sentence x, a small set of relevanttm {zi}mi=1, and relevance scores {f (x, zi)}mi=1,the translation model deﬁnes the conditional proba-bility p(y|x, z1, f (x, z1), .
.
.
, zm , f (x, zm ))..our translation model is built upon the standardencoder-decoder nmt model (bahdanau et al.,2015; vaswani et al., 2017): the (source) encodertransforms the source sentence x into dense vec-tor representations.
the decoder generates an out-put sequence y in an auto-regressive fashion.
ateach time step t, the decoder attends over both.
previously generated sequence y1:t−1 and the out-put of the source encoder, generating a hiddenstate ht.
the hidden state ht is then convertedto next-token probabilities through a linear pro-jection followed by softmax function, i.e., pv =softmax(wvht + bv)..to accommodate the extra memory input, weextend the standard encoder-decoder nmt frame-work with a memory encoder and allow cross-attention from the decoder to the memory encoder.
speciﬁcally, the memory encoder encodes each tmsentence zi individually, resulting in a set of con-textualized token embeddings {zi,k}lik=1, where liis the length of the token sequence zi.
we computea cross attention over all tm sentences:.
αij =.
(cid:80)m.i=1.
twmzi,j)).
exp(ht(cid:80)li.
k=1 exp(htli(cid:88)m(cid:88).
twmzi,k).
(1).
ct = wc.
αijzi,j.
i=1.
j=1.
where αij is the attention score of the j-th tokenin zi, ct is a weighted combination of memory em-beddings, and wm and wc are trainable matrices.
the cross attention is used twice during decod-ing.
first, the decoder’s hidden state ht is updatedby a weighted sum of memory embeddings, i.e.,ht = ht + ct. second, we consider each attentionscore as a probability of copying the correspondingtoken (gu et al., 2016; see et al., 2017).
formally,the next-token probabilities are computed as:.
p(yt|·) = (1 − λt)pv(yt) + λt.
αij1zij =yt.
m(cid:88).
li(cid:88).
i=1.
j=1.
where 1 is the indicator function and λt is a gatingvariable computed by another feed-forward net-work λt = g(ht, ct)..inspired by lewis et al.
(2020a), to enable thegradient ﬂow from the translation output to theretrieval model, we bias the attention scores withthe relevance scores, rewriting eq.
(1) as:twmzi,j + βf (x, zi)).
(cid:80)m.i=1.
k=1 exp(ht.
twmzi,k + βf (x, zi))(2)where β is a trainable scalar that controls the weightof the relevance scores.
we let φ denote all param-eters associated with the translation model..exp(ht(cid:80)li.
αij =.
3.4 training.
we optimize the model parameters θ andφ usingon.
stochastic.
gradient.
descent.
7310loss.
negative.
function.
thelog-likelihood− log p(y∗|x, z1, f (x, z1), .
.
.
, zm , f (x, zm )),where y∗ refers to the reference translation.
asimplied by eq.
(2), tm sentences that improve thelikelihood of reference translations should receivehigher attention scores and higher relevance scores,so gradient descent on the loss function willimprove the quality of the retrieval model as well..cross-alignment pre-training however, if theretrieval model starts from random initialization,all top tm sentences zi will likely be unrelatedto x (or equally useless).
this leads to a problemthat the retrieval model cannot receive meaningfulgradients and improve, and the translation modelwill learn to completely ignore the tm input.
toavoid this cold-start problem, we propose two cross-alignment tasks to warm-start the retrieval model.
the ﬁrst task is sentence-level cross-alignment.
this task aims to ﬁnd the right translation for asource sentence given a set of other translations,which is directly related to our retrieval function.
concretely, we sample b source-target pairs fromthe training corpus at each training step.
let xand z be the (b × d) matrix of the source and tar-get vectors encoded by esrc and etgt respectively.
s = xzt is a (b × b) matrix of relevance scores,where each row corresponds to a source sentenceand each column corresponds to a target sentence.
any (xi, zj) pair should be aligned when i = j,and should not otherwise.
the objective is to max-imize the scores along the diagonal of the matrixand henceforth reduce the values in other entries.
the loss function can be written as:.
l(i).
snt =.
− exp(sii).
exp(sii) + (cid:80).
j(cid:54)=i exp(sij).
..the second task is token-level cross-alignment,which aims to predict the tokens in the target lan-guage given the source sentence representation andvice versa.
formally, we use bag-of-words losses:.
l(i).
tok = −.
(cid:88).
wy∈yi.
log p(wy|xi) +.
log p(wx|yi).
(cid:88).
wx∈xi.
where xi (yi) represents the set of tokens in the i-thsource (target) sentence and the token probabilitiesare computed by a linear projection followed by thesoftmax function.
the joint loss for pre-trainingis 1tok.
in practice, we ﬁnd thatbboth the sentence-level and token-level objectivesare crucial for achieving superior performance..snt + l(i).
i=1 l(i).
(cid:80)b.dataseten⇔esen⇔de.
#train pairs679,088699,569.
#dev pairs2,5332,454.
#test pairs2,5962,483.table 1: data statistics for the jrc-acquis corpus..asynchronous index refresh to employ fastmips, we must pre-compute etgt(z) for everyz ∈ z and build an index.
however, the indexcannot remain consistent with the running modelduring training as θ will be updated over time.
onestraightforward solution to ﬁx the parameters ofetgt after the pre-training described above and onlyﬁne-tune the parameters of esrc.
however, this mayhurt performance since etgt cannot adapt to thetranslation objective.
another solution is to asyn-chronously refresh the index by re-computing andre-indexing all tm sentences at regular intervals.
the index is slightly outdated between refreshes,however, we use fresh etgt in gradient estimate.
weexplore both options in our experiments..4 experiments.
we experiment with the proposed approach in threesettings: (1) the conventional setting where theavailable tm is limited to the bilingual trainingcorpus, (2) the low-resource setting where bilin-gual training pairs are scarce but extra monolingualdata is exploited as additional tm, and (3) non-parametric domain adaptation using monolingualtm.
note that existing tm-augmented nmt mod-els are only applicable to the ﬁrst setting, the lasttwo settings only become possible with our pro-posed model.
we use bleu score (papineni et al.,2002) as the evaluation metric..4.1.implementation details.
we build our model using transformer blockswith the same conﬁguration as transformer base(vaswani et al., 2017) (8 attention heads, 512 di-mensional hidden state, and 2048 dimensionalfeed-forward state).
the number of transformerblocks is 3 for the retrieval model, 4 for the mem-ory encoder in the translation model, and 6 forthe encoder-decoder architecture in the translationmodel.
we retrieve the top 5 tm sentences.
thefaiss index code is “ivf1024 hnsw32,sq8”and the search depth is 64..we follow the learning rate schedule, dropoutand label smoothing settings described in vaswaniet al.
(2017).
we use adam optimizer (kingmaand ba, 2014) and train models with up to 100k.
7311#.
system.
retriever.
es⇒en.
en⇒es.
de⇒en.
en⇒de.
dev.
test.
dev.
test.
dev.
test.
dev.
test.
gu et al.
(2018)zhang et al.
(2018)xia et al.
(2019).
source similaritysource similaritysource similarity.
12345.this work.
nonesource similaritycross-lingual (ﬁxed)cross-lingual (ﬁxed etgt)†cross-lingual†.
existing nmt systems*.
63.1663.9766.37.
62.9464.3066.21our nmt systems64.0766.4866.2467.1667.42.
64.2566.9866.6867.6667.73.
-61.5062.50.
62.2763.0463.0663.7364.18.
-61.5662.76.
61.5462.7662.7363.2263.86.
-60.1061.85.
59.8263.6263.2564.3964.48.
-60.2661.72.
60.7663.8563.0664.0164.62.
-55.5457.43.
55.0157.8857.6158.1258.77.
-55.1456.88.
54.9057.5356.9757.9258.42.table 2: experimental results (bleu scores) on four translation tasks.
∗results are from xia et al.
(2019).
†thetwo variants of our method (model #4 and model #5) are signiﬁcantly better than other baselines with p-value <0.01, tested by bootstrap re-sampling (koehn, 2004)..steps throughout all experiments.
when trainedwith asynchronous index refresh, the re-indexinginterval is 3k training steps.1.
4.2 conventional experiments.
following prior work in tm-augmented nmt, weﬁrst conduct experiments in a setting where thebilingual training corpus is the only source for tm..data we use the jrc-acquis corpus (steinbergeret al., 2006) for our experiments.
the jrc-acquiscorpus contains the total body of european union(eu) law applicable to the eu member states.
this corpus was also used by gu et al.
(2018);zhang et al.
(2018); xia et al.
(2019) and wemanaged to get the datasets originally prepro-cessed by gu et al.
(2018), making it possibleto fairly compare our results with previously re-ported bleu scores.
speciﬁcally, we select fourtranslation directions, namely, spanish⇒english(es⇒en), en⇒es, german⇒english (de⇒en),and en⇒de, for evaluation.
detailed data statisticsare shown in table 1..models to study the effect of each model com-ponent, we implement a series of model variants(model #1 to #5 in table 2)..1. nmt without tm.
to measure the help fromtm, we remove the model components re-lated to tm (including the retrieval modeland the memory encoder), and only employthe encoder-decoder architecture for nmt.
the resulted model is equivalent to the trans-former base model (vaswani et al., 2017)..1our code is released at https://github.com/.
jcyk/copyisallyouneed..2. tm-augmented nmt using source similar-ity search.
to isolate the effect of architec-tural changes in nmt models, we replaceour cross-lingual memory retriever with tradi-tional source-side similarity search.
speciﬁ-cally, we use the fuzzy match system used inxia et al.
(2019) and many others, which isbased on bm25 and edit distance..3. tm-augmented nmt using pre-trained cross-lingual retriever.
to study the effect of end-to-end task-speciﬁc optimization of the retrievalmodel, we pre-train the retrieval model usingthe cross-alignment tasks introduced in §3.4and keep it ﬁxed in the following nmt train-ing..4. our full model using a ﬁxed tm index; af-ter pre-training, we ﬁx the parameter of etgtduring nmt training..5. our full model trained with asynchronous in-.
dex refresh..results the results of the above models are pre-sented in table 2. we have the following observa-tions: (1) our full model trained with asynchronousindex refresh (model #5) delivers the best perfor-mance on test sets across all four translation tasks,outperforming the non-tm baseline (model #1)by 3.26 bleu points in average and up to 3.86bleu points (de⇒en).
this result conﬁrms thatmonolingual tm can boost nmt performance; (2)the end-to-end learning of the retriever model isthe key for substantial performance improvement.
we can see that using a pre-trained ﬁxed cross-lingual retriever only gives moderate test perfor-mance, ﬁne-tuning esrc and ﬁxing etgt signiﬁcantlyboosts the performance, and ﬁne-tuning both esrc.
7312figure 2: test results with 1/4 bilingual pairs (upper) and 2/4 bilingual pairs (lower) across different tm sizes..and etgt leads to the strongest performance (model#5>model #4>model #3); (3) cross-lingual re-trieval (model #4 and model #5) can obtain betterresults than that of the source similarity search(model #2).
this is remarkable since the cross-lingual retrieval only requires monolingual tm,while the source similarity search relies on bilin-gual tm.
we attribute the success, again, to the end-to-end adaptability of our cross-lingual retriever.
this is manifested by the fact that model #3 evenslightly underperforms model #2 in some of trans-lation tasks..contrast to previous bilingual tm systemswe also compare our results with the best previ-ously reported models.2 we can see that our resultssigniﬁcantly outperform previous arts.
notably, ourbest model (model #5) surpasses the best reportedmodel (xia et al., 2019) by 1.69 bleu points in av-erage and up to 2.9 bleu points (de⇒en).
thisresult veriﬁes the effectiveness of our proposedmodels.
in fact, we can see that our translationmodel using traditional similarity search (model#2) already outperforms the best previously re-ported results, which reveals that the architecturaldesign of our translation model is surprisingly ef-fective despite its simplicity..2some recent work used different datasets other than jrc-acquis with unspeciﬁed data split, which makes it hard tomake an exhaustive comparison.
however, note that our in-house baseline (model #2) is quite strong..4.3 low-resource scenarios.
one most unique characteristic of our proposedmodel is that it uses monolingual tm.
this moti-vates us to conduct experiments in low-resourcescenarios, where we use extra monolingual data inthe target language to boost translation quality..data we create low-resource scenarios by ran-domly partitioning each training set in jrc-acquiscorpus into four subsets of equal size.
we set uptwo series of experiments: (1) we only use thebilinguals pairs in the ﬁrst subset and gradually en-large the tm by including more monolingual datain other subsets.
(2) similar to (1), but we insteaduse the bilingual pairs in the ﬁrst two subsets..models as shown in §4.2, the model trained withasynchronous index refresh (model #5) is slightlybetter than the model using ﬁxed etgt (model #4),however, the computational cost of training model#5 is much bigger.
for simplicity and environmen-tal consideration, we only test model #4 in low-resource scenarios.
nevertheless, we note thereare still two modeling choices: (1) train the modelonce with the tm limited to training pairs andonly enlarge the tm during testing; (2) re-train themodel with every enlarged tm.
note that whenusing the ﬁrst choice, the model may retrieve atm sentence that has never been seen during train-ing.
to measure the performance improvementsfrom additional monolingual tm, we also includea transformer base baseline (model #1, denoted as.
7313data.
1/4 bilingual +4/4 monolingual.
2/4 bilingual +4/4 monolingual.
model.
oursbtours+btoursbtours+bt.
es⇒en.
en⇒es.
de⇒en.
en⇒de.
dev61.4662.4765.9865.1763.8266.95.test61.0261.9965.5164.6963.1066.38.dev57.8660.2862.4861.3161.5963.22.test57.4059.5962.2261.0160.8362.90.dev56.7757.7562.2261.4359.1763.68.test56.5458.2061.7961.1959.2663.10.dev51.1152.4756.7555.5554.1857.69.test51.5852.9656.5055.3554.2957.40.table 3: comparison with back-translation (bt)..#bilingual pairs#monolingual sents.
transformer baseours.
ours + domain-speciﬁcours + all-domains.
medical61,388184,165.law114,930344,791.it55,060165,181.koran4,45813,375.subtitle124,992374,977.avg.
--.
avg.
∆--.
using bilingual pairs only.
47.8147.52.
50.3250.23.
+ monolingual memory.
51.4051.17.
53.9754.12.
33.9034.64.
35.3335.24.
14.6415.49.
16.2616.24.
21.6422.66.
22.7822.78.
33.8834.30.
-+0.42.
35.7335.72.
+1.85+1.84.
table 4: test results on domain adaptation..base) and a bilingual tm baseline (model #2)..4.4 non-parametric domain adaptation.
results figure 2 shows the main results on thetest sets.
the general patterns are consistent acrossall experiments: the larger the tm becomes, thebetter translation performance the model achieves.
when using all available monolingual data (4/4),the translation quality is boosted signiﬁcantly.
in-terestingly, the performance of models without re-training is comparable to, if not better than, thosewith re-training.
we also observe that when thetraining pairs are very scarce (only 1/4 bilingualpairs are available), a small size of tm even hurtsthe model performance.
the reason could be over-ﬁtting.
we speculate that better results would beobtained by tuning the model hyper-parameters ac-cording to different tm sizes..lastly, the “plug and play” property of tm furthermotivates us to domain adaptation, where we adapta single general-domain model to a speciﬁc domainby using domain-speciﬁc monolingual tm..data to simulate a diverse multi-domain setting,we use the data splits in aharoni and goldberg(2020) originally collected by koehn and knowles(2017).
it includes german-english parallel datafor train/dev/test sets in ﬁve domains: medical,law, it, koran and subtitles.
similar to the experi-ments in §4.3, we only use one fourth of bilingualpairs for training.
the target side of the remainingdata is treated as additional monolingual data forbuilding domain-speciﬁc tm, and the source sideis discarded.
the data statistics can be found in theupper block of table 4. the dev and test sets foreach domain contains 2k instances..contrast to back-translation we compare ourmodels with back-translation (bt) (sennrich et al.,2016), a popular way of utilizing monolingual datafor nmt.
we train a target-to-source transformerbase model using bilingual pairs and use the resul-tant model to translate monolingual sentences toobtain additional synthetic parallel data.
as shownin table 3, our method performs better than btwith 2/4 bilingual pairs but performs worse with1/4 bilingual pairs.
interestingly, the combinationof bt and our method yields signiﬁcant furthergains, which demonstrates that our method is notonly orthogonal but also complementary to bt..models we ﬁrst train a transformer base base-line (model #1) on the concatenation of bilingualpairs in all domains.
as in §4.3, we train our modelusing ﬁxed etgt (model #4).
one advantage ofour approach is the possibility of training a singlemodel which can be adapted to any new domain atthe inference time without any re-training, by justswitching the tm.
when adapting to a new tm,we do not re-train our model.
as the purpose hereis to verify that our approach can tackle domainadaptation without any domain-speciﬁc training,we leave the comparison and combination of otherdomain adaptation techniques (moore and lewis,.
73142010; chu and wang, 2018) as future work..results the results are presented in table 4. wecan see that when only using the bilingual data, thetm-augmented model obtains higher bleu scoresin domains with less data but slightly lower scoresin other domains compared to the non-tm baseline.
however, as we switch the tm to domain-speciﬁctm, the translation quality is signiﬁcantly boostedin all domains, improving the non-tm baseline byan average of 1.85 bleu points, with improve-ments as large as 2.57 bleu points on law and2.51 bleu point on medical.
we also attempt tocombine all domain-speciﬁc tms to one and use itfor all domains (the last row in table 4).
however,we do not obtain noticeable improvement.
thisreveals that the out-of-domain data can providelittle help so that a smaller in-domain tm is sufﬁ-cient, which is also conﬁrmed by the fact that about90.21% of the retrieved sentences come from thecorresponding domain in the combined tm..4.5 running speed.
with the help of faiss in-gpu index, search overmillions of vectors can be made incredibly efﬁcient(often in tens of milliseconds).
in our implementa-tion, the memory search performs even faster thannaive bm253.
for the results in table 2, takingthe vanilla transformer base model (model #1) asthe baseline.
the inference latency of our mod-els (both model #4 and model #5) is about 1.36times of the baseline (all use a single nividia v100gpu).
note that the corresponding number for theprevious state-of-the-art model (xia et al., 2019) is1.80. as for training cost, the averaged time costper training step of model #4 and model #5 is 2.62times and 2.76 times of the baseline respectively,which are on par with traditional tm-augmentedbaselines (model #2 is 2.59 times) (all use two ni-vidia v100 gpus).
table 5 presents the results.
inaddition, we also observe that memory-augmentedmodels converge much faster than vanilla modelsin terms of training steps..5 conclusion.
we introduced an effective approach that augmentsnmt models with monolingual tm.
we show thata task-speciﬁc cross-lingual memory retriever canbe learned by end-to-end mt training.
our ap-proach achieves new state-of-the-art results on sev-.
3elasticsearchelastic.co/.
implementation:.
https://www..#1245-.
modeltransformer basesource similaritycross-lingual (ﬁxed etgt)cross-lingualxia et al.
(2019).
training1.00x2.59x2.62x2.76x-.
inference1.00x-1.36x1.36x1.80x.
table 5: latency cost for training and inference.
fortraining, we measure the averaged time cost per train-ing step.
the number of xia et al.
(2019) is inferredfrom their paper..eral datasets, leads to large gains in low-resourcescenarios where the bilingual data is limited, andcan specialize a nmt model for speciﬁc domainswithout further training..future work should aim to build over our pro-posed framework.
two obvious directions are: (1)even though our experiments validated that thewhole framework can be learned from scratch us-ing standard mt corpora, it is possible to initializeeach model component in our framework with mas-sively pre-trained models for performance enhance-ment; and (2) the nmt model can beneﬁt fromaggregating over a set of diverse memories, whichis not explicitly encouraged in current design..references.
roee aharoni and yoav goldberg.
2020. unsuperviseddomain clusters in pretrained language models.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7747–7763..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in 3rd internationalconference on learning representations, iclr..jane bromley, isabelle guyon, yann lecun, eduards¨ackinger, and roopak shah.
1993. signature veri-ﬁcation using a” siamese” time delay neural network.
in proceedings of the 6th international conferenceon neural information processing systems, pages737–744..t. brown, b. mann, nick ryder, melanie sub-biah, j. kaplan, p. dhariwal, arvind neelakantan,pranav shyam, girish sastry, amanda askell, sand-hini agarwal, ariel herbert-voss, g. kr¨uger, tomhenighan, r. child, aditya ramesh, d. ziegler, jef-frey wu, clemens winter, christopher hesse, markchen, e. sigler, mateusz litwin, scott gray, ben-jamin chess, j. clark, christopher berner, sam mc-candlish, a. radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
arxiv, abs/2005.14165..7315bram bulte and arda tezcan.
2019. neural fuzzy re-pair: integrating fuzzy matches into neural machinetranslation.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 1800–1809..deng cai, yan wang, wei bi, zhaopeng tu, xi-aojiang liu, wai lam, and shuming shi.
2019a.
skeleton-to-response: dialogue generation guidedin proceedings of the 2019by retrieval memory.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1219–1228..deng cai, yan wang, wei bi, zhaopeng tu, xiao-jiang liu, and shuming shi.
2019b.
retrieval-guided dialogue response generation via a matching-in proceedings of theto-generation framework.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1866–1875..qian cao, shaohui kuang, and deyi xiong.
2019.learning to reuse translations: guiding neural ma-arxiv preprintchine translation with examples.
arxiv:1911.10732..qian cao and deyi xiong.
2018. encoding gatedtranslation memory into neural machine translation.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages3042–3047..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879..chenhui chu and rui wang.
2018. a survey of do-main adaptation for neural machine translation.
inproceedings of the 27th international conference oncomputational linguistics, pages 1304–1319..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atin proceedings of the 2018 conference onscale.
empirical methods in natural language processing,pages 489–500..marzieh fadaee, arianna bisazza, and christof monz.
2017. data augmentation for low-resource neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computational.
linguistics (volume 2: short papers), pages 567–573..yang feng, shiyue zhang, andi zhang, dong wang,and andrew abel.
2017. memory-augmented neu-ral machine translation.
in proceedings of the 2017conference on empirical methods in natural lan-guage processing, pages 1390–1399..jiatao gu, zhengdong lu, hang li, and victor o.k.
incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640..jiatao gu, yong wang, kyunghyun cho, and vic-tor ok li.
2018. search engine guided neural ma-chine translation.
in aaai, pages 5133–5140..caglar gulcehre, orhan firat, kelvin xu, kyunghyuncho, loic barrault, huei-chi lin, fethi bougares,holger schwenk, and yoshua bengio.
2015. on us-ing monolingual corpora in neural machine transla-tion.
arxiv preprint arxiv:1503.03535..kelvin guu, tatsunori b. hashimoto, yonatan oren,and percy liang.
2018. generating sentences byediting prototypes.
transactions of the associationfor computational linguistics, 6:437–450..kelvin guu, kenton lee, zora tung, panupong pasu-pat, and ming-wei chang.
2020. realm: retrieval-arxivaugmented language model pre-training.
preprint arxiv:2002.08909..tatsunori b hashimoto, kelvin guu, yonatan oren,and percy s liang.
2018. a retrieve-and-edit frame-work for predicting structured outputs.
in advancesin neural information processing systems, pages10052–10062..di he, yingce xia, tao qin, liwei wang, nenghai yu,tie-yan liu, and wei-ying ma.
2016. dual learningfor machine translation.
advances in neural infor-mation processing systems, 29:820–828..junxian he, jiatao gu, jiajun shen, and marc’aurelioranzato.
2019. revisiting self-training for neuralin international conferencesequence generation.
on learning representations..qiuxiang he, guoping huang, qu cui, li li, andlemao liu.
2021. fast and accurate neural machinetranslation with translation memory.
in proceedingsof the 59th annual meeting of the association forcomputational linguistics..yifan he, yanjun ma, josef van genabith, and andyway.
2010. bridging smt and tm with translationrecommendation.
in proceedings of the 48th annualmeeting of the association for computational lin-guistics, pages 622–630..7316guoping huang, lemao liu, xing wang, longyuewang, huayang li, zhaopeng tu, chengyan huang,and shuming shi.
2021. transmart: a practical in-teractive machine translation system.
arxiv preprintarxiv..s´ebastien jean, orhan firat, kyunghyun cho, rolandmemisevic, and yoshua bengio.
2015. montrealneural machine translation systems for wmt’15.
inproceedings of the tenth workshop on statisticalmachine translation, pages 134–140..wenxiang jiao, xing wang, zhaopeng tu, shumingshi, r. michael lyu, and irwin king.
2021. self-training sampling with monolingual data uncertaintyin proceedings offor neural machine translation.
the 59th annual meeting of the association for com-putational linguistics..jeff johnson, matthijs douze, and herv´e j´egou.
2019.ieee.
billion-scale similarity search with gpus.
transactions on big data..vladimir karpukhin, barlas oguz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen, andwen-tau yih.
2020. dense passage retrieval foropen-domain question answering.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6769–6781..urvashi khandelwal, angela fan, dan jurafsky, lukenearestarxiv preprint.
zettlemoyer, and mike lewis.
2020.neighbor machine translation.
arxiv:2010.00710..urvashi khandelwal, omer levy, dan jurafsky, lukezettlemoyer, and mike lewis.
2019. generalizationthrough memorization: nearest neighbor languagemodels.
arxiv preprint arxiv:1911.00172..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..philipp koehn.
2004. statistical signiﬁcance tests forin proceedings ofmachine translation evaluation.
the 2004 conference on empirical methods in naturallanguage processing, pages 388–395..philipp koehn and rebecca knowles.
2017.challenges for neural machine translation.
preprint arxiv:1706.03872..sixarxiv.
mike lewis, marjan ghazvininejad, gargi ghosh, ar-men aghajanyan, sida wang, and luke zettlemoyer.
2020a.
pre-training via paraphrasing.
advances inneural information processing systems, 33..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich k¨uttler, mike lewis, wen-tau yih, timrockt¨aschel, et al.
2020b.
retrieval-augmented gen-arxiveration for knowledge-intensive nlp tasks.
preprint arxiv:2005.11401..yury a malkov and dmitry a yashunin.
2018. ef-ﬁcient and robust approximate nearest neighborsearch using hierarchical navigable small worldgraphs.
ieee transactions on pattern analysis andmachine intelligence..robert c. moore and william lewis.
2010. intelligentin pro-selection of language model training data.
ceedings of the acl 2010 conference short papers,pages 220–224..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96..anshumali shrivastava and ping li.
2014. asymmetriclsh (alsh) for sublinear time maximum inner productsearch (mips).
advances in neural information pro-cessing systems, 27:2321–2329..philipp koehn and jean senellart.
2010. convergenceof translation memory and statistical machine trans-in proceedings of amta workshop on mtlation.
research and the translation industry, pages 21–31..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 6086–6096..ralf steinberger, bruno pouliquen, anna widiger,camelia ignat, tomaz erjavec, dan tuﬁs, andd´aniel varga.
2006. the jrc-acquis: a multilingualaligned parallel corpus with 20+ languages.
arxivpreprint cs/0609058..masao utiyama, graham neubig, takashi onishi, andeiichiro sumita.
2011. searching translation mem-ories for paraphrases.
in machine translation sum-mit, volume 13, pages 325–331..7317ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..kun wang, chengqing zong, and keh-yih su.
2013.integrating translation memory into phrase-basedin proceed-machine translation during decoding.
ings of the 51st annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 11–21..jason weston, emily dinan, and alexander miller.
2018. retrieve and reﬁne: improved sequence gen-eration models for dialogue.
in proceedings of the2018 emnlp workshop scai: the 2nd interna-tional workshop on search-oriented conversationalai, pages 87–92..yu wu, furu wei, shaohan huang, yunli wang, zhou-jun li, and ming zhou.
2019. response generationby context-aware prototype editing.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 33, pages 7281–7288..mengzhou xia, guoping huang, lemao liu, andshuming shi.
2019. graph based translation mem-ory for neural machine translation.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 33, pages 7297–7304..jitao xu, josep crego, and jean senellart.
2020. boost-ing neural machine translation with similar trans-in proceedings of the 58th annual meet-lations.
ing of the association for computational linguistics,pages 1580–1590..masaru yamada.
2011. the effect of translation mem-ory databases on productivity.
translation researchprojects, 3:63–73..jingyi zhang, masao utiyama, eiichro sumita, gra-ham neubig, and satoshi nakamura.
2018. guidingneural machine translation with retrieved translationpieces.
in proceedings of the 2018 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 1 (long papers), pages 1325–1335..7318