integrating semantics and neighborhood information with graph-drivengenerative models for document retrievalzijing ou1, qinliang su1∗, jianxing yu2, bang liu3,jingwen wang4, ruihui zhao4, changyou chen5 and yefeng zheng41school of computer science and engineering, sun yat-sen university, guangzhou, china,2school of artiﬁcial intelligence, sun yat-sen university, guangdong, china,3rali & mila, universit´e de montr´eal, 4tencent jarvis lab,5cse department, suny at buffaloouzj@mail2.sysu.edu.cn, {suqliang, yujx26}@mail.sysu.edu.cn, bang.liu@umontreal.ca,{v wjwowang, zacharyzhao, yefengzheng}@tencent.com, changyou@buffalo.edu.
abstract.
with the need of fast retrieval speed and smallmemory footprint, document hashing has beenplaying a crucial role in large-scale informa-tion retrieval.
to generate high-quality hash-ing code, both semantics and neighborhoodinformation are crucial.
however, most ex-isting methods leverage only one of them orsimply combine them via some intuitive cri-teria, lacking a theoretical principle to guidethe integration process.
in this paper, weencode the neighborhood information with agraph-induced gaussian distribution, and pro-pose to integrate the two types of informationwith a graph-driven generative model.
to dealwith the complicated correlations among doc-uments, we further propose a tree-structuredapproximation method for learning.
under theapproximation, we prove that the training ob-jective can be decomposed into terms involv-ing only singleton or pairwise documents, en-abling the model to be trained as efﬁciently asuncorrelated ones.
extensive experimental re-sults on three benchmark datasets show thatour method achieves superior performanceover state-of-the-art methods, demonstratingthe effectiveness of the proposed model for si-multaneously preserving semantic and neigh-borhood information.1.
1.introduction.
similarity search plays a pivotal role in a varietyof tasks, such as image retrieval (jing and baluja,2008; zhang et al., 2018), plagiarism detection(stein et al., 2007) and recommendation systems(koren, 2008).
if the search is carried out in theoriginal continuous feature space directly, the re-quirements of computation and storage would be.
∗corresponding author.
qinliang su is also afﬁliated with(i) guangdong key lab.
of big data analysis and processing,guangzhou, china, and (ii) key lab.
of machine intelligenceand advanced computing, ministry of education, china..1our code is available at https://github.com/j-zin/snuh..the mindspore code will also be released soon..extremely high, especially for large-scale applica-tions.
semantic hashing (salakhutdinov and hin-ton, 2009b) sidesteps this problem by learning acompact binary code for every item such that simi-lar items can be efﬁciently found according to thehamming distance of binary codes..unsupervised semantic hashing aims to learnfor each item a binary code that can preserve thesemantic similarity information of original items,without the supervision of any labels.
motivated bythe success of deep generative models (salakhutdi-nov and hinton, 2009a; kingma and welling, 2013;rezende et al., 2014) in unsupervised representa-tion learning, many recent methods approach thisproblem from the perspective of deep generativemodels, leading to state-of-the-art performance onbenchmark datasets.
speciﬁcally, these methodstrain a deep generative model to model the underly-ing documents and then use the trained generativemodel to extract continuous or binary representa-tions from the original documents (chaidaroon andfang, 2017; shen et al., 2018; dong et al., 2019;zheng et al., 2020).
the basic principle behindthese generative hashing methods is to have thehash codes retaining as much semantics informa-tion of original documents as possible so that se-mantically similar documents are more likely toyield similar codes..in addition to semantics information, it is widelyobserved that neighborhood information among thedocuments is also useful to generate high-qualityhash codes.
by constructing an adjacency ma-trix from the raw features of documents, neighbor-based methods seek to preserve the informationin the constructed adjacency matrix, such as thelocality-preserving hashing (he et al., 2004; zhaoet al., 2014), spectral hashing (weiss et al., 2009; liet al., 2012), and etc.
however, since the ground-truth neighborhood information is not availableand the constructed one is neither accurate nor.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2238–2249august1–6,2021.©2021associationforcomputationallinguistics2238complete, neighbor-based methods alone do notperform as well as the semantics-based ones.
de-spite both semantics and neighborhood informationare derived from the original documents, differentaspects are emphasized in them.
thus, to obtainhigher-quality hash codes, it has been proposed toincorporate the constructed neighborhood informa-tion into semantics-based methods.
for examples,chaidaroon et al.
(2018) and hansen et al.
(2020)require the hash codes can reconstruct neighboringdocuments, in addition to the original input.
otherworks (shen et al., 2019; hansen et al., 2019) usean extra loss term, derived from the approximateneighborhood information, to encourage similardocuments to produce similar codes.
however, allof the aforementioned methods exploit the neigh-borhood information by using it to design differ-ent kinds of regularizers to the original semantics-based models, lacking a basic principle to unifyand leverage them under one framework..to fully exploit the two types of information, inthis paper, we propose a hashing method that uni-ﬁes the semantics and neighborhood informationwith the graph-driven generative models.
specif-ically, we ﬁrst encode the neighborhood informa-tion with a multivariate gaussian distribution.
withthis gaussian distribution as a prior in a generativemodel, the neighborhood information can be natu-rally incorporated into the semantics-based hash-ing model.
despite the simplicity of the modeling,the correlation introduced by the neighbor-encodedprior poses a signiﬁcant challenge to the trainingsince it invalidates the widely used identical-and-independent-distributed (i.i.d.)
assumption, mak-ing all documents correlated.
to address this issue,we propose to use a tree-structured distributionto capture as much as possible the neighborhoodinformation.
we prove that under the tree approx-imation, the evidence lower bound (elbo) canbe decomposed into terms involving only single-ton and pairwise documents, enabling the modelto be trained as efﬁciently as the models withoutconsidering the document correlations.
to capturemore neighborhood information, a more accurateapproximation by using multiple trees is also devel-oped.
extensive experimental results on three pub-lic datasets demonstrate that the proposed methodcan outperform state-of-the-art methods, indicatingthe effectiveness of the proposed framework in uni-fying the semantic and neighborhood informationfor document hashing..2 preliminaries.
semantics-based hashing due to the similar-ities among the underlying ideas of these meth-ods, we take the variational deep semantic hashing(vdsh) (chaidaroon and fang, 2017) as an exam-ple to illustrate their working ﬂow.
given a docu-ment x (cid:44) {wj}|x|j=1, vdsh proposes to model adocument by a generative model as.
p(x, z) = pθ(x|z)p(z),.
(1).
where p(z) is the prior distribution and is chosen tobe the standard gaussian distribution n (z; 0, i d),with i d denoting the d-dimensional identity matrix;and pθ(x|z) is deﬁned to be(cid:89).
pθ(x|z) =.
pθ(wi|z).
(2).
wi∈x.
with.
pθ(wi|z) (cid:44).
exp(zt ewi + bi)j=1 exp(zt ewj + bj).
,.
(cid:80)|v |.
(3).
in which wj denotes the |v |-dimensional one-hotrepresentation of the j-th word, with |x| and |v |denoting the document and vocabulary size, respec-tively; and e ∈ rd×|v | represents the learnableembedding matrix.
for a corpus containing n doc-uments x = {x1, x2, · · · , xn }, due to the i.i.d.
assumption for documents, it is modelled by simplymultiplying individual document models as.
p(x, z) =.
pθ(xk|zk)p(zk),.
(4).
n(cid:89).
k=1.
where z (cid:44) [z1; z2; · · · ; zn ] denotes a long vectorobtained by concatenating the individual vectors zi.
the model is trained by optimizing the evidencelower bound (elbo) of the log-likelihood functionlog p(x).
after training, outputs from the trainedencoder are used as documents’ representations,from which binary hash codes can be obtained bythresholding the real-valued representations..neighborhood information the ground-truthsemantic similarity information is not available forthe unsupervised hashing task in practice.
to lever-age this information, an afﬁnity n × n matrix ais generally constructed from the raw features (e.g.,the tfidf) of original documents.
for instances,we can construct the matrix as.
aij =.
.
.
e−0,.
||xi−xj||2.
σ., xi ∈ nk (xj)otherwise.
(5).
2239where aij denotes the (i, j)-th element of a; andnk(x) denotes the k-nearest neighbors of docu-ment x. given the afﬁnity matrix a, some methodshave been proposed to incorporate the neighbor-hood information into the semantics-based hashingmodels.
however, as discussed above, these meth-ods generally leverage the information based onsome intuitive criteria, lacking theoretical supportsbehind them..3 a hashing framework with uniﬁed.
semantics-neighborhood information.
in this section, we present a more effective frame-work to unify the semantic and neighborhood in-formation for the task of document hashing..3.1 reformulating the vdsh.
to introduce the neighborhood information into thesemantics-based hashing models, we ﬁrst rewritethe vdsh model into a compact form as.
p(x, z) = pθ(x|z)pi(z),.
(6).
where pθ(x|z) = (cid:81)npi(z) = (cid:81)n.k=1 pθ(xk|zk); and the priork=1 p(zk), which can be shown to be.
pi(z) = n (z; 0, i n ⊗ i d) ..(7).
here, ⊗ denotes the kronecker product and thesubscript i indicates independence among zk.
theelbo of this model can be expressed as.
l= eqφ(z|x)[log pθ(x|z)]−kl(qφ(z|x)||pi(z))(cid:123)(cid:122)(cid:125)(cid:125)l2.
(cid:124).
(cid:124).
(cid:123)(cid:122)l1.
where kl(·) denotes the kullback-leibler (kl)divergence.
by restricting the posterior to indepen-dent gaussian form.
qφ(z|x) =.
n(cid:89).
k=1.
n (cid:0)zk; µk, diag(σ2(cid:124)(cid:123)(cid:122)qφ(zk|xk).
k)(cid:1),(cid:125).
(8).
the l1 can be handled using the reparameteriza-tion trick.
thanks to the factorized forms assumedin qφ(z|x) and pi(z), the l2 term can also beexpressed analytically and evaluated efﬁciently..3.2.injecting the neighborhood information.
given an afﬁnity matrix a, the covariance matrixi n +λa can be used to reveal the neighborhood in-formation of documents, where the hyperparameterλ ∈ [0, 1) is used to control the overall correlation.
strength.
if two documents are neighboring, thenthe corresponding correlation value in i n + λawill be large; otherwise, the value will be zero.
to have the neighborhood information reﬂectedin document representations, we can require thatthe representations zi are drawn from a gaussiandistribution of the form.
pg(z) = n (z; 0, (i n + λa) ⊗ i d) ,.
(9).
where the subscript g denotes that the distributionis constructed from a neighborhood graph.
tosee why the representations z ∼ pg(z) havealready reﬂected the neighborhood information,let us consider an example with three documents{x1, x2, x3}, in which x1 is connected to x2,x2 is connected to x3, and no connection existsbetween x1 and x3.
under the case that zi is atwo-dimensional vector zi ∈ r2, we have theconcatenated representations [z1; z2; z3] follow agaussian distribution with covariance matrix of.
z1.
z2.
z3.
z1.
z2.
z3.
.
.
10λa21000.
010λa2100.λa12010λa320.
0λa12010λa32.
00λa23010.
.
.
000λa2301.from the property of gaussian distribution, it canbe known that z1 is strongly correlated with z2on the corresponding elements, but not with z3.
this suggests that z1 should be similar to z2, butdifferent from z3, which is consistent with theneighborhood relation that x1 is a neighbor of x2,but not of x3..now that the neighborhood information can bemodeled by requiring z being drawn from pg(z),and the semantic information can be reﬂected in thelikelihood function pθ(x|z).
the two types of in-formation can be taken into account simultaneouslyby modeling the corpus as.
p(x, z) = pθ(x|z)pg(z)..(10).
comparing to the vdsh model in (6), it canbe seen that the only difference lies in the em-ployed priors.
here, a neighborhood-preservingprior pg(z) is employed, while in vdsh, an in-dependent prior pi(z) is used.
although onlya modiﬁcation to the prior is made from the per-spective of modeling, signiﬁcant challenges areposed for the training.
speciﬁcally, by replac-ing pi(z) with pg(z) in the l2 of l, it can be.
2240(cid:1)−1.
shown that the expression of l2 involves the ma-trix (cid:0) (i n+λa) ⊗ i d.
due to the introduceddependence among documents, for example, if thecorpus contains over 100,000 documents and therepresentation dimension is set to 100, the l2 in-volves the inverse of matrices with dimension ashigh as 107, which is computationally prohibitivein practice..distribution pt (z) captures the neighborhood in-formation reﬂected on the spanning tree t. by us-ing pt (z) to replace pi(z) of l2, it can be shownthat l2 can be expressed as the summation of termsinvolving only one or two variables, which can behandled easily.
due to the limitation of space, theconcrete expression for the lower bound is given inthe supplementary material..4.1 approximating the prior pg(z) with a.qt (z|x)=.
qφ(zi|xi).
4 training with tree approximations.
although the prior pg(z) captures the full neigh-borhood information, its induced model is not prac-tically trainable.
in this section, to facilitate thetraining, we ﬁrst propose to use a tree-structuredprior to partially capture the neighborhood infor-mation, and then extend it to multiple-tree case formore accurate modeling..tree-structured distribution.
the matrix a represents a graph g (cid:44) (v, e),where v = {1, 2, · · · , n } is the set of documentindices; and e = {(i, j)|aij (cid:54)= 0} is the set ofconnections between documents.
from the graphg, a spanning tree t = (v, et ) can be obtainedeasily, where et denotes the set of connections onthe tree.2 based on the spanning tree, we constructa new distribution as.
pt (z) =.
pg(zi).
(cid:89).
i∈v.
(cid:89).
(i,j)∈et.
pg(zi, zj)pg(zi)pg(zj).
,.
(11).
where pg(zi) and pg(zi, zj) represent one- andtwo-variable marginal distributions of pg(z), re-spectively.
from the properties of gaussian distri-bution, it is known that.
pg(zi) = n(zi; 0, i d),.
pg(zi, zj) = n([zi;zj]; 0,(i 2 +λaij)⊗i d) , (12).
where aij (cid:44).
(cid:20) 0 aijaji 0.
(cid:21).
because pt (z) is de-.
ﬁned on a tree, as proved in (wainwright and jor-dan, 2008), it is guaranteed to be a valid probabil-ity distribution, and more importantly, it satisﬁesthe following two relations: i) pt (zi) = pg(zi);ii) pt (zi, zj) = pg(zi, zj) for any (i, j) ∈ et ,where pt (zi) and pt (zi, zj) denote the marginaldistributions of pt (z).
that is, the tree-structured.
2we assume the graph is connected.
for more general.
cases, results can be derived similarly..4.2.imposing correlations on the posterior.
the posterior distribution qφ(z|x) in the previoussection is assumed to be in independent form, asthe form shown in (8).
but since a prior pt (z)considering the correlations among documents isused, assuming an independent posterior is not ap-propriate.
hence, we follow the tree-structuredprior and also construct a tree-structured posterior.
(cid:89).
i∈v.
(cid:89).
qφ (zi, zj|xi, xj)qφ(zi|xi)qφ(zj|xj).
,.
(i,j)∈et.
where qφ(zi|xi) is the same as that in (8); andqφ (zi, zj|xi, xj) is also deﬁned to be gaussian,with its mean deﬁned as [µi; µj] and covariancematrix deﬁned as.
(cid:20).
diag(σ2i )diag(γij (cid:12)σi (cid:12)σj).
(cid:21)diag(γij (cid:12)σi (cid:12)σj)diag(σ2j ).
, (13).
in which γij ∈ rd controls the correlation strengthbetween zi and zj, whose elements are restrictedin (−1, 1) and (cid:12) denotes the hadamard product.
by taking the correlated posterior qt (z|x) intothe elbo, we obtain.
(cid:88).
lt =.
eqφ[log pθ(xi|zi)]−kl(qφ(zi)||pg(zi)).
i∈v(cid:88).
(cid:16).
−(i,j)∈et.
kl (qφ(zi, zj|xi, xj)||pg(zi, zj)).
(cid:17)−kl(qφ(zi)||pg(zi))−kl(qφ(zj)||pg(zj)).
,.
where we brieﬂy denote the variational distribu-tion qφ(zi|xi) as qφ(zi).
since pg(zi), pg(zi, zj),qφ(zi|xi) and qφ(zi, zj|xi, xj) are all gaussiandistributions, the kl-divergence terms above canbe derived in closed-form.
moreover, it can be seenthat lt involves only single or pairwise variables,thus optimizing it is as efﬁcient as the models with-out considering document correlation..with the trained model, hash codes can be ob-tained by binarizing the posterior mean µi witha threshold, as done in (chaidaroon and fang,.
22412017).
however, if without any constraint, therange of mean lies in (−∞, +∞).
thus, if webinarize it directly, lots of information in the orig-inal representations will be lost.
to alleviate thisproblem, in our implementation, we parameterizethe posterior mean µi by a function of the formµi = sigmoid(nn(xi)/τ ), where the outermostsigmoid function forces the mean to look like bi-nary value and thus can effectively reduce the quan-tization loss, with nn(·) denoting a neural networkfunction and τ controlling the slope of the sigmoidfunction..4.3 extending to multiple spanning trees.
obviously, approximating the graph with a span-ning tree may lose too much information.
to alle-viate this issue, we propose to capture the similar-ity information by a mixture of multiple distribu-tions, with each built on a spanning tree.
speciﬁ-cally, we ﬁrst construct a set of m spanning treestg = {t1, t2, · · · , tm } from the original graphg. based on the set of spanning trees, a mixture-distribution prior and posterior can be constructedas.
pmt (z) =.
pt (z),.
(14).
qmt (z|x) =.
qt (z|x),.
(15).
1m.1m.(cid:88).
t ∈tg(cid:88).
t ∈tg.
where pt (z) and qt (z|x) are the prior and pos-terior deﬁned on the tree t , as done in (11) and(13).
by taking the mixture distributions above intothe elbo of l to replace the prior and posterior,we can obtain a new elbo, denoted as lmt .
ob-viously, it is impossible to obtain a closed-formexpression for the bound lmt .
but as proved in(tang et al., 2019), by using the log-sum inequality,lmt can be further lower bounded by.
(cid:101)lmt =.
1m.(cid:88).
t ∈tg.
lt ..(16).
given the expression of lt , the lower bound of(cid:101)lmt can also be expressed in closed-form andoptimized efﬁciently.
for detailed derivations andconcrete expressions, please refer to the supple-mentary..4.4 details of modeling.
the parameters µi, µj, σi, σj and γij in the ap-proximate posterior distribution qφ(zi|xi) of (8).
figure 1: illustration of how the proposed model pre-serves the semantic and similarity information in therepresentations, where the color and link represent se-mantic similarity and neighborhood, respectively..and qφ(zi, zj|xi, xj) of (13) are all deﬁned as theoutputs of neural networks, with the parameters de-noted as φ. speciﬁcally, the entire model is mainlycomposed of three components:.
i) the variational encoder qφ(zi|xi), whichtakes single document as input, and outputsthe mean and variance of gaussian distribu-tion, i.e., [µi; σ2i ] = fφ(xi);.
ii) the correlated encoder, which takes pairwisedocuments as input, and outputs the corre-i.e., γij = fφ(xi, xj).
lation coefﬁcient,note that the correlation encoder is requiredto be order-irrelevant, that is, fφ(xi, xj) =fφ(xj, xi), which is achieved in this paper as(cid:0)fφ(xi, xj) + fφ(xj, xi)(cid:1);fφ = 12.iii) the generative decoder pθ(xi|zi), whichtakes the latent variable zi as input and outputthe document xi.
the decoder is modeled bya neural network parameterized by θ..the model is trained by optimizing the lower bound(cid:101)lmt w.r.t.
φ and θ. after training, hash codesare obtained by passing the documents through thevariational encoder and binarizing the outputs onevery dimension by a the threshold value, which issimply set as 0.5 in our experiments..to intuitively understand the insight behind ourmodel, an illustration is shown in figure 1. we seethat if the two documents are neighbors and seman-tically similar, the representations will be stronglycorrelated to each other.
but if they are not semanti-cally similar neighbors, the representations becomeless correlated.
if they are neither neighbors norsemantically similar, the representations becomenot correlated at all.
since our model can simulta-neously preserve semantics and neighborhood in-formation, we name it as semantics-neighborhooduniﬁed hahing (snuh)..2242𝒛𝒋𝒛𝒊𝒛𝒋𝒛𝒍encoderx𝒊x𝒋x𝒌x𝒍𝒛𝒌𝒛𝒊x𝒊x𝒌x𝒋x𝒍x𝒊x𝒋encoderencoder5 related work.
deep generative models (rezende et al., 2014)have attracted a lot of attention in semantics-based hashing, due to their successes in unsuper-vised representation learning.
vdsh (chaidaroonand fang, 2017) ﬁrst employed variational auto-encoder (vae) (kingma and welling, 2013) tolearn continuous representations of documents andthen casts them into binary codes.
however, forthe sake of information leaky problem during bi-narization step, such a two-stage strategy is proneto result in local optima and undermine the perfor-mance.
nash (shen et al., 2018) tackled this is-sue by replacing the gaussian prior with bernoulliand adopted the straight-through technique (ben-gio et al., 2013) to achieve end-to-end training.
tofurther improve the model’s capability, dong et al.
(2019) proposed to employ mixture distributionas a priori knowledge and zheng et al.
(2020) ex-ploited boltzmann posterior to introduce correla-tion among bits.
beyond generative frameworks,ammi (stratos and wiseman, 2020) achieved supe-rior performance by maximizing the mutual infor-mation between codes and documents.
neverthe-less, the aforementioned semantic hashing methodsare consistently under the i.i.d.
assumption, whichmeans they ignore the neighborhood information.
spectral hashing (weiss et al., 2009) and self-taught hashing (zhang et al., 2010) are two typicalmethods of neighbor-based hashing models.
butthese algorithms generally ignore the rich semanticinformation associated with documents.
recently,some vae-based models tried to concurrently takeaccount of semantic and neighborhood informa-tion, such as nbrreg (chaidaroon et al., 2018),rbsh (hansen et al., 2019) and pairrec(hansenet al., 2020).
however, as mentioned before, allof them simply regarded the proximity as regu-larization, lacking theoretical principles to guidethe incorporation process.
thanks to the virtueof graph-induced distribution, we effectively pre-serve the two types of information in a theoreticalframework..6 experiments.
6.1 experiment setup.
i) reuters25178, which contains 10,788 news docu-ments with 90 different categories; ii) tmc, whichis a collection of 21,519 air trafﬁc reports with 22different categories; iii) 20newsgroups (ng20),which consists of 18,828 news posts from 20 dif-ferent topics.
note that the category labels of eachdataset are only used to compute the evaluationmetrics, as we focus on unsupervised scenarios..baselines we compare our method with the fol-lowing models: sph (weiss et al., 2009), sth(zhang et al., 2010), vdsh (chaidaroon and fang,2017), nash (shen et al., 2018), gmsh(donget al., 2019), nbrreg (chaidaroon et al., 2018),corrsh (zheng et al., 2020) and ammi (stratosand wiseman, 2020).
for all baselines, we take thereported performance from their original papers..training details for fair comparisons, we fol-low the same network architecture used in vdsh,gmsh and corrsh, using a one-layer feed-forward neural network as the variational and thecorrelated encoder.
the graph g is constructedwith the k-nearest neighbors (knn) algorithmbased on cosine similarity on the tfidf featuresof documents.
in our experiments, the corre-lation strength coefﬁcient λ in (12) is ﬁxed to0.99. according to the performance observedon the validation set, we choose the learningrate from {0.0005, 0.001, 0.003}, batch size from{32, 64, 128}, the temperature τ in sigmoid func-tion from {0.1, 0.2, · · · , 1}, the number of trees mand neighbors k both form {1,2,. .
.
,20}, with thebest used for evaluation on the test set.
the modelis trained using the adam optimizer (kingma andba, 2014).
more detailed experimental settings,along with the generating method of spanning trees,are given in the supplementary materials..evaluation metrics the retrieval precision isused as our evaluation metric.
for each query doc-ument, we retrieve 100 documents most similar toit based on the hamming distance of hash codes.
then, the retrieval precision for a single sample ismeasured as the percentage of the retrieved docu-ments with the same label as the query.
finally, theaverage precision over the whole test set is calcu-lated as the performance of the evaluated method..datasets we verify the proposed methods onthree public datasets which published by vdsh3:.
3https://github.com/unsuthee/variationaldeepsemantic.
hashing/tree/master/dataset.
6.2 performance and analysis.
overall performance the performances of allthe models on the three public datasets are shown intable 1. we see that our model performs favorably.
2243method.
reuters.
tmc.
20newsgroups.
avg.
16bits.
32bits.
64bits 128bits 16bits.
32bits.
64bits 128bits 16bits.
32bits.
64bits 128bits.
0.6340 0.6513 0.6290 0.6045 0.6055 0.6281 0.6143 0.5891 0.3200 0.3709 0.3196 0.2716 0.5198sphsth0.7351 0.7554 0.7350 0.6986 0.3947 0.4105 0.4181 0.4123 0.5237 0.5860 0.5806 0.5443 0.5662vdsh 0.7165 0.7753 0.7456 0.7318 0.6853 0.7108 0.4410 0.5847 0.3904 0.4327 0.1731 0.0522 0.53660.4120 0.4644 0.4768 0.4893 0.4249nbrregnash 0.7624 0.7993 0.7812 0.7559 0.6573 0.6921 0.6548 0.5998 0.5108 0.5671 0.5071 0.4664 0.6462gmsh 0.7672 0.8183 0.8212 0.7846 0.6736 0.7024 0.7086 0.7237 0.4855 0.5381 0.5869 0.5583 0.6807ammi0.8173 0.8446 0.8506 0.8602 0.7096 0.7416 0.7522 0.7627 0.5518 0.5956 0.6398 0.6618 0.7323corrsh 0.8212 0.8420 0.8465 0.8482 0.7243 0.7534 0.7606 0.7632 0.5839 0.6183 0.6279 0.6359 0.7355.n.a..n.a..n.a..n.a..n.a..n.a..n.a..n.a..snuh 0.8320 0.8466 0.8560 0.8624 0.7251 0.7543 0.7658 0.7726 0.5775 0.6387 0.6646 0.6731 0.7474.table 1: the precision on three datasets with different numbers of bits in unsupervised document hashing..ablation study.
16bits.
32bits.
64bits.
128bits.
reuters.
tmc.
ng20.
snuhindsnuhpriorsnuh.
snuhindsnuhpriorsnuh.
snuhindsnuhpriorsnuh.
0.7823 0.8094 0.81800.8043 0.8295 0.84310.8320 0.8466 0.8560.
0.6978 0.7307 0.74210.7177 0.7408 0.75180.7251 0.7543 0.7658.
0.4806 0.5503 0.60170.5443 0.6071 0.62120.5775 0.6387 0.6646.
0.83850.84600.8624.
0.75260.75280.7726.
0.60600.60140.6731.table 2: the performance of variant models.
snuhindand snuhprior indicate the model without consideringany document correlations (independent) and only con-sidering correlations in the prior, respectively..to the current state-of-the-art method, yielding bestaverage performance across different datasets andsettings.
compared with vdsh and nash, whichsimply employ isotropic gaussian and bernoulliprior, respectively, we can observe that our model,which leverages correlated prior and posterior dis-tributions, achieves better results on all the threedatasets.
although gmsh improves performanceby exploiting a more expressive gaussian mixtureprior, our model still outperforms it by a substantialmargin, indicating the superiority of incorporatingdocument correlations.
it is worth noting that, byunifying semantics and neighborhood informationunder the generative models, the two types of in-formation can be preserved more effectively.
thiscan be validated by that our model performs sig-niﬁcantly better than nbrreg, which naively incor-porates the neighborhood information by using aneighbor-reconstruction regularizer.
the superior-ity of our uniﬁed method can be further corrobo-rated in the comparisons with rbsh and pairrec,which are given in the supplementary since theyemployed a different preprocessing method as themodels reported here.
comparing to the currentsota methods of ammi and corrsh, our method.
figure 2: the precision of 64-bit hash codes with vary-ing number of trees m and neighbors k..is still able to achieve better results by exploit-ing the correlation among documents.
moreover,thanks to the beneﬁt of correlation regularization,remarkable gratuity can be acquired proﬁtably in64 and 128 bits..impact of introducing correlations in priorand posterior to understand the inﬂuences ofthe proposed document-correlated prior and pos-terior, we further experiment with two variants ofour model: i) snuhind: which does not considerdocument correlations in neither the prior nor theposterior distribution; ii) snuhprior: which onlyconsiders the correlations in the prior, but not inthe posterior.
obviously, the proposed snuh rep-resents the method that leverage the correlations inboth of the prior and posterior.
as seen from ta-ble 2, snuhprior achieves better performance thansnuhind, demonstrating the beneﬁt of consideringthe correlation information of documents only inthe prior.
by further taking the correlations intoaccount in the posterior, improvements of snuhcan be further observed, which fully corroboratesthe superiority of considering document correla-tions in the prior and posterior.
another interest-ing observation is that the performance gap be-.
2244012345678910 1 x p e h u  r i  7 u h h v0.730.740.750.760.770.78 3 u h f l v l r q    tmc012345678910 1 x p e h u  r i  7 u h h v0.580.600.620.640.660.680.70 3 u h f l v l r q    20newsgroups012345678910 1 x p e h u  r i  1 h l j k e r u v0.730.740.750.760.770.78 3 u h f l v l r q    tmc012345678910 1 x p e h u  r i  1 h l j k e r u v0.580.600.620.640.660.680.70 3 u h f l v l r q    20newsgroupsdistance category.
title/subject.
query11020507090.hockeyhockeyhockeyhockeyforsalehardware.
nhl playoff results for games played 4-21-93nhl playoff results for games played 4-19-93nhl summary parse results for games played thur, april 15, 1993ahl playoff results (4/15)re: == moving sale ===re: quadra scsi problems?.
politics.misc re: employment (was re: why not concentrate on child molesters?.
table 3: qualitative analysis of the learned 128-bit hash codes on the 20newsgroups dataset.
we present thedocuments with hamming distance of 1, 10, 20, 50, 70 and 90 to the query..tween snuhind and snuhprior becomes small asthe length of bits increases.
this may be attributedto the fact that the increased generalization abilityof models brought by large bits is inclined to alle-viate the impact of priori knowledge.
however, byadditionally incorporating correlation constraintson posterior, signiﬁcant performance gains wouldbe obtained, especially in large bits scenarios..effect of spanning trees for more efﬁcienttraining, spanning trees are utilized to approximatethe whole graph by dropping out some edges.
tounderstand its effects, we ﬁrst investigate the im-pact of the number of trees.
the ﬁrst row of figure2 shows the performance of our method as a func-tion of different numbers of spanning trees.
weobserve that, compared to not using any correlation,one tree alone can bring signiﬁcant performancegains.
as the tree number increases, the perfor-mance rises steadily at ﬁrst and then converges intoa certain level, demonstrating that the documentcorrelations can be mostly captured by several span-ning trees.
then, we further explore the impact ofthe neighbor number when constructing the graphsusing the knn method, as shown in the second rowof figure 2. it can be seen that more neighbors con-tributes to better performance.
we hypothesize thatthis is partly due to the more diverse correlationinformation captured by the increasing number ofneighbors.
however, incorporating too many neigh-bors may lead to the problem of introducing noiseand incorrect correlation information to the hashcodes.
that explains why no further improvementis observed after the number reaches a level..(a) snuh.
(b) ammi.
figure 3: visualization of the 64-dimensional latent se-mantic embeddings learned by the proposed models forthe 20newsgroups dataset..vdsh in 2.038s, 4.364s, 1.051s.
it can be seenthat our model, though with much stronger per-formance, can be trained almost as efﬁciently asvanilla vdsh due to the tree approximations..case studyin table 3, we present a retrievalcase of the given query document.
it can be ob-served that as the hamming distance increases, thesemantic (topic) of the retrieved document gradu-ally becomes more irrelevant, illustrating that thehamming distance can effectively measure the doc-ument relevance..visualization of hash codes to evaluate thequality of generated hash code more intuitively,we project the latent representations into a 2-dimensional plane with the t-sne (van der maatenand hinton, 2008) technique.
as shown in figure3, the representations generated by our method aremore separable than those of ammi, demonstrat-ing the superiority of our method..7 conclusion.
empirical study of computational efﬁciencywe also investigate the training complexity bycomparing the training duration of our methodand vdsh, on tesla v100-sxm2-32gb.
on thereuters, tmc, 20newsgroups datasets with 64-bit hash codes, our method ﬁnishes one epoch oftraining respectively in 3.791s, 5.238s, 1.343s and.
we have proposed an effective and efﬁcient seman-tic hashing method to preserve both the seman-tics and neighborhood information of documents.
speciﬁcally, we applied a graph-induced gaussianprior to model the two types of information in auniﬁed framework.
to facilitate training, a tree-structure approximation was further developed to.
2245atheismgraphicswindows.mischardwarehardwarewindows.xforsaleautosmotorcyclesbaseballhockeycryptelectronicssci.medspacechristiangunsmideastpolitics.miscreligion.miscdecompose the elbo into terms involving only sin-gleton or pairwise variables.
extensive evaluationsdemonstrated that our model signiﬁcantly outper-forms baseline methods by incorporating both thesemantics and neighborhood information..acknowledgements.
this work is supported by the national naturalscience foundation of china (no.
61806223,61906217, u1811264), key r&d program ofguangdong province (no.
2018b010107005), na-tional natural science foundation of guangdongprovince (no.
2021a1515012299).
this work isalso supported by mindspore..references.
yoshua bengio, nicholas l´eonard,.
and aaroncourville.
2013. estimating or propagating gradi-ents through stochastic neurons for conditional com-putation.
arxiv preprint arxiv:1308.3432..suthee chaidaroon, travis ebesu, and yi fang.
2018.deep semantic text hashing with weak supervision.
in the 41st international acm sigir conference onresearch & development in information retrieval,pages 1109–1112..suthee chaidaroon and yi fang.
2017. variationaldeep semantic hashing for text documents.
in pro-ceedings of the 40th international acm sigir con-ference on research and development in informa-tion retrieval, pages 75–84..wei dong, qinliang su, dinghan shen, and changyouchen.
2019. document hashing with mixture-priorgenerative models.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing, pages 5226–5235..casper hansen, christian hansen, jakob grue simon-sen, stephen alstrup, and christina lioma.
2019.unsupervised neural generative semantic hashing.
in proceedings of the 42nd international acm si-gir conference on research and development ininformation retrieval..casper hansen, christian hansen, jakob grue simon-sen, stephen alstrup, and christina lioma.
2020.unsupervised semantic hashing with pairwise recon-struction.
in proceedings of the 43rd internationalacm sigir conference on research and develop-ment in information retrieval..xiaofei he, deng cai, haifeng liu, and wei-ying ma.
2004. locality preserving indexing for documentin proceedings of the 27th annualrepresentation.
international acm sigir conference on research.
and development in information retrieval, pages96–103..yushi jing and shumeet baluja.
2008. visualrank:applying pagerank to large-scale image search.
ieee transactions on pattern analysis and machineintelligence, 30(11):1877–1890..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..diederik p kingma and max welling.
2013. auto-arxiv preprint.
encoding variational bayes.
arxiv:1312.6114..yehuda koren.
2008. factorization meets the neighbor-hood: a multifaceted collaborative ﬁltering model.
in proceedings of the 14th acm sigkdd inter-national conference on knowledge discovery anddata mining, pages 426–434..peng li, meng wang, jian cheng, changsheng xu,and hanqing lu.
2012.spectral hashing withsemantically consistent graph for image indexing.
ieee transactions on multimedia, 15(1):141–152..laurens van der maaten and geoffrey hinton.
2008.visualizing data using t-sne.
journal of machinelearning research, pages 2579–2605..danilo jimenez rezende, shakir mohamed, and daanwierstra.
2014. stochastic backpropagation and ap-proximate inference in deep generative models.
inproceedings of the 31st international conference onmachine learning, pages 1278–1286..ruslan salakhutdinov and geoffrey hinton.
2009a.
deep boltzmann machines.
in artiﬁcial intelligenceand statistics, pages 448–455.
the proceedings ofmachine learning research..ruslan salakhutdinov and geoffrey hinton.
2009b.
se-international journal of approxi-.
mantic hashing.
mate reasoning, 50(7):969–978..dinghan shen, qinliang su, paidamoyo chapfuwa,wenlin wang, guoyin wang, ricardo henao, andlawrence carin.
2018. nash: toward end-to-endneural architecture for generative semantic hashing.
in proceedings of the 56th annual meeting of theassociation for computational linguistics, pages2041–2050..yuming shen, li liu, and ling shao.
2019. unsuper-vised binary representation learning with deep varia-tional networks.
international journal of computervision, 127(11):1614–1628..benno stein, sven meyer zu eissen, and martin pot-thast.
2007. strategies for retrieving plagiarized doc-uments.
in proceedings of the 30th annual interna-tional acm sigir conference on research and de-velopment in information retrieval, pages 825–826..2246karl stratos and sam wiseman.
2020..learningdiscrete structured representations by adversariallyin proceedingsmaximizing mutual information.
of the 37th international conference on machinelearning, pages 9144–9154..da tang, dawen liang, tony jebara, and nicholasruozzi.
2019. correlated variational auto-encoders.
in proceedings of the 36th international conferenceon machine learning, pages 6135–6144..martin j wainwright and michael irwin jordan.
2008.graphical models, exponential families, and varia-tional inference.
now publishers inc..yair weiss, antonio torralba, and rob fergus.
2009.in advances in neural informa-.
spectral hashing.
tion processing systems, pages 1753–1760..dell zhang, jun wang, deng cai, and jinsong lu.
2010. self-taught hashing for fast similarity search.
in proceedings of the 33rd international acm si-gir conference on research and development ininformation retrieval, pages 18–25..yanhao zhang, pan pan, yun zheng, kang zhao,yingya zhang, xiaofeng ren, and rong jin.
2018.visual search at alibaba.
in proceedings of the 24thacm sigkdd international conference on knowl-edge discovery & data mining, pages 993–1001..kang zhao, hongtao lu, and jincheng mei.
2014. lo-in twenty-eighth aaai.
cality preserving hashing.
conference on artiﬁcial intelligence..lin zheng, qinliang su, dinghan shen, and changyouchen.
2020. generative semantic hashing enhancedin proceedings of thevia boltzmann machines.
58th annual meeting of the association for compu-tational linguistics, pages 777–788..appendices.
a derivation of formulas.
derivation of kl (qφ(z|x)||pt (z))in themain paper, we propose a tree-type distributionto introduce partial neighborhood information sothat the l2 term can be expressed as the summa-tion over terms involving only one or two variables.
here, we provide the detail derivation..kl (qφ(z|x)||pt (z)).
(cid:90).
=.
qφ(z|x)log.
qφ(zi|xi).
(cid:81)i∈vpg(zi) (cid:81).
(i,j)∈et.
(cid:81)i∈v.
dz.
pg (zi,zj )pg (zi)pg (zj ).
kl (qφ(zi|xi)||pθ(zi)).
eqφ(zi,zj |xi,xj ).
log.
(cid:20).
pg(zi)pg(zj)pg(zi, zj).
(cid:21).
..=.
(cid:88).
i∈v.
(cid:88).
−.
(i,j)∈et.
obviously, the kl divergence is decomposed intothe terms involving singleton and pairwise vari-ables, which can be calculated efﬁciently..expressing lt in analytical form for sim-in the following, we use µ1, σ1pliﬁcation,to represent the mean and variance matrix ofqt (zi, zj|xi, xj), respectively, and represent thoseof pg(zi, zj) as µ2, σ2, respectively.
besides wedenote λaij as τij so we have τij = λaij = λaji.
by applying the cholesky decomposition on thecovariance matrix of σ1 and σ2.
σ1=.
σ2 =.
(cid:34) σiγijσj(cid:34) i dτiji d.(cid:113).
0d1 − γ2.
ijσj.
(cid:113).
01 − τ 2.iji d.(cid:35)(cid:34) σi0d(cid:35)(cid:34) i d0.
(cid:113).
γijσj1 − γ2.
ijσj.
(cid:113).
τiji d1 − τ 2.iji d.(cid:35).
(cid:35).
,.
,.
where we omit diag(·) for simplifying, we have.
kl (qφ(zi, zj|xi, xj)||pg(zi, zj)).
log(1 − τ 2ij).
(cid:110).
d(cid:88).
=.
12n=1− (cid:0) log σ2in+σ2σ2.
+.
jn + log(1 − γ2.
in + log σ2jn−2τijγijnσinσjn+µ2.
ijn)(cid:1) − 2in+µjn−2τijµinµjn(cid:111)..1 − τ 2ij.
2247algorithm 1 model training algorithminput: document representations x; edges list of spanningtrees e; batch size b.output: optimal parameters (θ, φ).
1: θ, φ ← initialize parameters2: repeat3:4:5:6:.
v m ← {x1, · · · , xb} ∼ xe mt ← {e1, · · · , eb} ∼ eg ← ∇φ,θ (cid:101)lmθ, φ ← update parameters using gradients g (e.g.,.
(cid:46) sample nodes(cid:46) sample endges.
mt (θ, φ; v m , e mt ).
adam optimizer).
7: until convergence of parameters (θ, φ).
input.
document pair (xi; xj).
variational enc.
correlated enc.
encoder.
linear(|v |, d) linear(|v |, d) linear(2|v |, d)γ = 2 ∗ f (·) − 1σ = g(·)µ = f (·/τ ).
generator.
linear(d, |v |).
table 4: the neural network architecture of the pro-posed model, in which f (·) and g(·) represent the sig-moid and softplus function, respectively..then, we can express lt in an analytical form.
lt =.
eqφ(zi|xi)[log pθ(xi|zi)] −.
(cid:88).
(cid:16).
i∈v.
+ σ2.
(cid:17)in −1−2 log σin).
12.d(cid:88).
n=1.
(µ2in.
log(1 − τ 2ij).
d(cid:88).
(cid:110).
(cid:18) 12.
(cid:88).
−(i,j)∈et.
n=1jn + log(1 − γ2.
derivation of (cid:101)lmt with lmt , we extend thesingle-tree approximation to multi-tree approxima-tion.
although the kl divergence between themixture distributions does not have a closed-formsolution, we can obtain its explicit upper bound byusing the log-sum inequality as.
lmt ≥.
(cid:88).
1m.eqt (z|x)[log pθ(x|z)].
kl (qt (z|x)||pt (x)).
t ∈tg1m.−.
(cid:88).
t ∈tg.
(cid:44) (cid:101)lmt ..we can further express (cid:101)lmt in a more intuitiveform as(cid:16)(cid:88).
(cid:17)eqφ(zi|xi)[log pθ(xi|zi)]−kl(qφ(zi|xi)||pg(zi)).
i∈v.
−.
(cid:88).
(cid:16).
(i,j)∈et.
wij.
kl(qφ(zi, zj|xi, xj)||pg(xi, xj)).
(cid:17)−kl(qφ(zi|xi)||pg(zi))−kl(qφ(zj|xj)||pg(zj)).
,.
(cid:46) visited node list.
(cid:46) choose node(cid:46) initial queue.
(cid:46) input: #tree n(cid:46) initial edges list.
i ← rc[v ==f alse]q = [i]while len(q) > 0 do.
e = [ ]for k ← 0, · · · , n − 1 dov = [f alse]|v|while f alse in v do.
algorithm 2 spanning tree generation algorithminput: graph g; number of trees n.output: edges list of spanning trees e.1: procedure treegen(n)2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23: end procedure.
i ← q[0]v [i] ← t ruen = id[v [n (i)]==f alse]if len(n ) == 0 thenp op (q, −1)break.
end ifj ← rc[n ]v [j] ← t rueap p en d(q, j)ap p en d(e, [i, j]).
(cid:46) choose neighbor.
end while.
end while.
end for.
where wij = |{t ∈tg|(i,j)∈et }|denotes the propor-tion of times that the edge (i, j) appears.
to opti-mize this objective, we can construct an estimatorof the elbo, based on the minibatch.
m.lvm (xi) −.
(cid:88).
wijlem.
t.(xi, xj),.
i∈vm.
(i,j)∈emt.where v m is the subset of documents, e msubset of edges and.
t is the.
lvm (xi) (cid:44) eqφ(zi|xi)[log pθ(xi|zi)].
− kl (qφ(zi|xi)||pg(zi)) ;.
(xi, xj) (cid:44) kl(qφ(zi, zj|xi, xj)||pg(xi, xj)).
lem−kl(qφ(zi|xi)||pg(zi))−kl(qφ(zj|xj)||pg(zj)) ..t.then we can update the parameters by using thegradient ∇φ,θ (cid:101)lmmt .
the training procedure issummarized in algorithm 1..b tree generation algorithm.
algorithm 2 shows the spanning tree generation al-gorithm treegen(·) used in our graph-induced gen-erative document hashing model.
treegen(·) uti-lizes a depth-ﬁrst search (dfs) algorithm to gener-ate meaningful neighborhood information for eachnode.
in this algorithm, rc[·] means randomly.
− (cid:0)µ2σ2.
in + µ2in+σ2.
jn + σ2.
in + σ2jn−2τijγijnσinσjn+µ2.
ijn)(cid:1)in+µjn−2τijµinµjn.
(cid:111)(cid:19).
+.
1 − τ 2ij.
(cid:101)lmt (cid:39) (cid:101)lmmt(cid:88)=.
2248datasets methods.
16bits.
32bits.
64bits.
128bits.
reuters.
tmc.
ng20.
rbsh 0.7740 0.8149 0.8120pairrec0.8028 0.8268 0.8329snuh 0.8063 0.8369 0.8483.rbsh 0.7959 0.8138 0.82240.7991 0.8239 0.8280pairrecsnuh 0.7901 0.8145 0.8293.rbsh 0.6087 0.6385 0.6655pairrecsnuh 0.5679 0.6444 0.6806.n.a..n.a..n.a..0.80880.84680.8567.
0.81930.83030.8329.
0.6668n.a.
0.7004.table 5: the precision of variant models on threedatasets with different numbers of bits..choosing one index according to the indicator func-tion; id[·] represents the set of node indexes sat-isfying the indicator condition and n (i) denotesthe neighbors of node i. due to the importance ofedges precision, when choosing a neighbor (line16 in algorithm 2), instead of using uniform sam-pling, we exploit a temperature α to control thetrade-off between the precision and diversity ofedges.
speciﬁcally, the probability of samplingneighbor j of node i is.
exp(cos(xtn∈n (i) exp(cos(xt.
j xi)/α).
n xi)/α).
..(cid:80).
we ﬁnd the best conﬁguration of α on the validationset with the values in {0.1, 0.2, · · · , 1} ..c experiment details.
for fair comparisons, we follow the experimen-tal setting of vdsh.
speciﬁcally, the vocabularysize |v | is 7164, 20000, and 10000 for reuters,tmc and 20newsgroups, respectively.
the splitof training, validation, and test set is as follows:7752, 967, 964 for reuters; 21286, 3498, 3498 fortmc and 11016, 3667, 3668 for 20newsgroups,respectively.
moreover, the kl term in eq.
(18) ofthe main paper is weighted with a coefﬁcient β toavoid posterior collapse.
we ﬁnd the best conﬁg-uration of β on the validation set with the valuesin {0.01, 0.02, · · · , 0.1}.
to intuitively understandour model, we illustrate the whole architecture intable 4..d additional experiments.
comparing with rbsh and pairrec as men-tioned before, the reason we do not directly com-pare our method with rbsh (hansen et al., 2019)and pairrec (hansen et al., 2020) is that their data.
figure 4: the precision of 64-bit hash codes on threedatasets with varying temperature τ and kl weight β..processing methods are different from the main-stream methods (e.g., vdsh, nash, gmsh, nbr-reg, ammi and corrsh).
to further compare ourmethod with them, we evaluate our model on threedatasets that are published by rbsh4.
the resultsare illustrated in table 5. we observe that ourmethod achieves the best performances in mostexperimental settings, which further conﬁrms thesuperiority of simultaneously preserving the seman-tics and similarity information in a more principledframework..parameter sensitivity to understand the robust-ness of our model, we conduct a parameter sensi-tivity analysis of τ and β in figure 4. comparedwith β = 0 (without using neighborhood informa-tion), models with β (cid:54)= 0 improve performancesigniﬁcantly, but gradually performs steadily as βgetting larger, which once again conﬁrms the im-portance of simultaneously modeling semantic andneighborhood information.
as for temperature co-efﬁcient τ used in variational encoder, our modelperforms steadily with various values of τ in thereuters dataset.
but in tmc and 20newsgroups,increasing τ would deteriorate the model perfor-mance.
generally speaking, the model can achievebetter performance with smaller τ (i.e., steeper sig-moid function).
as we utilize 0.5 as the thresholdvalue, steeper sigmoid functions make it easier todistinguish hash codes..4https://github.com/casperhansen/rbsh.
2249012345678910 . /  z h l j k w (102)0.810.820.830.840.850.860.87 3 u h f l v l r q    reuters 0.10.20.30.40.50.60.70.80.91.0 w h p s h u d w x u h 0.810.820.830.840.850.860.87 3 u h f l v l r q    reuters 012345678910 . /  z h l j k w (102)0.730.740.750.760.770.78 3 u h f l v l r q    tmc0.10.20.30.40.50.60.70.80.91.0 w h p s h u d w x u h 0.730.740.750.760.770.78 3 u h f l v l r q    tmc012345678910 . /  z h l j k w (102)0.600.620.640.660.68 3 u h f l v l r q    20newsgroups0.10.20.30.40.50.60.70.80.91.0 w h p s h u d w x u h 0.610.620.630.640.650.660.670.68 3 u h f l v l r q    20newsgroups