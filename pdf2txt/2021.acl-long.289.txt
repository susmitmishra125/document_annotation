structural guidance for transformer language models.
peng qian1 tahira naseem2 roger levy1 ram´on fernandez astudillo2.
1 department of brain and cognitive sciences, mit.
2 ibm research.
pqian@mit.edu.
rplevy@mit.edu.
tnaseem@us.ibm.comramon.astudillo@ibm.com.
abstract.
in transformer.
transformer-based language models pre-trained on large amounts of text data haveproven remarkably successfulin learninggeneric transferable linguistic representations.
here we study whether structural guidanceleads to more human-like systematic linguisticgeneralizationlanguagemodels without resorting to pre-training onvery large amounts of data.
we explore twogeneral ideas.
the “generative parsing” ideajointly models the incremental parse andword sequence as part of the same sequencemodeling task.
the “structural scaffold” ideaguides the language model’s representationvia additional structure loss that separatelypredicts the incremental constituency parse.
we train the proposed models along with avanilla transformer language model baselineon a 14 million-token and a 46 million-tokensubset of the bllip dataset, and evaluatemodels’perfor-mances on sg test suites and sized blimp.
experiment results across two benchmarkssuggest converging evidence that generativestructural supervisions can induce more robustand humanlike linguistic generalization intransformertheneed for data intensive pre-training..language models without.
generalization.
syntactic.
1.introduction.
pre-trained transformer architectures have led tohuge progress in building more human-like lan-guage processing systems (radford et al.
; devlinet al., 2019; brown et al., 2020, among others).
these models achieve impressive perplexity resultson language modelling datasets, perform well ongrammatical judgments (warstadt et al., 2020), andprovide useful linguistic representations that ben-eﬁt a wide range of downstream tasks.
probinganalyses also suggest that these models learn to im-plicitly encode syntactic information (hewitt and.
manning, 2019; clark et al., 2019) that may sup-port better linguistic generalization than recurrentneural network architectures (rnns)..however, the transformer architecture (vaswaniet al., 2017) is an interesting subject of study be-yond its success in transfer-learning settings.
trans-former models lack the inductive biases of rnns.
rather than maintaining vector-valued state andupdating it in a recurrent manner, auto-regressivetransformer models encode all past decisions si-multaneously at each inference step, thanks to aself-attention mechanism.
the only notion of se-quence order is also given by position embeddingssummed to content embeddings in both input andauto-regressive signals..previous works have shown the advantage ofstructural supervision in rnns in learning to main-tain syntactic states and non-local dependencies(kuncoro et al., 2018; wilcox et al., 2019; futrellet al., 2019).
it remains an open question whethertransformer language models can similarly beneﬁtfrom generative structural supervision, and whatform of structural supervision would more effec-tively induce human-like syntactic generalization.
this work hypothesizes that the transformer lan-guage model may beneﬁt from explicit generativestructural supervision to systematically generalizesyntactic knowledge.
here we explore two ma-jor classes of structural guidance for transformerlanguage models based on joint modeling of lan-guage and constituency parses.
the “generativeparsing as language modeling” approach builds atransformer-parameterized model to learn to pre-dict actions that incrementally build constituencytrees along with terminal words, following priorwork on rnns (dyer et al., 2016; choe and char-niak, 2016).
the “structural scaffolding” approachfollows the general idea of regularizing hidden rep-resentation through multi-task learning objective,with prior success in various nlp tasks (zhang.
s.y0:1.y1:2.y2:3.y3:4.np.
vp.
(cid:104)bos(cid:105) nt(s) nt(np) the birds reduce nt(vp).
sang nt(advp).
· · ·.
w0.
w1.
w2.
w3.
the.
birds.
sang advp.
y0:1.y1:2.y2:3.
(cid:104)pad(cid:105).
y0:1.y1:2.w1.
w2.
w3.
nt(s) nt(np) the.
birds reduce.
w1.
w2.
w3.
w1.
w2.
w3.
w0.
w1.
w2.
(cid:104)bos(cid:105) nt(s) nt(np) the.
birds.
w0.
w1.
w2.
w0.
w1.
w2.
(a) vanilla language model.
(b) parsing as language modelling.
(c) language models with structural scaffold.
figure 1: top: illustration of a partial constituency tree and corresponding transitions.
bottom: unidirectionaltransformer language model (a) without explicit structural supervision, (b) for modelling generative action parsingsequence, and (c) with structural scaffold for predicting the local incremental parsing state..and weiss, 2016; søgaard and goldberg, 2016;swayamdipta et al., 2018)..we test these two approaches on two subsets ofthe bllip dataset (charniak et al., 2000) and evalu-ate models’ syntactic generalization performanceson sg test suites (hu et al., 2020) and a sam-pled subset of the blimp benchmark (warstadtet al., 2020).
we show evidence that generativestructural supervision indeed induces more robustand human-like linguistic generalization in trans-former language models and explore the differenttrade-offs involved in the presented methods..2 models.
here we explore joint modelling of structures andwords parametrized with transformers by consid-ering both a sentence w and its constituency parsey and modeling the joint distribution p (w, y )..2.1 generative parsing as language.
modeling.
a language model can be described formally as aprobability distribution over strings of a languagew1, · · · , wt , usually left-to-right factored..p(w ) = p(w1, · · · , wt ) =.
p(wt | w<t) (1).
t(cid:89).
t=1.
there are many possible approaches that can com-bine both language modeling and syntax model-ing tasks.
as long as both tasks share some ofthe parameters they can be considered a case ofmulti-task learning (caruana, 1997).
of interest.
here is the model proposed in recurrent neuralnetwork grammars (rnngs; dyer et al., 2016)and parsing as language model (lstm-lm; choeand charniak, 2016).
both approaches model thejoint distribution of words w and constituency treecomponents y as.
r(cid:89).
t=1.
p(y, w ) = p(a1, · · · , ar) =.
p(at | a<t) (2).
where at are transitions of a state machine thatgenerates both the sentence and the tree.
thesetransitions are similar to the well-established transi-tion sets used for transition-based parsing (earley,1970) but adapted to generate both text and parsesimultaneously.
for the reminder of this work, wewill consider each at to be integer valued and in-dexing a dictionary of transitions.
a transition acan be a word w or a transition action that gener-ates a component of the constituency tree y. theactions include non-terminal symbols that open andlabel a new constituent with the label x, indicatedas nt(x), or a reduce action closing the closestopen constituent.
an example of a partial parse treeand transitions can be found at the top of figure 1.rnng and lstm-lm parametrize the same fac-torization in equation 2 in different ways.
rnngutilizes stack-lstms, which allow it to dynami-cally create representations for partial tree compo-nents by composition.
the lstm-lm, however,uses a ﬂat parametrization treating the transitionsas a sequence in a conventional language modellearnt with an lstm (hochreiter and schmidhu-ber, 1997).
it should also be noted that the lstm-.
lm is designed as a parser, while rnng is alsoused as a language model.
in order to derive a lan-guage model from a joint model, it is is necessaryto marginalize over all possible parse trees.
p(w ) =.
p(y, w ).
(3).
(cid:88).
y ∈y(w ).
which is an intractable problem since there is anexponentially large number of possible trees.
theoriginal rnng work (dyer et al., 2016) proposesan approximate solution based on importance sam-pling.
in this work we use the word-synchronousbeam search approximation introduced in sternet al.
(2017)..the marginalized likelihood language model inequation 3 is desirable because it makes no statis-tical independence assumption between languageand syntax and shares all parameters across bothtasks, with the exception of action speciﬁc embed-dings.
particularly relevant for this work is the factthat both word and non-word transitions are pre-dicted as language model output indiscriminatelyand are available at each prediction step through itshistory a<t..in this work we propose to parametrize eq 2with a transformer language model (vaswani et al.,2017).
this is equivalent to the ﬂat parametrizationof the lstm-lm but using a transformer languagemodel instead.
unlike lstm-lm, which is a pars-ing model, we derive from it a language model bymarginalization as in the rnng.
a transformerlanguage model can be succinctly described as aneural network of vertically stacked layers wherethe m-th layer is given by.
<t = ffmhm.
o ·.
(4).
.
.
.
.
.
.
amam.
1 (hm−1<t2 (hm−1<t· · ·n (hm−1<t.
am.)
).
).
...
here hm−1<t ∈ rh×t is the output of the previousdecoder layer for all previous predictions of themodel at time step t and h is the size of the hid-den vector.
the input to the ﬁrst layer i.e.
h0<tare the embeddings of all previous transitions a<tconcatenated with a start symbol.
each embeddingis the sum of both a content embedding, dictionaryvector that is being indexed, and a position embed-ding that encodes the absolute or relative positionof each action in the sequence..ffm() is a feed-forward layer, am.
n ()are multiple self-attention heads and o ∈ rh×h.
1 () · · · am.
is a matrix multiplication performed on the con-catenated output of the attention heads.
both thefeed-forward and the projection of n attentionheads through o are wrapped around with residual,dropout and layer normalization operations that arehere removed for clarity..each attention head comprises a simple inner.
product attention mechanism.
am.
n (hm−1<t.)
= v msoftmax(cid:0)(km.
n · hm−1·<tn · hm−1)t ·qm<t.
n · hm−1.
<t + m(cid:1)(5).
n , qm.
n , km.
n ∈ rh/n ×h are value, keywhere v mand query projection matrices respectively and thesoftmax operation is normalized over columns tosum to one.
the matrix m ∈ {−∞, 0}t×t is usedto prevent the model from attending to future statesduring training, enabling efﬁcient parallelization.
it is displayed here due to its relevance for the nextsection..similarly to other models, to derive a distributionover all possible transitions, including words, non-terminal symbols and the reduce operation, wecan use a softmax together with an inner product.
p(at | a<t) = softmax(ew ∪y · hm.
<t)at.
(6).
where ew ∪y are the embeddings for the joint vo-cabulary of words, non-terminals and reducetransitions.
henceforth, we refer to this model asparsing as language model, or plm for short.
unlike lstms or the rnng, the transformerhas direct access to all past decisions through self-attention and relies on position embeddings to en-code word order.
thus, in principle, there is nostructural bias for the model to favor past deci-sions that are close in time to inform current pre-diction.
on one hand, this potential ability to uselong distance information can enable a less local,more human like processing of language, but onthe other hand, it can also result in an additionallearning burden, especially if there is not sufﬁcientlearning data available.
also worth noting for theexperiments proposed here is that the total num-ber of parameters of a typical transformer greatlyexceeds that of an lstm or a rnng model..2.2.incorporating rnng-like characteristics.
as previously mentioned, unlike any of the othermodels, the rnng is able to create partial tree rep-resentations by composition using stack-lstms..buffer head.
s.np.
the birds.
(cid:104)bos(cid:105) nt(s) nt(np) the.
birds.
stack head.
(cid:104)bos(cid:105) nt(s) nt(np) the.
birds.
figure 2: illustration of how the generated incrementalconstituency parse is used to inform attention patternsin the structure-guided attention heads..this changes the rnng model structure dynami-cally as a function of the partial parse, a very de-sirable property to derive syntax-aware represen-tations.
moreover, the fact that recurrent neuralnetworks such as lstms summarize all informa-tion about previous time steps on two hidden vec-tors, creates a bottleneck that forces the model tofocus on the local state.
this is a situation where asyntax-aware representation can provide additionalvalue by enabling the local state to better encom-pass past structures.
we conjecture that a similarlyconstrained local state might beneﬁt transformermodels in learning linguistic regularities, especiallyin a limited training data scenario..in an attempt to capture a similar effect in thetransformer, we explore here the idea of maskingsome attention heads to reﬂect the parser state asin the stack-transformer (astudillo et al., 2020).
in the stack-transformer, two attention heads arespecialized to attend only to the contents of bufferand stack respectively for dependency and seman-tic parsing tasks.
here we choose to specializetwo heads as well for each layer in equation 4, asdepicted in fig.
2. one attention head attends tothe contents of the last open constituent whereasanother head attends all other past decisions notinvolving that constituent.
the rest of the heads areleft free as in the original transformer architecture.
to constrain the attention heads, we only need toalter the mask m in equation 5 to depend on headindex n and past actions mn(a<t), which resultsin a negligible computation overhead..this hard masking makes the model structurechange dynamically depending on the partial parseand it forces some heads to focus on the local syn-.
tactic state.
nevertheless, unlike the rnng, it doesnot create new representations of partial parses thatcan be composed in a recurrent manner at each timestep, and some attention heads can still operate un-restricted.
we hypothesize that structure-aware at-tention mechanism may still help the model achievebetter generalization.
the symbolic representationinduces a strong inductive bias to how the modelshould use the structure that it generates on the ﬂy.
we henceforth refer to this model plm-mask..2.3 scaffolding by learning to predict local.
parse states.
given the strong coupling between the tasks, themarginal likelihood transformer language modelof the previous section can be expected to bestrongly inﬂuenced by the additional syntax predic-tion task.
this comes however at a big cost.
first,sequences combine both words and non-terminaland reduce transitions, yielding longer sentencesthan those of a normal language model r > t .
furthermore the approximated marginalization iscomputationally intensive and also introduces anapproximation error..one well-established regime that allows jointmodeling of tasks at a low complexity is that of thesyntactic scaffold (zhang and weiss, 2016; søgaardand goldberg, 2016; swayamdipta et al., 2018).
scaffolding adds an additional structure predictiontask at one of the layers of the model as a separatelayer and only during training.
this is a minimallyintrusive change since it just branches some hiddenvector of the network and computes an additionalloss.
it also has no inﬂuence on test runtime andavoids expensive steps such as marginalization..however, applying the idea of syntactic scaffold-ing to our present scenario poses one difﬁculty.
ifwe use a standard language model predicting wordsw and predict the non-word symbols y separately,we face the problem that the two sequences havedifferent lengths.
to overcome this in a straight-forward way, we predict the n-gram of non-wordactions yt:t+n(t) corresponding to the partial parsesynchronous with step t when we predict word wt.
we use a secondary softmax layer for this actionn-gram prediction..p(yt:t+n | y<t) = softmax(ey ∗.
· hm.
<t)yt:t+n.
(7).
here ey ∗is the vocabulary of all transition n-grams excluding words found in the train corpusplus a blank symbol.
note that since scaffolding.
operates only at train time, we do not need to worryabout generalization of these n-grams to test time.
the models are thus trained to minimize the loss.
function − log p(y, w ) where.
p(y, w ).
= (cid:81)t.t=1 p(wt | w<t)t=1 p(yt:t+n(t) | w<t).
+ (cid:81)t.(8).
the scaffold can be set so that the synchronousnon-word action n-grams yt:t+n(t) are predicted ei-ther before (figure 1c, left) or after (figure 1c,right) producing wt.
we considered both vari-ants in our experiments to empirically assess theirimpact on performance.
we refer to this modelas transformer language model with syntacticscaffold, or sclm in short, and its two versionssclm-past and sclm-next, for past and next n-gram prediction..3 experiments.
3.1 model training.
all models, including the baseline vanilla languagemodels (lm in short), the syntactic scaffold mod-els, and the generative parsing models, are basedon the same architecture of gpt-2 small (radfordet al.)
(117m parameters, 12 layers, h = 768) anduse the same bpe tokenizer, but with randomlyinitialized weights.
we believe this would give usa fair comparison to pretrained gpt-2 as well, inorder to evaluate whether structural guidance helpsimprove sample efﬁciency.
we implemented all theproposed models using huggingface’s transformerpackage (wolf et al., 2020)1..as our goal here is to study whether structuralguidance helps models learn robust humanlike gen-eralization of syntactic knowledge, we train ourmodel on the bllip dataset (charniak et al., 2000),an english newswire style corpus used in hu et al.
(2020).
this makes the results here more com-parable to the results reported in previous work,especially with rnngs.
we train the proposedmodels and the baseline vanilla transformer lan-guage models on bllip-md, a 14 million-tokencorpus, and bllip-lg, a 46 million-token corpus,both of which are auto-parsed using a state-of-the-art constituency parser (kitaev and klein, 2018).
we used the parsed sentences to generate oracleparsing action sequence for plm and plm-mask.
we collected a list of word-synchronous parsing.
1code available at https://github.com/ibm/.
transformers-struct-guidance.
action sequences from the train and developmentoracle of bllip-lg and use it to parametrize theaction n-gram vocabulary of sclms trained onboth bllip-md and bllip-lg.
there are 3756action n-gram types from the corpora, includingone padding token and one blank token..all models were trained with learning rate 10−5,adamw optimizer, and minibatch of size 5. wetrained the models with multiple seeds within thecapacity of our resources, in order to accommodatepotential variance.
in total, there are three seeds oflm, four of sclm-past, four of sclm-next, threeof plm, and three of plm-mask for bllip-md,and the same number of seeds of each model typefor bllip-lg.
models were trained until conver-gence, as suggested by the loss of the developmentset during training..3.2 targeted syntactic evaluation.
to assess whether a trained model systematicallygeneralizes its syntactic knowledge, we employ tar-geted syntactic evaluation paradigm (marvin andlinzen, 2018).
speciﬁcally, we measure models’performance on two held-out test datasets, a collec-tion of syntactic generalization test suites from huet al.
(2020) and blimp benchmark from warstadtet al.
(2020).
these two datasets cover a wide rangeof english syntactic phenomena..tests from hu et al.
(2020), which we referas sg test suites, consist of hand-designed testsuites for evaluating ﬁne-grained syntactic gener-alization in incremental processing of a linguisticinput.
the general method is to compare mod-els’ surprisals p(continuation|preﬁx) of grammati-cal and ungrammatical continuations given certainsentence preﬁxes.
we report the accuracy averagedacross sg test suites.
blimp benchmark featuresminimal pairs of a grammatical sentence w andan ungrammatical counterpart w ∗.
to evaluate amodel on these minimal pairs, one simply com-pares the likelihood of w and w ∗ assigned by themodel..as is implied by the evaluation methods, weneed to marginalize out the structure variables forplm or plm-mask models in order to estimatethe surprisal of a continuation, given a sentencepreﬁx or the likelihood of a complete sentence.
we follow similar setup as in futrell et al.
(2019);wilcox et al.
(2019) applying word-synchronousbeam search (stern et al., 2017) to ﬁnd a list yk ofk incremental parses given a sentence preﬁx w<t..also include the results for the rnngs taken fromhu et al.
(2020).
rnng lags behind all trans-former models by a large margin in average scores.
we also notice that among different forms of struc-tural guidance, generative parsing as language mod-eling is the most effective in improving syntac-tic generalization performance against the baselinetransformer language models.
we didn’t observeconsistent beneﬁts of adding dynamic maskingmechanism to plm.
while scaffolding approachslightly improves vanilla transformer languagemodels, it still falls behind the best performanceof the model trained with generative parsing.
wehypothesize that our scaffold did not fully exploitthe compositional structure in the local parses bymodelling each action n-gram as a distinct type,while the generative parsing models only predictactions in a relatively small set of non-terminal ac-tion space, which might make it easier for plm andplm-mask to learn compositional generalization.
we leave it for future work to design new scaffoldsthat can take advantage of the combinatorial natureof syntactic structure..for completeness, we also ran the pre-trainedgpt-2 model on the syntactic suites.
this yieldeda score of 0.808 on the sg test suites and 0.827 onblimp-10% for the small version of pre-trainedgpt-2.
among models trained on bllip-lg, theaverage accuracy score on the sg test suites is0.723 for plms, 0.748 for plm-masks, and 0.665for lms.
similar trend is observed on blimp-10%as well, where among models trained on bllip-lg the average accuracy is 0.751 for plms, 0.753for plm-masks, and 0.708 for lms.
the pro-posed plm method is able to close the gap be-tween gpt-2 small and the same model trainedwith bllip-lg by about half, while the improve-ment for blimp is more modest but still signi-ﬁcative.
it remains an open question whether scal-ing syntactic supervision to a larger dataset thanbllip-lg would bring the generalization perfor-mance of plm models closer to that of the pre-trained gpt-2 model..3.2.1 relationship between perplexity and.
syntactic generalization performance.
we compare perplexity on the bllip held-out testset against syntactic generalization performancein figure 4. perplexities of plm and plm-maskmodels are computed setting the parse tree equalto the gold parse in equation 3 to approximate thelikelihood.
note that, unlike hu et al.
(2020), all.
figure 3: comparing models’ overall accuracy acrosstest suites from sg test suites (top) and blimp-10%(bottom).
rnng performances are from hu et al.
(2020)..we then sum the joint probability p(w<t, y<t) overthe list of incremental parses given by the model toapproximate the likelihood of p(w<t).
we set theparse beam size to 100, word-synchronous beamsize k as 10, and fast track size of 5. since thesearch process can be computationally intensive,the large number of items in blimp benchmarkposes a computational challenge.
we thereforeselect the ﬁrst 10% out of the 1000 items in eachof the 67 tests of blimp benchmark.
we reportthe accuracy over the 100 items and refer to thisdown-sized blimp benchmark as blimp-10%..we compare models’ performance on the sgtest suites and blimp-10% in figure 3. each barshows a model’s performance averaged across mul-tiple seeds on a given benchmark, with each dotplotting the accuracy of a speciﬁc seed.
overall,syntactic generalization performance improves asthe training data size increases from bllip-md(14 million tokens) to bllip-lg (42 million to-kens).
models with structural guidance achievehigher accuracy than the vanilla transformer lan-guage model trained on the same set of raw textdata without explicit structural information.
we.
bllip-mdbllip-lg0.550.600.650.700.750.80accuracymodel performance on sg test suitesrnnglmsclm-pastsclm-nextplmplm-maskgpt-2bllip-mdbllip-lg0.550.600.650.700.750.80accuracymodel performance on blimp-10% test suiteslmsclm-pastsclm-nextplmplm-maskgpt-2parsing can robustly improve learning of subject-verb agreement and npi licensing, and helps themodel to better capture incremental processing phe-nomenon such as garden-path effects, but seemsto slightly hurt the performance on gross syntacticstate.
while overall the rnng shows a poor per-formance this is mostly due to its very low scoresfor licensing suites.
excluding these suites onlythe rnng shows a performance close to the plmmodel, even outperforming it clearly for the grosssyntactic state suites.
in this category and bindingplm variants seem inferior to all other models..4 related work.
multitask learning (caruana, 1997) has been ap-plied to a variety of nlp tasks with traditionalmodeling approaches (miller et al., 2000; suttonand mccallum, 2005; sutton et al., 2007) as well asmore recent neural models (collobert et al., 2011;li et al., 2020a).
a recurring theme has been theuse of structure in the form of syntactic trees tobeneﬁt other nlp tasks.
among the early worksexploring this direction, punyakanok et al.
(2008)showed that syntactic parses can beneﬁt semanticrole labeling (srl).
poon and domingos (2009)extended this idea to induce ﬁrst-order logic repre-sentation in a unsupervised fashion, by clusteringthe dependency structures.
in both cases syntaxforms part of a pipeline and is not strictly supervi-sion for the end task..this trend continued with the rise of neural mod-els.
collobert et al.
(2011) improved deep convolu-tion neural network for syntactic chunking modelswith additional pos supervision.
zhang and weiss(2016); søgaard and goldberg (2016) observe thebeneﬁts of pos supervision at different depths of aneural network model with impact on dependencyparsing, tagging and ccg super tagging perfor-mance.
he et al.
(2019) perform a syntax-basedpruning of semantic roles, showing beneﬁts in amultilingual setting.
more recently, sachan et al.
(2020) incorporate a syntactic graph recurrent neu-ral network into bert models for better semanticrole labeling.
however, their method shows littleor no beneﬁt of syntax modeling for named entityrecognition and relation linking task.
neural ma-chine translation (chen et al., 2018) and text gen-eration (li et al., 2020a) have also been shown tobeneﬁt from syntactic modeling.
in a recent work,li et al.
(2020b) use syntactic modeling in bertbased transformers to achieve performance gains.
figure 4: comparison between model perplexity onbllip test data and syntactic generalization perfor-mance on sg test suites (top) and blimp-10% (bot-tom)..our models use the same bpe vocabulary and wordtokenization from gpt-2.
the only exception arethe additional parsing actions in the vocabulary y.from figure 4, both perplexity and syntactic gen-eralization performance improve with dataset size.
however, for both training dataset sizes, we see thatstructural guidance can improve syntactic general-ization.
plm models consistently perform betterthan vanilla models.
while all models achieve verysimilar perplexity results after being trained on aspeciﬁc dataset, their syntactic generalization per-formances differ dramatically..3.2.2 effect of structural guidance on.
learning speciﬁc syntactic structures.
in addition to comparing model’s aggregated per-formances, we also compare their generalizationperformances in the clustered subsets of tests in sgtest suites and blimp-10%.
these subsets con-sist of several related tests that target speciﬁc typeof syntactic phenomenon, such as npi licensing,subject-verb agreement, ﬁller-gap dependencies,etc.
we also include the results for the rnngstaken from hu et al.
(2020)..results in figure 5 show converging evidencethat structural guidance in the form of generative.
5060word-level perplexity0.6250.6500.6750.7000.7250.750sg accuracymodellmsclm-pastsclm-nextplmplm-maskcorpusbllip-mdbllip-lgcorpusbllip-mdbllip-lg5060word-level perplexity0.680.700.720.740.76blimp-10% accuracymodellmsclm-pastsclm-nextplmplm-maskcorpusbllip-mdbllip-lgcorpusbllip-mdbllip-lgfigure 5: model performance comparison by speciﬁc linguistic phenomena clustered in sg test suites (top) andblimp-10% (bottom).
rnng performances are from hu et al.
(2020)..on several text classiﬁcation benchmarks.
otherworks have found that structural supervision in theform of intermediate ﬁne-tuning (e.g., on ccgsuper tagging) is not helpful or even harmful (pruk-sachatkun et al., 2020; warstadt et al., 2019)..the focus of our work is on gauging the impactof joint modeling on syntactic generalization perfor-mance.
in this direction, the work of swayamdiptaet al.
(2018) is close to the scaffolding version ofour model.
they predict multiple labels, extractedfrom syntactic information, as auxiliary task andshow positive effects on shallow semantic parsingand co-reference resolution.
we use however a sin-gle feature, constituency parsing n-gram, which iscloser to prior work relying on part-of-speech in-formation.
in addition, we explore impact of usingpreceding structure as feature vs postceding struc-ture, which as shown plays a role in the learningprocess..in terms of modeling objective and syntactic rep-.
resentations, our method is closest to the works ofchoe and charniak (2016); dyer et al.
(2016) thatjointly model syntax and language.
a more recentwork from peng et al.
(2019) uses rational neuralnetworks language model that can derive binaryunlabeled constituents from attention weights andcan supervise the attention to attain a structuralinductive bias.
the proposed models show lowerlanguage modeling perplexity compared to theirstructure agnostic counterparts.
we also extendhere the idea of syntax-aware language modelingto transformer-based language models..finally, our approach relates to the other worksthat propose ways of incorporating structural in-formation into transformer-based models.
thisincludes the use of dependency or tree structure forconstraining self-attention patterns (strubell et al.,2018; wang et al., 2019; zhang et al., 2020), guid-ing cross-attention (chen et al., 2018; astudilloet al., 2020), modelling syntactic distance (du et al.,.
0.050.150.250.350.450.550.65accuracylicensing (10 suites)0.400.450.500.550.600.650.700.75long-distance dependencies (8 suites)0.30.40.50.60.70.8agreement (3 suites)bllip-mdbllip-lg0.550.600.650.700.750.800.85accuracygarden-path effects (6 suites)bllip-mdbllip-lg0.700.750.800.850.900.95gross syntactic state (4 suites)bllip-mdbllip-lg0.500.550.600.650.700.750.800.85center embedding (2 suites)model performance on specific clusters of sg test suitesrnnglmsclm-pastsclm-nextplmplm-mask0.500.550.600.650.700.750.800.85accuracyanaphor agreement (2 suites)0.550.600.650.70argument structure (9 suites)0.600.650.700.75binding (7 suites)0.600.650.700.75control/raising (5 suites)0.700.750.800.85accuracydeterminer-noun agreement (8 suites)0.500.550.600.650.700.750.80ellipsis (2 suites)0.600.650.700.75filler gap (7 suites)0.550.600.650.700.750.800.85irregular forms (2 suites)bllip-mdbllip-lg0.400.450.500.550.60accuracyisland effects (8 suites)bllip-mdbllip-lg0.500.550.600.650.700.750.800.85npi licensing (7 suites)bllip-mdbllip-lg0.550.600.650.700.75quantifiers (4 suites)bllip-mdbllip-lg0.700.750.800.85subject-verb agreement (6 suites)model performance on specific clusters of blimp-10% test suiteslmsclm-pastsclm-nextplmplm-mask2020), using syntactic information to guide thecomputation ﬂow in the model (shen et al., 2021),or through knowledge distillation (kuncoro et al.,2020).
our structured masking in parsing as lan-guage modeling approach is close in spirit to themethods that modify attention mechanism accord-ing to syntactic connections (astudillo et al., 2020);this work, however, primarily aims to study theimpact of structural guidance on syntactic general-ization.
therefore, we resort to simpler methods ofincorporating structure to minimize the impact ofmodeling intricacies..5 conclusion.
our work explores two forms of syntactic super-vision as structural guidance for transformer lan-guage models.
experiments suggest that generativeparsing approach can effectively improve system-atic generalization of learned syntactic knowledgein small training data regime, while a naive syntac-tic scaffold approach does not improve the baselineto the same extent despite reduced computationcost at inference time.
future work may explorealternative structural guidance strategies that com-bine the best of both approaches..acknowledgments.
the authors would like to thank the anonymousreviewers for their helpful comments.
this workwas supported by the mit-ibm watson ai lab..references.
ram´on fernandez astudillo, miguel ballesteros,tahira naseem, austin blodgett, and radu flo-rian.
2020. transition-based parsing with stack-transformers.
page 1001–1007..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..rich caruana.
1997. multitask learning.
machine.
learning, 28(1):41–75..eugene charniak, don blaheta, niyu ge, keith hall,john hale, and mark johnson.
2000. bllip 1987-89wsj corpus release 1. linguistic data consortium,philadelphia, 36..kehai chen, rui wang, masao utiyama, eiichirosumita, and tiejun zhao.
2018. syntax-directedin pro-attention for neural machine translation.
ceedings of the aaai conference on artiﬁcial intel-ligence, volume 32..do kook choe and eugene charniak.
2016. parsingas language modeling.
in proceedings of the 2016conference on empirical methods in natural lan-guage processing, pages 2331–2336, austin, texas.
association for computational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..ronan collobert, jason weston, l´eon bottou, michaelkarlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) fromjournal of machine learning research,scratch.
12(article):2493–2537..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..wenyu du, zhouhan lin, yikang shen, timothy j.o’donnell, yoshua bengio, and yue zhang.
2020.exploiting syntactic structure for better languagein pro-modeling: a syntactic distance approach.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6611–6628, online.
association for computational lin-guistics..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a. smith.
2016. recurrent neural networkgrammars.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 199–209, san diego, california.
association for computational linguistics..jay earley.
1970. an efﬁcient context-free parsing al-gorithm.
communications of the acm, 13(2):94–102..richard futrell, ethan wilcox, takashi morita, pengqian, miguel ballesteros, and roger levy.
2019.neural language models as psycholinguistic sub-jects: representations of syntactic state.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 32–42, minneapolis,minnesota.
association for computational linguis-tics..shexia he, zuchao li, and hai zhao.
2019. syntax-aware multilingual semantic role labeling.
arxivpreprint arxiv:1909.00310..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..hao peng, roy schwartz, and noah a. smith.
2019.inpalm: a hybrid parser and language model.
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3644–3651, hong kong, china.
association for computa-tional linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger levy.
2020. a systematic assessmentof syntactic generalization in neural language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 1725–1744, online.
association for compu-tational linguistics..nikita kitaev and dan klein.
2018. constituencyin proceed-parsing with a self-attentive encoder.
ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), melbourne, australia.
association for com-putational linguistics..adhiguna kuncoro, chris dyer, john hale, dani yo-gatama, stephen clark, and phil blunsom.
2018.lstms can learn syntax-sensitive dependencieswell, but modeling structure makes them better.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1426–1436, melbourne, aus-tralia.
association for computational linguistics..adhiguna kuncoro, lingpeng kong, daniel fried,dani yogatama, laura rimell, chris dyer, and philblunsom.
2020. syntactic structure distillation pre-training for bidirectional encoders.
transactionsof the association for computational linguistics,8:776–794..yinghao li, rui feng, isaac rehg, and chao zhang.
text genera-transformer-based neuralarxiv preprint.
2020a.
tion with syntactic guidance.
arxiv:2010.01737..zhongli li, qingyu zhou, chao li, ke xu, and yunbocao.
2020b.
improving bert with syntax-aware localattention.
arxiv preprint arxiv:2012.15150..rebecca marvin and tal linzen.
2018. targeted syn-in proceed-tactic evaluation of language models.
ings of the 2018 conference on empirical methodsin natural language processing, pages 1192–1202,brussels, belgium.
association for computationallinguistics..scott miller, heidi fox, lance ramshaw, and ralphweischedel.
2000. a novel use of statistical parsingto extract information from text.
in 1st meeting ofthe north american chapter of the association forcomputational linguistics..hoifung poon and pedro domingos.
2009. unsu-in proceedings of thepervised semantic parsing.
2009 conference on empirical methods in naturallanguage processing, pages 1–10..yada pruksachatkun,.
jason phang, haokun liu,phu mon htut, xiaoyi zhang, richard yuanzhepang, clara vania, katharina kann, and samuel rbowman.
2020. intermediate-task transfer learningwith pretrained models for natural language under-arxivstanding: when and why does it work?
preprint arxiv:2005.00628..vasin punyakanok, dan roth, and wen-tau yih.
2008.the importance of syntactic parsing and inference insemantic role labeling.
computational linguistics,34(2):257–287..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
language mod-els are unsupervised multitask learners..devendra singh sachan, yuhao zhang, peng qi, andwilliam hamilton.
2020. do syntax trees help pre-arxivtrained transformers extract information?
preprint arxiv:2008.09084..yikang shen, shawn tan, alessandro sordoni, sivareddy, and aaron courville.
2021. explicitly mod-eling syntax in language models with incrementalparsing and a dynamic oracle.
in proceedings of the2021 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 1660–1672, on-line.
association for computational linguistics..anders søgaard and yoav goldberg.
2016. deep multi-task learning with low level tasks supervised at lowerin proceedings of the 54th annual meet-layers.
ing of the association for computational linguistics(volume 2: short papers), pages 231–235, berlin,germany.
association for computational linguis-tics..mitchell stern, daniel fried, and dan klein.
2017. ef-fective inference for generative neural parsing.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages1695–1700, copenhagen, denmark.
association forcomputational linguistics..patrick verga, daniel andor,emma strubell,david weiss,and andrew mccallum.
2018.linguistically-informed self-attention for semanticin proceedings of the 2018 confer-role labeling.
ence on empirical methods in natural languageprocessing, pages 5027–5038..clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..yuan zhang and david weiss.
2016..stack-propagation: improved representation learning forin proceedings of the 54th annual meet-syntax.
ing of the association for computational linguistics(volume 1: long papers), pages 1557–1566, berlin,germany.
association for computational linguis-tics..zhuosheng zhang, yuwei wu, junru zhou, sufengduan, hai zhao, and rui wang.
2020. sg-net: syn-tax guided transformer for language representation.
ieee transactions on pattern analysis and machineintelligence..charles sutton and andrew mccallum.
2005..jointparsing and semantic role labeling.
technical report,massachusetts univ amherst dept ofcomputer science..charles sutton, andrew mccallum, and khashayar ro-hanimanesh.
2007. dynamic conditional randomﬁelds: factorized probabilistic models for labelingand segmenting sequence data.
journal of machinelearning research, 8(3)..swabha swayamdipta, sam thomson, kenton lee,luke zettlemoyer, chris dyer, and noah a. smith.
2018. syntactic scaffolds for semantic structures.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 3772–3782, brussels, belgium.
associationfor computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..yaushian wang, hung-yi lee, and yun-nung chen.
2019. tree transformer: integrating tree structuresinto self-attention.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1061–1070, hong kong, china.
as-sociation for computational linguistics..alex warstadt, yu cao, ioana grosu, wei peng, ha-gen blix, yining nie, anna alsop, shikha bordia,haokun liu, alicia parrish, et al.
2019.investi-gating bert’s knowledge of language: five analysismethods with npis.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2870–2880..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of the as-sociation for computational linguistics, 8:377–392..ethan wilcox, peng qian, richard futrell, miguelballesteros, and roger levy.
2019. structural super-vision improves learning of non-local grammaticalin proceedings of the 2019 confer-dependencies.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3302–3312, minneapolis, minnesota.
association for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,.