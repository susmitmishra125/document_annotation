meta-kd: a meta knowledge distillation framework for languagemodel compression across domains.
haojie pan1,2∗, chengyu wang2∗, minghui qiu2†, yichang zhang2, yaliang li2, jun huang21 zhejiang lab2 alibaba group{haojie.phj, chengyu.wcy, minghui.qmh}@alibaba-inc.com{yichang.zyc, yaliang.li, huangjun.hj}@alibaba-inc.com.
abstract.
pre-trained language models have been ap-plied to various nlp tasks with considerableperformance gains.
however, the large modelsizes, together with the long inference time,limit the deployment of such models in real-time applications.
one line of model com-pression approaches considers knowledge dis-tillation to distill large teacher models intosmall student models.
most of these stud-ies focus on single-domain only, which ig-nores the transferable knowledge from otherdomains.
we notice that training a teacherwith transferable knowledge digested acrossdomains can achieve better generalization ca-pability to help knowledge distillation.
hencewe propose a meta-knowledge distillation(meta-kd) framework to build a meta-teachermodel that captures transferable knowledgeacross domains and passes such knowledgeto students.
speciﬁcally, we explicitly forcethe meta-teacher to capture transferable knowl-edge at both instance-level and feature-levelfrom multiple domains, and then proposea meta-distillation algorithm to learn single-domain student models with guidance from themeta-teacher.
experiments on public multi-domain nlp tasks show the effectiveness andsuperiority of the proposed meta-kd frame-work.
further, we also demonstrate the capa-bility of meta-kd in the settings where thetraining data is scarce..1.introduction.
pre-trained language models (plm) such asbert (devlin et al., 2019) and xlnet (yang et al.,2019) have achieved signiﬁcant success with thetwo-stage “pre-training and ﬁne-tuning” process.
despite the performance gain achieved in variousnlp tasks, the large number of model parameters.
∗h.
pan and c. wang contributed equally to this work.
† m. qiu is the corresponding author..figure 1: a motivation example of academic learning.
a physics student may learn physics equations betterwith a powerful all-purpose teacher..and the long inference time have become the bot-tleneck for plms to be deployed in real-time ap-plications, especially on mobile devices (jiao et al.,2019; sun et al., 2020; iandola et al., 2020).
thus,there are increasing needs for plms to reduce themodel size and the computational overhead whilekeeping the prediction accuracy..knowledge distillation (kd) (hinton et al.,2015) is one of the promising ways to distill theknowledge from a large “teacher” model to a small“student” model.
recent studies show that kd canbe applied to compress plms with acceptable per-formance loss (sanh et al., 2019; sun et al., 2019b;jiao et al., 2019; turc et al., 2019; chen et al.,2020a).
however, those methods mainly focuson single-domain kd.
hence, student models canonly learn from their in-domain teachers, payinglittle attention to acquiring knowledge from otherdomains.
it has been shown that it is beneﬁcialto consider cross-domain information for kd, byeither training a teacher using cross-domain dataor multiple teachers from multiple domains (youet al., 2017; liu et al., 2019; yang et al., 2020;.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3026–3036august1–6,2021.©2021associationforcomputationallinguistics3026(a) learning from an in-domain teacher.
(b) learning from multiple teachers of varied domains.
(c) learning from meta-teacher with multi-domain knowledge.physicsteacherphysicsteacherphysicsequationphysicsstudentmathteacherphysicsequationall-purposeteacherphysicsequationphysicsstudentphysicsstudentpeng et al., 2020).
consider an academic scenarioin figure 1. a typical way for a physics studentto learn physics equations is to directly learn fromhis/her physics teacher.
if we have a math teacherto teach him/her basic knowledge of equations, thestudent can obtain a better understanding of physicsequations.
this “knowledge transfer” technique inkd has been proved efﬁcient only when two do-mains are close to each other (hu et al., 2019).
inreality, however, it is highly risky as teachers ofother domains may pass non-transferable knowl-edge to the student model, which is irrelevant tothe current domain and hence harms the overallperformance (tan et al., 2017; wang et al., 2020).
besides, current studies ﬁnd multi-task ﬁne-tuningof bert does not necessarily yield better perfor-mance across all the tasks (sun et al., 2019a)..to address these issues, we leverage the idea ofmeta-learning to capture transferable knowledgeacross domains, as recent studies have shown thatmeta-learning can improve the model generaliza-tion ability across domains (finn et al., 2017; javedand white, 2019; yin, 2020; ye et al., 2020).
wefurther notice that meta-knowledge is also help-ful for cross-domain kd.
re-consider the examplein figure 1. if we have an “all-purpose teacher”(i.e., the meta-teacher) who has the knowledge ofboth physics principles and mathematical equations(i.e., the general knowledge of the two courses), thestudent may learn physics equations better with theteacher, compared to the other two cases.
hence, itis necessary to train an “all-purpose teacher” modelfor domain-speciﬁc student models to learn..in this paper, we propose the meta-knowledgedistillation (meta-kd) framework, which facili-ties cross-domain kd.
generally speaking, meta-kd consists of two parts, meta-teacher learningand meta-distillation.
different from the k-wayn-shot problems addressed in traditional meta-learning (vanschoren, 2018), we propose to traina “meta-learner” as the meta-teacher, which learnsthe transferable knowledge across domains so thatit can ﬁt new domains easily.
the meta-teacheris jointly trained with multi-domain datasets toacquire the instance-level and feature-level meta-knowledge.
for each domain, the student modellearns to solve the task over a domain-speciﬁcdataset with guidance from the meta-teacher.
toimprove the student’s distillation ability, the meta-distillation module minimizes the distillation lossfrom both intermediate layers, output layers, and.
transferable knowledge, combined with domain-expertise weighting techniques..to verify the effectiveness of meta-kd, weconduct extensive experiments on two nlp tasksacross multiple domains, namely natural languageinference (williams et al., 2018) and sentimentanalysis (blitzer et al., 2007).
experimental re-sults show the effectiveness and superiority of theproposed meta-kd framework.
moreover, we ﬁndour method performs well especially when i) thein-domain dataset is very small or ii) there is noin-domain dataset during the training of the meta-teacher.
in summary, the contributions of this studycan be concluded as follows:.
• to the best of our knowledge, this work is theﬁrst to explore the idea of meta-teacher learn-ing for plm compression across domains..• we propose the meta-kd framework to ad-dress the task.
in meta-kd, the meta-teacherdigests transferable knowledge across do-mains, and selectively passes the knowledgeto student models with different domain ex-pertise degrees..• we conduct extensive experiments to demon-strate the superiority of meta-kd and alsoexplore the capability of this framework in thesettings where the training data is scarce..the rest of this paper is summarized as follows.
section 2 describes the related work.
the detailedtechniques of the meta-kd framework are pre-sented in section 3. the experiments are reportedin section 4. finally, we conclude our work anddiscuss the future work in section 5.
1.
2 related work.
our study is close to the following three lines ofstudies, introduced below..2.1 knowledge distillation (kd).
kd was ﬁrst proposed by (hinton et al., 2015), aim-ing to transfer knowledge from an ensemble or alarge model into a smaller, distilled model.
most ofthe kd methods focus on utilizing either the darkknowledge, i.e., predicted outputs (hinton et al.,2015; chen et al., 2020b; furlanello et al., 2018;you et al., 2017) or hints, i.e., the intermediate.
1the experimental code can be found in https:.
//github.com/alibaba/easytransfer/tree/master/scripts/metakd..3027representations (romero et al., 2015; yim et al.,2017; you et al., 2017) or the relations betweenlayers (yim et al., 2017; tarvainen and valpola,2017) of teacher models.
you et al.
(2017) alsoﬁnd that multiple teacher networks together canprovide comprehensive guidance that is beneﬁcialfor training the student network.
ruder et al.
(2017)show that multiple expert teachers can improve theperformances of sentiment analysis on unseen do-mains.
tan et al.
(2019) apply the multiple-teachersframework in kd to build a state-of-the-art mul-tilingual machine translation system.
feng et al.
(2021) considers to build a model to automaticallyaugment data for kd.
our work is one of the ﬁrstattempts to learn a meta-teacher model that digesttransferable knowledge from multiple domains tobeneﬁt kd on the target domain..2.2 plm compression.
due to the massive number of parameters in plms,it is highly necessary to compress plms for ap-plication deployment.
previous approaches oncompressing plms such as bert (devlin et al.,2019) include kd (hinton et al., 2015), param-eter sharing (ullrich et al., 2017), pruning (hanet al., 2015) and quantization (gong et al., 2014).
in this work, we mainly focus on kd for plms.
in the literature, tang et al.
(2019) distill bertinto bilstm networks to achieve comparable re-sults with elmo (peters et al., 2018).
chenet al.
(2021) studies cross-domain kd to facilitatecross-domain knowledge transferring.
zhao et al.
(2019) use dual distillation to reduce the vocabularysize and the embedding size.
distillbert (sanhet al., 2019) applies kd loss in the pre-trainingstage, while bert-pkd (sun et al., 2019b) distillbert into shallow transformers in the ﬁne-tuningstage.
tinybert (jiao et al., 2019) further dis-tills bert with a two-stage kd process for hiddenattention matrices and embedding matrices.
ad-abert (chen et al., 2020a) uses neural architec-ture search to adaptively ﬁnd small architectures.
our work improves the prediction accuracy of com-pressed plms by leveraging cross-domain knowl-edge, which is complementary to previous works..2.3 transfer learning and meta-learning.
tl has been proved to improve the performance onthe target domain by leveraging knowledge fromrelated source domains (pan and yang, 2010; mouet al., 2016; liu et al., 2017; yang et al., 2017).
inmost nlp tasks, the “shared-private” architecture.
is applied to learn domain-speciﬁc representationsand domain-invariant features (mou et al., 2016;liu et al., 2017; chen et al., 2018, 2019).
com-pared to tl, the goal of meta-learning is to trainmeta-learners that can adapt to a variety of differenttasks with little training data (vanschoren, 2018).
a majority of meta-learning methods for includemetric-based (snell et al., 2017; pan et al., 2019),model-based (santoro et al., 2016; bartunov et al.,2020) and model-agnostic approaches (finn et al.,2017, 2018; vuorio et al., 2019).
meta-learningcan also be applied to kd in some computer visiontasks (lopes et al., 2017; jang et al., 2019; liuet al., 2020; bai et al., 2020; li et al., 2020).
forexample, lopes et al.
(2017) record per-layer meta-data for the teacher model to reconstruct a trainingset, and then adopts a standard training procedureto obtain the student model.
in our work, we useinstance-based and feature-based meta-knowledgeacross domains for the kd process..3 the meta-kd framework.
in this section, we formally introduce the meta-kd framework.
we begin with a brief overview ofmeta-kd.
after that, the techniques are elaborated..3.1 an overview of meta-kd.
take text classiﬁcation as an example.
assumethere are k training sets, corresponding to k do-mains.
in the k-th dataset dk = {x (i)k , y(i)k }nki=1,is the i-th sample 2 and y(i)x (i)k is the class labelkof x (i)k .
nk is the total number of samples in dk.
let mk be the large plm ﬁne-tuned on dk.
giventhe k datasets, the goal of meta-kd is to obtainthe k student models s1, · · · , sk that are small insize but has similar performance compared to thek large plms, i.e., m1, · · · , mk..in general, the meta-kd framework can be di-.
vided into the following two stages:.
• meta-teacher learning: learn a meta-.
teacher m over all domains.
dk.
the.
model digests transferable knowledge fromeach domain and has better generalizationwhile supervising domain-speciﬁc students..k(cid:83)k=1.
• meta-distillation: learn k in-domain stu-dents s1, · · · , sk that perform well in their.
2x (i).
k can be a sentence, a sentence pair or any other tex-.
tual units, depending on the task inputs..3028respective domains, given only in-domain datadk and the meta-teacher m as input..during the learning process of the meta-teacher,we consider both instance-level and feature-levelinspired by prototype-transferable knowledge.
based meta-learning (snell et al., 2017; pan et al.,2019),the meta-teacher model should memo-rize more information about prototypes.
hence,we compute sample-wise prototype scores asthe instance-level transferable knowledge.
the lossof the meta-teacher is deﬁned as the sum of classi-ﬁcation loss across all k domains with prototype-based, instance-speciﬁc weighting.
besides, italso learns feature-level transferable knowledge byadding a domain-adversarial loss as an auxiliaryloss.
by these steps, the meta-teacher is more gen-eralized and digests transferable knowledge beforesupervising student models..for meta-distillation, each sample is weightedby a domain-expertise score to address the meta-teacher’s capability for this sample.
the transfer-able knowledge is also learned for the students fromthe meta-teacher.
the overall meta-distillationloss is a combination of the mean squared er-ror (mse) loss from intermediate layers of bothmodels (sun et al., 2019b; jiao et al., 2019), thesoft cross-entropy loss from output layers (hintonet al., 2015), and the transferable knowledge distil-lation loss, with instance-speciﬁc domain-expertiseweighting applied..3.2 meta-teacher learning.
k,1), h(tok(i).
k,2), .., h(tok(i).
we take bert (devlin et al., 2019) as our baselearner for text classiﬁcation due to its widefor each sample x (i)popularity.
the inputk ,k,1, tok(i)is: [cls], tok(i)k,2, · · · , [sep], wherek,n is the n-th token in x (i)tok(i)the lastk .
hidden outputs of this sequence is denoted ash[cls], h(tok(i)k,n ), whereh(tok(i)k,j) represents the last layer embedding ofthe j-th token in x (i)k , and n is the maximum se-quence length.
for simplicity, we deﬁne h(x (i)k )as the average pooling of the token embeddings,i.e., h(x (i)n=1 h(tok(i)learning instance-level transferable knowl-edge.
to select transferable instances across do-mains, we compute a prototype score t(i)k for eachsample x (i)k .
here, we treat the prototype repre-sentation for the m-th class of the k-th domain:.
k ) = (cid:80)n.k,n)..(cid:80).
k ), where d(m)p(m)k = 1|d(m)kkis the k-th training set with the m-th class label.
the prototype score t(i)k is:.
k ∈d(m).
h(x (i).
x (i).
k.|.
, h(x (i).
k )).
k =α cos(p(m)t(i)kk(k(cid:48)(cid:54)=k)(cid:88).
+ ζ.k(cid:48)=1.
cos(p(m).
k(cid:48).
, h(x (i).
k )),.
k.k(cid:48).
where cos is the cosine similarity function, α is apre-deﬁned hyper-parameter and ζ = 1−αk−1 .
wecan see that the deﬁnition of the prototype scorehere is different from previous meta-learning, aswe require that an instance x (i)should be closekto its class prototype representation in the embed-ding space (i.e., p(m)), as well as the prototype rep-resentations in out-of-domain datasets (i.e., p(m)with k(cid:48) = 1, · · · , k, k(cid:48)(cid:54)= k).
this is becausethe meta-teacher should learn more from instancesthat are prototypical across domains instead of in-domain only.
for the text classiﬁcation task, thecross-entropy loss of the meta-teacher is deﬁned us-ing the cross-entropy loss with the prototype scoreas a weight assigned to each instance.
learning feature-level transferable knowl-edge.
apart from the cross-entropy loss, we pro-pose the domain-adversarial loss to increase themeta-teacher’s ability for learning feature-leveltransferable knowledge.
for each sample x (i).
k )|-dimensional domain embedding of the true domainlabel d(i)k by mapping one-hot domain representa-tions to the embeddings, denoted as ed(x (i)k ).
asub-network is then constructed by:.
k , we ﬁrst learn an |h(x (i).
hd(x (i).
k )) = tanh((h(x (i).
k ) + ed(x (i).
k ))w + b),.
where w and b are sub-network parameters.
thedomain-adversarial loss for x (i)k.is deﬁned as:.
lda(x (i).
k ) = −.
1.k=z(i)k.· log σ(hd(x (i).
k )),.
k(cid:88).
k=1.
where σ is the k-way domain classiﬁer, and 1 isthe indicator function that returns 1 if k = z(i)k , and0 otherwise.
here, z(i)k is a false domain la-kbel of x (i)3. hence, we deliberately maximize thekprobability that the meta-teacher makes the wrong.
(cid:54)= d(i).
3for ease of implementation, we shufﬂe the domain labels.
of all instances in a mini-batch..3029figure 2: an overview of meta-distillation and the neural architecture that we adopt for knowledge distillation..predictions of domain labels.
we call hd(x (i)the transferable knowledge for x (i)insensitive to domain differences..k )) ask , which is more.
let lce(x (i).
k ) be the normal cross-entropy lossof the text classiﬁcation task.
the total loss of themeta-teacher lm t is the combination of weightedlce(x (i).
k ) and lda(x (i).
k ), shown as follows:.
lm t =.
(cid:88).
x (i).
k ∈.
k(cid:83)k=1.
dk.
k ) + γ1lda(x (i)k lce(x (i)t(i)k )(cid:80)kk=1 |dk|.
,.
where γ1 is the factor to represent how the domain-adversarial loss contributes to the overall loss..3.3 meta-distillation.
we take bert as our meta-teacher and use smallerbert models as student models.
the distillationframework is shown in figure 2. in our work, wedistill the knowledge in the meta-teacher modelconsidering the following ﬁve elements: input em-beddings, hidden states, attention matrices, outputlogits, and transferable knowledge.
the kd pro-cess of input embeddings, hidden states and atten-tion matrices follows the common practice (sunet al., 2019b; jiao et al., 2019).
recall that m andsk are the meta-teacher and the k-th student model.
let lembd(m, sk, x (i)k ), lhidn(m, sk, x (i)k ) andlattn(m, sk, x (i)k ) be the sample-wise mse lossvalues of input embeddings, hidden states and at-tention matrices of the two models, respectively.
k ), lhidn(m, sk, x (i)here, lembd(m, sk, x (i)k )and lattn(m, sk, x (i)k ) refer to the sum of msevalues among multiple hidden layers.
we refer.
interested readers to jiao et al.
(2019) for moredetails.
lpred(m, sk, x (i)k ) is the cross-entropyloss of “softened” output logits, parameterized bythe temperature (hinton et al., 2015).
a naive ap-proach to formulating the total kd loss lkd is thesum of all previous loss functions, i.e.,(cid:88).
(cid:0)lembd(m, sk, x (i).
k )+.
lkd =.
x (i).
k ∈dkk ) + lattn(m, sk, x (i)lhidn(m, sk, x (i)lpred(m, sk, x (i).
k )+k )(cid:1)..however, the above approach does not give spe-cial considerations to the transferable knowledge ofthe meta-teacher.
let hmk ) bethe transferable knowledge of the meta-teacher andthe student model w.r.t.
the input x (i)k .
we furtherdeﬁne the transferable knowledge distillation losslt kd(m, sk, x (i)k ) as follows:.
k ) and hs.
d (x (i).
d (x (i).
ltkd(m, sk, x (i)(cid:88).
k ) =m se(cid:0)hm.
1|dk|.
x (i).
k ∈dk.
d (x (i).
k )w m.d , hs.
d (x (i).
k )(cid:1).
d.where w mis a learnable projection matrix tomatch the dimension between hmk ) andd (x (i)hsk ), and m se is the mse loss function w.r.t.
single element.
in this way, we encourage studentmodels to learn more transferable knowledge fromthe meta-teacher..d (x (i).
we further notice that although the knowledgeof the meta-teacher should be highly transferable,there still exists the domain gap between the meta-teacher and domain-speciﬁc student models.
in this.
3030embeddingencoderencoder......encoder...embeddingencoderencoder......ℒ!"transferrableknowledgetransferrableknowledgeℒ#!
"domain 1domain 2class 1 sampleclass 1 centroidclass 2 sampleclass 2 centroidprototype score 𝑡!
(%)predictedlabel #𝑦!
(%)domain-expertise weight 𝜆!
(%)inputinstance w. label (𝑥!(%),𝑦!
(%))outputlayeroutputlayermeta-teacher modeldomain-specific student modelkd lossk , we deﬁne the domain.
dataset.
domain.
#train.
#dev.
#test.
work, for each sample x (i)expertise weight λ(i).
k as follows:1 + t(i)kk −y(i)k )2 +1.
exp(ˆy(i).
,.
λ(i)k =.
is the predicted result of x (i).
where ˆy(i)k ’s classklabel.
here, the weight λ(i)k is large when the meta-teacher model i) has a large prototype score t(i)k andii) makes correct predictions on the target input, i.e.,ˆy(i)k = y(i)k .
we can see that the weight reﬂects howwell the meta-teacher can supervise the student ona speciﬁc input.
finally, we derive the completeformulation of the kd loss l(cid:48).
kd as follows:(cid:0)lembd(m, sk, x (i).
k )+.
λ(i)k.l(cid:48).
kd =.
(cid:88).
k ∈dk.
x (i)lhidn(m, sk, x (i).
k ) + lattn(m, sk, x (i)k )(cid:1) + γ2ltkd(m, sk, x (i).
k )+k )(cid:1),lpred(m, sk, x (i)where γ2 is the transferable kd factor to representhow the transferable knowledge distillation losscontributes to the overall loss..4 experiments.
in this section, we conduct extensive experimentsto evaluate the meta-kd framework on two populartext mining tasks across domains..4.1 tasks and datasets.
we evaluate meta-kd over natural language infer-ence and sentiment analysis, using the followingtwo datasets mnli and amazon reviews.
thedata statistics are in table 1..• mnli (williams et al., 2018) is a large-scale, multi-domain natural language infer-ence dataset for predicting the entailment re-lation between two sentences, containing ﬁvedomains (genres).
after ﬁltering samples withno labels available, we use the original devel-opment set as our test set and randomly sam-ple 10% of the training data as a developmentset in our setting..• amazon reviews (blitzer et al., 2007) isa multi-domain sentiment analysis dataset,widely used in multi-domain text classiﬁca-tion tasks.
the reviews are annotated as pos-itive or negative.
for each domain, there are2,000 labeled reviews.
we randomly split thedata into train, development, and test sets..mnli.
amazonreviews.
fictiongov.
slatetelephonetravel.
bookdvdelec.
kitchen.
69,61369,61569,57575,01369,615.
1,6311,6211,6151,613.
7,7357,7357,7318,3357,735.
170194172184.
1,9731,9451,9551,9661,976.
199185213203.table 1: statistics of the two datasets..4.2 baselines.
for the teacher side, to evaluate the cross-domaindistillation power of the meta-teacher model, weconsider the following models as baseline teachers:.
• bert-single: train the bert teacher modelon the target distillation domain only.
if wehave k domains, then we will have k bert-single teachers..• bert-mix: train the bert teacher on acombination of k-domain datasets.
hence,we have one bert-mix model as the teachermodel for all domains..• bert-mtl: similar to the “one-teacher”paradigm as in bert-mix, but the teachermodel is generated by the multi-task ﬁne-tuning approach (sun et al., 2019a)..• multi-teachers: it uses k domain-speciﬁcbert-single models to supervise k studentmodels, ignoring the domain difference..for the student side, we follow tinybert (jiaoet al., 2019) to use smaller bert models as ourstudent models.
in single-teacher baselines (i.e.,bert-single, bert-mix and bert-mtl), we usetinybert-kd as our baseline kd approach.
inmulti-teachers, because tinybert-kd does notnaturally support distilling from multiple teachermodels, we implement a variant of the tinybert-kd process based on mtn-kd (you et al., 2017),which uses averaged softened outputs as the incor-poration of multiple teacher networks in the outputlayer.
in practice, we ﬁrst learn the representationsof the student models by tinybert, then applymtn-kd for output-layer kd..3031method.
bertb-singlebertb-mixbertb-mtlmeta-teacher.
tinybert-kd−−−−−−−−→ berts.
bertb-singlebertb-mixbertb-mtlmulti-teachers mtn-kd.
tinybert-kd−−−−−−−−→ bertstinybert-kd−−−−−−−−→ berts−−−−−→ berts.
meta-teachermeta-teacher meta-distillation.
tinybert-kd−−−−−−−−→ berts−−−−−−−−−→ berts.
fiction government slate telephone travel average.
82.284.883.785.1.
78.8.
79.6.
79.7.
77.4.
80.3.
80.5.
84.287.287.186.5.
83.2.
83.3.
83.1.
81.1.
83.0.
83.7.
76.780.580.681.0.
73.6.
74.8.
74.2.
72.2.
75.1.
75.0.
82.483.883.983.9.
78.8.
79.0.
79.3.
77.2.
80.2.
80.5.
84.285.585.885.5.
81.9.
81.5.
82.0.
78.0.
81.6.
82.1.
81.984.484.284.4.
79.3.
79.6.
79.7.
77.2.
80.0.
80.4.table 2: results over mnli (with ﬁve domains) in terms of accuracy (%).
here x a−→ y means it uses x as theteacher and y as the student, with a as the kd method, hereinafter the same..method.
bertb-singlebertb-mixbertb-mtlmeta-teacher.
tinybert-kd−−−−−−−−→ berts.
bertb-singlebertb-mixbertb-mtlmulti-teachers mtn-kd.
tinybert-kd−−−−−−−−→ bertstinybert-kd−−−−−−−−→ berts−−−−−→ berts.
meta-teachermeta-teacher meta-distillation.
tinybert-kd−−−−−−−−→ berts−−−−−−−−−→ berts.
books dvd electronics kitchen average.
87.989.990.592.5.
83.4.
88.4.
90.5.
83.9.
89.9.
91.5.
83.885.986.587.0.
83.2.
81.6.
81.6.
78.4.
84.3.
86.5.
89.290.191.191.1.
89.2.
89.7.
88.7.
88.7.
87.3.
90.1.
90.692.191.189.2.
91.1.
89.7.
90.1.
87.7.
91.6.
89.7.
87.989.589.889.9.
86.7.
87.3.
87.7.
84.7.
88.3.
89.4.table 3: results over amazon reviews (with four domains) in terms of accuracy (%)..4.3.implementation details.
in the implementation, we use the original bertbmodel (l=12, h=768, a=12, total parame-ters=110m) as the initialization of all of the teach-ers, and use the berts model (l=4, h=312, a=12,total parameters=14.5m) as the initialization of allthe students4..the hyper-parameter settings of the meta-teachermodel are as follows.
we train 3-4 epochs with thelearning rate to be 2e-5.
the batch size and γ1are chosen from {16, 32, 48} and {0.1, 0.2, 0.5},respectively.
all the hyper-parameters are tuned onthe development sets..4https://github.com/huawei-noah/pretrained-language-model/tree/master/tinybert.
for meta-distillation, we choose the hidden lay-ers in {3, 6, 9, 12} of the teacher models in thebaselines and the meta-teacher model in our ap-proach to learn the representations of the studentmodels.
due to domain difference, we train stu-dent models in 3-10 epochs, with a learning rateof 5e-5.
the batch size and γ2 are tuned from {32,256} and {0.1, 0.2, 0.3, 0.4, 0.5} for intermediate-layer distillation, respectively.
following jiao et al.
(2019), for prediction-layer distillation, we run themethod for 3 epochs, with the batch size and learn-ing rate to be 32 and 3e-5.
the experiments areimplemented on pytorch and run on 8 tsela v100gpus..30324.4 experimental results.
method.
accuracy.
table 2 and table 3 show the general testing perfor-mance over mnli and amazon reviews of base-lines and meta-kd, in terms of accuracy.
from theresults, we have the following three major insights:.
• compared to all the baseline teacher models,using the meta-teacher for kd consistentlyachieves the highest accuracy in both datasets.
our method can help to signiﬁcantly reducemodel size while preserving similar perfor-mance, especially in amazon review, we re-duce the model size to 7.5x smaller with onlya minor performance drop (from 89.9 to 89.4)..• the meta-teacher has similar performanceas bert-mix and bert-mtl, but showsteacherto be a betterfor distillation,tinybert-kd−−−−−−−−→ berts andas meta-teachermeta-teacher meta-distillation−−−−−−−−−→ berts have bet-ter performance than other methods.
thisshows the meta-teacher is capable of learn-ing more transferable knowledge to help thestudent.
the fact that meta-teacher → meta-distillation has better performance than otherdistillation methods conﬁrms the effectivenessof the proposed meta-kd method..• meta-kd gains more improvement on thesmall datasets than large ones, e.g.
it improvesfrom 86.7 to 89.4 in amazon reviews while79.3 to 80.4 in mnli.
this motivates us toexplore our model performance on domainswith few or no training samples.
4.5 ablation study.
we further investigate meta-kd’s capability withregards to different portion training data for both oftwo phases and explore how the transferable knowl-edge distillation loss contributes to ﬁnal results..4.5.1 no in-domain data during.
meta-teacher learning.
in this set of experiments, we consider a specialcase where we assume all the “ﬁction” domaindata in mnli is unavailable.
here, we train ameta-teacher without the “ﬁction” domain datasetand use the distillation method proposed in jiaoet al.
(2019) to produce the student model for the“ﬁction” domain with in-domain data during dis-tillation.
the results are shown in table 4. weﬁnd that kd from the meta-teacher can have large.
bertb-s (ﬁction)meta-teacher (w/o ﬁction).
bertb-s (ﬁction).
tinybert-kd−−−−−−−→ berts.
bertb-s (govern)bertb-s (telephone)bertb-s (slate)bertb-s (travel).
tinybert-kd−−−−−−−→ bertstinybert-kd−−−−−−−→ berts.
tinybert-kd−−−−−−−→ bertstinybert-kd−−−−−−−→ berts.
meta-teacher.
tinybert-kd−−−−−−−→ berts.
82.2%81.6%.
78.8%.
75.3%.
75.6%.
77.1%.
74.1%.
78.2%.
table 4: results under the setting where no in-domaindata used for meta-teacher learning on mnli.
here,“bertb-s” refers to the “bertb-single” method.
thedistillation is performed on the “ﬁction” domain data.
we report accuracy on the domain dataset..figure 3: improvement rate w.r.t different portion (sam-ple rate) of training data in usage..improvement, compared to kd from other out-domain teachers.
additionally, learning from theout-domain meta-teacher has a similar performanceto kd from the in-domain “ﬁction” teacher modelitself.
it shows the meta-kd framework can beapplied in applications for emerging domains..4.5.2 few in-domain data available during.
meta-distillation.
we randomly sample a part of the mnli dataset asthe training data in this setting.
the sample ratesthat we choose include 0.01, 0.02, 0.05, 0.1 and0.2. the sampled domain datasets are employedfor training student models when learning from thein-domain teacher or the meta-teacher.
the experi-mental results are shown in figure 3, with resultsreported by the improvement rate in averaged ac-curacy.
the experimental results show that whenless data is available, the improvement rate is muchlarger.
for example, when we only have 1% of theoriginal mnli training data, the accuracy can be.
30339.9%5.2%2.4%2.5%0.9%1.1%opinions, ﬁndings, and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reﬂect those of thesponsor..references.
haoli bai, jiaxiang wu, irwin king, and michael r.lyu.
2020. few shot network compression via crossdistillation.
in aaai, pages 3203–3210..sergey bartunov, jack w. rae, simon osindero, andtimothy p. lillicrap.
2020. meta-learning deepenergy-based memory models.
in iclr..john blitzer, mark dredze, and fernando pereira.
2007.biographies, bollywood, boom-boxes and blenders:domain adaptation for sentiment classiﬁcation.
inacl..cen chen, minghui qiu, yinfei yang, jun zhou, junhuang, xiaolong li, and forrest sheng bao.
2019.multi-domain gated cnn for review helpfulness pre-diction.
in the world wide web conference, pages2630–2636..cen chen, chengyu wang, minghui qiu, dehong gao,linbo jin, and wang li.
2021. cross-domain knowl-edge distillation for retrieval-based question answer-ing systems.
in the world wide web conference..cen chen, yinfei yang, jun zhou, xiaolong li, andforrest sheng bao.
2018. cross-domain reviewhelpfulness prediction based on convolutional neu-ral networks with auxiliary domain discriminators.
in naacl-hlt, pages 602–607..daoyuan chen, yaliang li, minghui qiu, zhen wang,bofang li, bolin ding, hongbo deng, jun huang,wei lin, and jingren zhou.
2020a.
adabert: task-adaptive bert compression with differentiable neu-ral architecture search.
in ijcai, pages 2463–2469..defang chen, jian-ping mei, can wang, yan feng,and chun chen.
2020b.
online knowledge distilla-tion with diverse peers.
in aaai, pages 3430–3437..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt, pages 4171–4186..lingyun feng, minghui qiu, yaliang li, hai-taozheng, and ying shen.
2021. learning to augmentfor data-scarce domain bert knowledge distillation.
in aaai..chelsea finn, pieter abbeel, and sergey levine.
2017.model-agnostic meta-learning for fast adaptation ofdeep networks.
in icml, pages 1126–1135..chelsea finn, kelvin xu, and sergey levine.
2018.in.
probabilistic model-agnostic meta-learning.
neurips, pages 9537–9548..figure 4: model performance w.r.t.
kd factor γ2.
the transferable.
increased by approximately 10% when the studenttries to learn from the meta-teacher.
it shows meta-kd can be more beneﬁcial when we have fewerin-domain data..4.5.3.inﬂuence of the transferableknowledge distillation loss.
here, we explore how the transferable kd fac-tor γ2 affects the distillation performance over theamazon reviews dataset.
we tune the value of γ2from 0.1 to 1.0, with results are shown in figure4. we ﬁnd that the optimal value of γ2 generallylies in the range of 0.2 - 0.5. the trend of accu-racy is different in the domain “dvd” is differentfrom those of the remaining three domains.
thismeans the beneﬁts from transferable knowledge ofthe meta-teacher vary across domains..5 conclusion and future work.
in this work, we propose the meta-kd frameworkwhich consists of meta-teacher learning and metadistillation to distill plms across domains.
ex-periments on two widely-adopted public multi-domain datasets show that meta-kd can train ameta-teacher to digest knowledge across domainsto help better teach in-domain students.
quantita-tive evaluations conﬁrm the effectiveness of meta-kd and also show the capability of meta-kd inthe settings where the training data is scarce i.e.
there is no or few in-domain data.
in the future,we will examine the generalization capability ofmeta-kd in other application scenarios and applyother meta-learning techniques to kd for plms..acknowledgements.
this work is supported by open research projectsof zhejiang lab (no.
2019kd0ad01/004).
any.
30340.20.40.60.81.0transferable kd factor 27580859095accuracy (%)booksdvdelectronicskitchentommaso furlanello, zachary chase lipton, michaeltschannen, laurent itti, and anima anandkumar.
2018. born-again neural networks.
in icml, pages1602–1611..lili mou, zhao meng, rui yan, ge li, yan xu,lu zhang, and zhi jin.
2016. how transferable areneural networks in nlp applications?
in emnlp,pages 479–489..yunchao gong, liu liu, ming yang, and lubomir d.bourdev.
2014. compressing deep convolutionalnetworks using vector quantization.
corr..sinno jialin pan and qiang yang.
2010. a survey onieee trans.
knowl.
data eng.,.
transfer learning.
pages 1345–1359..song han, jeff pool, john tran, and william j. dally.
2015. learning both weights and connections forefﬁcient neural network.
in neurips, pages 1135–1143..yingwei pan, ting yao, yehao li, yu wang, chong-wah ngo, and tao mei.
2019. transferrable pro-totypical networks for unsupervised domain adapta-tion.
in cvpr, pages 2239–2247..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..shuke peng, feng ji, zehao lin, shaobo cui, haiqingchen, and yin zhang.
2020. mtss: learn from mul-tiple domain teachers and become a multi-domaindialogue expert.
in aaai, pages 8608–8615..mengting hu, yike wu, shiwan zhao, honglei guo,renhong cheng, and zhong su.
2019. domain-invariant feature distillation for cross-domain senti-ment classiﬁcation.
corr..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in naacl-hlt, pages 2227–2237..forrest n. iandola, albert e. shaw, ravi krishna, andkurt keutzer.
2020. squeezebert: what can com-puter vision teach nlp about efﬁcient neural net-works?
corr..yunhun jang, hankook lee, sung ju hwang, andjinwoo shin.
2019. metadistiller: network self-boosting via meta-learned top-down distillation.
inicml..khurram javed and martha white.
2019. meta-learning representations for continual learning.
inneurips, pages 1818–1828..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2019. tinybert: distilling bert for natural lan-guage understanding.
corr..tianhong li, jianguo li, zhuang liu, and changshuizhang.
2020. few sample knowledge distillationfor efﬁcient network compression.
in cvpr, pages14627–14635..benlin liu, yongming rao, jiwen lu, jie zhou, andcho jui hsieh.
2020. metadistiller: network self-boosting via meta-learned top-down distillation.
ineccv..adriana romero, nicolas ballas, samira ebrahimi ka-hou, antoine chassang, carlo gatta, and yoshuabengio.
2015. fitnets: hints for thin deep nets.
iniclr..sebastian ruder, parsa ghaffari, and john g. breslin.
2017. knowledge adaptation: teaching to adapt.
corr..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled version ofbert: smaller, faster, cheaper and lighter.
corr..adam santoro, sergey bartunov, matthew botvinick,daan wierstra, and timothy p. lillicrap.
2016.meta-learning with memory-augmented neural net-works.
in icml, pages 1842–1850..jake snell, kevin swersky, and richard s. zemel.
2017. prototypical networks for few-shot learning.
in neurips, pages 4077–4087..chi sun, xipeng qiu, yige xu, and xuanjing huang.
2019a.
how to ﬁne-tune bert for text classiﬁca-tion?
in ccl, pages 194–206..siqi sun, yu cheng, zhe gan, and jingjing liu.
2019b.
patient knowledge distillation for bert model com-pression.
corr..linqing liu, huan wang, jimmy lin, richard socher,and caiming xiong.
2019. mkd: a multi-task knowl-edge distillation approach for pretrained languagemodels.
corr..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
2020. mobilebert:a compact task-agnostic bert for resource-limiteddevices.
in acl, pages 2158–2170..pengfei liu, xipeng qiu, and xuanjing huang.
2017.adversarial multi-task learning for text classiﬁca-tion.
in acl, pages 1–10..ben tan, yu zhang, sinno jialin pan, and qiang yang.
in aaai,.
2017. distant domain transfer learning.
pages 2604–2610..raphael gontijo lopes, stefano fenu, and thadstarner.
2017. data-free knowledge distillationfordeep neural networks.
corr..xu tan, yi ren, di he, tao qin, zhou zhao, and tie-yan liu.
2019. multilingual neural machine transla-tion with knowledge distillation.
in iclr..3035shan you, chang xu, chao xu, and dacheng tao.
2017. learning from multiple teacher networks.
insigkdd, pages 1285–1294..sanqiang zhao, raghav gupta, yang song, and dennyzhou.
2019. extreme language model compres-sion with optimal subwords and shared projections.
corr..raphael tang, yao lu, linqing liu, lili mou, olgavechtomova, and jimmy lin.
2019. distilling task-speciﬁc knowledge from bert into simple neuralnetworks.
corr..antti tarvainen and harri valpola.
2017. mean teach-ers are better role models: weight-averaged consis-tency targets improve semi-supervised deep learningresults.
in neurips, pages 1195–1204..iulia turc, ming-wei chang, kenton lee, and kristinatoutanova.
2019. well-read students learn better:on the importance of pre-training compact models.
corr..karen ullrich, edward meeds, and max welling.
2017.soft weight-sharing for neural network compression.
in iclr..joaquin vanschoren.
2018. meta-learning: a survey..corr..risto vuorio, shao-hua sun, hexiang hu, andjoseph j. lim.
2019. multimodal model-agnosticmeta-learning via task-aware modulation.
inneurips, pages 1–12..chengyu wang, minghui qiu, jun huang, and xi-aofeng he.
2020. meta ﬁne-tuning neural languagein emnlp,models for multi-domain text mining.
page 3094–3104..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in naacl-hlt, pages 1112–1122..ze yang, linjun shou, ming gong, wutao lin, anddaxin jiang.
2020. model compression with two-stage multi-teacher knowledge distillation for webquestion answering system.
in wsdm, pages 690–698..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin neurips, pages 5754–language understanding.
5764..zhilin yang, ruslan salakhutdinov, and william w.cohen.
2017. transfer learning for sequence tag-ging with hierarchical recurrent networks.
in iclr..zhiquan ye, yuxia geng, jiaoyan chen, jingminchen, xiaoxiao xu, suhang zheng, feng wang, junzhang, and huajun chen.
2020. zero-shot text clas-siﬁcation via reinforced self-training.
in acl, pages3014–3024..junho yim, donggyu joo, ji-hoon bae, and junmokim.
2017. a gift from knowledge distillation:fast optimization, network minimization and trans-fer learning.
in cvpr, pages 7130–7138..wenpeng yin.
2020. meta-learning for few-shot natu-.
ral language processing: a survey.
corr..3036