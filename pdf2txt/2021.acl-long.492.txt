document-level event extraction via parallel prediction networks.
hang yang1,2, dianbo sui1,2, yubo chen1,2, kang liu1,2, jun zhao1,2, taifeng wang31national laboratory of pattern recognition, institute of automation,chinese academy of sciences, beijing, 100190, china2school of artiﬁcial intelligence, university of chinese academy of sciences,beijing, 100049, china3ant group, hangzhou, 310013, china{hang.yang, dianbo.sui, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn ,taifeng.wang@antgroup.com.
abstract.
document-level event extraction (dee)isindispensable when events are describedthroughout a document.
we argue thatsentence-level extractors are ill-suited to thedee task where event arguments always scat-ter across sentences and multiple events mayco-exist in a document.
it is a challenging taskbecause it requires a holistic understanding ofthe document and an aggregated ability to as-semble arguments across multiple sentences.
in this paper, we propose an end-to-end model,which can extract structured events from adocument in a parallel manner.
speciﬁcally,we ﬁrst introduce a document-level encoderto obtain the document-aware representations.
then, a multi-granularity non-autoregressivedecoder is used to generate events in parallel.
finally, to train the entire model, a matchingloss function is proposed, which can bootstrapa global optimization.
the empirical results onthe widely used dee dataset show that our ap-proach signiﬁcantly outperforms current state-of-the-art methods in the challenging deetask.
code will be available at https://github.com/hangyang-nlp/de-ppn..figure 1: an example of a document contains two eq-uity freeze type events: event-1 and event-2.
wordsin bold-faced are arguments that scatter across multiplesentences..1.introduction.
the goal of event extraction (ee) is to identifyevents of a pre-speciﬁed type along with corre-sponding arguments from plain texts.
a great num-ber of previous studies (ahn, 2006; ji and grish-man, 2008; liao and grishman, 2010; hong et al.,2011; li et al., 2013; chen et al., 2015; nguyenet al., 2016; yang and mitchell, 2016; chen et al.,2017; huang et al., 2018; yang et al., 2019; liuet al., 2020) focus on the sentence-level ee (see),while most of these works are based on the aceevaluation (doddington et al., 2004).
1 however,these see-based methods make predictions within.
1https://www.ldc.upenn.edu/collaborations/past-projects/ace.
a sentence and fail to extract events across sen-tences.
to this end, document-level ee (dee) isneeded when the event information scatters acrossthe whole document..in contrast to see, there are two speciﬁc chal-lenges in dee: arguments-scattering and multi-events.
speciﬁcally, arguments-scattering indi-cates that arguments of an event may scatter acrossmultiple sentences.
for example, as shown in fig-ure 1, the arguments of event-1 are distributed indifferent sentences ([s3] and [s7]) and extractionwithin an individual sentence will lead to incom-plete results.
so this challenge requires the deemodel to have a holistic understanding of the entiredocument and an ability to assemble all relevant.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6298–6308august1–6,2021.©2021associationforcomputationallinguistics6298[s3] onnovember 1, 2018, shenzhen 007 co., ltd. received  a notice that the corporate shareholder shanghai fukong co., ltdand the actual controller jing yan were judicial  frozen.
[s7] the corporate shareholder holds 150000 shares of the company.
the 10000 shares were frozen by the shenzhen intermediate peoples court from october 30, 2018 to october 30, 2019.
[s8] the controller of the company  holds 310000 sharesof the company.
the 20000 shares were frozen by the shenzhen inter -mediate people's courton november 1, 2018.shanghai fukong co., ltdjing yan10000 shares 20000 shares shenzhen intermediate people's court october 30, 2018 october 30, 2019november 1, 2018\arguments across sentences.
furthermore, it willbe more difﬁcult when coupled with the secondchallenge: multi-events, where multiple events arecontained in a document.2 as shown in figure 1,there are two events event-1 and event-2 in a doc-ument with the same event type and there is noobvious textual boundary between the two events.
the multi-events problem requires the dee methodto recognize how many events are contained in adocument and achieve accurate arguments assem-bling (i.e., assign arguments to the correspondingevent).
as a result of these two complications,see methods are ill-suited for the dee task, whichcalls for a model that can integrate document-levelinformation, assemble relevant arguments acrossmultiple sentences and capture multiple events si-multaneously..to handle these challenges in dee, previousworks (yang et al., 2018; zheng et al., 2019) for-mulate dee as an event table ﬁlling task, i.e., ﬁll-ing candidate arguments into a predeﬁned eventtable.
speciﬁcally, they model the dee as a se-rial prediction paradigm, in which arguments arepredicted in a predeﬁned role order and multipleevents are also extracted in predeﬁned event order.
such a manner is restricted to the extraction of in-dividual arguments, and the former extraction willnot consider the latter extraction results.
as a re-sult, errors will be propagated and the extractionperformance is under satisfaction..in this paper, to avoid the shortage of serialprediction and tackle the aforementioned chal-lenges in dee, we propose an end-to-end model,named document-to-events via parallel predictionnetworks (de-ppn).
de-ppn is based on anencoder-decoder framework that can extract struc-tured events from a whole document in a parallelmanner.
in detail, we ﬁrst introduce a document-level encoder to obtain the document-aware repre-sentations.
in such a way, a holistic understand-ing of the entire document is obtained.
then, weleverage a multi-granularity decoder to generateevents, which consists of two key parts: a role de-coder and an event decoder.
the role decoder isdesigned for handling the argument-scattering chal-lenge, which can assemble arguments for an eventbased on document-aware representations.
for ad-dressing the challenge of multi-events effectively,an event decoder is designed to support generating.
2according to our statistics, there are about 30% docu-ments include multiple events in the widely used chfinann(zheng et al., 2019).
multiple events.
both of them are based on thenon-autoregressive mechanism (gu et al., 2018),which supports the extraction of multiple events inparallel.
finally, for comparing extracted eventsto ground truths, we propose a matching loss func-tion inspired by the hungarian algorithm (kuhn,1955; munkres, 1957).
the proposed loss functioncan perform a global optimization by computing abipartite matching between predicted and ground-truth events..in summary, our contributions are as follows:.
• we propose an encoder-decoder model, de-ppn, that is based on a document-level en-coder and a multi-granularity decoder to ex-tract events in parallel with document-awarerepresentations..• we introduce a novel matching loss functionto train the end-to-end model, which can boot-strap a global optimization..• we conduct extensive experiments on thewidely used dee dataset and experimentalresults demonstrate that de-ppn can signif-icantly outperform state-of-the-art methodswhen facing the speciﬁc challenges in dee..2 methodology.
before introducing our proposed approach for deein this section, we ﬁrst describe the task formal-ization of dee.
formally, we denote t and ras the set of pre-deﬁned event types and role cat-egories, respectively.
given an input documentcomprised of ns sentences d = {si}nsi=1, the deetask aims to extract one or more structured eventsy = {yi}ki with event typet contains a series of roles (r1i , .
.
.
, rni ) ﬁlledby arguments (a1i , .
.
.
, ani ).
k is the number ofevents contained in the document, n is the numberof pre-deﬁned roles for the event type t, t ∈ t andr ∈ r..i=1, where each event yti , r2.
i , a2.
the key idea of our proposed model, de-ppn,is that aggregate the document-level context to pre-dict events in parallel.
figure 2 illustrates the ar-chitecture of de-ppn, which consists of ﬁve keycomponents: (1) candidate argument recognition,(2) document-level encoder, (3) multi-granularitydecoder, (4) events prediction, and (5) matchingloss function..6299figure 2: the overall architecture of de-ppn.
given a document, the de-ppn ﬁrst encodes each sentence sepa-rately and recognizes candidate arguments from it.
then a document-level encoder is designed to get the document-level representations.
and a multi-granularity decoder is used to generate events in parallel based on document-aware representations.
finally, the matching loss function can produce an optimal bipartite matching betweenpredicted and ground-truth events, which bootstrap a global optimization..2.1 candidate argument recognitiongiven a document d = {si}nsi=1 with ns sentences,each sentence si with a sequence of tokens is ﬁrstembedded as [wi,1, wi,2, .
.
.
, wi,l], where l is thesentence length.
then, the word embeddings arefed into an encoder to obtain the contextualizedrepresentation.
in this paper, we adopt the trans-former (vaswani et al., 2017) as the primary con-text encoder.
through the encoder, we can get thecontext-aware embedding ci of sentence si:.
ci = transformer-1(si).
(1).
where ci ∈ rl×d and d is the size of the hiddenlayer, and we represent each sentence in the givendocument as {ci}nsi=1..finally, following zheng et al.
(2019), we modelthe sentence-level candidate argument recognitionas a typical sequence tagging task.
through candi-date argument recognition, we can obtain candidatearguments a = {ai}nai=1 from the given sentencesi, where na is the number of recognized candi-date arguments..2.2 document-level encoder.
to enable the awareness of document-level con-texts for sentences and candidate arguments, weemploy a document-aware encoder to facilitate theinteraction between all sentences and candidate ar-guments.
formally, given an argument ai with itsspan covering j-th to k-th in sentence si, we con-duct a max-pooling operation over the token-levelembedding [ci,j, .
.
.
, ci,k] ∈ ci to get the local.
i ∈ rd for it.
similarly, the sentenceembedding cai ∈ rd can be obtained by the max-embedding cspooling operation over the token sequence repre-sentation ci of sentence si.
then, we employ thetransformer module, transformer-2, as the encoderto model the interaction between all sentences andcandidate arguments by a multi-head self-attentionmechanism.
then we can get the document-awarerepresentations for sentences and arguments.
notethat we add the sentence representation with sen-tence position embeddings to inform the sentenceorder before feeding them into transformer-2..[ha; hs] = transformer-2(ca.
1...ca.
na; cs.
1...cs.
ns).
(2)since arguments may have many mentions in adocument, we utilize the max-pooling operationto merge multiple argument embeddings with thesame char-level tokens into a single embedding.
after the document-level encoding stage, we canobtain the document-aware sentences representa-tion hs ∈ rns×d and candidate arguments a(cid:48) ={ai}n (cid:48).
i=1 with representation ha ∈ rn (cid:48).
a×d..a.before decoding, we stack a linear classiﬁerover the document representation by operating themax-pooling over hs to conduct a binary classi-ﬁcation for each event type.
then, for the pre-dicted event type t with pre-deﬁned role types,de-ppn learns to generate events according tothe document-aware candidate argument represen-tations ha ∈ rn (cid:48)a×d and sentence representationshs ∈ rns×d..6300event queriesevent role queriescandidate argumentssentence positiondocumentsentencesevent typeclassificationpredicted eventtargeteventnon-nullnull 2.3 multi-granularity decoder.
where hrole ∈ rn×d..to effectively address arguments-scattering andmulti-events in dee, we introduce a multi-granularity decoder to generate all possible eventsin parallel based on document-aware representa-tions (ha and hs).
the multi-granularity decoderis composed of three parts: event decoder, roledecoder, and event-to-role decoder.
all of these de-coders are based on the non-autoregressive mecha-nism (gu et al., 2018), which supports the extrac-tion of all events in parallel..event decoder.
the event decoder is designedto support the extraction of all events in paral-lel and is used to model the interaction betweenevents.
before the decoding stage, the decoderneeds to know the size of events to be generated.
we use m learnable embeddings as the input of theevent decoder, which are denoted as event queriesqevent ∈ rm×d.
m is a hyperparameter that de-notes the number of the generated events.
in ourwork, m is set to be signiﬁcantly large than theaverage number of events in a document.
then,the event query embeddings qevent are fed into anon-autoregressive decoder which is composed ofa stack of n identical transformer layers.
in eachlayer, there are a multi-head self-attention mecha-nism to model the interaction among events and amulti-head cross-attention mechanism to integratethe document-aware representation hs into eventqueries qevent.
formally, the m event queries aredecoded into m output embeddings hevent by:.
hevent = event-decoder(qevent; hs).
(3).
where hevent ∈ rm×d..role decoder.
the role decoder is designed tosupport the ﬁlling of all roles in an event in par-allel and model the interaction between roles.
asthe predicted event type t with semantic role types(r1, r2, .
.
.
, rn), we use n learnable embeddings asthe input of the role decoder, which are denotedas event queries qrole ∈ rn×d.
then, the rolequery embeddings qrole are fed into the decoder,which has the same architecture as the event de-coder.
speciﬁcally, the self-attention mechanismcan model the relationship among roles, and thecross-attention mechanism can fuse the informa-tion of the document-aware candidate argumentrepresentations ha.
formally, the n role queriesare decoded into n output embeddings hrole by:hrole = role-decoder(qrole; ha).
(4).
event-to-role decoder.
to generate diversiformevents with relevant arguments for different eventqueries, an event-to-role decoder is designed tomodel the interaction between the event querieshevent and the role queries hrole:.
he2r = event2role-decoder(hrole; hevent).
(5).
where he2r ∈ rm×n×d..2.4 events prediction.
after the multi-granularity decoding, the m eventqueries and n role queries are transformed into mpredicted events and each of them contains n roleembeddings.
to ﬁlter the spurious event, the mevent queries hevent are fed into a feed-forwardnetworks (ffn) to judge each event prediction isnon-null or null.
concretely, the predicted eventcan be obtained by:.
pevent = softmax(heventwe).
(6).
where we ∈ rd×2 is learnable parameters..then, for each predicted event with pre-deﬁnedroles, the predicted arguments are decoded by ﬁll-ing the candidate indices or the null value with(n (cid:48).
a + 1)-class classiﬁers:3.prole = softmax(tanh(he2rw1 + haw2) · v1)(7)where w1 ∈ rd×d, w2 ∈ rd×d and v1 ∈ rd arelearnable parameters, and prole ∈ rm×n×(n (cid:48)a+1).
after the prediction network, we can obtain them events ˆy = ( ˆy1, ˆy2, .
.
.
, ˆym) where each eventˆyi = (p1i , .
.
.
, pni ) contains n predicted argu-ments with role types.
where pji = prole[i, j, :] ∈r(n (cid:48).
i , p2.
a+1)..2.5 matching loss.
the main problem for training is that how to assignpredicted m events with a series of arguments tothe ground truth k events.
inspired by the assign-ing problem in the operation research (kuhn, 1955;munkres, 1957), we propose a matching loss func-tion, which can produce an optimal bipartite match-ing between predicted and ground-truth events..formally, we denote predicted and groundtruth events as ˆy = ( ˆy1, ˆy2, .
.
.
, ˆym) and y =(y1, y2, .
.
.
, yk), respectively.
where k is the.
3note that we append candidate argument representations.
ha with a learnable embedding to represent the null value..6301i , p2.
real number of events in the document and mis ﬁxed size for generated events.
note thatm (cid:62) k. the i-th predicted event is denoted asˆyi = (p1i can be calcu-lated by the equation 7. and the i-th ground truthevent is denoted as yi = (r1i ) , whererji is the candidate argument indix for j-the roletype in i-th target event..i ) , where pj.
i , .
.
.
, pn.
i , .
.
.
, rn.
i , r2.
to ﬁnd a bipartite matching between these twosets, we search for a permutation of m elementswith the lowest cost:.
3 experiments and analysis.
in this section, we present empirical studies to an-swer the following questions:.
1. what is the overall performance of our de-ppn compared to the state-of-the-art (sota)method evaluated on the dee task?.
2. how does de-ppn perform when facing thearguments-scattering and multi-event chal-lenges in dee?.
ˆσ = argmaxσ∈(cid:81)(m).
m(cid:88).
i.cmatch( ˆyσ(i), yi).
(8).
3. how does each design of our proposed de-.
ppn matter?.
where (cid:81)(m) is the space of all m-length permuta-tions and cmatch( ˆyσ(i), yi) is a pair-wise matchingcost between ground truth yi and a prediction ˆyσ(i)with index σ(i).
by taking into account all of theprediction arguments for roles in an event, we de-ﬁne cmatch( ˆyσ(i), yi) as:.
n(cid:88).
j=1.
cmatch( ˆyσ(i), yi) = −1{judgei(cid:54)=φ}.
σ(i)(rjpj.
i )).
(9)where the judgei is the judgement of event i to benon-null or null that is calculated by the equation 6.the optimal assignment σ(i) can be computed ef-fectively with the hungarian algorithm.
4 then forall pairs matched in the previous step, we deﬁnethe loss function with negative log-likelihood as:.
l( ˆy , y ) =.
1{judgei(cid:54)=φ}[.
−logpj.
ˆσ(i)(rji )].
m(cid:88).
i=1.
n(cid:88).
j=1.
(10)where ˆσ is the optimal assignment computed in theequation 8..2.6 optimization.
during training, we sum the matching loss forevents prediction with preconditioned steps beforedecoding as follows:.
lall = λ1lsee + λ2lec + λ3l(y, ˆy ).
(11).
where lae and lec are the cross-entropy loss func-tion for sentence-level candidate argument recogni-tion and event type classiﬁcation, respectively.
λ1,λ2 and λ3 are hyper-parameters..4. what is the inﬂuence of setting different num-bers of the generated events on the results?.
3.1 experimental setup.
dataset.
following zheng et al.
(2019), we usethe chfinann dataset5 to evaluate our proposeddee method.
the chfinann is a large-scale deedataset, which contains 32,040 documents in to-tal and includes ﬁve ﬁnancial event types: equityfreeze (ef), equity repurchase (er), equity un-derweight (eu), equity overweight (eo) and eq-uity pledge (ep)..evaluation metrics.
for a fair compari-son, we adopt the evaluation standard used indoc2edag (zheng et al., 2019).
speciﬁcally,for each predicted event, the most similar ground-truth is selected without replacement to calculatethe precision (p), recall (r), and f1-measure (f1-score).
as an event type often includes multipleroles, micro-averaged role-level scores are calcu-lated as the ﬁnal dee metric..implementation details.
for a document as input,we set the maximum number of sentences and themaximum sentence length as 64 and 128, respec-tively.
we adopt the basic transformer, each layerhas 768 hidden units, and 8 attention heads, as theencoder and decoder architecture.
during training,we employ the adamw optimizer (kingma andba, 2014) with the learning rate 1e-5 with batchsize 16. testing set performance is chosen by thebest development set performance step within 100epochs.
we leave detailed hyper-parameters andadditional results in the appendix..4https://en.wikipedia.org/wiki/.
hungarianalgorithm.
5https://github.com/dolphin-zs/.
doc2edag/blob/master/data.zip.
6302models.
pdcfee-o66.0dcfee-m 51.879.5greedydecdoc2edag 77.177.8de-ppn-178.2de-ppn.
efr41.640.746.864.555.869.4.f151.145.658.970.264.973.5.p84.583.783.391.375.689.3.err81.878.074.983.676.485.6.f183.180.878.987.376.087.4.p62.749.568.780.276.469.7.eur35.439.940.865.063.779.9.f145.344.251.271.869.474.4.p51.442.569.782.177.181.0.eor42.647.540.669.054.371.3.f146.644.951.375.063.775.8.p64.359.885.780.085.583.8.epr63.666.448.774.843.073.7.f163.962.962.177.357.278.4.table 1: overall event-level precision (p), recall (r) and f1-score (f1) evaluated on the test set..models.
s.56.0dcfee-odcfee-m 48.475.9greedydecdoc2edag 80.082.4de-ppn-182.1de-ppn.
ef.
er.
eu.
eo.
ep.
m.46.543.140.861.346.363.5.s.86.783.881.789.478.389.1.m.54.153.449.868.453.970.5.s.48.548.162.277.482.279.7.m.41.239.634.664.645.666.7.s.47.747.165.779.478.180.6.m.45.242.029.469.539.369.6.s.68.467.088.585.582.888.0.m.61.160.042.372.538.573.2.s.61.558.974.882.380.783.9.avg.
m.49.647.739.467.344.768.7.s.& m.58.055.760.576.366.277.9.table 2: f1-score for all event types and the averaged ones (avg.)
on single-event (s.) and multi-event (m.) sets..models.
dcfee-mdoc2edagde-ppn.
asr(cid:54)0.5 0.5(cid:54)asr(cid:54)1 asr(cid:62)153.574.476.1.
42.264.467.1.
65.778.479.5.table 3: averaged f1-score for different asr intervals..3.2 baselines.
we compare our de-ppn with the sota methodsas follows: dcfee (yang et al., 2018) proposeda key-event detection to guide event table ﬁlledwith the arguments from key-event mention andsurrounding sentences.
there are two versions ofdcfee: dcfee-o only extracts one event anddcfee-m extracts multiple events from a docu-ment.
doc2edag (zheng et al., 2019) proposedan end-to-end model for dee, which transformsdee as directly ﬁlling event tables with entity-based path expending.
there is a simple baselineof doc2edag, named greedydec, which onlyﬁlls one event table entry greedily.
besides, we fur-ther introduce a simple baseline of de-ppn, namedas de-ppn-1, which only generates one event..3.3 main results.
de-ppn vs. sota.
table 1 shows the compar-ison between de-ppn and baseline methods onthe test set for each event type.
overall, our pro-posed model de-ppn signiﬁcantly outperformsother baselines and achieves sota performance.
in all event types.
speciﬁcally, de-ppn improves3.3, 0.1, 2.6, 0.8, 1.1, 1.6 f1-score over the sotamethod, doc2edag, on the event type ef, er, eu,eo, ep and the average f1-score, respectively.
theimproved performance indicates that the encoder-decoder generative framework of de-ppn is ef-fective, which can predict events in parallel witha global optimization for training.
besides, asthe baseline of our proposed method, de-ppn-ocan achieve the best performance compared withdcfee-o and greedydec while all of them onlypredict one event for a document, which also provesthe effectiveness of the document-aware end-to-endmodeling of de-ppn..results on arguments-scattering.
to show theextreme difﬁculty of the arguments-scattering chal-lenge in dee, we conduct experiments on differentscenarios.
we introduce an arguments-scatteringratio (asr) to measure the scatter of arguments inan event for a document.
the asr is calculated by:.
asr = numments/numargs.
(12).
where numments denotes the number of event men-tions (i.e., sentences that contains arguments) andnumargs denotes the number of arguments.
thehigher the asr, the more scattering of the argu-ments in an event.
table 3 shows the results withthe different intervals of asr.
we can observethat it is more difﬁcult to extract scattering argu-ments as the asr increase.
but de-ppn still.
6303figure 3: f1-score for performance differences of event decoder and role decoder layers..maintains the best performance and the results indi-cate that the encoder-decoder framework can betterassemble arguments to the corresponding eventacross sentences with the parallel prediction andthe document-aware representations..single-event vs. multi-event.
to show the ex-treme difﬁculty when arguments-scattering meetsmulti-events for dee, we conduct experiments ontwo scenarios: single-event (i.e., documents con-tain one event) and multi-event (i.e., documentscontain multiple events).
table 2 shows the f1-score on single-event and multi-event sets for eachevent type and the averaged (avg.).
we can observethat multi-events is extremely challenging as theextraction performance of all models drops signiﬁ-cantly.
but de-ppn still improves the average f1-score from 67.3% to 68.7% over the doc2edag.
the results demonstrate the effectiveness of ourproposed method when handling the challenge ofmulti-events.
this performance improvement ben-eﬁts from the event decoder which can generatemultiple events in parallel and the matching lossfunction which can perform a global optimization.
besides, the de-ppn-1 model achieves an accept-able performance on the scenario of single eventextraction which demonstrates the effectiveness ofour end-to-end model.
but de-ppn-1 only gen-erates one event and cannot deal with the multi-events problem, resulting in low performance onthe multi-event sets..3.4 ablation studies.
to verify the effectiveness of each component ofde-ppn, we conduct ablation tests on the nextvariants: 1) -docenc: removing the transformer-based document-level encoder, which can supportthe document-aware information for decoding.
2)-multidec: replacing the multi-granularity decodermodule with simple embedding initialization forevent queries and role queries.
3) -matchingloss:.
model.
ef er eu eo ep avg..de-ppn.
73.5 87.4.
74.4.
75.8.
78.4.
77.9.
-dcoenc-multidec.
-2.6-4.3-matchingloss -9.2 -12.8 -13.1 -17.5 -14.3 -13.4.
-2.1-5.1.
-1.7-4.3.
-3.4-3.8.
-3.2-3.6.
-2.6-4.7.table 4: f1-score of ablation studies on de-ppn vari-ants for each event type and the averaged (avg.)..
replacing the matching loss function with normalcross-entropy loss.
the results are shown in table 4and we can observe that: 1) the document-level en-coder is of prime importance that enhances thedocument-aware representations for the generativedecoder and contributes +2.6 f1-score on average;2) the multi-granularity decoder alleviates the chal-lenges of argument-scattering and multi-events byassembling arguments and generating events in par-allel, improving by +4.3 f1-score on average.
3)the matching loss function is a very important com-ponent for events extraction with +13.4 f1-scoreimprovement which indicates that the matchingloss guide a global optimization between predictedand ground-truth events during training..3.5 effect of different decoder layers.
the multi-to investigate the importance ofgranularity decoder, we explore the effect of differ-ent layers of the event decoder and the role decoderon the results.
speciﬁcally, the number of decoderlayers is set to 0,1,2,3 and 4, where 0 means remov-ing this decoder.
1) the effect of different eventdecoder layers are shown in the left of figure 3, andour method can achieve the best average f1-scorewhen the number of layers is set to be 2. we con-jecture that more layers of the non-autoregressivedecoder allow for better modeling the interactionbetween event queries and generating diversiformevents.
however, when the layer is set to be large,it is easy to generate redundant events.
2) the.
630401234number of event decoder layers60657075808590f1-score (%)75.382.583.582.582.661.468.168.668.068.201234number of role decoder layers60657075808590f1-score (%)79.481.383.082.683.965.366.368.768.168.7multi_eventsingle_eventeffect of different role decoder layers are shownin the right of figure 3, and we can observe thatthe more decoder layers, the better performanceon the results.
we conjecture that more layers ofthe decoder with the more self-attention modulesallow for better modeling the relationship betweenevent roles and more inter-attention modules allowfor integrating information of candidate argumentsinto roles..3.6 effect of different generated sets.
for the training and testing process of the de-ppn,the number of generated events is an importanthyperparameter.
in this section, we explore theinﬂuence of setting different numbers of gener-ated events on the results.
we divide the develop-ment set into 5 sub-class where each class contains1,2,3,4 and (cid:62) 5 events.
table 5 shows the statisticsof the documents with different annotated eventsin the development set.
to validate the impactof the number of generated events on the perfor-mance, we evaluate de-ppn with various numbersof generated events: 1, 2, 5, 10, named de-ppn-1,de-ppn-2, de-ppn-5, de-ppn-10, respectively.
the results of de-ppn with different generatedevents are shown in figure 4, which are also com-pared with the sota model doc2edag.
we canobserve that as the number of events increases, itis more difﬁcult for events prediction, which canbe reﬂected in the decline of all performance.
ingeneral, de-ppn almost achieves the best perfor-mance on the average f1-score when the numberof generated sets is set to be 5. besides, there isa performance gap between doc2edag and ourmethod de-ppn when the number of annotatedevents is large than 2 in a document.
it also demon-strates that our proposed parallel decoder can betterhandle the challenge of multi-events in dee..number of events.
1.
2.
3.number of documents.
2207.
609.
203.
4 (cid:62) 5 total318777.
91.table 5: the statistics about the documents annotatedwith different numbers of events in the developmentset..4 related work.
4.1 sentence-level event extraction.
most work in ee has focused on the sentencelevel and is based on the benchmark dataset ace2005 (doddington et al., 2004).
many approaches.
figure 4: f1-score for performance differences of gen-erated events..have been proposed to improve performance onthis task.
these studies are mainly based on hand-designed features (li et al., 2013; kai and gr-ishman, 2015) and neural-based to learn featuresautomatically (chen et al., 2015; nguyen et al.,2016; bj¨orne and salakoski, 2018; yang et al.,2019; chan et al., 2019; yang et al., 2019; liuet al., 2020).
a few methods make extraction de-cisions beyond individual sentences.
ji and grish-man (2008) and liao and grishman (2010) usedevent type co-occurrence patterns for event detec-tion.
yang and mitchell (2016) introduced eventstructure to jointly extract events and entities withina document.
although these approaches make deci-sions beyond sentence boundary, their extractionsare still done at the sentence level..4.2 document-level event extraction.
many real-world applications need dee, in whichthe event information scatters across the wholedocument.
muc-4 (1992) proposed the muc-4template-ﬁlling task that aims to identify event roleﬁllers with associated role types from a document.
recent works explore the local and additional con-text to extract the role ﬁllers by manually designedlinguistic features (patwardhan and riloff, 2009;huang and riloff, 2011, 2012) or neural-based con-textual representation (chen et al., 2020; du et al.,2020; du and cardie, 2020).
recently, ebner et al.
(2020) published the roles across multiple sen-tences (rams) dataset, which contains annotationfor the task of multi-sentence argument linking.
a two-step approach (zhang et al., 2020) is pro-posed for argument linking by detecting implicitargument across sentences.
li et al.
(2021) extendthis task and compile a new benchmark dataset.
63051234>=5number of annotated events304050607080f1-score (%)83.273.456.273.169.365.368.242.156.469.861.667.335.750.871.362.965.426.438.362.159.653.2de-ppn-1de-ppn-2de-ppn-5de-ppn-10doc2eadgwikievents for exploring document-level ar-gument extraction task.
then, li et al.
(2021)propose an end-to-end neural event argument ex-traction model by conditional text generation.
how-ever, these works focused on the sub-task of dee(i.e., role ﬁller extraction or argument extraction)and ignored the challenge of multi-events..to simultaneously address both challenges fordee (i.e., arguments-scattering and multi-events),previous works focus on the chfinann (zhenget al., 2019) dataset and model dee as an eventtable ﬁlling task, i.e., ﬁlling candidate argumentsinto predeﬁned event table.
yang et al.
(2018) pro-posed a key-event detection to guide event tableﬁlled with the arguments from key-event mentionand surrounding sentences.
zheng et al.
(2019)transforms dee into ﬁlling event tables follow-ing a predeﬁned order of roles with an entity-basedpath expanding, which achieved the sota for dee.
however, these methods suffered from a serial pre-diction which will lead to error propagation andindividual argument prediction..5 conclusion and future work.
in this paper, we propose an encoder-decodermodel, de-ppn, to extract events in parallel froma document.
for addressing the challenges (i.e.,arguments-scattering and multi-events) in dee, weintroduce a document-level encoder and a multi-granularity decoder to generate events in parallelwith document-aware representations.
for trainingthe parallel networks, we propose a matching lossfunction to perform a global optimization.
experi-mental results show that de-ppn can signiﬁcantlyoutperform sota methods especially facing thespeciﬁc challenges in dee..acknowledgements.
we thank the anonymous reviewers for their con-structive and insightful comments.
this work issupported by the national natural science foun-dation of china (no.
u1936207, no.
61922085and no.
61806201), beijing academy of arti-ﬁcial intelligence (no.
baai2019qn0301), thekey research program of the chinese academy ofsciences (no.
zdbs-ssw-jsc006), independentresearch project of national laboratory of patternrecognition and a grant from ant group..references.
david ahn.
2006. the stages of event extraction.
inproceedings of the workshop on annotating andreasoning about time and events, pages 1–8..jari bj¨orne and tapio salakoski.
2018. biomedi-cal event extraction using convolutional neural net-in proceedings ofworks and dependency parsing.
the bionlp 2018 workshop, pages 98–108, mel-bourne, australia.
association for computationallinguistics..yee seng chan, joshua fasching, haoling qiu, and bo-nan min.
2019. rapid customization for event ex-in proceedings of the 57th annual meet-traction.
ing of the association for computational linguis-tics: system demonstrations, pages 31–36, flo-rence, italy.
association for computational linguis-tics..pei chen, hang yang, kang liu, ruihong huang,yubo chen, taifeng wang, and jun zhao.
2020. re-constructing event regions for event extraction viagraph attention networks.
in proceedings of the 1stconference of the asia-paciﬁc chapter of the associ-ation for computational linguistics and the 10th in-ternational joint conference on natural languageprocessing, pages 811–820, suzhou, china.
associ-ation for computational linguistics..yubo chen, shulin liu, xiang zhang, kang liu, andjun zhao.
2017. automatically labeled data genera-tion for large scale event extraction.
in proceedingsof the 55th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 409–419..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the acl..george r doddington, alexis mitchell, mark a przy-bocki, lance a ramshaw, stephanie m strassel, andralph m weischedel.
2004. the automatic contentextraction (ace) program-tasks, data, and evaluation.
in lrec, volume 2, page 1. lisbon..xinya du and claire cardie.
2020. document-levelevent role ﬁller extraction using multi-granularitycontextualized encoding.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 8010–8020, online.
asso-ciation for computational linguistics..xinya du, alexander rush, and claire cardie.
2020.document-level event-based extraction using gener-ative template-ﬁlling transformers.
arxiv preprintarxiv:2008.09249..seth ebner, patrick xia, ryan culkin, kyle rawlins,and benjamin van durme.
2020. multi-sentence ar-gument linking.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 8057–8077, online.
association forcomputational linguistics..6306jiatao gu, james bradbury, caiming xiong, victor okli, and richard socher.
2018. non-autoregressiveneural machine translation.
in international confer-ence on learning representations..yu hong, jianfeng zhang, bin ma, jianmin yao,guodong zhou, and qiaoming zhu.
2011. usingcross-entity inference to improve event extraction.
in proceedings of the 49th annual meeting of the as-sociation for computational linguistics: human lan-guage technologies, pages 1127–1136..lifu huang, heng ji, kyunghyun cho, ido dagan, se-bastian riedel, and clare voss.
2018. zero-shotin proceed-transfer learning for event extraction.
ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 2160–2170, melbourne, australia.
as-sociation for computational linguistics..ruihong huang and ellen riloff.
2011. peeling backthe layers: detecting event role ﬁllers in secondarycontexts.
in proceedings of the 49th annual meet-ing of the association for computational linguistics:human language technologies-volume 1, pages1137–1147.
association for computational linguis-tics..ruihong huang and ellen riloff.
2012. bootstrappedtraining of event extraction classiﬁers.
in proceed-ings of the 13th conference of the european chap-ter of the association for computational linguistics,pages 286–295.
association for computational lin-guistics..heng ji and ralph grishman.
2008. reﬁning event ex-traction through cross-document inference.
in pro-ceedings of acl-08: hlt, pages 254–262..xiang li thien huu nguyen kai and cao ralph gr-ishman.
2015.improving event detection with ab-stract meaning representation.
acl-ijcnlp 2015,page 11..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..harold w kuhn.
1955. the hungarian method for theassignment problem.
naval research logistics quar-terly, 2(1-2):83–97..qi li, heng ji, and liang huang.
2013. joint eventextraction via structured prediction with global fea-tures.
in proceedings of the 51st annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 73–82..sha li, heng ji, and jiawei han.
2021. document-level event argument extraction by conditional gener-ation.
in proceedings of the 2021 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, pages 894–908, online.
association for com-putational linguistics..shasha liao and ralph grishman.
2010. using doc-ument level cross-event inference to improve eventextraction.
in proceedings of the 48th annual meet-ing of the association for computational linguistics,pages 789–797..jian liu, yubo chen, kang liu, wei bi, and xiaojiangliu.
2020. event extraction as machine reading com-prehension.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 1641–1651, online.
associa-tion for computational linguistics..muc-4.
1992..fourth message uunderstandingin in proceedings ofconference (muc-4).
fourth message understanding con-ference (muc4), mclean, virginia..james munkres.
1957. algorithms for the assignmentand transportation problems.
journal of the societyfor industrial and applied mathematics, 5(1):32–38..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentin proceedings of the 2016 con-neural networks.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 300–309..siddharth patwardhan and ellen riloff.
2009. a uni-ﬁed model of phrasal and sentential evidence for in-in proceedings of the 2009formation extraction.
conference on empirical methods in natural lan-guage processing: volume 1-volume 1, pages 151–160. association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..bishan yang and tom m. mitchell.
2016. joint extrac-tion of events and entities within a document context.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 289–299, san diego, california.
associationfor computational linguistics..hang yang, yubo chen, kang liu, yang xiao, and junzhao.
2018. dcfee: a document-level chinese ﬁnan-cial event extraction system based on automaticallylabeled training data.
in proceedings of acl 2018,system demonstrations, pages 50–55..sen yang, dawei feng, linbo qiao, zhigang kan,and dongsheng li.
2019. exploring pre-trained lan-guage models for event extraction and generation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5284–5294..zhisong zhang, xiang kong, zhengzhong liu, xuezhema, and eduard hovy.
2020. a two-step approach.
6307in proceed-for implicit event argument detection.
ings of the 58th annual meeting of the associationfor computational linguistics, pages 7479–7485,online.
association for computational linguistics..shun zheng, wei cao, wei xu, and jiang bian.
2019.doc2edag: an end-to-end document-level frame-work for chinese ﬁnancial event extraction.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 337–346, hong kong, china.
association for computa-tional linguistics..a.2 additional results.
table 6 shows the results of event type classiﬁca-tion and candidate argument extraction.
they arethe two preceding sub-tasks for decoder to predictevents with corresponding arguments in parallel.
we can observe that: 1) the document-level eventtype classiﬁcation can achieve a good performancewhich proves that event classiﬁcation is not a dif-ﬁcult problem in this task.
2) how to assemblecandidate arguments to corresponding events is thekey challenge for dee..equity freezeequity repurchaseequity underweightequity overweightequity pledge.
p.r.f1.
100.0 99.6 99.8100.0 99.5 99.898.1 98.098.094.9 96.197.599.9 99.799.5.candidate argument recognition 90.0.
89.5 89.7.table 6: evaluation results of candidate argument ex-traction and event type classiﬁcation on the test set..a.3 hyperparameter setting.
the detail hyperparameter is shown in table 7.hyper-parameter.
value.
number of generated eventsembedding sizehidden sizetagging schemelayers of transformer-1layers of transformer-2layers of event decoderlayers of role decoderlayers of event-to-role decoderoptimizerlearning rate for encoderlearning rate for decoderbatch sizeλ1λ2λ3dropouttraining epoch.
5768768bio (begin, inside, other)44242adamw1e−52e−5160.10.40.50.1100.table 7: the hyper-parameter setting..a appendix.
in the appendix, we incorporate the following de-tails that are omitted in the main body due to thespace limit..• section a.1 introduce the hungarian algo-.
rithm..• section a.2 complements additional evalua-tion results for event classiﬁcation and candi-date arguments extraction..• section a.3 show the hyper-parameter setting..a.1 hungarian algorithm.
the linear sum assignment problem is also knownas minimum weight matching in bipartite graphs.
a problem instance is described by a matrix c,where each ci,j is the cost of matching vertex i ofthe ﬁrst partite set (a “worker”) and vertex j of thesecond set (a “job”).
the goal is to ﬁnd a completeassignment of workers to jobs of minimal cost..formally, let x be a boolean matrix wherexi,j = 1 if row i is assigned to column j. ci,jis the cost matrix of the bipartite graph.
then theoptimal assignment has cost:.
(cid:88).
(cid:88).
min.
ci,jxi,j.
i.j.
(13).
s.t.
each row is assignment to at most one column,and each column to at most one row.
this func-tion can also solve a generalization of the classicassignment problem where the cost matrix is rect-angular.
if it has more rows than columns, thennot every row needs to be assigned to a column,and vice versa.
the method used is the hungarianalgorithm, also known as the munkres or kuhn-munkres algorithm..6308