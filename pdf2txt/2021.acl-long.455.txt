math word problem solving with explicit numerical values.
qinzhuo wu, qi zhang, zhongyu wei, xuanjing huang∗shanghai key laboratory of intelligent information processing,school of computer science, fudan university, shanghai, china(qzwu17,qz,zywei,xjhuang)@fudan.edu.cn.
abstract.
treat.
most methods.
received considerable.
the numerical values.
and ignore the prominent.
in recent years, math word problem solvinghasattention andachieved promising results, but previousmethods rarely take numerical values intoconsideration.
thenumerical values in the problems as numbersymbols,rolein solving theofproblem.
in this paper, we propose a novelapproach called nums2t, which enhancesmath word problem solving performance byexplicitly incorporating numerical values intoa sequence-to-tree network.
in addition, anumerical properties prediction mechanism isused to capture the category and comparisonand measurenumeralsinformationtheirexpressions.
inexperimental results on the math23k andape datasets demonstrate that our modelachieves better performance than existingstate-of-the-art models.
1.importance.
global.
of.
1.introduction.
taking a math word problem as input, the mathword problem solving task aims to generate a cor-responding solvable expression and answer.
withthe advancements in natural language processing,math word problem solving has received growingattention in recent years (roy and roth, 2015;mitra and baral, 2016; ling et al., 2017; huanget al., 2018).
many methods have been proposedthat use sequence-to-sequence (seq2seq) modelswith an attention mechanism (bahdanau et al.,2014) for math word problem solving (wang et al.,2017b, 2018b, 2019).
to better utilize expressionstructure information, some methods use sequence-to-tree (seq2tree) models to generate expressions.
∗ corresponding author.
1code.
availableisqinzhuowu/nums2t/.
at https://github.com/.
figure 1: example of a math word problem.
thesame problem with different numerical values maycorrespond to different math expressions.
withoutnumerical value information,the model can hardlydetermine which expression is correct..and have achieved promising results (liu et al.,2019; xie and sun, 2019; wu et al., 2020).
thesemethods convert the target expression into a binarytree, and generate a pre-order traversal sequence ofthis expression tree based on the parent and siblingnodes of each node..although promising results have been achieved,previous methods rarely take numerical values intoconsideration, despite the fact that in math wordproblem solving, numerical values provide vitalinformation.
as an inﬁnite number of numerals canappear in math word problems, it is impossible tolist them all in the vocabulary.
previous methods re-place all the numbers in the problems with numbersymbols (e.g., v1, v2) in order in the preprocessingstage.
these replaced problems are used as input.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5859–5869august1–6,2021.©2021associationforcomputationallinguistics5859problem: a school purchased several pairs of new desks and chairs for grade        students.
each desk is worth $           , and each chair is worth $          .
the price difference between the tables and the chairs is $           .
there are              more students than chairs.
how many students are there?
expression 1:  𝑣4/ (𝑣2-𝑣3)+ 𝑣5numerical expression: 100 / ( 15–10 ) + 25expression 2:  𝑣4/ (𝑣2-𝑣3)* ( 1 +𝑣5)numerical expression: 64 / (9.9–8.3) * (1+20%)expression 3:  𝑣4/ (𝑣3-𝑣2)* ( 1 +𝑣5)numerical expression: 18 / (8–6.5) * (1 + (1/3))𝑣1137159.96.5𝑣2108.38𝑣3𝑣41006418𝑣52520%(1/3)to directly generate expressions containing numbersymbols.
the number symbols in the expressionsare then replaced with the numerical values in theoriginal problems to obtain executable expressions.
as shown in figure 1, taking the problem withnumerical values {v2=15, v3=10, v4=100, v5=25}as input, the target expression of the problem wouldbe “v4/(v2 − v3) + v5”.
however, if the numbersymbol v5 = 20%, the target expression for thesame problem would be “v4/(v2 − v3) ∗ (1 + v5)”.
similarly, without numerical value information, themodel can hardly determine whether the numbergap between the table and the chair should bev2 − v3 or v3 − v2.
as such, it will incorrectlygenerates the same expression for problems withdifferent numerical values..to address these problems, we propose a novelapproach called nums2t to better capture nu-merical value information and utilize numericalproperties.
speciﬁcally, the proposed model uses asequence-to-tree network with a digit-to-digit num-ber encoder that explicitly incorporates numericalvalues into the model and captures number-awareproblem representations.
in addition, we designeda numerical properties prediction mechanism to fur-ther utilize the numerical properties.
nums2t pre-dicts the comparative relationship between pairednumerical values, determines the category of eachnumeral, and measures their importance for gen-erating the ﬁnal expression.
with the categoryand comparison information, the model can betteridentify the interactive relationship between thenumerals, and thus generate better results.
withconsideration of the importance of the numerals,the model can capture the global relationshipbetween the numerals and target expressions ratherthan simply focusing on the local relationshipbetween numeral pairs..the main contributions of this paper can be.
summarized as follows:.
• we explicitly incorporate numerical valueinformation into math word problem solvingtasks..• we propose a numerical properties predictionmechanism to utilize numerical properties.
toincorporate the local relationship between nu-merals and the global relationship associatedwith the ﬁnal expression, nums2t comparesthe paired numerical values, determines thecategory of each numeral, and then measures.
whether they should appear in the ﬁnal expres-sion..• we conducted experiments on two large-scale math23k and ape210k datasets toverify the effectiveness of our nums2t model.
the results show that our model achievedbetter performance than existing state-of-the-art methods..2 models.
in this section, we present details regarding ourproposed nums2t model.
as shown in figure 2,we use an attention-based sequence-to-tree modelwith a problem encoder (section 2.2) and a tree-structured decoder to generate math expressions(section 2.4).
in addition, we explicitly incorporatenumerical values to obtain number-aware problemrepresentations (section 2.3).
finally, we proposea numerical properties prediction mechanism tofurther utilize the numerical properties (section2.5)..2.1 problem deﬁnition.
a math word problem x = (x1, x2, .
.
.
, xm) is asequence of m words.
our goal is to generate amath expression y = (y1, y2, .
.
.
, yn), where y isthe pre-order traversal sequence of a binary mathexpression tree, which can be executed to producethe answer to problem x..here, we replace all of the numbers in the prob-lem x with a list of number symbols based on theirorder of appearance.
let vc = (v1, v2, .
.
.
, vk)be the k numbers that appear in problem x.the numerical value of the k-th number vk isa sequence of l characters (v1k).
thegenerated vocabulary vg is composed of severalcommon numbers (e.g., 1,100,π) and several mathoperators (e.g., +,-,*,/).
at each time step duringdecoding,the nums2t model either copies anumber from vc or generates a number from vg..k, .
.
.
, vl.
k, v2.
2.2 problem encoder.
we use a two-layer bidirectional lstm (bil-stm) (hochreiter and schmidhuber, 1997) net-work as the encoder, which encodes the mathword problem x into a sequence of hidden states.
5860figure 2: main structure of our nums2t model.
given a math word problem sequence, we use (a) anattention-based sequence-to-tree model to generate its math expression.
to explicitly incorporate numerical valueinformation, we use (b) a numerical values encoder to obtain the number-aware problem states hnum, which arethen concatenated with the problem hidden states in (a) to obtain number-aware problem representations hi.
inaddition, we propose (c) a numerical properties prediction mechanism for comparing the paired numerical values,determining the category of each numeral, and measuring whether they should appear in the target expression..i.h=(hx.
1, hx.
2, .
.
.
, hxm) ∈ rm×2d as follows:−→hxi ,.
←−hxi ],.
i = [.
i = bilstm(e(xi),.
i = bilstm(e(xi),.
i−1),.
−−→hx←−−hx.
i−1)..hx−→hx←−hx.
to their categories.
see wu et al.
(2020) for moredetails..the knowledge-aware problem states hkgareobtained from a two-layer graph attention network(veliˇckovi´c et al., 2018) on the entity graph:.
i.
(1).
here, word embedding vectors e(xi) are obtainedvia a wording embedding layer e(·).
d is thedimension of the hidden state and hxis theiconcatenation of the forward and backward lstmhidden states..following wu et al.
(2020), we enrich the prob-lem representations with common-sense knowl-edge information from external knowledge bases.
the words in problem sequences x and their cate-gories in external knowledge bases are constructedas an entity graph.
in this entity graph, each wordis related to its neighbor in the problem.
if thereare two nouns belonging to the same category inthe knowledge base, these two nouns are related.
αij = softmax.
( f (wt.
h [wxhx.
i : wxhx.
j ])),.
aij=1.
hkg.
i = ||.
t=1,...,t.(cid:88).
σ(.
aij=1.
αijwkhx.
j ),.
(2).
where wt.
h , wx, wk are weight vector and ma-trices.
|| and [:] are concatenation functions.
f (·)and σ are the leakyrelu and sigmoid activationfunctions.
t is the number of heads in gat layer.
if the i-th word is related to the j-th word, the scoreof the adjacent matrix aij is set to 1, otherwise it isset to 0..5861𝐡𝐯𝟏𝐡𝐯𝟐𝐡𝐯𝒌𝐡𝐯𝒌+𝟏𝐡𝐯𝑲−𝑔7≥𝑔6.5𝑔7<𝑔8𝑔7<𝑔18𝑔7≥𝑔(1/3)𝑔6.5<𝑔7−𝑔6.5<𝑔8𝑔6.5<𝑔18𝑔6.5≥𝑔(1/3)𝑔8≥𝑔7𝑔8≥𝑔6.5−𝑔8<𝑔18𝑔8≥𝑔(1/3)𝑔18≥𝑔7𝑔18≥𝑔6.5𝑔18≥𝑔8−𝑔18≥𝑔(1/3)𝑔(1/3)<𝑔7𝑔(1/3)≥𝑔6.5𝑔(1/3)<𝑔8𝑔(1/3)<𝑔18−pairwise comparison scorepairwise comparison loss𝐡𝐯𝟏𝐡𝐯𝟐𝐡𝐯𝒌𝐡𝐯𝒌+𝟏𝐡𝐯𝑲integerdecimalfractionpercentagenumeral categories losscategory𝐡𝐯𝟏𝐡𝐯𝟐𝐡𝐯𝒌𝐡𝐯𝒌+𝟏𝐡𝐯𝑲target expressionscalar value 𝑔𝑣𝑘′𝑔𝑣1′𝑔𝑣2′𝑔𝑣𝑘′𝑔𝑣𝑘+1′𝑔𝑣𝐾′global relationship loss𝑥1𝑥2𝑥𝑖𝑥𝑖+1𝑥𝑚embedding layerbi-lstmgraph attention network𝐡𝟏𝐱𝐡𝟐𝐱𝐡𝐢𝐱𝐡𝐢+𝟏𝐱𝐡𝐦𝐱𝐡𝟏𝐤𝐠𝐡𝟐𝐤𝐠𝐡𝐢𝐤𝐠𝐡𝐢+𝟏𝐤𝐠𝐡𝐦𝐤𝐠𝐡𝟏𝐧𝐡𝟐𝐧𝐡𝐢𝐧𝐡𝐢+𝟏𝐧𝐡𝐦𝐧𝐡𝟏𝐜𝐧𝐡𝟐𝐜𝐧𝐡𝐢𝐜𝐧𝐡𝐢+𝟏𝐜𝐧𝐡𝐦𝐜𝐧𝐡𝟏𝐡𝟐𝐡𝐢𝐡𝐢+𝟏𝐡𝐦attention & aggregation & copy mechanism𝑦2𝑦1𝑦3𝑦4generated expressionmath word problem*/𝑣4-𝑦1𝑦2𝑦2𝑦4𝑦3𝑦1𝑦2𝑦2𝑦3𝑦1𝑦2𝑦2𝑦1generated partial expression tree*       */          */ 𝑣4*/ 𝑣4-(b) numerical values encoder𝑣𝑘1𝑣𝑘2𝑣𝑘𝑗𝑣𝑘𝑗+1𝑣𝑘𝑙𝐡𝐯𝐤,𝟏𝐧𝐡𝐯𝐤,𝟐𝐧𝐡𝐯𝐤,𝐣𝐧𝐡𝐯𝐤,𝐣+𝟏𝐧𝐡𝐯𝐤,𝐥𝐧average pooling layer(          1         /         3         )    digit-to-digit encode𝐡𝐯𝟏𝐧𝐡𝐯𝟐𝐧𝐡𝐯𝐤𝐧𝐡𝐯𝐤+𝟏𝐧𝐡𝐯𝐊𝐧𝑣1𝑣2𝑣𝑘𝑣𝑘+1𝑣𝐾encodeencodeencodeencodeencode7       6.5      8       18    (1/3)    𝐡𝐯𝟏𝐜𝐧𝐡𝐯𝟐𝐜𝐧𝐡𝐯𝐤𝐜𝐧𝐡𝐯𝐤+𝟏𝐜𝐧𝐡𝐯𝐊𝐜𝐧self attentionnumerical valuesconcatenatereplacingnumber-aware problem states7       6.5      8       18    (1/3)    7       6.5      8       18    (1/3)    7       6.5      8       18    (1/3)    (a) sequence-to-tree model(c) numerical properties prediction mechanismrepresentations of all numerals 𝐡𝐯𝐤𝐡𝐢𝐱problem hidden states𝐡𝐢𝐤𝐠knowledge-aware problem hidden states𝐡𝐢𝐧numeral hidden states𝐡𝐢𝐜𝐧contextual numeral hidden statesnumber-aware problem representations𝐡𝐢2.3 number-aware problem representations.
2.4 tree structured decoder.
to solve the issues mentioned in the introductionsection, we need to incorporate explicit numericalvalue information into nums2t.
however, thereare an inﬁnite number of numerals that can appearin math word problems.
for example, among the18,529 problems in the training set of math23k,there are 3,058 different numerical values.
there-fore, rather than list all these numerals in thevocabulary, we encode each numeral value digitby digit..k, .
.
.
, vl.
all the digits in the numerical value vk aretreated as a sequence (v1k, v2k) and embed-ded via the embed layer e(·).
take a 5-digit valuevk = (1/3) as an example, we have e(vk) ∈r5×demb.
similar to the architecture shown inequation 1, we use a bilstm network to encodethe numeral values and obtain the numeral hiddenstates hvk with an average pooling layer:.
hn.
vk,j.
= bilstm(e(vj.
k), hn.
vk,j−1.
),.
hnvk.
=.
1l.(cid:88)l.hn.
vk,j.
..j=1.
(3).
to capture the relations and dependency betweennumeral pairs, we use a self-attention mechanism(wang et al., 2017a) on the hidden state of allthe numerals hn}kk=1 to compute thecontextual numeral hidden states hcnvk:.
v = {hnvk.
αvk = softmax( (hnhcn= αvk · hnv,vk.
v)twhhnvk.
),.
(4).
where αvk is the attention distribution of vk on allthe numerals in the problem x..combining the numeral hidden states hnwith the original problem hidden states hxwe have number-aware problem states hnumhanced with explicit numeral value information:.
vk, hcnvki , hkg,ien-.
i.hnumi.
=.
(cid:26)[hn: hcnvkvki : hkg[hx.
i.xi = vk.]
] xi is not a number.
(5).
the ﬁnal number-aware problem representationsare obtained by concatenating the problem hiddeni , the knowledge-aware problem states hkgstates hxand the number-aware problem states hnum.
:.
i.i.hi = [hx.
i : hkg.
i.: hnumi.
]..(6).
previous works (xie and sun, 2019; liu et al.,2019; wu et al., 2020) have conﬁrmed that asequence-to-tree model can better represent theexpression structures than a sequence-to-sequencemodel, because a tree structured decoder cancapture the global expression information andfocus on the features of adjacent nodes..the tree structured decoder takes the ﬁnalnumber-aware problem representations hi as inputand generates the target expression from top tobottom.
the target expression can be regarded as apre-order traversal of a binary tree, with operatorsas internal nodes and numbers as leaf nodes.
thedecoder is a one-layer lstm, which updates itsstates as follows:.
st+1 = lstm([e(yt) : ct : rt], st)..(7).
at time step t+1, the decoder uses the last generatedword embedding e(yt), the problem context statect and the expression context state rt to update itsprevious hidden state st..the problem context state ct is computed via.
attention mechanism as follows:.
αti = softmax(tanh(whhi +ws[st : rt])),.
(8).
ct =.
αtihi,.
m(cid:88).
i=1.
where wh, ws are weight matrices.
αti is theattention distribution on the number-aware problemrepresentations hi..the expression context state rt is computedvia a state aggregation mechanism (wu et al.,2020).
it describes the global representation ofthe partial expressions y<t = (y1, y2, .
.
.
, yt−1)being generated by the decoder.
at time step t,the decoder aggregates each node’s context statewith its neighbor nodes in the generated partialexpression tree.
the aggregation functions are asfollows:.
r0t = st,t = σ(wr[rηrη+1.
t : rη.
t,p : rη.
t,l : rη.
t,r]),.
(9).
where σ is the sigmoid function and wr is a weightmatrix.
r0t is initialized with decoder hidden statest when η = 0,. rt,p, rt,l, rt,r are the context stateof the parent node, the left child node, and theright child node of yt in the expression tree.
rη+1represents the expression context state updated with.
t.5862global information from all nodes in the generatedpartial expression..lastly, the decoder can generate a word froma given vocabulary vg.
it can also generate anumber symbol in vc, and use it to copy a numberfrom the problem x. the ﬁnal distribution is thecombination of the generated probability and copyprobability:.
k=1,.
hv = {hvk}kpc = σ(wz[st : ct : rt] + wvhv),pc(yt) = softmax(f ([st : ct : rt : hv])),pg(yt) = softmax(f ([st : ct : rt])),p(yt|y<t,x) = pcpc(yt) + (1−pc)pg(yt)..(10).
here, hv are the number-aware problem represen-tations of all the numerals vk in x. wz, wv are theweight matrices.
f (·) is a perception layer.
pc isthe probability that the current word is a numbercopied from the problem..2.5 numerical properties prediction.
mechanism.
our nums2t model explicitly incorporates numer-ical values information.
furthermore, utilize thenumerical properties to the degree possible througha numerical properties prediction mechanism.
weconsider three numerical properties to be useful forsolving math word problems:.
pairwise numeral comparison..if we con-sider the question “what is the difference betweenv1 and v2,” the comparative relationship betweenthese two numerals can help the model decidewhether to generate v1 − v2 or v2 − v1.
inthis paper, we compare each numeral vk in thequestion with the other numerals.
then, wecalculate the pairwise comparison scores zkj basedon their number-aware problem representations,and we optimize the pairwise comparison lossto assign numerals with larger numerical valueshigher pairwise comparison scores.
the pairwisecomparison loss lcr is calculated as follows:.
gvk = σ(whhvk),.
(cid:40).
zkj =.
max(0, gvj − gvk )max(0, gvk − gvj )k(cid:88).
k(cid:88).
zkj,.
1k2.
k=1.
j=1.
lcr = −.
if vk ≥ vjif vk < vj.
,.
(11).
numeral categories..in the sentence “thenumber of apples is 5 more than the number ofpears,” replacing the numeral 5 with the integer 100may not affect the structure of the target expression,but replacing the numeral 5 with 20% may changethe structure from “+5” to “*(1 + 20%)”.
weroughly divide all numbers into four categories:{integer, decimal, fraction, percentage}, and assigna category label c = {1,2,3,4}, respectively.
giventhe number-aware problem representations hvk foreach numeral vk, we calculate the category scoredistribution p(cvk |hvk) and then minimize thenegative log likelihood:.
p(cvk |hvk) = softmax(wchvk),k(cid:88).
lca = −.
log p(cvk |hvk)..1k.k=1.
(12).
global relationship with target expressions.
current models tend to focus on the local rela-tionship between numerals, while sometimes thesenumerals are not related to the target expression.
given “3 bags of rice weighing 60 kg,” the numeral3 is highly correlated with 60. however, if theproblem relates to the total price of the rice ratherthan the weight of each bag of rice, the numeral3 is not so important for generating the targetexpression.
the nums2t model predicts a scalarvalue g(cid:48)vk for each numeral that denotes whetherthis numeral will be used in a math expression.
the importance label avk =1 when vk is used inthe ground truth math expression, otherwise avk =0.
the supervised loss is deﬁned by:.
g(cid:48)vk.
= σ(wghvk),.
1k.k(cid:88).
k=1.
2.6 training.
lgr = −.
ai log g(cid:48)vk.
+(1−ai) log (1−g(cid:48)vk.
)..(13).
during training, for each question–expression pair(x, y), we ﬁrst train the nums2t by optimizingthe maximum likelihood estimation (mle) lossll on the probability distribution p(yt|y<t, x)).
then, the ﬁnal loss function l is a combination ofthe mle loss and three numerical properties lossfunctions:.
1n.n(cid:88).
i=1.
ll = −.
log p(yt|y<t, x)),.
(14).
l = ll + β1lcr + β2lca + β3lgr..5863here, β1, β2, β3 are hyper-parameters..math23k ape210k.
3 experiment.
3.1 dataset.
we present the experimental results of math wordproblem solving using our proposed models onthe math23k (wang et al., 2017b) and ape210k(zhao et al., 2020)2 datasets.
following xie andsun (2019), we removed the problems that thecorresponding expressions could not be executedto obtain the given answers and the problems thatomit intermediate calculation expressions.
formath23k, following previous studies (xie andsun, 2019; wu et al., 2020), we randomly splitthe dataset into a training set, a development setand a test set with 18,529, 2,316, 2,316 problems.
for ape210k, we use the ofﬁcial data partition.
there are 166,270, 4,157, and 4,159 problemsin our training set, development set and test set,respectively..we report answer accuracy as the main evalu-ation metrics of the math word problem solvingtask..3.2.implementation details.
in this paper, we truncate the problem to a maxsequence length of 150, and the expression toa max sequence length of 50. we select 4,000words that appear most frequently in the trainingset of each dataset as the vocabulary, and replacethe remaining words with a special token unk.
we initialize the word embedding with the pre-trained 300-dimension word vectors3.
the problemencoder used two external knowledge bases: cilin(mei, 1985) and hownet (dong et al., 2010).
thenumber of heads t in gat is 8. the hiddensize is 512 and the batch size is 64. we usethe adam optimizer (kingma and ba, 2014) tooptimize the models an the learning rate is 0.001.we compute the ﬁnal loss function with β1, β2, β3of 0.5. dropout (srivastava et al., 2014) is set to 0.5.models are trained in 80 epoches for the math23kdataset and 50 epoches for the ape210k dataset.
during testing, the beam size is set to 5. once allinternal nodes in the expression tree have two childnodes, the decoder stops generating the next word.
the hyper-parameters are tuned on the valid set..2https://github.com/yuantiku/ape210k3https://github.com/embedding/chinese-word-vectors.
modelsdnsdns-retrievals2srecursivenntree-decodergtska-s2tnums2t.
58.1%64.7%66.7%68.7%69.0%74.3%76.3%78.1%.
--56.6%-66.5%67.7%68.7%70.5%.
table 1: answer accuracy of our model and otherstate-of-the-art models on the math23k and ape210kdatasets..3.3 baselines.
we compare our proposed nums2t model withthe following baseline models: dns (wang et al.,2017b) is a seq2seq model with a two-layer gruas an encoder and a two-layer lstm as a decoder.
dns-retrieval is a variant of dns that combiness2s (wang et al., 2018a)a retrieval model.
is a standard bidirectional lstm-based seq2seqmodel with an attention mechanism.
recursivenn(wang et al., 2019) uses a recursive neural networkon the predicted tree structure templates tree-decoder (liu et al., 2019) is a seq2tree model witha tree structured decoder.
the decoder generateseach node based on its parent node and its siblingnode.
gts (xie and sun, 2019) generates eachnode based on its parent node and its left siblingsubtree embedding.
the subtree embedding isobtained by merging the embedding of the subtreefrom bottom to top.
ka-s2t (wu et al., 2020) isa seq2tree model with external knowledge and astate aggregation mechanism.
the decoder use atwo-layer gcn to recursively aggregate neighborsof each node in the partial expression tree..3.4 results analysis.
the main evaluation results are presented in table1. compared with baseline methods, our modelobtains the highest answer accuracy of 78.1% inthe math23k dataset and 70.5% in the ape210kdataset, which is signiﬁcantly better than otherstate-of-the-art methods.
the experimental resultsprovide the following observations:.
1) the methods with a tree-structured decoder(tree-decoder, gts, ka-s2t) perform better thanmethods with a sequence-structured decoder (dns,s2s).
these methods treat the math expression asa binary tree and directly use adjacent nodes in the.
5864tree instead of the previous word in the sequenceto generate the next word.
in this way, the modelcan better capture the structure information of themath expressions..2) the kas2t model with external knowledgeperforms better than gts, which proves thatexternal knowledge enables the model to obtainbetter interaction between words..3) nums2t outperforms all the other baselines.
this result shows the effectiveness of the explicitlyincorporated numerical values and use of a numeri-cal properties prediction mechanism..3.5 ablation study.
effect of explicitly incorporating numerical val-ues: we designed several nums2t variants thatreduce the numerical values incorporated in themodel.
here, “nums2t w/o numerals” meansthat we remove the character-level numeric valueencoder.
an input example is “alan boughtv1 apples for $ v2”.
“nums2t w/o symbols”means that we not only remove the character-levelnumeric value encoder, but also replace the mathsymbols in math problems with character-levelnumeric values.
an input example is “alan bought2 5 apples for $ 1 5 0”..table 2 shows the results of these different.
variants, from which we can see:.
1)the experimental results show that modelperformance of “nums2t w/o symbols” is sig-niﬁcantly reduced in both datasets.
we believe thisis because directly replacing the number symbolswill make it difﬁcult for the model to obtain theoverall representation of each number..2) the use of a self-attention mechanism signiﬁ-cantly improves the accuracy by 0.8% in math23kand 0.7% in ape210k.
this is because the samenumerical value may describe different informationin different problems.
therefore, the self-attentionmechanism combines numerical values with othernumerical values in the problem, which helps tomodel numerical information and the relationsbetween these numerals..3) without numerical values, the answer ac-curacy of “nums2t w/o numerals” would bereduced to 76.6% and 69.2%.
the results showthe beneﬁt of explicitly incorporating numericalvalues.
effect of the numerical properties predictionmechanism: table 3 shows the results of severalnums2t variants designed to measure the effect.
modelska-s2tnums2t w/o symbolsnums2t w/o numeralsnums2t w/o selfattnums2t.
math23k ape210k.
76.3%75.4%76.6%77.3%78.1%.
68.7%64.4%69.2%69.8%70.5%.
table 2: ablation study on reducing the numericalvalues incorporated into the model..modelska-s2tnums2t-basenums2t-base + crnums2t-base + canums2t-base + grnums2t.
math23k ape210k.
76.3%77.0%77.7%77.4%77.3%78.1%.
68.7%69.6%70.1%70.0%69.8%70.5%.
table 3: ablation study on reducing the numericalproperties used in the numerical properties predictionmechanism.
cr, ca and gr respectively indicatepairwise numeral comparison, numeral category andglobal relationship with the target expression..of the numerical properties prediction mechanism.
from the table we can observe that:.
1) nums2t-base is the variant of nums2twithout the numerical properties prediction mech-anism.
without numerical properties, the answeraccuracy in the math23k and ape210k datasetsare reduced to 77.0% and 69.6%, which showthat the numerical properties prediction mechanismcontributes considerably to improving performance.
in addition, nums2t-base still outperforms thestate-of-the-art baseline ka-s2t, which once againproves the effectiveness of explicitly incorporatingnumerical values..2) the use of pairwise numeral comparison,numeral category and global relationship with atarget expression can improve accuracy by ap-proximately 0.6%, 0.4% and 0.3%, respectively.
their combination achieves further improvementsin model performance.
these results show theeffectiveness of the numerical properties predictionmechanism because it enables the model to furtherutilize numerical properties.
model performance on problems with a differ-ent number of numerals: table 4 shows theresults for how accuracy changes as the number ofnumerals in the problem increases.
the nums2tmodel outperforms the best-performing baselinewith respect to problems with a different number of.
5865math23k.
prop.
ka-s2t nums2t imp.
(↑)2.1%2.0%80.9%0.5%36.8% 84.6%1.0%46.1% 77.4%2.3%11.4% 58.3%9.7%45.2%2.8%13.4%33.3%0.7%25.0%12.5%0.3%ape210k.
83.0%85.1%78.4%60.6%54.9%46.7%37.5%.
prop.
ka-s2t nums2t imp.
(↑)3.5%9.1%67.9%0.9%34.4% 74.6%3.4%36.9% 72.2%4.2%12.7% 53.2%4.6%30.1%3.6%13.5%40.7%1.4%8.9%19.0%1.9%.
71.4%75.5%75.6%57.4%43.7%54.2%27.9%.
num.
≤123456≥ 7.num.
≤123456≥ 7.table 4: model performance on problems with adenotes thedifferent number of numerals.
prop.
proportion of these problems in the dataset.
imp.
denotes the accuracy improvement between nums2tand ka-s2t..numerals.
in addition, as the number of numeralsin the problems increase, the performance gapbetween nums2t and kas2t also increases.
thisis because with more numerals in the problem,nums2t, which explicitly incorporate numericalvalue information, is able to more readily achievebetter performance.
meanwhile, nums2t alsoachieved a considerable improvement on problemswith only one numeral.
this further demonstratesthe effect of utilizing numerical category informa-tion and global relationship information..3.6 case study.
table 5 shows three cases generated by ka-s2t(wu et al., 2020) and our nums2t model.
in theﬁrst problem, without numerical values, ka-s2tincorrectly uses the smaller value to subtract thelarger value when calculating the price differencebetween footballs and basketballs.
this caserequires the model to choose the larger valuebetween two numerals.
our nums2t modelcan better handle this problem.
in the secondproblem, ka-s2t replaces all of the numerals inthe problems with number symbols (v1, v2) anddoes not know that v2=20% is not an integer.
ourproposed method can capture numerical valuesand numeral category information to generate.
problem: each football is worth $ 76, and eachbasketball is worth $ 45. the schoolbought the same number of basketballsand footballs, with a price differenceof $ 248. how many footballs did theschool buy?
248/(45-76)248/(76-45).
ka-s2t:nums2t:problem: there are 250 pear trees in the orchard,.
25% more than peach trees.
there are3 times as many orange trees as peartrees.
how many more orange trees arethere than peach trees?
(250*3)-(250-25%)(250*3)-250/(1-25%).
ka-s2t:nums2t:problem: the concert was held in a hall with 80seats.
52 tickets have been sold, eachpriced at $ 25. how much is the ticketrevenue?
(80-52)*2552*25.ka-s2t:nums2t:.
table 5: three cases of generated expressions by ka-s2t (wu et al., 2020) and nums2t..correct results.
in the third problem, 80 seatsand 52 tickets are strongly semantically related,so ka-s2t generates the sub-expression “80-52”.
however, this problem is about the fares that havealready been sold rather than how many tickets areleft.
with numerical properties, nums2t is able torealize that 80 is not related to the target expressionand should not appear in the generated result..4 related work.
math word problem solving: in recent years,seq2seq (sutskever et al., 2014) has been widelyused in math word problem solving tasks (linget al., 2017; wang et al., 2017b, 2018a).
to betterutilize expression structure information, recentstudies have used seq2tree models (liu et al.,2019; zhang et al., 2020a).
xie and sun (2019)proposed a tree structured decoder that uses agoal-driven approach to generate expression trees.
wu et al.
(2020) proposed a knowledge-awareseq2tree model with a state aggregation mech-anism that incorporates common-sense knowledgefrom external knowledge bases.
recently, severalmethods have attempted to use the contextualinformation of the numbers in the problem.
liet al.
(2019) propose a group attention mechanismto extract quantity-related features and quantity-pair features.
zhang et al.
(2020b) connects each.
5866number in the problem with nearby nouns to enrichthe problem representations..however, these methods rarely take numericalvalues into consideration.
they replace all thenumbers in the problems with number symbolsand ignore the vital information provided by thenumerical values in math word problem solving.
as such, these methods will incorrectly generatesthe same expression for problems with differentnumerical values.
numerical value representations: some re-cent studies have explored the numerical valuerepresentations in language models (naik et al.,2019; chen et al., 2019; wallace et al., 2019).
spithourakis and riedel (2018) investigated severalof the strategies used for language models for theirpossible application to model numerals.
gong et al.
(2020) proposed the use of contextual numericalvalue representations to enhance neural contentplanning by helping models to understand datavalues.
to incorporate numerical value informationinto math word solving tasks, we use a digit-to-digit numerical value encoder to obtain the number-aware problem representations.
to further utilizethe numerical properties, we propose a numericalproperties prediction mechanism..5 conclusion.
in this study, we proposed a novel approach callednums2t, that better captures numerical valueinformation and utilizes numerical properties.
inthis model, we use a digit-to-digit numerical valueencoder to explicitly incorporate numerical values.
in addition, we designed a numerical propertiesprediction mechanism that compares the pairednumerical values, determines the category of eachnumeral, and measures whether they should appearin the ﬁnal expression.
experimental results showthat our proposed nums2t model outperformsother state-of-the-art baseline methods..acknowledgments.
the authors wish to thank the anonymous reviewersfor their helpful comments.
this work was partiallyfunded by china national key r&d program(no.
2018yfb1005100), national natural sciencefoundation of china (no.
62076069, 61976056),shanghai municipal science and technology ma-jor project (no.2021shzdzx0103)..references.
dzmitry bahdanau, kyunghyun cho, and yoshuabengio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..chung-chi chen, hen-hsen huang, hiroya takamura,and hsin-hsi chen.
2019. numeracy-600k: learn-ing numeracy for detecting exaggerated informationin proceedings of the 57thin market comments.
annual meeting of the association for computa-tional linguistics, pages 6307–6313, florence, italy.
association for computational linguistics..zhendong dong, qiang dong, and changling hao.
2010. hownet and its computation of meaning.
incoling 2010: demonstrations, pages 53–56, beijing,china.
coling 2010 organizing committee..heng gong, wei bi, xiaocheng feng, bing qin,xiaojiang liu, and ting liu.
2020. enhancing con-tent planning for table-to-text generation with datain findings of theunderstanding and veriﬁcation.
association for computational linguistics: emnlp2020, pages 2905–2914, online.
association forcomputational linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural computation, 9:1735–80..danqing huang, jin-ge yao, chin-yew lin, qingyuzhou, and jian yin.
2018. using intermediateinrepresentations to solve math word problems.
proceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 419–428, melbourne, australia.
association for computational linguistics..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..jierui li, lei wang,.
jipeng zhang, yan wang,bing tian dai, and dongxiang zhang.
2019.modeling intra-relation in math word problemsinwith different functional multi-head attentions.
thethe 57th annual meeting ofproceedings ofassociation for computational linguistics, pages6162–6167, florence, italy.
association for compu-tational linguistics..wang ling, dani yogatama, chris dyer, and philblunsom.
2017. program induction by rationalegeneration: learning to solve and explain algebraicword problems.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 158–167,vancouver, canada.
association for computationallinguistics..qianying liu, wenyv guan, sujian li, and daisukekawahara.
2019.tree-structured decoding forsolving math word problems.
in proceedings of the2019 conference on empirical methods in natural.
5867language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2370–2379, hong kong,china.
association for computational linguistics..jiaju mei.
1985.chubanshe..tongyi ci cilin..shangai cishu.
arindam mitra and chitta baral.
2016. learning to useformulas to solve simple arithmetic problems.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2144–2153, berlin, germany.
association for computational linguistics..aakanksha naik, abhilasha ravichander, carolynrose, and eduard hovy.
2019. exploring numeracyin proceedings of the 57thin word embeddings.
annual meeting of the association for computa-tional linguistics, pages 3374–3380, florence, italy.
association for computational linguistics..subhro roy and dan roth.
2015. solving generalin proceedings ofarithmetic word problems.
the 2015 conference on empirical methods innatural language processing, pages 1743–1752,lisbon, portugal.
association for computationallinguistics..georgios spithourakis and sebastian riedel.
2018.numeracy for language models: evaluating andinimproving their ability to predict numbers.
thethe 56th annual meeting ofproceedings ofassociation for computational linguistics (volume1: long papers), pages 2104–2115, melbourne,australia.
association for computational linguis-tics..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksjournal of machine learningfrom overﬁtting.
research, 15(56):1929–1958..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
information processingin advances in neuralsystems, volume 27. curran associates, inc..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..eric wallace, yizhong wang, sujian li, sameer singh,and matt gardner.
2019. do nlp models knownumbers?
probing numeracy in embeddings.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on naturallanguage processing (emnlp-ijcnlp), pages5307–5315, hong kong, china.
association forcomputational linguistics..lei wang, yan wang, deng cai, dongxiang zhang,translating a mathand xiaojiang liu.
2018a.
word problem to a expression tree.
in proceedingsof the 2018 conference on empirical methods innatural language processing, pages 1064–1069,brussels, belgium.
association for computationallinguistics..lei wang, dongxiang zhang, lianli gao, jingkuansong, long guo, and heng tao shen.
2018b.
mathdqn: solving arithmetic word problems viadeep reinforcement learning..lei wang, dongxiang zhang, jipeng zhang, xingxu, lianli gao, bing tian dai, and heng shen.
2019. template-based math word problem solverswith recursive neural networks.
proceedings of theaaai conference on artiﬁcial intelligence, 33:7144–7151..wenhui wang, nan yang, furu wei, baobao chang,and ming zhou.
2017a.
gated self-matchingnetworks for reading comprehension and questionin proceedings of the 55th annualanswering.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 189–198,vancouver, canada.
association for computationallinguistics..yan wang, xiaojiang liu, and shuming shi.
2017b.
deep neural solver for math word problems.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages845–854, copenhagen, denmark.
association forcomputational linguistics..qinzhuo wu, qi zhang, jinlan fu, and xuanjinghuang.
2020. a knowledge-aware sequence-to-treenetwork for math word problem solving.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages7137–7146, online.
association for computationallinguistics..zhipeng xie and shichao sun.
2019. a goal-driventree-structured neural model for math word prob-in proceedings of the twenty-eighth inter-lems.
national joint conference on artiﬁcial intelligence,ijcai-19, pages 5299–5305.
international jointconferences on artiﬁcial intelligence organization..jipeng zhang, roy ka-wei lee, ee-peng lim, weiqin, lei wang, jie shao, and qianru sun.
2020a.
teacher-student networks with multiple decodersin proceedingsfor solving math word problem.
of the twenty-ninth international joint conferenceon artiﬁcial intelligence, ijcai-20, pages 4011–4017. international joint conferences on artiﬁcialintelligence organization.
main track..jipeng zhang, lei wang, roy ka-wei lee, yi bin, yanwang, jie shao, and ee-peng lim.
2020b.
graph-to-tree learning for solving math word problems.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages.
58683928–3937, online.
association for computationallinguistics..wei zhao, mingyue shang, yang liu, liang wang, andjingming liu.
2020. ape210k: a large-scale andtemplate-rich dataset of math word problems.
arxivpreprint arxiv:2009.11506..5869