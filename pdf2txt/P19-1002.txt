incremental transformer with deliberation decoderfor document grounded conversationszekang li†♦, cheng niu‡, fandong meng‡∗, yang feng♦, qian li♠, jie zhou‡†dian group, school of electronic information and communicationshuazhong university of science and technology‡pattern recognition center, wechat ai, tencent inc, china♦key laboratory of intelligent information processinginstitute of computing technology, chinese academy of sciences♠school of computer science and engineering, northeastern university, chinazekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.comfengyang@ict.ac.cn, qianli@stumail.neu.edu.cn.
abstract.
document grounded conversations is a taskto generate dialogue responses when chattingabout the content of a given document.
ob-viously, document knowledge plays a criticalrole in document grounded conversations,while existing dialogue models do not exploitthis kind of knowledge effectively enough.
inthis paper, we propose a novel transformer-based architecture for multi-turn documentgrounded conversations.
in particular, we de-vise an incremental transformer to encodemulti-turn utterances along with knowledgein related documents.
motivated by the hu-man cognitive process, we design a two-passdecoder (deliberation decoder) to improvecontext coherence and knowledge correctness.
our empirical study on a real-world documentgrounded dataset proves that responses gen-erated by our model signiﬁcantly outperformcompetitive baselines on both context coher-ence and knowledge relevance..1.introduction.
past few years have witnessed the rapid develop-ment of dialogue systems.
based on the sequence-to-sequence framework (sutskever et al., 2014),most models are trained in an end-to-end man-ner with large corpora of human-to-human di-alogues and have obtained impressive success(shang et al., 2015; vinyals and le, 2015; li et al.,2016; serban et al., 2016).
while there is stilla long way for reaching the ultimate goal of di-alogue systems, which is to be able to talk likehumans.
and one of the essential intelligenceto achieve this goal is the ability to make use ofknowledge..∗∗ fandong meng is the corresponding author of the pa-per.
this work was done when zekang li was interning atpattern recognition center, wechat ai, tencent..there are several works on dialogue sys-tems exploiting knowledge.
the mem2seq(madotto et al., 2018) incorporates structuredknowledge into the end-to-end task-oriented di-alogue.
liu et al.
(2018) introduces fact-matching and knowledge-diffusion to generatemeaningful, diverse and natural responses usingstructured knowledge triplets.
ghazvininejadet al.
(2018), parthasarathi and pineau (2018),yavuz et al.
(2018), dinan et al.
(2018) andlo and chen (2019) apply unstructured text factsin open-domain dialogue systems.
these worksmainly focus on integrating factoid knowledgeinto dialogue systems, while factoid knowledgerequires a lot of work to build up, and is onlylimited to expressing precise facts.
documents asa knowledge source provide a wide spectrum ofknowledge, including but not limited to factoid,event updates, subjective opinion, etc.
recently,intensive research has been applied on usingdocuments as knowledge sources for question-answering (chen et al., 2017; huang et al., 2018;yu et al., 2018; rajpurkar et al., 2018; reddyet al., 2018)..the document grounded conversation is a taskto generate natural dialogue responses when chat-ting about the content of a speciﬁc document.
thistask requires to integrate document knowledgewith the multi-turn dialogue history.
differentfrom previous knowledge grounded dialogue sys-tems, document grounded conversations utilizedocuments as the knowledge source, and henceare able to employ a wide spectrum of knowl-edge.
and the document grounded conversationsis also different from document qa since the con-textual consistent conversation response should begenerated.
to address the document groundedconversation task, it is important to: 1) exploitdocument knowledge which are relevant to the.
proceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages12–21florence,italy,july28-august2,2019.c(cid:13)2019associationforcomputationallinguistics12conversation; 2) develop a uniﬁed representationcombining multi-turn utterances along with therelevant document knowledge..in this paper, we propose a novel and effec-tive transformer-based (vaswani et al., 2017) ar-chitecture for document grounded conversations,named incremental transformer with deliberationdecoder.
the encoder employs a transformer ar-chitecture to incrementally encode multi-turn his-tory utterances, and incorporate document knowl-edge into the the multi-turn context encoding pro-cess.
the decoder is a two-pass decoder similarto the deliberation network in neural machinetranslation (xia et al., 2017), which is designedto improve the context coherence and knowledgecorrectness of the responses.
the ﬁrst-pass de-coder focuses on contextual coherence, while thesecond-pass decoder reﬁnes the result of the ﬁrst-pass decoder by consulting the relevant documentknowledge, and hence increases the knowledgerelevance and correctness.
this is motivated byhuman cognition process.
in real-world humanconversations, people usually ﬁrst make a draft onhow to respond the previous utterance, and thenconsummate the answer or even raise questions byconsulting background knowledge..we test the effectiveness of our proposed modelon document grounded conversations dataset(zhou et al., 2018).
experiment results show thatour model is capable of generating responses ofmore context coherence and knowledge relevance.
sometimes document knowledge is even well usedto guide the following conversations.
both auto-matic and manual evaluations show that our modelsubstantially outperforms the competitive base-lines..our contributions are as follows:.
• we build a novel incremental transformerto incrementally encode multi-turn utteranceswith document knowledge together..• we are the ﬁrst to apply a two-pass decoderto generate responses for document groundedconversations.
two decoders focus on con-text coherence and knowledge correctness re-spectively..2 approach.
figure 1: the framework of incremental transformerwith deliberation decoder for document groundedconversations..i.i.i., ..., u(k).
formally, let u = u(1), ..., u(k), ..., u(k) be awhole conversation composed of k utterances.
we use u(k) = u(k)1 , ..., u(k)to denotethe k-th utterance containing i words, where u(k)denotes the i-th word in the k-th utterance.
foreach utterance u(k), likewise, there is a speciﬁedrelevant document s(k) = s(k), ..., s(k)j ,which represents the document related to the k-th utterance containing j words.
we deﬁne thedocument grounded conversations task as gen-erating a response u(k+1) given its related doc-ument s(k+1) and previous k utterances u≤kwith related documents s≤k, where u≤k =u(1), ..., u(k) and s≤k = s(1), ..., s(k).
note thats(k), s(k+1), ..., s(k+n) may be the same..1 , ..., s(k).
j.therefore, the probability to generate the re-.
sponse u(k+1) is computed as:.
p (u(k+1)|u≤k, s≤k+1; θ)= (cid:81)i.i=1 p (uk+1.
i.
|u≤k, s≤k+1, u(k+1).
<i.
; θ).
(1).
where u(k+1).
<i = u(k+1).
1., ..., u(k+1).
i−1.
..2.1 problem statement.
2.2 model description.
our goal is to incorporate the relevant doc-ument knowledge into multi-turn conversations..figure 1 shows the framework of the proposedincremental transformer with deliberation de-.
13utterance k-1utterance k-2utterance kdocument k-2incrementaltransformerincrementaltransformerincrementaltransformersecond-pass decoderself-attentive encoderself-attentive encoderdocument k-1self-attentive encoderdocument kself-attentiveencoder(cid:335)(cid:335)(cid:335)utterance k+1first-pass decoderdocument k+1self-attentiveencoderfirst-pass outputself-attentiveencoderdeliberation decoderincremental transformer encoderfigure 2: (1) detailed architecture of model components.
(a) the self-attentive encoder(sa).
(b) incrementaltransformer (ite).
(c) deliberation decoder (dd).
(2) simpliﬁed version of our proposed model used to verifythe validity of our proposed incremental transformer encoder and deliberation decoder.
(d) knowledge-attentiontransformer(kat).
(e) context-knowledge-attention decoder (ckad)..coder.
please refer to figure 2 (1) for more details.
it consists of three components:.
2017) to compute the representation of documentknowledge..1) self-attentive encoder (sa) (in orange) isa transformer encoder as described in (vaswaniet al., 2017), which encodes the document knowl-edge and the current utterance independently..2) incremental transformer encoder (ite) (onthe top) is a uniﬁed transformer encoder which en-codes multi-turn utterances with knowledge repre-sentation using an incremental encoding scheme.
this module takes previous utterances u(i) and thedocument s(i)’s sa representation as input, anduse attention mechanism to incrementally build upthe representation of relevant context and docu-ment knowledge..3) deliberation decoder (dd) (on the bottom)is a two-pass uniﬁed transformer decoder for bet-ter generating the next response.
the ﬁrst-pass de-coder takes current utterance u(k)’s sa representa-tion and ite output as input, and mainly relies onconversation context for response generation.
thesecond-pass decoder takes the sa representationof the ﬁrst pass result and the relevant documents(k+1)’s sa representation as input, and uses doc-ument knowledge to further reﬁne the response..self-attentive encoder.
as document knowledge often includes severalit’s important to capture long-rangesentences,dependencies and identify relevant information.
we use multi-head self-attention (vaswani et al.,.
as shown in figure 2 (a), we use a self-attentive encoder to compute the representationof the related document knowledge s(k).
the in-put (in(k)s ) of the encoder is a sequence of docu-ment words embedding with positional encodingadded.
(vaswani et al., 2017):.
in(k).
1 , ..., s(k)s = [s(k)j ]s(k)j = esj + pe(j).
(2).
(3).
and.
is the word embedding of s(k)where esjjpe(·) denotes positional encoding function..the self-attentive encoder contains a stack ofnx identical layers.
each layer has two sub-layers.
the ﬁrst sub-layer is a multi-head self-attention (multihead) (vaswani et al., 2017).
multihead(q, k, v) is a multi-head attentionfunction that takes a query matrix q, a key ma-trix k, and a value matrix v as input.
in cur-rent case, q = k = v. that’s why it’s calledself-attention.
and the second sub-layer is a sim-ple, position-wise fully connected feed-forwardnetwork (ffn).
this ffn consists of two lin-ear transformations with a relu activation in be-tween.
(vaswani et al., 2017).
a(1) = multihead(in(k).
s , in(k)s ).
(4).
s , in(k)d(1) = ffn(a(1)).
ffn(x) = max(0, xw1 + b1)w2 + b2.
(5).
(6).
14utteranceembeddingknowledgeattentionself-attentioncontextattentionfeed-forwardtargetembeddingself-attentioncontextattentionutteranceattentionfeed-forwardtargetembeddingself-attentionknowledgeattentionfirst-passattentionfeed-forwarddocument/utteranceembeddingfeed-forwardself-attentiontargetembeddingself-attentioncontextattentionknowledgeattentionfeed-forward(a)(c)(d)(e)utteranceembeddingknowledgeattentionself-attentionfeed-forward(b)softmaxsoftmaxsoftmax(1)(2)where a(1) is the hidden state computed by multi-head attention at the ﬁrst layer, d(1) denotes therepresentation of s(k) after the ﬁrst layer.
notethat residual connection and layer normalizationare used in each sub-layer, which are omitted inthe presentation for simplicity.
please refer to(vaswani et al., 2017) for more details.
for each layer, repeat this process:a(n) = multihead(d(n−1), d(n−1), d(n−1)).
d(n) = ffn(a(n)).
(7)(8).
where n = 1, ..., ns and d(0) = in(k)s ..we use sas(·) to denote this whole process:d(k) = d(nx) = sas(s(k))(9)where d(k) is the ﬁnal representation for the docu-ment knowledge s(k)..i.
1 , ..., u(k).
similarly,u = [u(k).
for each utterance u(k), we usein(k)] to represent the sequenceof the position-aware word embedding.
then thesame self-attentive encoder is used to computethe representation of current utterance u(k), andwe use sau(u(k)) to denote this encoding result.
the self-attentive encoder is also used to encodethe document s(k+1) and the ﬁrst pass decoding re-sults in the second pass of the decoder.
note thatsas and sau have the same architecture but dif-ferent parameters.
more details about this will bementioned in the following sections..incremental transformer encoderto encode multi-turn document grounded ut-terances effectively, we design an incrementaltransformer encoder.
incremental transformeruses multi-head attention to incorporate documentknowledge and context into the current utterance’sencoding process.
this process can be stated re-cursively as follows:.
c(k) = ite(c(k−1), d(k), in(k)u )(10)where ite(·) denotes the encoding function, c(k)denotes the context state after encoding utteranceu(k), c(k−1) is the context state after encoding lastutterance u(k−1), d(k) is the representation of doc-ument s(k) and in(k)is the embedding of currentuutterance u(k)..as shown in figure 2 (b), we use a stack of nuidentical layers to encode u(k).
each layer consistsof four sub-layers.
the ﬁrst sub-layer is a multi-head self-attention:.
b(n) = multihead(c(n−1), c(n−1), c(n−1)).
(11).
where n = 1, ..., nu, c(n−1) is the output of thelast layer and c(0) = in(k)u .
the second sub-layeris a multi-head knowledge attention:.
e(n) = multihead(b(n), d(k), d(k)).
(12).
the third sub-layer is a multi-head context atten-tion:.
f(n) = multihead(e(n), c(k−1), c(k−1)).
(13)where c(k−1) is the representation of the previousutterances.
that’s why we called the encoder ”in-cremental transformer”.
the fourth sub-layer isa position-wise fully connected feed-forward net-work:.
c(n) = ffn(f(n)).
(14).
we use c(k) to denote the ﬁnal representation atnu-th layer:.
c(k) = c(nu).
(15).
deliberation decoder.
motivated by the real-world human cognitive pro-cess, we design a deliberation decoder contain-ing two decoding passes to improve the knowledgerelevance and context coherence.
the ﬁrst-passdecoder takes the representation of current utter-ance sau(u(k)) and context c(k) as input and fo-cuses on how to generate responses contextual co-herently.
the second-pass decoder takes the rep-resentation of the ﬁrst-pass decoding results andrelated document s(k+1) as input and focuses onincreasing knowledge usage and guiding the fol-lowing conversations within the scope of the givendocument..when generating the i-th response word u(k+1),ias inputto denoteas following:.
we have the generated words u(k+1)(vaswani et al., 2017).
we use in(k+1)rthe matrix representation of u(k+1)= [u(k+1)0.
<i, ..., u(k+1).
, u(k+1)1.
(16).
i−1.
<i.
].
is the vector representation of.
in(k+1)rwhere u(k+1)0sentence-start token..as shown in figure 2 (c),.
the deliberationdecoder consists of a ﬁrst-pass decoder and asecond-pass decoder.
these two decoders havethe same architecture but different input for sub-layers.
both decoders are composed of a stackof ny identical layers.
each layer has four sub-layers.
for the ﬁrst-pass decoder, the ﬁrst sub-layer is a multi-head self-attention:1 = multihead(r(n−1).
, r(n−1)1., r(n−1)1.g(n).
1.)
(17).
15r.is the output of the.
the second.
where n = 1, ..., ny, r(n−1)1previous layer, and r(0)sub-layer is a multi-head context attention:1 , c(k), c(k)).
1 = multihead(g(n).
1 = in(k+1).
(18)where c(k) is the representation of context u≤k.
the third sub-layer is a multi-head utterance at-tention:.
h(n).
m(n).
1 = multihead(h(n).
1 , sau(u(k)),sau(u(k))).
(19)where sau(·) is a self-attentive encoder whichencodes latest utterance u(k).
eq.
(18) mainly en-codes the context and document knowledge rele-vant to the latest utterance, while eq.
(19) encodesthe latest utterance directly.
we hope optimal per-formance can be achieved by combining both..the fourth sub-layer is a position-wise fully.
connected feed-forward network:1 = ffn(m(n)r(n)1 )after ny layers, we use softmax to get the wordsprobabilities decoded by ﬁrst-pass decoder:.
(20).
p (ˆu(k+1)(1).)
= softmax(r(ny).
).
1.
(21).
is the response decoded by the ﬁrst-.
(1).
where ˆu(k+1)pass decoder.
for second-pass decoder:, r(n−1)2.
2 = multihead(r(n−1).
g(n).
h(n).
2 = multihead(g(n)m(n).
2 = multihead(h(n).
2., r(n−1)2.)
(22)2 , d(k+1), d(k+1)) (23)2 , sau(ˆu(k+1)sau(ˆu(k+1).
(1).
),.
(1).
r(n).
2 = ffn(m(n)2 ).
p (ˆu(k+1)(2).)
= softmax(r(ny).
).
2.))
(24)(25).
(26).
2.is the counterpart to r(n−1).
where r(n−1)in passtwo decoder, referring to the output of the previ-ous layer.
d(k+1) is the representation of docu-ment s(k+1) using self-attentive encoder, ˆu(k+1)is the output words after the second-pass decoder..(2).
1.training.
in contrast to the original deliberation network(xia et al., 2017), where they propose a com-plex joint learning framework using monte carlomethod, we minimize the following loss as xionget al.
(2018) do:.
lmle = lmle1 + lmle2.
(27).
lmle1 = −.
log p (ˆu(k+1).
).
(1)i.lmle2 = −.
log p (ˆu(k+1).
).
(2)i.k(cid:88).
i(cid:88).
k=1.
i=1.
k(cid:88).
i(cid:88).
k=1.
i=1.
(28).
(29).
3 experiments.
3.1 dataset.
we evaluate our model using the documentgrounded conversations dataset (zhou et al.,2018).
there are 72922 utterances for training,3626 utterances for validation and 11577 utter-ances for testing.
the utterances can be either ca-sual chats or document grounded.
note that weconsider consequent utterances of the same per-son as one utterance.
for example, we consider a:hello!
b: hi!
b: how’s it going?
as a: hello!
b: hi!
how’s it going?.
and there is a relateddocument given for every several consequent ut-terances, which may contain movie name, casts,introduction, ratings, and some scenes.
the aver-age length of documents is about 200. please referto (zhou et al., 2018) for more details..3.2 baselines.
we compare our proposed model with the fol-.
lowing state-of-the-art baselines:models not using document knowledge:.
seq2seq: a simple encoder-decoder model(shang et al., 2015; vinyals and le, 2015) withglobal attention (luong et al., 2015).
we concate-nate utterances context to a long sentence as input.
hred: a hierarchical encoder-decoder model(serban et al., 2016), which is composed ofa word-level lstm for each sentence and asentence-level lstm connecting utterances..transformer: the state-of-the-art nmt modelbased on multi-head attention (vaswani et al.,2017).
we concatenate utterances context to along sentence as its input.
models using document knowledge:.
seq2seq (+knowledge) and hred (+knowl-edge) are based on seq2seq and hred respec-tively.
they both concatenate document knowl-edge representation and last decoding output em-bedding as input when decoding.
please refer to(zhou et al., 2018) for more details..wizard transformer: a transformer-basedmodel for multi-turn open-domain dialogue withunstructured text facts (dinan et al., 2018).
it con-catenates context utterances and text facts to a long.
16bleu(%) fluency relevance coherence.
knowledge.
context.
modelseq2seq without knowledgehred without knowledgetransformer without knowledgeseq2seq (+knowledge)hred (+knowledge)wizard transformerite+dd (ours)ite+ckad (ours)kat (ours).
ppl80.9380.8487.3278.4779.1270.3015.1164.9765.36.
0.380.430.360.390.770.660.950.860.58.
1.621.251.601.501.561.621.671.681.58.
0.180.180.290.220.350.470.560.500.33.
0.540.300.670.610.470.560.900.820.78.table 1: automatic evaluation and manual evaluation results for baselines and our proposed models..modelwizardite+ckadite+dd.
knowledge.
context.
relevance(%) coherence(%).
64/25/1167/16/1764/16/20.
58/28/1440/37/2338/34/28.
table 2: the percent(%) of score (0/1/2) of knowledgerelevance and context coherence for wizard trans-former, ite+ckad and ite+dd..sequence as input.
we replace the text facts withdocument knowledge..here, we also conduct an ablation study to il-lustrate the validity of our proposed incrementaltransformer encoder and deliberation decoder..it uses.
ite+ckad:.
incremental trans-former encoder (ite) as encoder and context-knowledge-attention decoder (ckad) as shownin figure 2 (e).
this setup is to test the validity ofthe deliberation decoder..knowledge-attention transformer (kat):as shown in figure 2 (d), the encoder of thisis a simpliﬁed version of incrementalmodeltransformer encoder (ite), which doesn’t havecontext-attention sub-layer.
we concatenate ut-terances contextto a long sentence as its in-put.
the decoder of the model is a simpliﬁedcontext-knowledge-attention decoder (ckad).
it doesn’t have context-attention sub-layer either.
this setup is to test how effective the context hasbeen exploited in the full model..3.3 experiment setup.
we use opennmt-py1 (klein et al., 2017) asthe code framework2.
for all models, the hiddensize is set to 512. for rnn-based models (seq2seq,hred), 3-layer bidirectional lstm (hochreiter.
and schmidhuber, 1997) and 1-layer lstm is ap-plied for encoder and decoder respectively.
fortransformer-based models, the layers of both en-coder and decoder are set to 3. the number ofattention heads in multi-head attention is 8 andthe ﬁlter size is 2048. the word embedding isshared by utterances, knowledge and generated re-sponses.
the dimension of word embedding is setto 512 empirically.
we use adam (kingma andba, 2014) for optimization.
when decoding, beamsize is set to 5. we use the previous three utter-ances and its related documents as input..3.4 evaluation metrics.
automatic evaluation: we adopt perplexity(ppl) and bleu (papineni et al., 2002) to au-tomatically evaluate the response generation per-formance.
models are evaluated using perplexityof the gold response as described in (dinan et al.,2018).
lower perplexity indicates better perfor-mance.
bleu measures n-gram overlap betweena generated response and a gold response.
how-ever, since there is only one reference for each re-sponse and there may exist multiple feasible re-sponses, bleu scores are extremely low.
wecompute bleu score by the multi-bleu.perl3manual evaluation: manual evaluations are es-sential for dialogue generation.
we randomlysampled 30 conversations containing 606 utter-ances from the test set and obtained 5454 utter-ances from the nine models.
we have annotatorsscore these utterances given its previous utterancesand related documents.
we deﬁned three metrics- ﬂuency, knowledge relevance (liu et al., 2018)and context coherence for manual evaluation.
allthese metrics are scored 0/1/2..ﬂuency: whether the response is natural and.
1https://github.com/opennmt/opennmt-py2the code and models are available at https://.
github.com/lizekang/itdd.
3https://github.com/google/seq2seq/blob/master/bin/tools/multi-bleu.perl.
17documentmoviename despicable me ... cast: steve carell as gru ... rotten tomatoes: 81% ... it is the debutﬁlm of illumination entertainment.
... gru, a supervillain, is disheartened when an unknownsupervillain steals the great pyramid of giza.
gru, with the assistance of his colleague dr. nefarioand his minions, resolves to one-up this mystery rival by shrinking and stealing the moon.
as itwould be costly to steal the moon, gru seeks a loan from the bank of evil.
... upon learning thatan up-and-coming villain named vector was responsible for the pyramid theft ...utterance 1 what do you think about despicable me?.
ite+dd i think it is an awesome movie, what about you?.
wizard i think it’s a great movie!.
utterance 2.i rather liked it.
it wasn’t what i was expecting but it was still good..ite+dd yeah, it makes me laugh, even when they make a movie about it..wizard it’s a great movie for kids and adults..utterance 3.i like it too.
funny and heart warming at the same time..ite+dd i think it’s interesting that gru decided to take a loan from the bank of evil..wizard i think they should have made a sequel..utterance 4.gru was a really interesting character, too.
used for gru..i liked steve carell and the voice he.
ite+dd yeah, now that you mention it, what did you think of the minions?.
wizard i think the entire cast was great..utterance 5.yes, he did an excellent job portraying the character.
81% in ratings i think is great..ite+dd yes, 81% is a great score!.
wizard i also liked the villain of the movie..table 3: responses generated by incremental transformer with deliberation decoder(ite+dd) and the wizardtransformer(wizard).
these utterances are from continuous dialogues.
ite+dd and wizard generate responsesusing context utterances and a given document.
note that utterance i + 1 is the gold response of utterance i..ﬂuent.
score 0 represents not ﬂuent and incom-prehensible; 1 represents partially ﬂuent but stillcomprehensible; 2 represents totally ﬂuent..obtains the highest ﬂuency of 1.68 and ite+ddhas the highest knowledge relevance of 0.56 andhighest context coherence of 0.90..knowledge relevance: whether the responseuses relevant and correct knowledge.
score 0 rep-resents no relevant knowledge; 1 represents con-taining relevant knowledge but not correct; 2 rep-resents containing relevant knowledge and correct.
context coherence: whether the response iscoherent with the context and guides the follow-ing utterances.
score 0 represents not coherent orleading the dialogue to an end; 1 represents coher-ent with the utterance history but not guiding thefollowing utterances; 2 represents coherent withutterance history and guiding the next utterance..3.5 experimental results.
table 1 shows the automatic and manual evalu-ation results for both the baseline and our models.
in manual evaluation, among baselines, wizardtransformer and rnn without knowledge havethe highest ﬂuency of 1.62 and wizard obtains thehighest knowledge relevance of 0.47 while trans-former without knowledge gets the highest contextcoherence of 0.67. for all models, ite+ckad.
in automatic evaluation, our proposed modelhas lower perplexity and higher bleu scores thanbaselines.
for bleu, hred with knowledge ob-tains the highest bleu score of 0.77 among thebaselines.
and ite+dd gets 0.95 bleu score,which is the highest among all the models.
forperplexity, wizard transformer obtains the low-est perplexity of 70.30 among baseline models andite+dd has remarkably lower perplexity of 15.11than all the other models.
a detailed analysis is insection 3.6..3.6 analysis and discussion.
to our surprise, ite+dd reaches an extremelylow ground truth perplexity.
we ﬁnd that theground truth perplexity after the ﬁrst-pass decod-ing is only similar to the ite+ckad.
it showsthat the second-pass decoder utilizes the docu-ment knowledge well, and dramatically reducedthe ground truth perplexity..as shown in table 2, ite+dd has a higher per-cent of score 2 both on knowledge relevance and.
18id utterance1.i think rachel mcadamshad an even better roleas regina georgehowever!
would youagree?.
yeah, i guess that’salways worth it, and atruce was made as well..2.
3.i liked the scene wherebuzz thinks he’s a bigshot hero but then thecamera reveals him tobe a tiny toy..fan.
two-pass responsesi’m notofakristen bell, but i thinkshe did a great job.
i’m not a huge fan ofrachel mcadams,buthe did a great job.
yeah, not only doesshe reconcile with theplastics.
yeah,reconcileswith janis , damien andaaron.
ithink that’s one ofthe best scenes in themovie.
oh,isiwhat makes the movieunique as well.
haveyou seen any of theother pixar movies?.
think that.
she.
baseline models).
our proposed model can gener-ate better responses than wizard transformer onknowledge relevance and context coherence..to demonstrate the effectiveness of the two-pass decoder, we compare the results from theﬁrst-pass decoding and the second-pass decoding.
table 4 shows the improvement after the second-pass decoding.
for case 1, the second-pass de-coding result revises the knowledge error in theﬁrst-pass decoding result.
for case 2, the second-pass decoder uses more detailed knowledge thanthe ﬁrst-pass one.
for case 3, the second-pass de-coder cannot only respond to the previous utter-ance but also guide the following conversations byasking some knowledge related questions..table 4: examples of the two pass decoding.
under-lined texts are the differences between two results.
foreach case, the ﬁrst-pass response is on the top..4 related work.
context coherence than ite+ckad.
this resultalso demonstrates that deliberation decoder canimprove the knowledge correctness and guide thefollowing conversations better..although the perplexity of ite+ckad is onlyslightly better than kat, the bleu score, flu-ency, knowledge relevance and context coher-ence of ite+ckad all signiﬁcantly outperformthose of kat model, which indicates that incre-mental transformer can deal with multi-turn doc-ument grounded conversations better..wizard transformer has a great performanceon knowledge relevance only second to our pro-posed incremental transformer.
however,itsscore on context coherence is lower than someother baselines.
as shown in table 2, wizardtransformer has knowledge relevance score 1 re-sults twice more than score 2 results, which indi-cates that the model tends to generate responseswith related knowledge but not correct.
andthe poor performance on context coherence alsoshows wizard transformer does not respond to theprevious utterance well.
this shows the limitationof representing context and document knowledgeby simple concatenation..3.7 case study.
in this section, we list some examples to show.
the effectiveness of our proposed model..table 3 lists some responses generated by ourproposed incremental transformer with delibera-tion decoder (ite+dd) and wizard transformer(which achieves overall best performance among.
the closest work to ours lies in the area of open-domain dialogue system incorporating unstruc-tured knowledge.
ghazvininejad et al.
(2018)uses an extended encoder-decoder where the de-coder is provided with an encoding of both thecontext and the external knowledge.
parthasarathiand pineau (2018) uses an architecture containinga bag-of-words memory network fact encoderand an rnn decoder.
dinan et al.
(2018) com-bines memory network architectures to retrieve,read and condition on knowledge, and trans-former architectures to provide text representa-tion and generate outputs.
different from theseworks, we greatly enhance the transformer ar-chitectures to handle the document knowledge inmulti-turn dialogue from two aspects: 1) using at-tention mechanism to combine document knowl-edge and context utterances; and 2) exploiting in-cremental encoding scheme to encode multi-turnknowledge aware conversations..our work is also inspired by several works inother areas.
zhang et al.
(2018) introduces docu-ment context into transformer on document-levelneural machine translation (nmt) task.
guanet al.
(2018) devises the incremental encodingscheme based on rnn for story ending genera-tion task.
in our work, we design an incrementaltransformer to achieve a knowledge-aware con-text representation using an incremental encodingscheme.
xia et al.
(2017) ﬁrst proposes deliber-ation network based on rnn on nmt task.
ourdeliberation decoder is different in two aspects:1) we clearly devise the two decoders targetingcontext and knowledge respectively; 2) our sec-.
19ond pass decoder directly ﬁne tunes the ﬁrst passresult, while theirs uses both the hidden states andresults from the ﬁrst pass..hsin-yuan huang, eunsol choi, and wen-tau yih.
2018. flowqa: grasping ﬂow in history for con-versational machine comprehension.
arxiv preprintarxiv:1810.06683..5 conclusion and future work.
in this paper, we propose an incremental trans-former with deliberation decoder for the task ofdocument grounded conversations.
through anincremental encoding scheme, the model achievesa knowledge-aware and context-aware conversa-tion representation.
by imitating the real-worldhuman cognitive process, we propose a delibera-tion decoder to optimize knowledge relevance andcontext coherence.
empirical results show that theproposed model can generate responses with muchmore relevance, correctness, and coherence com-pared with the state-of-the-art baselines.
in the fu-ture, we plan to apply reinforcement learning tofurther improve the performance..6 acknowledgments.
this work is supported by 2018 tencent rhino-bird elite training program, national naturalscience foundation of china (no.
61662077,no.61876174) and national key r&d programof china (no.ys2017yfgh001428).
we sin-cerely thank the anonymous reviewers for theirthorough reviewing and valuable suggestions..references.
danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), volume 1,pages 1870–1879..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2018. wizardof wikipedia: knowledge-powered conversationalagents.
arxiv preprint arxiv:1811.01241..marjan ghazvininejad, chris brockett, ming-weichang, bill dolan, jianfeng gao, wen-tau yih, andmichel galley.
2018. a knowledge-grounded neuralconversation model.
in thirty-second aaai confer-ence on artiﬁcial intelligence..jian guan, yansen wang, and minlie huang.
2018.story ending generation with incremental encod-ing and commonsense knowledge.
arxiv preprintarxiv:1808.10113..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..guillaume klein, yoon kim, yuntian deng, jeansenellart, and alexander m. rush.
2017. open-nmt: open-source toolkit for neural machine trans-lation.
in proc.
acl..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting objec-tive function for neural conversation models.
in pro-ceedings of the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages110–119..shuman liu, hongshen chen, zhaochun ren, yangfeng, qun liu, and dawei yin.
2018. knowledgein pro-diffusion for neural dialogue generation.
ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), volume 1, pages 1489–1498..hao-tong ye kai-ling lo and shang-yu su yun-nungchen.
2019. knowledge-grounded response gen-eration with deep attentional latent-variable model.
thirty-third aaai conference on artiﬁcial intelli-gence..thang luong, hieu pham, and christopher d man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in naturallanguage processing, pages 1412–1421..andrea madotto, chien-sheng wu, and pascale fung.
2018. mem2seq: effectively incorporating knowl-edge bases into end-to-end task-oriented dialog sys-tems.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), volume 1, pages 1468–1478..kishore papineni, salim roukos, todd ward, and weijing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
pages 311–318..prasanna parthasarathi and joelle pineau.
2018. ex-tending neural generative conversational model us-ing external knowledge sources.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 690–695..pranav rajpurkar, robin jia, and percy liang.
2018.know what you dont know: unanswerable ques-tions for squad.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 2: short papers), volume 2, pages784–789..20siva reddy, danqi chen, and christopher d manning.
2018. coqa: a conversational question answeringchallenge.
arxiv preprint arxiv:1808.07042..iulian v serban, alessandro sordoni, yoshua bengio,aaron courville, and joelle pineau.
2016. buildingend-to-end dialogue systems using generative hier-archical neural network models.
in thirtieth aaaiconference on artiﬁcial intelligence..lifeng shang, zhengdong lu, and hang li.
2015.neural responding machine for short-text conversa-tion.
in proceedings of the 53rd annual meeting ofthe association for computational linguistics andthe 7th international joint conference on naturallanguage processing (volume 1: long papers), vol-ume 1, pages 1577–1586..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural net-works.
in advances in neural information process-ing systems, pages 3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..oriol vinyals and quoc le.
2015. a neural conversa-tional model.
arxiv preprint arxiv:1506.05869..yingce xia, fei tian, lijun wu, jianxin lin, tao qin,nenghai yu, and tie-yan liu.
2017. deliberationnetworks: sequence generation beyond one-pass de-coding.
in advances in neural information process-ing systems, pages 1784–1794..hao xiong, zhongjun he, hua wu, and haifeng wang.
2018. modeling coherence for discourse neural ma-chine translation.
arxiv preprint arxiv:1811.05683..semih yavuz, abhinav rastogi, guan-lin chao, dilekhakkani-t¨ur, and amazon alexa ai.
2018. deep-copy: grounded response generation with hierarchi-cal pointer networks.
advances in neural informa-tion processing systems..adams wei yu, david dohan, minh-thang luong, ruizhao, kai chen, mohammad norouzi, and quoc vle.
2018. qanet: combining local convolutionwith global self-attention for reading comprehen-sion.
arxiv preprint arxiv:1804.09541..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 533–542..kangyan zhou, shrimai prabhumoye, and alan wblack.
2018. a dataset for document grounded con-versations.
in proceedings of the 2018 conferenceon empirical methods in natural language pro-cessing, pages 708–713..21