a pre-training strategy for zero-resource response selection inknowledge-grounded conversations.
chongyang tao1∗ , changyu chen2∗ , jiazhan feng1, jirong wen2,3 and rui yan2,3†1peking university, beijing, china2gaoling school of artiﬁcial intelligence, renmin university of china3beijing academy of artiﬁcial intelligence1{chongyangtao,fengjiazhan}@pku.edu.cn2{chen.changyu,jrwen,ruiyan}@ruc.edu.cn.
abstract.
response.
3) multi-turn.
recently, many studies are emerging towardsbuilding a retrieval-based dialogue systemthat is able to effectively leverage backgroundknowledge (e.g., documents) when conversingwith humans.
however, it is non-trivial tocollect large-scale dialogues that are naturallygrounded on the background documents,which hinders the effective and adequatetraining of knowledge selection and responsematching.
to overcome the challenge, weconsider decomposing the training oftheselectionknowledge-groundedinto three tasks including: 1) query-passage2) query-dialogue historymatching task;matchingresponsetask;matching task, and joint learning all thesetasks in a uniﬁed pre-trained language model.
the former two tasks could help the modelin knowledge selection and comprehension,while the last task is designed for matchingthe proper response with the given query andbackground knowledge (dialogue history).
bythis means, the model can be learned to selectrelevant knowledge and distinguish properresponse, with the help of ad-hoc retrievalcorpora and a large number of ungroundedmulti-turn dialogues.
experimental resultson two benchmarks of knowledge-groundedresponse selection indicate that our model canachieve comparable performance with severalexisting methods that rely on crowd-sourceddata for training..1.introduction.
along with the very recent prosperity of artiﬁcialintelligence empowered conversation systems inthe spotlight, many studies have been focused onbuilding human-computer dialogue systems (wenet al., 2017; zhang et al., 2020) with either retrieval-based methods (wang et al., 2013; wu et al., 2017;.
∗equal contribution.
† corresponding author: rui yan (ruiyan@ruc.edu.cn)..whang et al., 2020) or generation-based meth-ods (li et al., 2016; serban et al., 2016; zhang et al.,2020), which both predict the response with onlythe given context.
in fact, unlike a person who mayassociate the conversation with the backgroundknowledge in his or her mind, the machine canonly capture limited information from the querymessage itself.
as a result, it is difﬁcult for amachine to properly comprehend the query, and topredict a proper response to make it more engaging.
to bridge the gap of the knowledge between thehuman and the machine, researchers have begun tosimulating this motivation by grounding dialogueagents with background knowledge (zhang et al.,2018; dinan et al., 2019; li et al., 2020), and lotsof impressive results have been obtained..in this paper, we consider the response selectionproblem in knowledge-grounded conversion andspecify the background knowledge as unstructureddocuments that are common sources in practice.
the task is that given a conversation context anda set of knowledge entries, one is required 1):to select proper knowledge and grasp a goodcomprehension of the selected document materials(knowledge selection); 2): to distinguish the trueresponse from a candidate pool that is relevant andconsistent with both the conversation context andthe background documents (knowledge matching).
while there exists a number of knowledgedocuments on the web, it is non-trivial to collectlarge-scale dialogues that are naturally groundedon the documents for training a neural responseselection model, which hinders the effective andadequate training of knowledge selection and re-sponse matching.
although some benchmarks builtupon crowd-sourcing have been released by recentworks (zhang et al., 2018; dinan et al., 2019), therelatively small training size makes it hard for thedialogue models to generalize on other domains ortopics (zhao et al., 2020).
thus, in this work, we.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4446–4457august1–6,2021.©2021associationforcomputationallinguistics4446focus on a more challenging and practical scenario,learning a knowledge-grounded conversation agentwithout any knowledge-grounded dialogue data,which is known as zero-resource settings..since knowledge-grounded dialogues are un-available in training, it raises greater challengesfor learning the grounded response selection model.
fortunately, there exists a large number of unstruc-tured knowledge (e.g., web pages or wiki articles),passage search datasets (e.g., query-passage pairscoming from ad-hoc retrieval tasks) (khattab andzaharia, 2020) and multi-turn dialogues (e.g.,context-response pairs collected from reddit) (hen-derson et al., 2019), which might be beneﬁcial tothe learning of knowledge comprehension, knowl-edge selection and response prediction respectively.
besides, in multi-turn dialogues, the backgroundknowledge and conversation history (excludingthe latest query) are symmetric in terms of theinformation they convey, and we assume that thedialogue history can be regarded as another formatof background knowledge for response prediction..based on the above intuition, in this paper, weconsider decomposing the training of the groundedresponse selection task into several sub-tasks, andjoint learning all those tasks in a uniﬁed model.
totake advantage of the recent breakthrough on pre-training for natural language tasks, we build thegrounded response matching models on the basisof a pre-trained language model (plms) (devlinet al., 2019; yang et al., 2019), which are trainedwith large-scale unstructured documents from theweb.
on this basis, we further train the plmswith query-passage matching task, query-dialoguehistory matching task, and multi-turn responsematching task jointly.
the former two tasks couldhelp the model not only in knowledge selectionbut also in knowledge (and dialogue history)comprehension, while the last task is designed formatching the proper response with the given queryand background knowledge (dialogue history).
bythis means, the model can be learned to select rele-vant knowledge and distinguish proper responses,with the help of a large number of ungroundeddialogues and ad-hoc retrieval corpora.
duringthe testing stage, we ﬁrst utilize the trained modelto select proper knowledge, and then feed thequery, dialogue history, selected knowledge, andthe response candidate into our model to calculatethe ﬁnal matching degree.
particularly, we designtwo strategies to compute the ﬁnal matching score..in the ﬁrst strategy, we directly concatenate theselected knowledge and dialogue history as along sequence of background knowledge and feedinto the model.
in the second strategy, we ﬁrstcompute the matching degree between each query-knowledge and the response candidates, and thenintegrate all matching scores..we conduct experiments with benchmarks ofknowledge-grounded dialogue that are constructedby crowd-sourcing,such as the wizard-of-wikipedia corpus (dinan et al., 2019) andthe cmu dog corpus (zhou et al., 2018a).
evaluation results indicate that our model achievescomparable performance on knowledge selectionand response selection with several existingmodels trained on crowd-sourced benchmarks..our contributions are summarized as follows:• to the best of our knowledge, this is the ﬁrstexploration of knowledge-grounded responseselection under the zero-resource setting.
• we propose decomposing the training ofthe grounded response selection models intoseveral sub-tasks, so as to empower the modelthrough these tasks in knowledge selectionand response matching..• we achieve a comparable performance of re-sponse selection with several existing modelslearned from crowd-sourced training sets..2 related work.
early studies of retrieval-based dialogue focus onsingle-turn response selection where the input of amatching model is a message-response pair (wanget al., 2013; ji et al., 2014; wang et al., 2015).
recently, researchers pay more attention to multi-turn context-response matching and usually adoptthe representation-matching-aggregation paradigmto build the model.
representative methods in-clude the dual-lstm model (lowe et al., 2015),the sequential matching network (smn) (wuet al., 2017), the deep attention matching network(dam) (zhou et al., 2018b),interaction-over-interaction network (ioi) (tao et al., 2019) andmulti-hop selector network (msn) (yuan et al.,2019).
more recently, pre-trained language mod-els (devlin et al., 2019; yang et al., 2019) haveshown signiﬁcant beneﬁts for various nlp tasks,and some researchers have tried to apply themon multi-turn response selection.
vig and ramea(2019) exploit bert to represent each utterance-response pair and fuse these representations to.
4447calculate the matching score; whang et al.
(2020)and xu et al.
(2020) treat the context as a longsequence and conduct context-response matchingwith bert.
besides, gu et al.
(2020a) integratespeaker embeddings into bert to improve theutterance representation in multi-turn dialogue..to bridge the gap of the knowledge between thehuman and the machine, researchers have investi-gated into grounding dialogue agents with unstruc-tured background knowledge (ghazvininejad et al.,2018; zhang et al., 2018; dinan et al., 2019).
forexample, zhang et al.
(2018) build a persona-basedconversation data set that employs the interlocu-tor’s proﬁle as the background knowledge; zhouet al.
(2018a) publish a data where conversationsare grounded in articles about popular movies;dinan et al.
(2019) release another document-grounded data with wiki articles covering a widerange of topics.
meanwhile, several retrieval-based knowledge-grounded dialogue models areproposed, such as document-grounded matchingnetwork (dgmn) (zhao et al., 2019) and duallyinteractive matching network (dim) (gu et al.,2019) which let the dialogue context and all knowl-edge entries interact with the response candidaterespectively via the cross-attention mechanism.
gu et al.
(2020b) further propose to pre-ﬁlter thecontext and the knowledge and then use the ﬁlteredcontext and knowledge to perform the matchingwith the response.
besides, with the help of goldknowledge index annotated by human wizards,dinan et al.
(2019) consider joint learning theknowledge selection and response matching in amulti-task manner or training a two-stage model..3 model.
in this section, we ﬁrst formalize the knowledge-grounded response matching problem and thenintroduce our method from preliminary to responsematching with plms to details of three pre-trainingtasks..3.1 problem formalization.
we ﬁrst describe a standard knowledge-groundedresponse selection task such as wizard-of-wikipedia.
suppose that we have a knowledge-grounded dialogue data set d = {ki, ci, ri, yi}ni=1where ki = {p1, p2, .
.
.
, plk } represents athe j-thcollection of knowledge with pjknowledge entry (a.k.a., passage) and lk is thenumber of entries; ci = {u1, u2, .
.
.
, ulc} denotes.
multi-turn dialogue context with uj the j-th turnand lc is the number of dialogue turns.
it shouldbe noted that in this paper we denote the latestturn ulc as dialogue query qi, and dialogue contextexcept for query is denoted as hi = ci/{qi}.
ristands for a candidate response.
yi = 1 indicatesthat ri is a proper response for ci and ki, otherwiseyi = 0. n is the number of samples in data set.
the goal knowledge-grounded dialogue is to learna matching model g(k, c, r) from d, and thus forany new (k, c, r), g(k, c, r) returns the matchingdegree between r and (k, c).
finally, one cancollect the matching scores of a series of candidateresponses and conduct response ranking..zero-resource grounded response selection thenis formally deﬁned as follows.
there is a standardmulti-turn dialogue dataset dc = {qi, hi, ri}ni=1and an ad-hoc retrieval dataset dp = {qi, pi, zi}mi=1where qi is a query and pi stands a candidatepassage, zi = 1 indicates that pi is a relevantpassage for qi, otherwise zi = 0. our goal is tolearn a model g(k, h, q, r) from dc and dp, andthus for any new input (k, h, q, r), our model canselect proper knowledge ˆk from k and calculate thematching degree between r and (ˆk, q, h)..3.2 preliminary: response matching with.
plms.
pre-trained language models have been widely usedin many nlp tasks due to the strong ability oflanguage representation and understanding.
in thiswork, we consider building a knowledge-groundedresponse matching model with bert..speciﬁcally, given a query q, a dialoguehistory h = {u1, u2, ..., unh} where uiis the i-th turn in the history, a responsecandidate r = {r1, r2, ..., rlr } with lr words,sequences as a singlewe concatenate allconsecutivespecialsequence withtokenstokens, which can be represented as x ={[cls], u1, [sep], .
.
.
, [sep], ulh, [sep], q, [sep],[cls] and [sep] are classiﬁcationr, [sep]}.
segmentsymbolsymbolseparationfor each token in x, bertrespectively.
uses a summation of three kinds of embeddings,including wordpiece embedding (wu et al., 2016),segment embedding, and position embedding..and.
then, the embedding sequence of x is fed intobert, giving us the contextualized embeddingsequence {e[cls], e2, .
.
.
, elx}.
e[cls] is anaggregated representation vector that contains the.
4448figure 1: the overall architecture of our model..semantic interaction information between the query,history, and response candidate.
finaly, e[cls] isfed into a non-linear layer to calculate the ﬁnalmatching score, which is formulated as:.
history).
by this means, the model can be learnedto select relevant knowledge and distinguish theproper response, with the help of a large number ofungrounded dialogues and ad-hoc retrieval corpora..g(h, q, r) = σ(w2 · tanh(w1e[cls] + b1) + b2).
(1).
where w{1,2} and b{1,2} is training parameters forresponse selection task, σ is a sigmoid function..in knowledge-grounded dialogue, each dialogueis associated with a large collection of knowledgeentries k = {p1, p2, .
.
.
, plk }1. the model isrequired to select m(m ≥ 1) knowledge entriesbased on semantic relevance between the queryand each knowledge, and then performs theresponse matching with the query, dialogue historyand the highly-relevant knowledge.
speciﬁcally,we denote ˆk = (ˆp1, .
.
.
, ˆpm) as the selectedknowledge entries, and feed the input sequencex = {[cls], ˆp1, [sep], .
.
.
, [sep], ˆpm, [sep], u1,[sep], .
.
.
, [sep], ulh, [sep], q, [sep], r, [sep]}to bert.
the ﬁnal matching score g(ˆk, h, q, r)can be computed based on [cls] representation..3.3 pre-training strategies.
on the basis of bert, we further jointly trainit with three tasks including 1) query-passagematching task; 2) query-dialogue history match-ing task; 3) multi-turn response matching task.
the former two tasks could help the model inknowledge selection and knowledge (and dialoguehistory) comprehension, while the last task isdesigned for matching the proper response with thegiven query and background knowledge (dialogue.
1the scale of the knowledge referenced by each dialogue.
usually exceeds the limitation of input length in plms..3.3.1 query-passage matchingalthough there exist a huge amount of conversationdata on social media, it is hard to collect sufﬁcientdialogues that are naturally grounded on knowledgedocuments.
existing studies (dinan et al., 2019)usually extract the relevant knowledge before theresponse matching or jointly train the knowledgeretrieval and response selection in a multi-taskmanner.
however, both methods need in-domainknowledge-grounded dialogue data (with goldknowledge label) to train, making the model hardto generalize to a new domain.
fortunately, thead-hoc retrieval task (harman, 2005; khattab andzaharia, 2020) in the information retrieval areaprovides a potential solution to simulate the processof knowledge seeking.
to take advantage ofthe parallel data in the ad-hoc retrieval task, weconsider incorporating the query-passage matchingtask, so as to help the knowledge selection andknowledge comprehension for our task..given a query-passage pair (q, p), we ﬁrstconcatenate the query q and the passage p as asingle consecutive token sequence with specialtokens separating them, which is formulated as:.
sqp = {[cls], wp.
1 , .
.
.
, wp.
np , [sep], wq.
1, .
.
.
, wq.
nq } (2).
i , wq.
where wpj denotes the i-th and j-th token ofknowledge entry p and query q respectively.
foreach token in sqp, token, segment and positioni.
4449input···dialogue historyor knowledgeresponsepre-trained language model (bert)𝑔𝑞,𝑘,𝑟outputlayermlptoken embeddingsposition embeddingssegment embeddings············response matching taskqueryquery-dialogue history matching taskquery-passage matching task···[background knowledge][response][query]··················𝐸!"#𝑬$!𝐸#%&𝑬$"𝐸#%&𝑬$#𝐸#%&𝑬’𝐸#%&𝐸(!𝐸($%𝐸#%&·········[cls]𝑢![sep]𝑢"[sep]𝑢#[sep]𝑞[sep]𝑟!𝑟$!
[sep]𝑤",!𝑤",$"𝑤&,!𝑤&,$#embeddings are summated and fed into bert.
it is worth noting that here we set the segmentembedding of the knowledge to be the same asthe dialogue history.
finally, we feed the outputrepresentation of [cls] eqp[cls] into a mlp toobtain the ﬁnal query-passage matching scoreg(q, p).
the loss function of each training samplefor query-passage matching task is deﬁned by.
lp(q, p+, p−.
1 , .
.
.
, p−.
np )eg(q,p+)eg(q,p+) + (cid:80)δp.
= − log(.
).
j=1 eg(q,p−j ).
(3).
where p+ stands for the positive passage for q, p−jis the j-th negative passage and δp is the numberof negative passage..3.3.2 query-dialogue history matchingin multi-turn dialogues, the conversation history(excluding the latest query) is a piece of supple-mentary information for the current query andcan be regarded as another format of backgroundknowledge during the response matching.
besides,due to the natural sequential relationship betweendialogue turns, the dialogue query usually showsa strong semantic relevance with the previousturns in the dialogue history.
inspired by suchcharacteristics, we design a query-dialogue historymatching task with the multi-turn dialogue context,so as to enhance the capability of the model tocomprehend the dialogue history with the givendialogue query and to rank relevant passages withthese pseudo query-passage pairs..ﬁrst.
speciﬁcally,.
theconcatenatewethedialogue history into a long sequence.
task requires the model to predict whether aquery q = {wqnq } and a dialogue historysequence h = {wh} are consecutive andrelevant.
we concatenate two sequences into asingle consecutive sequence with [sep] tokens,.
1 , .
.
.
, whnh.
1, .
.
.
, wq.
sqh = {[cls], wh.
1 , .
.
.
, wh.
nh , [sep], wq.
1, .
.
.
, wq.
nq } (4).
for each word in sqh, token, segment and positionembeddings are summated and fed into bert.
finally, we feed eqh[cls] into a mlp to obtain theﬁnal query-history matching score g(q, h).
theloss function of each training sample for query-history matching task is deﬁned by.
lh(q, h+, h−.
1 , .
.
.
, h−.
nh )eg(q,h+)eg(q,h+) + (cid:80)δh.
= − log(.
).
j=1 eg(q,h−j ).
where h+ stands for the true dialogue history for q,h−j is the j-th negative dialogue history randomlysampled from the training set and δh is the numberof sampled dialogue history..3.3.3 multi-turn response matchingthe above two tasks are designed for empoweringthe model to knowledge or history comprehensionand knowledge selection.
in this task, we aim attraining the model to match reasonable responsesbased on dialogue history and query.
sincewe treat the dialogue history as a special formof background knowledge and they share thesame segment embeddings in the plms, ourmodel can acquire the ability to identify theproper response with either dialogue history orthe background knowledge through the multi-turnresponse matching task..speciﬁcally, we format the multi-turn dialoguesas query-history-response triples and requires themodel to predict whether a response candidate1, .
.
.
, wrr = {wrnr } is appropriate for a given query1, .
.
.
, wqq = {wqnq } and a concatenated dialoguehistory sequence h = {wh}.
concretely,we concatenate three input sequences into a singleconsecutive tokens sequence with [sep] tokens,.
1 , .
.
.
, whnh.
shqr = {[cls], wh1, .
.
.
, wq.
1 , .
.
.
, whnq , [sep], wr.
wq.
nh , [sep],1, .
.
.
, wrnr }.
(6).
similarly, we feed an embedding sequence ofwhich each entry is a summation of token, segmentand position embeddings into bert.
finally, wefeed ehqr[cls] into a mlp to obtain the ﬁnal responsematching score g(h, q, r)..the loss function of each training sample for.
multi-turn response matching task is deﬁned by.
lr(h, q, r+, r−.
1 , .
.
.
, r−δr )eg(h,q,r+).
= − log(.
eg(h,q,r+) + (cid:80)nr.
i=j eg(h,q,r−j ).
(7).
).
where r+ is the true response for a given q andh, r−is the j-th negative response candidatejrandomly sampled from the training set and δr isthe number of negative response candidate..joint learning.
3.3.4we adopt a multi-task learning manner and deﬁnethe ﬁnal objective function as:.
lfinal = lp + lh + lr.
(8).
(5).
in this way, all tasks are jointly learned so thatthe model can effectively leverage two training.
4450corpus and learn to select relevant knowledge anddistinguish the proper response..3.4 calculating matching score.
after learning model from dc and dp, we ﬁrstrank {pi}nki=1 according to g(q, ki) and then selecttop m knowledge entries {p1, .
.
.
, pm} for thesubsequent response matching process.
herewe design two strategies to compute the ﬁnalmatching score g(k, h, q, r).
in the ﬁrst strategy,we directly concatenate the selected knowledge anddialogue history as a long sequence of backgroundknowledge and feed into the model to obtain theﬁnal matching score, which is formulated as,.
g(k, h, q, r) = g(p1 ⊕ .
.
.
⊕ pm ⊕ c, q, r).
(9).
where ⊕ denotes the concatenation operation..in the second strategy, we treat each selectedknowledge entry and the dialogue history equallyas the background knowledge, and compute thematching degree between each query, backgroundknowledge, and the response candidates with thetrained model.
consequently, the matching scoreis deﬁned as an integration of a set of knowledge-grounded response matching scores, formulated as,.
g(k, h, q, r) = g(h, q, r)+ maxi∈(0,m).
g(pi, q, r) (10).
where m is the number of selected knowledgeentries.
we name our model with the two strategiesas ptkgccat and ptkgcsep respectively.
wecompare the two learning strategies through empir-ical studies, as will be reported in the next section..4 experiments.
4.1 datasets and evaluation metrics.
training set.
we adopt ms marco passageranking dataset (nguyen et al., 2016) built onbing’s search for query-passage matching task.
the dataset contains 8.8m passages from webpages gathered from bing’s results to real-worldqueries and each passage contains an average of55 words.
each query is associated with sparserelevance judgments of one (or very few) passagemarked as relevant.
the training set contains about500k pairs of query and relevant passage, andanother 400m pairs of query and passages thathave not been marked as relevant, from which thenegatives are sampled in our task..for the query-dialogue history matching taskand multi-turn response matching task, we use themulti-turn dialogue corpus constructed from thereddit (dziri et al., 2018).
the dataset containsmore than 15 million dialogues and each dialoguehas at least 3 utterances.
after the pre-processing,we randomly sample 2.28m/20k dialogues as thetraining/validation set.
for each dialogue session,we regard the last turn as the response, the lastbut one as the query, and the rest as the positivedialogue history.
the negative dialogue historiesare randomly sampled from the whole dialogue set.
on average, each dialogue contains 4.3 utterances,and the average length of the utterances is 42.5..test set.
we tested our proposed method onthe wizard-of-wikipedia (wow) (dinan et al.,2019) and cmu dog (zhou et al., 2018a).
bothdatasets contain multi-turn dialogues grounded ona set of background knowledge and are built withcrowd-sourcing on amazon mechanical turk.
inwow, the given knowledge collection is obtainedfrom wikipedia and covers a wide range of topicsor domains, while in cmu dog, the underlyingknowledge focuses on the movie domain.
unlikecmu dog where the golden knowledge indexfor each turn is unknown, the golden knowledgeindex for each turn is provided in wow.
twoconﬁgurations (e.g., test-seen and test-unseen) areprovided in wow.
following existing works (dinanet al., 2019; zhao et al., 2019), positive responsesare true responses from humans and negative onesare randomly sampled.
the ratio between positiveand negative responses is 1 : 99 for wow and1 : 19 for cmu dog.
more details of the twobenchmarks are shown in appendix a.1..evaluation metrics.
following previous workson knowledge-grounded response selection (guet al., 2020b; zhao et al., 2019), we also employrecall n at k rn@k (where n = 100 for wow andn = 20 for cmu dog and k = {1, 2, 5}) as theevaluation metrics..4.2.implementation details.
our model is implemented by pytorch (paszkeet al., 2019).
without loss of generality, we selectenglish uncased bertbase (110m) as the matchingmodel.
during the training, the maximum lengthsof the knowledge (a.k.a., passage), the dialoguehistory, the query, and the response candidate wereset to 128, 120 60, and 40.intuitively, the lasttokens in the dialogue history and the previous.
4451test seen.
test unseen.
models.
r@1 r@2 r@5.models.
ir baselinebow memnettwo-stage transformertransformer memnet.
dim (gu et al., 2019)fire (gu et al., 2020b).
ptkgccatptkgcsep.
r@1 r@2 r@5 r@1 r@2 r@5.
17.871.384.287.4.
83.188.3.
85.789.5.
----.
91.195.3.
94.696.7.
----.
95.797.7.
98.298.9.
14.233.163.169.8.
60.368.3.
65.569.6.
----.
77.884.5.
82.085.8.
----.
92.395.1.
94.796.3.starspace (wu et al., 2018)bow memnet (zhang et al., 2018)kv proﬁle memory (zhang et al., 2018)transformer memnet (mazar´e et al., 2018)dgmn (zhao et al., 2019)dim (gu et al., 2019)fire (gu et al., 2020b).
50.751.656.160.365.678.781.8.
61.666.1.
64.565.869.974.478.389.090.8.
73.577.8.
80.381.482.487.491.297.197.4.
86.188.7.table 1: evaluation results on the test set of wow..evaluation results on the test set of.
ptkgccatptkgcsep.
table 2:cmu dog..tokens in the query and response candidate aremore important, so we cut off the previous tokensfor the context but do the cut-off in the reversedirection for the query and response candidate ifthe sequences are longer than the maximum length.
we set a batch size of 32 for multi-turn responsematching and query-dialogue history matching,and 8 for query-document matching in order totrain these tasks jointly under the circumstance oftraining examples inequality.
we set δp = 6, δh =1 and δr = 12 for the query-passage matching,the query-dialogue history matching and the multi-turn response matching respectively.
particularly,the negative dialogue histories are sampled fromother training instances in a batch.
the model isoptimized using adam optimizer with a learningrate set as 5e − 6. the learning rate is scheduledby warmup and linear decay.
a dropout rate of 0.1is applied for all linear transformation layers.
thegradient clipping threshold is set as 10.0. earlystopping on the corresponding validation data isadopted as a regularization strategy.
during thetesting, we vary the number of selected knowledge-entries m ∈ {1, .
.
.
, 15} and set m = 2 forptkgccat and set m = 14 for ptkgcsep becausethey achieve the best performance..4.3 baselines.
since the characteristics of the two data setsare different (only wow provides the goldenknowledge label), we compare the proposed modelwith the baselines on both data sets individually..1) ir baseline (dinan et al.,baselines on wow.
2019) uses simple word overlap for responseselection; 2) bow memnet (dinan et al., 2019)is a memory network where knowledge entries areembedded via bag-of-words representation, and themodel learns the knowledge selection and responsematching jointly; 3) transformer memnet (dinanet al., 2019) is an extension of bow memnet,.
and the dialogue history, response candidate andknowledge entries are encoded with transformerencoder (vaswani et al., 2017) pre-trained on alarge data set.
4) two-stage transformer (dinanet al., 2019) trains two separately models forknowledge selection and response retrieval respec-tively.
a best-performing model on the knowledgeselection task is used for the dialogue retrieval task..baselines on cmu dog 1) starspace (wuet al., 2018) selects the response by the cosinesimilarity between a concatenated sequence ofdialogue context, knowledge, and the responsecandidate represented by starspace (wu et al.,2018); 2) bow memnet (zhang et al., 2018)is a memory network with the bag-of-wordsrepresentation of knowledge entries asthememory items; 3) kv proﬁle memory (zhanget al., 2018) is a key-value memory networkgrounded on knowledge proﬁles; 4) transformermemnet (mazar´e et al., 2018) is similar to bowmemnet and all utterances are encoded with apre-trained transformer; 5) dgmn (zhao et al.,2019) lets the dialogue context and all knowledgeentries interact with the response candidaterespectively via the cross-attention; 6) dim (guet al., 2019) is similar to dgmn and all utteranceare encoded with bilstms; 7) fire (gu et al.,2020b) ﬁrst ﬁlters the context and knowledge andthen use the ﬁltered context and knowledge toperform the iterative response matching process..4.4 evaluation results.
performance of response selection.
table 1and table 2 report the evaluation results of re-sponse selection on wow and cmu dog whereptkgccat and ptkgcsep represent the ﬁnalmatching score computed with the ﬁrst strategy(equation 9) and the second strategy (equation10) respectively.
we can see that ptkgcsep is.
4452models.
ptkgcsep.
ptkgcsep (q)ptkgcsep (q+h)ptkgcsep (q+k).
ptkgcsep,m=1ptkgcsep,m=1 - lpptkgcsep,m=1 - lh.
wizard of wikipedia.
test seen.
test unseen.
cmu dog.
r@1.r@2.r@5.r@1.r@2.r@5.r@1.r@2.r@5.
89.5.
70.684.989.5.
85.684.784.9.
96.7.
79.793.996.4.
94.493.593.7.
98.9.
86.897.898.6.
97.997.597.6.
69.6.
55.964.967.0.
66.763.465.5.
85.8.
70.881.784.0.
82.880.581.7.
96.3.
83.494.396.0.
94.394.094.1.
66.1.
47.359.562.7.
60.458.759.4.
77.8.
58.872.373.8.
72.570.871.4.
88.7.
75.086.184.8.
86.085.685.3.table 3: ablation study..models.
wizard seen.
wizard unseen.
r@1 r@2 r@5 r@1 r@2 r@5.randomir baselinebow memnettransformertransformer (w/ pretrain).
our modelour model - lpour model - lh.
2.75.823.022.525.5.
22.012.821.2.
-----.
-----.
-----.
-----.
31.222.629.9.
48.845.247.6.
32.123.331.2.
50.745.549.2.
2.37.68.912.222.9.
23.113.322.7.table 4: the performance of knowledge selection onthe test sets of wow data.
all baselines come fromdinan et al.
(2019).
the details for all baselines areshown in appendix a.2..consistently better than ptkgccat over all metricson two data sets, demonstrating that individuallyrepresenting each knowledge-query-response triplewith bert can lead to a more optimal matchingsignal than representing a single long sequence.
our explanation to the phenomenon is that there isinformation loss when a long sequence composedof the knowledge and dialogue history passesthrough the deep architecture of bert.
thus, theearlier different knowledge entries and dialoguehistory are fused together, the more informationof dialogue history or background knowledge willbe lost in matching.
particularly, on the wow,in terms of r@1, our ptkgcsep achieves acomparable performance with the existing state-of-the-art models that are learned from the crowd-sourced training set, indicating that the modelcan effectively learn how to leverage externalknowledge feed for response selection through theproposed pre-training approach..notably, we can observe that our ptkgcsepperforms worse than dim and fire on thecmu dog.
our explanation to the phenomenonis that the dialogue and knowledge in cmu dogfocus on the movie domain while our train dataincluding ad-hoc retrieval corpora and multi-turn.
dialogues come from the open domain.
thus, ourmodel may not select proper knowledge entriesand can not well recognize the semantics clues forresponse matching due to the domain shift.
despitethis, ptkgcsep can still show better performancethan several existing models, such as transformermemnet and dgmn, though ptkgcsep does notaccess any training examples in the benchmarks..performance of knowledge selection.
we alsoassess the ability of models to predict the knowl-edge selected by human wizards in wow data.
the results are shown in table 4. we can ﬁndthat the performance of our method is comparablewith various supervised methods trained on thegold knowledge index.
in particular, on the test-seen, our model is slightly worse than transformer(w/ pretrain), while on the test-unseen, our modelachieves slightly better results.
the results demon-strate the advantages of our pretraining tasks andthe good generalization ability of our model..4.5 discussions.
ablation study.
we conduct a comprehensiveablation study to investigate the impact of differentinputs and different tasks.
first, we remove thedialogue history, knowledge, and both of them fromthe model, which is denoted as ptkgcsep(q+k),ptkgcsep(q+h) and ptkgcsep(q) respectively.
according to the results of the ﬁrst four rowsin table 3, we can ﬁnd that both the dialoguehistory and knowledge are crucial for responseselection as removing anyone will generally causea performance drop on the two data.
besides, thebackground knowledge is more critical for responseselection as removing the background knowledgecauses more signiﬁcant performance degradationthan removing the dialogue history..then, we remove each training task individ-ually from ptkgcsep, and denote the models.
4453models.
wizard seen.
wizard unseen.
r@1 r@2 r@5 r@1 r@2 r@5.ptkgcsep (q+h)ptkgcsep (q+h) -lhptkgcsep (q+h) -lpptkgcsep (q+h) -lh-lp.
84.984.183.483.2.
93.993.793.593.8.
97.897.797.997.6.
64.964.360.960.9.
81.781.980.280.1.
94.393.893.593.8.table 5: ablation study of our model withoutconsidering the grounded knowledge..as ptkgcsep-x, where x ∈ {lp, lh} meaningquery-passage matching task and query-dialoguehistory matching task respectively.
table 4 showsthe ablation results of knowledge selection.
wecan ﬁnd that both tasks are useful in the learning ofknowledge selection, and query-passage matchingplays a dominant role since the performance ofknowledge selection drops dramatically when thetask is removed from the pre-training process.
thelast two rows in table 3 show the ablation resultsof response selection.
we report the ablationresults when only 1 knowledge is provided sincethe knowledge recalls for different ablated modelsand the full model are very close when m is large(m = 14).
we can see that both tasks are helpfuland the performance of response selection dropsmore when removing the query-passage matchingtask.
particularly, lp plays a more important roleand the performance on test-unseen of wow dropsmore obvious when removing each training task..to further investigate the impact of our pre-training tasks on the performance of the multi-turn response selection (without considering thegrounded knowledge), we conduct an ablationstudy and the results are shown in table 5. wecan observe that the performance of the responsematching model (no grounded knowledge) dropsobviously when removing one of the pretrainingtasks or both tasks.
particularly, the query-passagematching task contributes more to the responseselection..the impact of the number of selected knowl-edge.
we further study how the number of se-lected knowledge (m) inﬂuences the performanceof ptkgcsep.
figure 2 shows how the per-formance of our model changes with respect todifferent numbers of selected knowledge.
weobserve that the performance increases mono-tonically until the knowledge number reaches acertain value, and then stable when the numberkeeps increasing.
the results are rational becausemore knowledge entries can provide more useful.
figure 2: the performance of response selection acrossdifferent number of selected knowledge..information for response matching, but when theknowledge becomes enough, the noise will bebrought to matching..5 conclusion.
in this paper, we study response matching inknowledge-grounded conversations under a zero-resource setting.
in particular, we propose decom-posing the training of the knowledge-groundedresponse selection into three tasks and joint train alltasks in a uniﬁed pre-trained language model.
ourmodel can be learned to select relevant knowledgeand distinguish proper response, with the helpof ad-hoc retrieval corpora and amount of multi-turn dialogues.
experimental results on twobenchmarks indicate that our model achieves acomparable performance with several existingin themethods trained on crowd-sourced data.
future, we would like to explore the ability of ourproposed method in retrieval-augmented dialogues..acknowledgement.
we would like to thank the anonymous reviewersthis workfor their constructive comments.
was supported by the national key researchand development program of china(no.
2020yfb1406702),sciencefoundation of china (nsfc no.
61876196) andbeijing outstanding young scientist program(no.
bjjwzyjh012019100020098).
rui yanis the corresponding author, and is supported asa young fellow at beijing academy of artiﬁcialintelligence (baai)..the national.
44540.850.860.870.880.890.900.8560.8640.8690.8750.8770.8820.8850.8870.8890.8910.8920.8930.8940.8950.895seenunseen123456789101112131415the number of selected knowledge (m)0.650.660.670.680.690.700.6670.6720.6750.6820.6820.6810.6820.6820.6850.6870.6880.6900.6920.6960.696r100@1references.
jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 4171–4186.
association forcomputational linguistics..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2019. wizardof wikipedia: knowledge-powered conversationalin international conference on learningagents.
representations..nouha dziri, ehsan kamalloo, kory w mathewson,and osmar r zaiane.
2018. augmenting neuralresponse generation with context-aware topicalattention.
arxiv preprint arxiv:1811.01063..marjan ghazvininejad, chris brockett, ming-weichang, bill dolan, jianfeng gao, wen-tau yih, andmichel galley.
2018. a knowledge-grounded neuralin the thirty-second aaaiconversation model.
conference on artiﬁcial intelligence, pages 5110–5117..jia-chen gu, tianda li, quan liu, zhen-hua ling,zhiming su, si wei, and xiaodan zhu.
2020a.
speaker-aware bert for multi-turn response selectionin proceedings of thein retrieval-based chatbots.
29th acm international conference on informationand knowledge management, cikm ’20, pages2041–2044.
acm..jia-chen gu, zhen-hua ling, xiaodan zhu, and quanliu.
2019. dually interactive matching network forpersonalized response selection in retrieval-basedchatbots.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference onnatural language processing (emnlp-ijcnlp),pages 1845–1854, hong kong, china..jia-chen gu, zhenhua ling, quan liu, zhigang chen,and xiaodan zhu.
2020b.
filtering before iterativelyreferring for knowledge-grounded response selec-tion in retrieval-based chatbots.
in findings of theassociation for computational linguistics: emnlp2020, pages 1412–1422, online.
association forcomputational linguistics..donna k harman.
2005. the trec ad hoc experiments..matthew henderson, paweł budzianowski,.
i˜nigocasanueva, sam coope, daniela gerz, girishkumar, nikola mrkˇsi´c, georgios spithourakis,pei-hao su, ivan vuli´c, and tsung-hsien wen.
2019. a repository of conversational datasets.
inproceedings ofthe first workshop on nlp forconversational ai, pages 1–10, florence, italy..zongcheng ji, zhengdong lu, and hang li.
2014.text.
an information retrieval approach to shortconversation.
arxiv preprint arxiv:1408.6988..omar khattab and matei zaharia.
2020. colbert: efﬁ-cient and effective passage search via contextualizedlate interaction over bert.
in proceedings of the 43rdinternational acm sigir conference on researchand development in information retrieval, pages39–48..jiwei li, michel galley, chris brockett, jianfenggao, and bill dolan.
2016. a diversity-promotingobjective function for neural conversation models.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110–119, san diego, california.
associationfor computational linguistics..linxiao li, can xu, wei wu, yufan zhao, xueliangzhao, and chongyang tao.
2020. zero-resourceknowledge-grounded dialogue generation.
inproceedings ofthe 34th conference on neuralinformation processing systems..ryan lowe, nissan pow, iulian serban, and joellepineau.
2015. the ubuntu dialogue corpus: alarge dataset for research in unstructured multi-in proceedings of the 16thturn dialogue systems.
annual meeting of the special interest group ondiscourse and dialogue, pages 285–294, prague,czech republic.
association for computationallinguistics..pierre-emmanuel mazar´e, samuel humeau, martintrainingraison, and antoine bordes.
2018.millions of personalized dialogue agents.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages2775–2779, brussels, belgium.
association forcomputational linguistics..tri nguyen, mir rosenberg, xia song, jianfeng gao,saurabh tiwary, rangan majumder, and li deng.
2016. ms marco: a human generated machinereading comprehension dataset.
in coco@ nips..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucapytorch: an imperativeantiga, et al.
2019.instyle, high-performance deep learning library.
advances in neural information processing systems,volume 32. curran associates, inc..iulian vlad serban, alessandro sordoni, yoshuabengio, aaron c courville, and joelle pineau.
2016. building end-to-end dialogue systems usinggenerative hierarchical neural network models.
inproceedings of the thirtieth aaai conference onartiﬁcial intelligence, volume 16, pages 3776–3784..chongyang tao, wei wu, can xu, wenpeng hu,dongyan zhao, and rui yan.
2019. one timeof interaction may not be enough: go deep withan interaction-over-interaction network for responsein proceedings of the 57thselection in dialogues.
annual meeting of the association for computationallinguistics, pages 1–11..4455ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention isin advances in neural informationall you need.
processing systems, volume 30. curran associates,inc..zhilin yang, zihang dai, yiming yang,.
jaimecarbonell, russ r salakhutdinov, and quoc v le.
2019. xlnet: generalized autoregressive pretrainingfor language understanding.
in advances in neuralinformation processing systems, volume 32. curranassociates, inc..jesse vig and kalai ramea.
2019. comparison oftransfer-learning approaches for response selectionin workshop onin multi-turn conversations.
dstc7..hao wang, zhengdong lu, hang li, and enhonga dataset for research on short-chen.
2013.in proceedings of the 2013text conversations.
in naturalconference on empirical methodslanguage processing, pages 935–945.
associationfor computational linguistics..mingxuan wang, zhengdong lu, hang li, and qunliu.
2015. syntax-based deep matching of shorttexts.
in ijcai, pages 1354–1361..tsung-hsien wen, david vandyke, nikola mrkˇsi´c,milica gaˇsi´c, lina m. rojas-barahona, pei-hao su,stefan ultes, and steve young.
2017. a network-based end-to-end trainable task-oriented dialoguein proceedings of the 15th conference ofsystem.
the european chapter of the association for com-putational linguistics, pages 438–449.
associationfor computational linguistics..taesun whang, dongyub lee, chanhee lee, kisuyang, dongsuk oh, and heuiseok lim.
2020. aneffective domain adaptive post-training method forin proceedings ofbertinterspeech 2020, pages 1585–1589..in response selection..ledell yu wu, adam fisch, sumit chopra, keithadams, antoine bordes, and jason weston.
2018.in thirty-secondstarspace: embed all the things!
aaai conference on artiﬁcial intelligence, pages5569–5577..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, et al.
2016. google’sneural machine translation system: bridging the gapbetween human and machine translation.
corr,abs/1609.08144..yu wu, wei wu, chen xing, ming zhou, and zhoujunli.
2017. sequential matching network: a newarchitecture for multi-turn response selection inretrieval-based chatbots.
in proceedings of the 55thannual meeting of the association for computa-tional linguistics, pages 496–505.
association forcomputational linguistics..ruijian xu, chongyang tao, daxin jiang, xueliangzhao, dongyan zhao, and rui yan.
2020. learningan effective context-response matching model withself-supervised tasks for retrieval-based dialogues.
in proceedings of the thirty-fifth aaai conferenceon artiﬁcial intelligence..chunyuan yuan, wei zhou, mingming li, shangwenlv, fuqing zhu, jizhong han, and songlin hu.
2019. multi-hop selector network for multi-turninresponse selection in retrieval-based chatbots.
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on naturallanguage processing, pages 111–120.
associationfor computational linguistics..saizheng zhang, emily dinan, jack urbanek, arthurszlam, douwe kiela, and jason weston.
2018.personalizing dialogue agents:i have a dog, doin proceedings of the 56thyou have pets too?
annual meeting of the association for computa-tional linguistics, pages 2204–2213.
associationfor computational linguistics..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020. dialogpt : large-scale generative pre-training for conversationalin proceedings of the 58thresponse generation.
annual meeting of the association for computa-tional linguistics: system demonstrations, pages270–278, online.
association for computationallinguistics..xueliang zhao, chongyang tao, wei wu, can xu,dongyan zhao, and rui yan.
2019. a document-grounded matching network for response selectionin proceedings of thein retrieval-based chatbots.
twenty-eighth international joint conference onartiﬁcial intelligence, pages 5443–5449..xueliang zhao, wei wu, can xu, chongyang tao,dongyan zhao, and rui yan.
2020. knowledge-grounded dialogue generation with pre-trainedthe 2020language models.
conference on empirical methodsin naturallanguage processing (emnlp), pages 3377–3390,online.
association for computational linguistics..in proceedings of.
kangyan zhou, shrimai prabhumoye, and alan wblack.
2018a.
a dataset for document groundedin proceedings of the 2018 confer-conversations.
ence on empirical methods in natural languageprocessing, pages 708–713, brussels, belgium.
association for computational linguistics..xiangyang zhou, lu li, daxiang dong, yi liu, yingchen, wayne xin zhao, dianhai yu, and hua wu.
2018b.
multi-turn response selection for chatbotswith deep attention matching network.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics, pages 1118–1127.
association for computational linguistics..4456a appendices.
a.1 details of test sets.
wizard of wikipedia.
cmu dog.
test seen test unseen.
statistics.
avg.
# turns.
avg, # words per turn.
avg.
# knowledge entries.
avg.
# words per knowledge.
9.0.
16.4.
60.8.
36.9.
9.1.
16.1.
61.0.
37.0.test.
12.4.
18.1.
31.8.
27.0.table 6: the statistics of test sets of two benchmarks..we tested our proposed method on the wizard-of-wikipedia (wow) (dinan et al., 2019) andcmu dog (zhou et al., 2018a).
both datasetscontain multi-turn dialogues grounded on a set ofbackground knowledge and are built with crowd-sourcing on amazon mechanical turk..in the wow dataset, one of the paired speakersis asked to play the role of a knowledgeable expertwith access to the given knowledge collection ob-tained from wikipedia, while the other of a curiouslearner.
the dataset consists of 968 completeknowledge-grounded dialogues for testing.
it isworth noting that the golden knowledge index foreach turn is available in the dataset.
responseselection is performed at every turn of a completedialogue, which results in 7512 for testing in total.
following the setting of the original paper, positiveresponses are true responses from humans andnegative ones are randomly sampled.
the ratiobetween positive and negative responses is 1 : 99 intesting sets.
besides, the test set is divided into twosubsets: test seen and test unseen.
the formershares 533 common topics with the training set,while the latter contains 58 new topics uncoveredby the training or validation set..the cmu dog data contains knowledge-grounded human-human conversations where theunderlying knowledge comes from wiki articlesand focuses on the movie domain.
similar todinan et al.
(2019), the dataset was also built in twoscenarios.
in the ﬁrst scenario, only one workercan access the provided knowledge collections,and he/she is responsible for introducing themovie to the other worker; while in the secondscenario, both workers know the knowledge andthey are asked to discuss the content.
differentfrom wow, the golden knowledge index for eachturn is unknown for both scenarios.
since thedata size for an individual scenario is small, wemerge the data of the two scenarios followingthe setting with zhao et al.
(2019).
finally, there.
are 537 dialogues for testing.
we evaluate theperformance of the response selection at every turnof a dialogue, which results in 6637 samples fortesting.
we adopted the version shared in zhaoet al.
(2019), where 19 negative candidates wererandomly sampled for each utterance from thesame set.
more details about the two benchmarkscan be seen in table 6..a.2 baselines for knowledge selection.
to compare the performance of knowledge selec-tion, we choose the following baselines from dinanthe modelet al.
(2019) including (1) random:randomly selects a knowledge entry from a set ofknowledge entries; (2) ir baseline: the model usessimple word overlap between the dialogue contextand the knowledge entry to select the relevantknowledge; (3) bow memnet: the model is basedon memory network where each memory itemis a bag-of-words representation of a knowledgeentry, and the gold knowledge labels for eachturn are used to train the model; (4) transformer:the model trains a context-knowledge matchingnetwork based on transformer architecture; (5)transformer (w/ pretrain): the model is similar tothe former model, but the transformer is pre-trainedon reddit data and ﬁne-tuned for the knowledgeselection task..a.3 results of low-resource setting.
ration (t).
wizard seen.
wizard unseen.
r@1 r@2 r@5 r@1 r@2 r@5.
0%.
10%50%100%.
89.5.
90.891.592.2.
96.7.
97.197.197.6.
98.9.
99.499.399.4.
69.6.
73.273.974.3.
85.8.
86.987.988.1.
96.3.
96.896.997.1.table 7: evaluation results of our model in the low-resource setting on the wizard of wikipedia data..as an additional experiment, we also evaluatethe proposed model for a low-resource setting.
werandomly sample t ∈ {10%, 50%, 100%} portionof training data from wow, and use the data to ﬁne-tune our model.
the results are shown in table 7.we can ﬁnd that with only 10% training data,our model can signiﬁcantly outperform existingmodels,indicating the advantages of our pre-training tasks.
with 100% training data, our modelcan achieve 2.7% improvement in terms of r@1on the test-seen and 4.7% improvement on the test-unseen..4457