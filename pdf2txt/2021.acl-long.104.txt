diverse pretrained context encodings improve document translation.
domenic donato, lei yu, chris dyerdeepmindlondon, united kingdom{domenicd,leiyu,cdyer}@deepmind.com.
abstract.
we propose a new architecture for adaptinga sentence-level sequence-to-sequence trans-former by incorporating multiple pretraineddocument context signals and assess the im-pact on translation performance of (1) dif-ferent pretraining approaches for generatingthese signals,(2) the quantity of paralleldata for which document context is available,and (3) conditioning on source,target, orsource and target contexts.
experiments onthe nist chinese–english, and iwslt andwmt english–german tasks support four gen-eral conclusions:that using pretrained con-text representations markedly improves sam-ple efﬁciency, that adequate parallel data re-sources are crucial for learning to use doc-ument context,that jointly conditioning onmultiple context representations outperformsany single representation, and that source con-text is more valuable for translation perfor-mance than target side context.
our best multi-context model consistently outperforms thebest existing context-aware transformers..1.introduction.
generating an adequate translation for a sen-tence often requires understanding the contextin which the sentence occurs (and in which itstranslation will occur).
although single-sentencetranslation models demonstrate remarkable perfor-mance (chen et al., 2018; vaswani et al., 2017;bahdanau et al., 2015), extra-sentential informa-tion can be necessary to make correct decisionsabout lexical choice, tense, pronominal usage, andstylistic features, and therefore designing modelscapable of using this information is a necessary steptowards fully automatic high-quality translation.
aseries of papers have developed architectures thatpermit the broader translation model to conditionon extra-sentential context (zhang et al., 2018; mi-culicich et al., 2018), operating jointly on multiple.
sentences at once (junczys-dowmunt, 2019), orindirectly conditioning on target side documentcontext using bayes’ rule (yu et al., 2020b)..while noteworthy progress has been made atmodeling monolingual documents (brown et al.,2020), progress on document translation has beenless remarkable, and continues to be hampered bythe limited quantities of parallel document datarelative to the massive quantities of monolingualdocument data.
one recurring strategy for deal-ing with this data scarcity—and which is the ba-sis for this work—is to adapt a sentence-levelsequence-to-sequence model by making additionaldocument context available in a second stage oftraining (maruf et al., 2019; zhang et al., 2018;miculicich et al., 2018; haffari and maruf, 2018).
this two-stage training approach provides an in-ductive bias that encourages the learner to explaintranslation decisions preferentially in terms of thecurrent sentence being translated, but these can bemodulated at the margins by using document con-text.
however, a weakness of this approach is thatthe conditional dependence of a translation on itssurrounding context given the source sentence isweak, and learning good context representationspurely on the basis of scarce parallel document datais challenging..a recent strategy for making better use of doc-ument context in translation is to use pretrainedbert representations of the context, rather thanlearning them from scratch (zhu et al., 2020).
ourkey architectural innovation in this paper is anarchitecture for two-staged training that enablesjointly conditioning on multiple context types, in-cluding both the source and target language context.
practically, we can construct a weak context repre-sentation from a variety of different contextual sig-nals, and these are merged with the source sentenceencoder’s representation at each layer in the trans-former.
to examine the potential of this architec-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1299–1311august1–6,2021.©2021associationforcomputationallinguistics1299ture, we explore two high-level research questions.
first, using source language context, we explorethe relative impact of different kinds of pretrain-ing objectives on the performance obtained (bertand pegasus), the amount of parallel documenttraining data required, and the size of surroundingcontext.
second, recognizing that maintaining con-sistency in translation would seem to beneﬁt fromlarger contexts in the target language, we comparethe impact of source language context, target lan-guage context, and context containing both..our main ﬁndings are (1) that multiple kindsof source language context improves performanceof document translation over existing contextualrepresentations, especially those that do not use pre-trained context representations; (2) that althoughﬁne-tuning using pretrained contextual representa-tions improves performance, large performance isstrongly determined by the availability of contex-tual parallel data; and (3) that while both sourceand target language context provide beneﬁt, sourcelanguage context is more valuable, unless the qual-ity of the target language context translations isextremely high..2 model description.
our architecture is designed to incorporate multiplesources of external embeddings into a pretrainedsequence-to-sequence transformer model.
we exe-cute this by creating a new attention block for eachembedding we wish to incorporate and stack them.
we then insert this attention stack as a branchingpath in each layer of the encoder and decoder.
theoutputs of the new and original paths are averagedbefore being passed to the feed forward block atthe end of the layer.
details are discussed below(§2.4), and the architecture is shown in figure 1..the model design follows the adapter pattern(gamma et al., 1995).
the interface between the ex-ternal model and translation model takes the formof an attention block which learns to perform theadaptation.
the independence between the modelsmeans that different input data can be provided toeach, which enables extra information during thetranslation process.
in this work, we leverage thistechnique to: (1) enhance a sentence-level modelwith additional source embeddings; (2) convert asentence-level model to a document-level modelby providing contextual embeddings.
like bert-fused (zhu et al., 2020), we use pretrained maskedlanguage models to generate the external embed-.
dings..2.1 pre-trained models.
we use two kinds of pretrained models: bert (de-vlin et al., 2019) and pegasus (zhang et al.,2020).
although similar in architecture, we conjec-ture that these models will capture different signalson account of their different training objectives..bert is trained with a masked word objectiveand a two sentence similarity classiﬁcation task.
during training, it is provided with two sentencesthat may or may not be adjacent, with some of theirwords masked or corrupted.
bert predicts thecorrect words and determining if the two sentencesform a contiguous sequence.
intuitively, bert pro-vides rich word-in-context embeddings.
in termsof machine translation, it’s reasonable to postulatethat bert would provide superior representationsof the source sentence and reasonable near sen-tence context modulation.
on the other hand, weexpect it to fail to provide contextual conditioningwhen the pair of sentences are not adjacent.
thisshortcoming is where pegasus comes in..pegasus is trained with a masked sentence ob-jective.
during training, it is given a documentthat has had random sentences replaced by a masktoken.
its task is to decode the masked sentencesin the same order they appear in the document.
asa result, pegasus excels at summarization tasks,which require taking many sentences and compress-ing them into a representation from which anothersentence can be generated.
in terms of providingcontext for document translation, we conjecturethat pegasus will be able to discover signalsacross longer ranges that modulate output..2.2 embedding notation.
to keep track of the type of embeddings beingincorporated in a particular conﬁguration, we usethe notational convention modelside(inputs)..• model: b for bert, p for pegasus, and d fordocument transformer (zhang et al., 2018).
• side: s for the source and t for the target lan-.
guage..• inputs: c for the current source (or target), i.e.,xi, p for the previous source (target), and n forthe next one.
note that 3p means the three previ-ous sources (targets), (xi−3, xi−2, xi−1)..• when multiple embeddings are used, we includea ⇒ to indicate the order of attention operations..1300we can thus represent the bert-fused documentmodel proposed by zhu et al.
(2020) as bs(p,c)since it passes the previous and current source sen-tences as input to bert..2.3 enhanced models.
the core of this work is to understand the beneﬁtsthat adding a diverse set of external embeddingshas on the quality of document translation.
to thiseffect, we introduce two new models that leveragethe output from both bert and pegasus:.
multi-source := bs(c) ⇒ ps(c)multi-context := bs(p,c) ⇒ bs(c,n) ⇒ ps(3p,c,3n).
there are a few ways to integrate the output ofexternal models into a transformer layer.
we couldstack them vertically after the self-attention block(zhang et al., 2018) or we could place them hor-izontally and average all of their outputs togetherlike mat (fan et al., 2020).
our preliminary ex-periments show that the parallel attention stack,depicted in figure 1, works best.
therefore, weadopt this architecture in our experiments..2.4 parallel attention stack.
if we let a = bs(p,c), b = bs(c,n), and c =ps(3p,c,3n) refer to the output of the external pre-trained models computed once per translation ex-ample, then the multi-context encoder layer is de-ﬁned as.
r(cid:96) = attnblock(e(cid:96)−1, e(cid:96)−1, e(cid:96)−1)sa(cid:96) = attnblock(a, a, e(cid:96)−1)(cid:96) = attnblock(b, b, sasb(cid:96) )s(cid:96) = attnblock(c, c, sb(cid:96)).
t(cid:96) =.
(cid:26) dropbranch(r(cid:96), s(cid:96)).
12 · (r(cid:96) + s(cid:96)).
trainingotherwise.
e(cid:96) = layernorm(feedforward(t(cid:96))) + t(cid:96).
the intermediate outputs of the attention stack aresa(cid:96) ⇒ sb(cid:96) ⇒ s(cid:96).
to reproduce bert-fused, weremove sa(cid:96) and sb(cid:96) from the stack and set s(cid:96) di-rectly to attnblock(a, a, e(cid:96)−1).
we use atten-tion block to refer to the attention, layer normaliza-tion, and residual operations,.
attnblock(k, v, q) =.
layernorm(attn(k, v, q)) + q.while drop-branch (fan et al., 2020) is deﬁned as.
dropbranch(m, n) =.
1(u ≥ .5) · m + 1(u < .5) · n.where u ∼ uniform(0, 1) and 1 is the indicatorfunction..3 experiment setup.
3.1 datasets.
we evaluate our model on three translation tasks,the nist open mt chinese–english task,1 theiwslt’14 english-german translation task,2 andthe wmt’14 english-german news translationtask.3 table 1 provides a breakdown of the type,quantity, and relevance of the data used in the vari-ous dataset treatments.
nist provides the largestamount of in domain contextualized sentence pairs.
iwslt’14 and wmt’14 are almost an order ofmagnitude smaller.
see appendix a for prepro-cessing details..nist chinese–english is comprised of ldc dis-tributed news articles and broadcast transcripts.
weuse the mt06 dataset as validation set and mt03,mt04, mt05, and mt08 as test sets.
the val-idation set contains 1,649 sentences and the testset 5,146 sentences.
chinese sentences are fre-quently underspeciﬁed with respect to grammaticalfeatures that are obligatory in english (e.g., numberfor nouns, tense on verbs, and dropped arguments),making it a common language pair to study fordocument translation..iwslt’14 english–german is a corpus of trans-lated ted and tedx talks.
following prior work(zhu et al., 2020), we use the combination ofdev2010, dev2012, tst2010, tst2011, and tst2012as the test set which contains 6,750 sentences.
werandomly selected 10 documents from the trainingdata for validation.
we perform a data augmenta-tion experiment with this dataset by additionallyincluding news commentary v15.
we denote thistreatment as iwslt+ and consider this to be out ofdomain data augmentation..1https://www.nist.gov/itl/iad/mig/open-machine-translation-evaluation2https://sites.google.com/site/.
iwsltevaluation2014/mt-track3http://statmt.org/wmt14/.
translation-task.html.
1301figure 1: architecture of our multi-context model.
the pretrained pegasus encoder and bert model alongwith their inputs and resulting embeddings are shown on the left.
in this conﬁguration, a batch of two differentsentence pairs are passed to bert per translation example.
the left to right ordering of the three inputs going intoan attention block are: keys, values, queries.
during training the average operation is replaced with drop-branch.
a partially shaded box indicates data while full shading is used for operations.
a dashed border means the datais constant for a given translation.
we use dashed arrows for residual connections and blue arrows to indicateembeddings that originate from outside the transformer model..wmt’14 english–german is a collection of webdata, news commentary, and news articles.
we usenewstest2013 for validation and newstest2014 asthe test set.
for the document data, we use the orig-inal wmt’14 news commentary v9 dataset.
werun two document augmentation experiments onthis dataset.
the ﬁrst, denoted as wmt+, replacesnews commentary v9 with the newer news com-mentary v15 dataset.
the second augmentationexperiment, denoted as wmt++, builds on the ﬁrstby additionally incorporating the tilde rapid 2019corpus.
the rapid corpus is comprised of euro-pean commission press releases and the languagestyle is quite different from the style used in thenews commentary data.
for this reason, we con-sider rapid to be out of domain data for this task..3.2 training.
we construct enhanced models with additional at-tention blocks and restore all previously trained pa-rameters.
we randomly initialize the newly addedparameters and exclusively update these duringtraining.
for a given dataset, we train a model onall the training data it is compatible with.
thismeans that for document-level models, only docu-ment data is used, while for sentence-level modelsboth document and sentence data is used.
in ourwork, this distinction only matters for the wmt’14dataset where there is a large disparity between thetwo types of data..transformer models are trained on sentence pairdata to convergence.
for nist and iwslt’14we use transformer base while for wmt’14 weuse transformer big.
we use the following vari-.
1302pegasus encbertencoderlayerdecoderlayerdataset.
nistiwsltiwslt+wmtwmt+wmt++.
in domaindocsent1.45m 1.45m173k173k173k173k4.7m 200k4.85m 345k4.85m 345k 1.63m 1.63m.
out domaindocsent----345k345k----.
table 1: we breakdown the type, quantity, and rel-evance of parallel sentences used when training mod-els for each dataset.
taking into account input require-ments, models were trained on the sum of the in domainand out of domain data for a given dataset treatment.
the ratio of in domain vs out of domain data per train-ing batch was tuned on the validation set for each treat-ment.
we used the dataset descriptions to determine thedomain.
for example, iwslt’14 is a dataset of trans-lated ted talks so we considered news commentarydata which is composed of translated news articles tobe out of domain for this task..ants of bert from google research github:4bert-base chinese on nist, bert-base uncasedon iwslt’14, and bert-large uncased (wholeword masking) on wmt’14.
we pretrain threepegasus base models for the languages en, de,and zh using the multilingual c4 dataset as detailedin tensorflow’s dataset catalog.5 when trainingour models, we only mask a single sentence pertraining example and do not include a masked wordauxiliary objective.
we use the public pegasuslarge6 on the english side of wmt’14, for every-thing else, we use our models.
see appendix b forbatch size and compute details..3.3 evaluation.
to reduce the variance of our results and help withreproducibility, we use checkpoint averaging.
weselect the ten contiguous checkpoints with the high-est average validation bleu.
we do this at twocritical points: (1) with the transformer modelsused to bootstrap enhanced models; (2) before cal-culating the validation and test bleu scores wereport.
we use the sacrebleu script (post, 2018)7on our denormalized output to calculate bleu..4https://github.com/google-research/.
bert.
pegasus.
5https://www.tensorflow.org/datasets/.
catalog/c4#c4multilingual.
6https://github.com/google-research/.
7https://github.com/mjpost/sacrebleu.
4 results.
in this section, we present our main results andexplore the importance of each component in themulti-context model.
additionally, we investigatethe performance impact of document-level paralleldata scarcity, the value of source-side versus target-side context, and the importance of target contextquality..table 2 compares our multi-source and multi-context models to baselines of related prior work,transformer (vaswani et al., 2017), document trans-former (zhang et al., 2018), and the bert-fusedmodel for machine translation (zhu et al., 2020).
we see that a multi-embedding model outper-forms all the single embedding models in eachof the datasets we try.
however, the best multi-embedding conﬁguration varies by dataset.
we ﬁndthat incorporating target-side context does not im-prove performance beyond using source-side con-text alone.
we will present our ablation studies inthe subsequent sections to further shed light on thecauses of this pattern of results.
to preserve thevalue of test set, we report results on the validationset for these experiments..4.1 source context vs. target context.
in some language pairs, the source language isunderspeciﬁed with respect to the obligatory infor-mation that must be given in the target language.
for example, in english every inﬂected verb musthave tense and this is generally not overtly markedin chinese.
in these situations, being able to condi-tion on prior translation decisions would be valu-able.
however, in practice, the target context isonly available post translation, meaning there is arisk of cascading errors.
in this section, we seekto answer two questions: (1) how does the qualityof target context affect document-level translation;(2) whether incorporating high-quality target con-text into source only models adds additional value.
to answer the ﬁrst question, we evaluate thetarget context model pt(3p,3n) using various trans-lations as context.
table 3 shows the bleu scoresachieved by the target context models on the vali-dation set.
the lowest quality context comes fromusing the output of the baseline transformer modelto furnish the context (valid bleu of 48.76); themiddle level comes from a model that conditionson three views of source context (valid bleu of52.8) and the third is an oracle experiment thatuses a human reference translation.
we see that the.
1303base-lines.
thiswork.
modeltransformerdoc transformerbert-fusedmulti-sourcemulti-context+ target.
type embeddingssentdocdocsentdocdoc.
-ds(p,c)bs(p,c)bs(c) ⇒ ps(c)bs(p,c) ⇒ bs(c,n) ⇒ ps(3p,c,3n)multi-context ⇒ pt(3p,3n).
zh|enen|deen|denist iwslt wmt28.4628.6846.69-28.7447.2828.3529.4450.0829.6530.1749.7251.0728.1129.9728.2630.1050.93.table 2: our two main ﬁndings, sacrebleu on test.
(1) source embedding enrichment, represented by our multi-source model, provides a substantial boost to the baseline transformer model.
(2) with adequate quantities ofpaired document training data, models that incorporate extra-sentential context provide an additional performancegain..nist zh → entarget context quality.
modeltransformer.
pt(3p,3n).
context quality ↑ valid48.76-49.3548.7649.8352.8050.32100.00.nist zh → entwo sided context.
side model.
transformerpt(3p,3n).
tgtsrc multi-sourcesrc multi-contextboth multi-context ⇒ pt(3p,3n).
valid42.5143.5144.4245.9346.07.table 3: the value of using context on the target sideof a translation is dependent on its quality.
we test thisin the limit by providing oracle context, which uses oneof the references as context.
we report bleu scores onthe validation set.
the numbers in the second columnare the bleu scores of the translations used as the con-text, indicating the quality of the context..table 4: we remove one of the references from thevalidation dataset and use it to provide target contextonly.
the numbers are lower compared to other tablesbecause the bleu score is calculated w.r.t three refer-ences instead of four.
using human level target contextoffers little value over using source context alone..bleu score improves as the quality of the targetcontext improves; however, the impact is still lessthan the multi-context source model—even in theoracle case!.
next, we explore whether leveraging both sourceand target context works better than only usingsource context.
to control for the confoundingfactor of target context quality, we remove oneof the references from the validation dataset anduse it only as context.
we believe this providesan upper bound on the effect of target context fortwo reasons: (1) it’s reasonable to assume that, atsome point, machine translation will be capableof generating human quality translations; (2) evenwhen this occurs, we will not have access to thestyle of a speciﬁc translator ahead of time.
forthese reasons, we calculate bleu scores using onlythe three remaining references.
we can see in table4 that adding human quality target context to multi-context only produces a 0.14 bleu improvement.
this challenges the notion that target context canadd more value than source context alone..4.2 context ablation.
to assess the importance of the various embeddingsincorporated in the multi-context model, we per-form an ablation study by adding one componentat a time until we reach its full complexity.
table 5shows the study results.
we can see that much ofthe improvement comes from the stronger sentence-level model produced by adding bert’s encodingof the source sentence—a full 2.25 bleu improve-ment.
the beneﬁt of providing contextual embed-dings is more incremental, yet consistent.
addingthe previous sentence gives us 0.44 bleu, addingadditional depth provides another .49, and includ-ing the next sentence adds .37. finally, addingpegasus’ contextual embedding on top of allthis results in a boost of .49. holistically, we canassign 2.45 bleu to source embedding enrichmentand 1.59 to contextual representations..4.3 data scarcity.
nist is a high resource document dataset contain-ing over 1.4m contextualized sentence pairs.
in.
1304nist zh → enembedding ablation.
embeddingstransformerbs(c)bs(c) ⇒ ps(c)bs(p,c)bs(p,c) ⇒ bs(p,c)bs(p,c) ⇒ bs(c,n)bs(p,c) ⇒ ps(3p,c,3n)bs(p,c) ⇒ bs(p,c) ⇒ bs(c,n)bs(p,c) ⇒ bs(c,n) ⇒ ps(3p,c,3n).
valid48.7651.0151.2151.4551.9452.3152.3052.1052.80.table 5: we perform ablation experiments on the nistvalidation dataset to better understand the performanceincrease of the multi-context model.
we conclude that,in this document rich environment, multiple sourcesof embedding enrichment and document context con-tribute to performance.
adding additional parametersalso helps but we only see this when going from one totwo blocks.
parameter control experiments are shownin light grey..this section, we investigate to what extent the quan-tities of parallel documents affect the performanceof our models.
to do so, we retrain enhanced mod-els with subsets of the nist training dataset.
itis important to note that the underlying sentencetransformer model was not retrained in these ex-periments meaning that these experiments simulateadding document context to a strong baseline asdone in lopes et al.
(2020).
figure 2 shows thebleu scores of different models on the nist vali-dation set with respect to the number of contextu-alized sentences used for training.
we can see thatit requires an example pool size over 300k beforethese models outperform the baseline.
we conjec-ture that sufﬁcient contextualized sentence pairsare crucial for document-level models to achievegood performance, which would explain why thesemodels don’t perform well on the iwslt’14 andwmt’14 datasets..further, this pattern of results helps shed light onthe inconsistent ﬁndings in the literature regardingthe effectiveness of document context models.
afew works (kim et al., 2019; li et al., 2020; lopeset al., 2020) have found that the beneﬁt provided bymany document context models can be explainedaway by factors other than contextual conditioning.
we can now see from figure 2 that these experi-ments were done in the low data regime.
the ran-domly initialized context model needs around 600k.
training examples before it signiﬁcantly outperformthe baseline, while the pretrained contextual mod-els reduce this to about 300k.
it is important tonote that none of the conextual models we triedoutperformed the baseline below this point.
thisindicates that data quantity is not the only factorthat matters but it is a prerequisite for the currentclass of document context architectures..4.4 document data augmentation.
we further validate our hypothesis about the im-portance of sufﬁcient contextualized data by exper-imenting with document data augmentation, thistime drawing data from different domains.
weaugment the iwslt dataset with news commen-tary v15, an additional 345k document contextsentence pairs, and repeat the iwslt experiments.
during training, we sample from the datasets suchthat each batch contains roughly 50% of the origi-nal iwslt data.
to ensure a fair comparison, weﬁrst ﬁnetune the baseline transformer model on thenew data, which improves its performance by 1.61bleu.
we use this stronger baseline as the foun-dation for the other models and show the results intable 6. although multi-context edges ahead ofmulti-source, the signiﬁcance lies in the relativeimpact additional document data has on the twoclasses of models.
the average improvement ofthe sentence-level models is 1.58 versus the 1.98experienced by the document models.
huo et al.
(2020) observed a similar phenomenon when us-ing synthetic document augmentation.
this furtheremphasizes the importance of using sufﬁcient con-textualized data when comparing the impact of var-ious document-level architectures, even when thecontextualized data is drawn from a new domain..4.5 three stage training.
wmt’14 offers an opportunity to combine theinsights gained from the aforementioned experi-ments.
this dataset provides large quantities ofsentence pair data and a small amount of documentpair data.
not surprisingly, both bert-fused8 andmulti-context struggle in this environment.
on theother hand, multi-source beneﬁts from the abun-dance of sentence pair data..in order to make the most of the training data,.
8here we mention that, while we were able to reproducethe baseline relative uplift of bert-fused on the other datasets,we were unable to do so on the wmt’14 dataset.
we donot know what document data they used and this probablyaccounts for the differences observed..1305impact of data scarcity.
figure 2: document context models require sufﬁcient contextualized training data in order to be effective.
wesimulate data scarcity on the nist dataset by randomly sampling a subset of the data and using it to train the variousmodels.
in order to outperform the baseline, pretrained models need 300k examples while the doc transformerneeds 600k examples..iwslt’14 en → dedocument augmentation.
type model.
sent.
doc.
transformermulti-sourcebert-fusedmulti-context.
iwslt iwslt+28.6830.1729.4429.97.
30.2931.7131.5031.86.table 6: model performance before and after docu-ment data augmentation.
we see that most of the im-provement is coming from source embedding enrich-ment.
data augmentation is required for document-level models to additionally learn to leverage contex-tual information.
the document-level models beneﬁtsigniﬁcantly more from additional document data thanthe sentence-level models..we add a third stage to our training regime.
as be-fore, in stage one, we train the transformer modelwith the sentence pair data.
in stage two, we trainthe multi-source model also using the sentence pairdata.
in stage three, we add an additional ps(3p,3n)attention block to the multi-source model and trainit with document data.
we perform two documentaugmentation experiments.
in the ﬁrst, we replacenews commentary v9 with v15.
in the second, wetrain on a mix of news commentary v15 and tilderapid 2019. the optimal mix was 70% and 30%respectably, which we found by tuning on the vali-dation dataset.
for each of the augmentation exper-iments, we created new multi-source baselines byﬁne-tuning the original baseline on the new data..when training these new baselines we only up-dated the parameters in the bs(c) and ps(c) atten-tion blocks.
in contrast, when training the treatmentmodels, we froze these blocks and only updatedthe parameters in the ps(3p,3n) block.
in this way,both the new baselines and treatments started fromthe same pretrained multi-source model, were ex-posed to the same data, and had only the parametersunder investigation updated..we see in table 7 that this method can be usedto provide the document-level model with a muchstronger sentence-level model to start from.
as wesaw in the previous data augmentation experiments(§4.4), document augmentation helps the document-level model more than the sentence-level model.
itis interesting to note that out of domain documentdata helps the document-level model yet hurts thesentence-level model.9.
5 related work.
this work is closely related to two lines of research:document-level neural machine translation and rep-resentation learning via language modeling..earlier work in document machine translationexploits the context by taking a concatenated stringof adjacent source sentences as the input of neu-ral sequence-to-sequence models (tiedemann and.
9while tuning on the validation dataset, we observed thatthe optimal proportion of rapid data to include for the newbaseline was 0%.
meaning, don’t include any of the off do-main data.
however, we needed a fair comparison baseline soleft it at 30% when making table 7..1306105106number of sentences with document context4648505254valid bleuiwsltwmtiwslt+wmt+nistmulti-contextbert-fusedmulti-sourcedoc transformerbaselinewmt’14 en → dethree stage training.
stage model.
12.
3.transformermulti-source.
multi-source.
multi-source.
⇒ ps(3p,3n).
testdata28.46sent29.64sent-wmtsent-wmt+29.74sent-wmt++ 29.6229.60doc-wmt29.78doc-wmt+29.89doc-wmt++.
table 7: results from using a three staged trainingapproach.
when there is large disparity between theamount of sentence pair data and document data, thismethod enables training new attention blocks with themaximum amount of available data given their inputrestrictions..scherrer, 2017).
follow-up work adds additionalcontext layers to the neural sequence-to-sequencemodels in order to have a better encoding of thecontext information (zhang et al., 2018; miculi-cich et al., 2018, inter alia).
they vary in termsof whether to incorporate the source-side context(bawden et al., 2018; zhang et al., 2018; miculi-cich et al., 2018) or target-side context (tu et al.,2018), and whether to condition on a few adjacentsentences (jean et al., 2017; wang et al., 2017; tuet al., 2018; voita et al., 2018; zhang et al., 2018;miculicich et al., 2018) or the full document (haf-fari and maruf, 2018; maruf et al., 2019).
our workis similar to this line of research since we have alsointroduced additional attention components to thetransformer.
however, our model is different fromtheirs in that the context encoders were pretrainedwith a masked language model objective..there has also been work on leveraging mono-lingual documents to improve document-level ma-chine translation.
junczys-dowmunt (2019) cre-ates synthetic parallel documents generated bybacktranslation (sennrich et al., 2016; edunovet al., 2018) and uses the combination of the origi-nal and the synthetic parallel documents to train thedocument translation models.
voita et al.
(2019)train a post-editing model from monolingual docu-ments to post-edit sentence-level translations intodocument-level translations.
yu et al.
(2020b,a)uses bayes’ rule to combine a monolingual doc-ument language model probability with sentencetranslation probabilities..finally, large-scale representation learning withlanguage modeling has achieved success in im-.
proving systems in language understanding, lead-ing to state-of-the-art results on a wide range oftasks (peters et al., 2018; devlin et al., 2019; rad-ford et al., 2018; mccann et al., 2017; yang et al.,2019; chronopoulou et al., 2019; lample and con-neau, 2019; brown et al., 2020).
they have alsobeen used to improve text generation tasks, suchas sentence-level machine translation (song et al.,2019; edunov et al., 2019; zhu et al., 2020) andsummarization (zhang et al., 2019, 2020; donget al., 2019), and repurposing unconditional lan-guage generation (ziegler et al., 2019; de oliveiraand rodrigo, 2019).
our work is closely related tothat from zhu et al.
(2020), where pretrained large-scale language models are applied to document-level machine translation tasks.
we advance thisline of reasoning by designing an architecture thatuses composition to incorporate multiple pretrainedmodels at once.
it also enables conditioning ondifferent inputs to the same pretrained model, en-abling us to circumvent bert’s two sentence em-bedding limit..6 conclusion.
we have introduced an architecture and trainingregimen that enables incorporating representationsfrom multiple pretrained masked language modelsinto a transformer model.
we show that this tech-nique can be used to create a substantially strongersentence-level model and, with sufﬁcient documentdata, further upgraded to a document-level modelthat conditions on contextual information.
throughablations and other experiments, we establish doc-ument augmentation and multi-stage training aseffective strategies for training a document-levelmodel when faced with data scarcity.
and thatsource side context is sufﬁcient for these models,with target context adding little additional value..acknowledgments.
we would like to thank our teammates, laurentsartran, phil blunsom, susie young, wang ling,and wojciech stokowiec, for their feedback andshared engineering efforts.
we thank yao zhaofor helping us to better understand the pegasuscodebase.
we thank dani yogatama and our threeanonymous reviewers for their feedback on theearlier draft of the paper.
their feedback was takenseriously and we believe this work has beneﬁtedfrom the items they requested..1307references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin proceedings oflearning to align and translate.
iclr..of reusable object-oriented software.
addison-wesley longman publishing co., inc., usa..gholamreza haffari and sameen maruf.
2018. docu-ment context neural machine translation with mem-ory networks.
in proceedings of acl..rachel bawden, rico sennrich, alexandra birch, andbarry haddow.
2018. evaluating discourse phenom-ena in neural machine translation.
in proceedings ofnaacl-hlt..jingjing huo, christian herold, yingbo gao, leonarddahlmann, shahram khadivi, and hermann ney.
2020. diving deep into context-aware neural ma-chine translation.
in proceedings of wmt..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in proceedings of neurips..mia xu chen, orhan firat, ankur bapna, melvinjohnson, wolfgang macherey, george foster, llionjones, mike schuster, noam shazeer, niki parmar,ashish vaswani, jakob uszkoreit, lukasz kaiser,zhifeng chen, yonghui wu, and macduff hughes.
2018. the best of both worlds: combining recentadvances in neural machine translation.
in proceed-ings of acl..alexandra chronopoulou, christos baziotis,.
andalexandros potamianos.
2019. an embarrassinglysimple approach for transfer learning from pre-trained language models.
in proceedings of naacl-hlt..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of naacl-hlt..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
corr, abs/1905.03197..sergey edunov, alexei baevski, and michael auli.
2019. pre-trained language model representationsfor language generation.
in proceedings of naacl-hlt..sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atscale.
in proceedings of emnlp..yang fan, shufang xie, yingce xia, lijun wu, taoqin, xiang-yang li, and tie-yan liu.
2020. multi-branch attentive transformer..erich gamma, richard helm, ralph johnson, andjohn vlissides.
1995. design patterns: elements.
s´ebastien jean, stanislas lauly, orhan firat, anddoes neural machinecorr,.
kyunghyun cho.
2017.translation beneﬁt from larger context?
abs/1704.05135..marcin junczys-dowmunt.
2019. microsoft translatorat wmt 2019: towards large-scale document-levelneural machine translation.
in proceedings of wmt..yunsu kim, duc thanh tran, and hermann ney.
2019.when and why is document-level context useful inneural machine translation?
in proceedings of thefourth workshop on discourse in machine transla-tion (discomt 2019)..taku kudo.
2018. subword regularization: improvingneural network translation models with multiple sub-word candidates.
in proceedings of acl..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of emnlp..guillaume lample and alexis conneau.
2019. cross-corr,language model pretraining..lingualabs/1901.07291..bei li, hui liu, ziyang wang, yufan jiang, tong xiao,jingbo zhu, tongran liu, and changliang li.
2020.does multi-encoder help?
a case study on context-aware neural machine translation.
in proceedings ofacl..ant´onio lopes, m. amin farajian, rachel bawden,michael zhang, and andr´e f. t. martins.
2020.document-level neural mt: a systematic compari-son.
in proceedings of the 22nd annual conferenceof the european association for machine transla-tion..sameen maruf, andr´e f. t. martins, and gholam-reza haffari.
2019. selective attention for context-aware neural machine translation.
in proceedings ofnaacl-hlt..bryan mccann, james bradbury, caiming xiong, andlearned in translation:in proceedings of.
richard socher.
2017.contextualized word vectors.
neurips..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neuralmachine translation with hierarchical attention net-works.
in proceedings of emnlp..1308luke de oliveira and alfredo l´ainez rodrigo.
2019. repurposing decoder-transformer languagearxiv,models for abstractive summarization.
abs/1909.00325..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proceedings of naacl..matt post.
2018. a call for clarity in reporting bleu.
scores.
in proceedings of wmt..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..rico sennrich, barry haddow, and alexandra birch.
2016. improving neural machine translation modelswith monolingual data.
in proceedings of acl..lei yu, laurent sartran, wojciech stokowiec, wangling, lingpeng kong, phil blunsom, and chrisdyer.
2020b.
better document-level machine trans-lation with bayes’ rule.
trans.
assoc.
comput.
lin-guistics, 8:346–360..haoyu zhang, yeyun gong, yu yan, nan duan, jian-jun xu, ji wang, ming gong, and ming zhou.
2019.pretraining-based natural language generation fortext summarization.
corr, abs/1902.09243..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of emnlp..jingqing zhang, yao zhao, mohammad saleh, and pe-ter j. liu.
2020. pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization.
in proceedings of icml..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in pro-ceedings of icml..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tie-yan liu.
2020.incorporating bert into neural machinetranslation.
in proceedings of iclr..zachary m. ziegler, luke melas-kyriazi, sebastiangehrmann, and alexander m. rush.
2019. encoder-agnostic adaptation for conditional language genera-tion.
corr, abs/1908.06938..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of discomt@emnlp..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018. learning to remember translation history witha continuous cache.
tacl, 6:407–420..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of neurips..elena voita, rico sennrich, and ivan titov.
2019.context-aware monolingual repair for neural ma-in proceedings of emnlp-chine translation.
ijcnlp..elena voita, pavel serdyukov, rico sennrich, andivan titov and.
2018. context-aware neural ma-chine translation learns anaphora resolution.
in pro-ceedings of acl..longyue wang, zhaopeng tu, andy way, and qun liu.
2017. exploiting cross-sentence context for neuralmachine translation.
in proceedings of emnlp..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
corr, abs/1906.08237..lei yu, laurent sartran, po-sen huang, woj-ciech stokowiec, domenic donato, srivatsan srini-vasan, alek andreev, wang ling, sona mokra,agustin dal lago, yotam doron, susannah young,phil blunsom, and chris dyer.
2020a.
the deep-mind chinese-english document translation systemat wmt2020.
in proceedings of wmt@emnlp..1309a preprocessing.
a.1 text.
we perform text normalization on the datasets be-fore tokenization..• all languages - unicode canonicalization(nkfd from), replacement of common mul-tiple encoding errors present in training data,standardization of quotation marks into “di-rectional” variants..• english - replace non-american spelling vari-ants with american spellings using the aspelllibrary.10 punctuation was split from englishwords using a purpose-built library..• chinese - convert any traditional chinesecharacters into simpliﬁed forms and segmentinto word-like units using the jieba segmenta-tion tool.11.
• english & german for wmt’14 - lower-case ﬁrst word of sentence unless it was ina whitelist of proper nouns and common ab-breviations..• english & german for iwslt’14 - lowercase.
• chinese & english for nist - lowercase all.
all words..words..a.2 tokenization.
we encode text into sub-word units using thesentencepiece tool (kudo and richardson,2018).
when generating our own subword segmen-tation, we used the algorithm from kudo (2018)with a minimum character coverage of 0.9995.other than for bert, we use tensorflow senten-cepiecetokenizer for tokenization given a senten-cepiece model..• bert (all) - used vocabulary provided withdownload and tensorflow berttokenizer..• pegasus large & en small - used sentence-piece model provided with pegasus largedownload..• pegasus zh small - generated subword vo-cabulary of 34k tokens from the nist dataset..10http://wordlist.aspell.net/.
varcon-readme/.
11https://github.com/fxsjy/jieba.
• pegasus de small - generated subword vo-cabulary of 34k tokens from the wmt’14dataset..• transformers - generated joint subword vo-cabulary of 34k tokens for nist & wmt’14and 20k for iwslt’14..b compute.
we train and evaluate on google tpu v2.
we usea 4x2 conﬁguration which contains 16 processingunits.
we use the following global batch sizesduring training (examples / tokens):.
• transformer baselines: (1024 / 131,072).
• wmt’14 multi-source: (1024 / 131,072).
• wmt’14 others: (128 / 16,384).
• nist: (256 / 32,767).
• iwslt’14: (256 / 32,767).
using a global batch size of 32 and a beam widthof 5, the following are the number of samples persecond our models and key baselines managed dur-ing inference:.
• transformer: 11.94.
• bert-fused: 7.37.
• multi-source: 5.45.
• multi-context: 4.80.c qualitative analysis.
we manually inspected the translations outputsfrom the multi-source model and multi-contextmodel and have found that the multi-context modelindeed does better in terms maintaining the con-sistency of lexical usage across sentences.
unlikeenglish, chinese does not mark nouns for pluralvs singular nor verbs for tense.
therefore, thisneeds to be inferred from context to generate ac-curate english translations.
it is not possible for asentence-level mt system to capture this informa-tion when the relevant context is not in the currentsentence.
tables 8, 9, and 10 provide various exam-ples where the sentence-level model cannot knowthis information and the document-level model isable to correctly condition on it..1310example 1consistency of tense.
source:.
reference:.
multi-source:.
multi-context:.
金先生说,五十几岁时,王选便开始注意培养年轻人,他一直强调,要铺路,要甘为人梯,给年轻人让路。mr. jin said that wang xuan started to focus on mentoring young people whenhe was in his 50s.
he constantly stressed that he wanted to pave the way foryoung people and that he wanted to be their stepping stone..mr. chin says that when he was in his ﬁfties, wang began to pay attention tocultivating young people.
he has always stressed that to pave the way, he mustbe willing to serve as a ladder for young people..mr. jin said that when he was in his ﬁfties, wang xuan began to pay attentionto cultivating young people.
he always stressed that he wanted to pave the way,to be willing to serve as a ladder, and to give young people a way..table 8: this came from an article describing an interview with a celebrity.
the entire article used past tense..example 2consistency of proper noun.
source:.
巴政府是决不会让这种企图得逞的。.
reference:.
the pakistani government will never allow such attempt to materialize..multi-source:.
the palestinian government will never let this attempt succeed..multi-context:.
the pakistani government will never let this attempt succeed..table 9: the pronoun 巴政府 is ambiguous since 巴 could be short for 巴西 (brazil)，巴勒斯坦 (palestine)，巴基斯坦 (pakistan).
the model has to refer to the context to know that 巴 refers to pakistan in this instance sincethis is where the entire article takes place..example 2consistency of pronoun.
source:.
reference:.
multi-source:.
multi-context:.
那是在十年前的一天，当这位老师正利用中午休息时间，在家里睡觉时，突然间，电话铃响了，.
on that day ten years ago, when this teacher was taking a nap at home duringnoontime break, the telephone rang suddenly..that was ten years ago.
when this teacher was taking advantage of his lunchbreak, he was sleeping at home.
suddenly, the phone rang..one day ten years ago, when this teacher was taking advantage of her lunchbreak, she was sleeping at home.
suddenly, the telephone rang..table 10: this is a story about a mother.
the pronouns she/her have been used across the document.
one cannotinfer the gender of the teacher from the source sentence alone.
thus, the context model has to refer to the othersentences in order to get this correct..1311