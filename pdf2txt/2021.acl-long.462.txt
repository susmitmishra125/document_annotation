instantaneous grammatical error correction withshallow aggressive decoding.
xin sun1∗† tao ge2† furu wei2 houfeng wang11 moe key lab of computational linguistics, school of eecs, peking university;2 microsoft research asia{sunx5,wanghf}@pku.edu.cn;{tage,fuwei}@microsoft.com.
abstract.
in this paper, we propose shallow aggressivedecoding (sad) to improve the online infer-ence efﬁciency of the transformer for instan-taneous grammatical error correction (gec).
sad optimizes the online inference efﬁciencyfor gec by two innovations: 1) it aggres-sively decodes as many tokens as possible inparallel instead of always decoding only onetoken in each step to improve computationalparallelism; 2) it uses a shallow decoder in-stead of the conventional transformer archi-tecture with balanced encoder-decoder depthto reduce the computational cost during infer-ence.
experiments in both english and chi-nese gec benchmarks show that aggressivedecoding could yield the same predictions asgreedy decoding but with a signiﬁcant speedupfor online inference.
its combination with theshallow decoder could offer an even higheronline inference speedup over the powerfultransformer baseline without quality loss.
notonly does our approach allow a single model toachieve the state-of-the-art results in englishgec benchmarks: 66.4 f0.5 in the conll-14 and 72.9 f0.5 in the bea-19 test set withan almost 10× online inference speedup overthe transformer-big model, but also it is easilyadapted to other languages.
our code is avail-able at https://github.com/autotemp/shallow-aggressive-decoding..2020; omelianchuk et al., 2020) for its poor infer-ence efﬁciency in modern writing assistance ap-plications (e.g., microsoft ofﬁce word1, googledocs2 and grammarly3) where a gec model usu-ally performs online inference, instead of batch in-ference, for proactively and incrementally checkinga user’s latest completed sentence to offer instanta-neous feedback..to better exploit the transformer for instanta-neous gec in practice, we propose a novel ap-proach – shallow aggressive decoding (sad) toimprove the model’s online inference efﬁciency.
the core innovation of sad is aggressive decoding:instead of sequentially decoding only one token ateach step, aggressive decoding tries to decode asmany tokens as possible in parallel with the assump-tion that the output sequence should be almost thesame with the input.
as shown in figure 1, if theoutput prediction at each step perfectly matches itscounterpart in the input sentence, the inference willﬁnish, meaning that the model will keep the inputuntouched without editing; if the output token ata step does not match its corresponding token inthe input, we will discard all the predictions afterthe bifurcation position and re-decode them in theoriginal autoregressive decoding manner until weﬁnd a new opportunity for aggressive decoding.
inthis way, we can decode the most text in parallelin the same prediction quality as autoregressivegreedy decoding, but largely improve the inferenceefﬁciency..in addition to aggressive decoding, sad pro-poses to use a shallow decoder, instead of theconventional transformer with balanced encoder-decoder depth, to reduce the computational cost forfurther accelerating inference.
the experimental.
1.introduction.
the transformer (vaswani et al., 2017) has becomethe most popular model for grammatical error cor-rection (gec).
in practice, however, the sequence-to-sequence (seq2seq) approach has been blamedrecently (chen et al., 2020; stahlberg and kumar,.
∗ this work was done during the author’s internship at.
microsoft-365/word.
msr asia.
contact person: tao ge (tage@microsoft.com).
† co-ﬁrst authors with equal contributions.
2https://www.google.com/docs/about3https://www.grammarly.com.
1https://www.microsoft.com/en-us/.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5937–5947august1–6,2021.©2021associationforcomputationallinguistics5937figure 1: the overview of aggressive decoding.
aggressive decoding tries decoding as many tokens as possiblein parallel with the assumption that the input and output should be almost the same in gec.
when we ﬁnd a bifur-cation between the input and the output of aggressive decoding, then we accept the predictions before (including)the bifurcation, and discard all the predictions after the bifurcation and re-decode them using original one-by-oneautoregressive decoding.
if we ﬁnd a sufﬁx match (i.e., some advice highlighted with the blue dot lines) betweenthe output and the input during one-by-one re-decoding, we switch back to aggressive decoding by copying thetokens (highlighted with the orange dashed lines) following the matched tokens in the input to the decoder inputby assuming they are likely to be the same..results in both english and chinese gec bench-marks show that both aggressive decoding and theshallow decoder can signiﬁcantly improve onlineinference efﬁciency.
by combining these two tech-niques, our approach shows a 9× ∼ 12× onlineinference speedup over the powerful transformerbaseline without sacriﬁcing the quality..the contributions of this paper are two-fold:.
• we propose a novel aggressive decoding ap-proach, allowing us to decode as many tokenas possible in parallel, which yields the samepredictions as greedy decoding but with a sub-stantial improvement of computational paral-lelism and online inference efﬁciency..• we propose to combine aggressive decodingwith the transformer with a shallow decoder.
our ﬁnal approach not only advances the state-of-the-art in english gec benchmarks withan almost 10× online inference speedup butalso is easily adapted to other languages..2 background: transformer.
the transformer is a seq2seq neural network archi-tecture based on multi-head attention mechanism,which has become the most successful and widely.
used seq2seq models in various generation taskssuch as machine translation, abstractive summa-rization as well as gec..the original transformer follows the balancedencoder-decoder architecture: its encoder, consist-ing of a stack of identical encoder layers, maps aninput sentence x = (x1, .
.
.
, xn) to a sequenceof continuous representation z = (z1, .
.
.
, zn);and its decoder, which is composed of a stack ofthe same number of identical decoder layers asthe encoder, generates an output sequence o =(o1, .
.
.
, om) given z..in the training phase, the model learns an autore-gressive scoring model p (y | x; φ), implementedwith teacher forcing:.
φ∗ = arg max.
log p (y | x; φ).
= arg max.
log p (yi+1 | y≤i, x; φ).
(1).
φ.φ.l−1(cid:88).
i=0.
where y = (y1, .
.
.
, yl) is the ground-truth targetsequence and y≤i = (y0, .
.
.
, yi).
as ground truthis available during training, eq (1) can be efﬁcientlyobtained as the probability p (yi+1 | y≤i, x) at eachstep can be computed in parallel..during inference, the output sequence o =.
5938[bos]i'mwri,ngtoinformsomesomeadviceontravelingandworking.[pad]i'mwri,ngtogiveyouadviceadviceontravelingandworking.
[eos]﹅✔✔✔✔✔✘✘✘✘✘✘✘✘✘✘inputoutput[bos]i'mwri,ngtogiveyou[bos]i'mwri,ngtogiveyousome[bos]i'mwri,ngtogiveyousomeadviceone-by-onedecodingforsuﬃxmatchini9alaggressivedecoding(inparallel)re-decodingswitchbacktoaggressivedecoding(inparallel)bifurcation✔accept✘discard﹅no prediction[bos]i'mwri,ngtogiveyousomeadviceontravelingandworking.[pad]﹅﹅﹅﹅﹅﹅﹅﹅ontravelingandworking.
[eos]﹅✔✔✔✔✔✔✘decoderinputoutputbifurcation(o1, .
.
.
, om) is derived by maximizing the follow-ing equation:.
o∗ = arg max.
log p (o | x; φ).
= arg max.
log p (oj+1 | o≤j, x; φ).
(2).
o.o.m−1(cid:88).
j=0.
since no ground truth is available in the infer-ence phase, the model has to decode only one tokenat each step conditioning on the previous decodedtokens o≤j instead of decoding in parallel as in thetraining phase..3 shallow aggressive decoding.
3.1 aggressive decoding.
as introduced in section 2, the transformer de-codes only one token at each step during inference.
the autoregressive decoding style is the main bot-tleneck of inference efﬁciency because it largelyreduces computational parallelism..for gec, fortunately, the output sequence is usu-ally very similar to the input with only a few editsif any.
this special characteristic of the task makesit unnecessary to follow the original autoregres-sive decoding style; instead, we propose a noveldecoding approach – aggressive decoding whichtries to decode as many tokens as possible duringinference.
the overview of aggressive decoding isshown in figure 1, and we will discuss it in detailin the following sections..3.1.1.initial aggressive decoding.
the core motivation of aggressive decoding isthe assumption that the output sequence o =(o1, .
.
.
, om) should be almost the same with theinput sequence x = (x1, .
.
.
, xn) in gec.
at theinitial step, instead of only decoding the ﬁrst tokeno1 conditioning on the special [bos] token o0, ag-gressive decoding decodes o1...n conditioning onthe pseudo previous decoded tokens ˆo0...n−1 in par-allel with the assumption that ˆo0...n−1 = x0,...,n−1.
speciﬁcally, for j ∈ {0, 1, .
.
.
, n − 2, n − 1}, oj+1is decoded as follows:.
o∗j+1 = arg maxoj+1.
log p (oj+1 |o≤j, x; φ).
= arg maxoj+1.
= arg maxoj+1.
log p (oj+1 | ˆo≤j, x; φ).
(3).
log p (oj+1 | x≤j, x; φ).
where ˆo≤j is the pseudo previous decoded tokensat step j + 1, which is assumed to be the same withx≤j..after we obtain o1...n, we verify whether o1...nif o1...n isis actually identical to x1...n or not.
fortunately exactly the same with x1...n, the infer-ence will ﬁnish, meaning that the model ﬁnds nogrammatical errors in the input sequence x1...n andkeeps the input untouched.
in more cases, how-ever, o1...n will not be exactly the same with x1...n.in such a case, we have to stop aggressive decod-ing and ﬁnd the ﬁrst bifurcation position k so thato1...k−1 = x1...k−1 and ok (cid:54)= xk..since o1...k−1 = ˆo1...k−1 = x1...k−1, the pre-dictions o1...k could be accepted as they will notbe different even if they are decoded through theoriginal autoregressive greedy decoding.
however,for the predictions ok+1...n, we have to discard andre-decode them because ok (cid:54)= ˆok..3.1.2 re-decodingas ok (cid:54)= ˆok = xk, we have to re-decode for oj+1(j ≥ k) one by one following the original autore-gressive decoding:.
o∗j+1 = arg maxoj+1.
p (oj+1 | o≤j, x; φ).
(4).
after we obtain o≤j (j > k), we try to match itssufﬁx to the input sequence x for further aggressivedecoding.
if we ﬁnd its sufﬁx oj−q...j (q ≥ 0) is theunique substring of x such that oj−q...j = xi−q...i,then we can assume that oj+1... will be very likelyto be the same with xi+1... because of the specialcharacteristic of the task of gec..if we fortunately ﬁnd such a sufﬁx match, thenwe can switch back to aggressive decoding to de-code in parallel with the assumption ˆoj+1... =xi+1.... speciﬁcally, the token oj+t (t > 0) isdecoded as follows:.
o∗j+t = arg maxoj+t.
p (oj+t | o<j+t, x; φ).
(5).
in eq (5), o<j+t is derived as follows:.
o<j+t = cat(o≤j, ˆoj+1...j+t−1)= cat(o≤j, xi+1...i+t−1).
(6).
where cat(a, b) is the operation that concatenatestwo sequences a and b..otherwise (i.e., we cannot ﬁnd a sufﬁx match atthe step), we continue decoding using the original.
5939algorithm 1 aggressive decodinginput: φ, x = ([bos], x1, .
.
.
, xn, [p ad]), o = (o0) = ([bos]);output: o1...j = (o1, .
.
.
, oj);.
1: initialize j ← 0;2: while oj (cid:54)= [eos] and j < max len do3:.
if oj−q...j (q ≥ 0) is a unique substring of x such that ∃ !
i : oj−q...j = xi−q...i then.
aggressive decode (cid:101)oj+1... according to eq (5) and eq (6);find bifurcation j + k (k > 0) such that (cid:101)oj+1...j+k−1 = xi+1...i+k−1 and (cid:101)oj+k (cid:54)= xi+k;o ← cat(o, (cid:101)oj+1...j+k);j ← j + k;.
j+1 = arg maxoj+1 p (oj+1 | o≤j, x; φ);.
4:.
5:.
6:.
7:.
8:.
9:.
10:.
11:.
else.
decode o∗o ← cat(o, o∗j ← j + 1;.
j+1);.
end if12:13: end while.
autoregressive greedy decoding approach until weﬁnd a sufﬁx match..we summarize the process of aggressive decod-ing in algorithm 1. for simplifying implementa-tion, we make minor changes in algorithm 1: 1) weset o0 = x0 = [bos] in algorithm 1, which en-ables us to regard the initial aggressive decoding asthe result of sufﬁx match of o0 = x0; 2) we appenda special token [p ad] to the end of x so that thebifurcation (in the 5th line in algorithm 1) must ex-ist (see the bottom example in figure 1).
since wediscard all the computations and predictions afterthe bifurcation for re-decoding, aggressive decod-ing guarantees that generation results are exactlythe same as greedy decoding (i.e., beam=1).
how-ever, as aggressive decoding decodes many tokensin parallel, it largely improves the computationalparallelism during inference, greatly beneﬁting theinference efﬁciency..3.2 shallow decoder.
even though aggressive decoding can signiﬁcantlyimprove the computational parallelism during infer-ence, it inevitably leads to intensive computationand even possibly introduces additional computa-tion caused by re-decoding for the discarded pre-dictions..to reduce the computational cost for decoding,we propose to use a shallow decoder, which hasproven to be an effective strategy (kasai et al.,2020; li et al., 2021) in neural machine transla-tion (nmt), instead of using the transformer withbalanced encoder-decoder depth as the previousstate-of-the-art transformer models in gec.
by.
combining aggressive decoding with the shallowdecoder, we are able to further improve the infer-ence efﬁciency..4 experiments.
4.1 data and model conﬁguration.
we follow recent work in english gec to con-duct experiments in the restricted training settingof bea-2019 gec shared task (bryant et al.,2019): we use lang-8 corpus of learner en-glish (mizumoto et al., 2011), nucle (dahlmeieret al., 2013), fce (yannakoudakis et al., 2011) andw&i+locness (granger; bryant et al., 2019)as our gec training data.
for facilitating faircomparison in the efﬁciency evaluation, we fol-low the previous studies (omelianchuk et al., 2020;chen et al., 2020) which conduct gec efﬁciencyevaluation to use conll-2014 (ng et al., 2014)dataset that contains 1,312 sentences as our maintest set, and evaluate the speedup as well as max-match (dahlmeier and ng, 2012) precision, recalland f0.5 using their ofﬁcial evaluation scripts4.
forvalidation, we use conll-2013 (ng et al., 2013)that contains 1,381 sentences as our validation set.
we also test our approach on nlpcc-18 chinesegec shared task (zhao et al., 2018), followingtheir training5 and evaluation setting, to verify theeffectiveness of our approach in other languages.
to compare with the state-of-the-art approachesin english gec that pretrain with synthetic data,.
4https://github.com/nusnlp/m2scorer5following chen et al.
(2020), we sample 5,000 training.
instances as the validation set..5940model.
synthetic data total latency (s).
speedup.
transformer-big (beam=5)transformer-big (greedy)transformer-big (aggressive)transformer-big (beam=5)transformer-big (greedy)transformer-big (aggressive).
nononoyesyesyes.
4403285443732060.conll-13r18.0018.3418.3423.6224.7024.70.f0.538.5038.3638.3644.4744.9144.91.p53.8452.7552.7557.0656.4556.45.
1.0×1.3×8.1×1.0×1.4×7.3×.
table 1: the performance and online inference efﬁciency of the transformer-big with aggressive decoding in ourvalidation set (conll-13) that contains 1,381 sentences.
we use transformer-big (beam=5) as the baseline tocompare the performance and efﬁciency of aggressive decoding..we also synthesize 300m error-corrected sentencepairs for pretraining the english gec model follow-ing the approaches of grundkiewicz et al.
(2019)and zhang et al.
(2019).
note that in the followingevaluation sections, the models evaluated are bydefault trained without the synthetic data unlessthey are explicitly mentioned..we use the most popular gec model architec-ture – transformer (big) model (vaswani et al.,2017) as our baseline model which has a 6-layerencoder and 6-layer decoder with 1,024 hiddenunits.
we train the english gec model using anencoder-decoder shared vocabulary of 32k bytepair encoding (sennrich et al., 2016) tokens andtrain the chinese gec model with 8.4k chinesecharacters.
we include more training details in thesupplementary notes.
for inference, we use greedydecoding6 by default..all the efﬁciency evaluations are conducted inthe online inference setting (i.e., batch size=1)as we focus on instantaneous gec.
we performmodel inference with fairseq7 implementation us-ing pytorch 1.5.1 with 1 nvidia tesla v100-pcieof 16gb gpu memory under cuda 10.2..4.2 evaluation for aggressive decoding.
we evaluate aggressive decoding in our validationset (conll-13) which contains 1,381 validationexamples.
as shown in table 1, aggressive decod-ing achieves a 7× ∼ 8× speedup over the originalautoregressive beam search (beam=5), and gener-ates exactly the same predictions as greedy decod-ing, as discussed in section 3.1.2. since greedydecoding can achieve comparable overall perfor-mance (i.e., f0.5) with beam search while it tends.
6our implementation of greedy decoding is simpliﬁed forhigher efﬁciency (1.3× ∼ 1.4× speedup over beam=5) thanthe implementation of beam=1 decoding in fairseq (around1.1× speedup over beam=5)..7https://github.com/pytorch/fairseq.
figure 2: the speedup (over greedy decoding) distribu-tion of all the 1,381 validation examples with respectto their edit ratio in conll-13..to make more edits resulting in higher recall butlower precision, the advantage of aggressive decod-ing in practical gec applications is obvious givenits strong performance and superior efﬁciency..we further look into the efﬁciency improve-ment by aggressive decoding.
figure 2 showsthe speedup distribution of the 1,381 examples inconll-13 with respect to their edit ratio whichis deﬁned as the normalized (by the input length)edit distance between the input and output.
it isobvious that the sentences with fewer edits tend toachieve higher speedup, which is consistent withour intuition that most tokens in such sentencescan be decoded in parallel through aggressive de-coding; on the other hand, for the sentences thatare heavily edited, their speedup is limited becauseof frequent re-decoding.
to give a more intuitiveanalysis, we also present concrete examples withvarious speedup in our validation set to understandhow aggressive decoding improves the inferenceefﬁciency in table 2..moreover, we conduct an ablation study to in-.
59410.000.050.100.150.200.250.300.350.400.45edit ratio05101520253035speedupspeedup16.7×.
edit ratio0.
5.8×.
6.8×.
0.
0.03.
5.1×.
0.06.
3.5×.
1.5×.
1.4×.
0.13.
0.27.
0.41.inputpersonally , i think surveillance technologysuch as rfid ( radio-frequency identiﬁca-tion ) should not be used to track people , forthe beneﬁt it brings to me can not match theconcerns it causes .
nowadays , people use the all-purpose smartphone for communicating .
because that the birth rate is reduced whilethe death rate is also reduced , the percentageof the elderly is increased while that of theyouth is decreased .
more importantly , they can share their ideasof how to keep healthy through internet ,to make more interested people get involveand ﬁnd ways to make life longer and morewonderful .
as a result , people have more time to enjoyadvantage of modern life .
nowadays , technology is more advancethan the past time .
people are able to predicate some disasterslike the earth quake and do the preventionbeforehand ..output[personally , i think surveillance technologysuch as rfid ( radio-frequency identiﬁcation) should not be used to track people , for the ben-eﬁt it brings to me can not match the concerns itcauses .
]0[nowadays , people use the all-purpose smartphone for communicating .
]0[because the]0 [birth]1 [rate is reduced whilethe death rate is also reduced , the percentage ofthe elderly is increased while that of the youthis decreased .
]2[more importantly , they can share their ideas ofhow to keep healthy through the]0 [internet]1 [,to make more interested people get involved]2[and]3 [ﬁnd]4 [ways to make life longer andmore wonderful .
]5[as a result , people have more time to enjoythe]0 [advantages]1 [of]2 [modern life .
]3[nowadays , technology is more advanced]0[than]1 [in]2 [the]3 [past .
]4[people are able to predict]0 [disasters]1 [likethe earthquake]2 [and]3 [prevent]4 [them]5 [be-forehand]6 [.]7.
table 2: examples of various speedup ratios by aggressive decoding over greedy decoding in conll-13.
weshow how the examples are decoded in the column of output, where the tokens within a blue block are decoded inparallel through aggressive decoding while the tokens in red blocks are decoded through the original autoregressivegreedy decoding..lmax1 (baseline)235102040unlimited.
total latency (s)32820814810975645454.speedup1.0×1.6×2.2×3.0×4.4×5.1×6.1×6.1×.
table 3: the ablation study of the effect of constrainingthe maximal aggressive decoding length lmax on theonline inference efﬁciency in conll-13.
note that inconll-13, the average length of an example is 21 and96% examples are shorter than 40 tokens..vestigate whether it is necessary to constrain themaximal aggressive decoding length8, because itmight become highly risky to waste large amountsof computation because of potential re-decodingfor a number of steps after the bifurcation if we ag-gressively decode a very long sequence in parallel.
table 3 shows the online inference efﬁciency withdifferent maximal aggressive decoding lengths.
itappears that constraining the maximal aggressive.
8constraining the maximal aggressive decoding length tolmax means that the model can only aggressively decode atmost lmax tokens in parallel..model(enc+dec)6+63+69+66+36+97+58+49+310+211+1.
conll-13f0.538.3636.2638.8237.9538.0238.4938.6338.8838.2138.15.totallatency32831434517545727124018113786.speedup.
1.0×1.0×1.0×1.9×0.7×1.2×1.4×1.8×2.4×3.8×.
table 4: the performance and efﬁciency of the trans-former with different encoder and decoder depths inconll-13, where 6+6 is the original transformer-bigmodel that has a 6-layer encoder and a 6-layer decoder..decoding length does not help improve the efﬁ-ciency; instead, it slows down the inference if themaximal aggressive decoding length is set to asmall number.
we think the reason is that sen-tences in gec datasets are rarely too long.
forexample, the average length of the sentences inconll-13 is 21 and 96% of them are shorter than40 tokens.
therefore, it is unnecessary to constrainthe maximal aggressive decoding length in gec..5942model.
synthetic data.
transformer-big (beam=5)levenshtein transformer(cid:63) (gu et al., 2019)lasertagger(cid:63) (malmi et al., 2019)span correction(cid:63) (chen et al., 2020)our approach (9+3)transformer-big (beam=5)pie(cid:63) (awasthi et al., 2019)span correction(cid:63) (chen et al., 2020)our approach (9+3)seq2edits (stahlberg and kumar, 2020)gector(roberta) (omelianchuk et al., 2020)gector(xlnet) (omelianchuk et al., 2020)our approach (12+2 bart-init).
nononononoyesyesyesyesyesyesyesyes.
multi-stagefine-tuningnononononononononoyesyesyesyes.
conll-14r32.123.626.924.733.138.143.037.241.345.641.540.152.8.f0.551.242.543.249.550.961.659.761.063.558.664.065.366.4.speedup1.0×2.9×29.6×2.6×10.5×1.0×10.3×2.6×10.3×-12.4×-9.6×.
p60.253.150.966.058.873.066.172.673.363.073.977.571.0.table 5: the performance and online inference efﬁciency evaluation of efﬁcient gec models in conll-14.
forthe models with (cid:63), their performance and speedup numbers are from chen et al.
(2020) who evaluate the onlineefﬁciency in the same runtime setting (e.g., gpu and runtime libraries) with ours.
the underlines indicate thespeedup numbers of the models are evaluated with tensorﬂow based on their released codes, which are not strictlycomparable here.
note that for gector, we re-implement its inference process of gector (roberta) usingfairseq for testing its speedup in our setting.
- means the speedup cannot be tested in our runtime environmentbecause the model has not been released or not implemented in fairseq..4.3 evaluation for shallow decoder.
we study the effects of changing the number ofencoder and decoder layers in the transformer-bigon both the performance and the online inferenceefﬁciency.
by comparing 6+6 with 3+6 and 9+6in table 4, we observe the performance improvesas the encoder becomes deeper, demonstrating theimportance of the encoder in gec.
in contrast, bycomparing the 6+6 with 6+3 and 6+9, we do notsee a substantial ﬂuctuation in the performance,indicating no necessity of a deep decoder.
more-over, it is observed that a deeper encoder does notsigniﬁcantly slow down the inference but a shal-low decoder can greatly improve the inference efﬁ-ciency.
this is because transformer encoders canbe parallelized efﬁciently on gpus, whereas trans-former decoders are auto-regressive and hence thenumber of layers greatly affects decoding speed,as discussed in section 3.2. these observationsmotivate us to make the encoder deeper and thedecoder shallower..as shown in the bottom group of table 4, wetry different combinations of the number of en-coder and decoder layers given approximately thesame parameterization budget as the transformer-big.
it is interesting to observe that 7+5, 8+4 and9+3 achieve the comparable and even better per-formance than the transformer-big baseline withmuch less computational cost.
when we furtherincrease the encoder layer and decrease the decoderlayer, we see a drop in the performance of 10+2.
and 11+1 despite the improved efﬁciency becauseit becomes difﬁcult to train the transformer withextremely imbalanced encoder and decoder well,as indicated9 by the previous work (kasai et al.,2020; li et al., 2021; gu and kong, 2020)..since the 9+3 model achieves the best resultwith an around 2× speedup in the validation setwith almost the same parameterization budget, wechoose it as the model architecture to combine withaggressive decoding for ﬁnal evaluation..4.4 results.
we evaluate our ﬁnal approach – shallow aggres-sive decoding which combines aggressive decodingwith the shallow decoder.
table 5 shows the perfor-mance and efﬁciency of our approach and recentlyproposed efﬁcient gec models that are all fasterthan the transformer-big baseline in conll-14test set.
our approach (the 9+3 model with aggres-sive decoding) that is pretrained with synthetic dataachieves 63.5 f0.5 with 10.3× speedup over thetransformer-big baseline, which outperforms themajority10 of the efﬁcient gec models in terms ofeither quality or speed.
the only model that showsadvantages over our 9+3 model is gector whichis developed based on the powerful pretrained mod-.
9they show that sequence-level knowledge distillation(kd) may beneﬁt training the extremely imbalanced trans-former in nmt.
however, we do not conduct kd for faircomparison to other gec models in previous work..10it is notable that pie is not strictly comparable here be-cause their training data is different from ours: pie does notuse the w&i+locness corpus..5943model.
transformer-big (beam=5)levenshtein transformer(cid:63)lasertagger(cid:63)span correction(cid:63)our approach (9+3).
nlpcc-18r17.215.010.514.520.5.f0.529.622.019.928.429.4.speedup1.0×3.1×38.0×2.7×12.0×.
p36.024.925.637.333.0.table 6: the performance and online inference efﬁciency evaluation for the language-independent efﬁcient gecmodels in the nlpcc-18 chinese gec benchmark..els (e.g., roberta (liu et al., 2019) and xl-net (yang et al., 2019)) with its multi-stage trainingstrategy.
following gector’s recipe, we leveragethe pretrained model bart (lewis et al., 2019)to initialize a 12+2 model which proves to workwell in nmt (li et al., 2021) despite more parame-ters, and apply the multi-stage ﬁne-tuning strategyused in stahlberg and kumar (2020).
the ﬁnal sin-gle model11 with aggressive decoding achieves thestate-of-the-art result – 66.4 f0.5 in the conll-14test set with a 9.6× speedup over the transformer-big baseline..unlike gector and pie that are difﬁcult toadapt to other languages despite their competitivespeed because they are specially designed for en-glish gec with many manually designed language-speciﬁc operations like the transformation of verbforms (e.g., vbd→vbz) and prepositions (e.g.,in→at), our approach is data-driven without de-pending on language-speciﬁc features, and thuscan be easily adapted to other languages (e.g., chi-nese).
as shown in table 6, our approach consis-tently performs well in chinese gec, showing anaround 12.0× online inference speedup over thetransformer-big baseline with comparable perfor-mance..5 related work.
the state-of-the-art of gec has been signiﬁcantlyadvanced owing to the tremendous success ofseq2seq learning (sutskever et al., 2014) and thetransformer (vaswani et al., 2017).
most recentwork on gec focuses on improving the perfor-mance of the transformer-based gec models.
however, except for the approaches that add syn-thetic erroneous data for pretraining (ge et al.,2018a; grundkiewicz et al., 2019; zhang et al.,.
11the same model checkpoint also achieves the state-of-the-art result – 72.9 f0.5 with a 9.3× speedup in the bea-19test set..2019; lichtarge et al., 2019; zhou et al., 2020;wan et al., 2020), most methods that improve per-formance (ge et al., 2018b; kaneko et al., 2020)introduce additional computational cost and thusslow down inference despite the performance im-provement..to make the transformer-based gec modelmore efﬁcient during inference for practical appli-cation scenarios, some recent studies have startedexploring the approaches based on edit operations.
among them, pie (awasthi et al., 2019) and gec-tor (omelianchuk et al., 2020) propose to accel-erate the inference by simplifying gec from se-quence generation to iterative edit operation tag-ging.
however, as they rely on many language-dependent edit operations such as the conversionof singular nouns to plurals, it is difﬁcult for themto adapt to other languages.
lasertagger (malmiet al., 2019) uses the similar method but it is data-driven and language-independent by learning op-erations from training data.
however, its perfor-mance is not so desirable as its seq2seq counterpartdespite its high efﬁciency.
the only two previ-ous efﬁcient approaches that are both language-independent and good-performing are stahlbergand kumar (2020) which uses span-based edit op-erations to correct sentences to save the time forcopying unchanged tokens, and chen et al.
(2020)which ﬁrst identiﬁes incorrect spans with a tag-ging model then only corrects these spans with agenerator.
however, all the approaches have to ex-tract edit operations and even conduct token align-ment in advance from the error-corrected sentencepairs for training the model.
in contrast, our pro-posed shallow aggressive decoding tries to accel-erate the model inference through parallel autore-gressive decoding which is related to some previ-ous work (ghazvininejad et al., 2019; stern et al.,2018) in neural machine translation (nmt), andthe imbalanced encoder-decoder architecture which.
5944is recently explored by kasai et al.
(2020) and liet al.
(2021) for nmt.
not only is our approachlanguage-independent, efﬁcient and guarantees thatits predictions are exactly the same with greedy de-coding, but also does not need to change the wayof training, making it much easier to train with-out so complicated data preparation as in the editoperation based approaches..6 conclusion and future work.
in this paper, we propose shallow aggressive de-coding (sad) to accelerate online inference efﬁ-ciency of the transformer for instantaneous gec.
aggressive decoding can yield the same predic-tion quality as autoregressive greedy decoding butwith much less latency.
its combination with thetransformer with a shallow decoder can achievestate-of-the-art performance with a 9× ∼ 12× on-line inference speedup over the transformer-bigbaseline for gec..based on the preliminary study of sad in gec,we plan to further explore the technique for acceler-ating the transformer for other sentence rewritingtasks, where the input is similar to the output, suchas style transfer and text simpliﬁcation.
we believesad is promising to become a general accelera-tion methodology for writing intelligence models inmodern writing assistant applications that requirefast online inference..acknowledgments.
we thank all the reviewers for their valuable com-ments to improve our paper.
we thank xingxingzhang, xun wang and si-qing chen for their in-sightful discussions and suggestions.
the work issupported by national natural science foundationof china under grant no.62036001.
the corre-sponding author of this paper is houfeng wang..references.
abhijeet awasthi, sunita sarawagi, rasna goyal,sabyasachi ghosh, and vihari piratla.
2019. par-allel iterative edit models for local sequence trans-duction.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4251–4261..christopher bryant, mariano felice, øistein e an-dersen, and ted briscoe.
2019.the bea-2019shared task on grammatical error correction.
in pro-ceedings of the fourteenth workshop on innovative.
use of nlp for building educational applications,pages 52–75..mengyun chen, tao ge, xingxing zhang, furu wei,and ming zhou.
2020. improving the efﬁciency ofgrammatical error correction with erroneous span de-tection and correction.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 7162–7169..daniel dahlmeier and hwee tou ng.
2012. betterevaluation for grammatical error correction.
in pro-ceedings of the 2012 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages568–572..daniel dahlmeier, hwee tou ng, and siew mei wu.
2013. building a large annotated corpus of learnerenglish: the nus corpus of learner english.
in pro-ceedings of the eighth workshop on innovative useof nlp for building educational applications, pages22–31..tao ge, furu wei, and ming zhou.
2018a.
fluencyboost learning and inference for neural grammati-cal error correction.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1055–1065, melbourne, australia.
association for compu-tational linguistics..tao ge, furu wei, and ming zhou.
2018b.
reachinghuman-level performance in automatic grammaticalerror correction: an empirical study.
arxiv preprintarxiv:1807.01270..marjan ghazvininejad, omer levy, yinhan liu, andluke zettlemoyer.
2019. mask-predict: parallel de-coding of conditional masked language models.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 6114–6123..sylviane granger.
the computer learner corpus: a ver-.
satile new source of data for sla research..roman grundkiewicz, marcin junczys-dowmunt, andkenneth heaﬁeld.
2019. neural grammatical errorcorrection systems with unsupervised pre-trainingon synthetic data.
in proceedings of the fourteenthworkshop on innovative use of nlp for building ed-ucational applications, pages 252–263..jiatao gu and xiang kong.
2020..fully non-autoregressive neural machine translation: tricks ofthe trade.
arxiv preprint arxiv:2012.15833..jiatao gu, changhan wang, and junbo zhao.
2019.levenshtein transformer.
in advances in neural in-formation processing systems, pages 11181–11191..5945masahiro kaneko, masato mita, shun kiyono, junsuzuki, and kentaro inui.
2020. encoder-decodermodels can beneﬁt from pre-trained masked lan-guage models in grammatical error correction.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4248–4254..jungo kasai, nikolaos pappas, hao peng, jamescross, and noah a smith.
2020. deep encoder,shallow decoder: reevaluating the speed-qualityarxiv preprinttradeoff in machine translation.
arxiv:2006.10369..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..yanyang li, ye lin, tong xiao, and jingbo zhu.
2021.an efﬁcient transformer decoder with compressedsub-layers.
arxiv preprint arxiv:2101.00542..jared lichtarge, chris alberti, shankar kumar, noamshazeer, niki parmar, and simon tong.
2019. cor-pora generation for grammatical error correction.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 3291–3301..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..eric malmi, sebastian krause, sascha rothe, daniilmirylenka, and aliaksei severyn.
2019. encode,tag, realize: high-precision text editing.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5057–5068..tomoya mizumoto, mamoru komachi, masaaki na-gata, and yuji matsumoto.
2011. mining revisionlog of language learning sns for automated japaneseerror correction of second language learners.
in pro-ceedings of 5th international joint conference onnatural language processing, pages 147–155..hwee tou ng, siew mei wu, yuanbin wu, christianhadiwinoto, and joel tetreault.
2013. the conll-2013 shared task on grammatical error correction.
in proceedings of the seventeenth conference oncomputational natural language learning: sharedtask, pages 1–12..kostiantyn omelianchuk, vitaliy atrasevych, artemchernodub, and oleksandr skurzhanskyi.
2020.gector–grammatical error correction: tag, notrewrite.
arxiv preprint arxiv:2005.12592..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725..felix stahlberg and shankar kumar.
2020. seq2edits:sequence transduction using span-level edit opera-in proceedings of the 2020 conference ontions.
empirical methods in natural language processing(emnlp), pages 5147–5159..mitchell stern, noam shazeer, and jakob uszkoreit.
2018. blockwise parallel decoding for deep autore-gressive models.
in neurips..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
arxiv preprint arxiv:1409.3215..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..zhaohong wan, xiaojun wan, and wenguang wang.
2020. improving grammatical error correction withdata augmentation by editing latent representation.
in proceedings of the 28th international conferenceon computational linguistics, pages 2202–2212..zhilin yang, zihang dai, yiming yang, jaime car-bonell, ruslan salakhutdinov, and quoc v le.
2019. xlnet: generalized autoregressive pretrain-arxiv preprinting for language understanding.
arxiv:1906.08237..hwee tou ng, siew mei wu, ted briscoe, christianhadiwinoto, raymond hendy susanto, and christo-pher bryant.
2014. the conll-2014 shared task ongrammatical error correction.
in proceedings of theeighteenth conference on computational naturallanguage learning: shared task, pages 1–14..helen yannakoudakis, ted briscoe, and ben medlock.
2011. a new dataset and method for automaticallyin proceedings of the 49th an-grading esol texts.
nual meeting of the association for computationallinguistics: human language technologies, pages180–189..5946yi zhang, tao ge, furu wei, ming zhou, and xu sun.
2019. sequence-to-sequence pre-training with dataaugmentation for sentence rewriting.
arxiv preprintarxiv:1909.06002..yuanyuan zhao, nan jiang, weiwei sun, and xiao-jun wan.
2018. overview of the nlpcc 2018 sharedtask: grammatical error correction.
in ccf interna-tional conference on natural language processingand chinese computing, pages 439–445.
springer..model(enc+dec)6+69+36+69+3.
thread.
8822.beam=5 greedy aggressivespeedup1×1.5×1×1.5×.
speedup6.5×8.0×6.1×7.6×.
speedup1.6×2.5×2.1×3.1×.
table 8: the efﬁciency of the transformer with differ-ent encoder and decoder depths in conll-13 on cpuwith 8 and 2 threads..b cpu efﬁciency.
latency and speedup oftable 8 shows totalthe transformer with different encoder-decoderdepth on an intel® xeon® e5-2690 v4 proces-sor(2.60ghz) with 8 and 2 threads12, respectively.
our approach achieves a 7× ∼ 8× online infer-ence speedup over the transformer-big baseline oncpu..wangchunshu zhou, tao ge, chang mu, ke xu, furuwei, and ming zhou.
2020. improving grammaticalerror correction with machine translation pairs.
inproceedings of the 2020 conference on empiricalmethods in natural language processing: findings,pages 318–328..a hyper-parameters.
hyper-parameters of training the transformer forenglish gec are listed in table 7. the hyper-parameters for chinese gec are the same withthose of training from scratch..conﬁgurations.
model architecture.
number of epochsdevicesmax tokens per gpuupdate frequencyoptimizer.
learning ratelearning rate schedulerwarmupweight decayloss function.
train from scratch.
values.
transformer (big)(vaswani et al., 2017)604 nvidia v100 gpu51204adam(β1=0.9, β2=0.98, (cid:15)=1 × 10−8)(kingma and ba, 2014)[3 × 10−4 , 5 × 10−4]inverse sqrt40000.0label smoothed cross entropy(label-smoothing=0.1)(szegedy et al., 2016)[0.3, 0.4, 0.5].
pretrain.
dropout.
number of epochsdevicesupdate frequencylearning ratewarmupdropout.
number of epochsdevicesupdate frequencylearning ratewarmupdropout.
fine-tune.
108 nvidia v100 gpu83 × 10−480000.3.
604 nvidia v100 gpu43 × 10−440000.3.table 7: hyper-parameters values of training fromscratch, pretraining and ﬁne-tuning..12we set omp num threads to 8 or 2..5947