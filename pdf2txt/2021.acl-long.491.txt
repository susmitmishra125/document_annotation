cleve: contrastive pre-training for event extraction.
ziqi wang1∗, xiaozhi wang1∗, xu han1, yankai lin3, lei hou1,2† ,zhiyuan liu1,2, peng li3, juanzi li1,2, jie zhou31department of computer science and technology, bnrist;2kirc, institute for artiﬁcial intelligence,tsinghua university, beijing, 100084, china3pattern recognition center, wechat ai, tencent inc, china{ziqi-wan16, wangxz20, hanxu17}@mails.tsinghua.edu.cn.
abstract.
event extraction (ee) has considerably ben-eﬁted from pre-trained language models(plms) by ﬁne-tuning.
however, existingpre-training methods have not involved mod-eling event characteristics, resulting in the de-veloped ee models cannot take full advan-tage of large-scale unsupervised data.
tothis end, we propose cleve, a contrastivepre-training framework for ee to better learnevent knowledge from large unsupervised dataand their semantic structures (e.g.
amr) ob-tained with automatic parsers.
cleve con-tains a text encoder to learn event seman-tics and a graph encoder to learn event struc-tures respectively.
speciﬁcally, the text en-coder learns event semantic representationsby self-supervised contrastive learning to rep-resent the words of the same events closerthan those unrelated words;the graph en-coder learns event structure representations bygraph contrastive pre-training on parsed event-related semantic structures.
the two com-plementary representations then work togetherto improve both the conventional supervisedee and the unsupervised “liberal” ee, whichrequires jointly extracting events and discov-ering event schemata without any annotateddata.
experiments on ace 2005 and mavendatasets show that cleve achieves signiﬁcantimprovements, especially in the challengingunsupervised setting.
the source code andpre-trained checkpoints can be obtained fromhttps://github.com/thu-keg/cleve..1.introduction.
event extraction (ee) is a long-standing crucial in-formation extraction task, which aims at extractingevent structures from unstructured text.
as illus-trated in figure 1, it contains event detection taskto identify event triggers (the word “attack”) and.
∗ indicates equal contribution† correspondence to l.hou (houlei@tsinghua.edu.cn).
figure 1: an example sampled from the ace 2005dataset with its event annotation and amr structure..classify event types (attack), as well as eventargument extraction task to identify entities serv-ing as event arguments (“today” and “netanya”)and classify their argument roles (time-withinand place) (ahn, 2006).
by explicitly captur-ing the event structure in the text, ee can beneﬁtvarious downstream tasks such as information re-trieval (glavaˇs and ˇsnajder, 2014) and knowledgebase population (ji and grishman, 2011)..existing ee methods mainly follow thesupervised-learning paradigm to train advancedneural networks (chen et al., 2015; nguyen et al.,2016; nguyen and grishman, 2018) with human-annotated datasets and pre-deﬁned event schemata.
these methods work well in lots of public bench-marks such as ace 2005 (walker et al., 2006)and tac kbp (ellis et al., 2016), yet they stillsuffer from data scarcity and limited generaliz-ability.
since annotating event data and deﬁningevent schemata are especially expensive and labor-intensive, existing ee datasets typically only con-tain thousands of instances and cover limited eventtypes.
thus they are inadequate to train large neuralmodels (wang et al., 2020) and develop methodsthat can generalize to continually-emerging newevent types (huang and ji, 2020)..inspired by the success of recent pre-trained lan-guage models (plms) for nlp tasks, some pio-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6283–6297august1–6,2021.©2021associationforcomputationallinguistics6283cnn's kelly wallace reports on today's attack in netanya.attacktime-withinplaceevent typeargument roleattackertargettextcnn’s kelly wallacereport-01attack-01todaynetanyaarg0arg1arg1timeamr structureevent schemaneering work (wang et al., 2019a; wadden et al.,2019) attempts to ﬁne-tune general plms (e.g,bert (devlin et al., 2019)) for ee.
beneﬁtingfrom the strong general language understandingability learnt from large-scale unsupervised data,these plm-based methods have achieved state-of-the-art performance in various public benchmarks..although leveraging unsupervised data with pre-training has gradually become a consensus for eeand nlp community, there still lacks a pre-trainingmethod orienting event modeling to take full ad-vantage of rich event knowledge lying in large-scale unsupervised data.
the key challenge hereis to ﬁnd reasonable self-supervised signals (chenet al., 2017; wang et al., 2019a) for the diversesemantics and complex structures of events.
fortu-nately, previous work (aguilar et al., 2014; huanget al., 2016) has suggested that sentence semanticstructures, such as abstract meaning representation(amr) (banarescu et al., 2013), contain broad anddiverse semantic and structure information relat-ing to events.
as shown in figure 1, the parsedamr structure covers not only the annotated event(attack) but also the event that is not deﬁned inthe ace 2005 schema (report)..considering the fact that the amr structuresof large-scale unsupervised data can be easily ob-tained with automatic parsers (wang et al., 2015),we propose cleve, an event-oriented contrastivepre-training framework utilizing amr structuresto build self-supervision signals.
cleve consistsof two components, including a text encoder tolearn event semantics and a graph encoder to learnevent structure information.
speciﬁcally, to learneffective event semantic representations, we em-ploy a plm as the text encoder and encouragethe representations of the word pairs connectedby the arg, time, location edges in amrstructures to be closer in the semantic space thanother unrelated words, since these pairs usuallyrefer to the trigger-argument pairs of the sameevents (as shown in figure 1) (huang et al., 2016).
this is done by contrastive learning with the con-nected word pairs as positive samples and unrelatedwords as negative samples.
moreover, consider-ing event structures are also helpful in extractingevents (lai et al., 2020) and generalizing to newevent schemata (huang et al., 2018), we need tolearn transferable event structure representations.
hence we further introduce a graph neural net-work (gnn) as the graph encoder to encode amr.
structures as structure representations.
the graphencoder is contrastively pre-trained on the parsedamr structures of large unsupervised corpora withamr subgraph discrimination as the objective..by ﬁne-tuning the two pre-trained models ondownstream ee datasets and jointly using the tworepresentations, cleve can beneﬁt the conven-tional supervised ee suffering from data scarcity.
meanwhile, the pre-trained representations can alsodirectly help extract events and discover new eventschemata without any known event schema or an-notated instances, leading to better generalizability.
this is a challenging unsupervised setting named“liberal event extraction” (huang et al., 2016).
ex-periments on the widely-used ace 2005 and thelarge maven datasets indicate that cleve canachieve signiﬁcant improvements in both settings..2 related work.
event extraction.
most of the existing eeworks follow the supervised learning paradigm.
traditional ee methods (ji and grishman, 2008;gupta and ji, 2009; li et al., 2013) rely onmanually-crafted features to extract events.
in re-cent years, the neural models become mainstream,which automatically learn effective features withneural networks, including convolutional neuralnetworks (nguyen and grishman, 2015; chen et al.,2015), recurrent neural networks (nguyen et al.,2016), graph convolutional networks (nguyen andgrishman, 2018; lai et al., 2020).
with the recentsuccesses of bert (devlin et al., 2019), plmshave also been used for ee (wang et al., 2019a,b;yang et al., 2019; wadden et al., 2019; tong et al.,2020).
although achieving remarkable perfor-mance in benchmarks such as ace 2005 (walkeret al., 2006) and similar datasets (ellis et al., 2015,2016; getman et al., 2017; wang et al., 2020),these plm-based works solely focus on better ﬁne-tuning rather than pre-training for ee.
in this paper,we study pre-training to better utilize rich eventknowledge in large-scale unsupervised data..event schema induction.
supervised ee mod-els cannot generalize to continually-emerging newevent types and argument roles.
to this end, cham-bers and jurafsky (2011) explore to induce eventschemata from raw text by unsupervised cluster-ing.
following works introduce more featureslike coreference chains (chambers, 2013) and enti-ties (nguyen et al., 2015; sha et al., 2016).
re-cently, huang and ji (2020) move to the semi-.
6284figure 2: overall cleve framework.
best viewed in color..supervised setting allowing to use annotated dataof known types.
following huang et al.
(2016), weevaluate the generalizability of cleve in the mostchallenging unsupervised “liberal” setting, whichrequires to induce event schemata and extract eventinstances only from raw text at the same time..structure pre-training, of which details are intro-duced in section 3.2 and section 3.3, respectively.
at the beginning of this section, we ﬁrst introducethe required preprocessing in section 3.1, includingthe amr parsing and how we modify the parsedamr structures for our pre-training..contrastive learning.
contrastive learning wasinitiated by hadsell et al.
(2006) following an in-tuitive motivation to learn similar representationsfor “neighboors” and distinct representations for“non-neighbors”, and is further widely used for self-supervised representation learning in various do-mains, such as computer vision (wu et al., 2018;oord et al., 2018; hjelm et al., 2019; chen et al.,2020; he et al., 2020) and graph (qiu et al., 2020;you et al., 2020; zhu et al., 2020).
in the con-text of nlp, many established representation learn-ing works can be viewed as contrastive learningmethods, such as word2vec (mikolov et al., 2013),bert (devlin et al., 2019; kong et al., 2020)and electra (clark et al., 2020).
similar tothis work, contrastive learning is also widely-usedto help speciﬁc tasks, including question answer-ing (yeh and chen, 2019), discourse modeling (iteret al., 2020), natural language inference (cui et al.,2020) and relation extraction (peng et al., 2020)..3 methodology.
the overall cleve framework is illustrated in fig-ure 2. as shown in the illustration, our contrastivepre-training framework cleve consists of twocomponents: event semantic pre-training and event.
3.1 preprocessing.
cleve relies on amr structures (banarescu et al.,2013) to build broad and diverse self-supervisionsignals for learning event knowledge from large-scale unsupervised corpora.
to do this, we useautomatic amr parsers (wang et al., 2015; xuet al., 2020) to parse the sentences in unsupervisedcorpora into amr structures.
each amr struc-ture is a directed acyclic graph with concepts asnodes and semantic relations as edges.
moreover,each node typically only corresponds to at mostone word, and a multi-word entity will be repre-sented as a list of nodes connected with name andop (conjunction operator) edges.
considering pre-training entity representations will naturally bene-ﬁts event argument extraction, we merge these listsinto single nodes representing multi-word entities(like the “cnn’s kelly wallace” in figure 1) dur-ing both event semantic and structure pre-training.
formally, given a sentence s in unsupervised cor-pora, we obtain its amr graph gs = (vs, es)after amr parsing, where vs is the node set af-ter word merging and es denotes the edge set.
es = {(u, v, r) | (u, v) ∈ vs × vs, r ∈ r}, wherer is the set of deﬁned semantic relation types..6285parsed amr graphstrigger-argument pair discriminationcnn's kelly wallacereport-01attack-01todaynetanyaarg0arg1arg1timeunsupervised corporacnn's kelly wallace reports on today's attack in netanya.the army said two soldiers were also among the dead.…armysay-01soldierarg0arg1dead2alsoarg1quantmodattacknetanyareportsreportstoday'sattack-01todaynetanyaarg1timereport-01attack-01todayarg1timeamr subgraph discriminationtriggerreplacementargumentreplacementevent semantic pre-trainingevent structure pre-trainingtextencodersampled subgraphreport-01attack-01todayarg1timeattack-01todaynetanyaarg1timealsomoddeadsoldierarg12quantsay-01soldierarg12quantsubgraph samplinggraph encodernodeinitializationamr parsingcnn'skellywallacesay-01soldierarg12quantalsomoddeadsoldierarg12quant3.2 event semantic pre-training.
to model diverse event semantics in large unsu-pervised corpora and learn contextualized eventsemantic representations, we adopt a plm as thetext encoder and train it with the objective to dis-criminate various trigger-argument pairs..text encoderlike most plms, we adopt a multi-layer trans-former (vaswani et al., 2017) as the text encodersince its strong representation capacity.
given asentence s = {w1, w2, .
.
.
, wn} containing n to-kens, we feed it into the multi-layer transformerand use the last layer’s hidden vectors as tokenrepresentations.
moreover, a node v ∈ vs may cor-respond to a multi-token text span in s and we needa uniﬁed representation for the node in pre-training.
as suggested by baldini soares et al.
(2019), weinsert two special markers [e1] and [/e1] atthe beginning and ending of the span, respectively.
then we use the hidden vector for [e1] as thespan representation xv of the node v. and we usedifferent marker pairs for different nodes..as our event semantic pre-training focuses onmodeling event semantics, we start our pre-trainingfrom a well-trained general plm to obtain generallanguage understanding abilities.
cleve is ag-nostic to the model architecture and can use anygeneral plm, like bert (devlin et al., 2019) androberta (liu et al., 2019)..trigger-argument pair discriminationwe design trigger-argument pair discrimination asour contrastive pre-training task for event seman-tic pre-training.
the basic idea is to learn closerrepresentations for the words in the same eventsthan the unrelated words.
we note that the wordsconnected by arg, time and location edgesin amr structures are quite similar to the trigger-argument pairs in events (huang et al., 2016, 2018),i.e., the key words evoking events and the entitiesparticipating events.
for example, in figure 1, “ne-tanya” is an argument for the “attack” event, whilethe disconnected “cnn’s kelly wallace” is not.
with this observation, we can use these specialword pairs as positive trigger-argument samplesand train the text encoder to discriminate them fromnegative samples, so that the encoder can learn tomodel event semantics without human annotation.
let rp = {arg, time, location} and ps ={(u, v)|∃(u, v, r) ∈ es, r ∈ rp} denotes the setof positive trigger-argument pairs in sentence s..for a speciﬁc positive pair (t, a) ∈ ps, as shown infigure 2, we construct its corresponding negativesamples with trigger replacement and argumentreplacement.
speciﬁcally, in the trigger replace-ment, we construct mt number of negative pairsby randomly sample mt number of negative trig-gers ˆt ∈ vs and combine them with the positiveargument a. a negative trigger ˆt must do not havea directed arg, time or location edge with a,i.e., (cid:64)(ˆt, a, r) ∈ es, r ∈ rp.
similarly, we con-struct ma more negative pairs by randomly samplema number of negative arguments ˆa ∈ vs satis-fying (cid:64)(t, ˆa, r) ∈ es, r ∈ rp.
as the examplein figure 2, (“attack”, “reports”) is a valid nega-tive sample for the positive sample (“attack”, “ne-tanya”), but (“attack”, “today’s”) is not valid sincethere is a (“attack”, “today’s”, time) edge..to learn to discriminate the positive trigger-argument pair from the negative pairs and so thatmodel event semantics, we deﬁne the training ob-jective for a positive pair (t, a) as a cross-entropyloss of classifying the positive pair correctly:.
lt,a = − x(cid:62).
t w xa.
(cid:16).
(cid:16).
+ log.
exp.
x(cid:62).
t w xa.
(cid:17).
+.
(cid:16).
exp.
(cid:17).
x(cid:62)ˆti.
w xa.
(1).
mt(cid:88).
i=1.
+.
ma(cid:88).
j=1.
(cid:16).
exp.
x(cid:62).
t w xˆaj.
(cid:17) (cid:17).
,.
where mt, ma are hyper-parameters for negativesampling, and w is a trainable matrix learning thesimilarity metric.
we adopt the cross-entropy losshere since it is more effective than other contrastiveloss forms (oord et al., 2018; chen et al., 2020)..then we obtain the overall training objectivefor event semantic pre-training by summing up thelosses of all the positive pairs of all sentences s inthe mini batch bs:.
lsem(θ) =.
(cid:88).
(cid:88).
lt,a,.
s∈bs.
(t,a)∈ps.
(2).
where θ denotes the trainable parameters, includingthe text encoder and w ..3.3 event structure pre-training.
previous work has shown that event-related struc-tures are helpful in extracting new events (lai et al.,2020) as well as discovering and generalizing tonew event schemata (huang et al., 2016, 2018;huang and ji, 2020).
hence we conduct event struc-ture pre-training on a gnn as graph encoder tolearn transferable event-related structure represen-tations with recent advances in graph contrastive.
6286pre-training (qiu et al., 2020; you et al., 2020; zhuet al., 2020).
speciﬁcally, we pre-train the graphencoder with amr subgraph discrimination task..where 1[j(cid:54)=2i−1] ∈ {0, 1} is an indicator functionevaluating to 1 iff j (cid:54)= 2i − 1 and θ is the trainableparameters of graph encoder..graph encoder.
4 experiment.
in cleve, we utilize a gnn to encode the amr(sub)graph to extract the event structure informa-tion of the text.
given a graph g, the graphencoder represents it with an graph embeddingg = g(g, {xv}), where g(·) is the graph encoderand {xv} denotes the initial node representationsfed into the graph encoder.
cleve is agnostic tospeciﬁc model architectures of the graph encoder.
here we use a state-of-the-art gnn model, graphisomorphism network (xu et al., 2019), as ourgraph encoder for its strong representation ability.
we use the corresponding text span represen-tations {xv} produced by our pre-trained text en-coder (introduced in section 3.2) as the initial noderepresentations for both pre-training and inferenceof the graph encoder.
this node initialization alsoimplicitly aligns the semantic spaces of event se-mantic and structure representations in cleve, sothat can make them cooperate better..amr subgraph discrimination.
to learn transferable event structure representa-tions, we design the amr subgraph discriminationtask for event structure pre-training.
the basic ideais to learn similar representations for the subgraphssampled from the same amr graph by discrimi-nating them from subgraphs sampled from otheramr graphs (qiu et al., 2020)..a.batch.
given.
of m amr graphs{g1, g2, .
.
.
, gm}, each graph corresponds toa sentence in unsupervised corpora.
for the i-thgraph gi, we randomly sample two subgraphs fromit to get a positive pair a2i−1 and a2i.
and all thesubgraphs sampled from the other amr graphs inthe mini-batch serve as negative samples.
like infigure 2, the two green (w/ “attack”) subgraphsare a positive pair while the other two subgraphssampled from the purple (w/ “solider”) graphare negative samples.
here we use the subgraphsampling strategy introduced by qiu et al.
(2020),whose details are shown in appendix c..similar to event semantic pre-training, we adoptthe graph encoder to represent the samples ai =g (ai, xv)and deﬁne the training objective as:.
lstr(θ) = −.
log.
m(cid:88).
i=1.
(cid:1)exp (cid:0)a(cid:62)j=1 1[j(cid:54)=2i−1] exp (cid:0)a(cid:62).
2i−1a2i.
(cid:80)2m.
2i−1aj.
(cid:1) , (3).
we evaluate our methods in both the supervisedsetting and unsupervised “liberal” setting of ee..4.1 pre-training setup.
before the detailed experiments, we introducethe pre-training setup of cleve in implemen-tation.
we adopt the new york times corpus(nyt)1 (sandhaus, 2008) as the unsupervised pre-training corpora for cleve.
it contains over 1.8million articles written and published by the newyork times between january 1, 1987, and june 19,2007. we only use its raw text and obtain the amrstructures with a state-of-the-art amr parser (xuet al., 2020).
we choose nyt corpus because (1)it is large and diverse, covering a wide range ofevent semantics, and (2) its text domain is simi-lar to our principal evaluation dataset ace 2005,which is helpful (gururangan et al., 2020).
to pre-vent data leakage, we remove all the articles shownup in ace 2005 from the nyt corpus during pre-training.
moreover, we also study the effect ofdifferent amr parsers and pre-training corpora insection 5.2 and section 5.3, respectively..for the text encoder, we use the same modelarchitecture as roberta (liu et al., 2019), whichis with 24 layers, 1024 hidden dimensions and 16attention heads, and we start our event semanticpre-training from the released checkpoint2.
forthe graph encoder, we adopt a graph isomorphismnetwork (xu et al., 2019) with 5 layers and 64hidden dimensions, and pre-train it from scratch.
for the detailed hyperparameters for pre-trainingand ﬁne-tuning, please refer to appendix d..4.2 adaptation of cleve.
as our work focuses on pre-training rather thanﬁne-tuning for ee, we use straightforward and com-mon techniques to adapt pre-trained cleve todownstream ee tasks.
in the supervised setting, weadopt dynamic multi-pooling mechanism (chenet al., 2015; wang et al., 2019a,b) for the textencoder and encode the corresponding local sub-graphs with the graph encoder.
then we concate-.
1https://catalog.ldc.upenn.edu/.
ldc2008t19.
2https://github.com/pytorch/fairseq.
6287metric.
p.f1.
p.r.f1.
metric.
ed.
r.eae.
jointbeamdmcnndbrnngatedgcnsemsyngtnrcee erroberta.
73.7 62.3 67.5 64.7 44.4 52.775.6 63.6 69.1 62.2 46.9 53.574.1 69.8 71.9 66.2 52.8 58.778.8 76.3 77.6 −.
−.
−.
−.
−.
− 69.3 55.9 61.975.6 74.2 74.9 63.0 64.2 63.675.1 79.2 77.1 53.5 66.8 59.4.cleve.
78.1 81.5 79.8 55.4 68.0 61.175.3 79.7 77.4 53.8 67.0 59.7w/o semantic78.0 81.1 79.5 55.1 67.6 60.7w/o structureon ace (golden) 76.2 79.8 78.0 54.2 67.5 60.175.7 79.5 77.6 53.6 66.9 59.5on ace (amr).
table 1: supervised ee performance (%) of variousmodels on ace 2005..nate the two representations as features and ﬁne-tune cleve on supervised datasets.
in the un-supervised “liberal” setting, we follow the overallpipeline of huang et al.
(2016) and directly use therepresentations produced by pre-trained cleve asthe required trigger/argument semantic representa-tions and event structure representations.
for thedetails, please refer to appendix a..4.3 supervised ee.
dataset and evaluation.
we evaluate our models on the most widely-usedace 2005 english subset (walker et al., 2006) andthe newly-constructed large-scale maven (wanget al., 2020) dataset.
ace 2005 contains 599english documents, which are annotated with 8event types, 33 subtypes, and 35 argument roles.
maven contains 4, 480 documents and 168 eventtypes, which can only evaluate event detection.
wesplit ace 2005 following previous ee work (liaoand grishman, 2010; li et al., 2013; chen et al.,2015) and use the ofﬁcial split for maven.
eeperformance is evaluated with the performance oftwo subtasks: event detection (ed) and event ar-gument extraction (eae).
we report the precision(p), recall (r) and f1 scores as evaluation results,among which f1 is the most comprehensive metric..ed.
r.55.967.064.864.172.370.972.2.
72.672.472.5.f1.
60.662.864.163.867.167.868.0.
68.568.268.4.p.66.359.863.463.462.765.064.3.
64.964.564.7.dmcnnbilstmbilstm+crfmoganeddmbertbert+crfroberta.
cleve.
w/o semanticw/o structure.
table 2: supervised ee performance (%) of variousmodels on maven..without event structure pre-training..on ace 2005, we set two more variants toinvestigate the effectiveness of cleve.
the onace (golden) model is pre-trained with the goldentrigger-argument pairs and event structures of ace2005 training set instead of the amr structuresof nyt.
similarly, the on ace (amr) modelis pre-trained with the parsed amr structures oface 2005 training set.
we also compare clevewith various baselines, including: (1) feature-basedmethod, the top-performing jointbeam (li et al.,2013); (2) vanilla neural model dmcnn (chenet al., 2015); (3) the model incorporating syntacticknowledge, dbrnn (sha et al., 2018); (4) state-of-the-art models on ed and eae respectively, in-cluding gatedgcn (lai et al., 2020) and semsyn-gtn (pouran ben veyseh et al., 2020); (5) a state-of-the-art ee model rcee er (liu et al., 2020),which tackle ee with machine reading comprehen-sion (mrc) techniques.
the last four models adoptplms to learn representations..on maven, we compare cleve with the of-ﬁcial ed baselines set by wang et al.
(2020),including dmcnn (chen et al., 2015), bil-stm (hochreiter and schmidhuber, 1997), bil-stm+crf, moganed (yan et al., 2019), dm-bert (wang et al., 2019a), bert+crf..evaluation results.
baselines we ﬁne-tune our pre-trained cleveand set the original roberta without our eventsemantic pre-training as an important baseline.
todo ablation studies, we evaluate two variants ofcleve on both datasets: the w/o semantic modeladopts a vanilla roberta without event semanticpre-training as the text encoder, and the w/o struc-ture only uses the event semantic representations.
the evaluation results are shown in table 1 and(1) clevetable 2. we can observe that:achieves signiﬁcant improvements to its basicmodel roberta on both ace 2005 and maven.
the p-values under the t-test are 4×10−8, 2×10−8and 6 × 10−4 for ed on ace 2005, eae on ace2005, and ed on maven, respectively.
it alsooutperforms or achieves comparable results with.
6288metric (b-cubed).
p.f1.
p.r.f1.
metric (b-cubed).
p.ed.
r.eae.
liberalee.
55.7 45.1 49.8 36.2 26.5 30.6.
44.3 24.9 31.9 24.2 17.3 20.2robertaroberta+vgae 47.0 26.8 34.1 25.6 17.9 21.1.cleve.
62.0 47.3 53.7 41.6 30.3 35.160.6 46.2 52.4 40.9 29.8 34.5w/o semantic45.7 25.6 32.8 25.0 17.9 20.9w/o structureon ace (amr) 61.1 46.7 52.9 41.5 30.1 34.9.table 3: unsupervised “liberal” ee performance (%)of various models on ace 2005..all the baselines, including those using dependencyparsing information (dbrnn, gatedgcn, semsyn-gtn and moganed).
this demonstrates the ef-fectiveness of our proposed contrastive pre-trainingmethod and amr semantic structure.
it is note-worthy that rcee er outperforms our method ineae since its special advantages brought by refor-mulating ee as an mrc task to utilize sophisti-cated mrc methods and large annotated externalmrc data.
considering that our method is essen-tially a pre-training method learning better event-oriented representations, cleve and rcee ercan naturally work together to improve ee fur-ther.
(2) the ablation studies (comparisons be-tween cleve and its w/o semantic or structurerepresentations variants) indicate that both event se-mantic pre-training and event structure pre-trainingis essential to our method.
(3) from the compar-isons between cleve and its variants on ace(golden) and ace (amr), we can see that theamr parsing inevitably brings data noise com-pared to golden annotations, which results in aperformance drop.
however, this gap can be eas-ily made up by the beneﬁts of introducing largeunsupervised data with pre-training..4.4 unsupervised “liberal” ee.
dataset and evaluationin the unsupervised setting, we evaluate cleveon ace 2005 and maven with both objectiveautomatic metrics and human evaluation.
for theautomatic evaluation, we adopt the extrinsic clus-tering evaluation metrics: b-cubed metrics (baggaand baldwin, 1998), including b-cubed precision,recall and f1.
the b-cubed metrics evaluate thequality of cluster results by comparing them togolden standard annotations and have been shownto be effective (amig´o et al., 2009).
for the humanevaluation, we invite an expert to check the outputs.
robertaroberta+vgae.
cleve.
w/o semanticw/o structure.
32.137.7.
55.653.232.8.ed.
r.25.228.5.
46.444.826.1.f1.
28.232.5.
50.648.629.1.table 4: unsupervised “liberal” ee performance (%)of various models on maven..of the models to evaluate whether the extractedevents are complete and correctly clustered as wellas whether all the events in text are discovered..baselines we compare cleve with reproducedliberalee (huang et al., 2016), roberta androberta+vgae.
roberta here adopts theoriginal roberta (liu et al., 2019) without eventsemantic pre-training to produce semantic repre-sentations for trigger and argument candidates inthe same way as cleve, and encode the wholesentences to use the sentence embeddings (embed-dings of the starting token <s>) as the needed eventstructure representations.
roberta+vgae addi-tionally adopts an unsupervised model variationalgraph auto-encoder (vgae) (kipf and welling,2016) to encode the amr structures as event struc-ture representations.
roberta+vgae shares sim-ilar model architectures with cleve but is withoutour pre-training.
specially, for fair comparisonswith liberalee, all the models in the unsupervisedexperiments adopt the same camr (wang et al.,2015) as the amr parser, including cleve pre-training.
moreover, we also study cleve variantsas in the supervised setting.
the w/o semanticvariant replaces the cleve text encoder with aroberta without event structure pre-training.
thew/o structure variant only uses cleve text en-coder in a similar way as roberta.
the on ace(amr) model is pre-trained with the parsed amrstructures of ace test set.
as shown in huang et al.
(2016), the amr parsing is signiﬁcantly superiorto dependency parsing and frame semantic pars-ing on the unsupervised “liberal” event extractiontask, hence we do not include baselines using othersentence structures in the experiments..evaluation results.
the automatic evaluation results are shown intable 3 and table 4. as the human evaluationis laborious and expensive, we only do human.
6289eae.
amr 1.0 ace 2005 maven.
metric (human).
p.f1.
p.r.f1.
parsing.
ed eae.
liberalee.
cleve.
51.2 46.9 49.0 33.5 27.2 30.0.
60.4 48.4 53.7 39.4 31.1 34.8.wang et al.
(2015)xu et al.
(2020).
62.079.1.
79.8 61.180.6 61.5.ed.
68.569.0.ed.
r.table 5: unsupervised “liberal” ee human-evaluationperformance (%) on ace 2005..table 6: supervised results (f1,%) on ace 2005 andmaven of cleve using different amr parsers, aswell as the performance (f1,%) of the parsers on amr1.0 (ldc2015e86) dataset..ace 2005.maven.
ed.
79.877.479.5.
79.177.378.8.eae.
61.159.760.7.
60.459.560.0.ed.
68.568.268.4.
68.868.468.6.nyt.
w/o semanticw/o structure.
wikipedia.
w/o semanticw/o structure.
figure 3:maven with different training data size..supervised ed performance (f-1) on.
evaluations for cleve and the most competi-tive baseline liberalee on ace 2005, and theresults are shown in table 5. we can observethat:(1) cleve signiﬁcantly outperforms allthe baselines, which shows its superiority in bothextracting event instances and discovering eventschemata.
(2) roberta ignores the structure in-formation.
although roberta+vage encodesevent structures with vgae, the semantic repre-sentations of roberta and the structure represen-tations of vgae are distinct and thus cannot worktogether well.
hence the two models even under-perform liberalee, while the two representationsof cleve can collaborate well to improve “lib-eral” ee.
(3) in the ablation studies, the discardingof event structure pre-training results in a muchmore signiﬁcant performance drop than in the su-pervised setting, which indicates event structuresare essential to discovering new event schemata..5 analysis.
table 7: supervised results (f1,%) on ace 2005 andmaven of cleve pre-trained on different corpora..provements of cleve compared to roberta andthe pre-training models compared to the non-pre-training model are generally larger when less su-pervised data available.
it indicates that cleve isespecially helpful for low-resource ee tasks, whichis common since the expensive event annotation..5.2 effect of amr parsers.
cleve relies on automatic amr parsers to buildself-supervision signals for large unsupervised data.
intuitively, the performance of amr parsers willinﬂuence cleve performance.
to analyze theeffect of different amr parsing performance, wecompare supervised ee results of cleve modelsusing the established camr (wang et al., 2016)and a new state-of-the-art parser (xu et al., 2020)during pre-training in table 6. we can see thata better amr parser intuitively brings better eeperformance, but the improvements are not so sig-niﬁcant as the corresponding amr performanceimprovement, which indicates that cleve is gen-erally robust to the errors in amr parsing..5.1 effect of supervised data size.
5.3 effect of pre-training domain.
in this section, we study how the beneﬁts of pre-training change along with the available superviseddata size.
we compare the ed performance onmaven of cleve, roberta and a non-pre-training model bilstm+crf when trained on dif-ferent proportions of randomly-sampled maventraining data in figure 3. we can see that the im-.
pre-training on similar text domains may fur-ther improve performance on corresponding down-stream tasks (gururangan et al., 2020; gu et al.,2020).
to analyze this effect, we evaluate the su-pervised ee performance of cleve pre-trainedon nyt and english wikipedia in table 7. wecan see pre-training on a similar domain (nyt for.
62905%20%40%60%80%100%supervised data size50.052.555.057.560.062.565.067.5f-1 (%)cleverobertabilstm+crface 2005, wikipedia for maven) surely bene-ﬁts cleve on corresponding datasets.
on ace2005, although wikipedia is 2.28 times as largeas nyt, cleve pre-trained on it underperformscleve pre-trained on nyt (both in the news do-main).
moreover, we can see the in-domain bene-ﬁts mainly come from the event semantics ratherthan structures in cleve framework (from thecomparisons between the w/o semantic and w/ostructure results).
it suggests that we can developdomain adaptation techniques focusing on seman-tics for cleve, and we leave it to future work..6 conclusion and future work.
in this paper, we propose cleve, a contrastivepre-training framework for event extraction to uti-lize the rich event knowledge lying in large un-supervised data.
experiments on two real-worlddatasets show that cleve can achieve signiﬁcantimprovements in both supervised and unsupervised“liberal” settings.
in the future, we will (1) exploreother kinds of semantic structures like the framesemantics and (2) attempt to overcome the noise inunsupervised data brought by the semantic parsers..acknowledgement.
this work is supported by the national naturalscience foundation of china key project (nsfcno.
u1736204), grants from beijing academyof artiﬁcial intelligence (baai2019zd0502) andthe institute for guo qiang, tsinghua university(2019gqb0003).
this work is also supported bythe pattern recognition center, wechat ai, ten-cent inc. we thank lifu huang for his help onthe unsupervised experiments and the anonymousreviewers for their insightful comments..ethical considerations.
we discuss the ethical considerations and broaderimpact of the proposed cleve method in this sec-tion: (1) intellectual property.
nyt and ace2005 datasets are obtained from the linguistic dataconsortium (ldc), and are both licensed to be usedfor research.
maven is publicly shared under thecc by-sa 4.0 license3.
the wikipedia corpusis obtained from the wikimedia dump4, which is.
shared under the cc by-sa 3.0 license5.
the in-vited expert is fairly paid according to agreed work-ing hours.
(2) intended use.
cleve improvesevent extraction in both supervised and unsuper-vised settings, i.e., better extract structural eventsfrom diverse raw text.
the extracted events thenhelp people to get information conveniently and canbe used to build a wide range of application sys-tems like information retrieval (glavaˇs and ˇsnajder,2014) and knowledge base population (ji and grish-man, 2011).
as extracting events is fundamentalto various applications, the failure cases and po-tential bias in ee methods also have a signiﬁcantnegative impact.
we encourage the community toput more effort into analyzing and mitigating thebias in ee systems.
considering cleve does notmodel people’s characteristics, we believe clevewill not bring signiﬁcant additional bias.
(3) mis-use risk.
although all the datasets used in thispaper are public and licensed, there is a risk to usecleve methods on private data without autho-rization for interests.
we encourage the regulatorsto make efforts to mitigate this risk.
(4) energyand carbon costs.
to estimate the energy and car-bon costs, we present the computing platform andrunning time of our experiments in appendix efor reference.
we will also release the pre-trainedcheckpoints to avoid the additional carbon costsof potential users.
we encourage the users to trymodel compression techniques like distillation andquantization in deployment to reduce carbon costs..references.
jacqueline aguilar, charley beller, paul mcnamee,benjamin van durme, stephanie strassel, zhiyisong, and joe ellis.
2014. a comparison of theevents and relations across ace, ere, tac-kbp,and framenet annotation standards.
in proceedingsof the second workshop on events: deﬁnition, de-tection, coreference, and representation, pages 45–53..david ahn.
2006. the stages of event extraction.
inproceedings of acl workshop on annotating andreasoning about time and events, pages 1–8..enrique amig´o, julio gonzalo, javier artiles, and fe-lisa verdejo.
2009. a comparison of extrinsic clus-tering evaluation metrics based on formal con-straints.
inf.
retr., 12(4):461–486..amit bagga and breck baldwin.
1998. entity-basedcross-document coreferencing using the vector.
5https://creativecommons.org/licenses/.
3https://creativecommons.org/licenses/.
by-sa/4.0/.
4https://dumps.wikimedia.org/.
by-sa/3.0/.
6291space model.
pages 79–85..in proceedings of acl-coling,.
livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-ing.
in proceedings of acl, pages 2895–2905..laura banarescu, claire bonial, shu cai, madalinageorgescu, kira grifﬁtt, ulf hermjakob, kevinknight, philipp koehn, martha palmer, and nathanschneider.
2013. abstract meaning representationfor sembanking.
in proceedings of the 7th linguis-tic annotation workshop and interoperability withdiscourse, pages 178–186..nathanael chambers.
2013. event schema inductionin pro-.
with a probabilistic entity-driven model.
ceedings of emnlp, pages 1797–1807..nathanael chamberstemplate-basedout the templates.
pages 976–986..jurafsky..and dan.
2011.information extraction with-in proceedings of acl-hlt,.
ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
in proceedings of icml, pages 1597–1607..yubo chen, shulin liu, xiang zhang, kang liu, andjun zhao.
2017. automatically labeled data gener-ation for large scale event extraction.
in proceed-ings of acl, pages 409–419..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of acl-ijcnlp, pages 167–176..kevin clark, minh-thang luong, quoc v le, andchristopher d manning.
2020. electra: pre-training text encoders as discriminators ratherthan generators.
in proceedings of iclr..wanyun cui, guangyu zheng, and wei wang.
2020.unsupervised natural language inference via de-coupled multimodal contrastive learning.
in pro-ceedings of emnlp, pages 5511–5520..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of naacl-hlt, pages4171–4186..joe ellis, jeremy getman, dana fore, neil kuster,zhiyi song, ann bies, and stephanie m strassel.
2015. overview of linguistic resources for the tackbp 2015 evaluations: methodologies and results.
in tac..joe ellis, jeremy getman, dana fore, neil kuster,zhiyi song, ann bies, and stephanie m strassel.
2016. overview of linguistic resources for thetac kbp 2016 evaluations: methodologies and re-sults.
in tac..jeremy getman, joe ellis, zhiyi song, jennifer tracey,and stephanie strassel.
2017. overview of linguisticresources for the tac kbp 2017 evaluations: method-ologies and results.
in tac..goran glavaˇs and jan ˇsnajder.
2014. event graphsfor information retrieval and multi-document sum-expert systems with applications,marization.
41(15):6904–6916..yuxian gu, zhengyan zhang, xiaozhi wang, zhiyuanliu, and maosong sun.
2020. train no evil: se-lective masking for task-guided pre-training.
inproceedings of emnlp, pages 6966–6974..prashant gupta and heng ji.
2009. predicting un-known time arguments based on cross-event prop-in proceedings of acl-ijcnlp, pagesagation.
369–372..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
inproceedings of acl, pages 8342–8360..raia hadsell, sumit chopra, and yann lecun.
2006.dimensionality reduction by learning an invariantmapping.
in proceedings of cvpr, volume 2, pages1735–1742..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for un-supervised visual representation learning.
in pro-ceedings of cvpr, pages 9726–9735..r devon hjelm, alex fedorov, samuel lavoie-marchildon, karan grewal, phil bachman, adamtrischler, and yoshua bengio.
2019. learning deeprepresentations by mutual information estimationand maximization.
in proceedings of iclr..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..lifu huang, taylor cassidy, xiaocheng feng, hengji, clare r. voss, jiawei han, and avirup sil.
2016.liberal event extraction and event schema induc-tion.
in proceedings of acl, pages 258–268..lifu huang and heng ji.
2020. semi-supervised newevent type induction and event detection.
in pro-ceedings of emnlp, pages 718–724..lifu huang, heng ji, kyunghyun cho, ido dagan, se-bastian riedel, and clare voss.
2018. zero-shottransfer learning for event extraction.
in proceed-ings of acl, pages 2160–2170..dan iter, kelvin guu, larry lansing, and dan jurafsky.
2020. pretraining with contrastive sentence objec-tives improves discourse performance of languagemodels.
in proceedings of acl, pages 4859–4870..6292heng ji and ralph grishman.
2008. reﬁning event ex-traction through cross-document inference.
in pro-ceedings of acl, pages 254–262..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
in proceedings of nips..heng ji and ralph grishman.
2011. knowledge basepopulation: successful approaches and challenges.
in proceedings of acl, pages 1148–1158..thomas n kipf and max welling.
2016. variationalgraph auto-encoders.
nips workshop on bayesiandeep learning..lingpeng kong, cyprien de masson d’autume, wangling, lei yu, zihang dai, and dani yogatama.
2020.a mutual information maximization perspective oflanguage representation learning.
in proceedingsof iclr..viet dac lai, tuan ngo nguyen, and thien huunguyen.
2020. event detection: gate diversity andsyntactic importance scores for graph convolutionneural networks.
in proceedings of emnlp, pages5405–5411..qi li, heng ji, and liang huang.
2013. joint eventextraction via structured prediction with global fea-tures.
in proceedings of acl, pages 73–82..shasha liao and ralph grishman.
2010. using doc-ument level cross-event inference to improve eventextraction.
in proceedings of acl, pages 789–797..jian liu, yubo chen, kang liu, wei bi, and xiaojiangliu.
2020. event extraction as machine readingcomprehension.
in proceedings of emnlp, pages1641–1651..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrain-ing approach.
corr, abs/1907.11692..tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space.
in proceedings of iclr..kiem-hieu nguyen, xavier tannier, olivier ferret,and romaric besanc¸on.
2015. generative eventschema induction with entity disambiguation.
inproceedings of acl, pages 188–197..thien nguyen and ralph grishman.
2018. graph con-volutional networks with argument-aware poolingfor event detection.
in proceedings of aaai, pages5900–5907..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentneural networks.
in proceedings of naacl, pages300–309..hao peng, tianyu gao, xu han, yankai lin, pengli, zhiyuan liu, maosong sun, and jie zhou.
2020.learning from context or names?
an empiricalin proceed-study on neural relation extraction.
ings of emnlp, pages 3661–3672..amir pouran ben veyseh, tuan ngo nguyen, andthien huu nguyen.
2020. graph transformer net-works with syntactic and semantic structures forevent argument extraction.
in findings of the as-sociation for computational linguistics: emnlp2020, pages 3651–3661..jiezhong qiu, qibin chen, yuxiao dong, jing zhang,hongxia yang, ming ding, kuansan wang, and jietang.
2020. gcc: graph contrastive coding forin proceed-graph neural network pre-training.
ings of kdd, page 1150–1160..evan sandhaus.
2008. the new york times annotatedcorpus.
linguistic data consortium, 6(12):e26752..lei sha, sujian li, baobao chang, and zhifang sui.
2016. joint learning templates and slots for eventschema induction.
in proceedings of naacl-hlt,pages 428–434..lei sha, feng qian, baobao chang, and zhifang sui.
2018.jointly extracting event triggers and ar-guments by dependency-bridge rnn and tensor-in proceedings ofbased argument interaction.
aaai, pages 5916–5923..meihan tong, bin xu, shuai wang, yixin cao, leihou, juanzi li, and jun xie.
2020. improving eventdetection via open-domain trigger knowledge.
inproceedings of acl, pages 5887–5897..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin proceedings of nips, pages 5998–you need.
6008..ulrike von luxburg.
2007. a tutorial on spectral clus-tering.
statistics and computing, 17(4):395–416..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representa-in proceedings of emnlp-ijcnlp, pagestions.
5784–5789..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilingualtraining corpus.
linguistic data consortium, 57..thien huu nguyen and ralph grishman.
2015. eventdetection and domain adaptation with convolu-in proceedings of acl,tional neural networks.
pages 365–371..chuan wang, sameer pradhan, xiaoman pan, heng ji,and nianwen xue.
2016. camr at semeval-2016task 8: an extended transition-based amr parser.
in proceedings of semeval, pages 1173–1178..6293chuan wang, nianwen xue, and sameer pradhan.
2015. boosting transition-based amr parsing within pro-reﬁned actions and auxiliary analyzers.
ceedings of acl-ijcnlp, pages 857–862..xiaozhi wang, xu han, zhiyuan liu, maosong sun,and peng li.
2019a.
adversarial training forweakly supervised event detection.
in proceedingsof naacl-hlt, pages 998–1008..xiaozhi wang, ziqi wang, xu han, wangyi jiang,rong han, zhiyuan liu, juanzi li, peng li, yankailin, and jie zhou.
2020. maven: a massive gen-eral domain event detection dataset.
in proceed-ings of emnlp, pages 1652–1671..xiaozhi wang, ziqi wang, xu han, zhiyuan liu,juanzi li, peng li, maosong sun, jie zhou, andxiang ren.
2019b.
hmeae: hierarchical modu-in proceedings oflar event argument extraction.
emnlp-ijcnlp, pages 5777–5783..zhirong wu, yuanjun xiong, stella x yu, and dahualin.
2018. unsupervised feature learning via non-parametric instance discrimination.
in proceedingsof cvpr, pages 3733–3742..dongqin xu, junhui li, muhua zhu, min zhang, andimproving amr parsingin pro-.
guodong zhou.
2020.with sequence-to-sequence pre-training.
ceedings of emnlp, pages 2501–2511..keyulu xu, weihua hu, jure leskovec, and stefaniejegelka.
2019. how powerful are graph neural net-works?
in proceedings of iclr..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated at-tention.
in proceedings of emnlp-ijcnlp, pages5766–5770..sen yang, dawei feng, linbo qiao, zhigang kan, anddongsheng li.
2019. exploring pre-trained lan-guage models for event extraction and generation.
in proceedings of acl, pages 5284–5294..yi-ting yeh and yun-nung chen.
2019. qainfomax:learning robust question answering system bymutual information maximization.
in proceedingsof emnlp-ijcnlp, pages 3370–3375..yuning you, tianlong chen, yongduo sui, ting chen,zhangyang wang, and yang shen.
2020. graph con-in proceed-trastive learning with augmentations.
ings of neurips, pages 5812–5823..yanqiao zhu, yichen xu, feng yu, qiang liu, shu wu,and liang wang.
2020. deep graph contrastivein icml workshop onrepresentation learning.
graph representation learning and beyond..a downstream adaptation of cleve.
in this section, we introduce how to adapt pre-trained cleve to make the event semantic andstructure representations work together in down-stream event extraction settings in detail, includingsupervised ee and unsupervised “liberal” ee..a.1 supervised ee.
in supervised ee, we ﬁne-tune the pre-trained textencoder and graph encoder of cleve with an-notated data.
we formulate both event detection(ed) and event argument extraction (eae) as multi-class classiﬁcation tasks.
an instance is deﬁnedas a sentence with a trigger candidate for ed, anda sentence with a given trigger and an argumentcandidate for eae.
the key question here is howto obtain features of an instance to be classiﬁed..for the event semantic representation, we adoptdynamic multi-pooling to aggregate the embed-dings produced by text encoder into a uniﬁedsemantic representation xsem following previouswork (chen et al., 2015; wang et al., 2019a,b).
moreover, we also insert special markers to indi-cate candidates as in pre-training (section 3.2).
forthe event structure representation, we parse the sen-tence into an amr graph and ﬁnd the correspond-ing node v of the trigger/argument candidate to beclassiﬁed.
following qiu et al.
(2020), we encodev and its one-hop neighbors with the graph encoderto get the desired structure representation gstr.
theinitial node representation is also obtained with thetext encoder as introduced in section 3.3..we concatenate xsem and gstr as the instanceembedding and adopt a multi-layer perceptronalong with softmax to get the logits.
then weﬁne-tune cleve with cross-entropy loss..a.2 unsupervised “liberal” ee.
unsupervised “liberal” ee requires to discoverevent instances and event schemata only from rawtext.
we follow the pipeline of huang et al.
(2016)to parse sentences into amr graphs and identifytrigger and argument candidates with the amrstructures.
we also cluster the candidates to getevent instances and schemata with the joint con-straint clustering algorithm (huang et al., 2016),which requires semantic representations of the trig-ger and argument candidates as well as the eventstructure representations.
the details of this clus-tering algorithm is introduced in appendix b. herewe straightforwardly use the corresponding text.
6294span representations (section 3.2) as semantic rep-resentations and encode the whole amr graphswith the graph encoder to get desired event struc-ture representations..b joint constraint clustering algorithm.
in the unsupervised “liberal” event extrac-tion (huang et al., 2016), the joint constraint clus-tering algorithm is introduced to get trigger andargument clusters given trigger and argument can-didate representations.
cleve focuses on learningevent-speciﬁc representations and can use any clus-tering algorithm.
to fairly compare with huanget al.
(2016), we also use the joint constraint clus-tering algorithm in our unsupervised evaluation.
hence we brieﬂy introduce this algorithm here..b.1 preliminaries.
the input of this algorithm contains a trigger can-didate set t and an argument candidate set a aswell as their semantic representations etg and eag ,respectively.
there is also an event structure repre-sentation etr for each trigger t. we also previouslyset the ranges of the numbers of resulting triggerand argument clusters: the minimal and maximalnumber of trigger clusters kminas well asthe minimal and maximal number of argument clus-ters kmina .
the algorithm will output theoptimal trigger clusters ct = {ct} andargument clusters ca = {ca.
1 , ..., ctkt1 , ..., caka.
a , kmax.
, kmaxt.}..t.b.2 similarity functions.
the clustering algorithm requires to deﬁne trigger-trigger similarities and argument-argument similar-ities.
huang et al.
(2016) ﬁrst deﬁnes the constraintfunction f :.
where et.
g and ea.
g are trigger and argument se-mantic representations, respectively.
rt is theamr relation set in the parsed amr graph of trig-ger t. etr denotes the event structure representationof the node that has a semantic relation r to triggert in the event structure.
λ is a hyper-parameter.
simcos(·, ·) is the cosine similarity..b.3 objective.
huang et al.
(2016) also deﬁnes an objective func-tion o(·, ·) to evaluate the quality of trigger clus-ters ct = {ct} and argument clustersca = {ca.
1 , ..., ctkt1 , ..., caka.
}.
it is deﬁned as follows:.
o(c t , c a) = dinter(c t ) + dintra(c t )+ dinter(c a) + dintra(c a),.
dinter(c p ) =.
kp(cid:88).
(cid:88).
sim(pu, pv),.
(6).
dintra(c p ) =.
(1 − sim(pu, pv)),.
i(cid:54)=j=1.
u∈cp.
i ,v∈cpj.kp(cid:88).
(cid:88).
i=1.
u,v∈cpi.where dinter(·) measures the agreement acrossclusters, and dintra(·) measures the disagreementwithin clusters.
the clustering algorithm iterativelyminimizes the objective function..b.4 overall pipeline.
this algorithm updates its clustering results iter-atively.
at ﬁrst, it uses the spectral clusteringalgorithm (von luxburg, 2007) to get initial clus-tering results.
then for each iteration, it updatesclustering results and the best objective value usingprevious clustering results.
it selects the clusterswith the minimum o value as the ﬁnal result.
theoverall pipeline is shown in algorithm 1..f (p1, p2) = log(1 +.
|l1 ∩ l2||l1 ∪ l2|.
)..(4).
c subgraph sampling.
when p1 and p2 are two triggers, li has tupleelements (pi, r, id(a)), which means the argumenta has a relation r to trigger pi.
id(a) is the clusterid for the argument a. when pi is arguments, lichanges to corresponding triggers and semanticrelations accordingly..hence the similarity functions are deﬁned as:.
sim(t1, t2) = λ simcos(et1.
g , et2.
g ) + f (t1, t2).
+ (1 − λ).
(cid:80).
r∈rt1 ∩rt2.
simcos(et1.
r , et2r ).
|rt1 ∩ rt2 |.
sim(a1, a2) = simcos(ea1.
g , ea2.
g ) + f (a1, a2).
in the amr subgraph discrimination task of eventstructure pre-training, we need to sample subgraphsfrom the parsed amr graphs for contrastive pre-training.
here we adopt the subgraph samplingstrategy introduced by qiu et al.
(2020), whichconsists of the random walk with restart (rwr),subgraph induction and anonymization:.
• random walk with restart ﬁrst randomlychooses a starting node (the ego) from theamr graph to be sampled from.
the ego mustbe a root node, i.e., there is no directed edge inthe amr graph pointing to the node.
then wetreat the amr graph as an undirected graph.
,.
(5).
6295algorithm 1 joint constraint clustering algo-rithminput: trigger candidate set t , argument candidate set a,their semantic representations etg , structurerepresentations etr for each trigger t, the minimal andmaximal number of trigger clusters k minaswell as the minimal and maximal number of argumentclusters k min.
a , k maxa ;output: optimal trigger clusters c t = {c t.g and ea.
, k maxt.t.kt } and.
argument clusters c a = {c a.
1 , ..., c a.
1 , ..., c tka };.
• omin = ∞, c t = ∅, c a = ∅.
• for kt = k min.
t.to k maxt., ka = k min.
a to k max.
a.
– clustering with spectral clustering algorithm:– c t– c a– ocurr = o(c t– if ocurr ≤ omin.
curr = spectral(t, etcurr = spectral(a, eacurr).
g , etg , ka).
r, kt , c a.curr, c a.curr).
* omin = ocurr, c t = c t.curr, c a = c a.curr.
– while iterate time ≤ 10.curr = spectral(t, etcurr = spectral(a, eacurr).
* c t* c a* ocurr = o(c t* if ocurr ≤ omin.
curr, c a.g , etg , ka, c t.r, kt , c acurr).
curr).
· omin = ocurr, c t = c t.curr, c a =.
c a.curr.
• return omin, c t , c a.batch sizelearning rateadam (cid:15)adam β1adam β2trigger negative sampling size mtargument negative sampling size mamax sequence length#parameters of text encoder.
401 × 10−51 × 10−80.90.999930128355m.
table 8: hyperparameters for the event semantic pre-training..batch sizerestart probabilitytemperaturewarmup stepsweight decaytraining stepslearning rateadam (cid:15)adam β1adam β2number of layersdropout ratehidden dimensions#parameters of graph encoder.
10240.80.077, 5001 × 10−575, 0000.0051 × 10−80.90.99950.5640.2m.
and do random walks starting from the ego.
at each step, the random walk with a proba-bility to return to the ego and restart.
whenall the neighbouring nodes of the current nodehave been visited, the rwr ends..• subgraph induction is to take the inducedsubgraph of the node set obtained with rwras the sampled subgraphs..• anonymization is to randomly shufﬂe the in-dices of the nodes in the sampled subgraph toavoid overﬁtting to the node representations..in our event structure pre-training, we take sub-graphs of the same sentence (amr graph) as pos-itive pairs.
but, ideally, the two subgraphs in apositive pair should be taken from the same eventrather than only the same sentence.
however, it ishard to unsupervisedly determine which parts ofan amr graph belong to the same event.
we thinkthis task is almost as hard as event extraction itself.
the rule used in the event semantic pre-trainingonly handles the arg, time and location rela-tions, and for the other about 100 amr relations,we cannot ﬁnd an effective method to determine.
table 9: hyperparameters for the event structure pre-training..which event their edges belong to.
hence, to takeadvantage of all the structure information, we adopta simple assumption that the subgraphs from thesame sentence express the same event (or at leastclose events) to design the subgraph sampling parthere.
we will explore more sophisticated subgraph-sampling strategies in our future work..d hyperparameter setup.
d.1 pre-training hyperparameters.
during pre-training, we manually tune the hyper-parameters and select the models by the losseson a held-out validation set with 1, 000 sentences.
the event structure pre-training hyperparametersmainly follow the e2e model of qiu et al.
(2020).
table 8 and table 9 show the best-performinghyper-parameters used in experiments of the eventsemantic pre-training and event structure pre-training, respectively..6296batch sizetraining epochlearning rateadam (cid:15)adam β1adam β2max sequence length.
40301 × 10−51 × 10−80.90.999128.metric.
roberta.
ed.
eae.
runtime.
p.r f1.
p.r f1.
mins.
72.9 75.2 74.0 54.3 62.6 58.2.cleve.
73.7 79.4 76.4 56.2 66.0 60.772.1 77.9 74.9 54.5 65.6 59.5w/o semantic73.2 80.2 76.5 56.3 65.4 60.5w/o structureon ace (golden) 71.0 77.1 73.9 55.0 65.8 59.9on ace (amr) 70.2 77.3 73.6 54.1 65.5 59.3.
344.
410422355401408.table 10: fine-tuning hyperparameters for cleve androberta in the supervised setting..table 11: supervised ee performance (%) of variousmodels on ace 2005 validation set and the models’ av-erage ﬁne-tuning runtime..metric.
roberta.
cleve.
w/o semanticw/o structure.
p.65.3.
66.166.565.4.ed.
r.71.4.
70.269.371.7.f1.
68.2.
68.167.968.4.runtime.
mins.
530.
572588549.table 12: supervised ee performance (%) of variousmodels on maven validation set and the models’ av-erage ﬁne-tuning runtime..ace 2005 maven.
robertaroberta+vgae.
cleve.
w/o semanticw/o structure.
12 mins17 mins.
15 mins15 mins14 mins.
29 mins36 mins.
33 mins32 mins26 mins.
table 13: average runtime of various models on aceand maven for the unsupervised “liberal” ee..d.2 fine-tuning hyperparameters.
cleve in the unsupervised “liberal” setting di-rectly uses the pre-trained representations andhence does not have additional hyperparameters.
for the ﬁne-tuning in the supervised setting, wemanually tune the hyperparameters by 10 trials.
ineach trial, we train the models for 30 epochs andselect models by their f1 scores on the validationset.
table 10 shows the best ﬁne-tuning hyperpa-rameters for cleve models and roberta.
forthe other baselines, we take their reported results..e training details.
for reproducibility and estimating energy and car-bon costs, we report the computing infrastructuresand average runtime of experiments as well as vali-dation performance..e.1 pre-training details.
for pre-training, we use 8 rtx 2080 ti cards.
theevent semantic pre-training takes 12.3 hours.
theevent structure pre-training takes 60.2 hours..e.2 fine-tuning/inference details.
during the ﬁne-tuning in the supervised setting andthe inference in the unsupervised “liberal” setting,we also use 8 rtx 2080 ti cards..for the supervised ee experiments, table 11 andtable 12 show the runtime and the results on thevalidation set of the model implemented by us..in the unsupervised ”liberal” setting, we only doinference and do not involve the validation.
wereport the runtime of our models in table 13..6297