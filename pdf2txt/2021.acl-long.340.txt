intent classiﬁcation and slot filling for privacy policies.
, jianfeng chi‡∗.
wasi uddin ahmad†∗.
, tu le‡thomas norton§, yuan tian‡, kai-wei chang††university of california, los angeles, ‡university of virginia, §fordham university†{wasiahmad, kwchang}@cs.ucla.edu‡{jc6ub,tnl6wk,yuant}@virginia.edu§tnorton1@law.fordham.edu.
abstract.
understanding privacy policies is crucial forusers as it empowers them to learn about the in-formation that matters to them.
sentences writ-ten in a privacy policy document explain pri-vacy practices, and the constituent text spansconvey further speciﬁc information about thatpractice.
we refer to predicting the privacypractice explained in a sentence as intent clas-siﬁcation and identifying the text spans shar-ing speciﬁc information as slot ﬁlling.
in thiswork, we propose policyie, an english corpusconsisting of 5,250 intent and 11,788 slot an-notations spanning 31 privacy policies of web-sites and mobile applications.
policyie corpusis a challenging real-world benchmark withlimited labeled examples reﬂecting the costof collecting large-scale annotations from do-main experts.
we present two alternative neu-ral approaches as baselines, (1) intent classiﬁ-cation and slot ﬁlling as a joint sequence tag-ging and (2) modeling them as a sequence-to-sequence (seq2seq) learning task.
the exper-iment results show that both approaches per-form comparably in intent classiﬁcation, whilethe seq2seq method outperforms the sequencetagging approach in slot ﬁlling by a large mar-gin.
we perform a detailed error analysis toreveal the challenges of the proposed corpus..1.introduction.
privacy policies inform users about how a serviceprovider collects, uses, and maintains the users’ in-formation.
the service providers collect the users’data via their websites or mobile applications andanalyze them for various purposes.
the users’ dataoften contain sensitive information; therefore, theusers must know how their information will beused, maintained, and protected from unauthorizedand unlawful use.
privacy policies are meant toexplain all these use cases in detail.
this makes.
∗.
equal contribution.
listed by alphabetical order..privacy policies often very long, complicated, andconfusing (mcdonald and cranor, 2008; reiden-berg et al., 2016).
as a result, users do not tendto read privacy policies (commission et al., 2012;gluck et al.
; marotta-wurgler, 2015), leading to un-desirable consequences.
for example, users mightnot be aware of their data being sold to third-partyadvertisers even if they have given their consent tothe service providers to use their services in return.
therefore, automating information extraction fromverbose privacy policies can help users understandtheir rights and make informed decisions..in recent years, we have seen substantial effortsto utilize natural language processing (nlp) tech-niques to automate privacy policy analysis.
in lit-erature, information extraction from policy doc-uments is formulated as text classiﬁcation (wil-son et al., 2016a; harkous et al., 2018; zimmecket al., 2019), text alignment (liu et al., 2014; ra-manath et al., 2014), and question answering (qa)(shvartzshanider et al., 2018; harkous et al., 2018;ravichander et al., 2019; ahmad et al., 2020).
al-though these approaches effectively identify thesentences or segments in a policy document rele-vant to a privacy practice, they lack in extractingﬁne-grained structured information.
as shown inthe ﬁrst example in table 1, the privacy practice la-bel “data collection/usage” informs the user how,why, and what types of user information will becollected by the service provider.
the policy alsospeciﬁes that users’ “username” and “icon or pro-ﬁle photo” will be used for “marketing purposes”.
this informs the user precisely what and why theservice provider will use users’ information..the challenge in training models to extract ﬁne-grained information is the lack of labeled examples.
annotating privacy policy documents is expensiveas they can be thousands of words long and re-quires domain experts (e.g., law students).
there-fore, prior works annotate privacy policies at the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4402–4417august1–6,2021.©2021associationforcomputationallinguistics4402[we]data collector: first party entity may also [use]action or display [your]data provider: user[username]data collected: user online activities/proﬁles and [icon or proﬁle photo]data collected: user online activities/proﬁleson [marketing purpose or press releases]purpose: advertising/marketing.
privacy practice.
data collection/usage[we]data sharer: first party entity do [not]polarity: negation [sell]action [your]data provider: user[personal information]data shared: general data to [third parties]data receiver: third party entity.
privacy practice.
data sharing/disclosure.
table 1: annotation examples from policyie corpus.
best viewed in color..sentence level, without further utilizing the con-stituent text spans to convey speciﬁc information.
sentences written in a policy document explainprivacy practices, which we refer to as intent clas-siﬁcation and identifying the constituent text spansthat share further speciﬁc information as slot ﬁlling.
table 1 shows a couple of examples.
this formu-lation of information extraction lifts users’ burdento comprehend relevant segments in a policy docu-ment and identify the details, such as how and whyusers’ data are collected and shared with others..to facilitate ﬁne-grained information extraction,we present policyie, an english corpus consistingof 5,250 intent and 11,788 slot annotations over31 privacy policies of websites and mobile appli-cations.
we perform experiments using sequencetagging and sequence-to-sequence (seq2seq) learn-ing models to jointly model intent classiﬁcation andslot ﬁlling.
the results show that both modelingapproaches perform comparably in intent classiﬁ-cation, while seq2seq models outperform the se-quence tagging models in slot ﬁlling by a largemargin.
we conduct a thorough error analysis andcategorize the errors into seven types.
we observethat sequence tagging approaches miss more slotswhile seq2seq models predict more spurious slots.
we further discuss the error cases by consideringother factors to help guide future work.
we releasethe code and data to facilitate research.1.
2 construction of policyie corpus.
2.1 privacy policies selection.
the scope of privacy policies primarily dependson how service providers function.
for example,service providers primarily relying on mobile appli-cations (e.g., viber, whatsapp) or websites and ap-plications (e.g., amazon, walmart) have differentprivacy practices detailed in their privacy policies..in policyie, we want to achieve broad coverageacross privacy practices exercised by the serviceproviders such that the corpus can serve a widevariety of use cases.
therefore, we go through thefollowing steps to select the policy documents..initial collection ramanath et al.
(2014) intro-duced a corpus of 1,010 privacy policies of thetop websites ranked on alexa.com.
we crawledthose websites’ privacy policies in november 2019since the released privacy policies are outdated.
for mobile application privacy policies, we scrapeapplication information from google play store us-ing play-scraper public api2 and crawl theirprivacy policy.
we ended up with 7,500 mobileapplications’ privacy policies..filtering first, we ﬁlter out the privacy policieswritten in a non-english language and the mobileapplications’ privacy policies with the app reviewrating of less than 4.5. then we ﬁlter out privacypolicies that are too short (< 2,500 words) or toolong (> 6,000 words).
finally, we randomly se-lect 200 websites and mobile application privacypolicies each (400 documents in total).3.post-processing we ask a domain expert (work-ing in the security and privacy domain for morethan three years) to examine the selected 400 pri-vacy policies.
the goal for the examination is toensure the policy documents cover the four privacypractices: (1) data collection/usage, (2) datasharing/disclosure, (3) data storage/retention,and (4) data security/protection.
these four prac-tices cover how a service provider processes users’data in general and are included in the generaldata protection regulation (gdpr).
finally, weshortlist 50 policy documents for annotation, 25 ineach category (websites and mobile applications)..2https://github.com/danieliu/.
play-scraper.
1https://github.com/wasiahmad/.
3we ensure the mobile applications span different appli-.
policyie.
cation categories on the play store..44032.2 data annotation.
annotation schema to annotate sentences in apolicy document, we consider the ﬁrst four privacypractices from the annotation schema suggestedby wilson et al.
(2016a).
therefore, we performsentence categorization under ﬁve intent classesthat are described below.
(1) data collection/usage: what, why and how.
user information is collected;.
(2) data sharing/disclosure: what, why and howuser information is shared with or collectedby third parties;.
(3) data storage/retention: how long and where.
user information will be stored;.
(4) data security/protection: protection mea-.
sures for user information;.
(5) other: other privacy practices that do not fall.
into the above four categories..apart from annotating sentences with privacypractices, we aim to identify the text spans in sen-tences that explain speciﬁc details about the prac-tices.
for example, in the sentence “we collect per-sonal information in order to provide users with apersonalized experience”, the underlined text spanconveys the purpose of data collection.
in our anno-tation schema, we refer to the identiﬁcation of suchtext spans as slot ﬁlling.
there are 18 slot labels inour annotation schema (provided in appendix).
wegroup the slots into two categories: type-i and type-ii based on their role in privacy practices.
whilethe type-i slots include participants of privacy prac-tices, such as data provider, data receiver, type-iislots include purposes, conditions that characterizemore details of privacy practices.
note that type-iand type-ii slots may overlap, e.g., in the previousexample, the underlined text span is the purposeof data collection, and the span “user” is the dataprovider (whose data is collected).
in general, type-ii slots are longer (consisting of more words) andless frequent than type-i slots..in total, there are 14 type-i and 4 type-ii slotsin our annotation schema.
these slots are associ-ated with a list of attributes, e.g., data collectedand data shared have the attributes contact data,location data, demographic data, etc.
table 1illustrates a couple of examples.
we detail the slotsand their attributes in the appendix..annotation procedure general crowdworkerssuch as amazon mechanical turkers are not suit-able to annotate policy documents as it requires spe-cialized domain knowledge (mcdonald and cra-.
dataset.
# policies# sentences# type-i slots# type-ii slotsavg.
sentence lengthavg.
# type-i slot / sent.
avg.
# type-ii slot / sent.
avg.
type-i slot lengthavg.
type-ii slot length.
train.
254,2097,3272,26323.734.481.382.018.70.test.
61,0411,70449426.624.751.382.1510.70.table 2: statistics of the policyie corpus..nor, 2008; reidenberg et al., 2016).
we hire twolaw students to perform the annotation.
we usethe web-based annotation tool, brat (stenetorpet al., 2012) to conduct the annotation.
we writea detailed annotation guideline and pretest themthrough multiple rounds of pilot studies.
the guide-line is further updated with notes to resolve com-plex or corner cases during the annotation process.4the annotation process is closely monitored by adomain expert and a legal scholar and is grantedirb exempt by the institutional review board(irb).
the annotators are presented with one seg-ment from a policy document at a time and askedto perform annotation following the guideline.
wemanually segment the policy documents such that asegment discusses similar issues to reduce ambigu-ity at the annotator end.
the annotators worked 10weeks, with an average of 10 hours per week, andcompleted annotations for 31 policy documents.
each annotator is paid $15 per hour..post-editing and quality control we computean inter-annotator agreement for each annotatedsegment of policy documents using krippendorff’salpha (αk) (klaus, 1980).
the annotators areasked to discuss their annotations and re-annotatethose sections with token-level αk falling below0.75. an αk value within the range of 0.67 to 0.8is allowed for tentative conclusions (artstein andpoesio, 2008; reidsma and carletta, 2008).
afterthe re-annotation process, we calculate the agree-ment for the two categories of slots individually.
the inter-annotator agreement is 0.87 and 0.84 fortype-i and type-ii slots, respectively.
then the ad-judicators discuss and ﬁnalize the annotations.
theadjudication process involves one of the annotators,the legal scholar, and the domain expert..4we release the guideline as supplementary material..4404joint intent and slot tagginginput: [cls] we may also use or display your username and icon or proﬁle photo on marketingpurpose or press releases .
type-i slot tagging outputdata-collection-usage b-dc.fpe o o b-action o o b-dp.u b-dc.uoap o b-dc.uoapi-dc.uoap i-dc.uoap i-dc.uoap o o o o o o otype-ii slot tagging outputdata-collection-usage o o o o o o o o o o o o o o b-p.am i-p.am i-p.am i-p.am i-p.am o.sequence-to-sequence (seq2seq) learninginput: we may also use or display your username and icon or proﬁle photo on marketing purposeor press releases .
output: [in:data-collection-usage [sl:dc.fpe we] [sl:action use] [sl:dp.u your] [sl:dc.uoapusername] [sl:dc.uoap icon or proﬁle photo] [sl:p.am marketing purpose or press releases]].
table 3: an example of input / output used to train the two types of models on policyie.
for brevity, we re-placed part of label strings with symbols: dp.u, dc.fpe, dc.uoap, p.am represents data-provider.user, data-collector.first-party-entity, data-collected.user-online-activities-proﬁles, and purpose.advertising-marketing..data statistics & format table 2 presents thestatistics of the policyie corpus.
the corpus con-sists of 15 and 16 privacy policies of websites andmobile applications, respectively.
we release theannotated policy documents split into sentences.5each sentence is associated with an intent label,and the constituent words are associated with a slotlabel (following the bio tagging scheme)..3 model & setup.
policyie provides annotations of privacy practicesand corresponding text spans in privacy policies.
we refer to privacy practice prediction for a sen-tence as intent classiﬁcation and identifying thetext spans as slot ﬁlling.
we present two alterna-tive approaches; the ﬁrst approach jointly modelsintent classiﬁcation and slot tagging (chen et al.,2019), and the second modeling approach casts theproblem as a sequence-to-sequence learning task(rongali et al., 2020; li et al., 2021)..3.1 sequence tagging.
following chen et al.
(2019), given a sentences = w1, .
.
.
, wl from a privacy policy documentd, a special token (w0 = [cls]) is prepended toform the input sequence that is fed to an encoder.
the encoder produces contextual representationsof the input tokens h0, h1, .
.
.
, hl where h0 andh1, .
.
.
, hl are fed to separate softmax classiﬁers.
5we split the policy documents into sentences using ud-.
pipe (straka et al., 2016)..to predict the target intent and slot labels..yi = softmax(w tn = softmax(w tys.
i h0 + bi),s hn + bs), n ∈ 1, .
.
.
l,.
where wi ∈ rd×i , ws ∈ rd×s, br ∈ ri andbi ∈ ri , bs ∈ rs are parameters, and i, s arethe total number of intent and slot types, respec-tively.
the sequence tagging model (composed ofan encoder and a classiﬁer) learns to maximize thefollowing conditional probability to perform intentclassiﬁcation and slot ﬁlling jointly..p (yi, ys.
∣s) = p(yi.
∣s).
p(ys.
n∣s)..l∏n=1.
we train the models end-to-end by minimizingthe cross-entropy loss.
table 3 shows an exam-ple of input and output to train the joint intent andslot tagging models.
since type-i and type-ii slotshave different characteristics as discussed in § 2.2and overlap, we train two separate sequential tag-ging models for type-i and type-ii slots to keep thebaseline models simple.6 we use bilstm (liuand lane, 2016; zhang and wang, 2016), trans-former (vaswani et al., 2017), bert (vaswaniet al., 2017), and roberta (liu et al., 2019) asencoder to form the sequence tagging models..besides, we consider an embedding based base-line where the input word embeddings are fed tothe softmax classiﬁers.
the special token (w0 =.
6span enumeration based techniques (wadden et al.,2019; luan et al., 2019) can be utilized to perform taggingboth types of slots jointly, and we leave this as future work..4405model.
humanembeddingbilstmtransformerbertrobertaembedding w/ crfbilstm w/ crftransformer w/ crfbert w/ crfroberta w/ crf.
# param(in millions)-1.7834.81101241.7834.8110124.intent f1.
96.550.9±27.375.9±1.180.1±0.684.7±0.784.5±0.767.9±0.676.7±1.477.9±2.782.1±2.083.3±1.6.
type-i.
type-ii.
slot f184.319.1±0.340.8±0.941.0±3.555.5±1.154.2±1.926.0±1.545.1±1.243.7±2.356.0±0.857.0±0.6.
em56.60.8±0.37.6±0.96.5±2.817.0±1.114.3±2.41.20±0.39.2±0.98.9±3.019.2±1.118.2±1.2.
slot f162.30.0±0.03.9±3.03.5±1.029.6±2.429.8±1.75.7±4.626.8±2.25.7±0.931.7±1.934.5±1.3.
em55.60.0±0.010.0±2.713.1±2.424.2±4.224.8±1.43.1±0.618.1±2.011.0±2.119.7±2.627.7±3.9.
table 4: test set performance of the sequence tagging models on policyie corpus.
we individually train andevaluate the models on intent classiﬁcation and type-i and type-ii slots tagging and report average intent f1 score..[cls]) embedding is formed by applying aver-age pooling over the input word embeddings.
wetrain wordpiece embeddings with a 30,000 tokenvocabulary (devlin et al., 2019) using fasttext (bo-janowski et al., 2017) based on a corpus of 130,000privacy policies collected from apps on the googleplay store (harkous et al., 2018).
we use the hid-den state corresponding to the ﬁrst wordpiece of atoken to predict the target slot labels..conditional random field (crf) helps struc-ture prediction tasks, such as semantic role labeling(zhou and xu, 2015) and named entity recognition(cotterell and duh, 2017).
therefore, we modelslot labeling jointly using a conditional randomﬁeld (crf) (lafferty et al., 2001) (only interactionsbetween two successive labels are considered).
werefer the readers to ma and hovy (2016) for details..3.2 sequence-to-sequence learning.
recent works in semantic parsing (rongali et al.,2020; zhu et al., 2020; li et al., 2021) formulatethe task as sequence-to-sequence (seq2seq) learn-ing.
taking this as a motivation, we investigatethe scope of seq2seq learning for joint intent clas-siﬁcation and slot ﬁlling for privacy policy sen-tences.
in table 3, we show an example of encoderinput and decoder output used in seq2seq learn-ing.
we form the target sequences by following thetemplate: [in:label [sl:label w1, .
.
.
, wm].
.
.
].
during inference, we use greedy decoding andparse the decoded sequence to extract intent andslot labels.
note that we only consider text spans inthe decoded sequences that are surrounded by “[]”;the rest are discarded.
since our proposed policyie.
corpus consists of a few thousand examples, insteadof training seq2seq models from scratch, we ﬁne-tune pre-trained models as the baselines.
specif-ically, we consider ﬁve state-of-the-art models:minilm (wang et al., 2020), unilm (dong et al.,2019), unilmv2 (bao et al., 2020), mass (songet al.
), and bart (lewis et al., 2020)..3.3 setup.
implementation we use the implementation ofbert and roberta from transformers api(wolf et al., 2020).
for the seq2seq learning base-lines, we use their public implementations.7,8,9 wetrain bilstm, transformer baseline models andﬁne-tune all the other baselines for 20 epochs andchoose the best checkpoint based on validation per-formance.
from 4,209 training examples, we use4,000 examples for training (∼95%) and 209 ex-amples for validation (∼5%).
we tune the learningrate in [1e-3, 5e-4, 1e-4, 5e-5, 1e-5] and set thebatch size to 16 in all our experiments (to ﬁt in onegeforce gtx 1080 gpu with 11gb memory).
wetrain (or ﬁne-tune) all the models ﬁve times withdifferent seeds and report average performances..evaluation metrics to evaluate the baseline ap-proaches, we compute the f1 score for intent classi-ﬁcation and slot ﬁlling tasks.10 we also compute anexact match (em) accuracy (if the predicted intentmatches the reference intent and slot f1 = 1.0)..7https://github.com/microsoft/unilm8https://github.com/microsoft/mass9https://github.com/pytorch/fairseq/.
tree/master/examples/bart.
10we use a micro average for intent classiﬁcation..4406model.
humanminilmunilmunilmv2mass.
bart.
# param(in millions)-33110110123140400.intent f1.
96.583.9±0.383.6±0.584.7±0.581.8±1.283.3±1.183.6±1.3.
type-i.
type-ii.
slot f184.352.4±1.558.2±0.761.4±0.954.1±2.553.6±1.763.7±1.3.
em56.619.8±1.628.6±1.229.9±1.221.3±2.010.6±1.723.0±1.3.
slot f162.340.4±0.453.5±1.453.5±1.544.9±1.252.4±2.755.2±1.0.
em55.627.9±1.635.4±1.933.5±1.525.3±1.327.5±2.231.6±2.0.
table 5: test set performance of the seq2seq models on policyie corpus..human performanceis computed by consider-ing each annotator’s annotations as predictions andthe adjudicated annotations as the reference.
theﬁnal score is an average across all annotators..4 experiment results & analysis.
we aim to address the following questions..1. how do the two modeling approaches perform.
on our proposed dataset (§ 4.1)?.
2. how do they perform on different intent and.
3. what type of errors do the best performing.
slot types (§ 4.2)?.
models make (§ 4.3)?.
4.1 main results.
sequence tagging the overall performances ofthe sequence tagging models are presented in table4. the pre-trained models, bert and roberta,outperform other baselines by a large margin.
us-ing conditional random ﬁeld (crf), the modelsboost the slot tagging performance with a slightdegradation in intent classiﬁcation performance.
for example, roberta + crf model improvesover roberta by 2.8% and 3.9% in terms of type-i slot f1 and em with a 0.5% drop in intent f1score.
the results indicate that predicting type-iislots is difﬁcult compared to type-i slots as they dif-fer in length (type-i slots are mostly phrases, whiletype-ii slots are clauses) and are less frequent inthe training examples.
however, the em accuracyfor type-i slots is lower than type-ii slots due tomore type-i slots (∼4.75) than type-ii slots (∼1.38)on average per sentence.
note that if models fail topredict one of the slots, em will be zero..seq2seq learning seq2seq models predict theintent and slots by generating the labels and spansfollowing a template.
then we extract the intentand slot labels from the generated sequences.
theexperiment results are presented in table 5. to our.
surprise, we observe that all the models performwell in predicting intent and slot labels.
the bestperforming model is bart (according to slot f1score) with 400 million parameters, outperformingits smaller variant by 10.1% and 2.8% in terms ofslot f1 for type-i and type-ii slots, respectively..sequence tagging vs. seq2seq learning it isevident from the experiment results that seq2seqmodels outperform the sequence tagging modelsin slot ﬁlling by a large margin, while in intentclassiﬁcation, they are competitive.
however, boththe modeling approaches perform poorly in predict-ing all the slots in a sentence correctly, resultingin a lower em score.
one interesting factor is,the seq2seq models signiﬁcantly outperform se-quence tagging models in predicting type-ii slots.
note that type-ii slots are longer and less frequent,and we suspect conditional text generation helpsseq2seq models predict them accurately.
in com-parison, we suspect that due to fewer labeled exam-ples of type-ii slots, the sequence tagging modelsperform poorly on that category (as noted before,we train the sequence tagging models for the type-iand type-ii slots individually)..next, we break down roberta (w/ crf) andbart’s performances, the best performing modelsin their respective model categories, followed byan error analysis to shed light on the error types..4.2 performance breakdown.
intent classiﬁcation in the policyie corpus,38% of the sentences fall into the ﬁrst four cat-egories: data collection, data sharing, data stor-age, data security, and the remaining belong tothe other category.
therefore, we investigate howmuch the models are confused in predicting theaccurate intent label.
we provide the confusionmatrix of the models in appendix.
due to an im-balanced distribution of labels, bart makes many.
4407intent labels.
intent f1.
slot f1.
type-i.
type-ii.
robertadata collection 74.1±1.1 59.8±0.8 28.9±2.767.2±2.0 53.6±5.7 34.4±3.4data sharing61.7±3.6 40.1±3.7 31.6±3.1data storagedata security68.9±2.9 53.9±4.9 21.9±2.5bartdata collection 73.5±2.3 67.0±4.2 56.2±2.870.4±2.7 61.2±1.6 53.5±3.4data sharing63.1±4.7 56.2±8.2 64.9±2.5data storage67.2±3.9 66.0±2.2 32.8±1.3data security.
table 6: test performance of the roberta and bartmodel for each intent type..incorrect predictions.
we notice that bart is con-fused most between data collection and data stor-age labels.
our manual analysis reveals that bartis confused between slot labels {“data collector”,“data holder”} and {“data retained”, “data col-lected”} as they are often associated with the sametext span.
we suspect this leads to bart’s confu-sion.
table 6 presents the performance breakdownacross intent labels..slot filling we breakdown the models’ perfor-mances in slot ﬁlling under two settings.
first,table 6 shows slot ﬁlling performance under dif-ferent intent categories.
among the four classes,the models perform worst on slots associated withthe “data security” intent class as policyie hasthe lowest amount of annotations for that intentcategory.
second, we demonstrate the models’performances on different slot types in figure 1.roberta’s recall score for “polarity”, “protect-against”, “protection-method” and “storage-place”slot types is zero.
this is because these slot typeshave the lowest amount of training examples in pol-icyie.
on the other hand, bart achieves a higherrecall score, specially for the “polarity” label astheir corresponding spans are short..we also study the models’ performances on slotsof different lengths.
the results show that bartoutperforms roberta by a larger margin on longerslots (see figure 2), corroborating our hypothesisthat conditional text generation results in more ac-curate predictions for longer spans..4.3 error analysis.
we analyze the incorrect intent and slot predictionsby roberta and bart.
we categorize the errors.
figure 1: test set performance (recall score) on poli-cyie for the eighteen slot types..figure 2: test set performance (recall score) on poli-cyie for slots with different length..into seven types.
note that a predicted slot is con-sidered correct if its’ label and span both match(exact match) one of the references.
we character-ize the error types as follows..1. wrong intent (wi): the predicted intent la-bel does not match the reference intent label.
2. missing slot (ms): none of the predicted.
slots exactly match a reference slot..3. spurious slot (ss): label of a predicted slot.
does not match any of the references..4. wrong split (wsp): two or more predictedslot spans with the same label could be mergedto match one of the reference slots.
a mergedspan and a reference span may only differ inpunctuations or stopwords (e.g., and)..5. wrong boundary (wb): a predicted slotspan is a sub-string of the reference span orvice versa.
the slot label must exactly match..44080.00.20.40.60.8actionconditiondata-collecteddata-collectordata-holderdata-protecteddata-protectordata-providerdata-receiverdata-retaineddata-shareddata-sharerpolarityprotect-againstprotection-methodpurposeretention-periodstorage-placerobertabart0.00.20.40.60.82345678910[11-20][21-30][31-40]50+robertabart+ [in:data-collection-usage [sl:data-provider.third-party-entity third parties] [sl:action collect] [sl:data-provider.user your] [sl:data-collected.data-general information] [sl:data-collector.ﬁrst-party-entity us]]− [in:data-sharing-disclosure [sl:data-receiver.third-party-entity third parties] [sl:action share][sl:data-provider.user your] [sl:data-shared.data-general information] [sl:data-sharer.ﬁrst-party-entityus] [sl:condition where applicable] [sl:condition based on their own privacy policies]]error types: wrong intent (wi), wrong label (wl), wrong slot (ws), spurious slot (ss)+ [.
.
.
[sl:data-provider.third-party-entity third parties] [sl:condition it is allowed by applicable law oraccording to your agreement with third parties]]− [.
.
.
[sl:condition allowed by applicable law or according to your agreement with third parties]]error types: wrong boundary (wb), missing slot (ms)+ [.
.
.
[sl:data-receiver.third-party-entity social media and other similar platforms] .
.
. ]
− [.
.
.
[sl:data-receiver.third-party-entity social media] [sl:data-receiver.third-party-entity other similarplatforms] .
.
. ]
error types: wrong split (wsp).
table 7: three examples showing different error types appeared in bart’s predictions.
+ and − indicates thereference and predicted sequences, respectively.
best viewed in color..errorwrong intentspurious slotmissing slotwrong boundarywrong slotwrong splitwrong labeltotal slotscorrect predictiontotal errorstotal predictions.
roberta bart.
16147286713010332182,1981,0641,6222,686.
17872351716014327192,1981,3611,5892,950.table 8: counts for each error type on the test set ofpolicyie using roberta and bart models..6. wrong label (wl): a predicted slot spanmatches a reference, but the label does not.
7. wrong slot (ws): all other types of errors.
fall into this category..we provide one example of each error type intable 7. in table 8, we present the counts for eacherror type made by roberta and bart models.
the two most frequent error types are ss and ms.while bart makes more ss errors, robertasuffers from ms errors.
while both the modelsare similar in terms of total errors, bart makesmore correct predictions resulting in a higher re-call score, as discussed before.
one possible wayto reduce ss errors is by penalizing more on wrongslot label prediction than slot span.
on the otherhand, reducing ms errors is more challenging asmany missing slots have fewer annotations than.
others.
we provide more qualitative examples inappendix (see table 11 and 12) ..in the error analysis, we exclude the test exam-ples (sentences) with the intent label “other” andno slots.
out of 1,041 test instances in policyie,there are 682 instances with the intent label “other”.
we analyze roberta and bart’s predictions onthose examples separately to check if the modelspredict slots as we consider them as spurious slots.
while roberta meets our expectation of perform-ing highly accurate (correct prediction for 621 outof 682), bart also correctly predicts 594 out of682 by precisely generating “[in:other]”.
overallthe error analysis aligns with our anticipation thatthe seq2seq modeling technique has promise andshould be further explored in future works..5 related work.
automated privacy policy analysis automat-ing privacy policy analysis has drawn researchers’attention as it enables the users to know theirrights and act accordingly.
therefore, signiﬁcantresearch efforts have been devoted to understand-ing privacy policies.
earlier approaches (costanteet al., 2012) designed rule-based pattern matchingtechniques to extract speciﬁc types of information.
under the usable privacy project (sadeh et al.,2013), several works have been done (bhatia andbreaux, 2015; wilson et al., 2016a,b; sathyendraet al., 2016; bhatia et al., 2016; hosseini et al.,2016; mysore sathyendra et al., 2017; zimmecket al., 2019; bannihatti kumar et al., 2020).
no-.
4409table works leveraging nlp techniques include textalignment (liu et al., 2014; ramanath et al., 2014),text classiﬁcation (wilson et al., 2016a; harkouset al., 2018; zimmeck et al., 2019), and question an-swering (qa) (shvartzshanider et al., 2018; hark-ous et al., 2018; ravichander et al., 2019; ahmadet al., 2020).
bokaie hosseini et al.
(2020) is themost closest to our work that used named entityrecognition (ner) modeling technique to extractthird party entities mentioned in policy documents.
our proposed policyie corpus is distinct fromthe previous privacy policies benchmarks: opp-115 (wilson et al., 2016a) uses a hierarchical an-notation scheme to annotate text segments with aset of data practices and it has been used for multi-label classiﬁcation (wilson et al., 2016a; harkouset al., 2018) and question answering (harkous et al.,2018; ahmad et al., 2020); privacyqa (ravichan-der et al., 2019) frame the qa task as identifyinga list of relevant sentences from policy documents.
recently, bui et al.
(2021) created a dataset bytagging documents from opp-115 for privacy prac-tices and uses ner models to extract them.
incontrast, policyie is developed by following se-mantic parsing benchmarks, and we model the taskfollowing the nlp literature..intent classiﬁcation and slot filling voice as-sistants and chat-bots frame the task of natural lan-guage understanding via classifying intents and ﬁll-ing slots given user utterances.
several benchmarkshave been proposed in literature covering severaldomains, and languages (hemphill et al., 1990;coucke et al., 2018; gupta et al., 2018; upadhyayet al., 2018; schuster et al., 2019; xu et al., 2020;li et al., 2021).
our proposed policyie corpus isa new addition to the literature within the securityand privacy domain.
policyie enables us to buildconversational solutions that users can interact withand learn about privacy policies..6 conclusion.
this work aims to stimulate research on automat-ing information extraction from privacy policiesand reconcile it with users’ understanding of theirrights.
we present policyie, an intent classiﬁca-tion and slot ﬁlling benchmark on privacy policieswith two alternative neural approaches as baselines.
we perform a thorough error analysis to shed lighton the limitations of the two baseline approaches.
we hope this contribution would call for researchefforts in the specialized privacy domain from both.
privacy and nlp communities..acknowledgments.
the authors acknowledge the law students michaelrasmussen and martyna glaz at fordham univer-sity who worked as annotators to make the devel-opment of this corpus possible.
this work wassupported in part by national science foundationgrant oac 1920462. any opinions, ﬁndings, con-clusions, or recommendations expressed herein arethose of the authors, and do not necessarily reﬂectthose of the us government or nsf..broader impact.
privacy and data breaches have a signiﬁcant im-pact on individuals.
in general, security breachesexpose the users to different risks such as ﬁnan-cial loss (due to losing employment or businessopportunities), physical risks to safety, and iden-tity theft.
identity theft is among the most severeand fastest-growing crimes.
however, the risksdue to data breaches can be minimized if the usersknow their rights and how they can exercise themto protect their privacy.
this requires the usersto read the privacy policies of websites they visitor the mobile applications they use.
as readingprivacy policies is a tedious task, automating pri-vacy policy analysis reduces the burden of users.
automating information extraction from privacypolicies empowers users to be aware of their datacollected and analyzed by service providers fordifferent purposes.
service providers collect con-sumer data at a massive scale and often fail to pro-tect them, resulting in data breaches that have led toincreased attention towards data privacy and relatedrisks.
reading privacy policies to understand users’rights can help take informed and timely decisionson safeguarding data privacy to mitigate the risks.
developing an automated solution to facilitate pol-icy document analysis requires labeled examples,and the policyie corpus adds a new dimension tothe available datasets in the security and privacy do-main.
while policyie enables us to train models toextract ﬁne-grained information from privacy poli-cies, the corpus can be coupled with other existingbenchmarks to build a comprehensive system.
forexample, privacyqa corpus (ravichander et al.,2019) combined with policyie can facilitate build-ing qa systems that can answer questions withﬁne-grained details.
we believe our experimentsand analysis will help direct future research..4410references.
wasi ahmad, jianfeng chi, yuan tian, and kai-weichang.
2020. policyqa: a reading comprehensiondataset for privacy policies.
in findings of the as-sociation for computational linguistics: emnlp2020, pages 743–749, online.
association for com-putational linguistics..ron artstein and massimo poesio.
2008. survey ar-ticle: inter-coder agreement for computational lin-computational linguistics, 34(4):555–guistics.
596..vinayshekhar bannihatti kumar, roger iyengar, na-mita nisal, yuanyuan feng, hana habib, peterstory, sushain cherivirala, margaret hagan, lor-rie cranor, shomir wilson, et al.
2020. finding achoice in a haystack: automatic extraction of opt-in pro-out statements from privacy policy text.
ceedings of the web conference 2020, pages 1943–1954..hangbo bao, li dong, furu wei, wenhui wang, nanyang, xiaodong liu, yu wang, jianfeng gao, song-hao piao, ming zhou, et al.
2020. unilmv2: pseudo-masked language models for uniﬁed language modelin international conference on ma-pre-training.
chine learning, pages 642–652.
pmlr..jaspreet bhatia and travis d breaux.
2015. towardsan information type lexicon for privacy policies.
in2015 ieee eighth international workshop on re-quirements engineering and law (relaw), pages19–24.
ieee..jaspreet bhatia, morgan c evans, sudarshan wadkar,and travis d breaux.
2016. automated extractionof regulated information types using hyponymy re-in 2016 ieee 24th international require-lations.
ments engineering conference workshops (rew),pages 19–25.
ieee..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..mitra bokaie hosseini, pragyan k c, irwin reyes, andserge egelman.
2020.identifying and classifyingthird-party entities in natural language privacy poli-cies.
in proceedings of the second workshop on pri-vacy in nlp, pages 18–27, online.
association forcomputational linguistics..duc bui, kang g shin, jong-min choi, and junbumshin.
2021. automated extraction and presentationof data practices in privacy policies.
proceedings onprivacy enhancing technologies, 2021(2):88–110..qian chen, zhu zhuo, and wen wang.
2019. bertfor joint intent classiﬁcation and slot ﬁlling.
arxivpreprint arxiv:1902.10909..federal trade commission et al.
2012. protecting con-sumer privacy in an era of rapid change.
ftc report..elisa costante, jerry den hartog, and milan petkovi´c.
2012. what websites know about you.
in data pri-vacy management and autonomous spontaneous se-curity, pages 146–159.
springer..ryan cotterell and kevin duh.
2017..low-resource named entity recognition with cross-lingual, character-level neural conditional randomin proceedings of the eighth internationalﬁelds.
joint conference on natural language processing(volume 2: short papers), pages 91–96, taipei, tai-wan.
asian federation of natural language process-ing..alice coucke, alaa saade, adrien ball, th´eodorebluche, alexandre caulier, david leroy, cl´ementdoumouro, thibault gisselbrecht, francesco calta-girone, thibaut lavril, et al.
2018. snips voice plat-form: an embedded spoken language understandingsystem for private-by-design voice interfaces.
arxivpreprint arxiv:1805.10190..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, pages 13063–13075..joshua gluck, florian schaub, amy friedman, hanahabib, norman sadeh, lorrie faith cranor, and yu-vraj agarwal.
how short is too short?
implicationsof length and framing on the effectiveness of privacyin twelfth symposium on usable privacynotices.
and security ({soups} 2016)..abhirut gupta, anupama ray, gargi dasgupta, gau-tam singh, pooja aggarwal, and prateeti mohapatra.
2018. semantic parsing for technical support ques-tions.
in proceedings of the 27th international con-ference on computational linguistics, pages 3251–3259, santa fe, new mexico, usa.
association forcomputational linguistics..hamza harkous, kassem fawaz, r´emi lebret, florianschaub, kang g shin, and karl aberer.
2018. poli-sis: automated analysis and presentation of privacypolicies using deep learning.
in 27th {usenix} se-curity symposium ({usenix} security 18), pages531–548..charles t hemphill, john j godfrey, and george rdoddington.
1990. the atis spoken language sys-tems pilot corpus.
in speech and natural language:proceedings of a workshop held at hidden valley,pennsylvania, june 24-27, 1990..4411mitra bokaei hosseini, sudarshan wadkar, travis dbreaux, and jianwei niu.
2016. lexical similarityof information type hypernyms, meronyms and syn-onyms in privacy policies.
in 2016 aaai fall sym-posium series..krippendorff klaus.
1980. content analysis: an intro-.
duction to its methodology..john lafferty, andrew mccallum, and fernando cnpereira.
2001. conditional random ﬁelds: prob-abilistic models for segmenting and labeling se-quence data..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..haoran li, abhinav arora, shuohui chen, anchitgupta, sonal gupta, and yashar mehdad.
2021.mtop: a comprehensive multilingual task-orientedsemantic parsing benchmark.
in proceedings of the16th conference of the european chapter of theassociation for computational linguistics: mainvolume, pages 2950–2962, online.
association forcomputational linguistics..bing liu and ian lane.
2016. attention-based recur-rent neural network models for joint intent detectionin interspeech 2016, 17th annualand slot ﬁlling.
conference of the international speech communica-tion association, pages 685–689..fei liu, rohan ramanath, norman sadeh, and noah a.smith.
2014. a step towards usable privacy policy:automatic alignment of privacy statements.
in pro-ceedings of coling 2014, the 25th internationalconference on computational linguistics: techni-cal papers, pages 884–894, dublin, ireland..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-in proceedings of the 2019namic span graphs.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3036–3046, minneapolis, minnesota.
association for computational linguistics..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting of.
the association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..florencia marotta-wurgler.
2015. does “notice andchoice” disclosure regulation work?
an empiricalstudy of privacy policies,”.
in michigan law: lawand economics workshop..aleecia m mcdonald and lorrie faith cranor.
2008.the cost of reading privacy policies.
isjlp, 4:543..kanthashree mysore sathyendra, shomir wilson, flo-rian schaub, sebastian zimmeck, and normansadeh.
2017. identifying the provision of choices inprivacy policy text.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing..rohan ramanath, fei liu, norman sadeh, and noah asmith.
2014. unsupervised alignment of privacypolicies using hidden markov models.
in proceed-ings of the 52nd annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 605–610..abhilasha ravichander, alan w black, shomir wilson,thomas norton, and norman sadeh.
2019. ques-tion answering for privacy policies: combining com-putational and legal perspectives.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 4947–4958..joel r reidenberg, jaspreet bhatia, travis d breaux,and thomas b norton.
2016. ambiguity in privacypolicies and the impact of regulation.
the journalof legal studies, 45(s2):s163–s190..dennis reidsma and jean carletta.
2008. reliabilitymeasurement without limits.
computational lin-guistics, 34(3):319–326..subendhu rongali, luca soldaini, emilio monti, andwael hamza.
2020. don’t parse, generate!
a se-quence to sequence architecture for task-oriented se-mantic parsing.
in proceedings of the web confer-ence 2020, pages 2962–2968..norman sadeh, alessandro acquisti, travis d breaux,lorrie faith cranor, aleecia m mcdonald, joel rreidenberg, noah a smith, fei liu, n cameronrussell, florian schaub, et al.
2013. the usableprivacy policy project.
technical report, technicalreport, cmu-isr-13-119..kanthashree mysore sathyendra, florian schaub,shomir wilson, and norman sadeh.
2016. au-tomatic extraction of opt-out choices from privacypolicies.
in 2016 aaai fall symposium series..sebastian schuster, sonal gupta, rushin shah, andmike lewis.
2019. cross-lingual transfer learningin proceed-for multilingual task oriented dialog.
ings of the 2019 conference of the north american.
4412chapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3795–3805..yan shvartzshanider, ananth balashankar, thomaswies, and lakshminarayanan subramanian.
2018.recipe: applying open domain question answer-ing to privacy policies.
in proceedings of the work-shop on machine reading for question answering,pages 71–77, melbourne, australia.
association forcomputational linguistics..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
mass: masked sequence to sequence pre-in internationaltraining for language generation.
conference on machine learning..pontus stenetorp, sampo pyysalo, goran topi´c,tomoko ohta, sophia ananiadou, and jun’ichi tsu-jii.
2012. brat: a web-based tool for nlp-assistedin proceedings of the demonstra-text annotation.
tions at the 13th conference of the european chap-ter of the association for computational linguistics,pages 102–107..milan straka, jan hajic, and jana strakov´a.
2016.udpipe:trainable pipeline for processing conll-uﬁles performing tokenization, morphological anal-in proceedings ofysis, pos tagging and parsing.
the tenth international conference on language re-sources and evaluation (lrec’16), pages 4290–4297..shyam upadhyay, manaal faruqui, gokhan t¨ur,hakkani-t¨ur dilek, and larry heck.
2018.
(almost)zero-shot cross-lingual spoken language understand-in 2018 ieee international conference oning.
acoustics, speech and signal processing (icassp),pages 6034–6038.
ieee..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5784–5789, hong kong, china.
association for computa-tional linguistics..wenhui wang, furu wei, li dong, hangbo bao, nanyang, and ming zhou.
2020. minilm: deep self-attention distillation for task-agnostic compressionof pre-trained transformers.
in advances in neuralinformation processing systems..shomir wilson, florian schaub, aswarth abhilashdara, frederick liu, sushain cherivirala, pedro.
giovanni leon, mads schaarup andersen, sebas-tian zimmeck, kanthashree mysore sathyendra,n. cameron russell, thomas b. norton, eduardhovy, joel reidenberg, and norman sadeh.
2016a.
the creation and analysis of a website privacy pol-icy corpus.
in proceedings of the 54th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1330–1340..shomir wilson, florian schaub, rohan ramanath,norman sadeh, fei liu, noah a smith, and fred-erick liu.
2016b.
crowdsourcing annotations forwebsites’ privacy policies: can it really work?
inproceedings of the 25th international conferenceon world wide web, pages 133–143.
internationalworld wide web conferences steering committee..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..weijia xu, batool haider, and saab mansour.
2020.end-to-end slot alignment and recognition for cross-in proceedings of the 2020 confer-lingual nlu.
ence on empirical methods in natural languageprocessing (emnlp), pages 5052–5063, online.
as-sociation for computational linguistics..xiaodong zhang and houfeng wang.
2016. a jointmodel of intent determination and slot ﬁlling for spo-ken language understanding.
in ijcai, volume 16,pages 2993–2999..jie zhou and wei xu.
2015. end-to-end learning of se-mantic role labeling using recurrent neural networks.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages1127–1137, beijing, china.
association for compu-tational linguistics..qile zhu, haidar khan, saleh soltan, stephen rawls,and wael hamza.
2020. don’t parse, insert: multi-lingual semantic parsing with insertion based decod-ing.
in proceedings of the 24th conference on com-putational natural language learning, pages 496–506, online.
association for computational linguis-tics..sebastian zimmeck, peter story, daniel smullen, ab-hilasha ravichander, ziqi wang, joel reidenberg,n cameron russell, and norman sadeh.
2019.maps: scaling privacy compliance analysis to a mil-lion apps.
proceedings on privacy enhancing tech-nologies, 2019(3):66–86..4413type-i slots.
attributes.
action.
none.
data provider.
(1) user (2) third party entity.
data collector.
(1) first party entity.
data collected.
(1) general data (2) aggregated/non-identiﬁable data (3) contact data (4) financial data(5) location data (6) demographic data (7) cookies, web beacons and other technologies(8) computer/device data (9) user online activities/proﬁles (10) other data.
data sharer.
data shared.
(1) first party entity.
(1) general data (2) aggregated/non-identiﬁable data (3) contact data (4) financial data(5) location data (6) demographic data (7) cookies, web beacons and other technologies(8) computer/device data (9) user online activities/proﬁles (10) other data.
data receiver.
(1) third party entity.
data holder.
(1) first party entity (2) third party entity.
data retained.
(1) general data (2) aggregated/non-identiﬁable data (3) contact data (4) financial data(5) location data (6) demographic data (7) cookies, web beacons and other technologies(8) computer/device data (9) user online activities/proﬁles (10) other data.
storage place.
retention period.
none.
none.
data protected.
data protector.
(1) first party entity (2) third party entity.
(1) general data (2) aggregated/non-identiﬁable data (3) contact data (4) financial data(5) location data (6) demographic data (7) cookies, web beacons and other technologies(8) computer/device data (9) user online activities/proﬁles (10) other data.
protect against.
security threat.
type-ii slots.
attributes.
purpose.
condition.
polarity.
none.
(1) negation.
protection method.
(1) basic service/feature (2) advertising/marketing (3) legal requirement(4) service operation and security (5) personalization/customization(6) analytics/research (7) communications (8 merge/acquisition (9) other purpose.
(1) general safeguard method (2) user authentication (3) access limitation(5) encryptions (6) other protection method.
table 9: slots and their associated attributes.
“none” indicates there are no attributes for the those slots..4414data.
data.
data.
data.
collection/usage sharing/disclosure storage/retention security/protection.
privacy practices.
type-i slotsactiondata providerdata collectordata collecteddata sharerdata shareddata receiverdata holderdata retainedstorage placeretention perioddata protectordata protectedprotect againsttype-ii slotspurposeconditionpolarityprotection method# of slots# of sequences.
750 / 169784 / 172653 / 1511833 / 361----------.
894 / 193337 / 8150 / 15-5301 / 1142919 / 186.
344 / 70247 / 54--288 / 54541 / 110456 / 115-------.
327 / 65154 / 2621 / 1-2378 / 495380 / 83.
198 / 57139 / 44-----192 / 59291 / 11970 / 21101 / 17---.
168 / 4081 / 2522 / 1-1262 / 383232 / 61.
102 / 3165 / 20---------105 / 31119 / 3449 / 15.
5 / 043 / 718 / 5143 / 35649 / 178103 / 29.table 10: privacy practices and the associated slots with their distributions.
“x / y” indicates there are x instancesin the train set and y instances in the test set..figure 3: confusion matrix for intent classiﬁcationusing the roberta model..figure 4: confusion matrix for intent classiﬁcationusing the bart model..4415otherdatacollectiondatasharingdatastoragedatasecurityotherdatacollectiondatasharingdatastoragedatasecurity0.920.050.010.010.010.120.770.050.060.000.240.070.650.020.020.100.250.060.590.000.170.020.100.030.670.20.40.60.8otherdatacollectiondatasharingdatastoragedatasecurityotherdatacollectiondatasharingdatastoragedatasecurity0.880.080.020.010.010.060.810.050.080.010.150.090.720.010.020.080.210.060.660.000.160.030.050.030.730.000.150.300.450.600.75ground truth.
roberta(p:1.0, r: 0.75).
bart(p:1.0, r: 1.0).
ground truth.
roberta(p:0.0, r: 0.0).
bart(p:0.0, r: 0.0).
ground truth.
roberta(p:0.6, r: 0.6).
bart(p:0.83, r: 1.0).
ground truth.
roberta(p:1.0, r: 1.0).
bart(p:0.25, r: 0.25).
labeldata-holder.ﬁrst-party-entityactiondata-retained.data-generalretention-period.retention-period.
(cid:51) data-holder.ﬁrst-party-entity(cid:51) action(cid:51) retention-period.retention-period(cid:51) data-holder.ﬁrst-party-entity(cid:51) action(cid:51) data-retained.data-general(cid:51) retention-period.retention-perioddata-collector.ﬁrst-party-entityactiondata-collected.data-generaldata-sharer.ﬁrst-party-entitydata-shared.data-generaldata-sharer.ﬁrst-party-entityactiondata-shared.data-general.
(cid:55)(cid:55)(cid:55)(cid:55)(cid:55).
data-sharer.ﬁrst-party-entitydata-receiver.third-party-entitydata-shared.data-generaldata-provider.useraction(cid:55)data-receiver.third-party-entity(cid:55)data-sharer.ﬁrst-party-entity(cid:51) data-receiver.third-party-entity(cid:51) data-shared.data-general(cid:51) action(cid:51) data-sharer.ﬁrst-party-entity(cid:51) data-receiver.third-party-entity(cid:51) data-shared.data-general(cid:55)(cid:51) data-provider.user(cid:51) action.
data-sharer.ﬁrst-party-entity.
data-sharer.ﬁrst-party-entitydata-receiver.third-party-entityactiondata-shared.data-general(cid:51) data-sharer.ﬁrst-party-entity(cid:51) data-receiver.third-party-entity(cid:51) action(cid:51) data-shared.data-general(cid:55)(cid:55)(cid:51) action(cid:55).
data-collector.ﬁrst-party-entitydata-provider.third-party-entity.
data-collected.data-general.
textwekeeprecordsa period of no more than 6 yearswekeepa period of no more than 6 yearswekeeprecordsa period of no more than 6 years.
weaccessinformationweinformationwediscloseinformation.
marco polothird partypersonal informationuserstransferredmarcoourthird partypersonal informationtransferredmarco polothird partypersonal informationususerstransferred.
wethird partiesprovideinformationwethird partiesprovideinformationwethird partiesprovideinformation.
table 11: sample roberta and bart predictions of type-i slots.
((cid:51)) and ((cid:55)) indicates correct and incorrectpredictions, respectively.
precision (p) and recall (r) score is reported for each example in the left column..4416ground truth.
roberta(p:1.0, r: 1.0)bart(p:1.0, r: 1.0).
ground truth.
roberta(p:0.0, r: 0.0).
(cid:55).
bart(p:1.0, r: 1.0).
[label] condition[text] you use our product and service or view the content provided by us.
[text] you use our product and service or view the content provided by us.
(cid:51) [label] condition.
(cid:51) [label] condition.
[text] you use our product and service or view the content provided by us.
[label] purpose.other[text] their own purposes[label] purpose.advertising-marketing[text ] inform advertising related services provided to other clients[label] none[text] none.
(cid:51) [label] purpose.other.
[text] their own purposes.
(cid:51) [label] purpose.advertising-marketing.
[text] inform advertising related services provided to other clients.
[label] purpose.personalization-customization[text] provide more tailored services and user experiences[label] purpose.basic-service-feature[text] remembering your account identity[label] purpose.service-operation-and-security[text] analyzing your account ’s security[label] purpose.analytics-research[text] analyzing your usage of our product and service[label] purpose.advertising-marketing[text] advertisement optimization ( helping us to provide you with more targeted advertisementsinstead of general advertisements based on your information )[label] purpose.basic-service-feature[text] provide[label] purpose.other[text] purposes[label] purpose.analytics-research[text] remembering your account identity[label] purpose.analytics-research[text] analyzing your account ’s security.
ground truth.
roberta(p:0.17, r: 0.2).
(cid:51) [label] purpose.analytics-research.
[text] analyzing your usage of our product and service[label] purpose.advertising-marketing[text] advertisement optimization.
(cid:51) [label] purpose.personalization-customization.
(cid:55).
[text] provide more tailored services and user experiences[label] purpose.service-operation-and-security[text] remembering your account identity(cid:51) [label] purpose.service-operation-and-security[text] analyzing your account ’s security.
bart(p:0.43, r: 0.6).
(cid:51) [label] purpose.analytics-research.
[text] analyzing your usage of our product and service[label] purpose.advertising-marketing[text] advertisement optimization[label] purpose.advertising-marketing[text] provide you with more targeted advertisements instead of general advertisements[label] purpose.advertising-marketing[text] based on your information.
table 12: sample roberta and bart predictions of type-ii slots.
((cid:51)) and ((cid:55)) indicates correct and incorrectpredictions, respectively.
precision (p) and recall (r) score is reported for each example in the left column..(cid:55).
(cid:55).
(cid:55).
(cid:55).
(cid:55).
(cid:55).
(cid:55).
(cid:55).
4417