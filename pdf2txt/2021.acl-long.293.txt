textsettr: few-shot text styleextraction and tunable targeted restyling.
parker rileya∗, noah constantb, mandy guob,girish kumarc∗, david uthusb, zarana parekhb.
auniversity of rochester.
bgoogle research.
cstanford university.
abstract.
we present a novel approach to the problemof text style transfer.
unlike previous ap-proaches requiring style-labeled training data,our method makes use of readily-available un-labeled text by relying on the implicit connec-tion in style between adjacent sentences, anduses labeled data only at inference time.
weadapt t5 (raffel et al., 2020), a strong pre-trained text-to-text model, to extract a stylevector from text and use it to condition the de-coder to perform style transfer.
as our label-free training results in a style vector space en-coding many facets of style, we recast trans-fers as “targeted restyling” vector operationsthat adjust speciﬁc attributes of the input whilepreserving others.
we demonstrate that train-ing on unlabeled amazon reviews data resultsin a model that is competitive on sentimenttransfer, even compared to models trainedfully on labeled data.
furthermore, applyingour novel method to a diverse corpus of unla-beled web text results in a single model capa-ble of transferring along multiple dimensionsof style (dialect, emotiveness, formality, polite-ness, sentiment) despite no additional trainingand using only a handful of exemplars at infer-ence time..1.introduction.
there has been a recent surge of interest in textstyle transfer, with the aim of training models ableto modify speciﬁc attributes of input text (e.g., sen-timent or formality) while preserving the remainingcontent.
for example, a sentiment transfer modelmight transform the input “best book ever!” into“worst book ever!”, while a formality transfer modelmight change the same input into “this is the bestbook i have ever read.” in these contexts, we de-ﬁne “style” as the attributes intended to be changed,.
while “content” consists of the attributes intendedto be preserved.1.
work in this area falls into three categories.
su-pervised approaches like jhamtani et al.
(2017)transfer between pre-selected styles, and relyon parallel training data to learn the desired in-put/output correspondence.
this method is limitedby the availability of parallel corpora.
so-called“unsupervised” approaches like li et al.
(2018)and lample et al.
(2019) remove the need for par-allel data, but still require that all training exam-ples have style labels, and are limited to transferbetween a pre-speciﬁed set of styles.
few-shotapproaches like that of xu et al.
(2020) removethe need for any training labels, instead using asmall number of labeled examples during infer-ence.
while the most challenging, this offers thepotential for transferring between arbitrary styles atinference time and has signiﬁcant value, as curateddatasets are not available for many style attributes.
in this work, we explore the hypothesis that largepretrained text-to-text models like t5 (raffel et al.,2020) already contain a strong representation oftextual style, which can be extracted and used tocondition the decoder of a style transfer modelthrough a relatively lightweight ﬁne-tuning proce-dure.
to isolate style information in the absenceof labels, we rely on the observation that style isa “slow-moving” feature, which tends to be consis-tent over large spans of text.
speciﬁcally, given twoadjacent sentences from an unlabeled corpus, wetrain our model to extract a “style vector” from theﬁrst and use that vector to perform denoising andother reconstruction tasks on the second.
this tech-nique extends the approach of lample et al.
(2019)to the few-shot setting, and is loosely reminiscentof the work of akama et al.
(2018), who found.
∗ work done while at google research.
please di-rect correspondence to priley3@cs.rochester.edu,nconstant@google.com and xyguo@google.com..1krishna et al.
(2020) use a different deﬁnition of style,under which certain transfers such as sentiment would insteadbe examples of attribute transfer..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3786–3800august1–6,2021.©2021associationforcomputationallinguistics3786large context windows useful for encoding style in-formation in word embeddings.
our approach alsoallows us to reformulate the style transfer operationas a directional operation in style vector space us-ing the difference between target and source stylevectors; we call this “targeted restyling”.
whencombined with a novel “tunable inference” tech-nique for controlling token add/delete rates, thisgives our ﬁnal model: text style extraction andtunable targeted restyling (textsettr)..our main contributions are to: (1) present a new,ﬂexible approach to few-shot style transfer, (2) usesentence adjacency as a means for inducing textstyle representations, (3) reframe style transfer as“targeted restyling” directional operations in stylespace, (4) introduce “tunable inference” for ﬁner-grained control of transfers, (5) show the effective-ness of “noisy” back-translation training, and (6)illustrate few-shot generalization to a range of styleattributes including dialect, emotiveness, formality,politeness, and sentiment..2 method.
figure 1 illustrates our proposed textsettr ar-chitecture.
at a high level, our approach followslample et al.
(2019), who train a denoising auto-encoder conditioned on a ﬁxed-width style vector.
the key difference in our case is that the true styleis unknown at training time.
to overcome this, wejointly train a “style extractor” component to in-duce a useful style representation (that can aid inreconstruction) from text in the nearby context.
wedescribe this in more detail below..2.1 model architecture.
we conduct our experiments using a modiﬁed ver-sion of the text-to-text transfer transformer (t5)(raffel et al., 2020).
like t5, our model includes atransformer-based encoder and decoder.
as in t5pretraining, the input to the encoder is a corruptedversion of the target, resulting in a reconstructiontask.
our goal is to design a type of corruptionthat results in this training task resembling styletransfer, despite the lack of labeled training data..our core addition to t5 is the style extractor.
this component’s architecture is based on that ofthe encoder, and its input is an uncorrupted sen-tence in the same style as the target; relying onour assumption that style is a slow-moving fea-ture, we use the sentence preceding the target (the“context”) for this.
this encourages extracting a.style representation that is useful for repairing thecorrupted input.
we note that this can result in arepresentation that encodes slow-moving attributesin general, which may include some features thatdo not ﬁt an intuitive deﬁnition of textual style(such as topic)..the only architectural difference between the en-coder and style extractor is that we mean-pool thestyle extractor’s hidden state sequence into a singleﬁxed-width “style vector”; in our experiments, thedimensionality of this vector and the encoder hid-den states is 1024. to incorporate the style vectorinto the rest of the model, we simply add it to eachof the ﬁnal encoder hidden states..we initialize the weights of our model with thoseof a pretrained t5 model.
we initialize both thestyle extractor and encoder from the pretrained en-coder, but the weights are not tied during training..2.2 corruption strategies.
we experiment with combinations of three differ-ent reconstruction tasks, each contributing a lossterm.
all three share the same overall structure,where a sentence si in the dataset is corrupted bysome function f to produce ˜si = f (si).
the cross-entropy loss is calculated using the uncorruptedsentence si as the target, the corrupted sentence ˜sias the input, and the uncorrupted preceding sen-tence si−1 as the context.
the three choices of fare noise (n), back-translation (bt), and noisyback-translation (nbt), described below..noise (n) this function corrupts the input by(i) dropping, (ii) replacing, and/or (iii) shufﬂingtokens, in that order.
for each example we samplea separate noise probability p for each sub-typeof noise from a uniform distribution in the range20–60%; doing so should widen the model’s rangeof possible style transfers at test time..for drop noise, we drop each token in si withprobability p. for replace noise, let sik be the k-th token within si.
for each si, a random otherexample sj is chosen, and then each token sik isreplaced by sjk with probability p. if sj has fewerthan k tokens, then the replacement does not occur.
for shufﬂe noise, each token in si is chosen withprobability p, and then all chosen tokens are ran-domly shufﬂed to the position of another chosentoken, leaving non-chosen tokens in place..the use of drop and shufﬂe noise results in a lossterm similar to the denoising loss used by lampleet al.
(2019).
their motivation for this loss was.
3787figure 1: textsettr architecture for few-shot style transfer.
the encoder, decoder and style extractor (ex)are transformer stacks initialized from pretrained t5.
during training, the model reconstructs a corrupted input,conditioned on a ﬁxed-width “style vector” extracted from the preceding sentence.
at inference time, a newstyle vector is formed via “targeted restyling”: adding a directional delta to the extracted style of the input text.
stochastic tuning ranges provide extra conditioning for the decoder, and enable ﬁne-grained control of inference..to encourage language modeling.
as we ﬁne-tunean already-strong t5 language model in our ex-periments, our motivation is rather to introduce aconditional element to the language model, in theform of the extracted style vector input..back-translation (bt) this corruption func-tion, used by lample et al.
(2019), runs the currentversion of the model in inference mode to transfersi into a different style, giving the corrupted ˜si.
inprior work using labels, specifying a different tar-get style was straightforward.
in our case, becausewe do not have access to labels, we simply samplea random sentence sj to use as the context.
toincrease diversity of the generated examples, wedecode with sampling instead of greedy decoding.
because ˜si is produced by a strong languagemodel, bt should result in examples where boththe input and output are coherent sentences, match-ing our inference setting.
by contrast, noise cor-ruption does not resemble test-time inputs..noisy back-translation (nbt) this novel cor-ruption function is a composition of the previoustwo.
noise is ﬁrst applied to si as described above,and the result is used as the input (with randomly-sampled sj as the context) to the model in inferencemode to produce ˜si via sampling, as in bt..once the model has learned to undo randomnoise, nbt should produce training exampleswhere some of the tokens are preserved from siwhile others were generated by the model itselfunder the inﬂuence of the “incorrect” context sj.
this is similar to bt, but we hypothesize that itmay be better suited to style transfer.
bt was origi-.
nally used for machine translation (sennrich et al.,2016), a setting where most or all input tokens needto change.
in contrast, style transfer within a singlelanguage usually requires only changing a subset oftokens; the training examples resulting from nbtshould have this property.
we believe that this willencourage the model to identify which tokens inthe input do not match the target style indicated bysi−1 and change them, which is exactly what wewant a style transfer model to do..final loss the ﬁnal loss term used for trainingis the sum of the above loss terms, each calculatedfrom the same input si.
however, not every modelwe experiment with includes all three losses..2.3.inference procedure.
tunable add/delete rates in preliminary exper-iments, we observed a recurring problem that themodel would often change either far too little (fail-ing to achieve the target style), or far too much(failing to preserve the input content).
to addressthis problem, we introduce a “tunable inference”mechanism to constrain how much content shouldbe added and deleted at inference time..for every input/output pair during training, wecalculate the proportions of tokens that were addedand deleted.
the “add rate” is the proportion ofoutput tokens absent from the input, and the “deleterate” is the proportion of input tokens absent fromthe output.2 we provide these rates to the decoderas ranges covering but not necessarily centered.
2this calculation ignores word order.
as one example, ifa token appears three times in the input and ﬁve times in theoutput, two of the ﬁve occurrences are counted as “added”..3788style targetλ × (a − b) + inptuning rangesadd      delete40-70%  25-35%it doesn’t workinputencoderdecoderoutputit works greatexexstyle a exemplarsexexstyle b exemplarsexa great product.contexti really love itinputcorruptioncat really itencoderstyleextractordecodertargeti really love ittraininginferencetuning rangesadd      delete10-50%  10-50%on the true rates.3 this approach provides moreﬂexibility at inference time, so we can enforce tightor loose constraints on each rate..targeted restyling while previous work onstyle transfer has largely assumed a ﬁxed set ofdiscrete styles, we expect our model’s learned stylerepresentations to capture a rich summary of thesentence covering many attributes without specify-ing them beforehand.
for example, a given stylevector might encode that a sentence is informal,humorous, in british english, and so on..in this framework, transferring a single attribute(e.g., informal → formal) is not as simple as justproviding a vanilla “formal” style target, as thiswould ignore all the other attributes that deﬁnedthe original input.
rather, we must operate in stylespace to construct a new target style that is simulta-neously formal, humorous, british, and so on..concretely, at inference time, we assume accessto a small set of “exemplar” sentences (between 1and 100) for both the source value (e.g., informal)and target value (e.g., formal) of the attribute beingmodiﬁed.
we infer style vectors for each exemplarusing the style extractor, and take the mean of eachclass, giving vectors vsrc and vtrg.
assuming theexemplar pools are relatively diverse, this averag-ing should “wash out” most untargeted attributes.
to transfer an input sentence x, we apply a tar-geted restyling in the appropriate direction.
afterextracting the original style from the input itself,vx, we compute the target output style by movingin the direction of the delta between the source andtarget attributes values, as in (1), producing thestyle vector used for decoding.
in practice, we ﬁndthat the delta scale λ is an important hyperparame-ter to tune.
generally values in the range [1.0, 10.0]work well, with the best values depending on theattribute and the exemplars in question..vx + λ ×.
(cid:16).
vtrg − vsrc(cid:17).
(1).
3 experiments on sentiment transfer.
to evaluate our approach and better understandthe effects of our various design choices, we teston few-shot sentiment transfer, using the amazonreviews dataset of li et al.
(2018).
however, astheir training split doesn’t indicate which sentences.
3speciﬁcally, we sample each range width uniformly from[0,1], and uniformly sample the “alignment” of the true ratewithin the range.
the ﬁnal ranges are clipped to [0,1], and avector containing the upper and lower bound of each range isprepended to the encoder hidden state sequence..were adjacent in the original reviews, we make useof a different source of raw review text..training procedure our unlabeled trainingdata comes from the 233.1m amazon reviews pro-vided by ni et al.
(2019).
ignoring the star rat-ings completely, we extract adjacent lines frommulti-line reviews to use as the context and in-put for our training procedure, giving 23.6m ex-amples.
we also preprocess all text to match theformat of the li et al.
(2018) data, as detailed in ap-pendix a.4.
initializing our model from pretrainedt5 (t5.1.1.large), we ﬁne-tune on these examples,optimizing the joint reconstruction loss from sec-tion 2. our default textsettr conﬁguration isselected based on preliminary experiments (on de-velopment data) varying the set of reconstructiontasks and inference procedures.
the model usesan equally weighted combination of the noise (n)and noisy back-translation (nbt) tasks.
for bothtasks, we use drop and replace noise, but no shufﬂenoise.
we ﬁne-tune for 10k steps, with a batch sizeof 65,536 tokens, and a ﬁxed learning rate of 1e-3.
evaluation procedure following prior work,we use automatic metrics to assess attribute con-trol (sentiment) and content preservation on thedata from li et al.
(2018).
to estimate the sen-timent of the output, we ﬁne-tune a bert-largeclassiﬁer (devlin et al., 2019) on the train split,scoring 87.8% accuracy on the dev split.
for con-tent preservation, we follow sudhakar et al.
(2019)and xu et al.
(2020) and calculate self-bleu be-tween the output and input, using sacrebleu(post, 2018).4,5 following xu et al.
(2018), wereport “g-score” (the geometric mean of accuracyand content) as a summary of overall model quality.
to perform transfers, we follow the procedurefrom section 2.3. for our default setup, we sample100 positive and 100 negative exemplars from theli et al.
(2018) train split.
unless otherwise speci-ﬁed, we use greedy decoding, a delta scale of λ=8,and add/delete tuning ranges of 20–40%..core results figure 2 shows our core results.
our default textsettr conﬁguration (n+nbttraining, tuning ranges 20–40%) achieves 73.7%classiﬁer-judged accuracy at swapping sentiment,while still staying somewhat close to the original.
4version.
string:smooth.exp+tok.13a+version.1.4.13.
bleu+case.mixed+numrefs.1+.
5some prior work reports instead bleu scores betweenoutputs and human-generated transfers from li et al.
(2018);we found this to be highly correlated with self-bleu butreport it in appendix a.3 for completeness..3789model.
acc.
content g.textsettr (10–30%) 54.0textsettr (20–40%) 73.723.470.013.366.170.352.474.571.525.374.5.nnbtn + bt−replace noise+shufﬂe noisemanual exemplars1000 exemplars−tunable inferenceoverwrite stylesmall train set.
cp-gcp-b.
crossaligneddelete&retrieveb-gst.
51.136.3.
68.249.460.2.
55.834.784.427.898.742.134.144.237.239.455.833.4.
35.539.8.
2.956.954.2.
54.950.644.444.136.252.849.048.152.653.137.649.9.
42.638.0.
14.153.057.1.few-shot.
labeled.
figure 2: automatic evaluation metrics comparing our textsettr model, ablations, and previous work.
up-and-right is better.
we train for 10k steps and use add/delete:20–40% unless otherwise speciﬁed.
we recalculatemetrics for previous approaches, using our bert classiﬁer for accuracy, ensuring direct comparability..model.
accuracy content.
g.model.
sentiment.
preservation.
fluency.
textsettr (10–30%)textsettr (20–40%).
lample et al.
2019.
72.783.6.
82.6.
60.239.4.
54.8.
66.257.4.
67.3.textsettr (10–30%)textsettr (20–40%).
delete&retrieveb-gst.
2.02.5.
2.52.2.
3.52.6.
3.12.9.
2.94.0.
3.33.6.table 1: comparison with lample et al.
(2019) on thesetting that includes pos→pos and neg→neg transfers..table 2: human evaluation metrics..input text (self-bleu 34.7).
due to our tunableinference technique, we can also trade off accuracyfor content preservation by adjusting the add/deleterates, as seen in the points along the green line.
no-tably, textsettr outperforms the few-shot cp-gand cp-b models of xu et al.
(2020).
more remark-ably, textsettr outperforms several approachesthat rely on training labels: crossaligned (shenet al., 2017) and delete&retrieve (li et al., 2018).
however there is still a small gap between our few-shot approach and the best labeled model, b-gst(sudhakar et al., 2019)..in table 1, we compare with lample et al.
(2019)on the evaluation setting including pos→pos andneg→neg transfers.
this setting doesn’t match ourinference procedure, which assumes that the inputand output styles differ.
nevertheless, textsettrcomes close to the performance of lample et al.
(2019), despite not beneﬁting from training labels.
as automatic metrics can diverge from humanjudgment (sudhakar et al., 2019), we also conducthuman evaluations of the three strongest modelsfrom figure 2. we sample 200 examples per trans-fer direction from the li et al.
(2018) test set, andask three annotators to evaluate each input/output.
pair on three metrics: sentiment transfer (how wellthe model changed the sentiment), content preser-vation, and ﬂuency, on scales of 1–5.
the results intable 2 conﬁrm that textsettr achieves similarquality to models that beneﬁt from training labels.
further details are presented in appendix a.5..3.1 ablations.
modifying inference procedure to better under-stand the value of our proposed “targeted restyling”mechanism, we consider an alternative inferenceprocedure where we ignore the style of the inputand simply use the average target exemplar stylevtrg as the style vector.
we expect that since ourlearned style space covers multiple attributes, thiswill result in setting the target attribute (e.g.
senti-ment) while simultaneously overwriting all otherstyle attributes (e.g.
formality) using the averagestyle of the target exemplars.
this is borne out inour “overwrite style” ablation, which performs sig-niﬁcantly worse than our baseline: accuracy dropsfrom 54.0% to 25.3% with no gain in self-bleu.
to assess the value of tunable add/delete rates,we also train a model (−tunable) without this fea-ture.
while the automatic metrics are slightly abovethe textsettr line, we observe several advan-.
3790020406080100content preservation (self-bleu)020406080100sentiment transfer accuracy010%020%1030%2040%3050%4060%5070%nn(50k)nbtn+btn+bt(50k)tunable+shufflereplacemanualoverwrite1000-exemplarssmall-traincp-gcp-bcrossaligneddelete&retrieveb-gsttextsettrtextsettr ablationsother label-free modelsmodels trained with labelstages to the tunable model.
for one, we observeit signiﬁcantly reduces the variance in self-bleuacross different inputs.
for example, focusing onthe case of overly high self-bleu, we ﬁnd thatwithout tunable inference, 14.6% of dev eval out-puts are identical to their inputs, whereas with tun-able inference, this goes to 0.9%.
additionally,through qualitative analysis in section 4, we ﬁndthat tunable inference allows more ﬂexibility forcontrolling different types of transfer..adjusting data sizes while our unlabeled train-ing data set consists of 23.6m examples, our modelonly sees 5.1m of these over its 10k steps of train-ing.
yet this is still nearly 10× more data thanthe 0.6m examples in the li et al.
(2018) trainingset used by previous approaches.
for a more di-rect comparison, we experiment with a “small trainset”, sampling 0.6m examples from our trainingset.
remarkably, the results in figure 2 are nearlyidentical to our baseline, supporting our hypothesisthat a fairly lightweight adaptation is sufﬁcient toallow t5 to extract and transfer textual style..to test the limits of our model’s generalization,we reduce the set of exemplars to four manuallyselected examples of each class.
in this setting,we also ﬁnd reducing delta scale to λ=4 is bene-ﬁcial.
the results, shown as “manual exemplars”in figure 2, are still competitive, indicating thatour approach generalizes well to this very-few-shotinference setting.
in the other direction, we ﬁndthat increasing the number of sampled exemplarsfrom 100 to 1000 only provides small additionalgains..modifying training task lample et al.
(2019)showed promising results by combining noise (n)with back-translation (bt).
however we ﬁnd thiscombination unstable.6 when training for 10ksteps, our n and n+bt models nearly always copytheir input.
training for 50k steps recovers reason-able performance, but the metrics still fall belowthe textsettr line, using our novel nbt task.
wealso experiment with using nbt in isolation, butthis again underperforms our baseline.
we expectthat the denoising task helps to ensure the nbt in-puts (themselves the outputs of denoising) consistof realistic well-formed text.
finally, while lample.
6for all experiments in the paper, we use 0.0 for theadd/delete rates during the forward pass of back-translation.
however we later found that using random add/delete rates inback-translation can improve performance in the n+bt set-ting.
on sentiment transfer, this improved our n+bt ablationto self-bleu 42.4, accuracy 71.4%, g-score 55.0..et al.
(2019) use drop and shufﬂe noise, we ﬁndthat only drop and replace are valuable..3.2 embedding visualization.
to demonstrate that our learned style extractor en-codes multiple aspects of textual style, we computestyle vectors for 12,000 lines of text from threereview categories (fashion, software, pantry) fromthe ni et al.
(2019) amazon data.
within eachcategory, we sample 2,000 positives (4 or 5 star)and 2,000 negatives (1 or 2 star), ﬁltering exampleswhere our bert classiﬁer disagrees with the label.
figure 3 (bottom) plots a 2d umap dimensional-ity reduction (mcinnes et al., 2018) of the vectors,and shows clear separations among sentiments andproduct categories.
the top row runs umap withthe same settings, but over style vectors from ourmodel before training, where the style extractor isinitialized from pretrained t5.
the contrast is aclear indication that our training procedure is help-ing to learn a representation space where sentimentand topic values are well separated..to conﬁrm that the observed separation isn’tan artifact of dimensionality reduction, we com-pute the average distance between style vectors(a) within a class, and (b) across classes.
wemeasure “separation” as the relative increase inmean distance between these two conditions.
forproduct category, we ﬁnd textsettr training im-proves separation from 1.7% to 8.1%.
for sen-timent, textsettr training improves separationfrom 0.9% to 4.7%..4 one model for all styles.
an advantage of few-shot style transfer is that, intheory, a single model can perform transfer alongany “dimension” of style given only a few exem-plars, without the need for additional training.
inthis section, we investigate the degree to which ourapproach achieves this goal in practice.
for this pur-pose, we train a single general-purpose textsettrmodel, with the same conﬁguration as our modelfrom section 3, except ﬁne-tuned for 200k steps onenglish common crawl data (the same “c4” datathat t5 pretrained on) instead of amazon reviews.
qualitative evaluation given that our architec-ture limits the style representation to 1024 dimen-sions, one may ask how the unsupervised modelwill make use of this capacity, and which styleattributes will be encoded in the learned space.
en-couragingly, we ﬁnd that our model trained on un-.
3791beforetextsettr train-ing (pretrained t5initialization).
after textsettrtraining.
figure 3: 2d umap embeddings of the style vectors extracted by our textsettr model before and after training,for text inputs from amazon reviews covering three product categories and two sentiment labels.
within eachrow, the same embeddings are labeled with product category (left) and sentiment (right).
we sub-sample to 3,000points after dimensionality reduction.
note, we don’t expect perfect separation, as inputs may be underspeciﬁedfor category (“i love this product”) or for sentiment (“i bought this last month”).
we also don’t expect to seecrisp linear separation within each attribute since we aim for the learned embedding space to encode many styleattributes simultaneously..reserved ⇒ emotivei liked the movie.
⇒ i cannot even describe how amazing this movie was!!
i was impressed with the results.
⇒ i was absolutely blown away with the results!!.
emotive ⇒ reservedi loved every minute of the movie!
⇒ i liked the movie.
i was shocked by the amazing results!
⇒ i was surprised by the results..american ⇒ britishthe elevator in my apartment isn’t working.
⇒ the lift in my ﬂat isn’t working.
the senators will return to washington next week.
⇒ the mps will return to westminster next week..polite ⇒ rudeare you positive you’ve understood my point?
⇒ you’ve never understood my point!.
could you ask before using my phone?
⇒ i ask you to stop using my phone!.
formal ⇒ informali hereby commit to never purchase anything from this.
institution in the future..⇒ i gonna never buy anything from this place again..i couldn’t ﬁgure out what the author was trying to say.
⇒ i dont know what ur trying to say..positive ⇒ negativei was pretty impressed with the results.
⇒ i was pretty disappointed with the results.
i will deﬁnitely buy this brand again.
⇒ i will deﬁnitely not buy this brand again..british ⇒ americanthe lift in my ﬂat isn’t working.
⇒ the elevator in my apartment isn’t working.
mps will return to westminster next week.
⇒ representatives will return to washington next week..rude ⇒ politewhat the hell is wrong with your attitude?
⇒ perhaps the question is more about your attitude.
i could care less, go ﬁnd somebody else to do this crap.
⇒ i could be wrong, but i would try to ﬁnd somebody.
else to do this..informal ⇒ formal.
best book ever!!
⇒ the book is highly recommended..couldnt ﬁgure out what author tryna say⇒ the reader couldn’t ﬁgure out what the author was.
trying to say..negative ⇒ positivei was pretty disappointed with the results.
⇒ i was pretty impressed with the results.
i deﬁnitely won’t buy this brand again.
⇒ i deﬁnitely won’t hesitate to buy this brand again..table 3: examples of transferring along ﬁve different axes of style.
the same model is used across all examples,with no additional training.
words deleted from the input are red, and words added in the output are blue.
withineach category, a ﬁxed tiny set of exemplars is chosen, and ﬁxed delta scale and tuning rates are used.
the exemplarsand settings are provided in appendix a.2..3792labeled common crawl data is capable of trans-ferring along many independent axes of style.
ta-ble 3 shows selected successful examples of ourcommon crawl model transferring emotiveness,dialect, politeness, formality and sentiment.
thesame model is used in each case, with no additionaltraining.
at inference time, a tiny set of exemplars(1–5 examples of each class) is the only labeleddata used to compute the style vector delta; theseexemplars are presented in appendix a.2..across each type of transfer, we see evidence ofgeneralization beyond the speciﬁcs of the chosenexemplars.
in making text more emotive, the modeluses amazing and blown away, despite these termsnot occurring in the exemplars.
in making textmore polite, the model inserts novel hedges like per-haps and i could be wrong.
in transferring betweenamerican and british styles, the model generalizesto unseen vocabulary items (elevator ↔ lift) anddraws sound analogies (senators ↔ mps).
we donote though that the latter case illustrates that themodel is willing to change the semantic content ofthe input in cases where it would otherwise be out-of-place in the target style.
future work includesinvestigating ways to control this in settings wheresuch behavior is not desired..quantitative evaluation to assess the qual-ity of our general-purpose textsettr model, webenchmark the same model on three distinct trans-fer tasks in table 4.7 the sentiment transfer taskfollows the evaluation procedure from section 3.while our generic model underperforms our modeltrained on amazon reviews, it still outperformsother few-shot methods.
for author transfer, weuse the shakespeare-to-modern task of jhamtaniet al.
(2017).
here, textsettr outperforms theprevious best model of he et al.
(2020) that lever-aged 36,790 labeled examples during training.
forpersonality transfer, we use the task of li et al.
(2020), which requires transferring between threepersonalities: angry, happy, malicious.
we com-pare8 textsettr, which sees no labels in trainingand only 100 of each class in inference, with cara(li et al., 2020), which trained on 2,604 labels..7for each task, we set our tuning ranges to 20–40% andcompute target styles using 100 exemplars of each class takenfrom the train set.
we use λ values of sentiment:8, author:16,personality:8. to measure accuracy, we ﬁne-tune bert-largeclassiﬁers over the training data, reaching validation accura-cies of sentiment:87.8%, author:89.7%, personality:81.9%..8note, as li et al.
(2020) use a different classiﬁer to assess.
accuracy, those numbers may not be directly comparable..task.
model.
acc.
content.
g.sentiment.
author.
personality.
cp-gcp-btextsettr.
crossaligneddelete&retrieveb-gst.
unmtbt+nllhe et al.
2020textsettr.
caracaraabctrl-genarae−textsettr.
51.136.344.9.
68.249.460.2.
68.559.368.581.7.
91.666.267.688.049.3.
35.539.854.4.
2.956.954.2.
7.812.412.513.8.
21.629.722.920.346.0.
42.638.049.4.
14.153.057.1.
23.127.129.233.5.
44.544.339.342.347.6.table 4: automated metrics comparing our general-purpose textsettr model with recent work on threetransfer tasks.
to enable direct comparison, “content”refers to reference-bleu for author transfer, and self-bleu elsewhere.
apart from cp-g/cp-b, all competi-tors are trained for only one type of transfer using la-beled data.
personality transfer results are from li et al.
(2020), while all others are recalculated from scratch..4.1 dialect-sensitive completion.
in addition to performing style and attribute trans-fer, we ﬁnd that our system can also be used as astyle-aware language model capable of completingprompts in a speciﬁed style.
examples of comple-tions in american and british english are givenin table 5. in each case, the input is of the form“my favorite x: ”.
despite the fact that textsettris not trained speciﬁcally for completions, we canuse the add/delete rates to encourage the model toinsert a few additional tokens, while leaving theoriginal prompt largely unchanged.9.
the completions demonstrate knowledge ofstereotypical american and british culture.
it isremarkable that the model is able to generalizeto “deeper” cultural differences such as music anddrink preferences, given only the shallow vocabu-lary differences (e.g., neighbor vs. neighbour) pre-sented in the limited set of exemplars in table 9..it is also worth highlighting that, thanks to our di-rectional transfer procedure, these completions arenot merely “typical american” or “typical british”such as we would expect from a conditional lan-guage model trained on each sub-domain of text.
rather, since our inference procedure pushes thestyle away from one domain and towards the other,the resulting completions are distinctive represen-tations of each dialect.
as one example, we expect.
9we note that in transferring american to british, themodel prefers to change the prompt from favorite to favourite..3793american ⇒ britishmy favourite food: ﬁsh and chips.
my favourite hot drink: a mug of tea.
my favourite dessert: a scone!
my favourite city: cardiff.
my favourite band: the beatles.
my favourite sports league: the english premier league.
my favourite newspaper: the daily telegraph.
my favourite museum: the british museum..british ⇒ americanmy favorite food: quinoa.
my favorite hot drink: starbucks coffee.
my favorite dessert: a brownie.
my favorite city: san diego.
my favorite band: the black keys.
my favorite sports league: the nfl.
my favorite newspaper: the washington post.
my favorite museum: the national air and space museum..table 5: examples of dialect-sensitive completion (λ=8, add:40–70%, delete:0%).
in each case, the input textconsists of an unﬁnished phrase, for example: “my favorite food: ”.
the three exemplars used for each dialect arethe same as those used for the transfers in table 3, as listed in table 9..“quinoa” would not only be a common americanfavorite, but also an uncommon british favorite..additional examples of using our model fortasks other than pure style transfer are presented inappendix a.1..5 related work.
as mentioned at the outset, recent work on textstyle transfer falls into three classes: supervised,“unsupervised”, and few-shot.
supervised styletransfer has seen limited research due to the dif-ﬁculty of obtaining parallel data.
examples includejhamtani et al.
(2017) and carlson et al.
(2018)..unsupervised approaches the bulk of re-search has focused on “unsupervised” approaches,which rely on labeled but non-parallel data.
typi-cally, labels are assumed to be available for bothsource and target styles (shen et al.
2017, li et al.
2018, niu et al.
2018, and many others).
zhao et al.
(2018) explore the case where only the target styleis labeled.
the use of labels at training time can aidmodeling, but limits the applicability of these meth-ods, as labeled datasets are not readily available formany attributes of interest..our work differs from the above by removingthe need for training labels, and offering a singlemodel that can target an unrestricted set of style at-tributes.
despite these differences, our work sharessome similarities with past work.
for example, ourencoder-decoder architecture and corruption meth-ods are similar to lample et al.
(2019), and weleverage a strong pretrained language model, as insudhakar et al.
(2019) and wu et al.
(2019)..few-shot approaches a few-shot approachhas recently been explored by xu et al.
(2020).
the authors train a variational auto-encoder onunlabeled text, where a “manipulable” portion ofthe latent representation is constrained to fall ona k-dimensional simplex.
to perform transfer,.
they identify empirically the basis vector that moststrongly corresponds to the target attribute, and ma-nipulate its magnitude.
compared to our approach,a key difference is that the number of latent factorsmust be chosen ahead of time, which limits thenumber of attributes that may be controlled.
ad-ditionally, there is no guarantee that a single basisof the learned simplex will correspond to a targetattribute such as dialect or politeness..controlled generation a separate strand of re-search explores “controlled generation” methodsfor supplementing generative language models toallow control of speciﬁc attributes of the outputtext.
as with style transfer, this can be achieved ei-ther through labeled training examples, as in ctrl(keskar et al., 2019) and pplm (dathathri et al.,2020), or a few-shot approach, as in cocon (chanet al., 2020).
these models differ from style trans-fer models in that they aim to generate plausiblecontinuations following a prompt, as opposed totransferring attributes of a fully-formed input whilepreserving as much content as possible.
it is notclear if controlled generation models could be usedto perform style transfer, and they have not to ourknowledge been evaluated in this context..6 conclusion.
we have presented a unique approach to few-shottext style transfer that is competitive with systemstrained with labels (an easier setting), while allow-ing control of how much of the input is changed.
we demonstrate that this approach can produce asingle system capable of transferring many differ-ent styles while requiring only a handful of exem-plars at inference time..acknowledgmentswe thank llion jones, rami al-rfou, and danielgildea for helpful discussion and comments on anearlier draft..3794references.
reina akama, kento watanabe, sho yokoi, sosukekobayashi, and kentaro inui.
2018. unsupervisedlearning of style-sensitive word vectors.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 572–578, melbourne, australia.
asso-ciation for computational linguistics..keith carlson, allen riddell, and daniel rockmore.
2018. evaluating prose style transfer with the bible.
royal society open science, 5(10):171920..alvin chan, yew-soon ong, bill pung, aston zhang,cocon: a self-supervisedand jie fu.
2020.approach for controlled text generation.
corr,abs/2006.03535..sumanth dathathri, andrea madotto, janice lan, janehung, eric frank, piero molino, jason yosinski, androsanne liu.
2020. plug and play language mod-els: a simple approach to controlled text generation.
in international conference on learning represen-tations..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..junxian he, xinyi wang, graham neubig, and taylorberg-kirkpatrick.
2020. a probabilistic formulationof unsupervised text style transfer.
in internationalconference on learning representations..harsh jhamtani, varun gangal, eduard hovy, and ericnyberg.
2017. shakespearizing modern languageusing copy-enriched sequence to sequence models.
in proceedings of the workshop on stylistic varia-tion, pages 10–19, copenhagen, denmark.
associa-tion for computational linguistics..nitish shirish keskar, bryan mccann, lav r. varsh-ney, caiming xiong, and richard socher.
2019.ctrl: a conditional transformer language modelfor controllable generation.
corr, abs/1909.05858..kalpesh krishna, john wieting, and mohit iyyer.
2020.reformulating unsupervised style transfer as para-phrase generation.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 737–762, online.
asso-ciation for computational linguistics..guillaume lample, sandeep subramanian, eric smith,ludovic denoyer, marc’aurelio ranzato, and y-lan boureau.
2019. multiple-attribute text rewrit-ing.
in international conference on learning rep-resentations..juncen li, robin jia, he he, and percy liang.
2018.delete, retrieve, generate: a simple approach to sen-timent and style transfer.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 1865–1874, new orleans, louisiana.
associ-ation for computational linguistics..yuan li, chunyuan li, yizhe zhang, xiujun li, guo-qing zheng, lawrence carin, and jianfeng gao.
2020. complementary auxiliary classiﬁers for label-in proceedings ofconditionalthe aaai conference on artiﬁcial intelligence, vol-ume 34, pages 8303–8310..text generation..leland mcinnes, john healy, nathaniel saul, andlukas großberger.
2018. umap: uniform mani-fold approximation and projection.
journal of opensource software, 3(29):861..jianmo ni, jiacheng li, and julian mcauley.
2019.justifying recommendations using distantly-labeledin proceedingsreviews and ﬁne-grained aspects.
of the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 188–197, hongkong, china.
association for computational lin-guistics..xing niu, sudha rao, and marine carpuat.
2018.multi-task neural models for translating betweenstyles within and across languages.
in proceedingsof the 27th international conference on computa-tional linguistics, pages 1008–1021, santa fe, newmexico, usa.
association for computational lin-guistics..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, belgium, brussels.
association for computa-tional linguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96, berlin, germany.
association for computa-tional linguistics..tianxiao shen, tao lei, regina barzilay, and tommijaakkola.
2017. style transfer from non-parallel textby cross-alignment.
in i. guyon, u. v. luxburg,s. bengio, h. wallach, r. fergus, s. vishwanathan,.
3795and r. garnett, editors, advances in neural informa-tion processing systems 30, pages 6830–6841.
cur-ran associates, inc..akhilesh sudhakar, bhargav upadhyay, and arjun ma-heswaran.
2019.
“transforming” delete, retrieve,generate approach for controlled text style transfer.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3269–3279, hong kong, china.
association for computa-tional linguistics..xing wu, tao zhang, liangjun zang, jizhong han,and songlin hu.
2019. mask and inﬁll: apply-ing masked language model for sentiment transfer.
in proceedings of the twenty-eighth internationaljoint conference on artiﬁcial intelligence, ijcai-19, pages 5271–5277.
international joint confer-ences on artiﬁcial intelligence organization..jingjing xu, xu sun, qi zeng, xiaodong zhang, xu-ancheng ren, houfeng wang, and wenjie li.
2018.unpaired sentiment-to-sentiment translation: a cy-in proceed-cled reinforcement learning approach.
ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 979–988, melbourne, australia.
asso-ciation for computational linguistics..peng xu, jackie chi kit cheung, and yanshuai cao.
2020. on variational learning of controllable rep-in pro-resentations for text without supervision.
ceedings of the 37th international conference onmachine learning, volume 119 of proceedings ofmachine learning research, pages 10534–10543.
pmlr..yanpeng zhao, wei bi, deng cai, xiaojiang liu,kewei tu, and shuming shi.
2018.languagestyle transfer from sentences with arbitrary unknownstyles.
corr, abs/1808.04071..3796a appendix.
a.1 beyond style transfer.
in this section, we provide additional examplesillustrating the abilities of our textsettr modeltrained on common crawl data, beyond typicalstyle transfer..examples of shortening are given in table 6,with inputs taken from the ﬁrst ﬁve sentences ofthe wikipedia article “artiﬁcial neural network”.
as shortening may require minor rephrases, weset our tuning ranges to add:0–5%, delete:40–90%.
since our intention is to leave the style unchanged(apart from length), we extract the target style di-rectly from the input text, with no delta added.
themodel is largely successful at identifying and re-moving “superﬂuous” content, and ﬁnding ways ofrephrasing to shorten while preserving meaning..examples of random augmentations are givenin each case, we transfer the inputin table 7.sentence “what’ll the weather be tomorrow?” toa slightly different style.
speciﬁcally, for eachtransfer, we extract this sentence’s style vector andapply a small amount of noise, with each compo-nent of the noise vector sampled from a gaussiann (0, 0.08).
note that apart from the noise in thestyle vector, the transfer process is deterministic,as we use greedy decoding..the cells of table 7 apply different tuningranges, conditioning the model to change a littleor a lot.
within each cell, we repeatedly samplethe noised style, and present the ﬁrst ﬁve uniqueoutputs.
the results indicate that many randomchanges in style are largely meaning preserving,especially when a small change is requested.
withlarger add/delete rates, the outputs are still closelyrelated in meaning, despite low lexical overlap..a.2 settings used for qualitative analysis.
(e.g.,.
types.
the transfer.
for each offor-mal ↔ informal) in table 3, we specify the in-tended target styles through a tiny set of exemplars.
these exemplars are provided in tables 8–12.
ad-ditionally, for each transfer type, we select a deltascale λ and add/delete rates.
these settings areselected through initial experiments, and are heldﬁxed across all examples of transfer shown..a.3 human reference bleu.
li et al.
(2018) provide human reference transfersfor their amazon test data, and report bleu scoresof model outputs against these targets.
in principle,.
we believe this metric is less informative than self-bleu, as style transfer is a relatively open-endedtask, and successful transfers may differ signiﬁ-cantly from the single human reference.
however,for completeness, we report “reference bleu” ofour models and those of prior work in figure 4.we observe bleu and self-bleu are highly cor-related, and the “accuracy vs. bleu” plot conveysthe same relationships we saw in figure 2. asbefore, all bleu scores are calculated using sacre-bleu (post, 2018)..a.4 amazon reviews preprocessing.
we use the code in figure 5 to process raw ama-zon reviews from the ni et al.
(2019) dataset andextract pairs of adjacent lines, preprocessed to havea similar format to li et al.
(2018) dataset.
wesplit reviews on newlines, and clip lines to 100characters, always ending with a period.
this givesresults similar to li et al.
(2018), where one linemay contain multiple sentences, and may consistsof a “half-sentence” ending with “e.g.” or a similarnon-sentence-ﬁnal period.
additionally, we applyvarious tokenization and normalization operationsto roughly match the observed li et al.
(2018) text..a.5 human evaluation setup.
for the human evaluations of our models, we em-ployed 3 in-house annotators.
the annotators werepaid hourly wages that are competitive for their lo-cale and have standard rights as contractors.
theyspoke native english..for the evaluation task, the annotators wereshown both the original and transformed piecesof text.
they were then asked to evaluate for threemetrics: ﬂuency, meaning preservation, and senti-ment change..for ﬂuency, they were asked, “for the new text,how do you rate the ﬂuency, i.e., the quality andreadability of the text, with 1 being not ﬂuent at alland 5 being very ﬂuent.” for meaning preservation,they were asked, “comparing the new text againstthe original text, and ignoring the change of style,how well does the new text preserve as much of theoriginal meaning, with 1 being all meaning is lostand 5 being preserving as much as possible giventhe sentiment change?” and for sentiment change,they were asked, “comparing the new text againstthe original text, how well did the sentiment of thenew text become more positive, with 1 being notmore positive and 5 being a lot more positive?”.
3797artiﬁcial neural networks (ann) or connectionist systems are computing systems that are inspired by, but not identical to, biological neural.
networks that constitute animal brains..⇒ artiﬁcial neural networks (anns) are computing systems that are inspired by the biological neural networks that constitute animal brains..such systems “learn” to perform tasks by considering examples, generally without being programmed with task-speciﬁc rules.
⇒ such systems learn to perform tasks by considering examples, generally without explicit rules..for example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been.
manually labeled as “cat” or “no cat” and using the results to identify cats in other images..⇒ for example, image recognition systems might learn to identify images that contain cats by analyzing images that have been manually.
classiﬁed as “cat” or “no cat”..they do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces.
⇒ they do not know that cats have fur, tails, whiskers and cat-like faces..instead, they automatically generate identifying characteristics from the examples that they process.
⇒ instead, they automatically generate identifying characteristics..table 6: examples of shortening (add:0–5%, delete:40-90%), using the ﬁrst ﬁve sentences from the wikipediaarticle “artiﬁcial neural network”.
for each sentence, the target style is extracted directly from the input text, andno delta is added..add/delete: 10–30%what’ll the weather be like?
what’ll the weather be like tomorrow?
what’s the weather like tomorrow?
what’ll the weather be tomorrow?
what’s the weather supposed to be tomorrow?.
add/delete: 50–70%will the weather be perfect tomorrow?
what’s the weather for tomorrow?
what’s the weather like on the course?
hopefully the weather will be better tomorrow.
what’s the weather like for the next day?.
add/delete: 30–50%what’s the weather like?
what will the weather be like tomorrow?
will the weather be better tomorrow?
what’s the weather forecast for tomorrow?
how will the weather be tomorrow?.
add/delete: 70–90%how do you know what the weather will be like?
is it supposed to be cold tomorrow?
what will the weather be like in the south?
i’m not a fan of the weather.
what is the temperature and what is the humidity..table 7: random augmentations of input text “what’ll the weather be tomorrow?”, using random style vectordeltas with components sampled from n (0, 0.08)..reserved exemplars.
emotive exemplars.
1. that is a very pretty painting..2. i’m excited to see the show..1. omg, that’s such a beautiful painting!.
2. i’m sooo excited to see the show, it’s going to be stellar!!.
3. i’m surprised they rescheduled the meeting..3. i absolutely can not believe that they rescheduled the meeting!.
4. this specimen is an example of the baroque style..4. this wonderful specimen is a truly spectacular example of the.
5. after the performance, we ate a meal..baroque style..5. after the superb performance, we ate a delicious meal..table 8: emotiveness transfer exemplars.
transfer settings: λ=9, add/delete rates: 0–100%..american exemplars.
1. it cost ten bucks..2. my neighbor apologized..british exemplars.
1. it cost ten quid..2. my neighbour apologised..3. i’m heading out to the bar with some friends..3. i’m heading out to the pub with some mates..table 9: dialect transfer exemplars.
transfer settings: λ=8, add/delete rates: 10–30%..3798polite exemplars.
rude exemplars.
1. no thank you, i’d prefer not to..1. hell no, you can’t make me do that..2. this game could have been better designed..2. this game is such a piece of garbage!.
3. do you know why they might have delayed the launch?.
3. why in god’s name would they delay the damn launch?.
4. sorry, i wasn’t certain if you were joking..4. are you frigging kidding me?.
table 10: politeness transfer exemplars.
transfer settings: λ=5, add/delete rates: 20–50%..formal exemplars.
informal exemplars.
1. this was a remarkably thought-provoking read..1. reading this rly makes u think.
2. it is certainly amongst my favorites..2. its def one of my favs.
3. we humbly request your presence at our gala on the 12th..3. come swing by our bbq next week if ya can make it.
table 11: formality transfer exemplars.
transfer settings: λ=4, add/delete rates: 40–80%..positive exemplars.
1. five stars, i love it..negative exemplars.
1. zero stars, i hate it..table 12: sentiment transfer exemplars.
transfer settings: λ=3, add/delete rates: 0–100%..model.
bleu self-bleu.
crossaligneddelete&retrieveb-gstcp-gcp-btextsettr (0–20%)textsettr (10–30%)textsettr (20–40%)textsettr (30–50%)textsettr (40–60%)textsettr (50–70%).
2.029.729.017.019.439.030.720.010.65.52.2.
2.956.954.235.539.873.355.834.718.49.13.6.figure 4: bleu scores between model outputs and human references provided by li et al.
(2018), along withself-bleu for comparison.
the ﬁrst group of models in the table had access to labels at training time, while thesecond group did not.
textsettr (x–y%) refers to our model with add/delete rate ranges set to x–y%..37990510152025303540reference bleu020406080100sentiment transfer accuracy020%1030%2040%3050%4060%5070%cp-gcp-bcrossaligneddelete&retrieveb-gsttextsettrother label-free modelsmodels trained with labelsimport refrom html.parser import htmlparser.
html_parser = htmlparser().
def preprocess(line):.
"""simulate li et al.
preprocessing of one review line."""
# lowercase.
line = line.lower()# replace apostrophes, parens and quotes with spaces.
line = re.sub("[’()\"]", " ", line)# replace dollar values ==> $line = re.sub("\$[\d.
]*", "$", line)# replace percent values ==> %line = re.sub("[\d.
]*%", "%", line)# replace single digits ==> num_numline = re.sub(" \d[ ,]", " num_num ", line)# replace multi-digits and codes ==> num_extendline = re.sub(" \d[ˆ ]*", " num_extend", line)# remove remaining numbers, including decimals.
line = re.sub("\d[\d.
]*", "", line)# add spaces around certain punctuation marks.
line = re.sub("([.,?!
:])", r" \1 ", line)# remove double spaces after periods before words.
return re.sub(r"\..([a-z])", r".
\1", line).
def acceptable_line(line):.
"""check if text looks like an acceptable line from li et al."""
if not line or len(line) < 30 or len(line) >= 100:.
# avoid lines with any char absent from li et al.
train.
if re.search(’[ˆ !$%+,.:;>?
@\ˆ_‘a-z{|}]’, line):.
return false.
return false.
return true.
def clip_to_last_period(line):.
return line[:len(line) - line[::-1].index(’.’)].
def adjacent_lines(review):.
"""extract a list of adjacent line pairs from review text."""
review = html_parser.unescape(review)review = review.replace(’\\"’, ’"’)# simulate li et al.
splitting and filtering.
if ’\n’ not in review:.
return.
lines = review.split(’\n’)lines = [preprocess(clip_to_last_period(l[:100])).
for l in lines if l and "."
in l[:100]].
lines = [preprocess(l) for l in lines]lines = [l for l in lines if acceptable_line(l)]if len(lines) < 2:.
return.
return list(zip(lines[:-1], lines[1:])).
figure 5: python code to extract adjacent lines of text from raw amazon reviews, producing outputs in a similarstyle to li et al.
(2018)..3800