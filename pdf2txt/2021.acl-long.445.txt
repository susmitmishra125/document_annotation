importance-based neuron allocationfor multilingual neural machine translation.
wanying xie1,2,3 yang feng1,2∗ shuhao gu1,2 dong yu31 key laboratory of intelligent information processinginstitute of computing technology, chinese academy of sciences (ict/cas)2 university of chinese academy of sciences, beijing, china3 beijing language and culture university, chinaxiewanying07@gmail.com, yudong@blcu.edu.cn{fengyang, gushuhao19b}@ ict.ac.cn.
abstract.
multilingual neural machine translation with asingle model has drawn much attention due toits capability to deal with multiple languages.
however, the current multilingual translationparadigm often makes the model tend to pre-serve the general knowledge, but ignore thelanguage-speciﬁc knowledge.
some previousworks try to solve this problem by adding var-ious kinds of language-speciﬁc modules to themodel, but they suffer from the parameter ex-plosion problem and require specialized man-ual design.
to solve these problems, we pro-pose to divide the model neurons into generaland language-speciﬁc parts based on their im-portance across languages.
the general part isresponsible for preserving the general knowl-edge and participating in the translation ofall the languages, while the language-speciﬁcpart is responsible for preserving the language-speciﬁc knowledge and participating in thetranslation of some speciﬁc languages.
ex-perimental results on several language pairs,covering iwslt and europarl corpus datasets,demonstrate the effectiveness and universalityof the proposed method..1.introduction.
neural machine translation(nmt) (kalchbrennerand blunsom, 2013; sutskever et al., 2014; bah-danau et al., 2015; gehring et al., 2017; vaswaniet al., 2017) has shown its superiority and drawnmuch attention in recent years.
although thenmt model can achieve promising results for high-resource language pairs, it is unaffordable to trainseparate models for all the language pairs sincethere are thousands of languages in the world (tanet al., 2019; aharoni et al., 2019; arivazhagan et al.,2019).
a typical solution to reduce the model size.
∗corresponding author: yang feng.
our code can be got at https://github.com/ictnlp/na-.
mnmt.
and the training cost is to handle multiple languagesin a single multilingual neural machine translation(mnmt) model (ha et al., 2016; firat et al., 2016;johnson et al., 2017; gu et al., 2018).
the standardparadigm of mnmt proposed by johnson et al.
(2017) contains a language-shared encoder and de-coder with a special language indicator in the inputsentence to determine the target language..because different languages share all of themodel parameters in the standard mnmt model,the model tends to converge to a region where thereare low errors for all the languages.
therefore,the mnmt model trained on the combined datagenerally captures the general knowledge, but ig-nores the language-speciﬁc knowledge, renderingitself sub-optimal for the translation of a speciﬁclanguage (sachan and neubig, 2018; blackwoodet al., 2018; wang et al., 2020b).
to retain thelanguage-speciﬁc knowledge, some researches turnto augment the nmt model with language-speciﬁcmodules, e.g., the language-speciﬁc attention mod-ule (blackwood et al., 2018), decoupled multi-lingual encoders and/or decoders (v´azquez et al.,2019; escolano et al., 2020) and the lightweightlanguage adapters (bapna and firat, 2019).
how-ever, these methods suffer from the parameter incre-ment problem, because the number of parametersincreases linearly with the number of languages.
besides, the structure, size, and location of themodule have a large inﬂuence on the ﬁnal perfor-mance, which requires specialized manual design.
as a result, these problems often prevent the appli-cation of these methods in some scenarios..based on the above, we aim to propose a methodthat can retain the general and language-speciﬁcknowledge, and keep a stable model size as thenumber of language-pair increases without intro-ducing any specialized module.
to achieve this, wepropose to divide the model neurons into two partsbased on their importance: the general neurons.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5725–5737august1–6,2021.©2021associationforcomputationallinguistics5725which are used to retain the general knowledge ofall the languages, and the language-speciﬁc neu-rons which are used to retain the language-speciﬁcknowledge.
speciﬁcally, we ﬁrst pre-train a stan-dard mnmt model on all language data and thenevaluate the importance of each neuron in eachlanguage pair.
according to their importance, wedivide the neurons into the general neurons andthe language-speciﬁc neurons.
after that, we ﬁne-tune the translation model on all language pairs.
in this process, only the general neurons and thecorresponding language-speciﬁc neurons for thecurrent language pair participate in training.
ex-perimental results on different languages show thatthe proposed method outperforms several strongbaselines..our contributions can be summarized as follows:.
• we propose a method that can improve thetranslation performance of the mnmt modelwithout introducing any specialized modulesor adding new parameters..• we show that the similar languages sharesome common features that can be captured bysome speciﬁc neurons of the mnmt model..• we show that some modules tend to capturethe general knowledge while some modulesare more essential for capturing the language-speciﬁc knowledge..2 background.
in this section, we will give a brief introduction tothe transformer model (vaswani et al., 2017) andthe multilingual translation..2.1 the transformer.
we denote the input sequence of symbols asx(cid:48) = (x1, .
.
.
, xj ), the ground-truth sequence asy∗ = (y∗k∗) and the translation as y =(y1, .
.
.
, yk)..1, .
.
.
, y∗.
transformer is a stacked network with n iden-tical layers containing two or three basic blocksin each layer.
for a single layer in the encoder,it consists of a multi-head self-attention and aposition-wise feed-forward network.
for a singledecoder layer, besides the above two basic blocks, amulti-head cross-attention follows multi-head self-attention.
the input sequence x will be ﬁrst con-verted to a sequence of vectors and fed into theencoder.
then the output of the n -th encoder layer.
will be taken as source hidden states and fed intodecoder.
the ﬁnal output of the n -th decoder layergives the target hidden states and translate the targetsentences..2.2 multilingual translation.
in the standard paradigm of mnmt, all param-eters are shared across languages and the modelis jointly trained on multiple language pairs.
wefollow johnson et al.
(2017) to reuse standard bilin-gual nmt models for multilingual translation byaltering the source input with a language tokenlang, i.e.
changing x(cid:48) to x = (lang, x1, .
.
.
, xj )..3 approach.
our goal is to build a uniﬁed model, which canachieve good performance on all language pairs.
the main idea of our method is that different neu-rons have different importance to the translationof different languages.
based on this, we dividethem into general and language-speciﬁc ones andmake general neurons participate in the translationof all the languages while language-speciﬁc neu-rons focus on some speciﬁc languages.
speciﬁcally,the proposed approach involves the following stepsshown in figure 1. first, we pretrain the model onthe combined data of all the language pairs follow-ing the normal paradigm in johnson et al.
(2017).
second, we evaluate the importance of differentneurons on these language pairs and allocate theminto general neurons and language-speciﬁc neu-rons.
last, we ﬁne-tune the translation model onthe combined data again.
it should be noted that fora speciﬁc language pair only the general neuronsand the language-speciﬁc neurons for this languagepair will participate in the forward and backwardcomputation when the model is trained on this lan-guage pair.
other neurons will be zeroed out duringboth training and inference..3.1.importance evaluation.
the basic idea of importance evaluation is to deter-mine which neurons are essential to all languageswhile which neurons are responsible for some spe-ciﬁc languages.
for a neuron i, its average impor-tance i across language pairs is deﬁned as follow:.
i(i) =.
θm(i),.
(1).
1m.m(cid:88).
m=1.
where the θ(·) denotes the importance evaluationfunction and m denotes the number of language.
5726figure 1: the whole training process of the proposed method.
the red, yellow and blue circles represent language-speciﬁc neurons that are important for l1, l2&l3 and l1&l3, respectively..pairs.
this value correlates positively with howimportant the neuron is to all languages.
for theimportance evaluation function θ(·), we adopt twoschemes: one is based on the taylor expansion andthe other is based on the absolute value..taylor expansion we adopt a criterion basedon the taylor expansion (molchanov et al., 2017),where we directly approximate the change in losswhen removing a particular neuron.
let hi be theoutput produced from neuron i and h representsthe set of other neurons.
assuming the indepen-dence of each neuron in the model, the change ofloss when removing a certain neuron can be repre-sented as:.
|∆l(hi)| = |l(h, hi = 0) − l(h, hi)|,.
(2).
where l(h, hi = 0) is the loss value if the neuron iis pruned and l(h, hi) is the loss if it is not pruned.
for the function l(h, hi), its taylor expansion atpoint hi = a is:.
l(h, hi) =.
(hi − a)n + rn (hi),.
n(cid:88).
n=0.
ln(h, a)n!.
(3)where ln(h, a) is the n-th derivative of l(h, hi)evaluated at point a and rn (hi) is n -th remainder.
then, approximating l(h, hi = 0) with a ﬁrst-order taylor polynomial where hi equals zero:.
∂l(h, hi)∂hi.
l(h, hi = 0) = l(h, hi)−.
hi−r1(hi).
(4)the remainder r1 can be represented in the formof lagrange:.
r1(hi) =.
∂2l(h, hi)∂2δhi.
h2i ,.
(5).
where δ ∈ (0, 1).
considering the use of relu ac-tivation function (glorot et al., 2011) in the model,the ﬁrst derivative of loss function tends to be con-stant, so the second order term tends to be zero inthe end of training.
thus, we can ignore the remain-der and get the importance evaluation function asfollows:.
θte(i) = |∆l(hi)| =.
(cid:12)(cid:12)(cid:12)(cid:12).
∂l(h, hi)∂hi.
(cid:12)(cid:12)(cid:12)(cid:12).
hi.
..(6).
in practice, we need to accumulate the product ofthe activation and the gradient of the objective func-tion w.r.t to the activation, which is easily computedduring back-propagation.
finally, the evaluationfunction is shown as:.
θm.
te(il) =.
1tm.
(cid:88).
t.(cid:12)(cid:12)(cid:12)(cid:12).
δl(h, hli)δhli.hli.,.
(cid:12)(cid:12)(cid:12)(cid:12).
(7).
where hli is the activation value of the i-th neuronof l-th layer and tm is the number of the trainingexamples of language pair m. the criterion is com-puted on the data of language pair m and averagedover tm..absolute value we adopt the magnitude-basedneuron importance evaluation scheme (see et al.,2016), where the absolute value of each neuron’sactivation value is treated as the importance:.
θm.
av(il) =.
1tm.
(cid:88).
|hl.
i|..t.(8).
the notations in the above equation are the sameas those in the equation 7. after the importanceof each neuron is evaluated on the combined data,we need to determine the role of each neuron in theﬁne-tuning step following the method in the nextsection..57273.2 neuron allocation.
in this step, we should determine which neuronsare shared across all the language pairs and whichneurons are shared only for some speciﬁc languagepairs..general neurons according to the overall im-portance i(i) in equation 1, the value correlatespositively with how important the neuron is to alllanguages.
therefore, we rank the neurons in eachlayer based on the importance and make the top ρpercentage as general neurons that are responsiblefor capturing general knowledge..language-speciﬁc neurons next, we regardother neurons except for the general neurons asthe language-speciﬁc neurons and determine whichlanguage pair to assign them to.
to achieve this, wecompute an importance threshold for each neuron:.
λ(i) = k × max(θm(i)),.
m ∈ {1, .
.
.
, m }, k ∈ [0, 1].
(9).
, where max(θm(i)) denotes the maximum impor-tance of this neuron in all language pairs and k isa hyper-parameter.
the neuron will be assigned tothe language-pairs whose importance is larger thanthe threshold.
when the importance of neurons isdetermined, the number of language pairs associ-ated with each neuron can be adjusted according tok. the smaller the k, the more language-pairs willbe associated with the speciﬁc neurons.
in this way,we ﬂexibly determine the language pairs assignedto each neuron according to its importance in dif-ferent languages.
note that the neuron allocation isbased on the importance of language pair.
we havealso tried other allocation variants, e.g., based onthe source language, target language, and ﬁnd thatthe language pair-based method is the best amongof these methods.
the detailed results are listed inappendix a..after this step, the model is continually ﬁne-tuned on the combined multilingual data.
if thetraining data is from a speciﬁc language pair, onlythe general neurons and the language-speciﬁc neu-rons for this language pair will participate in theforward computation and the parameters associatedwith them will be updated during the backwardpropagation..4 experiments.
4.1 data preparation.
in this section, we describe the datasets using in ourexperiments on many-to-many and one-to-manymultilingual translation scenarios..many-to-many for this translation scenario,we test our approach on iwslt-171 translationdatasets, including english, italian, romanian,dutch (brieﬂy, en, it, ro, nl).
we experimentedin eight directions, including it↔en, ro↔en,nl↔en, and it↔ro, with 231.6k, 220.5k, 237.2k,and 217.5k data for each language pair.
we choosetest2016 and test2017 as our development and testset, respectively.
sentences of all languages weretokenized by the moses scripts2 and further seg-mented into subword symbols using byte-pair en-coding (bpe) rules (sennrich et al., 2016) with40k merge operations for all languages jointly..one-to-many we evaluate the quality of ourmultilingual translation models using training datafrom the europarl corpus3, release v7.
our ex-periments focus on english to twelve primary lan-guages: czech, finnish, greek, hungarian, lithua-nian, latvian, polish, portuguese, slovak, slovene,swedish, spanish (brieﬂy, cs, fi, el, hu, lt, lv,pl, pt, sk, sl, sv, es).
for each language pair,we randomly sampled 0.6m parallel sentences astraining corpus (7.2m in all).
the europarl eval-uation data set dev2006 is used as our validationset, while devtest2006 is our test set.
for languagepairs without available development and test set,we randomly split 1k unseen sentence pairs fromthe corresponding training set as the developmentand test data respectively.
we tokenize and true-case the sentences with moses scripts and apply ajointly-learned set of 90k bpe obtained from themerged source and target sides of the training datafor all twelve language pairs..4.2 systems.
to make the evaluation convincing, we re-implement and compare our method with four base-line systems, which can be divided into two cate-gories with respect to the number of models.
themultiple-model approach requires maintaining adedicated nmt model for each language:.
1https://sites.google.com/site/iwsltevaluation20172http://www.statmt.org/moses/3http://www.statmt.org/europarl/.
5728individualmultilingual.
it→en en→it ro→en en→ro nl→en en→nl34.9937.5538.1138.25our method-av 38.07our method-te 38.31.
30.2131.1332.0432.5632.2132.73.
23.1924.6424.9625.0826.0026.34.
28.5831.5831.8232.0732.1732.24.
31.2232.6233.4634.1634.1534.24.
27.6928.8630.0629.6630.1130.16.
+ts+adapter.
it→ro ro→it ave27.0420.9519.5228.8723.7920.8229.43+0.5623.5921.4329.65+0.7824.2621.1829.89+1.0224.4621.9630.12+1.2524.7622.21.para466.4m64.69m121.42m77.43m64.69m64.69m.
table 1: bleu scores on the many-to-many translation tasks.
’ave’ denotes the average bleu of the eight testsets and ’para’ denotes the number of parameters of the whole model.
’para’ of the individual system is the sumof the models for the eight language pairs with 58.3m parameters for each model..individualmultilingual.
cs36.1437.8737.7038.11our method-av 37.84our method-te 38.2138.03.
+ts+adapter.
+expansion.
el39.8640.3440.7040.2340.7540.7040.59.es41.1641.5842.0541.8342.1642.2242.28.fi22.9523.0323.2823.6623.7123.7423.73.hu31.7531.1031.7832.0031.4031.3232.47.lt32.3133.1132.9033.4933.5633.5534.12.lv38.1239.2239.4839.8739.9539.7840.12.pl32.9532.6733.6632.8533.2332.9433.95.pt35.5736.2036.0936.2536.5636.5836.41.sk40.5142.0542.0342.0042.0941.9142.44.sl43.8344.7644.2944.6345.2744.9445.30.sv33.2333.1633.1432.9033.3833.0733.43.ave35.7036.2636.43+0.1736.49+0.2336.66+0.4036.58+0.3236.91+0.65.
para746.76m90.42m273.77m109.54m90.42m90.42m102.14m.
table 2: bleu scores on one-to-many translation tasks.
language pair.
the denotations represent the same meaning as in table 1..’para’ of the individual system is 62.23m for each.
individual a nmt model is trained for eachlanguage pair.
therefore, there are n differentmodels for n language pairs..the uniﬁed model-based methods handle multi-ple languages within a single uniﬁed nmt model:multilingual (johnson et al., 2017) handlingmultiple languages in a single transformer modelwhich contains one encoder and one decoder witha special language indicator lang added to the inputsentence..+ts (blackwood et al., 2018) this method as-signs language-speciﬁc attention modules to eachlanguage pair.
we implement the target-speciﬁcattention mechanism because of its excellent per-formance in the original paper..+adapter (bapna and firat, 2019) this methodinjects tiny adapter layers for speciﬁc languagepairs into the original mnmt model.
we set thedimension of projection layer to 128 and train themodel from scratch..our method-av our model is trained just asthe approach section describes.
in this system, weadopt the absolute value based method to evaluatethe importance of neurons across languages..our method-te this system is implementedthe same as the system our method-av except thatwe adopt the taylor expansion based evaluationmethod as shown in equation 7..+expansion to make a fair comparison, we setthe size of feed forward network to 3000 to ex-pand the model capacity up to the level of other.
baselines, and then apply our taylor expansionbased method to this model..4.3 details.
for fair comparisons, we implement the proposedmethod and other contrast methods on the ad-vanced transformer model using the open-sourcetoolkit fairseq-py (ott et al., 2019).
we followvaswani et al.
(2017) to set the conﬁgurations ofthe nmt model, which consists of 6 stacked en-coder/decoder layers with the layer size being 512.all the models were trained on 4 nvidia 2080tigpus where each was allocated with a batch sizeof 4,096 tokens for one-to-many scenario and 2,048tokens for the many-to-many scenario.
we trainthe baseline model using adam optimizer (kingmaand ba, 2015) with β1 = 0.9, β2 = 0.98, and(cid:15) = 10−9.
the proposed models are further trainedwith corresponding parameters initialized by thepre-trained baseline model.
we vary the hyper-parameter ρ that controls the proportion of generalneurons in each module from 80% to 95% and setit to 90% in our main experiments according tothe performance.
the detailed results about thishyper-parameter are list in appendix b. we set thehyper-parameter k to 0.7 and do more analysis on itin section 5.3. for evaluation, we use beam searchwith a beam size of 4 and length penalty α = 0.6..4.4 results.
the ﬁnal translation is detokenized and then thequality is evaluated using the 4-gram case-sensitive.
5729(a) o2m-enc-6-ffn.
(b) o2m-dec-6-ffn.
bleu (papineni et al., 2002) with the sacrebleutool (post, 2018).4.many-to-many the results are given in table 1.we can see that the improvements brought by +tsand +adapter methods are not large.
for the +tsmethod, attention module may be not essential tocapture language-speciﬁc knowledge, and thus itis difﬁcult to converge to good optima.
for the+adapter method, adding an adapter module tothe end of each layer may be not appropriate forsome languages and hence has a loose capture toin all language pairs, ourthe speciﬁc features.
method based on taylor expansion outperforms allthe baselines in the datasets.
moreover, the param-eters in our model are the same as the multilingualsystem and less than other baselines..one-to-many the results are given in table 2,our method exceeds the multilingual baseline inall language pairs and outperforms other baselinesin most language pairs without capacity increment.
when we expand the model capacity to the levelof +adapter, our approach can achieve better trans-lation performance, which demonstrates the effec-tiveness of our method.
another ﬁnding is that theresults of the individual baseline are worse thanother baselines.
the reason may be the trainingdata is not big enough, individual baseline can notget a good enough optimization on 0.6m sentences,while the mnmt model can be well trained with atotal of 7.2m data..in our method, we allocate neurons based on theirimportance for different languages.
the rational-ity behind this mechanism is that different neu-rons should have distinct importance values so thatthese neurons can ﬁnd their relevant language pairs.
therefore, we show the importance of neurons com-puted by taylor expansion in different modules forthe one-to-many (o2m) and many-to-many (m2m)translation tasks.
for clarity and convenience, weonly show the importance values of three languagepairs in the sixth layer of encoder and decoder..the results of o2m are shown in figure 2(a) andfigure 2(b), and the language pairs are en→es,en→pt, and en→fi.
the ﬁrst two target languages.
4bleu+case.mixed+numrefs.1+smooth.exp+tok.13a.
+version.1.4.14.
(c) m2m-enc-6-ffn.
5.1 neuron importance for different.
5 analysis.
languages.
(d) m2m-dec-6-ffn.
figure 2: importance distribution of neurons computedby taylor expansion in each module.
for example,’o2m-enc-6-ffn’ represents the importance of thefeed forward network in the 6-th encoder layer..5730are spanish and portuguese, both of which belongto the western romance, the romance branch ofthe indo-european family, while the last one isfinnish, a member of the finnish-ugric branch ofthe ural family.
as we can see, the importanceof spanish and portuguese are always similar inmost neurons, but there is no obvious correlationbetween finnish and the other two languages.
it in-dicates that similar languages are also similar in thedistribution of the neuron importance, which im-plies that the common features in similar languagescan be captured by the same neurons..the results of m2m are shown in figure 2(c)and figure 2(d), and the language pairs are it→en,ro→it, and en→ro, whose bleu scores are 0.67,1, and 1.7 higher than the multilingual baseline, re-spectively.
in most neurons, the highest importancevalue is twice as high as the lowest and this highvariance of importance provides the theoretical ba-sis for later neuron allocation.
moreover, we cansee a lot of importance peaks of the two languagepairs: ro→it and en→ro, which means that theseneurons are especially important for generating thetranslation for these language pairs.
however, theﬂuctuation of it→en is ﬂat with almost no peaks,which means only a few neurons are speciﬁc to thislanguage pair.
this may be the reason why somelanguage pairs have higher improvements, whilesome have lower improvements..5.2 distribution of the language-speciﬁc.
neurons.
except for the general neurons shared by all the lan-guage pairs, our method allocates other neurons todifferent language pairs based on their importance.
these language-speciﬁc neurons are important forpreserving the language-speciﬁc knowledge.
tobetter understand the effectiveness of our method,we will show how these speciﬁc neurons are dis-tributed in the model..to evaluate the proportion of language-speciﬁcneurons for different language pairs at each layer,we introduce a new metric, lscore, formulated as:.
lscore(l, m) =.
, m ∈ {1, .
.
.
, m }.
(10).
˜i ml˜il.
where ˜i ml denotes the number of neurons allocatedto language pair m in the l-th layer, and ˜il denotesthe total number of the language-speciﬁc neuronsin the l-th layer.
the larger the lscore, the moreneurons allocated to the language pair m. we also.
(a) encoder.
(b) decoder.
(c) encoder.
(d) decoder.
figure 3: the distribution of the language-speciﬁc neu-rons in the encoder and decoder.
the importance ofneurons is computed by taylor expansion.
the ﬁrsttwo sub-ﬁgures show the proportion of speciﬁc neu-rons for different language pairs, while the last two sub-ﬁgures show the proportion of speciﬁc neurons in dif-ferent modules..5731figure 4: the average ∆ bleu over the multilin-gual baseline with different hyper-parameters k on themany-to-many translation task..figure 5: ∆ bleu over best performance when erasingthe general or language-speciﬁc neurons randomly onthe many-to-many translation task..introduce a metric to evaluate the average propor-tion of language-speciﬁc neurons of each languagein different modules, which formulated as:.
1m.m(cid:88).
m=0.
˜i ml,f˜il,f.
mscore(l, f ) =.
, m ∈ {1, .
.
.
, m }.
(11)where ˜i ml,f denotes the number of speciﬁc neuronsfor language pair m of in the f module of the l-th layer and m denotes the total number of thelanguage pair.
the larger the mscore is, the morespeciﬁc neurons are allocated to different languagepairs in this module..as shown in figure 3(a) and figure 3(b), thelanguage pairs have low lscores at the top and bot-tom layers and high lscores at the middle layers ofboth the encoder and decoder.
the highest lscoreappears at the third or fourth layers, which indicatesthat the neuron importance of different languagepairs is similar and the neurons of the middle layersare shared by more languages.
as a contrast, thebottom and top layers will be more specialized fordifferent language pairs.
next, from figure 3(c)and figure 3(d), we can see the mscores of the at-tention modules are almost near 1.0, which meansneurons in self attention and cross attention are al-most shared across all language pairs.
however,the mscores of feed forward network (ffn) grad-ually decrease as layer depth increases and it showsthat the higher layers in ffn are more essential forcapturing the language-speciﬁc knowledge..5.3 effects of the hyper-parameter k.when the importance of neurons for different lan-guages is determined, the number of language pairsassociated with each neuron can be adjusted ac-.
cording to k. when k = 1.0, the threshold ismax(θm(i)) as computed by equation 9, so theneurons will only be allocated to the language pairwith the highest importance, and when k = 0, thethreshold is 0 so the neurons will be shared acrossall language pairs just like the multilingual base-line.
to better show the overall impact of the hyper-parameter k, we vary it from 0 to 1 and the resultsare shown in figure 4. as we can see, the transla-tion performance of the two proposed approachesincreases with the increment of k and reach the bestperformance when k equals 0.7. as k continuesto increase, the performance deteriorates, whichindicates that the over-speciﬁc neurons are bad atcapturing the common features shared by similarlanguages and will lead to performance degrada-tion..5.4 the speciﬁc and general knowledge.
the main idea of our method is to let the generalknowledge and the language-speciﬁc knowledgebe captured by different neurons of our method.
to verify whether this goal has been achieved, weconduct the following experiments.
for the generalknowledge, we randomly erase 20% general neu-rons of the best checkpoint of our method, whichmeans we mask the output value of these neurons to0, then generate translation using it.
for language-speciﬁc knowledge, we randomly erase 50% spe-ciﬁc neurons and then generate translation..as shown in figure 5, when the general neuronsare erased, the bleu points of all the languagepairs drop a lot (about 15 to 20 bleu), whichindicates general neurons do capture the generalknowledge across languages.
for speciﬁc neurons,.
5732we show three language pairs for the sake of conve-nience.
we can see that when the neurons associ-ated with the current language pair are erased, theperformance of this language pair decreases greatly.
however, the performance of other language pairsonly declines slightly, because the speciﬁc knowl-edge captured by these speciﬁc neurons are not soimportant for other languages..6 related work.
our work closely relates to language-speciﬁc mod-eling for mnmt and model pruning which wewill recap both here.
early mnmt studies fo-cus on improving the sharing capability of indi-vidual bilingual models to handle multiple lan-guages, which includes sharing encoders (donget al., 2015), sharing decoders (zoph et al., 2016),and sharing sublayers (firat et al., 2016).
later,ha et al.
(2016) and johnson et al.
(2017) proposean universal mnmt model with a target languagetoken to indicate the translation direction.
whilethis paradigm fully explores the general knowledgebetween languages and hard to obtain the speciﬁcknowledge of each language (tan et al., 2019; aha-roni et al., 2019), the subsequent researches resortto language-speciﬁc modeling, trying to ﬁnd a bet-ter trade-off between sharing and speciﬁc.
suchapproaches involve inserting conditional language-speciﬁc routing layer (zhang et al., 2021), speciﬁcattention networks (blackwood et al., 2018; sachanand neubig, 2018), adding task adapters (bapnaand firat, 2019), and training model with differ-ent language clusters (tan et al., 2019), and so on.
however, these methods increase the capacity ofthe model which makes the model bloated..moreover, our method is also related to modelpruning, which usually aims to reduce the modelsize or improve the inference efﬁciency.
modelpruning has been widely investigated for both com-puter vision (cv) (luo et al., 2017) and natural lan-guage processing (nlp) tasks.
for example, seeet al.
(2016) examines three magnitude-based prun-ing schemes, zhu and gupta (2018) demonstratesthat large-sparse models outperform comparably-sized small-dense models, and wang et al.
(2020a)improves the utilization efﬁciency of parameters byintroducing a rejuvenation approach.
besides, lanet al.
(2020) presents two parameter reduction tech-niques to lower memory consumption and increasethe training speed of bert..7 conclusion.
the current standard models of multilingual neu-ral machine translation fail to capture the charac-teristics of speciﬁc languages, while the latest re-searches focus on the pursuit of speciﬁc knowl-edge while increasing the capacity of the modeland requiring ﬁne manual design.
to solve theproblem, we propose an importance-based neuronallocation method.
we divide neurons to generalneurons and language-speciﬁc neurons to retaingeneral knowledge and capture language-speciﬁcknowledge without model capacity incremental andspecialized design.
the experiments prove that ourmethod can get superior translation results withbetter general and language-speciﬁc knowledge..acknowledgments.
we thank all the anonymous reviewers for theirinsightful and valuable comments.
this work wassupported by national key r&d program of china(no.
2017yfe0192900)..references.
roee aharoni, melvin johnson, and orhan firat.
2019.massively multilingual neural machine translation.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages3874–3884.
association for computational linguis-tics..naveen arivazhagan, ankur bapna, orhan firat,dmitry lepikhin, melvin johnson, maxim krikun,mia xu chen, yuan cao, george f. foster, colincherry, wolfgang macherey, zhifeng chen, andyonghui wu.
2019. massively multilingual neuralmachine translation in the wild: findings and chal-lenges.
corr, abs/1907.05019..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..ankur bapna and orhan firat.
2019. simple, scal-able adaptation for neural machine translation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 1538–1548. association for computational linguistics..5733graeme w. blackwood, miguel ballesteros, and toddward.
2018. multilingual neural machine transla-tion with task-speciﬁc attention.
in proceedings ofthe 27th international conference on computationallinguistics, coling 2018, santa fe, new mexico,usa, august 20-26, 2018, pages 3112–3122.
asso-ciation for computational linguistics..daxiang dong, hua wu, wei he, dianhai yu, andhaifeng wang.
2015. multi-task learning for mul-in proceedings of thetiple language translation.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing of theasian federation of natural language processing,acl 2015, july 26-31, 2015, beijing, china, volume1: long papers, pages 1723–1732..carlos escolano, marta r. costa-juss`a, jos´e a. r.multi-fonollosa, and mikel artetxe.
2020.lingual machine translation: closing the gapbetween shared and language-speciﬁc encoder-decoders.
corr, abs/2004.06575..orhan firat, kyunghyun cho, and yoshua bengio.
2016. multi-way, multilingual neural machine trans-lation with a shared attention mechanism.
in naaclhlt 2016, the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, sandiego california, usa, june 12-17, 2016, pages866–875..jonas gehring, michael auli, david grangier, de-nis yarats, and yann n. dauphin.
2017. convolu-in proceed-tional sequence to sequence learning.
ings of the 34th international conference on ma-chine learning, icml 2017, sydney, nsw, australia,6-11 august 2017, pages 1243–1252..xavier glorot, antoine bordes, and yoshua bengio.
2011. deep sparse rectiﬁer neural networks.
in pro-ceedings of the fourteenth international conferenceon artiﬁcial intelligence and statistics, aistats2011, fort lauderdale, usa, april 11-13, 2011,volume 15 of jmlr proceedings, pages 315–323.
jmlr.org..jiatao gu, hany hassan, jacob devlin, and victor o. k.li.
2018. universal neural machine translation forextremely low resource languages.
in proceedingsof the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, naacl-hlt 2018,new orleans, louisiana, usa, june 1-6, 2018, vol-ume 1 (long papers), pages 344–354..thanh-le ha, jan niehues, and alexander h. waibel.
2016. toward multilingual neural machine trans-lation with universal encoder and decoder.
corr,abs/1611.04798..corrado, macduff hughes, and jeffrey dean.
2017.google’s multilingual neural machine translationtacl,system: enabling zero-shot5:339–351..translation..nal kalchbrenner and phil blunsom.
2013. recurrentin proceedings ofcontinuous translation models.
the 2013 conference on empirical methods in natu-ral language processing, emnlp 2013, 18-21 oc-tober 2013, grand hyatt seattle, seattle, washing-ton, usa, a meeting of sigdat, a special interestgroup of the acl, pages 1700–1709..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..jian-hao luo, jianxin wu, and weiyao lin.
2017.thinet: a ﬁlter level pruning method for deep neu-in ieee internationalral network compression.
conference on computer vision, iccv 2017, venice,italy, october 22-29, 2017, pages 5068–5076.
ieeecomputer society..pavlo molchanov, stephen tyree, tero karras, timoaila, and jan kautz.
2017. pruning convolutionalneural networks for resource efﬁcient inference.
in5th international conference on learning repre-sentations, iclr 2017, toulon, france, april 24-26, 2017, conference track proceedings.
openre-view.net..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, july 6-12, 2002, philadelphia,pa, usa, pages 311–318..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil tho-rat, fernanda b. vi´egas, martin wattenberg, greg.
devendra singh sachan and graham neubig.
2018.parameter sharing methods for multilingual self-in proceedings ofattentional translation models..5734the third conference on machine translation: re-search papers, wmt 2018, belgium, brussels, octo-ber 31 - november 1, 2018, pages 261–271.
associ-ation for computational linguistics..abigail see, minh-thang luong, and christopher d.manning.
2016. compression of neural machinein proceedings oftranslation models via pruning.
the 20th signll conference on computational nat-ural language learning, conll 2016, berlin, ger-many, august 11-12, 2016, pages 291–301.
acl..artiﬁcial intelligence conference, iaai 2020, thetenth aaai symposium on educational advancesin artiﬁcial intelligence, eaai 2020, new york, ny,usa, february 7-12, 2020, pages 9233–9241.
aaaipress..biao zhang, ankur bapna, rico sennrich, and orhanfirat.
2021. share or not?
learning to schedulelanguage-speciﬁc capacity for multilingual transla-tion.
in international conference on learning rep-resentations..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics, acl 2016, august 7-12, 2016, berlin, ger-many, volume 1: long papers..michael zhu and suyog gupta.
2018. to prune, ornot to prune: exploring the efﬁcacy of pruning forin 6th international confer-model compression.
ence on learning representations, iclr 2018, van-couver, bc, canada, april 30 - may 3, 2018, work-shop track proceedings.
openreview.net..barret zoph, deniz yuret, jonathan may, and kevinknight.
2016. transfer learning for low-resourcein proceedings of theneural machine translation.
2016 conference on empirical methods in naturallanguage processing, emnlp 2016, austin, texas,usa, november 1-4, 2016, pages 1568–1575.
theassociation for computational linguistics..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..xu tan, jiale chen, di he, yingce xia, tao qin, andtie-yan liu.
2019. multilingual neural machinetranslation with language clustering.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 963–973..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998–6008..ra´ul v´azquez, alessandro raganato, j¨org tiedemann,and mathias creutz.
2019. multilingual nmt with alanguage-independent attention bridge.
in proceed-ings of the 4th workshop on representation learn-ing for nlp (repl4nlp-2019), pages 33–39, flo-rence, italy.
association for computational linguis-tics..yong wang, longyue wang, victor o. k. li, andzhaopeng tu.
2020a.
on the sparsity of neuralmachine translation models.
in proceedings of the2020 conference on empirical methods in natu-ral language processing, emnlp 2020, online,november 16-20, 2020, pages 1060–1066.
associ-ation for computational linguistics..yong wang, longyue wang, shuming shi, victor o. k.li, and zhaopeng tu.
2020b.
go from the generalto the particular: multi-domain translation with do-main transformation networks.
in the thirty-fourthaaai conference on artiﬁcial intelligence, aaai2020, the thirty-second innovative applications of.
5735figure 6: ∆ bleu over multilingual baseline on many-to-many translation..a performance on different varieties.
in the proposed method we allocate neurons basedon importance of language pair.
there are threevarieties of our method: (a) source-speciﬁc, shareall neurons according to the source language only;(b) target-speciﬁc, share all neurons accordingto the target language only; (c) separate enc-dec,encoder neurons are shared according to the sourcelanguage and decoder neurons are shared accordingto the target language.
note that (c) is differentfrom our method since (c) is separate neurons totwo parts (encoder and decoder) and then connectspeciﬁc neurons of the two parts to form a whole,while our method is directly based on languagepairs..as shown in figure 6, we compare our taylorexpansion method with the other three varieties.
our approach outperforms other varieties on al-most all language pairs, and the performance of thelanguage-pair based approach is undoubtedly thebest.
the second is based on the target languageand the source language.
worst of all are the sep-arated encoder-decoder, which may be due to themismatch between the neurons of the encoder anddecoder when they are reconnected..b effects of the hyper-parameter ρ.we conducted several experiments on ρ to deter-mine the optimal hyper-parameter, so as to de-termine the proportion of universal neurons.
asshown in table 3, when ρ = 90% the model getsthe best translation result and reach best trade-offbetween general and language-speciﬁc neurons..5736it→en en→it ro→en en→ro nl→en en→nl.
ρ = 80% 38.3ρ = 90% 38.31ρ = 95% 38.28.
34.0534.1533.82.
32.1132.2432.05.
26.0126.3425.74.
32.2432.7331.97.
30.1230.1629.51.it→ro ro→it ave29.9424.3921.9630.1124.7622.2129.6424.1921.56.table 3: bleu scores on many-to-many translation tasks when k = 0.7.
5737