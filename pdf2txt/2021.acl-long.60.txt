from discourse to narrative: knowledge projectionfor event relation extractionjialong tang1,3, hongyu lin1, meng liao4, yaojie lu1,3,xianpei han1,2, le sun1,2,‚àó, weijian xie4, jin xu4,‚àó1chinese information processing laboratory 2state key laboratory of computer scienceinstitute of software, chinese academy of sciences, beijing, china3university of chinese academy of sciences, beijing, china4data quality team, wechat, tencent inc., china{jialong2019,hongyu,xianpei,sunle}@iscas.ac.cn{maricoliao, vikoxie, jinxxu}@tencent.com.
abstract.
current event-centric knowledge graphs highlyrely on explicit connectives to mine relationsbetween events.
unfortunately, due to the spar-sity of connectives, these methods severelyundermine the coverage of eventkgs.
thelack of high-quality labelled corpora furtherin this paper, weexacerbates that problem.
propose a knowledge projection paradigm forevent relation extraction: projecting discourseknowledge to narratives by exploiting thecommonalities between them.
speciÔ¨Åcally,we propose multi-tier knowledge projectionnetwork (mkpnet), which can leverage multi-tier discourse knowledge effectively for eventrelation extraction.in this way, the labelleddata requirement is signiÔ¨Åcantly reduced, andimplicit event relations can be effectively ex-tracted.
intrinsic experimental results show thatmkpnet achieves the new state-of-the-art per-formance, and extrinsic experimental resultsverify the value of the extracted event relations..1.introduction.
event-centric knowledge graphs(eventkgs)model the narratives of the world by represent-ing events and identifying relations between them,which are critical for machine understanding andcan beneÔ¨Åt many downstream tasks, such as ques-tion answering (costa et al., 2020), news read-ing (vossen, 2018), commonsense knowledge ac-quisition (zhang et al., 2020a) and so on..recently, semi-automatically constructing even-tkgs have gained much attention (tandon et al.,2015; rospocher et al., 2016; gottschalk and demi-dova, 2018; zhang et al., 2020b).
these methodsextract event knowledge from massive raw corporawith or without little human intervention, whichmakes them scalable solutions to build large-scale.
‚àócorresponding authors..figure 1: the knowledge projection paradigm for eventrelation extraction.
the explicit projection directlyprojects connectives to event relations, e.g., from ‚Äúbe-cause‚Äù to reason.
the implicit projection leverages thediscourse knowledge to discover implicit event relationswithout connectives via mkpnet..eventkgs.
commonly, each node in eventkgsrepresents an event, and each edge represents a pre-deÔ¨Åned relation between an event pair1.
currently,event relations are majorly extracted based on theexplicit connectives between them.
for example,in figure 1, a reason relation is extracted betweene2: ‚Äúper orders two hamburgers‚Äù and e3: ‚Äúperis so hungry‚Äù using the explicit connective ‚Äúbe-cause‚Äù between them..unfortunately, the connective-based approachesface the critical coverage problem due to the spar-sity of connectives.
that is, a large proportion ofevent pairs are not connected with explicit con-nectives, but with underlying event relations.
wedenote them as implicit event relations.
further-.
1computational and cognitive studies deÔ¨Åne nodes as even-tualities, which include activities, states and events.
in thispaper, we simplify the deÔ¨Ånition of each node to ‚Äúevent‚Äù dueto its popularity..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages732‚Äì742august1‚Äì6,2021.¬©2021associationforcomputationallinguistics732asynchronouscauseùê∑(cid:2869): tom goes to the restaurantùê∑(cid:2870): he orders two hamburgersùê∑(cid:2871): becausehe is so hungryùê∑(cid:2869)ùê∏(cid:2871): per is so hungryreason (because)reasonexplicit relationsimplicit relationsprecedencesuccessionùê∏(cid:2870): per orders two hamburgers ùê∏(cid:2869): per goes to the restaurant.knowledge projectionùê∑(cid:2870)ùê∑(cid:2871)causediscoursenarrativemore, the related events can even not close to eachother in a document.
for the example in figure 1,the implicit relation reason between e1: ‚Äúpergoes to the restaurant‚Äù and e3: ‚Äúper is so hun-gry‚Äù can not be extracted due to the absence ofexplicit connective as well as the discontinuity be-tween these two clauses.
the common practice inprevious connective-based approaches is to ignoreall these implicit instances (zhang et al., 2020b).
as a result, the coverage of eventkgs is signiÔ¨Å-cantly undermined.
besides, because the scale ofthe existed event relation corpus (hong et al., 2016)is limited, it is also impractical to build effectiveevent relation classiÔ¨Åers via supervised learning..in this paper, we propose a new paradigm forevent relation extraction ‚Äî knowledge projec-tion.
instead of relying on sparse connectivesor building classiÔ¨Åers starting from scratch, weproject discourse knowledge to event narratives byexploiting the anthropological linguistic connec-tions between them.
enlightened by livholts andtamboukou (2015); altshuler (2016); reyes andwortham (2017), discourses and narratives havesigniÔ¨Åcant associations, and their knowledge areshared at different levels: 1) token-level knowledge:discourses and narratives share similar lexical andsyntactic structures, 2) semantic-level knowledge:the semantics entailed in discourse pairs and eventpairs are analogical, e.g., e3-reason‚Üíe1 and d3-cause‚Üíd1 in figure 1., and 3) label-level knowl-edge: heterogeneous event and discourse relationshave the same coarse categories, e.g., both the eventrelation reason and the discourse relation causeare included in the coarse-grained relation contin-gency.
by exploiting the rich knowledge in manu-ally labelled discourse corpus and projecting theminto event relation extraction models, the perfor-mance of event relation extraction can be signiÔ¨Å-cantly improved, and the data requirement can bedramatically reduced..speciÔ¨Åcally, we design multi-tier knowledgeprojection network (mkpnet), which can lever-age multi-tier discourse knowledge effectively forevent relation extraction.
mkpnet introduces threekinds of adaptors to project knowledge from dis-courses into narratives: (a) token adaptor for token-level knowledge projection; (b) semantic adaptorfor semantic-level knowledge projection; (c) coarsecategory adaptor for label-level knowledge projec-tion.
by sharing the parameters of these three adap-tors, the commonalities between discourses and.
we.
conduct.
experiments.
narratives at various levels can be effectively ex-plored.
therefore, we can obtain more generaltoken representations, more accurate semantic rep-resentations, and more credible coarse categoryrepresentations to better predict event relations.
onintrinsicaser (zhang et al., 2020b), one of the rep-resentative eventkgs, and extrinsic experimentson winograd scheme challenge (wsc) (levesqueet al., 2012), one of the representative naturallanguage understanding benchmarks.
intrinsic ex-perimental results show that the proposed mkpnetsigniÔ¨Åcantly outperforms the state-of-the-art (soa)baselines, and extrinsic experimental results verifythe value of the extracted event relations2.
the main contributions of this paper are:.
‚Ä¢ we propose a new knowledge projectionparadigm, which can effectively leverage thecommonalities between discourses and narra-tives for event relation extraction..‚Ä¢ we design mkpnet, which can effectivelyleverage multi-tier discourse knowledge forevent relation extraction via token adaptor, se-mantic adaptor and coarse category adaptor..‚Ä¢ our method achieves the new sotaevent rela-tion extraction performance, and an enrichedeventkg is released by extracting both ex-plicit and implicit event relations.
we believeit can beneÔ¨Åt many downstream nlp tasks..2 background.
event relation extraction (ere).
given an ex-isting eventkg g = {e, r}, where nodes e areevents and edges r are their relations.
y ex ‚àà rare explicit event relations extracted by connective-based methods, and y im /‚àà r are implicit eventrelations without connectives.
commonly, im-plicit event relation extraction (iere) takes twoevents e1 = {e1|e2|},e1, e2 ‚àà e as inputs, then uses a neural networkto classify their underlying relation..|e1|}, e2 = {e2.
1, ..., e2.
1, ..., e1.
discourse relation recognition (drr).
drraims to recognize the relation of two discourse ar-guments.
discourse relations can be explicit orimplicit, where explicit relations are revealed by.
2our source codes with corresponding experimen-tal datasets and the enhanced eventkg are openlyavailable at https://github.com/tangjialong/knowledge-projection-for-ere..733figure 2: an overview of mkpnet, which projects discourse knowledge for event relation extraction: (a) tokenadaptor for token-level knowledge projection, (b) semantic adaptor for semantic-level knowledge projection, and (c)coarse category adaptor for label-level knowledge projection..connectives, while implicit relations lack these sur-face cues.
to resolve the implicit discourse rela-tion recognition (idrr) task, researchers constructhigh-quality labelled datasets (prasad et al., 2008)and design elaborate models (zhang et al., 2016b;bai and zhao, 2018; kishimoto et al., 2020)..associations between discourse and narra-tive.
recent nlp studies have proved that dis-course and narratives closely interact with eachother, and leveraging discourse knowledge beneÔ¨Åtsnarrative analysis signiÔ¨Åcantly, such as subeventsdetection (aldawsari and finlayson, 2019) andmain event relevant identiÔ¨Åcation (choubey et al.,2020).
motivated by the above observation, thispaper leverages the knowledge of discourse bya knowledge projection paradigm.
blessed withthe associations at token-, semantic- and coarsecategory-levels, the discourse corpora and knowl-edge can be effectively exploited for event relationextraction..3 multi-tier knowledge projection.
network for event relation extraction.
in this section, we describe how to learn an effec-tive event relation extractor by projecting resource-rich discourse knowledge to the resource-poor nar-rative task.
speciÔ¨Åcally, we propose multi-tierknowledge projection network (mkpnet) whichcan effectively leverage multi-tier discourse knowl-edge for implicit event relation extraction.
figure 2shows an overview of mkpnet, which uses to-ken adaptor, semantic adaptor and coarse categoryadaptor to fully exploit discourse knowledge at dif-ferent levels.
in the following, we Ô¨Årst describe theneural architecture of mkpnet and then describethe details of three adaptors..3.1 neural architecture of mkpnet.
for knowledge projection, we model both eventrelation extraction (ere) and discourse relationrecognition (drr) as an instance-pair classiÔ¨Åcationtask (devlin et al., 2019; kishimoto et al., 2020).
for ere, the input is an event pair such as <e1:‚Äúper goes to the restaurant‚Äù, e3: ‚Äúper is so hun-gry‚Äù> and the output is an event relation such asreason.
for drr, the input is a clause pair suchas <d1: ‚Äútom goes to the restaurant‚Äù, d3:‚Äúhe isso hungry‚Äù> and the output is a discourse relationsuch as cause..speciÔ¨Åcally, mkpnet extends the sotadrrmodel ‚Äî bert-cls (kishimoto et al., 2020) bythe vae-based semantic encoder and the coarse cat-egory encoder to model knowledge tier-by-tier (panet al., 2016; guo et al., 2019; kang et al., 2020; liet al., 2020b).
it 1) Ô¨Årst utilizes the bert-based to-ken encoder to encodes an instance pair as a tokenrepresentation h[cls]; 2) then obtains the seman-tic representation hz via a vae-based semanticencoder; 3) predicts the coarse-grained label andembeddings it as the coarse category representationhy c; 4) Ô¨Ånally classiÔ¨Åes its relation with the guid-ance of the aggregate instance-pair representation:.
y = classif ierf ine([h[cls] ‚äï hz ‚äï hy c]) (1).
where ‚äï means the concatenation operation.
inthis way,the parameters of mkpnet can begrouped by {Œ∏bert , Œ∏semantic, Œ∏coarse, Œ∏f ine},where Œ∏bert for bert-based token encoder,Œ∏semanticfor vae-based semantic encoder,Œ∏coarse for coarse category encoder and Œ∏f ine forthe Ô¨Ånal relation classiÔ¨Åer layer respectively..734<event(cid:2869),event(cid:2870) >event relation classifierbert-based token encodervae-based semantic encodercoarse category encodertoken adaptorsemantic adaptorcoarse category adaptor‚Ñé[(cid:3004)(cid:3013)(cid:3020)](cid:3032)‚Ä¶‚Ä¶‚Ñé(cid:3053)(cid:3032)‚Ñé(cid:3026)(cid:3278)(cid:3032)‚Ä¶<clause(cid:2869),clause(cid:2870) >discourse relation classifier‚Ñé[(cid:3004)(cid:3013)(cid:3020)](cid:3031)‚Ä¶‚Ä¶‚Ñé(cid:3053)(cid:3031)‚Ñé(cid:3026)(cid:3278)(cid:3031)‚Ä¶3.2 token adaptor.
recent studies have shown that similar tasks usu-ally share similar lexical and syntactic structuresand therefore lead to similar token representa-tions (pennington et al., 2014; peters et al., 2018).
the token adaptor tries to improve the token encod-ing for ere by sharing the parameters Œ∏bert ofthe bert-based encoders with drr.
in this way,the encoder is more effective due to the more su-pervision signals and is more general due to themulti-task settings..speciÔ¨Åcally, given an event pair <e1, e2>, we.
represent it as a sequence:.
[cls], e1.
1, ..., e1.
|e1|, [sep ], e2.
1, ..., e2.
|e2|, [sep ].
where[cls] and [sep] are special tokens.
for eachtoken in the input, its representation is constructedby concatenating the corresponding token, segmentand position embeddings.
then, the event pairrepresentation will be inputted into bert archi-tecture (devlin et al., 2019) and updated by multi-layer transformer blocks (vaswani et al., 2017).
finally, we obtain the hidden state correspondingto the special [cls] token in the last layer as thetoken-level event pair representation:.
[cls] = bert (e1, e2)he.
(2).
the token-level discourse pair representationhd[cls] can be obtained in the same way for drr.
to project the token-level knowledge, we usethe same bert for event pair and discourse pairencoding.
during the optimization process, it isÔ¨Åne-tuned using the supervision signals from bothere and drr..3.3 semantic adaptor.
because narrative and discourse analyses need toaccurately represent the deeper semantic of the in-stance pairs, the shallow token-level knowledgecaptured by the bert-based token encoder is notenough.
however, bert always induces a non-smooth anisotropic semantic space which is ad-verse for semantic modelling of large-grained lin-guistic units (li et al., 2020a)..to address this issue, we introduce an varia-tional autoencoder-based (vae-based) semanticencoder to represent the semantics of both eventsand clauses by transforming the anisotropic seman-tic distribution to a smooth and isotropic gaussiandistribution (kingma and welling, 2014; rezende.
figure 3: the illustration of the semantic encoder as adirected graph.
we use solid lines to denote the gener-ative model p = p(hy |h[cls], hz)p(hz|h[cls]), anddashed lines to denote the variational approximationq = q(hz|h[cls], hy ).
both variational parametersand generative parameters are learned jointly..et al., 2014; sohn et al., 2015).
to better learn thesemantic encoder, the semantic adaptor shares theparameters Œ∏semantic of it between ere and drrand train it using both classiÔ¨Åcation supervisionsignals and kl divergence..speciÔ¨Åcally, vae is a directed graphical modelwith the generative model p and the variationalmodel q, which learns the semantic representa-tion hz of the input by an autoencoder frame-work.
figure 3 illustrates the graphic represen-tation of the semantic encoder.
speciÔ¨Åcally, weassume that there exists a continuous latent vari-able hz ‚àº n (¬µ, diag(œÉ2)), where ¬µ and œÉ2 aremean and variance of the gaussian distribution re-spectively.
with this assumption, the original con-ditional probability of the event/discourse relationscan be expressed by the following formula:.
(cid:90).
hz.
p(hy |h[cls]) =.
p(hy |h[cls], hz).
(3).
p(hz|h[cls])dhz.
the.
posterior.
approximation.
is.
y or hd.
[cls] and hy can be he.
q(hz|h[cls], hy ), where h[cls] can be he[cls]or hdy according tothe different tasks.
we 1) Ô¨Årst obtain the input-and output-side representations via the sharedbert-based token encoder and the individualrelation embedding networks, i.e., h[cls] and hy ;2) then perform a non-linear transformation thatproject them onto the semantic space:.
h(cid:48)z = tanh(wz[h[cls]; hy ] + bz).
(4).
3) obtain the above-mentioned gaussian parame-ters ¬µ and logœÉ2 through linear regression:.
¬µ = w¬µh(cid:48).
z + b¬µ,.
logœÉ2 = wœÉh(cid:48).
z + bœÉ.
(5).
where w and b are the parameter matrix andbias term respectively; 4) use a reparameteriza-.
735‚Ñé(cid:3053)‚Ñé(cid:3026)‚Ñé[(cid:3004)(cid:3013)(cid:3020)]pqtion trick (kingma and welling, 2014; sohn et al.,2015) to get the Ô¨Ånal semantic representation:.
hz = ¬µ + œÉ (cid:12) (cid:15).
(6).
where (cid:15) ‚àº n (0, i) and hz can be he.
z or hdz..the neural model for the prior p(hz|h[cls]) isthe same as that for the posterior q(hz|h[cls], hy ),except for the absence of hy .
besides, those twomodels have parameters independent of each other.
during testing, due to the absence of the output-side representation hy , we set hz to be the mean ofp(hz|h[cls]) (zhang et al., 2016a), i.e., ¬µ. duringtraining, we minimize the kullback-leibler diver-gence kl(p ||q) between the generation model pand the inference model q. intuitively, kl diver-gence connects these two models:.
kl(q(hz|h[cls], hy )||p(hz|h[cls])).
(7).
to project the semantic-level knowledge, we usethe same vae for both event pair and discoursepair.
therefore, the commonalities of event seman-tics and discourse semantics can be captured moreaccurately..3.4 coarse category adaptor.
the token adaptor and the semantic adaptor com-mendably cover the knowledge entailed on theinput-side.
in addition, we found that ere anddrr share the same coarse-grained categories:temporal, contingency, comparison and expan-sion (prasad et al., 2008; zhang et al., 2020b), al-though they have different Ô¨Åne-grained categories.
to this end, we design the coarse category adap-tor in a coarse-to-Ô¨Åne framework (petrov, 2009)to bridge the gap between the heterogeneous Ô¨Åne-grained targets.
speciÔ¨Åcally, we share the param-eters Œ∏coarse of the coarse-grained classiÔ¨Åer andthe coarse label embedding network to obtain morecredible coarse category representations..speciÔ¨Åcally, we Ô¨Årst use the token representa-tion h[cls] and the semantic representation hz topredict the coarse-grained labels:.
label embedding network.
during the optimiza-tion process, both event instances and discourseinstances can be used to train this coarse categoryencoder.
the more supervision signals make itmore effective..3.5 full model training.
in this paper, we utilize multi-task learning (caru-ana, 1997) to implement the knowledge projectionfrom discourse to narrative.
it expects correlativetasks (ere and drr) can help each other to learnbetter by sharing the parameters of three adaptors.
given ere and drr training datasets, an alternateoptimization approach (dong et al., 2015) is usedto optimizate mkpnet:.
l(Œ∏) =Œ±(l(Œ∏; y ) + Œªkl(p ||q))+ (1 ‚àí Œ±)l(Œ∏; y c).
(9).
where y can be y im or y d according to thedifferent tasks, Œª, Œ± are two hyperparameters,kl(p ||q)) is the kl divergence in the seman-tic encoder, l(Œ∏; y ) and l(Œ∏; y c) are Ô¨Åne-grainedand coarse-grained objectives respectively:.
l(Œ∏; y ) = log p(y |h[cls], hz, hy c)l(Œ∏; y c) = log p(y c|h[cls], hz).
(10).
(11).
are.
that.
should.
noticed.
in mkpnet,itbe{Œ∏bert , Œ∏semantic, Œ∏coarse}sharedparameters of the bert-based token encoder,the vae-based semantic encoder and the coarsecategory encoder between ere and drr.
and {Œ∏f ine} are separated parameters of theÔ¨Åne-grained ere and drr classiÔ¨Åers..the.
4 experiments.
we conduct intrinsic experiments on aser (zhanget al., 2020b) to assess the effectiveness of theproposed mkpnet, and extrinsic experiments onwsc (levesque et al., 2012) to verify the value ofthe extracted event relations..y c = classif iercoarse(h[cls], hz).
(8).
4.1.intrinsic experiments.
where y c ‚àà {temporal, contingency, compari-son, expansion}.
after that, we use the coarselabel embedding network to obtain the correspond-ing coarse-grained label embedding hy c, which isreferred as the coarse category representation..to project that label-level knowledge, we use thesame coarse-grained classiÔ¨Åer and the same coarse.
datasets.
for discourse relation recognition(drr), we use pdtb 2.0 (prasad et al., 2008) withthe same splits of ji and eisenstein (2015): sections2-20/0-1/21-22 respectively for train/dev/test.
forevent relation extraction (ere), because there is nolabelled training corpus, we construct a new datasetby removing the connectives of the explicit event.
736relation instances in aser core version3 and re-taining at most 2200 instances with the highest con-Ô¨Ådence scores for each category4.
in this way, weobtain 23,181/1400/1400 train/dev/test instances ‚Äìwe denoted it as implicit event relation extraction(iere) dataset..implementation.
we implement our modelbased on pytorch-transformers (wolf et al., 2020).
we use bert-base and set all hyper-parametersusing the default settings ofthe sotadrrmodel (kishimoto et al., 2020)..baselines.
for ere, we compare the proposed.
mkpnet with the following baselines:.
‚Ä¢ baselines w/o discourse knowledge are onlytrained on iere training set.
we choose thebert-cls as the representative of them dueto its sotaperformance..‚Ä¢ baselines with discourse knowledge improvethe learning of ere via transfer learning (panand yang, 2009; pan et al., 2010) from dis-course models, i.e., Ô¨Årst pre-train a parameterprior on pdtb 2.0 and then Ô¨Åne-tune it oniere ‚Äì‚Äì we denote it as bert-transfer..for drr, we compare the proposed mkpnet.
with the following baselines:.
‚Ä¢ bai and zhao (2018) is a deep neural networkmodel augmented by variable grained text rep-resentations like character, sentence and sen-tence pair levels..‚Ä¢ kishimoto et al.
(2020) is the sotadrrmodel, bert-cls, which incorporatingbert with one additional output layer..4.1.1 overall results.
table 1-3 show the overall ere/drr results ofbaselines and mkpnet.
for our approach, weuse the full mkpnet and its four ablated settings:mkpnet w/o sa, mkpnet w/o ca, mkpnet w/osa & ca and mkpnet w/o kp, where sa, caand kp denote semantic adaptor, coarse categoryadaptor and knowledge projection correspondingly.
we can see that:.
1. based on mkpnet, we enrich the originalaser by abundant implicit event relations.
con-sidering the computational complexity, we classifythe event pairs co-occurrence in the same document.
3https://hkust-knowcomp.github.io/aser4higher conÔ¨Ådence score means more credible instance..number of relations.
framenet (baker et al., 1998)conceptnet (speer et al., 2017)event2mind (smith et al., 2018)atomic (sap et al., 2019)knowlywood (tandon et al., 2015)aser (zhang et al., 2020b)aser++ (core)aser++ (high)aser++ (full).
1,709116,09757,097877,1082,644,4151,287,0592,034,9633,530,7718,766,098.table 1: number comparison of event relations inaser++ and existing event-related resources..acc.
f1.
baselines w/o discourse knowledge.
bert-clsmkpnet w/o kp.
53.0053.94.baselines with discourse knowledge.
bert-transfer.
54.29multi-tier knowledge projection.
52.2453.52.
53.44.mkpnet w/o sa & ca 54.79(cid:52)0.8555.14(cid:52)1.20mkpnet w/o ca55.29(cid:52)1.35mkpnet w/o sa55.86(cid:52)1.92mkpnet.
53.90(cid:52)0.3854.42(cid:52)0.9054.92(cid:52)1.4055.36(cid:52)1.84.table 2: experimental results on iere test set, where (cid:52)means the improvements when compared with mkpnetw/o kp.
all improvements of mkpnet are statisticalsigniÔ¨Åcance at p<0.01 over the baseline mkpnet w/okp..modelbai and zhao (2018)bert-cls (kishimoto et al., 2020)bert-cls (ours)mkpnet w/o kpmkpnet.
acc.
48.2251.4050.9152.8654.09.table 3: experimental results on pdtb 2.0 test set.
fora fair comparison, the results of baselines are adaptedfrom their original papers..and Ô¨Ålter them by conÔ¨Ådence scores.
speciÔ¨Åcally,we compute the conÔ¨Ådence score by multiplyingthe classiÔ¨Åcation probability and the frequency ofthe event pair.
integrating with the original ex-plicit event relations, we can obtain the enrichedeventkgs aser++ (core/high/full) with the dif-ferent threshold conÔ¨Ådences (3/2/1).
table 1 showsthat when compared with existing event-related re-sources, aser++ has an overwhelming advantagein the number of event relations..2. the proposed mkpnet achieves sotaper-formance for ere.
mkpnet can signiÔ¨Åcantly out-perform the bert-transfer and achieves 55.86 ac-curacy and 55.36 f1.
mkpnet w/o kp obtains con-siderable performance improvements when com-.
737pared with bert-cls.
we believe this is becausemkpnet fully explores the knowledge on differ-ent tiers, and modelling knowledge tier-by-tier iseffective..3. by projecting knowledge at token-level, se-mantic level and label level, all three adaptorsare useful and are complementary with eachother.
when compared with the full model mkp-net, its four variants show declined performance indifferent degrees.
mkpnet outperforms mkpnetw/o ca 0.72 accuracy and 0.94 f1, which indi-cates that our coarse category adaptor successfullybridges the gap of heterogeneous Ô¨Åne-grained tar-gets.
mkpnet outperforms mkpnet w/o sa 0.57accuracy and 0.44 f1, and therefore we believe thatour latent semantic adaptor is helpful for capturethe semantic-level commonalities.
finally, thereis a signiÔ¨Åcant decline between mkpnet w/o kpand mkpnet w/o sa & ca, which means that to-ken adaptor is indispensable.
the insight in thoseobservations is that the commonalities between dis-courses and narratives under the hierarchical struc-ture, thus projecting them at different levels is ef-fective, and three adaptors can be complementarywith each other..4. the commonalities between discourses andnarratives are beneÔ¨Åcial for both ere and drr.
compared with the baselines w/o discourse knowl-edge ‚Äî bert-cls and mkpnet w/o kp, both thenaive transfer method ‚Äî bert-transfer and ourmkpnet achieve signiÔ¨Åcant performance improve-ments: bert-transfer gains 1.29 accuracy and1.20 f1 when compared to bert-cls, and mkp-net gains 1.92 accuracy and 1.84 f1 when com-pared to mkpnet w/o kp.
besides, for drr, ourmethod mkpnet also substantially outperformsthe other baselines and its variant mkpnet w/o kp.
these results veriÔ¨Åed the commonalities betweendiscourse knowledge and narrative knowledge..4.1.2 detailed analysis.
effects of semantic-level knowledge and label-level knowledge.
in these experiments, we com-pare the performance of our models, mkpnet,mkpnet w/o ca and mkpnet w/o sa with orwithout knowledge projection to Ô¨Ånd out the ef-fects of semantic-level knowledge and label-levelknowledge.
from table 4, we can see that: (1)compared with their counterparts, mkpnet, mkp-net w/o ca and mkpnet w/o sa with knowledgeprojection lead to signiÔ¨Åcant improvements.
thus,it is convincing that the performance improvements.
bert-cls.
mkpnet.
mkpnet w/o ca.
mkpnet w/o sa.
mkpnet w/o sa*.
kp.
fine-grained coarse-grainedacc.
acc.
f1.
f1.
53.0053.94(cid:88) 55.8653.79(cid:88) 55.1453.21(cid:88) 55.2970.50.
52.24 ‚Äî53.52 ‚Äî55.36 ‚Äî53.39 ‚Äî54.42 ‚Äî52.4854.9270.32.
66.5767.93100.0.
‚Äî‚Äî‚Äî‚Äî‚Äî63.0464.76100.0.table 4: effect of semantic-level knowledge and label-level knowledge on iere test set, where kp stands forknowledge projection and * stands for golden coarse-grained categories..figure 4: experimental results of using different sizesof iere training corpora..mainly come from the discourse knowledge ratherthan the neural architecture; (2) current knowledgeprojection can be further improved by exploitingmore accurate discourse knowledge: mkpnet w/osa*, which uses golden coarse categories, achievesstriking performance (acc 70.50; f1 70.32)..tradeoff between dataset quality and size.
as described above, the iere training dataset isconstructed using the most conÔ¨Ådent instances inaser core version.
we can construct a largerbut lower quality dataset by incorporating more in-stances with lower conÔ¨Ådence, i.e., the quality-sizetradeoff problem.
to analyze the tradeoff betweenthe quality and size, we construct a set of datasetswith different sizes/qualities, and figure 4 showsthe corresponding results of mkpnet on the devel-opment set.
we can see that the size is the mainfactors for performance improvements at the begin-ning: every 5,000 additional instances can result ina signiÔ¨Åcant improvement (about 2 to 3 f1 gain).
when the size is large (more than 20,000 instancesin our experiments), more instances will not resultin performance improvements, and the low-qualityinstances will hurt the performance..73851535557595k10k15k20k25k‚Ä¶50k‚Ä¶allscales of iere corporaf1modelpure knowledge-based methods.
knowledge hunting (emami et al., 2018)string match (zhang et al., 2020b).
language model-based methods.
lm (single) (trinh and le, 2018)lm (ensemble) (trinh and le, 2018)bert (w/o Ô¨Ånetuning) (devlin et al., 2019).
external knowledge enhanced methods.
bert (wscr) certu et al.
(2019)bert (aser) zhang et al.
(2020b)bert (aser & wscr) zhang et al.
(2020b)bert (aser++)bert (aser++ & wscr).
wsc.
57.356.6.
54.561.561.9.
71.464.572.566.274.1.table 5: the overall results of extrinsic experiments.
the evaluation metric is accuracy..extrinsic results.
table 5 shows the overallresults of extrinsic experiments.
we can see that:by Ô¨Åne-tuning bert on our enriched eventkg‚Äî aser++, the wsc performance can be signif-icantly improved.
bert (aser++) and bert(aser++ & wscr) outperform bert (aser)and bert (aser & wscr) respectively, whichveriÔ¨Åed the effectiveness of aser++ and implicitevent relations are beneÔ¨Åcial for downstream nlutasks..5 related work.
event-centric knowledge graphs.
knowledgegraphs have come from entity-centric ones (bankoet al., 2007; suchanek et al., 2007; bollacker et al.,2008; wu et al., 2012) to event-centric ones.
how-ever, the construction of traditional kgs takes do-main experts much effort and time, which are oftenwith limited size and cannot effectively resolve real-world applications, e.g., framenet (baker et al.,1998).
recently, many modern and large-scalekgs have been built semi-automatically, which fo-cus on events (tandon et al., 2015; rospocher et al.,2016; gottschalk and demidova, 2018; zhanget al., 2020b) and commonsense (speer et al., 2017;smith et al., 2018; huang et al., 2018; sap et al.,2019).
speciÔ¨Åcally, yu et al.
(2020) proposes anapproach to extract entailment relations betweeneventualities, e.g., ‚Äúi eat an apple‚Äù entails ‚Äúi eatfruit‚Äù, and release an event entailment graph (eeg).
different from eeg, this paper focuses on implicitevent relations which are not extracted due to theabsences of the connectives and discontinuity..knowledge transfer.
due to the data scarcityproblem, many knowledge transfer studies havebeen proposed, including multi-task learning (caru-.
figure 5: an overview of wsc implementation.
theblue color means the correct reference while the redcolor means the wrong one..4.2 extrinsic experiments.
the above intrinsic experiments veriÔ¨Åed the effec-tiveness of the proposed mkpnet for ere.
in thissection, we use the core version of our enrichedeventkgs ‚Äî aser++, and then conduct extrin-sic experiments on winograd schema challenge(wsc) (levesque et al., 2012) to verify the effectof aset++..wsc implementation.
wsc is challengingsince its schema is a pair of sentences that differonly in one or two words and that contain a referen-tial ambiguity that is resolved in opposite directionsin the two sentences.
according to certu et al.
(2019), Ô¨Åne-tuning pre-trained language models onwsc-schema style training sets is a robust methodto tackle wsc.
therefore, as figure 5 shows, wetransform aser++ to wsc-schema style trainingdata in the same way as zhang et al.
(2020b) andÔ¨Åne-tune bert on it, which we refer to as bert(aser++).
we compare bert (aser++) withthese baselines:.
‚Ä¢ pure knowledge-based methods are heuris-tical rule-based methods, such as knowledgehunting (emami et al., 2018) and stringmatch (zhang et al., 2020b)..‚Ä¢ language model-based methods use lan-guage model trained on large-scale corpus andtuned speciÔ¨Åcally for the wsc task, such aslm (trinh and le, 2018)..‚Ä¢ external knowledge enhanced methodsare models based on bert and trained withthe different external knowledge resource,e.g., wscr (ng, 2012; certu et al., 2019).
we implement our model based on pytorch-transformers (wolf et al., 2020).
bert-large isused.
all hyper-parameters are default settingsas certu et al.
(2019)..73997. the fish98.
the wormper eats hamburger per is hungryhamburger is tastyevent-centric knowledgegraphreasonreason‚Ä¶wsc questionspredictions97.
the fishate the worm.
itwas hungry.98.
the fish ate the worm.
itwas tasty.wsc-schema style training datapereats hamburger.
peris hungry.perdrinkscoffee.
peris sleepy.
pereatshamburger.
hamburgeris tasty.perdrinkscoffee.
coffeeis delicious.
ana, 1997), transfer learning (pan and yang, 2009;pan et al., 2010), and knowledge distillation (hin-ton et al., 2014).
recently, researchers are in-terested in training/sharing/transferring/distillingmodels layer by layer to fully excavate the knowl-edge (pan et al., 2016; guo et al., 2019; kang et al.,2020; li et al., 2020b).
in this paper, we proposea knowledge projection method which can projectdiscourse knowledge to narraties on different tiers..6 conclusions.
in this paper, we propose a knowledge projectionparadigm for event relation extraction and multi-tier knowledge projection network (mkpnet) isdesigned to leverage multi-tier discourse knowl-edge.
by effectively projecting knowledge fromdiscourses to narratives, mkpnet achieves thenew state-of-the-art event relation extraction per-formance, and extrinsic experimental results verifythe value of the extracted event relations.
for futurework, we want to design new data-efÔ¨Åcient algo-rithms to learn effective models using low-qualityand heterogeneous knowledge..acknowledgments.
this work is supported by the strategic priorityresearch program of chinese academy of sci-ences, grant no.
xda27020200, the nationalnatural science foundation of china under grantsno.
u1936207, and in part by the youth innovationpromotion association cas (2018141)..references.
mohammed aldawsari and mark finlayson.
2019. de-tecting subevents using discourse and narrative fea-tures.
in proceedings of acl 2019..daniel altshuler.
2016. events, states and times: anessay on narrative discourse in english.
walter degruyter gmbh & co kg..hongxiao bai and hai zhao.
2018. deep enhanced rep-resentation for implicit discourse relation cecognition.
in proceedings of iccl 2018..collin f. baker, charles j. fillmore, and john b. lowe.
1998. the berkeley framenet project.
in proceedingsof coling-acl 1998..michele banko, michael j. cafarella, stephen soder-land, matthew broadhead, and oren etzioni.
2007.open information extraction from the web.
in pro-ceedings of ijcai 2007..kurt d. bollacker, colin evans, praveen paritosh, timsturge, and jamie taylor.
2008. freebase: a collabo-ratively created graph database for structuring humanknowledge.
in proceedings of sigmod 2008..rich caruana.
1997. multitask learning.
machine.
learning, 28(1)..vid kocijanand ana-maria certu, oana-maria cam-buru, yordan yordanov, and thomas lukasiewicz.
2019. a surprisingly robust trick for the winogradin proceedings of acl 2019,schema challenge.
pages 4837‚Äì4842..prafulla kumar choubey, aaron lee, ruihong huang,and lu wang.
2020. discourse as a function of event:proÔ¨Åling discourse structure in news articles aroundthe main event.
in proceedings of acl 2020..tarc¬¥ƒ±sio souza costa, simon gottschalk, and elenademidova.
2020. event-qa: a dataset for event-centric question answering over knowledge graphs.
arxiv preprint arxiv:2004.11861..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of naacl 2019..daxiang dong, hua wu, wei he, dianhai yu, andhaifeng wang.
2015. multi-task learning for mul-tiple language translation.
in proceedings of acl-ijcnlp 2015..ali emami, noelia de la cruz, adam trischler, ka-heer suleman, and jackie chi kit cheung.
2018. aknowledge hunting framework for common sensereasoning.
in proceedings of emnlp 2018, pages1949‚Äì1958..simon gottschalk and elena demidova.
2018. even-tkg: a multilingual event-centric temporal knowl-edge graph.
in proceedings of eswc 2018..yunhui guo, honghui shi, abhishek kumar, kristengrauman, tajana rosing, and rogerio feris.
2019.spottune: transfer learning through adaptive Ô¨Åne-tuning.
in proceedings of cvpr 2019..geoffrey hinton, oriol vinyals, and jeff dea.
2014.indistilling the knowledge in a neural network.
proceedings of the workshop on nips 2014..yu hong, tongtao zhang, tim o‚Äôgorman, sharonehorowit-hendler, heng ji, and martha palmer.
2016.building a cross-document event-event relation cor-pus.
in proceedings of acl 2016..bhavana dalviand lifu huang, niket tandon, wen tauyih, and peter clark.
2018. tracking state changes inprocedural text: a challenge dataset and models forprocess paragraph comprehension.
in proceedingsof naacl-hlt 2018..740yangfeng ji and jacob eisenstein.
2015. one vectoris not enough: entity-augmented distributed seman-tics for discourse relations.
in proceedings of tacl2015..bingyi kang, saining xie, marcus rohrbach, zhichengyan, albert gordo, jiashi feng, and yannis kalan-tidis.
2020. decoupling representation and classiÔ¨Åerfor long-tailed recognition.
in proceedings of iclr2020..kingma and welling.
2014. auto-encoding variational.
bayes.
in proceedings of iclr 2014..yudai kishimoto, yugo murawaki, and sadao kuro-hashi.
2020. adapting bert to implicit discourse rela-tion classiÔ¨Åcation with a focus on discourse connec-tives.
in proceedings of lrec 2020..hector levesque, ernest davis, and leora morgenstern.
2012. the winograd schema challenge.
in proceed-ings of kr 2012..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020a.
on the sentenceembeddings from pre-trained language models.
inproceedings of emnlp 2020..jianquan li, xiaokang liu, honghong zhao, ruifengxu, min yang, and yaohong jin.
2020b.
bert-emd: many-to-many layer mapping for bert com-pression with earth mover‚Äôs distance.
in proceedingsof emnlp 2020..mona livholts and maria tamboukou.
2015. discourseand narrative methods: theoretical departures, an-alytical strategies and situated writings.
sage..altaf rahmanand vincent ng.
2012. resolving com-plex cases of deÔ¨Ånite pronouns: the winograd schemachallenge.
in proceedings of emnlp‚Äìconll 2012,pages 777‚Äì789..jianhan pan, xuegang hu, peipei li, huizong li, weihe, yuhong zhang, and yaojin lin.
2016. domainadaptation via multi-layer transfer learning.
neuro-computing..sinno jialin pan, ivor w tsang, james t kwok, andqiang yang.
2010. domain adaptation via transfercomponent analysis.
ieee transactions on neuralnetworks, 22(2)..sinno jialin pan and qiang yang.
2009. a survey on.
transfer learning.
tkde, 22(10)..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of emnlp 2014..slav orlinov petrov.
2009. coarse-to-fine natural lan-guage processing.
ph.d. thesis, eecs department,university of california, berkeley..rashmi prasad, nikhil dinesh, alan lee, eleni milt-sakaki, livio robaldo, aravind k joshi, and bon-nie l webber.
2008. the penn discourse treebank2.0. in proceedings of lrec 2008..angela reyes and stanton wortham.
2017. discourseand education, chapter discourse analysis acrossevents.
springer international publishing..danilo jimenez rezende, shakir mohamed, and daanwierstra.
2014. stochastic backpropagation and ap-proximate inference in deep generative models.
inproceedings of icml 2014..marco rospocher, marieke van erp, piek vossen,antske fokkens, itziar aldabe, german rigau, aitorsoroa, thomas ploeger, and tessel bogaard.
2016.building event-centric knowledge graphs from news.
journal of web semantics..maarten sap, ronan lebras, emily allaway, chan-dra bhagavatula, nicholas lourie, hannah rashkin,brendan roof, noah a. smith, and yejin choi.
2019.atomic: an atlas of machine commonsense for if-then reasoning.
in proceedings of aaai 2019..noah a. smith, yejin choi, maarten sap, hannahrashkin, and emily allaway.
2018. event2mind:commonsense inference on events, intents, and reac-tions.
in proceedings of acl 2018..kihyuk sohn, honglak lee, and xinchen yan.
2015.learning structured output representation using deepin proceedings ofconditional generative models.
nips 2015..robert speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-eral knowledge.
in proceedings of aaai 2017..fabian m. suchanek, gjergji kasneci, and gerhardweikum.
2007. yago: a core of semantic knowl-edge.
in proceedings of www 2007..niket tandon, gerard de melo, abir de, and ger-hard weikum.
2015. knowlywood: mining activityknowledge from hollywood narratives.
in proceed-ings of cikm 2015..trieu h. trinh and quoc v. le.
2018. a simplemethod for commonsense reasoning.
arxiv preprintarxiv:1806.02847..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomezand ≈Çukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of nips 2017..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proceedings of naacl 2018..piek vossen.
2018. newsreader at semeval-2018 task5: counting events by reasoning over event-centric-knowledge-graphs.
in proceedings of the 12th inter-national workshop on semantic evaluation..741thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtowicz,joe davison, sam shleifer, patrick von platen, clarama, yacine jernite, julien plu, canwen xu, teven lescao, sylvain gugger, mariama drame, quentinlhoest, and alexander m. rush.
2020. transformers:state-of-the-art natural language processing.
in pro-ceedings of emnlp 2020: system demonstrations..wentao wu, hongsong li, haixun wang, and kenny q.zhu.
2012. probase: a probabilistic taxonomy forin proceedings of sigmodtext understanding.
2012..changlong yu, hongming zhang, yangqiu song, wil-fred ng, and lifeng shang.
2020. enriching large-scale eventuality knowledge graph with entailmentrelations.
in proceedings of akbc 2020..biao zhang, deyi xiong, jinsong su, hong duan, andmin zhang.
2016a.
variational neural machine trans-lation.
in proceedings of emnlp 2016..biao zhang, deyi xiong, jinsong su, qun liu, ron-grong ji, hong duan, and min zhang.
2016b.
vari-ational neural discourse relation recognizer.
in pro-ceedings of emnlp 2016..hongming zhang, daniel khashabi, yangqiu song, anddan roth.
2020a.
transomcs: from linguistic graphsto commonsense knowledge.
in proceedings of ij-cai 2020..hongming zhang, xin liu, haojie pan, yangqiu song,and cane wing-ki leung.
2020b.
aser: a large-scale eventuality knowledge graph.
in proceedingsof www 2020..742