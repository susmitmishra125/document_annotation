learning to explain: generating stable explanations fast.
xuelin situ†.
ingrid zukerman†.
cecile paris‡.
sameen maruf†.
gholamreza haffari†.
†dept of data science and ai, faculty of it, monash university, australia‡csiro data61, australia†{firstname.lastname}@monash.edu‡{firstname.lastname}@data61.csiro.au.
abstract.
the importance of explaining the outcome ofa machine learning model, especially a black-box model, is widely acknowledged.
recentapproaches explain an outcome by identifyingthe contributions of input features to this out-come.
in environments involving large black-box models or complex inputs, this leads tocomputationally demanding algorithms.
fur-ther, these algorithms often suffer from lowstability, with explanations varying signiﬁ-in this pa-cantly across similar examples.
per, we propose a learning to explain (l2e)approach that learns the behaviour of an un-derlying explanation algorithm simultaneouslyfrom all training examples.
once the explana-tion algorithm is distilled into an explainer net-work, it can be used to explain new instances.
our experiments on three classiﬁcation tasks,which compare our approach to six explana-tion algorithms, show that l2e is between 5and 7.5 × 104 times faster than these algo-rithms, while generating more stable explana-tions, and having comparable faithfulness tothe black-box model..1.introduction.
explaining the mechanisms and reasoning behindthe outcome of complex machine learning models,such as deep neural networks (dnns), is crucial.
such explanations can shed light on the potentialﬂaws and biases within these powerful and widelyapplicable models, e.g., in medical diagnosis (caru-ana et al., 2015) and judicial systems (rich, 2016).
existing explainability methods mostly produceexplanations, or rationales (deyoung et al., 2020),which identify the attributions of features in an in-put example, e.g., are they contributing positivelyor negatively to the prediction of an outcome.
fortext classiﬁers, this means identifying words orphrases in an input document that account for a.novell’s microsoft attack completes linux conversion:novell inc. has completed its conversion to linux bylaunching an attack on microsoft corp., claiming thatthe company has stiﬂed software innovation and thatthe market will abandon microsoft windows at somepoint in the future..ˆyxxx = 99% sci/tech; ˆyxxx(cid:114)a = 14%; ˆyxxx(cid:114)l2e = 0.7%.
microsoft expands windows update release: microsoftcorp. is starting to ramp up distribution of its massivesecurity update for the windows xp operating system,but analysts say they still expect the company to moveat a relatively slow pace to avoid widespread glitches..ˆyxxx = 98% sci/tech; ˆyxxx(cid:114)a = 66%; ˆyxxx(cid:114)l2e = 0.4%.
figure 1: two similar examples from the news dataset.
the most important words (top 30%) found by our methodl2e are yellow-highlighted, and those from a baseline a areunderlined.
l2e considers words like ‘microsoft’ and ‘win-dows’ important in both examples.
ˆyxxx is the model’s pre-diction, and ˆyxxx(cid:114)· is the model’s prediction after removingimportant words in xxx..prediction.
current approaches are typically com-putationally demanding, requiring expensive op-erations, such as consulting a black-box modelmultiple times (zeiler and fergus, 2014), or gen-erating samples to learn an approximate but ex-plainable transparent model (ribeiro et al., 2016).
this computational demand reduces the utility ofthese explanation algorithms, especially for largeblack-box models, long documents and real-timescenarios (kim et al., 2018).
further, these algo-rithms generate explanations for different examplesindependently.
this may lead to the generation ofdifferent explanations for similar examples, whichis undesirable.
for example, a black-box predictswith similar conﬁdence (99% and 98%) that thetopic of the two semantically similar documents infigure 1 is sci/tech.
however, even though thewords ‘microsoft’ and ‘windows’ appear in bothdocuments, the baseline explainer a deems ‘win-dows’ to be important for the top document, and‘microsoft’ for the bottom document (that is, mask-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5340–5355august1–6,2021.©2021associationforcomputationallinguistics5340ing these words results in a signiﬁcant drop in theblack-box’s conﬁdence)..in this paper, we present a learning to explain(l2e) approach that efﬁciently learns the common-alities of the explanation process across differentexamples.
this, in turn, leads to explanations thatexhibit stability, i.e., important words are chosenconsistently, without loss of faithfulness to theunderlying black-box.1 given a set of examplespaired with their explanations produced by an ex-isting method, e.g., lime (ribeiro et al., 2016),our approach uses a dnn to learn the explana-tion algorithm.
dnns are turing complete (p´erezet al., 2019; montufar et al., 2014); therefore, givenenough training data and learning capacity, theyshould be able to learn the existing explanationalgorithms.
this is akin to knowledge distilla-tion (hinton et al., 2015), where a teacher, or in ourcase a teacher algorithm, distils knowledge into astudent network..our contributions are: (i) the l2e framework,which is general, and can successfully learn to pro-duce explanations from several teacher explainers;(ii) two learning formulations, i.e., ranking andsequence labelling, to enable l2e to circumventthe high variance of non-discrete teacher explana-tions via discretization; (iii) an experimental setupto compare l2e against six popular explanationalgorithms, and a comprehensive evaluation to in-vestigate the stability and faithfulness of l2e onthree text classiﬁcation tasks; (iv) a methodologythat employs human rationales as proxies for theground-truth explanations of a black-box model.
the core of this method is a modiﬁed training pro-tocol whereby the model makes neutral predictionsif human rationales are absent..2 related work.
we consider two main approaches to explanationgeneration: algorithmic and model-based..algorithmic approaches.
these approachescan be broadly categorized into gradient-based,attention-based and perturbation-based methods..gradient-based methods (simonyan et al., 2013;sundararajan et al., 2017; shrikumar et al., 2017;erion et al., 2019) or backpropagation-based meth-ods (bach et al., 2015) require access to the black-box, and are mostly applied to models with differ-entiable functions.
further, they may be sensitive.
1this approach does not aim to improve the trans-.
parency (lipton, 2018) of the black-box model..to randomized model initializations or permuteddata labels (adebayo et al., 2018), which is undesir-able.
these methods can be computationally heavyin the case of complex black-box models (wu andong, 2021), e.g., bert (devlin et al., 2018)..attention-based methods (wiegreffe and pinter,2019) can only be applied to transformer-basedmodels (vaswani et al., 2017), and their effective-ness is questionable (jain and wallace, 2019; ser-rano and smith, 2019)..perturbation-based methods approximate featureimportance by observing changes in a model’s out-come after a feature is changed.
they either con-sider changes in performance as an indicator offeature importance directly (martens and provost,2014; zeiler and fergus, 2014; schwab and karlen,2019), or they employ a higher-order approxima-tion of the decision boundary (ribeiro et al., 2016;lundberg and lee, 2017).
perturbation-basedmethods are typically computationally inefﬁcientfor explaining high-dimensional data, and they suf-fer from high variance due to perturbation random-ness (slack et al., 2020; chen et al., 2019)..model-based approaches.
these approachestrain the explainer with an objective function toimprove efﬁciency at test time.
the closest workto ours is by schwab and karlen (2019), whotrain an explainer using a causality-based expla-nation algorithm.
however, these approaches donot learn from arbitrary algorithms or discretizefeature weights — the high variation of continu-ous weights may impair the ability to capture thecommonalities in an explanation algorithm.
jainet al.
(2020) discretize the weights produced byan existing method, but they use these weights tobuild a faithful classiﬁer for an underlying black-box model, rather than using them to explain themodel directly..other works train a classiﬁer and an explainerjointly in order to incorporate explainability di-rectly into the classiﬁer (lei et al., 2016; cam-buru et al., 2018).
unlike these approaches, wedo not change the classiﬁer or require an ex-pensive process to collect human rationales, asdone in (camburu et al., 2018).
lastly, a fewworks use information-theoretic objectives to trainan explainer directly from the underlying classi-ﬁer (chen et al., 2018; bang et al., 2019).
these ex-plainers require careful training to select a low num-ber of important features (paranjape et al., 2020);hence, some input features do not have attributions..5341goodness of explanations.
researchers havequantiﬁed the goodness of an explanation in dif-ferent ways, such as brevity, alignment to humanrationales, contrastiveness and stability..minimal (brief) explanations are generatedin (martens and provost, 2014; ribeiro et al., 2018;alvarez-melis et al., 2019; bang et al., 2019).
explanations aligned with human rationales areproduced in (sen et al., 2020; atanasova et al.,2020), and contrastive explanations are generatedin (miller, 2018; alvarez-melis et al., 2019)..according to atanasova et al.
(2020), only a fewalgorithmic explanation methods produce stableexplanations (robnik- ˇsikonja and bohanec, 2018),e.g., lime (ribeiro et al., 2016).
to the best of ourknowledge, we are the ﬁrst to explore the stabilityof explanations in model-based approaches..3 learning to explain (l2e).
l2e can be applied to any natural language pro-cessing task to which an underlying feature-basedexplanation algorithm can be applied, such as nat-ural language inference and question answer-ing (wang et al., 2020).
in this paper, we focuson explaining text classiﬁcation models..our setup requires two inputs: (i) a black-boxtext classiﬁcation model ˆy = fθθθ(xxx), which as-signs document xxx to a label ˆy ∈ y , where y isthe label set; and (ii) an explanation algorithma(xxx, ˆy, fθθθ) → www, which generates explanationwww ∈ r|xxx| for the class of document xxx obtainedby the black-box fθθθ(xxx).
a can be any off-the-shelfexplanation algorithm; and wi can be thought asthe importance weight of xi – the ith token of adocument..the main idea of l2e is to train a separate ex-planation model gφφφ(xxx) to predict the explanationgenerated by a(.)
for fθθθ(.)
(figures 2a and 2b).
intuitively, our approach distils the explanation al-gorithm a into the explanation model gφφφ.
as con-ﬁrmed by our experiments (§4.5), this has severalbeneﬁts.
firstly, it leads to stable explanations, asgφφφ can capture a’s common patterns when generat-ing explanations for different documents.
secondly,it speeds up the explanation generation processcompared to many existing explanation algorithms,which rely on computationally heavy operations,such as consulting the black-box model multipletimes, e.g., occlusion (zeiler and fergus, 2014), orsampling, e.g., lime (ribeiro et al., 2016).
ourapproach, which learns a model with explanations.
algorithm 1 learning to explain (l2e).
z ← ∅for each input xxx ∈ d do.
ˆy ← fθθθ(xxx)www ← a(xxx, ˆy, fθθθ)z ← z ∪ (xxx, ˆy, www).
1: d: a training set of documents2: fθθθ: the original deep nn model3: gφφφ: the explainer deep nn model4: a: the underlying explanation method5: procedure trainexplainer(d, fθθθ)6:7:8:9:10:11:12:13:14:15:16:17:18:19:20: end procedure.
end forinitialize φφφ randomlyt ← 0while a stopping condition is not met dorandomly pick (xxxt, ˆyt, wwwt) ∈ zφφφ ← φφφ − ηt∇φφφl(gφφφ(xxxt, ˆyt), wwwt)t ← t + 1.end whilereturn the explanation model gφφφ.
of all training data, takes advantage of the com-putations done by a, and generates more stableexplanations faster..our approach to train the explanation model gφφφis summarized in algorithm 1. first, the algo-rithm generates training data in the form of triplets(xxx, ˆy, www) (lines 7–11), and then it trains the explana-tion model using supervised learning (lines 14–18).
at test time, the trained model is deployed to gen-erate explanations for unseen documents..a crucial component in training the explanationmodel under supervised learning is the loss func-tion l(gφφφ(xxxt, ˆy), www).
it penalizes a deviation of thepredicted explanation gφφφ(xxxt, ˆy) from the groundtruth explanation www.
this loss function is deter-mined by our supervised learning formulation..given that www is a continuous-valued vector, learn-ing the model gθθθ may be cast as a multivariate re-gression problem.
however, the continuous featureattributions generated by existing explanation algo-rithms could be sensitive to initializations (slacket al., 2020).
further, manually annotated ratio-nales (highlighting important words in a document)are sufﬁcient for people to understand/perform aclassiﬁcation task (zaidan et al., 2007).
so, insteadof a regression formulation, we consider two super-vised learning formulations for discretized outputs:ranking and sequence labeling..ranking formulation.
in this formulation, theexplanation model aims to learn the ranking of thedocument tokens from their importance weights.
that is, we consider the ordering of the tokenweights induced by www, and train the explanation.
5342figure 2: (a) pipeline of our l2e method; dashed arrows represent ofﬂine processes.
(b) detailed input and outputfor the sequence labeling formulation of our explanation model; red + label indicates that gθθθ considers ‘great’ tobe more important than other words in the prediction ˆy..(a).
(b).
model gφφφ such that it induces the same ordering.
speciﬁcally, the loss function is as follows:.
of feature attributions is required.
otherwise, thesequence labeling formulation is a better option..l(gφφφ(xxx, ˆy), www) = −.
|xxx|−1(cid:88).
|xxx|(cid:88).
log.
evkevi + evj.
j=i+1.
i=1where vi (vj) is the ith (jth) component of theimportance vector vvv = gφφφ(xxx, ˆy) predicted by theexplanation model, and k = arg maxk(cid:48)∈{i,j} |wk(cid:48)|.
in other words, each pair of token weights is com-pared, and the parameters are learnt such that atoken with a high importance weight under a alsogets a high score under gφφφ..sequence labeling formulation.
here, expla-nation generation is treated as a sequence label-ing problem, where the continuous importanceweights are discretized according to the heuris-tic h, whereby the importance weights are parti-tioned along two dimensions, high/low and posi-tive/neutral/negative, according to the mean valueof the positive/negative weights from the baselineexplanation method a. thus, the labels are re-coded to {high negative, low negative, neutral, lowpositive, high positive}.
the explanation modelgφφφ is then trained to predict the label of the tokensaccording to the following loss function:.
|xxx|(cid:88).
i=1.
l(gφφφ(xxx, ˆy), www) = −.
log pr(h(wi)|gφφφ,i(xxx, ˆy)).
where gφφφ,i(xxx, ˆy) is the predicted distribution overthe labels of the ith token of the document, andh(wi) is the discrete label produced using the dis-cretization heuristic h..owing to the quadratic complexity of the rank-ing formulation, compared to the linear complexityof sequence labeling, we recommend using rank-ing when the input is short, and a ﬁne-grained order.
4 experiments.
4.1 tasks and black-box models (fθθθ).
we conduct experiments on three classiﬁcationtasks; each task has a different black-box classi-ﬁer chosen based on the best accuracy on the se-lected dataset as reported in the literature.2 datasetstatistics are reported in appendix a..• topic classiﬁcation.
the ag corpus (zhanget al., 2015) comprises news articles on multipletopics.
we separate 10% of the training docu-ments for the dev set.
the black-box classiﬁer isa ﬁne-tuned bert model (devlin et al., 2018)with 12 hidden layers and 12 attention heads.
itachieves a 92.6% test accuracy..• sentiment analysis.
the sst dataset (socheret al., 2013) comprises movie reviews with pos-itive and negative sentiments.
the black-boxclassiﬁer is a distilled bert model (sanh et al.,2019) with 6 layers and 12 attention heads fromhugging face (wolf et al., 2019).
it achieves90% test accuracy..• linguistic acceptability..the coladataset (warstadt et al., 2019) contains sentencesthat are deemed acceptable or unacceptable interms of their grammatical correctness.
theblack-box classiﬁer is a ﬁne-tuned albertmodel (lan et al., 2020) with 12 attention headsand 12 layers.
it achieves a 74% test accuracy..2all black-box models are open-sourced by textat-.
tack (morris et al., 2020) unless otherwise stated..5343this is a great movie.
[0.01, -0.01, 0.02, 0.9, 0.06]this is a great movie.thisisagreatmovie+-+++4.2 baseline explanation methods (a).
we use six baselines for our experimental setup:occlusion (zeiler and fergus, 2014; schwab andkarlen, 2019), gradient (simonyan et al., 2013),lrp (bach et al., 2015), lime (ribeiro et al.,2016), kernel shap (lundberg and lee, 2017)and deep shap (shrikumar et al., 2017; lund-berg and lee, 2017).
the detailed setup of thesebaselines is provided in appendix b..4.3 explanation models (gφφφ).
we use a transformer encoder (vaswani et al.,2017) with 4 blocks and 4 attention heads as gφφφ.3all models are trained with a stochastic gradientdescent optimizer and a ﬁxed learning rate (1e−4)until convergence.
to balance the different statusesof model convergence, we train all models withthree random parameter initializations and reportthe average values of their performance metrics..we condition the explainer model gφφφ on the labelˆy predicted by the underlying black-box model fθθθby appending ˆy to the start and the end of the inputdocument before passing it to gφφφ (figure 2a).
thus,gφφφ can leverage the predicted label in the attentioncomputation.
for the sequence labeling formula-tion, we also introduce a softmax layer on top toproduce the labeling distribution over the discretelabels for each token, as detailed in figure 2b..4.4 performance metrics.
faithfulness.
a standard approach to evaluatethe faithfulness of an explanation to a black-boxclassiﬁcation model is to measure the degree ofagreement between the prediction given the fulldocument and the prediction given the explana-tion (ribeiro et al., 2016).
however, the aim of l2eis to approximate an existing explanation methoda, which constitutes a layer of separation fromthe original black-box fθθθ.
hence, we provide twofaithfulness evaluations for our approach when theground-truth explanation is unavailable:.
• prediction based.
we measure the agreementbetween: (a) the predictions of the black-boxmodel fθθθ when the explanations generated by gφφφare given as input, and (b) fθθθ’s predictions whena’s explanations are given as input (instead ofusing the full document);4.
3we use the fairseq framework (ott et al., 2019) for allour implementations of gφφφ.
our source code is available athttps://github.com/situsnow/l2e..4we do not evaluate the faithfulness of l2e to a in terms.
• conﬁdence based.
we adopt the ∆log-odds(xxx)metric used by schwab and karlen (2019), whichmeasures the difference in the conﬁdence of thefθθθ black-box model in a prediction before andafter masking the words in an explanation..log-odds(pr(ˆy|fθθθ(xxx)))−log-odds(pr(ˆy|fθθθ(˜xxx))).
where ˆy is the predicted output of fθθθ(xxx),log-odds(pr) = log pr1−pr , and ˜xxx is a versionof input xxx where the tokens in the explanationare masked out.
we expect a high ∆log-oddsvalue if we mask positive important words in˜x˜x˜x, and a low value if we mask unimportant ornegative important words..we report the average of each of these metrics.
across the test documents..stability.
we employ intersection over union(iou) to measure explanation stability across simi-lar instances.
speciﬁcally, for each test instance xxx,we select its nearest neighbors n (xxx) according toone of two pairwise document similarity metrics:semantic similarity – cosine of their bert repre-sentations; and lexical similarity – ratio of over-lapping n-grams.
details appear in appendix c.iou(xxx, n (xxx)) then measures the consistency of ex-planations of xxx and those of its neighbours,.
1|n (xxx)|.
(cid:88).
xxx(cid:48)∈n (xxx).
(cid:80).
(cid:80).
(cid:96)∈l(cid:96)(cid:54)=neutral.
(cid:96)∈l(cid:96)(cid:54)=neutral.
|vvv(cid:96).
xxx ∩ vvv(cid:96).
xxx(cid:48)|.
|vvv(cid:96).
xxx ∪ vvv(cid:96).
xxx(cid:48)|.
(1).
where l is the discretized label set in the sequencelabeling formulation or the top k words in theranking formulation, and vvv(cid:96)xxx is the set of tokenswith label (cid:96) in the predicted explanation gφφφ(xxx, ˆy).
we report the average of iou(xxx, n (xxx)) across doc-uments in the test set..4.5 results and discussion.
we start by investigating the faithfulness of an ex-planation model to the black-box model fθθθ.
oncefaithfulness has been established, we investigatestability and speed compared to the underlyingexplanation methods a. we also include a ran-dom baseline, which displays the performance ob-tained by randomly selecting the same k numberof words as we select from explanations producedby l2e and a in each row of the table, and averag-ing it over the six comparisons..of token importance, because a is not always faithful to theblack-box model..5344random.
occ..grad..lrp.
lime.
k’ shap.
d’ shap.
al2ebothal2ebothal2ebothal2ebothal2ebothal2eboth.
ranking.
seq.
labeling.
news sst cola news sst cola609489898991859790931009696798980827776.
43.6 45.8 7076898180908494858268718770709492778962838552858690839071849980829889809772237070907210051745970815973821006978.
656765986565100676710067659863631006565100.
37888385736673958384868181347058687064.table 1: percentage agreement of the black-box modelwith the baseline explanation algorithm a and l2e;“both” shows the agreement between l2e and a; boldindicates statistical signiﬁcance..faithfulness.
for the ranking formulation ofl2e, we select the top 30% of the important wordsin each test sample.5 for the sequence labelingformulation, we select the same number of posi-tive/negative words identiﬁed by l2e and a..table 1 shows the prediction-based agreementbetween the black-box model fθθθ and our methodl2e, between fθθθ and the underlying explainer a,and between l2e and a. we see that the explana-tions generated by l2e are equally predictive ofthe output class as those generated by a in boththe ranking and the sequence labeling formula-tions.
we also note that the l2e version that learnswith the ranking formulation is often less faithful,though not signiﬁcantly, to the black-box modelfθθθ than a compared to the version that learns withthe sequence labeling formulation.6 for example,the percentage agreement of l2e-ranking is lowerthan that of occlusion for the three datasets, whilethe agreement of l2e-sequencelabeling is higherthan that of occlusion for these datasets.
inter-estingly, when the baseline explanation algorithmdoes not perform well, e.g., kernel shap on sst,l2e is still able to ﬁnd words that are predictive ofthe output of fθθθ.
in such circumstances, the agree-.
5we select 30% to ensure sufﬁcient important words areselected in each dataset given their average document length.
we use the same percentage in the stability evaluation..6statistical signiﬁcance (α < 0.05) was measured by per-forming the wilcoxon signed-rank test (woolson, 2007)followed by a sequential holm-bonferroni correction (holm,1979; abdi, 2010) for all pairs of comparisons in a table..positive ∆log-odds ↑.
models news.
sst.
cola.
random.
occ..grad..lrp.
lime.
k’ shap.
d’ shap.
al2eal2eal2eal2eal2eal2e.
0.57±0.11 1.93±0.17 1.92±0.13.69±1.25.04±1.556.82±1.06 4.79±1.162.69±1.25.29±0.83 1.87±0.246.59±1.31 6.47±0.74 1.8±0.241.87±0.67 4.48±0.89 1.18±0.463.68±0.86 0.92±0.372.09±0.711.06±0.86 5.7±1.511.41±0.4411.31±0.53 5.26±0.91 1.41±0.444.33±1.21 0.22±0.37 1.96±0.283.24±0.92 2.81±0.65 1.97±0.241.16±0.54 4.82±2.65 1.22±0.582.25±0.72 7.61±1.86 1.02±0.48.
table 2: positive ∆log-odds when employing the se-quence labeling formulation; bold indicates statisticalsigniﬁcance; the experimental results also show thatl2e never performs signiﬁcantly worse than a; miss-ing entries are due to all words being considered as pos-itively important..ment between l2e and a is quite low (“both” is58% and 51% for ranking and sequence label-ing respectively).
the low performance of kernelshap may be attributed to insufﬁcient samples(103 in this case) in the kernel computation forsst, while l2e could still utilize all the samplesduring training..table 2 presents the ∆log-odds results for pos-itive explanation words in the sequence labelingformulation.
similar results are observed for neg-ative explanation words in the same formulation,and top important words in the ranking formula-tion.
these results appear in appendix d. theyare obtained by randomly selecting 100 documentsin the test set, and masking the same number ofimportant words in each document based on theexplanations generated by l2e and by a..we observe that some baselines have inconsis-tent faithfulness for different datasets.
for example,lrp and deep shap perform worse than kernelshap for the news dataset, but better for sst.
wealso note that, when one baseline performs worsethan the other baselines, e.g., kernel shap forsst, our method l2e still performs signiﬁcantlybetter than that baseline.
this result demonstratesthat our model can learn important words that yieldmore faithful explanations than those learned bythe teacher explainer.
interestingly, none of theresults for the cola dataset, from the baselinea or l2e, signiﬁcantly outperforms the randombaseline.
this ﬂags a drawback of evaluating ex-planation faithfulness on short documents..5345random.
occlusion.
gradient.
lrp.
lime.
k’ shap.
d’ shap.
models.
al2eal2eal2eal2eal2eal2e.
news6.19±0.387.18±1.358.96±1.677.17±1.1710.36±1.7710.2±1.7411.2±1.958.24±1.312.44±1.947.47±1.5212.84±1.976.68±1.088.67±1.3.
rankingsst3.74±0.394.58±1.198.52±1.483.87±0.977.41±1.21.13±0.467.75±1.223.01±0.875.01±0.882.49±0.722.82±0.832.87±0.711.43±0.68.
sequence labeling.
newscola11.0±0.69.29±0.915.59±3.8810.82±1.3421.26±3.72 13.94±1.498.63±1.899.02±1.0620.75±4.11 14.38±1.4213.84±1.4510.92±2.2514.67±1.4819.45±3.88.61±1.1113.96±2.9613.89±1.4716.78±3.825.75±1.219.12±3.919.15±1.6318.78±3.9616.8±4.187.22±1.0220.82±3.71 10.05±1.38.
sst6.75±0.367.04±0.878.38±0.756.33±0.747.27±0.676.28±0.797.45±0.796.92±0.868.24±0.731.47±0.521.73±0.536.66±0.888.41±0.81.
cola19.25±1.4523.67±3.7626.47±3.8410.4±1.8422.09±3.6122.53±3.3726.45±3.8417.48±3.4925.35±3.8222.39±4.821.82±4.713.05±3.2321.88±3.89.
table 3: intersection over union (iou) using semantic similarity; bold indicates statistical signiﬁcance.
since lrpconsiders all words to be positively important for the prediction, we only consider the iou of high positive wordsin the labeling formulation..stability.
for each test document, we considerthe top-3 similar documents in the test set, and re-port the average iou as explained in §4.4.
table 3shows the results obtained using semantic similar-ity for the baseline a and l2e.
similar results withlexical similarity appear in appendix c. from ta-ble 3, we see that, in most cases, our method statis-tically signiﬁcantly outperforms the baseline for allthree datasets.
for both formulations, ranking andsequence labeling, l2e achieves a higher stabilitythan the baseline a, even in cases where a’s iouis comparable to that of the random baseline, e.g.,gradient for sst and cola.
these results showthat learning the explanation process across differ-ent examples, as done by l2e, can capture morecommonalities (higher stability) than generatingexplanation individually (baselines)..overall, the lime baseline performs consis-tently better than most baselines in terms of faithful-ness and stability across the three datasets.
there-fore, l2e also performs better when it learns fromlime than when it learns from other baselines..computational efﬁciency.
we now comparethe efﬁciency of l2e against that of the baselineexplanation algorithms a when generating expla-nations for test documents.
in our experiments,the black-box is a transformer-based model com-prising l layers, h attention-heads and d embed-ding dimensions.
the complexity of this modelwhen predicting a document of size n is theno(l × n × d × (d + n + h)) (gu et al., 2020).
various factors contribute to the computational de-mands of existing explanation algorithms (detailsin appendix b), and make the complexity of thesealgorithms grow with the size of the black-box.
0.
8.
5 ± 5.
4.
9.
3.
1.
1.
9 ± 3.
4.
6.
5.
1 ± 6.
1 .
6.
3.
1.
0 ± 2.
9.]
)s(.
2gol[.
em.it.etupmoc.212.
28.
24.
20.
2−4.
8 ± 0 .
4.
1 .
4.
0 .
7 ± 0 .
2.
*.
5 ± 0 .
1.
0 .
1.l2e.
gradient.
l r p.li m e.o cclusion.
k’s h a p.d’s h a p.figure 3: average inference time for the baseline expla-nation algorithms and l2e-sequencelabeling for 100documents from imdb-r; lower is better; y-axis is inlog-scale; * indicates statistical signiﬁcance..model.
these factors include the size of the inputdocument (occlusion), the sample size (lime, ker-nel shap and deep shap) etc.
in contrast, l2eis a distillation of any explanation algorithm, em-ploying a smaller architecture than the black-box,e.g., fewer layers and attention heads, and lowerembedding dimensions..figure 3 shows the inference time of l2e-sequencelabeling compared to that of the baselineexplainers for the imdb-r dataset.7 we only showthe results obtained with sequence labeling, sincethe inference time of l2e models is independent ofthe learning formulation.
as seen in figure 3, l2erequires statistically signiﬁcantly less time thanany of the six baseline explanation algorithms forimdb-r. similar patterns were observed for the.
7all timing information is collected with the same hard-ware conﬁguration: intel xeon e5-2680 v3, nvidia teslak80, 32 gb ram..5346other three datasets (appendix e)..finally, l2e only needs a forward pass throughthe explainer dnn.
comparing with gradient andlrp, which require only one backpropagationthrough the black-box dnn, l2e is respectively 5and 10 times faster for all datasets (all black-boxsizes appear in §4.1 and appendix f)..5 evaluation with human rationales.
evaluation of explanation methods for dnns ischallenging, as ground-truth explanations are oftenunavailable.
in this section, we propose to addressthis issue using the imdb-r dataset (zaidan et al.,2007), which contains movie reviews xxx togetherwith their sentiment y, as well as rationales rrr an-notated by people for the sentiment label.
our useof rationales for evaluating explanations is relatedto that in (osman et al., 2020), where syntheticdata are generated from apriori ﬁxed rationales.
speciﬁcally, we generate new data by assigning a“neutral” label to an example where the human ra-tionales are masked.
we then use both the originaldata (without masking) and the new data to trainthe black-box model, where the training protocolforces the classiﬁer to make a “neutral” predictionwhen the human rationales are removed from thereview.
more formally, we maximize the followingtraining objective,.
(cid:88).
(xxx,rrr,y)∈d.
log pr(y = fθθθ(xxx))+.
log pr(neutral = fθθθ(xxx − rrr)).
where xxx − rrr denotes the input xxx with the rationalewords rrr masked out, neutral is an extra label,8and d is the training data..our classiﬁer achieves an accuracy of 83.83% onthe training set, 79.68% on the validation set and74.5% on the test set.
due to the large documentsizes (table 6 in appendix a) and the quadratictime complexity of the ranking formulation asa function of document size, we only train l2ewith the sequence labeling formulation; we uselexical similarity to measure iou, due to the time-consuming computation of semantic similarity withbert.
details about the dataset, the classiﬁer andthe explainer’s architecture appear in appendix f.the faithfulness and stability of the explanation.
methods are evaluated as follows..8it simulates abstaining from predicting any label from the.
original label set..positive reviews negative reviewspre.
re.
f1a144291892l2e 10a122191749l2e 11a5571112l2e 12a1217394517l2e 11a321122l2e 10a122291750l2e 11.pre.
re.
f116411219821215261318471514181216281215214249201332113214162812205413.occ..grad..lrp.
lime.
k’shap.
d’shap.
table 4: percentage precision, recall and f1 of explana-tions from l2e and corresponding baselines for datasetimdb-r; bold indicates statistical signiﬁcance.
de-tailed precision and recall values of positive reviewsappear in appendix g..faithfulness.
we select the top-k importantwords generated by an explanation method andcompute the precision, recall and f1 against thehuman-annotated rationales.
it is worth noting thatour l2e explainer is not supervised by human ra-tionales directly.
instead, we use the same exper-imental setup as in section 4.5 to ensure the l2eexplainer is learning from the baseline algorithmsrather than the human rationales..table 4 displays the average values over all testinstances.
as noted by carton et al.
(2020), the ra-tionales in the original dataset are not exhaustivelyidentiﬁed by human annotators.
for a particularevent, we expect to observe a lower precision thanrecall, since the black-box model might still be ableto utilize the words not being annotated in additionto the words annotated by a human.
the resultsin table 4 align with this hypothesis.
for instance,besides lrp for the positive reviews and kernelshap for both reviews, all baselines and the cor-responding l2e have higher recall than precision.
furthermore, l2e outperforms the correspondingbaseline a signiﬁcantly in most cases for both pos-itive and negative reviews, except when comparingwith lime’s precision.
this observation indicatesthat learning the explanations of multiple examplestogether, as done by l2e, achieves high faithful-ness to human rationales, as well as to the black-box model..stability.
table 5 displays stability computed inthree ways: (1) no ﬁltering (which extracts im-portant words only, table 3), (2) ﬁltering non-annotated words, and (3) ﬁltering stop-words.
forthe two ﬁltering measures, prior to ﬁltering, we.
5347no ﬁlter.
ﬁlter non-annotatedwords.
human.
occ..grad..lrp.
lime.
k’shap.
d’shap.
5.83±0.27 5.83±0.27a4.53±0.38 5.8±0.28l2e 4.57±0.38 5.86±0.274.06±0.25 3.64±0.31al2e 4.41±0.33 4.7±0.34a1.89±0.15 1.08±0.27l2e 4.01±0.24 5.66±0.43a1.36±0.08 1.05±0.16l2e 2.19±0.17 1.93±0.25a0.03±0.04 0.06±0.09l2e 0.45±0.17 0.69±0.59a4.29±0.18 3.33±0.29l2e 4.39±0.25 4.02±0.3.
ﬁlter stop-words.
3.06 ±0.272.46±0.192.48±0.191.65±0.152.33±0.191.7±0.132.45±0.170.93±0.071.97±0.150.03±0.040.45±0.171.61±0.132.37±0.17.
table 5: intersection over union (iou) using lexicalsimilarity for the imdb-r test set; bold indicates sta-tistical signiﬁcance..ensure the same number of important words is se-lected from the explanation produced by baselinea and l2e.
equation 1 is then used to computethe iou value.
to ensure a fair comparison, weselect the same number of words in l2e and acomparable baseline a before ﬁltering..similarly to the results in §4.5, as seen in table 5,l2e yields more stable explanations than the cor-responding baselines.
the best stability, obtainedwith l2e (58.6 ± 0.27) by ﬁltering non-annotatedwords when learning from occlusion, is compara-ble to that of the human rationales.
this is due tothe high recall (92 and 82 for positive and negativereviews respectively in table 4) in the explana-tions produced by l2e, which indicates they havehigh overlap with human rationales.
further, whenmeasuring the iou values, the l2e explanations ofsimilar examples have the same intersection withthe human rationales, but a lower union.
this re-sult indicates that people favour stable rationales insimilar documents, and reinforces our ﬁndings re-garding the greater consistency of the explanationsproduced by l2e compared to the baselines..lrp has been proven to have explanation con-tinuity (montavon et al., 2018), where the expla-nations of two nearly equivalent instances are alsoequivalent.
however, we do not observe such apattern in our experiments.
we hypothesize thatusing perturbed instances as neighbours, as doneby montavon et al.
(2018), does not necessarily fol-low the same distribution of the data.
instead, weposit that ﬁnding similar examples within a dataset,as done in our experiments, is a better proxy forstability evaluation..6 conclusions and future work.
we have presented a learning to explain (l2e)approach to learn the commonalities of the expla-nation generation processes across different exam-ples.
we have further proposed ranking and se-quence labeling formulations to effectively learnthe explainer model by discretizing feature weightsproduced by existing explanation algorithms..our experimental results show that our methodcan generate more stable explanations (i.e., not varymuch across similar documents) than those gener-ated by the explainer baselines, while maintainingthe same level of faithfulness to the underlyingblack-box model as the baseline algorithms.
more-over, our l2e approach produces explanations be-tween 5 and 7.5 × 104 times faster than the sixbaselines, making it suitable for long documentsand very large black-box models..our l2e approach trains an explainer, a black-box, to mimic the behaviour of an explanationmethod for an existing black-box model.
a keychallenge lies in the variation in the convergencestatus of such an explainer for different initializa-tions.
in order to mitigate this problem, we evalu-ate the performance of our explainer by averagingthree different initializations..the l2e approach opens up the possibilityof distilling multiple explanation algorithms intoone model.
although we focused on the stabil-ity, faithfulness and efﬁciency aspects of explana-tion generation, there are further desirable prop-erties, e.g., transparency, comprehensibility andnovelty (robnik- ˇsikonja and bohanec, 2018).
de-vising model-based explanation methods and theirevaluation with these desiderata are interesting di-rections for future research..acknowledgements.
this research was supported in part by grantdp190100006 from the australian research coun-cil.
the ﬁrst author was partly supported bycsiro/data61.
the computational resources forthis work were supported by the multi-modal aus-tralian sciences imaging and visualisation envi-ronment (massive) (www.massive.org.au).
wewould like to thank the anonymous reviewers fortheir insightful comments..5348references.
herv´e abdi.
2010. holm’s sequential bonferroni pro-cedure.
encyclopedia of research design, 1(8):1–8..julius adebayo, justin gilmer, michael muelly, iangoodfellow, moritz hardt, and been kim.
2018.in advancessanity checks for saliency maps.
in neural information processing systems, pages9505–9515..david alvarez-melis, hal daum´e iii, jennifer wort-man vaughan, and hanna wallach.
2019. weight ofevidence as a basis for human-oriented explanations.
in neurips workshop on hcml..pepa atanasova, jakob grue simonsen, christina li-oma, and isabelle augenstein.
2020. a diagnosticstudy of explainability techniques for text classi-ﬁcation.
in proceedings of the 2020 conference onempirical methods in natural language processing(emnlp), pages 3256–3274..sebastian bach, alexander binder, gr´egoire mon-tavon, frederick klauschen, klaus-robert m¨uller,and wojciech samek.
2015. on pixel-wise explana-tions for non-linear classiﬁer decisions by layer-wiserelevance propagation.
plos one, 10(7):e0130140..seojin bang, pengtao xie, heewook lee, wei wu, anderic xing.
2019. explaining a black-box using deepvariational information bottleneck approach..iz beltagy, matthew e peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv preprint arxiv:2004.05150..oana-maria camburu, tim rockt¨aschel, thomaslukasiewicz, and phil blunsom.
2018. e-snli: nat-ural language inference with natural language expla-nations.
in advances in neural information process-ing systems 31, pages 9539–9549..efﬁcient model interpretation for structured data.
ininternational conference on learning representa-tions..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..jay deyoung, sarthak jain, nazneen fatema rajani,eric lehman, caiming xiong, richard socher, andbyron c. wallace.
2020. eraser: a benchmark toevaluate rationalized nlp models.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 4443–4458..g erion, jd janizek, p sturmfels, s lundberg, andsi lee.
2019. improving performance of deep learn-ing models with axiomatic attribution priors and ex-pected gradients.
arxiv preprint arxiv:1906.10670..xiaotao gu, liyuan liu, hongkun yu, jing li, chenchen, and jiawei han.
2020. on the transformergrowth for progressive bert training.
arxiv preprintarxiv:2010.12562..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..s. holm.
1979. a simple sequentially rejective multi-ple test procedure.
scandinavian journal of statis-tics, 6(2):65–70..sarthak jain and byron c. wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556..samuel carton, anirudh rathore, and chenhao tan.
2020. evaluating and characterizing human ratio-in proceedings of the 2020 conference onnales.
empirical methods in natural language processing(emnlp), pages 9294–9307..sarthak jain, sarah wiegreffe, yuval pinter, and by-ron c. wallace.
2020. learning to faithfully rational-ize by construction.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4459–4473..rich caruana, yin lou, johannes gehrke, paul koch,marc sturm, and noemie elhadad.
2015.intelli-gible models for healthcare: predicting pneumoniain proceed-risk and hospital 30-day readmission.
ings of the 21th acm sigkdd international con-ference on knowledge discovery and data mining,kdd ’15, page 1721–1730..jianbo chen, le song, martin wainwright, andmichael jordan.
2018. learning to explain: aninformation-theoretic perspective on model interpre-in proceedings of the 35th internationaltation.
conference on machine learning, volume 80, pages883–892..jianbo chen, le song, martin j. wainwright, andmichael i. jordan.
2019. l-shapley and c-shapley:.
jinkyu kim, anna rohrbach, trevor darrell, johncanny, and zeynep akata.
2018. textual explana-tions for self-driving vehicles.
in proceedings of theeuropean conference on computer vision (eccv),pages 563–578..pieter-jan kindermans, kristof sch¨utt, klaus-robertm¨uller, and sven d¨ahne.
2016.investigatingthe inﬂuence of noise and distractors on the in-arxiv preprintterpretation of neural networks.
arxiv:1611.07270..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin interna-learning of language representations.
tional conference on learning representations..5349tao lei, regina barzilay, and tommi jaakkola.
2016.rationalizing neural predictions.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 107–117..marco tulio ribeiro, sameer singh, and carlosguestrin.
2018. anchors: high-precision model-in aaai, volume 18, pagesagnostic explanations.
1527–1535..zachary c lipton.
2018. the mythos of model inter-pretability: in machine learning, the concept of in-terpretability is both important and slippery.
queue,16(3):31–57..michael l rich.
2016. machine learning, automatedsuspicion algorithms, and the fourth amendment.
university of pennsylvania law review, pages 871–929..scott m lundberg and su-in lee.
2017. a uniﬁedin ad-approach to interpreting model predictions.
vances in neural information processing systems,pages 4765–4774..marko robnik- ˇsikonja and marko bohanec.
2018.perturbation-based explanations of prediction mod-in human and machine learning, pages 159–els.
175. springer..david martens and foster provost.
2014. explain-ing data-driven document classiﬁcations.
mis quar-terly, 38(1):73–100..tim miller.
2018.structural-modelarxiv:1811.03163..contrastive explanation: apreprint.
arxiv.
approach..gr´egoire montavon, wojciech samek, and klaus-robert m¨uller.
2018. methods for interpreting andunderstanding deep neural networks.
digital signalprocessing, 73:1–15..guido f montufar, razvan pascanu, kyunghyun cho,and yoshua bengio.
2014. on the number of lin-ear regions of deep neural networks.
in advances inneural information processing systems, pages 2924–2932..john x. morris, eli liﬂand, jin yong yoo, jakegrigsby, di jin, and yanjun qi.
2020. textattack:a framework for adversarial attacks, data augmenta-tion, and adversarial training in nlp..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled version ofbert: smaller, faster, cheaper and lighter.
in neuripsworkshop on emc2..patrick schwab and walter karlen.
2019. cxplain:causal explanations for model interpretation underuncertainty.
in advances in neural information pro-cessing systems, pages 10220–10230..cansu sen, thomas hartvigsen, biao yin, xiangnankong, and elke rundensteiner.
2020. human at-tention maps for text classiﬁcation: do humans andneural networks focus on the same words?
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4596–4608..soﬁa serrano and noah a. smith.
2019. is attentionin proceedings of the 57th annualinterpretable?
meeting of the association for computational lin-guistics, pages 2931–2951, florence, italy.
associa-tion for computational linguistics..ahmed osman, leila arras, and wojciech samek.
2020. towards ground truth evaluation of visual ex-planations.
arxiv:2003.07258..lloyd s shapley.
1953. a value for n-person games.
contributions to the theory of games, 2(28):307–317..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andmichael auli.
2019. fairseq: a fast, extensiblein proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations, pages 48–53..bhargavi paranjape, mandar joshi, john thickstun,hannaneh hajishirzi, and luke zettlemoyer.
2020.an information bottleneck approach for controllingconciseness in rationale extraction.
in proceedingsof emnlp, pages 1938–1952..avanti shrikumar, peyton greenside, and anshul kun-daje.
2017. learning important features throughin proceed-propagating activation differences.
ings of machine learning research, volume 70, in-ternational convention centre, sydney, australia.
pmlr..karen simonyan, andrea vedaldi, and andrew zisser-man.
2013. deep inside convolutional networks: vi-sualising image classiﬁcation models and saliencymaps.
arxiv preprint arxiv:1312.6034..jorge p´erez, javier marinkovi´c, and pablo barcel´o.
2019. on the turing completeness of modern neuralnetwork architectures.
in international conferenceon learning representations..dylan slack, sophie hilgard, sameer singh, andhimabindu lakkaraju.
2020. how much should itrust you?
modeling uncertainty of black box expla-nations.
arxiv preprint arxiv:2008.05030..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016. why should i trust you?
: explain-in proceed-ing the predictions of any classiﬁer.
ings of the 22nd acm sigkdd international con-ference on knowledge discovery and data mining,pages 1135–1144.
acm..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew y ng,and christopher potts.
2013. recursive deep mod-els for semantic compositionality over a sentimenttreebank.
in proceedings of the 2013 conference onempirical methods in natural language processing,.
5350xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-in advances in neural information pro-siﬁcation.
cessing systems, pages 649–657..pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..mukund sundararajan, ankur taly, and qiqi yan.
2017.in pro-axiomatic attribution for deep networks.
ceedings of the 34th international conference onmachine learning - volume 70, page 3319–3328.
jmlr.org..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..junlin wang, jens tuyls, eric wallace, and sameersingh.
2020. gradient-based analysis of nlp mod-in findings of the associationels is manipulable.
for computational linguistics: emnlp 2020, on-line.
association for computational linguistics..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019. neural network acceptability judgments.
transactions of the association for computationallinguistics, 7:625–641..sarah wiegreffe and yuval pinter.
2019. attention isnot not explanation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 11–20, hong kong, china.
associ-ation for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2019.huggingface’s transformers: state-of-the-art naturallanguage processing.
arxiv, abs/1910.03771..rf woolson.
2007. wilcoxon signed-rank test.
wiley.
encyclopedia of clinical trials, pages 1–3..zhengxuan wu and desmond c ong.
2021. on ex-plaining your explanations of bert: an empiricalstudy with sequence classiﬁcation.
arxiv preprintarxiv:2101.00196..omar zaidan, jason eisner, and christine piatko.
2007.using “annotator rationales” to improve machinelearning for text categorization.
in human languagetechnologies 2007: the conference of the northamerican chapter of the association for computa-tional linguistics; proceedings of the main confer-ence, pages 260–267..matthew d zeiler and rob fergus.
2014. visualizingand understanding convolutional networks.
in euro-pean conference on computer vision, pages 818–833.
springer..5351appendix a datasets statistics.
train/dev/testdatasets108000/12000/7600news6920/872/1821sst8551/527/516colaimdb-r 2864/320/200.
avg.
len38178666.table 6: dataset statistics used in the experiments..appendix b baseline explanation.
methods (a).
in this section, we describe our experimental setupsfor the six baselines..• occlusion (zeiler and fergus, 2014; schwaband karlen, 2019).
the occlusion method con-verts xxx into ˜x˜x˜x by masking token xi with a pre-deﬁned token.
the weight of xi is then deter-mined by the difference of the output or loss fromfθθθ(˜x˜x˜x) and fθθθ(xxx).
in our experiments, we use themask token from the corresponding black-boxtokenizer, and measure the feature weight basedon the changes between the loss functions beforeand after the masking.
the time complexity forthis baseline is o(|xxx|) at test time..• gradient (simonyan et al., 2013).
the weightof token xi is given by the accumulated gradientsof the highest probable prediction with regardsto each dimension of the token in the embed-ding layer.
we also multiply the correspondingembedding value before accumulation (kinder-mans et al., 2016).
gathering the weights ofall features in xxx requires one pass of backwardpropagation, and the time complexity for thisbaseline is dependent on the size of black-boxmodel at test time..• lrp (bach et al., 2015).
layer-wise rele-vance propagation decomposes a model’s out-come from the output layer into relevance scoresof neurons in each intermediate layer until itreaches features in the input layer.
the rule ofdecomposition is subject to the type of kerneland connectivity between neurons in adjacentlayers, such as linear or attention layers.
we fol-low the implementation by wu and ong (2021)to measure relevance score in variations of bertmodel (devlin et al., 2018)..• lime (ribeiro et al., 2016).
lime samples theneighbors of xxx by perturbing different xi, anduses these samples to learn a linear separatorwhich approximates the local behavior of the.
black-box fθθθ.
the weight of each xi is thengiven by the coefﬁcients of the separator..• kernel shap (lundberg and lee, 2017).
theshapley value (shapley, 1953) is a concept fromcooperative game theory which calculates theweight of feature xi by considering its interac-tion with all the other subsets of features.
ker-nel shap approximates the shapley value byweighted sampling (kernel).
the kernel is deter-mined by the number of permutations of features.
according to lundberg and lee (2017), limeand kernel shap only differ in the choice ofkernel.
we use cosine similarity in lime andthe size of subset permutations in kernel shapfor the kernel computation..• deep shap (shrikumar et al., 2017).
this is an-other method to approximate the shapley value.
it computes the weight of xi as the effect on theoutput when xi is set to a reference value.
suchan effect is achieved by linearizing the black-boxmodel through back-propagation.
hence, thecomplexity of deep shap is dependent on boththe size of reference samples and the black-box,which makes it the most computationally expen-sive method among all our baselines.
in ourexperiments, we use the api provided by lund-berg and lee (2017) and set the reference valueof xi to the corresponding value in each of therandomly selected samples.
a sample size of500/1000/1000/1000 respectively for datasetsimdb-r, agnews, sst and cola is used inthe baselines requiring sampling – lime, kernelshap and deep shap..appendix c document similarity forintersection over union.
the ﬁrst approach for computing iou uses semanticsimilarity between two documents.
this is mea-sured by summing the token representations alonghidden dimensions from a pre-trained bert basemodel with uncased english, open-sourced by hug-ging face (wolf et al., 2019)..our second approach is to compute the intersec-tion over union for overlapping n-grams betweentwo documents, referred to as lexical similarity.
inour experiment, we sum this value up to 4-gramsin two documents as the score of similarity.
theresults from this approach are reported in table 7..5352random.
occlusion.
gradient.
lrp.
lime.
k’ shap.
d’ shap.
models.
al2eal2eal2eal2eal2eal2e.
rankingnewscolasstnews13.11±0.5711.94±0.926.34±0.547.36±0.3416.83±3.3712.36±1.267.5±1.238.15±1.3523.37±3.71 16.8±1.2610.51±1.63 14.11±1.8611.95±2.3310.32±0.967.05±1.115.14±1.0522.52±3.81 17.35±1.1611.92±1.79 13.8±1.712.37±2.2716.33±1.232.67±0.5810.34±1.6421.87±3.85 17.6±1.2112.52±1.6111.87±1.929.83±0.978.65±1.2216.8±3.196.42±1.1716.71±1.2513.89±2.02 11.23±1.61 18.9±3.856.96±1.1322.17±4.025.74±1.318.85±1.4421.92±4.03 10.79±1.398.79±1.5214.59±1.7719.74±3.917.2±1.058.35±0.895.31±1.1323.77±3.62 12.53±1.1610.98±1.27 4.93±1.14.
sequence labeling.
sst13.1±0.5113.5±1.1915.56±0.8911.5±0.8713.77±0.812.76±0.8414.84±0.7313.92±1.1316.24±0.744.8±1.05.67±1.0111.88±1.0416.24±0.71.
cola23.35±1.4528.78±3.5131.78±3.5613.36±1.9525.77±3.3826.69±3.1631.8±3.5520.56±3.3830.48±3.5325.9±4.7825.12±4.6914.38±3.3825.38±3.65.
table 7: intersection over union (iou) using lexical similarity (measured according to overlapping n-grams); boldindicates statistical signiﬁcance..appendix d faithfulness.
methods models newsrandom.
we present the negative explanation words of thesequence labeling formulation in table 8 and thetop important words of ranking formulation intable 9..models news.
negative ∆log-odds ↓.
random.
occ..grad..lime.
k’ shap.
d’ shap.
al2eal2eal2eal2eal2e.
0.57±0.11-0.69±0.36-0.39±0.40.12±0.210.05±0.29-0.4±0.26-0.41±0.271.57±0.70.46±0.330.83±0.441.19±0.6.
cola.
sst1.93±0.17 1.92±0.11.47±1.11.57±1.150.99±0.55 1.53±0.360.87±0.41 1.48±0.331.51±1.24 1.05±0.711.03±0.95 1.04±0.763.78±0.911.8±0.760.37±0.52 1.67±0.291.93±0.220.5±0.78.
table 8: negative ∆log-odds when employing the se-quence labeling formulation; bold indicates statisticalsigniﬁcance; the experimental results show that l2enever performs signiﬁcantly worse than a; lrp is notincluded because all words were considered to be posi-tively important..appendix e computational efﬁciency.
figures 4, 5 and 6 show that l2e is more efﬁcientthan all baselines for agnews, sst and coladatasets..appendix f imdb dataset with.
human-annotated rationales.
there are 900 positive and 900 negative movie re-views with rationales annotated by human in theoriginal dataset from zaidan et al.
(2007).
werandomly assign 160 and 200 examples to the vali-.
occ..grad..lrp.
lime.
k’ shap.
d’ shap.
al2eal2eal2eal2eal2eal2e.
sst.
cola.
0.57±0.11 1.93±0.17 1.92±0.13.03±0.79 4.84±0.86 1.87±0.231.78±0.67 3.08±0.75 1.9±0.231.13±0.56 2.17±0.66 1.95±0.220.91±0.54 1.74±0.56 1.88±0.232.89±0.78 4.41±0.86 1.93±0.222.43±0.65 2.64±0.68 1.87±0.236.96±1.02 5.01±0.89 1.86±0.225.03±0.95 2.93±0.62 1.89±0.231.97±0.234.51±1.19 0.2±0.321.8±0.772.25±0.67 2.03±0.230.22±0.29 2.74±0.68 1.92±0.230.57±0.33 2.98±0.77 1.85±0.23.
table 9: ∆log-odds when employing the ranking for-mulation; bold indicates statistical signiﬁcance..dation and test set respectively, with each set hav-ing an even distribution of positive and negativereviews.
we also remove 8 very long documentsfrom the training set for the sake of cuda memory.
for each example in the training and validation sets,we construct a new example by masking the ratio-nales, i.e., we replace each words in the rationalewith a mask token, and assign this new exampleto a third label, e.g., neutral, so as to ensure theclassiﬁer ‘pays attention’ to the rationale.
the ﬁnaldataset split appears in table 6..the classiﬁer is trained by ﬁne-tuning the lastlayer of a pre-trained longformer (beltagy et al.,2020) with 12 layers and 12 attention heads fromhugging face (wolf et al., 2019).
it achieves83.83%/79.68%/74.5% accuracy for the train-ing/validation/test sets respectively after 40 epochs.
the statistics of our experiment are measured ontest examples that are predicted correctly by theclassiﬁer.
for each l2e explainer that learns from abaseline explanation method, we use a longformerwith 4 layers, 4 attention heads..53537 ± 1 .
421.
69 .
1.
4 .
2.
0.
1.
8.
9 ± 6 .
5.
3.
7 ± 0 .
2.
1 .
9.
5.
3 ± 0 .
0.
0 .
70.
6 ± 0 .
0.
0 .
11.
6 ± 0 .
0.
0 .
0*.
0.
1 ± 0 .
0.
0 .
0.l2e.
gradient.
l r p.o cclusion.
li m e.d’s h a p.k’s h a p.figure 4: average inference time for the six baselineexplanation algorithms and ours (l2e) for the same100 documents on the news dataset; lower is better;* indicates statistical signiﬁcance..])s(.
2gol[.
em.it.etupmoc.28.
24.
20.
2−4.
2−8.]
)s(.
2gol[.
em.it.etupmoc.28.
24.
20.
2−4.
2−8.
appendix g precision and recall on.
positive reviews.
we plot the precision versus recall from all the l2e-a pairs in dataset imdb-r in figure 7. the resultsshow that, in most case, l2e performs better thana in terms of faithfulness to the underlying black-box and alignment with the human rationales..2 1 .
5 7 ± 0 .
2 7.
9 .
3 6 ± 1 .
9 8.
1 .
4 ± 0 .
1 2.
0 .
1 8 ± 0 .
0 2.
0 .
0 8 ± 0 .
0 1.
0 .
0 2 ± 0 .
0.
0 .
0 1 ± 0 .
0 *.
1 8 .
4 1 ± 0 .
3 7.
1 .
4 4 ± 0 .
0 9.
0 .
4 5 ± 0 .
3 2.
0 .
1 7 ± 0 .
0 2.
0 .
1 9 ± 0 .
0 1.]
)s(.
2gol[.
em.it.etupmoc.24.
21.
2−2.
2−5.
2−8.
0 .
0 5 ± 0 .
0 2.
0 .
0 1 ± 0 .
0 *.
l2e.
gradient.
l r p.o cclusion.
li m e.k’s h a p.d’s h a p.l2e.
gradient.
o cclusion.
l r p.k’s h a p.li m e.d’s h a p.figure 5: average inference time for the six baselineexplanation algorithms and ours (l2e) for the same100 documents on the sst dataset; lower is better; *indicates statistical signiﬁcance..figure 6: average inference time for the six baselineexplanation algorithms and ours (l2e) for the same100 documents on the cola dataset; lower is better;* indicates statistical signiﬁcance..5354figure 7: precision and recall of l2e versus each of the six baselines for all correctly predicted positive reviewsfrom imdb-r test..5355