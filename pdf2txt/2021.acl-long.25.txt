learning language speciﬁc sub-network for multilingual machinetranslation.
zehui lin∗, liwei wu∗, mingxuan wang, lei libytedance ai lab{linzehui,wuliwei.000,wangmingxuan.89,lileilab}@bytedance.com.
abstract.
multilingual neural machine translation aimsat learning a single translation model for mul-tiple languages.
these jointly trained mod-els often suffer from performance degradationon rich-resource language pairs.
we attributethis degeneration to parameter interference.
inthis paper, we propose lass to jointly train asingle uniﬁed multilingual mt model.
lasslearns language speciﬁc sub-network (lass)for each language pair to counter parameterinterference.
comprehensive experiments oniwslt and wmt datasets with various trans-former architectures show that lass obtainsgains on 36 language pairs by up to 1.2 bleu.
besides, lass shows its strong generalizationperformance at easy adaptation to new lan-guage pairs and zero-shot translation.
lassboosts zero-shot translation with an averageof 8.3 bleu on 30 language pairs.
codesand trained models are available at https://github.com/nlp-playground/lass..1.introduction.
neural machine translation (nmt) has been verysuccessful for bilingual machine translation (bah-danau et al., 2015; vaswani et al., 2017; wu et al.,2016; hassan et al., 2018; su et al., 2018; wang,2019).
recent research has demonstrated the efﬁ-cacy of multilingual nmt, which supports transla-tion from multiple source languages into multipletarget languages with a single model (johnson et al.,2017; aharoni et al., 2019; zhang et al., 2020; fanet al., 2020; siddhant et al., 2020).
multilingualnmt enjoys the advantage of deployment.
further,the parameter sharing of multilingual nmt encour-ages transfer learning of different languages.
anextreme case is zero-shot translation, where directtranslation between a language pair never seen intraining is possible (johnson et al., 2017)..∗equal contribution..(a) full network.
(b) lass.
figure 1: illustration of a full network and language-speciﬁc ones (lass).
— represents shared weights.
—, — and — represents weights for en→zh, en→frand en→de, respectively.
compared to the full multi-lingual model, each lass learned model has languageuniversal and language speciﬁc weights..while very promising, several challenges remainin multilingual nmt.
the most challenging one isrelated to the insufﬁcient model capacity.
sincemultiple languages are accommodated in a singlemodel, the modeling capacity of nmt model hasto be split for different translation directions (aha-roni et al., 2019).
therefore, multilingual nmtmodels often suffer from performance degrada-tion compared with their corresponding bilingualbaseline, especially for rich-resource translationdirections.
the simplistic way to alleviate the in-sufﬁcient model capacity is to enlarge the modelparameters (aharoni et al., 2019; zhang et al.,2020).
however, it is not parameter or computa-tion efﬁcient and needs larger multilingual train-ing datasets to avoid over-ﬁtting.
an alternativesolution is to design language-aware components,such as division of the hidden cells into sharedand language-dependent ones (wang et al., 2018),adaptation layers (bapna and firat, 2019; philipet al., 2020), language-aware layer normalization.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages293–305august1–6,2021.©2021associationforcomputationallinguistics293enzhenfrendeenzhenfrendeand linear transformation (zhang et al., 2020), andlatent layers (li et al., 2020)..(2020) uses a binary conditional latent variable todecide which language each layer belongs to..in this work, we propose lass, a method todynamically ﬁnd and learn language speciﬁc sub-network for multilingual nmt.
lass accommo-dates one sub-network for each language pair.
eachsub-network has shared parameters with some otherlanguages and, at the same time, preserves its lan-guage speciﬁc parameters.
in this way, multilingualnmt can model language speciﬁc and languageuniversal features for each language pair in onesingle model without interference.
figure 1 is theillustration of vanilla multilingual model and lass.
each language pair in lass has both language uni-versal and language speciﬁc parameters.
the net-work itself decides the sharing strategy..the advantages of our proposed method are.
• lass is parameter efﬁcient, requiring no ex-tra trainable parameters to model languagespeciﬁc features..• lass alleviates parameter interference, po-tentially improving the model capacity andboosting performance..• lass shows its strong generalization perfor-mance at easy adaptation to new languagepairs and zero-shot translation.
lass can beeasily extended to new language pairs with-out dramatic degradation of existing languagepairs.
besides, lass can boost zero-shot trans-lation by up to 26.5 bleu..2 related work.
multilingual neural machine translationthe standard multilingual nmt model uses ashared encoder and a shared decoder for differentlanguages (johnson et al., 2017).
there is atransfer-interference trade-off in this architec-ture (arivazhagan et al., 2019): boosting theperformance of low resource languages or main-tain the performance of high resource languages.
to solve this trade-off, previous works assignsome parts of the model to be language speciﬁc:language speciﬁc decoders (dong et al., 2015),language speciﬁc encoders and decoders (firatet al., 2016; lyu et al., 2020) and language speciﬁchidden states and embeds (wang et al., 2018).
sachan and neubig (2018) compares differentsharing methods and ﬁnds different sharingmethods have a great impact on performance.
recently, zhang et al.
(2021) analyze when andwhere language speciﬁc capacity matters.
li et al..model pruning our approach follows the stan-dard pattern of model pruning: training, ﬁndingthe sparse network and ﬁne-tuning (frankle andcarbin, 2019; liu et al., 2019).
frankle and carbin(2019) and liu et al.
(2019) highlight the impor-tance of the sparse network architecture.
zhu andgupta (2018) proposed a method to automaticallyadjust the sparse threshold.
sun et al.
(2020) learnsdifferent sparse architecture for different tasks.
evci et al.
(2020) iteratively redistribute the sparsenetwork architecture by the gradient..3 methodology.
we describe lass method in this section.
thegoal is to learn a single uniﬁed model for manytranslation directions.
our overall idea is to ﬁndsub-networks corresponding to each language pair,and then only update the parameters of those sub-networks during the joint training..3.1 multilingual nmt.
a multilingual nmt model learns a mapping func-tion f from a sentence in one of many languagesto another language.
we adopt the multilingualtransformer (mtransformer) as the backbone net-work (johnson et al., 2017).
mtransformer has thesame encoder-decoder architecture with layers ofmultihead attention, residual connection, and layernormalization.
in addition, it has two lanuage iden-tifying tokens for the source and target.
deﬁne amultilingual dataset {dsi→ti}ni=1 where si, ti rep-resents the source and target language..we train an initial multilingual mt model with.
the following loss..(cid:88).
(cid:88).
l =.
− log pθ(y | x).
(1).
i.
(cid:104)x,y(cid:105)∼dsi→ti.
where (cid:104)x, y(cid:105) is a sentence pair from the languagesi to ti, and θ is the model parameter..3.2 finding language speciﬁc model masks.
training a single model jointly on multiple lan-guage directions will lead to performance degrada-tion for rich resource pairs (johnson et al., 2017).
the single model will improve on low resource lan-guage pairs, but will reduce performance on pairslike english-german.
intuitively, jointly trainingon all translation pairs will obtain an “average”.
294model.
for rich resources, such averaging may hurtthe performance since a multilingual mt modelmust distribute its modeling capacity for all trans-lation directions.
based on this intuition, our ideais to ﬁnd a sub-network of the original multilin-gual model.
such sub-network is speciﬁc to eachlanguage pair..we start from a multilingual base model θ0.
theθ0 is trained with eq.
(1).
a sub-network is indi-cated by a binary mask vector msi→ti ∈ {0, 1}|θ|for language pair si → ti.
each element being 1indicates to retain the weight and 0 to abandonthe weight.
then the parameters associated withsi → ti is θsi→ti = {θj= 1}, wherej denotes the jth element in θ0.
the parametersθsi→ti are only responsible for the particular lan-guage si and ti.
we intend to ﬁnd such languagespeciﬁc sub-networks.
figure 1 illustrates the orig-inal model and its language speciﬁc sub-networks.
given an initial model θ0, we adopt a simplemethod to ﬁnd the language speciﬁc mask for eachlanguage pairs..0 | mj.
si→ti.
trained on {dsi→ti}n.1. start with a multilingual mt model θ0 jointlyi=1.
2. for each language pair si → ti, ﬁne-tuningθ0 on dsi→ti.
intuitively, ﬁne-tuning θ0 onspeciﬁc language pair si → ti will amplifythe magnitude of the important weights forsi → ti and diminish the magnitude of theunimportant weights..3. rank the weights in ﬁne-tuned model andprune the lowest α percent.
the mask msi→tiis obtained by setting the remaining indices ofparameters to be 1..3.3 structure-aware joint training.
once we get masks msi→ti for all language pairs,we further continue to train θ0 with language-grouped batching and structure-aware updating..first, we create random batches of bilingual sen-tence pairs where each batch contains only samplesfrom one pair.
this is different from the plain jointmultilingual training where each batch can containfully random sentence pairs from all languages.
speciﬁcally, a batch bsi→ti is randomly drawnfrom the language-speciﬁc data dsi→ti.
second,we evaluate the loss in eq.
1 on the batch bsi→ti.
during the back-propagation step, we only updatethe parameters in θ0 belonging to the sub-networkindicated by msi→ti.
we iteratively update the pa-rameters until convergence..in this way, we still get a single ﬁnal model θ∗.
that is able to translate all language directions..during the inference, this model θ∗ and its masksmsi→ti, i = 1, .
.
.
, n are used together to makepredictions.
for every given input sentence in lan-guage s and a target language t, the forward infer-ence step only uses the parameter θ∗ (cid:12) ms→t tocalculate model output..4 experiment settings.
datasets and evaluation the experiments areconducted on iwslt and wmt benchmarks.
foriwslt, we collect 8 english-centric language pairsfrom iwslt2014, whose size ranges from 89kto 169k.
to simulate the scenarios of imbalanceddatasets, we collect 18 language pairs ranging fromlow-resource (gu, 11k) to rich-resource (fr, 37m)from previous years’ wmt.
the details of thedatasets are listed in appendix.
we apply byte pairencoding (bpe) (sennrich et al., 2016) to prepro-cess multilingual sentences, resulting in a vocab-ulary size of 30k for iwslt and 64k for wmt.
besides, we apply over-sampling for iwslt andwmt to balance the training data distribution witha temperature of t = 2 and t = 5 respectively.
similar to lin et al.
(2020), we divide the lan-guage pairs into 3 categories: low-resource (<1m),medium-resource (>1m and <10m) and rich re-source (>10m)..we perform many-to-many multilingual trans-lation throughout this paper, and add special lan-guage tokens at both the source and the targetside.
in all our experiments, we evaluate our modelwith commonly used standard testsets.
for zero-shot, where standard testsets (for example, fr→zh)of some language pairs are not available, we useopus-100 (zhang et al., 2020) testsets instead..we report tokenized bleu, as well as win ratio(wr), informing the proportion of language pairswe outperform the baseline.
in zero-shot transla-tion, we also report translation-language accuracy1,which is commonly used to measure the accuracyof translating into the right target language..model settings considering the diversity ofdataset volume, we perform our experiments withvariants of transformer architecture.
for iwslt,we adopt a smaller transformer (transformer-small2 (wu et al., 2019)).
for wmt, we adopt.
1https://github.com/mimino666/.
langdetect.
2transformer-base with df f = 1024 and nhead = 4.
295langsize.
fa89k.
baselinelass.
16.917.9∆ +1.0.
pl128k.
16.417.0+0.6.
ar.
he.
140k 144k.
20.922.9+2.0.
2930.9+1.9.
langsize.
nl.
de.
it.
es.
153k 160k 167k 169k.
baselinelass.
30.933.0∆ +2.1.
28.129.8+1.7.
29.230.9+1.7.
35.237.3+2.1.
table 1: results on iwlst dataset.
baseline denotesthe multilingual transformer-small baseline model.
lass consistently outperforms multilingual baselineon all language pairs.
we report the average bleu ofen→x and x→en within one language.
both the base-line and lass have the same number of parameters..transformer-base and transformer-big3.
the prun-ing rate α of iwslt and wmt is 0.7 and 0.3, re-spectively.
for simplicity, we only report the high-est bleu from the best pruning rate and we alsodiscuss the impact of different pruning rate on per-formance in sec.6.
in sec.
6 we discuss the rela-tionship of performance and pruning rate.
for moretraining details please refer to appendix..5 experiment results.
this section shows the efﬁcacy and generalizationof lass.
firstly, we show that lass obtains con-sistent performance gains on iwslt and wmtdatasets with different transformer architecturevariants.
further, we show that lass can easilygeneralize to new language pairs without losingthe accuracy for previous language pairs.
finally,we observe that lass can even improve zero-shottranslation, obtaining performance gains by up to26.5 bleu..5.1 main results.
results on iwslt we ﬁrst show our results oniwslt.
as shown in table 1, lass consistentlyoutperforms the multilingual baseline on all lan-guage pairs, conﬁrming that using lass to alleviateparameter interference can help boost performance..wmt, where the dataset is more imbalanced acrossdifferent language pairs.
we adopt two differenttransformer architecture variants, i.e., transformer-base and transformer-big..as shown in table 2, lass obtains consis-tent gains over multilingual baseline on wmt forboth transformer-base and transformer-big.
fortransformer-base, lass achieves an average im-provement of 1.2 bleu on 36 language pairs overbaseline, while for transformer-big, lass obtains0.6 bleu improvement..we observe that with the dataset scale of lan-guage pairs increasing, the improvements of bleuand wr become larger, suggesting that the lan-guage pairs with large scale dataset beneﬁt morefrom lass than language pairs of low resource.
this phenomenon is intuitive since rich resourcedataset suffers more parameter interference thanlow resource dataset.
we also ﬁnd that the bleuand wr gains obtained in transformer-base arelarger than that in transformer-large.
we attributeit to the more severe parameter interference forsmaller models..for comparison, we also include the results oflass with randomly initialized masks.
not sur-prising, random underperforms the baseline by alarge margin, since random intensiﬁes rather thanalleviates the parameter interference..5.2 generalization to new language pairs.
lass has shown its efﬁcacy in the above section.
a natural question arises that can lass adapt to anew language or language pair that it has not seenin training phase?
in other words, can lass gen-eralize to other language pairs?
in this section, weshow the generalization of lass in two settings.
we ﬁrstly show that lass can easily adapt to newunseen languages to match bilingual models withtraining for only a few hundred steps while keep-ing the performance of the existing language pairshardly dropping.
secondly, we show that lass canalso boost performance in zero-shot translationscenario, obtaining performance gains by up to26.5 bleu..the model is transformer-big trained on wmtdataset.
en↔ar and en↔it are both unseen lan-guage pairs..results on wmt to further verify the general-ization of lass, we also conduct experiments on.
3for details of the transformer setting, please refer to.
vaswani et al.
(2017).
5.2.1 extensibility to new languages.
previous works have studied the easy and rapidadaptation to a new task or language pair (bapnaand firat, 2019; rebufﬁ et al., 2017).
we show.
296arch setting.
transformer-base.
transformer-big.
low.
rich.
model.
mediumbleu wr bleu wr bleu wrbaseline16.7random -2.2+0.7lassbaseline18.8random -1.3+0.1lass.
25.3-0.0-2.685.7 +1.729.0-0.0-1.592.9 +0.8.
18.8-0.0-2.380.0 +1.322.2-0.0-1.850.0 +0.7.
allbleu wr20.4-0.0-2.4100.0 +1.223.5-0.0-1.6100.0 +0.6.
0.088.9-0.083.3.table 2: average bleu↑ and win ratio (wr) of wmt dataset on low (<1m), medium (1m∼10m) and rich(>10m) resource dataset.
random denotes lass with random masks.
lass obtains consistent gains for bothtransformer-big and transformer-base..that lass can also easily adapt to new unseenlanguages without dramatic drop for other exist-ing languages.
we distribute a new sub-network toeach new language pair and train the sub-networkwith the speciﬁc language pair for ﬁxed steps.
inthis way, the new language pair will only updatethe corresponding parameters and it can alleviatethe interference and catastrophic forgetting (kirk-patrick et al., 2016) to other language pairs..we verify the extensibility of lass on 4 lan-guage pairs.
for lass, as described in sec.3, weﬁrst ﬁne-tune the multilingual base model andprune to obtain the speciﬁc mask for the new lan-guage pair.
for both multilingual baseline and ourmethod, we train on only the speciﬁc language pairfor ﬁxed steps..figure 2 shows the trend of bleu score alongwith the training steps.
we observe that 1) lassconsistently outperforms the multilingual baselinemodel along with the training steps.
lass reachesthe bilingual model performance with fewer steps.
2) besides, the degradation of other language pairsis much smoother than the baseline.
when reachingthe bilingual baseline performance, lass hardlydrops on other language pairs, while the multilin-gual baseline model dramatically drops by a largemargin..we attribute the easy adaptation for speciﬁc lan-guages to the language speciﬁc sub-network.
lassonly updates the corresponding parameters, avoid-ing updating all parameters which will hurt theperformance of other languages.
another beneﬁtof updating corresponding parameters is its fastadaptation towards speciﬁc language pairs..5.2.2 zero-shot.
zero-shot translation is the translation betweenknown languages that the model has never seen.
(a) en→it.
(b) it→en.
(c) en→ar.
(d) ar→en.
figure 2: the trend of bleu score of new extendedlanguage pairs and other existing language pairs alongwith the training steps on the speciﬁc language pair.
compared to multilingual baseline, lass reaches thebilingual performance with fewer steps and only lit-tle performance degradation on other existing languagepairs..together at training time (e.g., fr→en and en→zhare both seen in training phase, while fr→zh isnot.).
it is the ultimate goal of multilingual nmtand has been a common indicator to measure themodel capability (johnson et al., 2017; zhang et al.,2020).
one of the biggest challenges is the off-target issue (zhang et al., 2020), which means thatthe model translates into a wrong target language.
in previous experiments, we apply speciﬁcmasks to their corresponding language pairs.
asthe training dataset is english-centric, non-english-centric masks are not available.
we remedy it bymerging two masks to create non-english-centricmasks.
for example, we create x→y mask bycombining the encoder mask of x→en and the.
29702505007501000steps102030bleubilingualmodellassbaselinedirectionen2it_bleuavg_other_bleu02505007501000steps2025303540bleubilingualmodellassbaselinedirectionit2en_bleuavg_other_bleu02505007501000steps0510152025bleubilingualmodellassbaselinedirectionen2ar_bleuavg_other_bleu02505007501000steps0102030bleubilingualmodellassbaselinedirectionar2en_bleuavg_other_bleu(a) en→x (x-axisand y-axis).
(b) x→en (x-axisand y-axis).
(c) x→en (x-axis)en→x(y-axis).
figure 3: mask similarity for language pairs withinen→x (x-axis and y-axis), within x→en (x-axis and y-axis) and between en→x (x-axis) and x→en (y-axis),respectively.
the mask similarity is positively corre-lated to the language family similarity..decoder mask of en→y.
we select 6 languagesand evaluate zero-shot translation in language pairsbetween each other..as shown in table 3, surprisingly, by directly ap-plying x→y masks, lass obtains consistent gainsover baselines in all language pairs for both bleuand translation-language accuracy, indicating thatthe superiority of lass in learning to bridge be-tween languages.
it is worth noting that for fr→zh,lass outperforms the baseline by 26.5 bleu,reaching 32 bleu..we also sample a few translation examples fromfr→zh to analyze why lass can help boost zero-shot (more examples are listed in appendix)..as shown in table 4 as well as translation-language accuracy in table 3, we observe that themultilingual baseline has severe off-target issue.
as a counterpart, lass signiﬁcantly alleviates theoff-target issue, translating into the right target lan-guage.
we attribute the success of “on-target” inzero-shot to the language speciﬁc parameters as astrong signal, apart from language indicator, to themodel to translate into the target language..6 analysis and discussion.
in this section, we conduct a set of analytic ex-periments to better understand the characteristicsof language speciﬁc sub-network.
we ﬁrst mea-sure the relationship between language speciﬁc sub-network as well as its capacity and language family.
secondly, we study how masks affect performancein zero-shot scenario.
lastly, we discuss the rela-tionship between pruning rate α and performance.
we conduct our analytic experiments on iwsltdataset.
for readers not familiar with language fam-ily and clustering, figure 4 is the hierarchical clus-tering according to language family..figure 4: language clustering of 8 languages iniwslt, according to language family.
es(spanish),it(italian), de(germany), nl(dutch) and pl(polish) areall european languages and written in latin whilear(arabic), fa(farsi) and he(hebrew) are similar lan-guages..6.1 mask similarity v.s language family.
ideally, similar languages should share more pa-rameters since they share more language charac-teristics.
therefore, a natural question arises: doesthe model automatically capture the relationship oflanguage family deﬁned by human?.
we calculate the similarity of masks betweenlanguage pairs to measure the sub-network rela-tionship between language pairs.
we deﬁne masksimilarity as the number of 1 where two masksshare divided by the number of 1 of the ﬁrst mask:.
sim(m1, m2) =.
(cid:107)m1 ∩ m2(cid:107)0(cid:107)m1(cid:107)0.,.
(2).
where (cid:107)·(cid:107)0 represent l0 norm.
mask similarity re-ﬂects the degree of sharing among different lan-guage pairs..figure 3(a) and 3(b) shows the mask similarityin en→x and x→en.
we observe that, for bothen→x and x→en, the mask similarity is posi-tively correlated to the language family similarity.
the color of grids in figure is deeper between sim-ilar languages (for example, es and it) while moreshallow between dissimilar languages (for example,es and he)..we also plot the similarity between en→x andx→en in figure 3(c) .
we observe that, unlikeen→x or x→en, the mask similarity does not cor-respond to language family similarity.
we suspectthat the mask similarity is determined by combi-nation of source and target languages.
that meansthat en→nl does not necessarily share more pa-rameters with nl→en than en→de..6.2 where language speciﬁc capacity.
matters?.
to take a step further, we study how model schedulelanguage speciﬁc capacity across layers.
figure 5.
298esitnldeplarfaheesitnldeplarfahe4748495051esitnldeplarfaheesitnldeplarfahe59606162636465esitnldeplarfaheesitnldeplarfahe4850525456esitnldeplarhefaromancegermanicslavicarabiciraniansemiticlatinlatinlatinlatinlatinarabicarabichebrewfr.
cs.
de.
es.
ru.
zh.
target languages.
bleu acc2.97.5.bleu acc1.54.6.fr.
cs.
de.
es.
ru.
zh.
baselinelass∆baselinelass∆baselinelass∆baselinelass∆baselinelass∆baselinelass∆.
bleu acc2.05.4+3.4---2.67.4.bleu acc---3.915.3+11.46.317.9+11.67.420.8+13.45.616.2+10.65.618+12.4.
---7.061.1+54.118.870.3+51.5 +4.817.566.3+48.8 +2.919.969.0+49.1 +5.64.053.2+49.2 +1.4.
2.48.0.
0.31.7.
2.04.9.
2.67.7+5.1---2.66.7.
1.732.6+30.9 +4.6---5.740.5+34.81.625.7+24.1 +4.18.147.7+39.6 +3.91.022.9+21.9 +0.1.
2.05.9.
1.11.2.segaugnalecruos.2.56.6.
2.26.1.
5.618.5.bleu acc15.16.477.723.0+62.6 +3.113.974.2+60.3 +4.114.075.1+61.1 +3.9---20.675.5+54.92.128.0+25.9 +2.4.
3.135.9+32.8 +16.62.137.2+35.1 +12.9---1.930.3+28.42.432.0+29.6 +12.51.67.1+5.5.
5.619.4+13.8---6.318.8.
1.44.5+3.1---4.87.2.
0.83.8+3.0.
bleu acc5.532.0+26.50.913.5.
5.716.1.
4.424.720.39.634.5+24.9 +12.68.633.2+24.6 +10.43.722.2+18.5 +11.6---5.627.6+22.0.
10.530.0+19.5---.
3.615.2.
4.931.3+26.40.935.3+34.419.641.6+22.09.242.8+33.613.433.1+19.7---.
table 3: bleu score and translation-language accuracy (acc, in percentage) of zero-shot translation for multi-lingual baseline and lass.
lass outperforms the multilingual baseline on both bleu and acc by a large marginfor most language pairs.
low accuracy indicates severe off-target translation..src.
la production annuelle d’acier était le sym-bole incontesté de la vigueur économiquedes nations.
钢的年产量是国家经济实力的重要象征refbaseline annual steel production was the undisputed.
src.
lass.
symbol of nations’ economic strength.
年度钢铁生产是各国经济活力的无可争辩的象征.
de l’avis de ma délégation donc, l’onudevrait élargir ces activités de la faon suiv-ante.
因此,我国代表团认为,联合国现在应该以下述方式扩大这些活动。baseline 因此, in my delegation’s view, the unitednations should expand these activities in thefollowing manner.
因此,我国代表团认为,联合国应该扩大这些活动,如下..lass.
ref.
table 4: fr→zh case study.
the multilingual baselinesuffers from severe off-target issue, while lass greatlyalleviates the issue..shows the similarity of different components on theencoder and decoder side along with the increaseof layer.
more concretely, we plot query, key, valueon the attention sub-layer and fully-connected layeron the positional-wise feed-forward sub-layer..we observe that a) on both the encoder anddecoder side, the model tends to distribute morelanguage speciﬁc components on the top and bot-.
tom layers rather than the middle ones.
this phe-nomenon is intuitive.
the bottom layers deal morewith embedding, which is language speciﬁc, whilethe top layers are near the output layer, which isalso language speciﬁc.
b) for fully-connected layer,the model tends to distribute more language spe-ciﬁc capacity on the middle layers for the encoder,while distribute more language speciﬁc capacity inthe decoder for the top layers..6.3 how masks affect zero-shot?.
in sec.4, we show that simply applying x→ymasks can boost zero-shot performance.
we con-duct experiments to analyze how masks affect zero-performance.
concretely, we take fr→zh as anexample, replacing the encoder or decoder maskwith another language mask, respectively..as shown in table 5, we observe that replacingthe encoder mask with other languages causes onlylittler performance drop, while replacing the de-coder mask causes dramatic performance drop.
itsuggests that the decoder mask is the key ingredientof performance improvement..6.4 about sparsity.
to better understand the pruning rate, we plot theperformance along with the increase of pruning.
299(a) iwslt.
(b) wmt.
(a) encoder.
(b) decoder.
figure 5: the mask similarity of different components(attention layer and feed-forward layer) on the encoderand decoder side along with the increase of layer.
themodel tends to distribute more language speciﬁc capac-ity on the top and bottom layers..fr-.
cs12.3.de13.8.es7.1.ru18.6.zh32.0.fr →x.
x → zh.
fr32.0.cs30.5.de29.6.es30.9.ru29.6.zh-.
table 5: performance of applying fr→x or x→zhmask to fr→zh testset.
replacing encoder maskcauses only little performance drop, while replacing de-coder mask causes dramatic performance drop..rate in figure 6. for wmt, the best choice for α is0.3 for both transformer-base and transformer-big,while for iwslt the best α lies between 0.6∼0.7.
the results are consistent with our intuition, thatlarge scale training data need a smaller pruning rateto keep the model capacity.
therefore, we suggesttuning α based on both the dataset and model size.
for large datasets such as wmt, setting a smallerα is better, while a larger α will slightly decreasethe performance (i.e.
less than 0.5 bleu score).
for small datasets like iwslt, setting a larger αmay yield better performance..7 conclusion.
figure 6: bleu score along with the increase ofpruning rate α. large α indicates small sub-network.
small dataset requires a larger α to yield better perfor-mance.
iwslt uses transformer-small and wmt usestransformer-base and transformer-big..nmt.
extensive experiments on iwslt and wmthave shown that lass is able to alleviate parameterinterference and boost performance.
further, lasscan generalize well to new language pairs by train-ing with a few hundred steps, while keeping the per-formance of existing language pairs.
surprisingly,in zero-shot translation, lass surpasses the multi-lingual baseline by up to 26.5 bleu.
extensive an-alytic experiments are conducted to understand thecharacteristics of language speciﬁc sub-network.
future work includes designing a more dedicatedend-to-end training strategy and incorporating theinsight we gain from analysis to design a furtherimproved lass..references.
roee aharoni, melvin johnson, and orhan firat.
2019.massively multilingual neural machine translation.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages3874–3884.
association for computational linguis-tics..naveen arivazhagan, ankur bapna, orhan firat,dmitry lepikhin, melvin johnson, maxim krikun,mia xu chen, yuan cao, george f. foster, colincherry, wolfgang macherey, zhifeng chen, andyonghui wu.
2019. massively multilingual neuralmachine translation in the wild: findings and chal-lenges.
corr, abs/1907.05019..in this paper, we propose to learn language-speciﬁc sub-network (lass) for multilingual.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointly.
300012345layer45505560similarityallattn_qattn_kattn_vattn_projfc012345layer45505560similarityallattn_qattn_kattn_vattn_projfc0.00.20.40.60.8alpha2627bleu23.5023.7524.00big0.00.10.20.30.40.5alpha20.521.021.5basein 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..ankur bapna and orhan firat.
2019. simple, scal-inable adaptation for neural machine translation.
proceedings ofthe 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 1538–1548. association for computational linguistics..daxiang dong, hua wu, wei he, dianhai yu, andhaifeng wang.
2015. multi-task learning for mul-in proceedings of thetiple language translation.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing of theasian federation of natural language processing,acl 2015, july 26-31, 2015, beijing, china, volume1: long papers, pages 1723–1732.
the associationfor computer linguistics..utku evci, trevor gale, jacob menick, pablo samuelcastro, and erich elsen.
2020. rigging the lottery:in proceedings of themaking all tickets winners.
37th international conference on machine learning,icml 2020, 13-18 july 2020, virtual event, volume119 of proceedings of machine learning research,pages 2943–2952.
pmlr..angela fan, shruti bhosale, holger schwenk, zhiyima, ahmed el-kishky, siddharth goyal, man-deep baines, onur celebi, guillaume wenzek,vishrav chaudhary, naman goyal, tom birch, vi-taliy liptchinsky, sergey edunov, edouard grave,be-michael auli, and armand joulin.
2020.yond english-centric multilingual machine transla-tion.
corr, abs/2010.11125..orhan firat, kyunghyun cho, and yoshua bengio.
2016. multi-way, multilingual neural machine trans-lation with a shared attention mechanism.
in naaclhlt 2016, the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, sandiego california, usa, june 12-17, 2016, pages866–875.
the association for computational lin-guistics..zhou.
2018. achieving human parity on auto-matic chinese to english news translation.
corr,abs/1803.05567..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..james kirkpatrick, razvan pascanu, neil c. rabi-nowitz, joel veness, guillaume desjardins, an-drei a. rusu, kieran milan, john quan, tiago ra-malho, agnieszka grabska-barwinska, demis hass-abis, claudia clopath, dharshan kumaran, and raiahadsell.
2016. overcoming catastrophic forgettingin neural networks.
corr, abs/1612.00796..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, emnlp2018: system demonstrations, brussels, belgium,october 31 - november 4, 2018, pages 66–71.
as-sociation for computational linguistics..xian li, asa cooper stickland, yuqing tang, and xi-ang kong.
2020. deep transformers with latentdepth.
in advances in neural information process-ing systems 33: annual conference on neural infor-mation processing systems 2020, neurips 2020, de-cember 6-12, 2020, virtual..zehui lin, xiao pan, mingxuan wang, xipeng qiu,jiangtao feng, hao zhou, and lei li.
2020. pre-training multilingual neural machine translation byin proceedingsleveraging alignment information.
of the 2020 conference on empirical methods innatural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 2649–2663.
as-sociation for computational linguistics..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
corr,abs/2001.08210..jonathan frankle and michael carbin.
2019. the lot-tery ticket hypothesis: finding sparse, trainable neu-in 7th international conference onral networks.
learning representations, iclr 2019, new orleans,la, usa, may 6-9, 2019. openreview.net..zhuang liu, mingjie sun, tinghui zhou, gao huang,and trevor darrell.
2019. rethinking the value ofin 7th international conferencenetwork pruning.
on learning representations, iclr 2019, new or-leans, la, usa, may 6-9, 2019. openreview.net..hany hassan, anthony aue, chang chen, vishalchowdhary, jonathan clark, christian federmann,xuedong huang, marcinjunczys-dowmunt,william lewis, mu li, shujie liu, tie-yan liu,renqian luo, arul menezes, tao qin, frank seide,xu tan, fei tian, lijun wu, shuangzhi wu, yingcexia, dongdong zhang, zhirui zhang, and ming.
sungwon lyu, bokyung son, kichang yang, andjaekyoung bae.
2020. revisiting modularized mul-tilingual nmt to meet industrial demands.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing, emnlp 2020,online, november 16-20, 2020, pages 5905–5918.
association for computational linguistics..301jerin philip, alexandre berard, matthias gall´e, andlaurent besacier.
2020. monolingual adapters forin proceed-zero-shot neural machine translation.
ings of the 2020 conference on empirical methodsin natural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 4465–4470.
as-sociation for computational linguistics..sylvestre-alvise rebufﬁ, hakan bilen, and andreavedaldi.
2017. learning multiple visual domainswith residual adapters.
in advances in neural infor-mation processing systems 30: annual conferenceon neural information processing systems 2017,december 4-9, 2017, long beach, ca, usa, pages506–516..devendra singh sachan and graham neubig.
2018.parameter sharing methods for multilingual self-attentional translation models.
in proceedings of thethird conference on machine translation: researchpapers, wmt 2018, belgium, brussels, october 31 -november 1, 2018, pages 261–271.
association forcomputational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics, acl 2016, august 7-12, 2016, berlin, ger-many, volume 1: long papers.
the association forcomputer linguistics..aditya siddhant, ankur bapna, yuan cao, orhan fi-rat, mia xu chen, sneha reddy kudugunta, naveenarivazhagan, and yonghui wu.
2020. leveragingmonolingual data with self-supervision for multilin-gual neural machine translation.
in proceedings ofthe 58th annual meeting of the association for com-putational linguistics, acl 2020, online, july 5-10,2020, pages 2827–2835.
association for computa-tional linguistics..jinsong su, shan wu, deyi xiong, yaojie lu, xianpeihan, and biao zhang.
2018. variational recurrentin proceedings of theneural machine translation.
thirty-second aaai conference on artiﬁcial intelli-gence, (aaai-18), the 30th innovative applicationsof artiﬁcial intelligence (iaai-18), and the 8th aaaisymposium on educational advances in artiﬁcial in-telligence (eaai-18), new orleans, louisiana, usa,february 2-7, 2018, pages 5488–5495.
aaai press..tianxiang sun, yunfan shao, xiaonan li, pengfeiliu, hang yan, xipeng qiu, and xuanjing huang.
2020. learning sparse sharing architectures for mul-tiple tasks.
in the thirty-fourth aaai conferenceon artiﬁcial intelligence, aaai 2020, the thirty-second innovative applications of artiﬁcial intelli-gence conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcial intel-ligence, eaai 2020, new york, ny, usa, february7-12, 2020, pages 8936–8943..xu tan, yi ren, di he, tao qin, zhou zhao, andtie-yan liu.
2019. multilingual neural machine.
translation with knowledge distillation.
in 7th inter-national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998–6008..mingxuan wang.
2019. towards linear time neural ma-chine translation with capsule networks.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 803–812.
associationfor computational linguistics..qiang wang, bei li, tong xiao,.
jingbo zhu,changliang li, derek f. wong, and lidia s. chao.
2019. learning deep transformer models for ma-chine translation.
in proceedings of the 57th confer-ence of the association for computational linguis-tics, acl 2019, florence, italy, july 28- august 2,2019, volume 1: long papers, pages 1810–1822.
as-sociation for computational linguistics..yining wang, jiajun zhang, feifei zhai, jingfang xu,and chengqing zong.
2018. three strategies to im-prove one-to-many multilingual translation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, brussels, bel-gium, october 31 - november 4, 2018, pages 2955–2960. association for computational linguistics..felix wu, angela fan, alexei baevski, yann n.dauphin, and michael auli.
2019. pay less atten-tion with lightweight and dynamic convolutions.
in7th international conference on learning represen-tations, iclr 2019, new orleans, la, usa, may 6-9, 2019. openreview.net..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, lukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation.
corr, abs/1609.08144..biao zhang, ankur bapna, rico sennrich, and orhanshare or not?
learning to schedulefirat.
2021.language-speciﬁc capacity for multilingual transla-tion.
in international conference on learning rep-resentations..302biao zhang, philip williams, ivan titov, and rico sen-nrich.
2020. improving massively multilingual neu-ral machine translation and zero-shot translation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1628–1639, online.
association for computational lin-guistics..michael zhu and suyog gupta.
2018. to prune, ornot to prune: exploring the efﬁcacy of pruning forin 6th international confer-model compression.
ence on learning representations, iclr 2018, van-couver, bc, canada, april 30 - may 3, 2018, work-shop track proceedings..a appendices.
a.1 datasets details.
iso language family.
script.
size.
farsifaararabiche hebrew.
iranianarabicsemitic.
89karabicarabic140khebrew 144k.
dutch.
nlde german.
germanic latingermanic latin.
italianspanish.
romance latinromance latin.
153k160k.
167k169k.
polish.
slavic.
latin.
128k.
ites.
pl.
table 6: statistics and language family of iwslt.
languages grouped together are similar languages..a.2 training details.
as stated in the previous section, we ﬁrst train amultilingual baseline (phase 1).
then we ﬁne-tunethe baseline on speciﬁc language pair to obtain themask (phase 2).
after that we train the lass modelwith the obtained masks (phase 3).
note that weonly apply masks on linear weights, which meansthat the embedding weights, layer normalizationare not masked out.
we also exclude the outputprojection weight.
we apply label smoothing ofvalue 0.1 in all our experiments..iwslt.
a.2.1model we adopt transformer-small 4 withdropout 0.1..data following tan et al.
(2019), we ﬁrst tok-enize the data then apply bpe.
the bpe vocab sizeis 30k.
we apply over-sampling with a temperatureof t = 2..training for phase 1, we train the baseline withadam with a learning rate schedule of (5e-4,4k).
the max tokens per batch is set to 262144. forphase 2, we keep all other settings unchanged ex-cept we set the max tokens to be 16384 and thedropout 0.3. for phase 3, we keep the same settingas phase 1, except we apply masks on the model..a.2.2 wmtmodel weandtransformer-big with pre-norm (wang et al.,.
transformer-base.
adopt.
4transformer-base with df f = 1024 and nhead = 4.
303iso language family.
script.
train.
valid.
test.
gujaratitamil.
kazakhturkish.
indo-aryan gujaratidravidian.
tamil.
wmt19wmt20.
newsdev19newsdev20.
newstest19newstest20.
turkicturkic.
cyrilliclatin.
wmt19wmt16.
newsdev19newsdev16.
newstest19newstest16.
romanian romanceromancespanishromancefrench.
wmt16wmt13wmt14.
newsdev16newstest12newstest13.
newstest16newstest13newstest14.
size.
11k64k.
120k205k.
597k13m37m.
pashto.
iranian.
arabic.
wmt20.
newsdev20.
newstest20.
1m.
farsilatvianestonian.
uralicbalticuralic.
lithuanian baltic.
russian.
czechpolish.
japanesechinese.
slavic.
slavicslavic.
japonicchinese.
wmt16wmt17wmt18.
newstest15newsdev17newsdev18.
newstest16newstest17newstest18.
2m2m2.1m.
wmt19.
newsdev19.
newstest19.
2.3m.
cyrillic.
wmt16.
newstest15.
newstest16.
2.5m.
wmt14wmt20.
newstest13newsdev20.
newstest14newstest20.
kanji; kana wmt20wmt17chinese.
newsdev20newsdev17.
newstest20newstest17.
11m11.1m.
16.8m20.8m.
german.
germanic.
latin.
wmt16.
newstest13.
newstest14.
4.5m.
latinlatinlatin.
latinlatinlatin.
latin.
latinlatin.
guta.
kktr.
roesfr.
ps.
ﬁlvet.
lt.ru.
cspl.
jazh.
de.
table 7: statistics and language family of wmt daatset.
languages grouped together are similar languages..tgt.
the bpe model..training for phase 1, we train the baseline withadam with a learning rate schedule of (5e-4,8k).
the max tokens per batch is set to 524288. forphase 2, the warm-up updates are set to 1000. toguarantee that the model does not overﬁt the data,we train on different language pairs with differentsteps and different batch size.
concretely, we ﬁne-tune on >10k, >100k, >1m, >10m language pairswith 1k, 2k, 4k, 8k steps and max tokens per batchwith 20480, 40960, 81920 and 163840. for phase3, we keep the setting the same as phase 1..a.3 case study.
fr cs de.
es.
ru.
zh.
nt13 nt13 nt13 nt13 opustednt13 nt13 nt13nt13 nt13 opustednt13opus.
crs.frcsdeesruzh.
table 8: datasets used in zero-shot translation.
“nt13”indicates newstest2013..2019).
we replace ﬁxed positional embedding withlearnable one and replace relu with gelu.
alsowe use layernorm-embedding (liu et al., 2020) tostabilize training..data we use sentencepiece (kudo and richard-son, 2018) to preprocess the data and learn bpe.
since the wmt dataset is highly imbalanced, weapply a temperature-based sampling strategy witht = 5. to ensure all languages are representedadequately in the vocabulary, we apply the sametemperature-based sampling strategy for training.
304fr → zh.
le symbole incontesté de la vigueur.
src.
la production annuelle d’acier étaitéconomique des nations.
钢的年产量是国家经济实力的重要象征.
refbaseline annual steel production was the undisputed symbol of nations’ economic strength.
lass.
年度钢铁生产是各国经济活力的无可争辩的象征.
de l’avis de ma délégation donc, l’onu devrait élargir ces activités de la faonsuivante.
因此,我国代表团认为,联合国现在应该以下述方式扩大这些活动。refbaseline 因此, in my delegation’s view, the united nations should expand these activities in.
src.
lass.
the following manner.
因此,我国代表团认为,联合国应该扩大这些活动,如下.
le domicile de la femme dépendait du lieu du mariage et de la résidence familiale.
妇女的住处取决于婚姻和家庭位置。.
srcrefbaseline the woman’s place of residence depended on the place of marriage and family.
lass.
residence.
妻子的住所取决于婚姻地点和家庭住所..de → zh.
du bist gebissen worden.
src你被咬了refbaseline you have been bitten.
你被咬了lass.
einmal würde schon reichen.
src你只需要道歉一次就够了!
refbaseline once upon a time it would be enough.
lass.
一次就足够了..wenn wir warten, hat er zeit zum tanken und munitionieren.
如果我们等待,他就有了 时间加油和补给弹药.
srcrefbaseline when we wait, he has time for tanks and ammunition.
当我们等待时,他有时间去坦克和弹药.
lass.
ru → zh.
src.
помощникминистразаместителяздравоохранения саудовской аравииего превосходительство д-р якуб бенюсуф аль-масрува沙特阿拉伯卫生部助理副部长雅各布·本·优素福·马斯如瓦博士阁下.
refbaseline dr yakub bin yusuf al-masruva, deputy minister of health of saudi arabialass.
沙特阿拉伯卫生部副部长的助理,his excellency dr yakub bin yusuf al-masruvaне хочу я, чтобы пит показывал нам фото,элли.
我不要皮特给我们看照片 艾莉i don’t want pete showing us a photo, elly.
我不想让皮特给我们看一下照片,艾丽.
роджерс!
я сказал, встать в строй!
罗杰斯 我说跟上.
refbaselinelass.
src.
srcrefbaseline 罗吉尔斯!
i said, get up!
lass.
罗吉尔斯,我说,你要站起来!.
table 9: case study.
305