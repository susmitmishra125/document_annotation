intrinsic dimensionality explains the effectivenessof language model fine-tuning.
armen aghajanyanfacebook aiarmenag@fb.com.
sonal guptafacebooksonalgupta@fb.com.
luke zettlemoyerfacebook aiuniversity of washingtonlsz@fb.com.
abstract.
although pretrained language models can beﬁne-tuned to produce state-of-the-art resultsfor a very wide range of language understand-ing tasks, the dynamics of this process arenot well understood, especially in the low dataregime.
why can we use relatively vanilla gra-dient descent algorithms (e.g., without strongregularization) to tune a model with hundredsof millions of parameters on datasets with onlyhundreds or thousands of labeled examples?
in this paper, we argue that analyzing ﬁne-tuning through the lens of intrinsic dimensionprovides us with empirical and theoretical intu-itions to explain this remarkable phenomenon.
we empirically show that common pre-trainedmodels have a very low intrinsic dimension;there exists a low dimension reparameteriza-tion that is as effective for ﬁne-tuning as thefull parameter space.
for example, by optimiz-ing only 200 trainable parameters randomlyprojected back into the full space, we cantune a roberta model to achieve 90% of thefull parameter performance levels on mrpc.
furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimen-larger mod-sion and, perhaps surprisingly,els tend to have lower intrinsic dimension af-ter a ﬁxed number of pre-training updates, atleast in part explaining their extreme effective-ness.
lastly, we connect intrinsic dimensional-ity with low dimensional task representationsand compression based generalization boundsto provide generalization bounds that are inde-pendent of the full parameter count..1.introduction.
pre-trained language models (radford et al., 2019;devlin et al., 2018; liu et al., 2019; lewis et al.,2019, 2020) provide the defacto initialization formodeling most existing nlp tasks.
however, theprocess of ﬁne-tuning them on often very smalltarget task datasets remains somewhat mysterious.
why can we use relatively vanilla gradient descent.
algorithms (e.g., without strong regularization) totune a model with hundreds of millions of param-eters on datasets with only hundreds or thousandsof labeled examples?.
we propose intrinsic dimensionality as a newlens through which ﬁne-tuning can be analyzed(li et al., 2018).
an objective function’s intrinsicdimensionality describes the minimum dimensionneeded to solve the optimization problem it de-ﬁnes to some precision level.
in the context ofpre-trained language models, measuring intrinsicdimensional will tell us how many free parametersare required to closely approximate the optimiza-tion problem that is solved while ﬁne-tuning foreach end task.
for example, we will show that 200parameters (randomly projected back into the fullparameter space) are enough to represent the prob-lem of tuning a roberta model to within 90%of the performance of the full model.
more gen-erally, we also describe a set of strong empiricaland theoretical connections between intrinsic di-mensionality, number of parameters, pre-training,and generalization..we ﬁrst empirically show that standard pre-trained models can learn a large set of nlp taskswith very few parameters and that the process ofpre-training itself implicitly minimizes the intrinsicdimension of later tuning for different nlp tasks.
we study over a dozen different pre-trained modelsto show that the number of parameters strongly in-versely correlates with intrinsic dimensionality, atleast in part justifying the extreme effectiveness ofsuch models.
we interpret pre-training as providinga framework that learns how to compress the aver-age nlp task.
finally, we connect intrinsic dimen-sional with low dimensional task representationsand compression-based generalization bounds toprovide intrinsic-dimension-based generalizationbounds independent of the full parameter count,further justifying why these methods generalize sowell in practice across tasks..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7319–7328august1–6,2021.©2021associationforcomputationallinguistics7319the contributions of our paper are the following:.
• we empirically show that common nlp taskswithin the context of pre-trained representa-tions have an intrinsic dimension several or-ders of magnitudes less than the full parame-terization..• we propose a new interpretation of intrinsic di-mension as the downstream ﬁne-tuning task’sminimal description length within the frame-work of the pre-trained model.
within thisinterpretation, we empirically show that theprocess of pre-training implicitly optimizesthe description length over the average of nlptasks, without having direct access to thosesame tasks..• we measure the intrinsic dimension of a largeset of recently developed pre-training method,and how that larger models tend to havesmaller intrinsic dimension..• lastly, we show that compression based gener-alization bounds can be applied to our intrinsicdimension framework to provide generaliza-tion bounds for large pre-trained models in-dependent of the pre-trained model parametercount..2 related work.
calculating the intrinsic dimension of an objectivefunction in the context of deep-learning was ﬁrstproposed by li et al.
(2018).
they analyzed theimpact of various architectures on the intrinsic di-mensionality of their objective.
our work is a directextension of this approach, focusing on analyzingpre-trained representations instead..there is a large collection of literature analyzingpre-trained models from the perspective of capacity.
for example, a recent line of work has shown thatpre-trained models such as bert are redundantin their capacity, allowing for signiﬁcant sparsiﬁ-cation without much degradation in end metrics(chen et al., 2020; prasanna et al., 2020; desaiet al., 2019).
houlsby et al.
(2019) showed that ﬁne-tuning top layers of pre-trained models is not effec-tive and that alternate methods allow ﬁne-tuningeffectively with a couple of percent of the param-eters.
furthermore, we can view computing theintrinsic dimensionality as a continuous relaxationof the sparsiﬁcation problem..there also exist connections between intrinsicdimensionality, knowledge distillation, and othermodel compression methods.
fundamentally intrin-sic dimensionality attempts to ﬁnd the smallest setof parameters needed to tune to reach satisfactorysolutions, which can be thought of as a sparsiﬁca-tion or distillation problem (hinton et al., 2015;chen et al., 2020).
unlike distillation approaches,the approach of intrinsic dimensionality does notchange parameter count, sparsity, or architecturebut instead looks at the underlying rank of the ob-jective function (li et al., 2018).
there are alsoconnections between representing multiple taskswithin a pre-trained model and compression whichwe explore in §5..moreover, standard approaches towards ﬁne-tuning seem to have non-trivial effects on the gen-eralization of pre-trained representations (agha-janyan et al., 2020, 2021).
a holistic explanatorypicture of the successes of ﬁne-tuning has not yetbeen painted.
a clear understanding of the un-derlying mechanisms which lead to the incrediblegeneralization of ﬁne-tuned pre-trained represen-tations is currently missing.
moreover, we still donot understand why various pre-training methodol-ogy manifests in universally useful representations,although recent line of works have attempted tocover this gap by looking at loss landscapes, andthe learned linguistic properties of pre-trained mod-els (hao et al., 2019; clark et al., 2019a)..3.intrinsic dimensionality of finetuning.
background the intrinsic dimension of an ob-jective function measures the minimum numberof parameters needed to reach satisfactory solu-tions to the respective objective (li et al., 2018).
alternatively, the intrinsic dimension representsthe lowest dimensional subspace in which one canoptimize the original function to within a certainlevel of approximation error.
computing the ex-act intrinsic dimensional of the objective functionis computation intractable; therefore, we resort toheuristic methods to calculate an upper bound.
letθd = [θ0, θ1, ..., θm] be a set of d parameters thatparameterize some model f (·, θ).
instead of opti-mizing the empirical loss in the original parame-terization (θd), the subspace method ﬁne-tunes themodel via the following re-parameterization in thelower-dimensional d-dimensions:.
θd = θd.
0 + p (θd).
(1).
7320where p : rd → rd projects from a parameterfrom a lower-dimensional d to the higher dimen-sional d and θd0 is the original model parameter-ization.
intuitively, we project using an arbitraryrandom projection onto a much smaller space; usu-ally, a linear projection, we then solve the optimiza-tion problem in that smaller subspace.
if we reacha satisfactory solution, we say the dimensionalityof that subspace is the intrinsic dimension.
thismethodology was proposed in the seminal paperby li et al.
(2018).
concretely li et al.
(2018)proposed three different parameteric forms for p ;a random linear dense projection (θdw ), randomlinear sparse projection (θdwsparse) and random lin-ear projection via the fastfood transform (le et al.,2013)..we will primarily use the fastfood transform,.
deﬁned as:.
θd = θd.
0 + θdm m = hgπhb.
(2).
the factorization of m consists of h, a hadamardmatrix, g, a random diagonal matrix with inde-pendent standard normal entries, b a random di-agonal matrix with equal probability ±1 entries,and π a random permutation matrix.
furthermore,the matrix multiplication with a hadamard ma-trix can be computed in o(d log d) via the fastwalsh-hadamard transform.
everything except θdis ﬁxed; therefore, the optimization problem liesonly in d-dimensions.1.
we use the fastfood transform due to its compu-tational complexity.
speciﬁcally, using hadamardmatrices instead of dense matrices allows us to com-pute a linear projection signiﬁcantly faster than adense matrix projection.
furthermore, when work-ing with large models such as roberta, the mem-ory required to store even a low-dimensional densematrix to calculate intrinsic dimension is unrea-sonable (d = 1000, 330, 000, 000 ∗ 1000 ∗ 4 bytes= 1.32 terabytes)..the standard method of measuring the intrin-sic dimensionality of an objective as proposed by(li et al., 2018) requires searching over variousd, training using standard sgd over the subspacereparameterization θd and selecting the smallest dwhich provides us with a satisfactory solution (d90).
(li et al., 2018) deﬁned the satisfactory solution asbeing 90% of the full training metric.
for example,.
1if we place a constraint of m being a binary matrix, werecover the sparsiﬁcation problem; therefore, we can also viewﬁnding intrinsic dimensionality as a continuous relaxation ofthe sparsiﬁcation problem..if we reach 85% accuracy training a model with allof its parameters, the goal is to ﬁnd the smallest d,which would reach 0.9 ∗ 85% = 76.5% accuracy;we call this dimension d90.2.
the way (li et al., 2018) deﬁne a satisfactorysolution reduces the dependence of the dataset sizeon the calculation of intrinsic dimension.
for asmall dataset, we will generally have worse endmetrics; therefore, we have a lower d90 cut-off;inversely, a larger dataset will require a more non-trivial d90 cut-off..structure aware intrinsic dimension due tothe large size of pre-trained language models (gen-erally in the hundreds of millions of parameters),the only computationally reasonable subspace op-timization method is one that utilizes the fastfoodtransform.
for example, if we are interested insubspace training with d = 1000 for the roberta-large model using a dense matrix, we would re-quire 1.42 terabytes of memory to store just theprojection matrix..unfortunately, the method of ﬁnding the intrinsicdimension proposed by (li et al., 2018) is unawareof the layer-wise structure of the function param-eterized by θ. existing literature argues that inattention-based pre-trained models, individual lay-ers specialize separately (clark et al., 2019b); there-fore, it is useful to incorporate a notion of structurewhen computing d90.
we deﬁne structure-awareintrinsic dimension (said) as the following.
i = θdθd.
0,i + λip (θd−m)i.
(3).
for m layers, we trade m parameters from our sub-space parameter θd to allow for layer-wise scal-ing through jointly learned λ, thus θd becomes[θd−m, λ].
this allows the said method to focusa larger capacity of θd−m towards speciﬁc layerswhat might carry more relevant information forthe task at hand.
conversely, we will refer to thelayer unaware method (equation 2) as the directintrinsic dimension (did) method..4.intrinsic dimensionality of commonnlp tasks.
4.1 sentence classiﬁcation.
we ﬁrst empirically calculate the intrinsic dimen-sion of various pre-trained models on a set of sen-tence prediction tasks from the glue benchmark.
2initializing θd = 0 we recover the original parameteri-0 which in the context of ﬁne-tuning represents the.
zation θdoriginal weights of the pre-trained model..7321said.
did.
model.
mrpc qqp mrpc qqp.
bert-basebert-large.
roberta-baseroberta-large.
16081037.
896207.
80301200.
896774.
18612493.
1000322.
92951389.
1389774.depth analysis of model parameter size on intrinsicdimensionality to a later section (§5.2)..lastly, we see that adding a notion of structure inthe computation of intrinsic dimension is beneﬁcialwith the said method consistently improving overthe structure unaware did method..table 1: estimated d90 intrinsic dimension computedwith said and did for a set of sentence predictiontasks and common pre-trained models..5.intrinsic dimension, pre-training, andgeneralization gap.
(wang et al., 2018).
we focus on analyzing bert(devlin et al., 2018) and roberta (liu et al.,2019) at both the base and large model sizes..we chose to experiment with mrpc (dolan andbrockett, 2005) and qqp (iyer et al., 2017) as ref-erence examples of small and large tuning datasets.
mrpc is a binary classiﬁcation task for predict-ing semantic equivalency for two paraphrases withroughly 3700 training samples, while qqp is abinary classiﬁcation task for predicting semanticequality of two questions, with roughly 363k sam-ples.
for every dataset and every model, we run100 subspace trainings with d ranging from 10 to10000 on a log scale.
for every training run, we doa small hyperparameter search across four learningrates.
we initialize every θd to the zero vector toallow for our starting point to be the original pre-trained model.
our subspace optimization methodalso operates over the randomly initialized sentenceclassiﬁcation head to ensure we have exactly d pa-rameters to optimize..we use both the said and did subspace op-timization methods, which we implemented inthe huggingface transformers library (wolf et al.,2019).
we present the results in figure 1..4.2 analysis.
the ﬁrst takeaway is the incredible low dimension-ality of viable solutions.
with roberta-large,we can reach 90% of the full ﬁne-tuning solutionof mrpc using roughly 200 parameters and 800parameters for qqp (table 1).
recall that our ap-proximation of intrinsic dimension is necessarilycrude by using random projections and restrictingthem to the use of fastfood transform; therefore, itis likely that the true intrinsic dimension is muchlower..furthermore, roberta consistently outper-forms bert across various subspace dimensions dwhile having more parameters.
we leave a more in-.
one interpretation of the intrinsic parameter vectoris that it encodes the task at hand with respect to theoriginal pre-trained representations.
therefore, wecan interpret d as the minimal description length ofthe task within the framework dictated by the pre-trained representations (hinton and zemel, 1993).
under this interpretation of intrinsic dimensional-ity, we hypothesize that pre-training is implicitlylowering the intrinsic dimensionality of the averagenlp task, and therefore compressing the minimaldescription length of those same tasks..what do we more precisely mean by intrinsicparameter encoding a task within the frameworkprovided by the pre-trained representations?
tra-ditionally, a ﬁnetuned model (e.g.
for a classiﬁca-tion tasks) simply consists of a classiﬁcation headg, parameterized by wg applied to ﬁne-tuned rep-resentations f , parameterized by wf per samplex. therefore, to fully describe a task, we needto pack together parameterizations and weights{g, f, wg, wf }.
this model description is com-pletely decoupled from the original weights of thepre-trained representation wf0, therefore to repre-sent n classiﬁcation tasks, we need to maintainn {wg, wf }; additionally, the task representationis incredibly high dimensional.
conversely, ﬁne-tuning utilizing said in d-dimensions requiresstoring only θd per task, a single random seed usedto generate m and the original pre-trained weightswf0.
therefore, we can represent arbitrary nlptasks within a single pre-trained model frameworkwith d + 1 parameters..for example, in the last section, we representedmrpc with roughly 200 parameters, which trans-lates to needing less than a kilobyte of data to en-code a complex natural language task within theframework provided by roberta..we hypothesize that the better the pre-trainedmodels are, the fewer bits (description length) areneeded to represent the average nlp task, as wewill demonstrate empirically in the next section..7322figure 1: evaluation accuracy on two datasets and four models across a range of dimensions d for the did method.
the horizontal lines in each ﬁgure represent the 90% solution of the respective full model..5.1 pre-training intrinsic dimension.
trajectory.
to verify our hypothesis of pre-training optimizingintrinsic dimension, we retrain a roberta-basefrom scratch and measure the intrinsic dimension ofvarious nlp tasks at different training checkpoints,using the said method.
we completely replicatethe setting as described by liu et al.
(2019) apartfrom only training for a total of 200k steps (in-stead of 500k) with half the batch size (1k).
tocalculate the intrinsic dimension more efﬁciently,we reuse the best learning rates discovered in sec-tion 4 for d < 10000 and use a ﬁxed learningrate for anything else.
to ﬁnd d90 we do a binarysearch across d per each checkpoint, with a mini-mum d of 100 and a maximum of 4 million.
the“full solution” that we use when deciding d90 cut-off is computed by ﬁne-tuning the checkpointedmodel in the standard way.
we compute said onsix datasets; mrpc, qqp, yelp polarity (zhanget al., 2015), sst-2 (socher et al., 2013), mnli(williams et al., 2018) and anli using all roundsof data (nie et al., 2019).
although we focus onbench-marking sentence classiﬁcation tasks the se-lected set of tasks contains variety, from sentimentclassiﬁcation (yelp polarity, sst-2) to natural lan-guage inference (mnli, anli) to question similar-ity (qqp)..we present our results in figure 2. the in-.
trinsic dimensionality of roberta-base mono-tonically decreases as we continue pre-training.
we do not explicitly optimize for intrinsic dimen-sionality, speciﬁcally during pre-training (the lan-guage model does not have access to downstreamdatasets!
), but none-the-less the intrinsic dimensionof these downstream tasks continues to decrease..more so, tasks that are easier to solve consis-tently show lower intrinsic dimensionality acrossall checkpoints, for example, yelp polarity vs. thenotoriously tough anli dataset.
the correlationbetween challenging tasks for roberta and theirlarge intrinsic dimension hints at a connection be-tween generalization and intrinsic dimension.
wewill discuss generalization further in section §5.3.
given our task representation interpretation ofintrinsic dimensionality, we argue that the largescale training of masked language models (mlm)learns generic and distributed enough representa-tions to facilitate downstream learning of highlycompressed task representations.
furthermore, weargue for another perspective of pre-training learn-ing representations that form a compression frame-work with respect to various nlp tasks..5.2 parameter count and intrinsic.
dimension.
we also measure the relationships between the pa-rameter count of arbitrary pre-trained models and.
7323102103104105d0.700.750.800.850.90accuracymrpc intrinsic dimensionmodelbert-basebert-largeroberta-baseroberta-large102103104105d0.650.700.750.800.850.90accuracyqqp intrinsic dimensionmodelbert-basebert-largeroberta-baseroberta-largefigure 2: every 10k updates of roberta-base that we trained from scratch, we compute d90 for six datasets;mrpc, qqp, yelp polarity, sst-2, mnli, and anli.
if we were unable to compute a d90 for a speciﬁc checkpoint,we do not plot the point, hence some datasets start at later points.
unable to compute means either we could notﬁne-tune the full checkpoint to accuracy above majority class or stabilize said training..the intrinsic dimension of downstream nlp tasks.
the optimal experiment to run would be to ﬁx thepre-training method, e.g., mlm roberta style,vary the architecture size from small to very big,and compute the intrinsic dimension of a group oftasks at every size of the model.
unfortunately,such an experiment is computationally infeasibledue to the need to train many roberta models..instead, we do an empirical study of many ex-isting pre-trained models, regardless of the pre-training method.
we show that the trend is strongenough to overcome differences in training method-ology.
we select the following models: bert(devlin et al., 2018), roberta (liu et al., 2019),bart (lewis et al., 2019), electra (clark et al.,2020), albert (lan et al., 2019), xlnet (yang et al.,2019), t5 (raffel et al., 2019), and xlm-r (con-neau et al., 2019).
furthermore, we selected var-ious sizes of these models, as available publiclywithin the huggingface transformers library (wolfet al., 2019)..we use the mrpc dataset and compute intrinsicdimension for every pre-trained model utilizingthe same binary search methodology mentioned inthe previous section with additional small hyper-parameter searches across learning rate (due to thewide range of learning rates needed by variousmodels)..we present our results in figure 3. there isa strong general trend that as the number of pa-rameters increases, the intrinsic dimension of ﬁne-tuning on mrpc decreases.
we ran this experimenton other datasets to ensure that this is not an dataartifact.
our experiments showed the same trend;we refer to the appendix for all trends per dataset.
within the same window of number of parame-.
ters, the pre-training methodology becomes moreimportant.
for example, in the regime of 108 pa-rameters, roberta pre-training dominates sim-ilar sized pre-training methods.
however, theredoes not seem to be a method that can overcomethe limitations induced by the number of parame-ters.
interpreting these results through the lens oflearning a compression framework for nlp tasksis straightforward; the more parameters we have inthe model, the less we need to represent a task..5.3 generalization bounds through intrinsic.
dimension.
we have shown strong empirical evidence connect-ing pre-training, ﬁne-tuning, and intrinsic dimen-sionality.
however, we have yet to argue the con-nection between intrinsic dimensionality and gen-eralization.
given that we have seen pre-trainingminimize intrinsic dimension, we hypothesize thatgeneralization improves as the intrinsic dimensiondecreases..to do so, we will empirically experiment withthe connections between d90 and evaluation set per-formance by looking at various checkpoints fromour roberta experiments in section §5.1.
we alsoplot the relative generalization gap (delta betweentrain time performance and test time performance).
in figure 4 we plot the evaluation accuracy’sachieved by our pre-training experiment in sec-tion §5.1.
a lower intrinsic dimension is stronglycorrelated with better evaluation performance.
ad-ditionally we are interested in measuring relativegeneralization gap ( acctrain−acceval) across intrin-sic dimension.
we select the training accuracy thatprovides us with the best evaluation metrics whencomputing this ﬁgure..1−acceval.
7324400006000080000100000120000140000160000180000200000updates103104105106d90roberta pre-training intrinsic dimension trajectorydatasetmrpcqqpyelpsst-2mnlianli (r1+r2+r3)figure 3: we calculate the intrinsic dimension for a large set of pre-trained models using the said method on themrpc dataset..figure 4: the evaluation accuracy of six datasets across various intrinsic dimensionalities.
there is a strong generaltrend that pre-trained models that are able to attain lower intrinsic dimensions generalize better..we present our results in figure 5. lower intrin-sic dimension once again correlates strongly with asmaller relative generalization gap.
if we interpretthe intrinsic dimension as a measure of complexity,we expect the generalization gap to decrease withintrinsic dimension..5.4 generalization bounds.
by applying standard compression based general-ization bounds, we can provide theoretical backingto the empirical connection between intrinsic di-mension and generalization (arora et al., 2018)..consider the following deﬁnition of multi-classclassiﬁcation loss with an optional margin over oursupervised dataset d.(cid:20).
(cid:21).
lγ(f ) = p(x,y)∼d.
f (x)[y] ≤ γ + maxj(cid:54)=y.
f (x)[j].
when γ = 0, l0 recovers the standard classiﬁca-tion loss.
furthermore, let ˆlγ(f ) be an unbiasedempirical estimate of the margin loss.
theorem 1. let f be a function which is parame-terized by θd as described in equation 1 with a to-tal of d trainable intrinsic parameters on a dataset.
with m samples.
then with a high probability, wecan state the following asymptotic generalizationbound.
l0(f ) ≤ ˆl0(f ) + o.
(4).
(cid:32)(cid:114).
(cid:33).
dm.proof.
we defer the proof section §a.1 in theappendix.
we note that this is an extension ofthe well-known compression based generalizationbound (arora et al., 2018)..this generalization bound is independent of theunderlying parameter count (d) of the pre-trainedmodel but depends on the ability to compress thedownstream task (d).
moreover, given that our pre-vious section shows larger models compress better,our bounds are aligned with general intuition andrecent empirical evidence that larger pre-trainedmodels generalize better.
explicitly, these boundsonly apply to pre-trained methods trained with theintrinsic dimension subspace method; research hasyet to show that standard sgd optimizes in thislow dimensional space (although experimentally,.
7325108109number of parameters101102103104105d90 bert-base bert-l roberta-b roberta-l xlm-r-b xlm-r electra-b xlnet-b xlnet-l t5-small t5-l bart-b bart-l albert-b albert-l albert-xl albert-xxl t5-3b103104105106d900.30.40.50.60.70.80.9eval accuracyroberta pre-training generalization studydatasetmrpcqqpyelpsst-2mnlianli (r1+r2+r3)figure 5: the intrinsic dimension and the respective relative generalization gap across a set of varied tasks..this seems to be conﬁrmed).
we leave the theoreti-cal contribution of showing sgd optimizes in thisspace, possibly resembling intrinsic subspace, forfuture work..we want to highlight that generalization is notnecessarily measured by the pre-trained model’sparameter count or measure of complexity but thepre-trained model’s ability to facilitate the com-pression of downstream tasks.
in some sense, ifwe want to compress downstream tasks better, wemust expect pre-trained representations to have aconsiderable measure of complexity..6 conclusion.
in conclusion, we proposed viewing the vari-ous phenomena surrounding ﬁne-tuning and pre-training through the lens of intrinsic dimension-ality.
we empirically showed that common natu-ral language tasks could be learned with very fewparameters, sometimes in the order of hundreds,when utilizing pre-trained representations.
we pro-vided an interpretation of pre-training as providinga compression framework for minimizing the av-erage description length of natural language tasksand showed that pre-training implicitly minimizesthis average description length..we continued by doing an empirical study of ex-isting pre-training methods and their respective in-trinsic dimension, uncovering the phenomena thatintrinsic dimensionality decreases as we increasethe number of pre-trained representation parame-ters.
this phenomenon provides some intuitionsto the trend of growing pre-trained representations.
we connected intrinsic dimensionality with gener-alization by ﬁrst showing that pre-trained modelswith lower intrinsic dimensions across various tasksachieve higher evaluation accuracies and lower rel-ative generalization gaps.
furthermore, we explainthese empirical results by applying well-known.
generalization bounds to the intrinsic dimension toget generalization bounds that grow on the order ofthe intrinsic dimension, not the parameter count..intrinsic dimensionality is a useful tool for un-derstanding the complex behavior of large models.
we hope that future work will make explicit theo-retical connections between sgd and optimizingthe intrinsic dimension as well as explain exactlywhy pre-training methods optimize the intrinsicdimensionailty of tasks before not seen..references.
armen aghajanyan, anchit gupta, akshat shrivas-tava, xilun chen, luke zettlemoyer, and sonalgupta.
2021. muppet: massive multi-task rep-arxiv preprintresentations with pre-ﬁnetuning.
arxiv:2101.11038..armen aghajanyan, akshat shrivastava, anchit gupta,naman goyal, luke zettlemoyer, and sonal gupta.
2020. better ﬁne-tuning by reducing representa-tional collapse.
arxiv preprint arxiv:2008.03156..sanjeev arora, rong ge, behnam neyshabur, andyi zhang.
2018. stronger generalization boundsfor deep nets via a compression approach.
arxivpreprint arxiv:1802.05296..tianlong chen, jonathan frankle, shiyu chang, sijialiu, yang zhang, zhangyang wang, and michaelcarbin.
2020. the lottery ticket hypothesis for pre-trained bert networks.
advances in neural informa-tion processing systems, 33..kevin clark, urvashi khandelwal, omer levy, andchristopher d manning.
2019a.
what does bertarxivlook at?
preprint arxiv:1906.04341..an analysis of bert’s attention..kevin clark, urvashi khandelwal, omer levy, andchristopher d manning.
2019b.
what does bertarxivlook at?
preprint arxiv:1906.04341..an analysis of bert’s attention..kevin clark, minh-thang luong, quoc v le, andchristopher d manning.
2020. electra: pre-training.
7326103104105106d905.0%10.0%15.0%20.0%25.0%relative generalization gaproberta pre-training generalization studydatasetmrpcqqpyelpsst-2mnlianli (r1+r2+r3)text encoders as discriminators rather than genera-tors.
arxiv preprint arxiv:2003.10555..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2019. unsupervisedcross-lingual representation learning at scale.
arxivpreprint arxiv:1911.02116..shrey desai, hongyuan zhan, and ahmed aly.
2019.evaluating lottery tickets under distributional shifts.
arxiv preprint arxiv:1910.12708..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..william b dolan and chris brockett.
2005. automati-cally constructing a corpus of sentential paraphrases.
in proceedings of the third international workshopon paraphrasing (iwp2005)..yaru hao, li dong, furu wei, and ke xu.
2019. visu-alizing and understanding the effectiveness of bert.
arxiv preprint arxiv:1908.05620..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
arxivpreprint arxiv:1503.02531..geoffrey e hinton and richard zemel.
1993. autoen-coders, minimum description length and helmholtzfree energy.
advances in neural information pro-cessing systems, 6:3–10..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
arxiv preprint arxiv:1902.00751..shankar iyer, nikhil dandekar, and kornel csernai.
2017. first quora dataset release: question pairs..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..quoc le, tam´as sarl´os, and alex smola.
2013.fastfood-approximating kernel expansions in loglin-ear time.
in proceedings of the international confer-ence on machine learning, volume 85..mike lewis, marjan ghazvininejad, gargi ghosh, ar-men aghajanyan, sida wang, and luke zettlemoyer.
2020. pre-training via paraphrasing..chunyuan li, heerad farkhoor, rosanne liu, and ja-son yosinski.
2018. measuring the intrinsic di-arxiv preprintmension of objective landscapes.
arxiv:1804.08838..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2019. ad-versarial nli: a new benchmark for natural languageunderstanding.
arxiv preprint arxiv:1910.14599..sai prasanna, anna rogers, and anna rumshisky.
2020. when bert plays the lottery, all tickets arewinning.
arxiv preprint arxiv:2005.00561..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122.
association forcomputational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, et al.
2019. huggingface’s transformers: state-of-the-art natural language processing.
arxiv, pagesarxiv–1910..7327zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for textclassiﬁcation.
arxiv:1509.01626 [cs]..a appendix.
a.1 proofs.
arora et al.
(2018) deﬁne (γ, s) compressible us-ing helper string s as the following..deﬁnition 1.
(γ, s) compressible using helperstring s.suppose ga,s = {gθ,s|θ ∈ a} is a class of clas-siﬁers indexed by trainable parameters a and ﬁxedstrings s. a classiﬁer f is (γ, s)-compressible withrespect to ga using helper string s if there existsθ ∈ a such that for any x ∈ s, we have for all y.
|f (x)[y] − gθ,s(x)[y]| ≤ γ.
(5).
remark 1. if we parameterize f (x; θ) via the in-trinsic dimension approach as deﬁned in equa-tion 1, then f is compressible losslessly using ahelper string consisting of the random seed used togenerate the static random projection weights andthe initial pre-trained representation θd0 .
thereforewe say f parameterized by either did or said is(0, s) compressible..theorem 2.1 in (arora et al., 2018) states givena compression consisting of r discrete states weachieve the following generalization bound..l0(f ) ≤ ˆlγ(f ) + o.
(cid:32)(cid:114).
(cid:33).
d log rm.(6).
we can trivially represent our parameters θd in adiscrete fashion through discretization, as was donein arora et al.
(2018), and the number of states isdependent on the level of quantization but is staticonce chosen (fp32 vs. fp16)..we then connect the fact that models trained inlow dimensional subspace using said/did meth-ods are (0, s)-compressible to derive the ﬁnalasymptotic bound..l0(f ) ≤ ˆl0(f ) + o.
(7).
(cid:32)(cid:114).
(cid:33).
dm.7328