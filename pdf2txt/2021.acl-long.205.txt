n -ary constituent tree parsing with recursive semi-markov model.
xin xinâˆ—, jinlong lischool of computer science and technology,beijing institute of technology, beijing, chinabj er center of hvlipcca, bit, beijing, china{xxin,jllee}@bit.edu.cn.
zeqi tanzhejiang university,hangzhou, chinazqtan@zju.edu.cn.
abstract.
in this paper, we study the task of graph-basedconstituent parsing in the setting that binariza-tion is not conducted as a pre-processing step,where a constituent tree may consist of nodeswith more than two children.
previous graph-based methods on this setting typically gen-erate hidden nodes with the dummy label in-side the n-ary nodes, in order to transformthe tree into a binary tree for prediction.
thelimitation is that the hidden nodes break thesibling relations of the n-ary nodeâ€™s children.
consequently, the dependencies of such sib-ling constituents might not be accurately mod-eled and is being ignored.
to solve this limi-tation, we propose a novel graph-based frame-work, which is called â€œrecursive semi-markovmodelâ€.
the main idea is to utilize 1-ordersemi-markov model to predict the immediatechildren sequence of a constituent candidate,which then recursively serves as a child can-didate of its parent.
in this manner, the de-pendencies of sibling constituents can be de-scribed by 1-order transition features, whichsolves the above limitation.
through experi-ments, the proposed framework obtains the f1of 95.92% and 92.50% on the datasets of ptband ctb 5.1 respectively.
specially, the recur-sive semi-markov model shows advantages inmodeling nodes with more than two children,whose average f1 can be improved by 0.3-1.1points in ptb and 2.3-6.8 points in ctb 5.1..1.introduction.
there are two settings for constituent parsing mod-els, including binary tree parsing and n-ary treeparsing.
in the former, the original constituent treewith n-ary nodes is converted into a binary tree bylanguage-speciï¬c rules.
the model ï¬rst predictsthe binary tree, and then converts it back.
in thelatter, the model directly predicts the n-ary treewithout the intermediate step of binarization..âˆ—xin xin is the corresponding author..figure 1: an n-ary node and the hidden nodes..in the paper, we focus on the setting of n-arytree parsing.
compared with binary tree parsing,which has the advantage of utilizing the lexicalhead information, n-ary tree parsing is more nat-ural to ï¬t the original tree structure, and is moreadaptable to languages that do not have head rulesfor binarization.
in addition, for languages withthe word segmentation issue, such as chinese, it isvery convenient for n-ary tree parsing models todeal with the joint task of word segmentation, part-of-speech (pos) tagging and constituent parsing,by just enlarging the label set with the pos labels,as shown in fig.
1 (a), which alleviates the errorpropagation from the pipeline..speciï¬cally, we target at improving graph-basedmodels for n-ary tree parsing, which obtain betterperformances in recent work (kitaev et al., 2019;zhang et al., 2020; wei et al., 2020) from the twostreams of well-developed parsing methods, graph-based and transition-based.
for n-ary tree parsing,the main idea of previous graph-based models isto generate hidden nodes with the dummy label Ï†inside the n-ary node, in order to expand the n-arytree into a binary tree.
in this way, n-ary tree pars-ing can be converted into binary tree parsing withhidden nodes, which are unobservable in the train-ing process.
consider the n-ary node â€œvpâ†’vv,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2631â€“2642august1â€“6,2021.Â©2021associationforcomputationallinguistics2631ipnpvpqpÏ•ï¼ˆaï¼‰ï¼ˆbï¼‰ï¼ˆcï¼‰Ï•ccccccccccccccccclpnpqpvpclpnpqpvpmvvnnmvvnncdcdclpccccccccccccccnpvpcpipnpnpcdvvmnnvvnnnnåˆ©æ¶¦profitå®Œæˆcompleteä¼ä¸šenterpriseæŠ•èµ„investä¸ªä½“individualä¸‰ç™¾ä¸‡3  millionå…ƒyuanåˆ©æ¶¦profitå®Œæˆcompleteä¸‰ç™¾ä¸‡3  millionå…ƒyuanåˆ©æ¶¦profitå®Œæˆcompleteä¸‰ç™¾ä¸‡3  millionå…ƒyuanfigure 2: comparisons of previous and our models.
(i,j) denotes the span from i to j âˆ’ 1. Ï(i, j) denotesthe feature score of span (i, j); Ïˆ(i, j, k) denotes thefeature score of the sibling span pair (i, j) and (j, k)..np, qpâ€ in fig.
1 (a) as an example.
the hiddennodes can be in two manners, as shown in fig.
1 (b,c).
either of them can be seen as being correctin training.
for convenience, the potential scoresof such hidden nodes are manually set to zero, toensure that the two manners are equivalent whencalculating the likelihood (kitaev and klein, 2018)..the limitation of previous methods is that thegenerated hidden nodes break the sibling relationsof the n-ary nodeâ€™s children.
consequently, suchsibling dependency feature might not be accuratelymodeled and is being ignored.
consider the nodeâ€œvpâ†’vv, np, qpâ€ in the above example.
if wemodel the 1-order dependency from the siblingnode pair, dependency feature scores should becalculated from both pairs of (vv, np) and (np,qp).
without loss of generality, suppose the hiddennode is as shown in fig.
1 (b), and the case infig.
1 (c) is similar.
as the hidden node Ï† is forcedto be the sibling node of â€œqpâ€, the dependencyfeature of (np, qp) cannot be directly calculated.
in implementation, only potential scores of eachnode are modeled, and the dependency potentialscores of sibling node pairs are being ignored..to solve this limitation, we propose a novelframework for n-ary tree parsing.
our main ideais to utilize 1-order semi-markov model to direct-ly predict the immediate children sequence of ann-ary node, without generating the hidden nodesfor binarization, as shown in fig.
2. different fromprevious models that only have potential scoreson nodes when evaluating a treeâ€™s likelihood, thepotential scores of sibling node pairs are also calcu-lated as 1-order transition features.
thus dependen-cies from sibling nodes can be naturally modeled,which solves the above limitation.
when generat-ing an n-ary tree, the semi-markov model is recur-sively conducted on the node spans in a bottom-upmanner, thus we call the proposed model â€œrecursivesemi-markov modelâ€..the main challenge of designing the recursivesemi-markov model is how to make the computa-tional complexity being acceptable.
in nowadaysgpu era, to make full use of parallel computationis an important issue to enhance the processingspeed.
for example, in the previous cyk (kasa-mi, 1966) algorithm for binary trees, the absolutetime complexity is o(n3), where n is the sentencelength.
but o(n2) out of it can be computed inparallel, by batchifying the spans with the samelength and the divisions within a span.
this meansthe hard time complexity of cyk, which cannotbe computed in parallel, is o(n).
in the case ofthe proposed recursive semi-markov model, thetime complexity of the straight-forward dynamicprogramming algorithm is o(n5).
but by carefuldesign, we propose an algorithm, whose complexi-ty is o(n4), with o(n3) out of it can be batchiï¬ed.
it means the increased o(n) complexity comparedwith cyk can be calculated in parallel.
in prac-tice, the proposed framework can process 26 and11 sentences per second in ptb and ctb 5.1 testsets respectively, by a single nvidia rtx gpu.
our main contributions can be summarized asfollows.
(1) we propose a novel graph-based frame-work, recursive semi-markov model, for n-ary con-stituent tree parsing, which can model the depen-dencies of sibling nodes.
(2) we design a dynamicprogramming algorithm for the proposed frame-work, whose complexity is o(n4), with o(n3) in-(3) experimental veriï¬-side can be batchiï¬ed.
cations demonstrate that the proposed frameworkoutperforms previous methods.
the f1 of the pro-posed framework is 95.92% and 92.50% in ptband ctb 5.1 respectively.
in the joint task withsegmentation and pos tagging in ctb 5.1, the f1is 91.84%.
in addition, the proposed frameworkcan effectively predict nodes with more than twochildren, improving the f1 by 0.3-1.1 points inptb and 2.3-6.8 points in ctb 5.1..our code is released at https://github.com/np-.
net-research/recursive-semi-markov-model,which is developed on the base of the open-sourceberkekey parser (kitaev and klein, 2018; kitaevet al., 2019)..2 related work.
2.1 early models for n -ary tree parsing.
a representative of classical methods for n-ary treeparsing is the earley algorithm (earley, 1970).
itcan ï¬nd legal trees of sentence ï¬tting the grammar.
2632Ï(0,4)Ï(1,4)Ï(2,4)(b) previous method0123(a) theoriginal treeÏ(0,4)Ï(2,4)ğœ“ğœ“(1,2,4)ğœ“ğœ“(2,3,4)(c) proposed methodğœ“ğœ“(0,1,2)Ï(0,1)Ï(1,2)Ï(2,3)Ï(3,4)Ï(0,1)Ï(1,2)Ï(2,3)Ï(3,4)rules with the complexity of o(cn3) by dynamicprogramming, where n is the sentence length andc is dependent on the complexity of grammar rules.
the dependency with the size of grammar rules inthe earley algorithm increases the computationalcomplexity substantially in practice.
therefore, re-cent studies have paid more attention to utilizingâ€œless grammarâ€ (hall et al., 2014), which is imple-mented in cyk/shift-reduce algorithms (durrettand klein, 2015; liu and zhang, 2017b; stern et al.,2017; teng and zhang, 2018) instead of the ear-ley algorithm.
it demonstrates it can reduce thecomplexity and also obtain better performances..our proposed framework is in line with the re-cent studies, whose complexity is independent withthe size of grammar rules..2.2 graph-based n -ary tree parsing.
graph-based parsing models utilize the cyk algo-rithm to ï¬nd the tree with the largest feature scoreas the prediction.
the main advantage is the largesearch space and the globally optimal inference.
arepresentative of graph-based n-ary tree parsingmodel is the berkeley parser (stern et al., 2017;kitaev and klein, 2018; kitaev et al., 2019), whichemploys hidden nodes to deal with n-ary nodes..the proposed framework belongs to graph-basedn-ary tree parsing models.
compared with previ-ous work, the novelty lies in that semi-markovmodel is utilized to directly model the children se-quence of an n-ary node, instead of generating abinary tree with hidden nodes.
consequently, itcan avoid breaking the sibling relation of nodes inthe sequence.
the proposed framework then makesuse of such dependencies to improve the parsingperformance..2.3 transition-based n -ary trees parsing.
transition-based models make predictions sequen-tially, with advantages of the low computationalcost and the utilization of high-order features.
themodels can be divided into post-order (cross andhuang, 2016; fernÂ´andez-gonzÂ´alez and gÂ´omez-rodrÂ´Ä±guez, 2019), pre-order (dyer et al., 2016), andin-order (liu and zhang, 2017a), according to thetraversal manner of the action sequence.
post-ordermodels require to deciding the number of reducednodes for n-ary nodes (fernÂ´andez-gonzÂ´alez andgÂ´omez-rodrÂ´Ä±guez, 2019), or to introducing hiddennodes with dummy label (cross and huang, 2016).
pre-order models and in-order models are born to.
figure 3: probabilistic graph of recursive semi-markovmodel..have convenience in dealing with n-ary nodes, asthe number of reduced nodes is ï¬xed..both the proposed framework and some of theabove methods directly model the sequence withinan n-ary node.
the novelty of the proposed frame-work is that it models the sequence as a graph-based model rather than a transition-based mod-el.
transition-based models suffer from the limita-tion of local optimization in the inference process,but graph-based models can guarantee the globallyoptimal inference.
in recent studies, graph-basedmodels have been demonstrated to perform betterthan transition-based models (kitaev et al., 2019;zhang et al., 2020; wei et al., 2020)..3 the recursive semi-markov model.
3.1 preliminaries.
a sentence is denoted by x = {xi}, with xi beingthe ith word.
the sentence length is denoted by n.let y be the set of the alphabet constituent labels.
following previous work (kitaev and klein, 2018;zhang et al., 2020), the nodes with unary gram-mars are collapsed, and its label is replaced by thejoint label of the collapsed nodes.
for example, infig.
1 (a), â€œcpâ†’ipâ€ will be replaced by â€œcp+ipâ€,where â€œcp+ipâ€ is an atomic label.
given x, thetask is to build an n-ary tree on top of it, and assigna label to each internal node.
when conducting thejoint parsing task with word segmentation and postagging in chinese, y is enriched with the poslabels and a â€œcâ€ label (denoting characters), and xidenotes the ith character.
for example, in fig.
1 (a),â€œnnâ€ is a pos label, and â€œnp+nnâ€ is treated asan atomic label for the corresponding node in thejoint parsing task..3.2 the framework structure.
in the proposed recursive semi-markov model, theprobabilistic graph of a constituent tree is shown.
2633å®Œæˆcompleteåˆ©æ¶¦profitä¸‰ç™¾ä¸‡å…ƒ3millionyuanä¼ä¸šenterpriseæŠ•èµ„investä¸ªä½“individualğ‘¥ğ‘¥0ğ‘¥ğ‘¥1ğ‘¥ğ‘¥2ğ‘¥ğ‘¥3ğ‘¥ğ‘¥4ğ‘¥ğ‘¥5ğ‘¥ğ‘¥6ğ‘¥ğ‘¥13ğ‘¥ğ‘¥12ğ‘¥ğ‘¥11ğ‘¥ğ‘¥10ğ‘¥ğ‘¥9ğ‘¥ğ‘¥8ğ‘¥ğ‘¥7figure 4: examples of potential scores for a whole tree.
for the convenience of presentation, we omit the labels.
the full presentation for Ï(i, j) is Ï(i, j, l) and the fullpresentation for Ïˆ(i, j, k) is Ïˆ(i, j, k, l1, l2)..in fig.
3. this graph corresponds to the tree infig.
1 (a).
full circles refer to the input x. blankcircles refer to the internal nodes, which can beseen as variables in the probabilistic graph.
thefull line, which connects two nodes, means thatthe two nodes are dependent with each other.
thedotted line pointing to an internal node refers to thesequence of the nodeâ€™s immediate children.
thereare two kinds of cliques in the graph, the one witha single node, and the one with two sibling nodes.
the former corresponds to 0-order cliques, and thelatter corresponds to 1-order cliques.
the wholeframework is a 1-order semi-markov model..potential scores, which are assigned to the abovetwo kinds of cliques, are denoted by Ï(i, j, l|x, Î¸),and Ïˆ(i, j, k, l1, l2|x, Î¸), respectively.
Î¸ is the mod-el parameters, including neural network weightsand word embeddings.
in the following, we omitthe symbol x and Î¸ in equations for presentationsimplicity.
Ï(i, j, l) deï¬nes the emission featurescore of a span, describing how likely the span is aconstituent.
(i, j) denotes a span which starts at iand ends at j âˆ’ 1, 0 â‰¤ i < j â‰¤ n. l âˆˆ y denotesthe spanâ€™s label.
Ïˆ(i, j, k, l1, l2) deï¬nes the transi-tion feature score of two sibling spans, describinghow likely the two spans are sibling neighbors with-in an n-ary node.
(i, j, k) denotes the two siblingspans (i, j) and (j, k).
l1 is the label of the leftspan, and l2 is the label of the right span..let y denote a predicted tree given x. the con-ditional probability p(y|x) can be deï¬ned on theprobabilistic graph, under the framework of condi-tional random ï¬elds (crf) (lafferty et al., 2001),as shown in the following equations.
c1(y) de-notes the set of emission scores, and c2(y) denotesthe set of transition scores.
t (x) denotes all legaln-ary trees that can be built on top of the input sen-tence x. s(y) is the sum of clique potential scoresdeï¬ned in a whole tree, with two examples shown.
figure 5: the neural architecture for feature learning..in fig.
4. given the parameters Î¸, the inferenceprocess is to ï¬nd a tree with the largest probability..s(y) = (cid:88)c1(y).
Ï(i, j, l) + (cid:88)c2(y).
Ïˆ(i, j, k, l1, l2).
(1).
p(y|x) =.
exp(s(y)))y(cid:48)âˆˆt (x) exp(s(y(cid:48))).
(cid:80).
3.3 potential score calculations.
given an input sentence x, we follow the neuralnetwork architecture of the berkeley parser (kitaevand klein, 2018) with some minor revisions, tocalculate the two kinds of potential scores, Ï(i, j, l),and Ïˆ(i, j, k, l1, l2), as shown in fig.
5..in the embedding layer, the bert (devlin et al.,2019; wolf et al., 2020) is selected to generate pre-trained vectors, denoted by ei, 0 â‰¤ i < n. for thechinese language, ei refers to the ith character, andthe embedding vector of last character within theword is chosen to represent the word..ei = bert(xi|x).
in the encoding layer, the transformer (vaswaniet al., 2017) is selected for extracting the contextâˆ’â†’features, denoted by hi, with odd dimensionsh iand the even dimensions.
â†âˆ’h i..hi = transformer(ei|x).
the representation of a single span (i, j) isâˆ’â†’â†âˆ’h iâˆ’1], andh j âˆ’formed by v(i, j) = [the representation of a sibling span pair (i, j) and(j, k) is formed by v(i, j, k) = [v(i, j); v(j, k)].
[; ].
â†âˆ’h jâˆ’1âˆ’.
âˆ’â†’h i;.
2634(a)s(y) = (0,5)(0,1)+(1,2)+(2,3)+(3,4)+(4,5)+(0,1,2)+(1,2,3)+(2,3,4)+(3,4,5)ÏÏÏÏÏÏ+ÏˆÏˆÏˆÏˆs(y)(0,5)(0,1)(1,5)(0,1,5)(1,2)(2,3)(3,4)(4,5)(1,2,3)(2,3,4)(3,4,5)ÏÏÏÏÏÏÏ=++++++++++ÏˆÏˆÏˆÏˆ(b)1230412304â„ğ‘—ğ‘—âˆ’ â„ğ‘–ğ‘–; â„ğ‘—ğ‘—âˆ’1 âˆ’â„ğ‘–ğ‘–âˆ’1clssepbertâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦ğ‘£ğ‘£ğ‘–ğ‘–,ğ‘—ğ‘—; ğ‘£ğ‘£(ğ‘—ğ‘—,ğ‘˜ğ‘˜)ğ‘£ğ‘£ğ‘–ğ‘–,ğ‘—ğ‘—ğ‘£ğ‘£ğ‘–ğ‘–, ğ‘—ğ‘—,ğ‘˜ğ‘˜span  representationtransformercontext  representationmlpÏ(ğ‘–ğ‘–,ğ‘—ğ‘—,ğ‘™ğ‘™|ğ‘¥ğ‘¥,ğœƒğœƒ)ğœ“ğœ“(ğ‘–ğ‘–,ğ‘—ğ‘—,ğ‘˜ğ‘˜,ğ‘™ğ‘™1,ğ‘™ğ‘™2|ğ‘¥ğ‘¥,ğœƒğœƒ)ÏÏˆ(i, j, k, l1, l2) = mlptransition.
l1,l2.
(v(i, j, k))..figure 6: comparisons between the cyk algorithmand the recursive semi-markov model..is the concatenate operation.
by passing v(i, j) andv(i, j, k) through multi-layer perceptrons (mlp),the emission potential score is ï¬nally deï¬ned as.
Ï(i, j, l) = mlpemission.
(v(i, j)),.
l.and the transition potential score is deï¬ned as.
there are totally |y| mlps for Ï.
|y| is the sizeof the label set y. parameters in the hidden layersare shared among them, and only the parametersof the output layers are different to distinguish dif-ferent labels.
similarly, there are |y|2 mlps for Ïˆ,whose parameters in hidden layers are also shared..3.4 the max-margin loss.
when designing the loss function, theoretically, wecan follow the crf framework to optimize the log-likelihood of the training data.
but in practice, ifwe do this, the gradients of all potential scores,which is o(n4) (n is the sentence length), shouldbe stored in the gpu memory.
this is impossi-ble to be implemented in a general gpu device.
therefore, we employ the max-margin loss as thetraining objective to learn the parameters of theproposed framework, following the berkeley pars-er (kitaev and klein, 2018).
by max-margin, onlythe gradients of the predicted tree structure and thegold structure need to be stored, which is o(n).
consequently, it saves a lot of memory in imple-mentation..let s(y) in eq.
1 denote the total potential scoreof a tree y. suppose the gold tree is yg, with thepotential score s(yg).
the key idea of the max-margin loss is to let the maximum potential scoreof the other trees, denoted as s(yâˆ—), be less thans(yg) by an acceptable margin.
in the probabilityspace, it is equivalent that the probability of thegold tree is larger than the maximum probability ofthe other trees by a margin.
the formal deï¬nitionof the objective is to minimize the following hingeloss, where âˆ†(y, yg) refers to the number of spansin yg not matched in y..(cid:32).
(cid:33).
l = max.
0, maxyâˆˆt (x).
[s(y) + âˆ†(y, yg)] âˆ’ s(yg).
3.5 explanations of the proposed model.
the semi-markov property.
the semi-markovproperty of the proposed model refers to the one.
mentioned in sarawagi and cohenâ€™s work (sarawa-gi and cohen, 2004).
when ï¬nding the immedi-ate children of a constituent span, the linear-chainmarkov structures are assumed over the sequenceof candidate immediate constituents.
in the imple-mentation, we treat it as a segmentation problem,where each immediate child span can be seen asa segment, which has the similar setting with theprevious work (sarawagi and cohen, 2004).
com-pared with the traditional â€œb-i-oâ€ tagging schemain segmentation, which assigns a label to each to-ken, the emission feature Ï is deï¬ned on the wholesegment of several tokens in the proposed model,which is non-markovian.
markov property existsin adjacent segments from the transition feature Ïˆ.this shows the semi-markov property..connections with crf.
traditional crf mod-els deï¬ne a conditional probability over a proba-bilistic graph, and utilize the maximum likelihoodestimation as the optimization objective.
the pro-posed model shares the same conditional probabili-ty deï¬nition from the explanation view, but utilizesa margin-based loss in order to save the computa-tional memory..4 algorithms.
4.1 the challenge.
the core for the optimization is to ï¬nd the tree withthe maximum potential score.
the previous cykalgorithm utilizes dynamic programming to ï¬nd themaximum score, in a bottom-up manner.
in orderto calculate the maximum score of a given span,all the divisions should be enumerated.
as shownin fig.
6 (left), in the binary tree case, the numberof the divisions is equal to l âˆ’ 1, where l is thespan length.
besides, the span length should beenumerated from 1 to n, and for each span lengthl there is l âˆ’ n + 1 spans.
therefore the totaltime complexity of previous cyk is o(n3).
inour case, a span can have more than two immediate.
2635(b) semi-markovmodel(1)(2)(3)(4)(5)(6)(7)(a)cyk(1)(2)(3)figure 7: an example of the dynamic programming..children.
therefore, all the segmentation sequencesshould be enumerated, which obviously enlargesthe search space.
in fig.
6 (right), for a span withthe length equal to 4, the number of sequences to beconsidered increases from 3 to 7. this differenceis the key issue to be solved in this section..4.2 straight-forward algorithm (o(n5)).
let (i, j) be a representative span (i < j).
we needto ï¬nd its immediate children sequence with themaximum potential score.
dynamic programmingis employed to accumulate the maximum potentialscore from the left to the right.
let Î±(i, j(cid:48), d, l) bean accumulated variable in the dynamic program-ming, which accumulates potential scores fromj(cid:48) = i + 1 to j(cid:48) = j. j(cid:48) denotes the current accu-mulated position.
d (i < d < j(cid:48)) means that thelast immediate child for span (i, j(cid:48)) is the span (d,j(cid:48)).
l refers to the label of (d, j(cid:48)).
the meaningof Î±(i, j(cid:48), d, l) is the maximum accumulated scorechosen from all the immediate children sequencesof span (i, j(cid:48)) whose last immediate child is (d, j(cid:48))with the label l. we also include the case of d = i,which refers to the maximum accumulated score ofthe span (i, j(cid:48))â€™s children and the span (i, j(cid:48)) itselfwith l as its label..Î±(i, i + 1, i, l) = Ï(i, i + 1, l).
Î±(i, j(cid:48), d, l) = max.
(cid:2)Î±(i, d, q, l(cid:48))+.
iâ‰¤q<d,l(cid:48)âˆˆy.
+Ïˆ(q, d, j(cid:48), l(cid:48), l) + Î±(d, j(cid:48), d, l)(cid:3).
Î±(i, j(cid:48), i, l) = Ï(i, j(cid:48), l) + max.
Î±(i, j(cid:48), k, l(cid:48)).
i<k<j(cid:48),l(cid:48)âˆˆy.
in semi-markov model, the above iterative cal-culation equations hold for the dynamic program-ming.
the ï¬rst equation is the initial state whenj(cid:48) = i + 1, and the second and third equations are.
figure 8: the main steps of the proposed algorithm..the iterative functions when (i < d < j(cid:48), j(cid:48) > i+1)and (d = i, j(cid:48) > i + 1), respectively.
an exampleof the dynamic programming is shown in fig.
7..in the iterative calculation of the above dynamicprogramming, we need to enumerate q, d, j(cid:48), i, j,each of which has the complexity of o(n).
the to-tal time complexity of the straight-forward methodis o(n5) âˆ— o(|y|2).
to simplify the complexity of|y|2, in calculating Ïˆ(q, d, j(cid:48), l(cid:48), l), we manuallygroup the labels in y into clusters, according tothe meaning of the constituent label, which reducesthe complexity of |y|2.
consequently, the maincomplexity comes from the o(n5) part..4.3 the proposed algorithm (o(n) âˆ— op(n3)).
in this section, we introduce how to reduce theabove complexity of o(n5) to o(n) âˆ— op(n3).
op(n3) means all the o(n3) calculations can bebatchï¬ed.
the hard complexity, which cannot becomputed in parallel, is o(n)..the overall procedure for designing the algorith-m is shown in fig.
8. it includes four steps forreducing or batchifying the time complexity.
in theï¬rst step, the complexity is reduced from o(n5) too(n4) by sharing the Î± values in a set of spans.
asshown in fig.
8 (a), in the span of (0, 5), we needto calculate Î±(0, j, d, l) by enumerating j from 1to 5. but the value Î±(0, 4, d, l) has been calculatedin the span of (0, 4).
iteratively, all the values ofÎ±(0, j, d, l)(0 < j < 5) have been calculated inprevious spans starting from 0. this means a setof spans that have the same start position can sharethe Î± values.
if we enumerate the span length inthe ascending order, in span (i, j), only the jthpositionâ€™s value Î±(i, j, d, l) needs to be calculated,instead of enumerating the position j(cid:48) from i + 1to j, which reduces o(n) of the time complexi-ty.
in the second step, the complexity is batchiï¬ed.
2636012345678ğ‘–ğ‘–=0ğ‘—ğ‘—â€²=8ğ‘‘ğ‘‘=5ğ‘ğ‘=3ğ›¼ğ›¼0,8,5,ğ‘™ğ‘™=maxğ‘™ğ‘™â€²ğ›¼ğ›¼0,5,4,ğ‘™ğ‘™â€²+ğœ“ğœ“4,5,8,ğ‘™ğ‘™â€²,ğ‘™ğ‘™+ğ›¼ğ›¼(5,8,5,ğ‘™ğ‘™)ğ›¼ğ›¼0,5,3,ğ‘™ğ‘™â€²+ğœ“ğœ“3,5,8,ğ‘™ğ‘™â€²,ğ‘™ğ‘™+ğ›¼ğ›¼(5,8,5,ğ‘™ğ‘™)ğ›¼ğ›¼0,5,2,ğ‘™ğ‘™â€²+ğœ“ğœ“2,5,8,ğ‘™ğ‘™â€²,ğ‘™ğ‘™+ğ›¼ğ›¼(5,8,5,ğ‘™ğ‘™)ğ›¼ğ›¼0,5,1,ğ‘™ğ‘™â€²+ğœ“ğœ“1,5,8,ğ‘™ğ‘™â€²,ğ‘™ğ‘™+ğ›¼ğ›¼(5,8,5,ğ‘™ğ‘™)ğ›¼ğ›¼0,5,0,ğ‘™ğ‘™â€²+ğœ“ğœ“0,5,8,ğ‘™ğ‘™â€²,ğ‘™ğ‘™+ğ›¼ğ›¼(5,8,5,ğ‘™ğ‘™)ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ›¼ğ›¼5,8,5,ğ‘™ğ‘™=ğœŒğœŒ5,8,ğ‘™ğ‘™+maxğ‘™ğ‘™â€²â€²ï¿½ğ›¼ğ›¼(5,8,7,ğ‘™ğ‘™ğ‘™ğ‘™)ğ›¼ğ›¼(5,8,6,ğ‘™ğ‘™ğ‘™ğ‘™)(a)(b)(c)(d)(0,5,,)dlÎ±(0,4,,)dlÎ±(0,5,,)dlÎ±(1,6,,)dlÎ±(2,7,,)dlÎ±(0,7,6,)lÎ±(0,7,5,)lÎ±(0,7,4,)lÎ±(0,7,3,)lÎ±(0,7,2,)lÎ±(0,7,1,)lÎ±(0,5,4,)lÎ±(0,7,5,)lÎ±(0,5,3,)lÎ±(0,7,5,)lÎ±(0,7,5,)lÎ±(0,5,1,)lÎ±(0,5,2,)lÎ±(0,7,5,)lÎ±algorithm 1 algorithm for recursive semi-markovmodel.
input: sentence x (length n ), model parameters Î¸.outputs: the constituent tree yâˆ— with the maximumpotential score s(yâˆ—|x; Î¸)..calculate Ï(i, j, l|x; Î¸)..1: for all spans (i, j) do2:3: end for4: for all sibling span pairs (i, j) and (j, k) do5:6: end for7: for span length t from 1 to n docalculate Î±(i, i + t, d, l).
8:0 â‰¤ i â‰¤ n âˆ’ t , i â‰¤ d < i + t.calculate Ïˆ(i, j, k, l1, l2|x; Î¸)..9: end for10: s(yâˆ—|x; Î¸) = maxd,l Î±(0, n, d, l).
11: trace back the tree yâˆ—..from o(n4) to o(n3) âˆ— op(n), by computing thespans of the same length in parallel, as shown infig.
8 (b).
in the third step, the complexity is batchi-ï¬ed from o(n3) âˆ— op(n) to o(n2) âˆ— op(n2), bycomputing different ds in Î±(i, j, d, l), i < d < j inparallel, as shown in fig.
8 (c).
in the fourth step,the complexity is batchiï¬ed from o(n2) âˆ— op(n2)to o(n) âˆ— op(n3), by computing Î±(i, j, d, l) whenenumerating the second last immediate child withi < q < d in parallel (to calculate the dynamicprogramming state at a new position given the lastchild, we need to enumerate previous states withdifferent second last children, in order to calculateÏˆ), as shown in fig.
8 (d)..the details of the proposed algorithm are shownin alg.
1. the calculation of Ï(i, j, l|x; Î¸) andÏˆ(i, j, k, l1, l2|x; Î¸) can be easily computed in par-allel, with the complexity op(n2) and op(n3), re-spectively.
the complexity of calculating Î± iso(n) âˆ— op(n3).
therefore, the total time complex-ity of the proposed algorithm is o(n) âˆ— op(n3)..5 experiments.
5.1 experimental setup.
we evaluate the proposed framework in both en-glish and chinese, on the datasets of ptb (wsjsections (marcus et al., 1993)) and ctb 5.1 (xueet al., 2005), respectively.
for chinese, we evaluateboth the single task of constituent parsing and thejoint task with word segmentation and pos tagging.
we follow the standard split of the datasets (kitaev.
value10âˆ’5.
value parameterparameterlearning rate32batch sizedecay patience 50.5decay factordropout3max decaymlp hidden1mlp layertrans.
hidden 1024 trans.
layerlabel hiddenhead number 8.
0.22502250.table 1: hyper-parameters..modeldyer et al.
(2016)choe and charniak (2016)liu and zhang (2017a)fried et al.
(2017)stern et al.
(2017)liu et al.
(2018)shen et al.
(2018)gÂ´omez-rodrÂ´Ä±guez and vilares (2018)gaddy et al.
(2018)teng and zhang (2018)hong and huang (2018)joshi et al.
(2018)vilares et al.
(2019)kitaev and klein (2018)kitaev et al.
(2019)zhou and zhao (2019)zhang et al.
(2020)wei et al.
(2020)ours.
pâ€“â€“â€“â€“.
â€“92.0â€“.
f1r93.3â€“93.8â€“94.2â€“â€“94.6692.98 90.63 91.7992.3â€“91.891.7â€“90.792.41 91.76 92.0892.492.292.592.092.591.594.394.893.8â€“90.60â€“94.85 95.40 95.1395.46 95.73 95.5995.70 95.98 95.8495.85 95.53 95.6996.195.595.896.29 95.55 95.92.table 2: single-task performances on test set of ptb..et al., 2019).
in the single task for chinese, someprevious work utilize the stanford tagger (toutano-va et al., 2003) to generate the pos tags as input,which leads to a ï¬xed error propagation.
in thispaper, pos tags are removed and not used as inputfeatures in both training and testing in ctb 5.1,following the previous work in (zhang et al., 2020).
standard precision, recall and f1-measure are em-ployed as evaluation metrics, where the evalb1tool is employed in the single task.
the hyper-parameters in the implementation are shown in ta-ble.
1. most of them are set following the berkeleyparser (kitaev and klein, 2018).
when choosingthe pre-train models (wolf et al., 2020), â€œbert-large-casedâ€ is utilized for english with a single rtx3090, â€œbert-base-chineseâ€ is utilized for chinesewith a single rtx 1080ti..5.2 performances.
the overall performances of the proposed frame-work in the single task of constituent parsing on.
1https://nlp.cs.nyu.edu/evalb.
2637modelwatanabe and sumita (2015)gÂ´omez-rodrÂ´Ä±guez and vilares (2018)dyer et al.
(2016)liu and zhang (2017b)vilares et al.
(2019)liu and zhang (2017a)shen et al.
(2018)wang et al.
(2015)fernÂ´andez-gonzÂ´alez and gÂ´omez-rodrÂ´Ä±guez (2019)fried and klein (2018)teng and zhang (2018)kitaev et al.
(2019)zhou and zhao (2019)zhang et al.
(2020)wei et al.
(2020)ours.
â€“â€“.
pâ€“â€“â€“.
f1r84.33â€“84.40â€“84.60â€“85.90 85.20 85.5085.61â€“86.10â€“86.60 86.40 86.5086.60â€“86.80â€“87.00â€“87.50 87.10 87.3091.96 91.55 91.7592.03 92.33 92.1892.51 92.04 92.2792.792.292.492.94 92.06 92.50.
â€“â€“â€“.
table 3: single-task performances on test set of ctb..modelwang et al.
(2006)jiang et al.
(2009)qian and liu (2012) 97.9697.86wang et al.
(2013)97.84zhang et al.
(2013)â€“zheng et al.
(2015)98.35baseline98.92ours.
seg-f1 pos-f1 par-f177.1078.0076.2081.07â€“â€“82.8593.8183.4294.4084.4394.8084.22â€“91.3896.3291.8496.70.
(a) ptb.
table 4: joint-task performances on test set of ctb.
the â€œbaselineâ€ row shows our running results using arevision of the berkeley parser (kitaev et al., 2019)..(b) ctb.
the test set are shown in table.
2 and table.
3.the baselines in the ï¬rst block are mainly basedon basic word embeddings, and the baselines inthe second block are based on bert (wolf et al.,2020).
it can be observed that the f1-measuresof proposed framework are 95.92% in ptb and92.50% in ctb 5.1, which outperform the previ-ous state-of-the-art methods.
our implementationfor the proposed framework is based on the berke-ley parser (kitaev et al., 2019).
therefore, manysettings are similar with it for fair comparisons,such as learning schedule and feature normaliza-tion.
our method outperforms it by 0.33 pointsin ptb and 0.5 points in ctb 5.1 (0.25% of the0.75% improvement is due to not utilizing auto-matically predicted pos tags in ctb 5.1), whichdemonstrates the advantage of modeling the siblingdependency features..the overall performances in the joint task on thetest set of ctb 5.1 are shown in table.
4. as thereare rare reports of performances with the bertembedding, we have implemented a minor revisionto the previous berkeley parser (kitaev et al., 2019).
figure 9: f1-measure values on constituent nodes withdifferent numbers of children.
the â€œbaselineâ€ refers toour running results using the berkeley parser (kitaevet al., 2019).
the percentage values at bottom refers tothe distribution of different nodes..to make it adaptable to the joint task, which servesas the baseline method in the second block.
it canobserved that the f1-measures of proposed recur-sive semi-markov model outperforms the competi-tive baseline by 0.46 points in f1, and consistentlyoutperforms previous method in all tasks of wordsegmentation, pos tagging, and parsing..the main improvement of the proposed frame-work comes from modeling the sibling dependen-cies of an n-ary nodeâ€™s children sequence.
it hasspecial advantage for predicting nodes with morechildren.
we have divided all the constituent nodesinto bins by how many children they have.
figure 9shows the comparisons.
the improvement is moreobvious when the number of children becomes larg-er.
for nodes with more than 2 immediate children,our framework outperforms the baseline by 0.3 to1.1 points in ptb and 2.3 to 6.8 points in ctb 5.1..2638                                          !          , p p h g l d w h  & k l o g u h q  1 x p e h u                                 )   p h d v x u h                          % d v h o l q h  5 h f x u v l y h  6 h p l markov model                                          !          , p p h g l d w h  & k l o g u h q  1 x p e h u                         )   p h d v x u h                          % d v h o l q h  5 h f x u v l y h  6 h p l markov modelmodelzhu et al.
(2013)stern et al.
(2017)shen et al.
(2018)gÂ´omez-rodrÂ´Ä±guez and vilares (2018)zhou and zhao (2019)wei et al.
(2020)zhang et al.
(2020)ours.
sent./sec.
9076111780159220109226.table 5: speed comparisons of different methods.
theresults of other methods are referred from the previouspapers, and the hardware equipments are different..5.3 speed analysis.
figure 10: speed analysis..figure 11: speed comparisons with the baseline.
ra-tio=speed(baseline)/speed(ours)..the average processing speed in ptb test set is26 sentences per second with a single rtx 3090,and the one in ctb 5.1 test set is 11 sentences persecond with a single rtx 1080ti (or 20 sentencesper second with single rtx 3090).
table 5 showsthe speed comparisons of the proposed model withprevious methods in the ptb dataset.
figure 10shows the detailed processing speed of the pro-posed model in ctb 5.1 dataset.
figure 10 (left)shows the processing speeds with different sen-tence lengths; and fig.
10 (right) shows the pro-cessing time of some special long sentences.
forthe longest sentence in the ctb 5.1, which con-tains 240 words, it takes around 6 seconds.
fig-.
ure 11 shows the processing speed ratio betweenthe berkeley parser (kitaev et al., 2019) and ourmodel.
it demonstrates that ratio does not growlinearly, by making full use of parallel computa-tions.
we know that the speed is still slower thansome previous methods.
on one hand, our pro-posed algorithm has already reduced the complexi-ty by parallel computations.
on the other hand, byconsidering its advantage in modeling nodes withmultiple children, which especially happens a lot inthe joint parsing task with segmentation and postagging in chinese, the processing speed is stillacceptable in many ofï¬‚ine cases..5.4 a further comparison on fine-grained.
noun phrase structures.
within the nodes having more than two children,some of them are noun phrases, whose internal hier-archical structures have been annotated in the ptbdataset by previous work (vadas and curran, 2007,2011).
we have also conducted experiments withthe berkeley parser (kitaev et al., 2019) on thisreï¬ned ptb data.
in the test process, we convertthe generated ï¬ne-grained trees back to the originaltrees for comparisons.
the f1 in the reï¬ned ptbtest dataset by the berkeley parser (kitaev et al.,2019) is 95.62%, which is also outperformed bythe proposed method in table 2..6 conclusion.
in this paper, a recursive semi-markov model is pro-posed for n-ary constituent tree parsing, with theadvantage of modeling the sibling relations withinn-ary node.
experimental veriï¬cations on ptb andctb 5.1 demonstrate that the proposed frameworkoutperforms previous work in the single parsingtask of both datasets and the joint task in ctb 5.1.for constituent nodes with more than 2 children,the f1 can be improved by 0.3 âˆ’ 1.1 points in ptband 2.3 âˆ’ 6.8 points in ctb 5.1..acknowledgments.
this work is supported by the grants from thenational natural science foundation of china(no.
61672100), and beijing natural sciencefoundation (no.
4202069).
it is partly support-ed by national key r&d program of china(no.
2018yfc0830705).
we thank the anonymousreviewers for their comments and constructive sug-gestions to improve this paper..2639               6 h q w h q f h  / h q j w k             3 u r f h v v h g  6 h q w   1 x p e h u  3 h u  6 h f r q g                   6 h q w h q f h  / h q j w k        7 k h  6 h f r q g v  w r  3 u r f h v v  2 q h  6 h q w                                                6 h q w h q f h  / h q j w k                 5 d w l r % d v h o l q h    5 h f x u v l y h  6 h p l markov modelreferences.
do kook choe and eugene charniak.
2016. parsingas language modeling.
in proceedings of the 2016conference on empirical methods in natural lan-guage processing, pages 2331â€“2336, austin, texas.
association for computational linguistics..james cross and liang huang.
2016. span-based con-stituency parsing with a structure-label system andprovably optimal dynamic oracles.
in proceedingsof the 2016 conference on empirical methods innatural language processing, pages 1â€“11, austin,texas.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171â€“4186, minneapolis, minnesota.
associ-ation for computational linguistics..greg durrett and dan klein.
2015. neural crf pars-in proceedings of the 53rd annual meetinging.
of the association for computational linguisticsand the 7th international joint conference on nat-ural language processing (volume 1: long paper-s), pages 302â€“312, beijing, china.
association forcomputational linguistics..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a. smith.
2016. recurrent neural networkgrammars.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 199â€“209, san diego, california.
association for computational linguistics..jay earley.
1970. an efï¬cient context-free parsing al-.
gorithm.
commun.
acm, 13(2):94â€“102..daniel fernÂ´andez-gonzÂ´alez.
and carlos gÂ´omez-rodrÂ´Ä±guez.
2019. faster shift-reduce constituentparsing with a non-binary, bottom-up strategy.
artiï¬cial intelligence, 275:559â€“574..daniel fried and dan klein.
2018. policy gradient asa proxy for dynamic oracles in constituency parsing.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume2: short papers), pages 469â€“476, melbourne, aus-tralia.
association for computational linguistics..daniel fried, mitchell stern, and dan klein.
2017. im-proving neural parsing by disentangling model com-bination and reranking effects.
in proceedings of the55th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages161â€“166, vancouver, canada.
association for com-putational linguistics..david gaddy, mitchell stern, and dan klein.
2018.whatâ€™s going on in neural constituency parsers?
an.
analysis.
in proceedings of the 2018 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long papers), pages 999â€“1010,new orleans, louisiana.
association for computa-tional linguistics..carlos gÂ´omez-rodrÂ´Ä±guez and david vilares.
2018.in pro-constituent parsing as sequence labeling.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1314â€“1324, brussels, belgium.
association for computa-tional linguistics..david hall, greg durrett, and dan klein.
2014. lessgrammar, more features.
in proceedings of the 52ndannual meeting of the association for computation-al linguistics (volume 1: long papers), pages 228â€“237, baltimore, maryland.
association for compu-tational linguistics..juneki hong and liang huang.
2018. linear-time con-stituency parsing with rnns and dynamic program-ming.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 2: short papers), pages 477â€“483, melbourne,australia.
association for computational linguistic-s..wenbin jiang, liang huang, and qun liu.
2009. au-tomatic adaptation of annotation standards: chineseword segmentation and pos tagging â€“ a case study.
in proceedings of the joint conference of the 47thannual meeting of the acl and the 4th internationaljoint conference on natural language processingof the afnlp, pages 522â€“530, suntec, singapore.
association for computational linguistics..vidur joshi, matthew peters, and mark hopkins.
2018.extending a parser to distant domains using a fewdozen partially annotated examples.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (volume 1: long paper-s), pages 1190â€“1199, melbourne, australia.
associ-ation for computational linguistics..tadao kasami.
1966..recognitionand syntax-analysis algorithm for context-free lan-guages.
coordinated science laboratory report no.
r-257..an efï¬cient.
nikita kitaev, steven cao, and dan klein.
2019. multi-lingual constituency parsing with self-attention andin proceedings of the 57th annualpre-training.
meeting of the association for computational lin-guistics, pages 3499â€“3505, florence, italy.
associa-tion for computational linguistics..nikita kitaev and dan klein.
2018. constituency pars-in proceedingsing with a self-attentive encoder.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long paper-s), pages 2676â€“2686, melbourne, australia.
associ-ation for computational linguistics..2640john lafferty, andrew mccallum, and fernandopereira.
2001. conditional random ï¬elds: prob-abilistic models for segmenting and labeling se-in proceedings of the eighteenth in-quence data.
ternational conference on machine learning, pages282â€“289..jiangming liu and yue zhang.
2017a..in-ordertransition-based constituent parsing.
transaction-s of the association for computational linguistics,5:413â€“424..jiangming liu and yue zhang.
2017b.
shift-reduceconstituent parsing with neural lookahead features.
transactions of the association for computationallinguistics, 5:45â€“58..lemao liu, muhua zhu, and shuming shi.
2018. im-proving sequence-to-sequence constituency parsing.
in proceedings of the thirty-second aaai confer-ence on artiï¬cial intelligence, pages 4873â€“4880.
aaai press..mitchell p. marcus, beatrice santorini, and mary an-n marcinkiewicz.
1993. building a large annotatedcorpus of english: the penn treebank.
computa-tional linguistics, 19(2):313â€“330..xian qian and yang liu.
2012. joint chinese wordin pro-segmentation, pos tagging and parsing.
ceedings of the 2012 joint conference on empiricalmethods in natural language processing and com-putational natural language learning, pages 501â€“511, jeju island, korea.
association for computa-tional linguistics..sunita sarawagi and w. william cohen.
2004. semi-markov conditional random ï¬elds for informationin neural information processing sys-extraction.
tems, pages 1185â€“1192..yikang shen, zhouhan lin, athul paul jacob, alessan-dro sordoni, aaron courville, and yoshua bengio.
2018. straight to the tree: constituency parsing withneural syntactic distance.
in proceedings of the 56thannual meeting of the association for computation-al linguistics (volume 1: long papers), pages 1171â€“1180, melbourne, australia.
association for compu-tational linguistics..mitchell stern, jacob andreas, and dan klein.
2017. aminimal span-based neural constituency parser.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 818â€“827, vancouver, canada.
association for computational linguistics..zhiyang teng and yue zhang.
2018. two local mod-els for neural constituent parsing.
in proceedings ofthe 27th international conference on computationallinguistics, pages 119â€“132, santa fe, new mexico,usa.
association for computational linguistics..kristina toutanova, dan klein, christopher d. man-ning, and yoram singer.
2003. feature-rich part-of-speech tagging with a cyclic dependency network..in proceedings of the 2003 human language tech-nology conference of the north american chapterof the association for computational linguistics,pages 252â€“259..david vadas and james curran.
2007. adding nounphrase structure to the penn treebank.
in proceed-ings of the 45th annual meeting of the association ofcomputational linguistics, pages 240â€“247, prague,czech republic.
association for computational lin-guistics..david vadas and james r. curran.
2011. parsing nounphrases in the penn treebank.
computational lin-guistics, 37(4):753â€“809..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, undeï¬ne-dukasz kaiser, and illia polosukhin.
2017. atten-tion is all you need.
in proceedings of the 31st inter-national conference on neural information process-ing systems, nipsâ€™17, pages 6000â€“6010, red hook,ny, usa.
curran associates inc..david vilares, mostafa abdou, and anders sÃ¸gaard.
2019. better, faster, stronger sequence tagging con-in proceedings of the 2019 con-stituent parsers.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3372â€“3383, minneapolis, minnesota.
association for computational linguistics..mengqiu wang, kenji sagae, and teruko mitamura.
2006. a fast, accurate deterministic parser for chi-nese.
in proceedings of the 21st international con-ference on computational linguistics and 44th an-nual meeting of the association for computationallinguistics, pages 425â€“432, sydney, australia.
as-sociation for computational linguistics..zhiguo wang, haitao mi, and nianwen xue.
2015.feature optimization for constituent parsing via neu-in proceedings of the 53rd annualral networks.
meeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing (volume 1: longpapers), pages 1138â€“1147, beijing, china.
associa-tion for computational linguistics..zhiguo wang, chengqing zong, and nianwen xue.
2013. a lattice-based framework for joint chineseinword segmentation, pos tagging and parsing.
proceedings of the 51st annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 623â€“627, soï¬a, bulgaria.
as-sociation for computational linguistics..taro watanabe and eiichiro sumita.
2015. transition-in proceedingsbased neural constituent parsing.
of the 53rd annual meeting of the association forcomputational linguistics and the 7th internationaljoint conference on natural language processing(volume 1: long papers), pages 1169â€“1179, beijing,china.
association for computational linguistics..2641yang wei, yuanbin wu, and man lan.
2020. a span-in pro-based linearization for constituent trees.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3267â€“3277, online.
association for computational lin-guistics..yu zhang, houquan zhou, and zhenghua li.
2020.fast and accurate neural crf constituency parsing.
inproceedings of the twenty-ninth international join-t conference on artiï¬cial intelligence, ijcai-20,pages 4046â€“4053.
international joint conferenceson artiï¬cial intelligence organization.
main track..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38â€“45, online.
asso-ciation for computational linguistics..naiwen xue, fei xia, fu-dong chiou, and martapalmer.
2005. the penn chinese treebank: phrasestructure annotation of a large corpus.
nat.
lang.
eng., 11(2):207238..meishan zhang, yue zhang, wanxiang che, and tingliu.
2013. chinese parsing exploiting characters.
inproceedings of the 51st annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 125â€“134, soï¬a, bulgaria.
as-sociation for computational linguistics..xiaoqing zheng, haoyuan peng, yi chen, pengjingzhang, and wenqiang zhang.
2015. character-based parsing with convolutional neural network.
inproceedings of the 24th international conference onartiï¬cial intelligence, ijcaiâ€™15, pages 1054â€“1060.
aaai press..junru zhou and hai zhao.
2019. head-driven phraseinstructure grammar parsing on penn treebank.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2396â€“2408, florence, italy.
association for computation-al linguistics..muhua zhu, yue zhang, wenliang chen, min zhang,fast and accurate shift-and jingbo zhu.
2013.in proceedings of thereduce constituent parsing.
51st annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages434â€“443, soï¬a, bulgaria.
association for computa-tional linguistics..2642