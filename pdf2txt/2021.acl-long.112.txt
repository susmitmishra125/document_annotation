factorising meaning and form for intent-preserving paraphrasing.
tom hosking.
mirella lapata.
institute for language, cognition and computationschool of informatics, university of edinburgh10 crichton street, edinburgh eh8 9ab.
tom.hosking@ed.ac.uk.
mlap@inf.ed.ac.uk.
abstract.
we propose a method for generating para-phrases of english questions that retain theoriginal intent but use a different surface form.
our model combines a careful choice of train-ing objective with a principled information bot-tleneck, to induce a latent encoding space thatdisentangles meaning and form.
we train anencoder-decoder model to reconstruct a ques-tion from a paraphrase with the same meaningand an exemplar with the same surface form,leading to separated encoding spaces.
we usea vector-quantized variational autoencoderto represent the surface form as a set of dis-crete latent variables, allowing us to use a clas-siﬁer to select a different surface form at testtime.
crucially, our method does not requireaccess to an external source of target exem-plars.
extensive experiments and a humanevaluation show that we are able to generateparaphrases with a better tradeoff between se-mantic preservation and syntactic novelty com-pared to previous methods..1.introduction.
a paraphrase of an utterance is “an alternative sur-face form in the same language expressing thesame semantic content as the original form” (mad-nani and dorr, 2010).
for questions, a paraphraseshould have the same intent, and should lead to thesame answer as the original, as in the examples intable 1. question paraphrases are of signiﬁcantinterest, with applications in data augmentation(iyyer et al., 2018), query rewriting (dong et al.,2017) and duplicate question detection (shah et al.,2018), as they allow a system to better identify theunderlying intent of a user query..recent approaches to paraphrasing use informa-tion bottlenecks with vaes (bowman et al., 2016)or pivot languages (wieting and gimpel, 2018) totry to extract the semantics of an input utterance,before projecting back to a (hopefully different)surface form.
however, these methods have lit-.
how is a dialect different from a language?
the differences between language and dialect?
what is the difference between language and dialect?
what is the weight of an average moose?
average weight of the moose?
how much do moose weigh?
how heavy is a moose?
what country do parrots live in?
in what country do parrots live?
where do parrots naturally live?
what part of the world do parrots live in?.
table 1: examples of question paraphrase clusters,drawn from paralex (fader et al., 2013).
each mem-ber of the cluster has essentially the same semantic in-tent, but a different surface form.
each cluster exhibitsvariation in word choice, syntactic structure and evenquestion type.
our task is to generate these differentsurface forms, using only a single example as input..tle to no control over the preservation of the inputmeaning or variation in the output surface form.
other work has speciﬁed the surface form to begenerated (iyyer et al., 2018; chen et al., 2019a;kumar et al., 2020), but has so far assumed that theset of valid surface forms is known a priori..in this paper, we propose separator, a methodfor generating paraphrases that exhibit high varia-tion in surface form while still retaining the orig-inal intent.
our key innovations are: (a) to traina model to reconstruct a target question from aninput paraphrase with the same meaning, and anexemplar with the same surface form, and (b) toseparately encode the form and meaning of ques-tions as discrete and continuous latent variablesrespectively, enabling us to modify the output sur-face form while preserving the original questionintent.
crucially, unlike prior work on syntax con-trolled paraphrasing, we show that we can generatediverse paraphrases of an input question at test timeby inferring a different discrete syntactic encoding,without needing access to reference exemplars..we limit our work to english questions for threereasons: (a) the concept of a paraphrase is more.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1405–1418august1–6,2021.©2021associationforcomputationallinguistics1405figure 1: overview of our approach.
the model is trained to reconstruct a target question from one input with thesame meaning and another input with the same form.
this induces separate latent encoding spaces for meaningand form, allowing us to vary the output form while keeping the meaning constant.
using a discretized space forthe syntactic encoding makes it tractable to predict valid surface forms at test time..clearly deﬁned for questions compared to genericutterances, as question paraphrases should lead tothe same answer; (b) the space of possible surfaceforms is smaller for questions, making the taskmore achievable, and (c) better dataset availability.
however, our approach does not otherwise makeany assumptions speciﬁc to questions..2 problem formulation.
the task is to learn a mapping from an input ques-tion, represented as a sequence of tokens x, toparaphrase(s) y which have different surface formto x, but convey the same intent..our proposed approach, which we callseparator, uses an encoder-decoder model totransform an input question into a latent encodingspace, and then back to an output paraphrase.
wehypothesize that a principled information bottle-neck (section 2.1) and a careful choice of trainingscheme (section 2.2) lead to an encoding space thatseparately represents the intent and surface form.
this separation enables us to paraphrase the inputquestion, varying the surface form of the outputby directly manipulating the syntactic encoding ofthe input and keeping the semantic encoding con-stant (section 2.3).
we assume access to referenceparaphrase clusters during training (e.g., table 1),sets of questions with different surface forms thathave been collated as having the same meaning orintent..our model is a variant of the standard encoder-decoder framework (cho et al., 2014), and con-sists of: (a) a vanilla transformer sentence en-coder (vaswani et al., 2017), that maps an input.
question x to a multi-head sequence of encodings,eh,t = encoder(x); (b) a principled choice ofinformation bottleneck, with a continuous varia-tional path and a discrete vector-quantized path,that maps the encoding sequence to a pair of latentvectors, zsem, zsyn = bottleneck(eh,t), repre-sented in more detail in figure 1; (c) a vanillatransformer decoder, that attends over the latentvectors to generate a sequence of output tokens,ˆy = decoder(zsem, zsyn).
the separation be-tween zsem and zsyn is induced by our proposedtraining scheme, shown in figure 1 and describedin detail in section 2.2..2.1 model architecture.
while the encoder and decoder used by the modelare standard transformer modules, our bottleneckis more complex and we now describe it in moredetail..let the encoder output be {eh,1, .
.
.
, eh,|x|} =encoder(x), where eh,t ∈ rd/ht , h ∈1, ..., ht with ht the number of transformerheads, |x| the length of the input sequence andd the dimension of the transformer.
we ﬁrst poolthis sequence of encodings to a single vector, usingthe multi-head pooling described in liu and lapata(2019).
for each head h, we calculate a distributionover time indexes αh,t using attention:.
αh,t =.
ah,t = kt.
(cid:80).
exp ah,tt(cid:48)∈|x| exp ah,t(cid:48)h eh,t,.
,.
(1).
(2).
with kh ∈ rd/h a learned parameter..1406paraphraseexemplarencoderdecodertarget<latexit sha1_base64="okz9ke0b/h/gupamqmigexgjogy=">aaab+xicbvbns8naen34wetx1koxybe8luqupra9ekxgp6anzbodtes3m7a7kdaqf+lfgyje/sfe/ddu2xy09cha470zzuyfieaaxffbwlldw9/ylg2vt3d29/btg8omjlpfomfieat2qduilqgbhaw0ewu0cgs0gtht1g+nqwkeywecjobhdcb5yblfi/vsu4vwiegypew9teou9+ykw3vncjajv5akkvdv2v/dfszsccqyqbxueg6cfkyvciygl3dtdqllizqajqgsrqd9bhz57pwape+estil0zmpvycygmk9iqltgvec6kvvkv7ndvimr/2myyrfkgy+keyfg7ezjchpcwumxcqqyhq3tzpssbvlamiqmxc8xzexsfo86l1w3fulsu2miknejskjosmeusi1ckfqpeeygznn8krermx6sd6tj3nrilxmhje/sd5/ajyiley=</latexit>zsem<latexit sha1_base64="hq5ncnpdosakxkwri48luxzifdy=">aaab+xicbvbns8naen34wetx1koxxsj4kokoeix68vjbfkabwma7azdunmf3uowh/8slb0w8+k+8+w/ctjlo64obx3szzmwlese1om63tbk6tr6xwdmqbu/s7u3bb4dthaekshanray6adfmcmlawegwbqiyiqlbosh4dup3jkxphsshyblmrwqoecgpasp5tt0h9ghbmd8vfq4zwfh2zak7m+bl4pakhko0ffurp4hpgjejvbcte66tgjctbzwkvlt7qwyjowmyzd1djymy9vlz5qu+ncoah7eyjqhp1n8toym0zqladeyernrrm4r/eb0uwmsv5zjjguk6xxsmakompzhgavemgsgmivrxcyumi6iibrnw1ytglr68tnrndfey7txf1bo3zrwvdixo0bly0rvqodvurc1e0qq9o1f0zuxwi/vufcxbv6xy5gj9gfx5a7yflfs=</latexit>zsynhow [heavy]advp is a [moose]np?how [much]advp is a [surgeons income]np?what is the weight of an average moose?codebookpooling<latexit sha1_base64="eoejbof/zwo8o3mxb5ng7lnkpyk=">aaacaxicbvbns8naen3ur1q/ql4el8eiecqjkhosevfywx5au8pmm2mxbjzhdykwec/+fs8efphqv/dmv3h7cddwbwop92aymecngmt0ng+rsls8srpwxc9tbg5t75r395o6thwdbotfrno+1sc4hazyfnbofndif9dyh9djv3upsvny3ueogw5e+5khnfe0uq984cexawqewgp6yqz53sv0soa9cswpohpyi8sdkqqzod4rf3lbzniijdjbte64toldjcrktebe8lincwvd2oeoozjgolvz5ipcpjzkyiexmixrnqi/jziaat2kfnmzurzoew8s/ud1ugwvuxmxsyog2xrrmaoby3schx1wbqzfybdkfde32mxafwvoqiuzenz5lxdj87tqnled27nk7wowr5eckinyqlxyqwrkhtrjgzdysj7jk3mznqwx6936mlywrnnmpvkd6/mhjyex/a==</latexit>˜esyn<latexit sha1_base64="aqo6jf7/kyssxw9mhnor7wlqdss=">aaacaxicbvbns8naen3ur1q/ol4el8eiecqjkhosevfywx5aw8pmo2mx7izhdykwec/+fs8efphqv/dmv3hb5qctdwye780wm8+pbdfout9wywl5zxwtuf7a2nza3rf39xo6shsdootepfo+1sb4chxkkkavk6dsf9d0r9ctv3kpsvmovmnxdf1jbyepoknopj590eeu+pb2eb7qd1lisl6qqwy9u+xw3cmcrellpexy1hr2v6cfsurciexqrdueg2m3pqo5e5cvoomgmlirhudb0jbk0n10+khmhbul7wsrmhwim1v/t6ruaj2wvumufid63pui/3ntbiplbsrdoeei2wxrkaghi2csh9pnchikssgukw5uddiqksrqhfyyixjzly+sxmnfo6+4t2fl6luer5eckinyqjxyqarkhtrintdysj7jk3mznqwx6936mluwrhxmn/yb9fkdbyqx5w==</latexit>˜esemdiscrete bottleneckcontinuous bottleneckexemplar predictor<latexit sha1_base64="sskmrt0vuov3ein9+ionou/6qcu=">aaab+xicbvbns8naen3ur1q/oh69bivgqssi6lhoxwmf+wftkjvtpf262ytdsbge/bmvhhtx6j/x5r9x2+agrq8ghu/nmdmvsatx6lrfvmltfwnzq7xd2dnd2z+wd49aok4vgyalraw6aduguiqmchtqsrtqkbdqdsz3m789aav5lb9xmoaf0ahkiwcujds37r7cewzh1sn7myyo79tvt+bo4awsrybvuqdrt796g5ileuhkgmrd9dwe/ywq5exaxumlghlkxnqixumljud72fzy3dkzysajy2vkojnxf09knnj6ggwmm6i40svetpzp66yy3vgzl0mkinliuzgkb2nnfomz4aoyiqkhlclubnxyicrk0irvmsf4yy+vktzfzbuquq+x1fpteueznjbtck48ck3q5j40sjmwmihp5jw8wzn1yr1bh4vwklxmhjm/sd5/aggslcq=</latexit>xsem<latexit sha1_base64="ifzmbqkuuiu4esepyhhavimcxdo=">aaab+xicbvbns8naen3ur1q/oh69lbbbu0le0wpri8ck9gpaedbbtbt0swm7k2ii+sdepcji1x/izx/jts1bwx8mpn6bywzekaiuwxg+rcra+sbmvnw7tro7t39ghx51djwqyto0frhqbuqzwsvrawfbeolijaoe6watu5nfntkleswfiuuyf5gr5cgnbizk2/ya2bmeyd4r/fxnsvdtutnw5scrxc1jhzvo+fbxybjtngisqcba910nas8ncjgvrkgnus0sqidkxpqgshix7exzywt8zpqhdmnlsgkeq78nchjpnuwb6ywijpwynxp/8/ophddezmwsapn0sshmbyyyz2laq64ybzezqqji5lzmx0qrciasmgnbxx55lxqugu5vw3m4rddvyziq6asdonpkomvurpeohdqioil6rq/ozcqtf+vd+li0vqxy5hj9gfx5a4gpldk=</latexit>xsyn<latexit sha1_base64="hw+q1/cbuw/wccghzl0ipt0+y/e=">aaab8xicbvbns8naen34wetx1aoxxsj4kokoeix68vjbfmgbymy7azdunmf3ipbqf+hfgyje/tfe/ddu2xy09cha470zzuyfirqgxffbwvpewv1bl2wun7e2d3zle/sne6eaq53hmtatgbmqqkedbupojrpyfehobsprid98bg1ero5wliafsb4soeamrftqqxjcimzux91s2a24u9bf4uwkthluuqwvti/maqqkuwtgtd03qt9jggwxmc52ugmj40pwh7alikvg/gx68zgew6vhw1jbukin6u+jjexgjkladkymb2bem4j/ee0uw0s/eypjersflqptstgmk/dpt2jgkeewmk6fvzxyadomow2paepw5l9eji3tindecw/pytwrpi4cosrh5ir45ijuyq2pktrhrjfn8krehoo8oo/ox6x1yclndsgfoj8/+qsrha==</latexit>ywe then take a weighted average of a linear pro-jection of the encodings, to give pooled output ˜eh,.
˜eh =.
αh,t(cid:48)vheh,t(cid:48),.
(3).
(cid:88).
t(cid:48)∈|x|.
with vh ∈ rd/h×d/h a learned parameter..transformer heads are assigned either to a se-mantic group hsem, that will be trained to encodethe intent of the input, ˜esem = [.
.
.
; ˜eh; .
.
.
], h ∈hsem, or to a syntactic group hsyn, that willbe trained to represent the surface form ˜esyn =[.
.
.
; ˜eh; .
.
.
], h ∈ hsyn (see figure 1)..the space of possible question intents is ex-tremely large and may be reasonably approximatedby a continuous vector space.
however, the possi-ble surface forms are discrete and smaller in num-ber.
we therefore use a vector-quantized varia-tional autoencoder (vq-vae, van den oord et al.,2017) for the syntactic encoding zsyn, and modelthe semantic encoding zsem as a continuous gaus-sian latent variable, as shown in the upper and lowerparts of figure 1, respectively..vector quantization let qh be discrete latentvariables corresponding to the syntactic quantizerheads, h ∈ hsyn.1 each variable can be one ofk possible latent codes, qh ∈ [0, k].
the headsuse distinct codebooks, ch ∈ rk×d/h , whichmap each discrete code to a continuous embeddingch(qh) ∈ rd/h .
given sentence x and its pooledencoding {˜e1, ..., ˜eh }, we independently quantizethe syntactic subset of the heads h ∈ hsyn to theirnearest codes from ch and concatenate, giving thesyntactic encoding.
zsyn = [c1(q1); .
.
.
; c|hsyn|(q|hsyn|)]..(4).
the quantizer module is trained through back-propagation using straight-through estimation(bengio et al., 2013), with an additional loss termto constrain the embedding space as described invan den oord et al.
(2017),.
lcstr = λ.
˜eh − sg(ch(qh)).
(5).
(cid:17)(cid:13)(cid:13)(cid:13)2.,.
(cid:88).
(cid:16).
(cid:13)(cid:13)(cid:13).
h∈hsyn.
where the stopgradient operator sg(·) is deﬁned asidentity during forward computation and zero onbackpropagation, and λ is a weight that controlsthe strength of the constraint.
we follow the soft.
1the number and dimensionality of the quantizer heads.
need not be the same as the number of transformer heads..em and exponentially moving averages trainingapproaches described in earlier work (roy et al.,2018; angelidis et al., 2021), which we ﬁnd im-prove training stability..variational bottleneck for the semantic path,we introduce a learned gaussian posterior, thatrepresents the encodings as smooth distributionsin space instead of point estimates (kingmaformally, φ(zh|eh) ∼and welling, 2014).
n (µ(eh), σ(eh)), where µ(·) and σ(·) are learnedlinear transformations.
to avoid vanishingly smallvariance and to encourage a smooth distribution,a prior is introduced, p(zh) ∼ n (0, 1).
the vaeobjective is the standard evidence lower bound(elbo), given by.
elbo = −kl[φ(zh|eh)||p(zh)].
+ eφ[log p(eh|zh)]..(6).
we use the usual gaussian reparameterisationtrick, and approximate the expectation in equa-tion (6) by sampling from the training set and up-dating via backpropagation (kingma and welling,2014).
the vae component therefore only adds anadditional kl term to the overall loss,.
lkl = −kl[φ(zh|eh)||p(zh)]..(7).
in sum, bottleneck(eh,t) maps a sequenceof token encodings to a pair of vectors zsem, zsyn,with zsem a continuous latent gaussian, and zsyn acombination of discrete code embeddings..2.2 factorised reconstruction objective.
we now describe the training scheme that causesthe model to learn separate encodings for meaningand form: zsem should encode only the intent ofthe input, while zsyn should capture any informa-tion about the surface form of the input.
althoughwe refer to zsyn as the syntactic encoding, it willnot necessarily correspond to any speciﬁc syntac-tic formalism.
we also acknowledge that meaningand form are not completely independent of eachother; arbitrarily changing the form of an utteranceis likely to change its meaning.
however, it is pos-sible for the same intent to have multiple phrasings, and it is this ‘local independence’ that we intendto capture..we create triples {xsem, xsyn, y}, where xsemhas the same meaning but different form to y(i.e., it is a paraphrase, as in table 1) and xsyn is aquestion with the same form but different meaning.
1407inputchunker outputtemplateexemplarinput.
how heavy is a moose?
how [heavy]advp is a [moose]np ?
how advp is a np ?
how much is a surgeon’s income?
what country do parrots live in.
chunker output what [country]np do [parrots]np [live]vp in ?.
templateexemplar.
what np do np vp in ?
what religion do portuguese believe in?.
table 2: examples of the exemplar retrieval processfor training.
the input is tagged by a chunker, ignoringstopwords.
an exemplar with the same template is thenretrieved from a different paraphrase cluster..
(i.e., it shares the same syntactic template as y),which we refer to as an exemplar.
we describethe method for retrieving these exemplars in sec-tion 2.3. the model is then trained to generate atarget paraphrase y from the semantic encodingzsem of the input paraphrase xsem, and from thesyntactic encoding zsyn of the exemplar xsyn, asdemonstrated in figure 1..recalling the additional losses from the varia-tional and quantized bottlenecks, the ﬁnal com-bined training objective is given by.
l = ly + lcstr + lkl,.
(8).
where ly(xsem, xsyn) is the cross-entropy lossof teacher-forcing the decoder to generate y fromzsem(xsem) and zsyn(xsyn)..2.3 exemplars.
it is important to note that not all surface forms arevalid or licensed for all question intents.
as shownin figure 1, our approach requires exemplars dur-ing training to induce the separation between latentspaces.
we also need to specify the desired surfaceform at test time, either by supplying an exemplaras input or by directly predicting the latent codes.
the output should have a different surface form tothe input but remain ﬂuent..exemplar construction during training, we re-trieve exemplars xsyn from the training data follow-ing a process which ﬁrst identiﬁes the underlyingsyntax of y, and ﬁnds a question with the samesyntactic structure but a different, arbitrary mean-ing.
we use a shallow approximation of syntax,to ensure the availability of equivalent exemplarsin the training data.
an example of the exemplarretrieval process is shown in table 2; we ﬁrst applya chunker (flairnlp, akbik et al., 2018) to y, thenextract the chunk label for each tagged span, ignor-ing stopwords.
this gives us the template that y.follows.
we then select a question at random fromthe training data with the same template to givexsyn.
if no other questions in the dataset use thistemplate, we create an exemplar by replacing eachchunk with a random sample of the same type..we experimented with a range of approaches todetermining question templates, including usingpart-of-speech tags and (truncated) constituencyparses.
we found that using chunks and preservingstopwords gave a reasonable level of granularitywhile still combining questions with a similar form.
the templates (and corresponding exemplars) needto be granular enough that the model is forced touse them, but abstract enough that the task is notimpossible to learn..prediction at test timein general, we do notassume access to reference exemplars at test timeand yet the decoder must generate a paraphrasefrom semantic and syntactic encodings.
since ourlatent codes are separated, we can directly predictthe syntactic encoding, without needing to retrieveor generate an exemplar.
furthermore, by using adiscrete representation for the syntactic space, wereduce this prediction problem to a simple classi-ﬁcation task.
formally, for an input question x,we learn a distribution over licensed discrete codesqh, h ∈ ˜hsyn.
we assume that the heads are in-dependent, so that p(q1, .
.
.
, q ˜hsyni p(qi).
we use a small fully connected network with thesemantic and syntactic encodings of x as inputs,giving p(qh|x) = mlp(zsem(x), zsyn(x))..) = (cid:81).
the network is trained to maximize the like-lihood of all other syntactic codes licensed byeach input.
we calculate the discrete syntacticcodes for each question in a paraphrase cluster,and minimize the cross-entropy loss of the networkwith respect to these codes.
at test time, we setqh = argmaxq(cid:48)h|xtest)]..[p(q(cid:48).
h.3 experimental setup.
datasets we evaluate our approach on twodatasets: paralex (fader et al., 2013), a dataset ofquestion paraphrase clusters scraped from wikian-swers; and quora question pairs (qqp)2 sourcedfrom the community question answering forumquora.
we observed that a signiﬁcant fraction ofthe questions in paralex included typos or were un-grammatical.
we therefore ﬁlter out any questionsmarked as non-english by a language detection.
2https://www.kaggle.com/c/quora-question-pairs.
1408script (lui and baldwin, 2012), then pass the ques-tions through a simple spellchecker.
while this de-structively edited some named entities in the ques-tions, it did so in a consistent way across the wholedataset.
there is no canonical split for paralex, sowe group the questions into clusters of paraphrases,and split these clusters into train/dev/test partitionswith weighting 80/10/10.
similarly, qqp does nothave a public test set.
we therefore partitioned theclusters in the validation set randomly in two, togive us our dev/test splits.
summary statistics ofthe resulting datasets are given in appendix b. allscores reported are on our test split..model conﬁguration following previous work(kaiser et al., 2018; angelidis et al., 2021), ourquantizer uses multiple heads (h = 4) with distinctcodebooks to represent the syntactic encoding as4 discrete categorical variables qh, with zsyn givenby the concatenation of their codebook embeddingsch(qh).
we use a relatively small codebook sizeof k = 256, relying on the combinatoric powerof the multiple heads to maintain the expressivityof the model.
we argue that, assuming each headlearns to capture a particular property of a template(see section 4.3), the number of variations in eachproperty is small, and it is only through combina-tion that the space of possible templates becomeslarge..we include a detailed list of hyperparametersin appendix a. our code is available at http://github.com/tomhosking/separator..comparison systems we compare separatoragainst several related systems.
these include amodel which reconstructs y only from xsem, withno signal for the desired form of the output.
in otherwords, we derive both zsem and zsyn from xsem,and no separation between meaning and form islearned.
this model uses a continuous gaussianlatent variable for both zsyn and zsem, but is oth-erwise equivalent in architecture to separator.
we refer to this as the vae baseline.
we also ex-periment with a vanilla autoencoder or ae baselineby removing the variational component, such thatzsem, zsyn = ˜esem, ˜esyn..we include our own implementation of thevq-vae model described in roy and grangier(2019).
they use a quantized bottleneck for bothzsem and zsyn, with a large codebook k = 64, 000,h = 8 heads and a residual connection within thequantizer.
for qqp, containing only 55,611 train-.
encodingzsemzsynz.encodingzsemzsynz.cluster typeparaphrase template.
(a) vae baseline.
cluster typeparaphrase template.
0.9430.9520.960.
0.9440.0650.307.
0.0960.0920.096.
0.0530.8660.849.
(b) separator.
table 3: retrieval accuracies for each encoding foreach cluster type.
the vae baseline is trained only onparaphrase pairs and receives no signal for the desiredform of the output.
separator is able to learn sepa-rate encodings for meaning and form, with negligibleloss in semantic encoding performance..ing clusters, the conﬁguration in roy and grangier(2019) leaves the model overparameterized andtraining did not converge; we instead report resultsfor k = 1, 000..paranmt (wieting and gimpel, 2018) trans-lates input sentences into a pivot language (czech),then back into english.
although this system wastrained on high volumes of data (including com-mon crawl), the training data contains relativelyfew questions, and we would not expect it to per-form well in the domain under consideration.
‘di-verse paraphraser using submodularity’ (dips; ku-mar et al.
2019) uses submodular optimisationto increase the diversity of samples from a stan-dard encode-decoder model.
latent bag-of-words(bow; fu et al.
2019) uses an encoder-decodermodel with a discrete bag-of-words as the latentencoding.
sow/reap (goyal and durrett, 2020)uses a two stage approach, deriving a set of feasi-ble syntactic rearrangements that is used to guidea second encoder-decoder model.
we additionallyimplement a simple tf-idf baseline (jones, 1972),retrieving the question from the training set withthe highest similarity to the input.
finally, we in-clude a basic copy baseline as a lower bound, thatsimply uses the input question as the output..4 results.
our experiments were designed to answer threequestions: (a) does separator effectively fac-torize meaning and form?
(b) does separator.
1409model.
copyvaeaetf-idfvq-vaeparanmtdipssow/reaplbowseparatororacle.
paralex.
bleu ↑ self-bleu ↓100.0066.1275.7125.2565.7139.9029.5837.0735.8635.3724.55.
37.1040.2640.1025.0840.2620.4224.9033.0934.9636.3653.37.qqp.
ibleu ↑ bleu ↑ self-bleu ↓100.0032.6135.2919.3639.8119.9061.8122.7326.1516.1956.4224.2432.4518.4724.1912.6429.0016.1714.8414.7016.0424.50.
−4.038.355.369.988.472.328.5612.0413.7114.84.
29.99.ibleu ↑−7.172.961.99−2.633.430.043.191.592.625.84.
12.34.table 4: generation results, without access to oracle exemplars.
our approach achieves the highest ibleu scores,indicating the best tradeoff between output diversity and ﬁdelity to the reference paraphrases..manage to generate diverse paraphrases (while pre-serving the intent of the input)?
(c) what does theunderlying quantized space encode (i.e., can weidentify any meaningful syntactic properties)?
weaddress each of these questions in the followingsections..4.1 veriﬁcation of separation.
inspired by chen et al.
(2019b) we use a semantictextual similarity task and a template detection taskto conﬁrm that separator does indeed lead to en-codings {zsem, zsyn} in latent spaces that representdifferent types of information..using the test set, we construct clusters of ques-tions that share the same meaning csem, and clus-ters that share the same template csyn.
for eachcluster cq ∈ {csem, csyn}, we extract one ques-tion at random xq ∈ cq, compute its encodings{zsem, zsyn, z}3, and its cosine similarity to the en-codings of all other questions in the test set.
wetake the question with maximum similarity to thequery xr, r = argmaxr(cid:48)(zq.zr(cid:48)), and compare thecluster that it belongs to, cr, to the query clusteri(cq = cr), giving a retrieval accuracy score foreach encoding type and each clustering type.
forthe vae, we set {zsem, zsyn} to be the same headsof z as the separated model..table 3 shows that our approach yields encod-ings that successfully factorise meaning and form,with negligible performance loss compared to thevae baseline; paraphrase retrieval performance us-ing zsem for the separated model is comparable tousing z for the vae..3z refers to the combined encoding, i.e., [zsem; zsyn]..4.2 paraphrase generation.
automatic evaluation while we have shownthat our approach leads to disentangled represen-tations, we are ultimately interested in generatingdiverse paraphrases for unseen data.
that is, givensome input question, we want to generate an outputquestion with the same meaning but different form.
we use ibleu (sun and zhou, 2012) as ourprimary metric, a variant of bleu (papineni et al.,2002; post, 2018) that is penalized by the similaritybetween the output and the input,.
ibleu = αbleu(output, ref erences)−(1 − α)bleu(output, input),.
(9).
where α = 0.7 is a constantthat weightsthe tradeoff between ﬁdelity to the referencesand variation from the input.
we also reportthe usual bleu(output, ref erences) as well asself-bleu(output, input).
the latter allows usto examine whether the models are making trivialchanges to the input.
the paralex test set con-tains 5.6 references on average per cluster, whileqqp contains only 1.3. this leads to lower bleuscores for qqp in general, since the models areevaluated on whether they generated the speciﬁcparaphrase(s) present in the dataset..table 4 shows that the copy, vae and aemodels display relatively high bleu scores, butachieve this by ‘parroting’ the input; they are goodat reconstructing the input, but introduce little vari-ation in surface form, reﬂected in the high self-bleu scores.
this highlights the importance ofconsidering similarity to both the references and tothe input.
the tf-idf baseline performs surprisingly.
1410input what is the most known singer?
vae what is the most known singer?
dips what was the most known famous singer?.
sow/reap what is the most famous singer?
latent bow what is the most famous singer?
separator who is the most famous singer in america?.
input what is the income for a soccer player?
vae what is the salary for a soccer player?
dips what is the median income in soccer?.
sow/reap what is us cer?
latent bow what is the salary of a soccer [unk]?
separator how much is a soccer players’ salary?.
input what has been the economic impact from brexit.
vae what has been the economic impact of brexit.
referendum so far?.
referendum so far?.
dips what will be a impact of brexit referendum?.
sow/reap how do i spend my virginity?
latent bow how did brexit referendum impact the brexit.
separator how much will the brexit referendum cost?.
input what are the basics i should know before learn-.
vae what are the basics should i know before learn-.
dips how do i know before i want to learn hadoop?.
sow/reap how can i know before learning hadoop?
latent bow what are the basics of learning hadoop?
separator how much should i know before learning.
hadoop?.
referendum?.
ing hadoop?.
ing hadoop?.
table 5: examples of output generated by various ap-proaches for a given input, from paralex and qqp.
sep-arator is able to generate questions with a differentsyntactic form to the input..well on paralex; the large dataset size makes itmore likely that a paraphrase cluster with a similarmeaning to the query exists in the training set..the other comparison systems (in the secondblock in table 4) achieve lower self-bleu scores,indicating a higher degree of variation introduced,but this comes at the cost of much lower scores withrespect to the references.
separator achieves thehighest ibleu scores, indicating the best balancebetween ﬁdelity to the references and novelty com-pared to the input.
we give some example output intable 5; while the other systems mostly introducelexical variation, separator is able to produceoutput with markedly different syntactic structureto the input, and can even change the question typewhile successfully preserving the original intent..the last row in table 4 (oracle) reports re-sults when our model is given a valid exemplarto use directly for generation, thus bypassing thecode prediction problem.
for each paraphrase clus-ter, we select one question at random to use asinput, and select another to use as the target.
weretrieve a question from the training set with the.
same template as the target to use as an oracle ex-emplar.
this represents an upper bound on ourmodel’s performance.
while separator outper-forms existing methods, our approach to predictingsyntactic codes (using a shallow fully-connectednetwork) is relatively simple.
separator usingoracle exemplars achieves by far the highest scoresin table 4, demonstrating the potential expressivityof our approach when exemplars are guaranteed tobe valid.
a more powerful code prediction modelcould close the gap to this upper bound, as well asenabling the generation of multiple diverse para-phrases for a single input question.
however, weleave this to future work..human evaluation in addition to automaticevaluation we elicited judgements from crowd-workers on amazon mechanical turk.
speciﬁcally,they were shown a question and two paraphrasesthereof (corresponding to different systems) andasked to select which one was preferred along threethe dissimilarity of the paraphrasedimensions:compared to the original question, how well theparaphrase reﬂected the meaning of the original,and the ﬂuency of the paraphrase (see appendix c).
we evaluated a total of 200 questions sampledequally from both paralex and qqp, and collected3 ratings for each sample.
we assigned each systema score of +1 when it was selected, −1 when theother system was selected, and took the mean overall samples.
negative scores indicate that a sys-tem was selected less often than an alternative.
wechose the four best performing models accordingto table 4 for our evaluation: separator, dips(kumar et al., 2019), latent bow (fu et al., 2019)and vae..figure 2 shows that although the vae baselineis the best at preserving question meaning, it isalso the worst at introducing variation to the out-put.
separator introduces more variation thanthe other systems evaluated and better preservesthe original question intent, as well as generatingsigniﬁcantly more ﬂuent output (using a one-wayanova with post-hoc tukey hsd test, p<0.05)..4.3 analysis.
when predicting latent codes at test time, we as-sume that the code for each head may be predictedindependently of the others, as working with thefull joint distribution would be intractable.
we nowexamine this assumption as well as whether differ-ent encodings represent distinct syntactic proper-.
1411figure 2: results of our human evaluation.
althoughthe vae baseline is the best at preserving questionmeaning, it is the worst at introducing variation to theoutput.
separator offers the best balance betweendissimilarity and meaning preservation, and is more ﬂu-ent than both dips and latent bow..ties.
following angelidis et al.
(2021), we computethe probability of a question property f1, f2, .
.
.
tak-ing a particular value a, conditioned by head h andquantized code kh as.
(cid:80)x∈x.
p (fi|h, kh) =.
i(qh(x) = kh)i(fi(x) = a).
(cid:80)x∈x.
i(qh(x) = kh).
,(10).
where i(·) is the indicator function, and examplesof values a are shown in figure 3. we then cal-culate the mean entropy of these distributions, todetermine how property-speciﬁc each head is:.
hh =.
p (a|h, kh) log p (a|h, kh).
(11).
1k.(cid:88).
(cid:88).
kh.
a.heads with lower entropies are more predictiveof a property, indicating specialisation and there-fore independence.
figure 3 shows our analysisfor four syntactic properties: head #2 has learnedto control the high level output structure, includ-ing the question type or wh- word, and whetherthe question word appears at the beginning or endof the question.
head #3 controls which type ofprepositional phrase is used.
the length of the out-put is not determined by any one head, implyingthat it results from other properties of the surfaceform.
future work could leverage this disentangle-ment to improve the exemplar prediction model,and could lead to more ﬁne-grained control overthe generated output form..in summary, we ﬁnd that separator success-fully learns separate encodings for meaning andform.
separator is able to generate question.
figure 3: predictive entropy by head for various ques-tion properties - lower entropy indicates higher predic-tive power..paraphrases with a better balance of diversity andintent preservation compared to prior work.
al-though we are able to identify some high-levelproperties encoded by each of the syntactic latentvariables, further work is needed to learn inter-pretable syntactic encodings..5 related work.
paraphrasing prior work on generating para-phrases has looked at extracting sentences withsimilar meaning from large corpora (barzilay andmckeown, 2001; bannard and callison-burch,2005; ganitkevitch et al., 2013), or identifyingparaphrases from sources that are weakly aligned(dolan et al., 2004; coster and kauchak, 2011)..more recently, neural approaches to paraphras-ing have shown promise.
several models have usedan information bottleneck to try to encode the se-mantics of the input, including vaes (bowmanet al., 2016), vq-vaes (van den oord et al., 2017;roy and grangier, 2019), and a latent bag-of-wordsmodel (fu et al., 2019).
other work has relied onthe strength of neural machine translation models,translating an input into a pivot language and thenback into english (mallinson et al., 2017; wietingand gimpel, 2018; hu et al., 2019)..kumar et al.
(2019) use submodular functionmaximisation to improve the diversity of para-phrases generated by an encoder-decoder model.
dong et al.
(2017) use an automatic paraphrasingsystem to rewrite inputs to a question answeringsystem at inference time, reducing the sensitivityof the system to the speciﬁc phrasing of a query..syntactic templates the idea of generatingparaphrases by controlling the structure of the out-put has seen recent interest, but most work so farhas assumed access to a template oracle.
iyyer et al..1412meaningdissimilarityfluency6040200204060relative preference %+58-6-12-39-56+7+2+47+38+3-20-21vaeseparator (ours)latent bowdips1234head #wh- wordfrontinglengthprepositionquestion propertyproperty entropy by quantizer head0.40.60.81.01.21.4(2018) use linearized parse trees as a template, thensample paraphrases by using multiple templatesand reranking the output.
chen et al.
(2019a) usea multi task objective to train a model to generateoutput that follows an input template.
their ap-proach is limited by their use of automatically gen-erated paraphrases for training, and their relianceon the availability of oracle templates.
bao et al.
(2019) use a discriminator to separate spaces, butrely on noising the latent space to induce variationin the output form.
their results show good ﬁdelityto the references, but low variation compared tothe input.
goyal and durrett (2020) use the artif-ically generated dataset paranmt-50m (wietingand gimpel, 2018) for their training and evaluation,which displays low output variation according toour results.
kumar et al.
(2020) show strong perfor-mance using full parse trees as templates, but focuson generating output with the correct parse and donot consider the problem of template prediction..huang and chang (2021) independently and con-currently propose training a model with a similar‘split training’ approach to ours, but using con-stituency parses instead of exemplars, and a ‘bag-of-words’ instead of reference paraphrases.
theirapproach has the advantage of not requiring para-phrase clusters during training, but they do notattempt to solve the problem of template predic-tion and rely on the availability of oracle targettemplates..russin et al.
(2020) modify the architecture of anencoder-decoder model, introducing an inductivebias to encode the structure of inputs separatelyfrom the lexical items to improve compositionalgeneralisation on an artiﬁcial semantic parsing task.
chen et al.
(2019b) use a multi-task setup to gener-ate separated encodings, but do not experiment withgeneration tasks.
shu et al.
(2019) learn discretelatent codes to introduce variation to the output ofa machine translation system..6 conclusion.
we present separator, a method for generatingparaphrases that balances high variation in surfaceform with strong intent preservation.
our approachconsists of: (a) a training scheme that causes anencoder-decoder model to learn separated latentencodings, (b) a vector-quantized bottleneck thatresults in discrete variables for the syntactic en-coding, and (c) a simple model to predict differentyet valid surface forms for the output.
extensive.
experiments and a human evaluation show that ourapproach leads to separated encoding spaces withnegligible loss of expressivity, and is able to gener-ate paraphrases with a better balance of variationand semantic ﬁdelity than prior methods..in future, we would like to investigate the prop-erties of the syntactic encoding space, and improveon the code prediction model.
it would also beinteresting to reduce the levels of supervision re-quired to train the model, and induce the separationwithout an external syntactic model or referenceparaphrases..acknowledgements.
we thank our anonymous reviewers for their feed-back.
we are grateful to stefanos angelidis formany valuable discussions, and hao tang for theircomments on the paper.
this work was supportedin part by the ukri centre for doctoral training innatural language processing, funded by the ukri(grant ep/s022481/1) and the university of edin-burgh.
lapata acknowledges the support of the eu-ropean research council (award number 681760,“translating multiple modalities into text”)..references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649, santa fe, new mexico, usa.
associ-ation for computational linguistics..stefanos angelidis, reinald amplayo, yoshihikosuhara, xiaolan wang, and mirella lapata.
2021.extractive opinion summarization in quantized trans-former spaces.
transactions of the association forcomputational linguistics, 9(0):277–293..colin bannard and chris callison-burch.
2005. para-phrasing with bilingual parallel corpora.
in proceed-ings of the 43rd annual meeting of the associationfor computational linguistics (acl’05), pages 597–604, ann arbor, michigan.
association for compu-tational linguistics..yu bao, hao zhou, shujian huang, lei li, lili mou,olga vechtomova, xin-yu dai, and jiajun chen.
2019. generating sentences from disentangled syn-in proceedings of thetactic and semantic spaces.
57th annual meeting of the association for com-putational linguistics, pages 6008–6019, florence,italy.
association for computational linguistics..regina barzilay and kathleen r. mckeown.
2001. ex-tracting paraphrases from a parallel corpus.
in pro-ceedings of the 39th annual meeting of the associ-.
1413ation for computational linguistics, pages 50–57,toulouse, france.
association for computationallinguistics..yoshua bengio, n. l´eonard, and aaron c. courville.
2013. estimating or propagating gradients throughstochastic neurons for conditional computation.
corr, abs/1308.3432..samuel r. bowman, luke vilnis, oriol vinyals, an-drew dai, rafal jozefowicz, and samy bengio.
2016. generating sentences from a continuousin proceedings of the 20th signll con-space.
ference on computational natural language learn-ing, pages 10–21, berlin, germany.
association forcomputational linguistics..mingda chen, qingming tang, sam wiseman, andkevin gimpel.
2019a.
controllable paraphrase gen-eration with a syntactic exemplar.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 5972–5984, florence,italy.
association for computational linguistics..mingda chen, qingming tang, sam wiseman, andkevin gimpel.
2019b.
a multi-task approach fordisentangling syntax and semantics in sentence rep-resentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 2453–2464, minneapolis, minnesota.
associ-ation for computational linguistics..kyunghyun cho, bart van merri¨enboer, dzmitry bah-danau, and yoshua bengio.
2014. on the propertiesof neural machine translation: encoder–decoder ap-proaches.
in proceedings of ssst-8, eighth work-shop on syntax, semantics and structure in statisti-cal translation, pages 103–111, doha, qatar.
asso-ciation for computational linguistics..william coster and david kauchak.
2011. simple en-glish wikipedia: a new text simpliﬁcation task.
inproceedings of the 49th annual meeting of the asso-ciation for computational linguistics: human lan-guage technologies, pages 665–669, portland, ore-gon, usa.
association for computational linguis-tics..bill dolan, chris quirk, and chris brockett.
2004.unsupervised construction of large paraphrase cor-pora: exploiting massively parallel news sources.
in coling 2004: proceedings of the 20th inter-national conference on computational linguistics,pages 350–356, geneva, switzerland.
coling..li dong, jonathan mallinson, siva reddy, and mirellalapata.
2017. learning to paraphrase for questionanswering.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 875–886, copenhagen, denmark.
associ-ation for computational linguistics..anthony fader, luke zettlemoyer, and oren etzioni.
2013. paraphrase-driven learning for open questionanswering.
in proceedings of the 51st annual meet-ing of the association for computational linguis-tics (volume 1: long papers), pages 1608–1618,soﬁa, bulgaria.
association for computational lin-guistics..yao fu, yansong feng, and john p cunningham.
2019.paraphrase generation with latent bag of words.
inadvances in neural information processing systems,volume 32, pages 13645–13656.
curran associates,inc..juri ganitkevitch, benjamin van durme, and chrisppdb: the paraphrasecallison-burch.
2013.database.
in proceedings of the 2013 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 758–764, atlanta, georgia.
associa-tion for computational linguistics..tanya goyal and greg durrett.
2020. neural syntacticpreordering for controlled paraphrase generation.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, pages 238–252, online.
association for computational linguis-tics..j. edward hu, rachel rudinger, matt post, and ben-jamin van durme.
2019.parabank: monolin-gual bitext generation and sentential paraphrasingvia lexically-constrained neural machine translation.
corr, abs/1901.03644..kuan-hao huang and kai-wei chang.
2021. generat-ing syntactically controlled paraphrases without us-ing annotated parallel pairs.
in proceedings of the16th conference of the european chapter of theassociation for computational linguistics: mainvolume, pages 1022–1033, online.
association forcomputational linguistics..mohit iyyer, john wieting, kevin gimpel, and lukezettlemoyer.
2018. adversarial example generationwith syntactically controlled paraphrase networks.
in proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 1875–1885, neworleans, louisiana.
association for computationallinguistics..karen sp¨arck jones.
1972. a statistical interpretationof term speciﬁcity and its application in retrieval.
journal of documentation, 28:11–21..lukasz kaiser, aurko roy, ashish vaswani, nikiandparmar, samy bengio,noam shazeer.
2018. fast decoding in sequencecorr,models using discrete latent variables.
abs/1803.03382..jakob uszkoreit,.
diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,.
1414iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..pennsylvania, usa.
association for computationallinguistics..diederik p. kingma and max welling.
2014. auto-in 2nd internationalencoding variational bayes.
conference on learning representations,iclr2014, banff, ab, canada, april 14-16, 2014, con-ference track proceedings..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, belgium, brussels.
association for computa-tional linguistics..ashutosh kumar, kabir ahuja, raghuram vadapalli,and partha talukdar.
2020.syntax-guided con-transactionstrolled generation of paraphrases.
of the association for computational linguistics,8(0):330–345..aurko roy and david grangier.
2019. unsupervisedparaphrasing without translation.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 6033–6039, florence,italy.
association for computational linguistics..ashutosh kumar, satwik bhattamishra, manik bhan-dari, and partha talukdar.
2019.submodularoptimization-based diverse paraphrasing and its ef-fectiveness in data augmentation.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 3609–3619, minneapolis,minnesota.
association for computational linguis-tics..yang liu and mirella lapata.
2019. hierarchical trans-formers for multi-document summarization.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5070–5081, florence, italy.
association for computa-tional linguistics..marco lui and timothy baldwin.
2012. langid.py: anin pro-off-the-shelf language identiﬁcation tool.
ceedings of the acl 2012 system demonstrations,pages 25–30, jeju island, korea.
association forcomputational linguistics..laurens van der maaten and geoffrey e. hinton.
2008.visualizing high-dimensional data using t-sne.
jour-nal of machine learning research, 9:2579–2605..nitin madnani and bonnie j. dorr.
2010. generat-ing phrasal and sentential paraphrases: a surveyof data-driven methods.
computational linguistics,36(3):341–387..jonathan mallinson, rico sennrich, and mirella lap-ata.
2017. paraphrasing revisited with neural ma-in proceedings of the 15th con-chine translation.
ference of the european chapter of the associationfor computational linguistics: volume 1, long pa-pers, pages 881–893, valencia, spain.
associationfor computational linguistics..aaron van den oord, oriol vinyals, and koraykavukcuoglu.
2017. neural discrete representationin advances in neural information pro-learning.
cessing systems, volume 30. curran associates, inc..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,.
aurko roy, ashish vaswani, arvind neelakantan, andniki parmar.
2018. theory and experiments on vec-tor quantized autoencoders.
corr, abs/1805.11063..jacob russin, jason jo, randall o’reilly, and yoshuabengio.
2020. compositional generalization by fac-torizing alignment and translation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics: student research work-shop, pages 313–327, online.
association for com-putational linguistics..darsh shah, tao lei, alessandro moschitti, salva-tore romeo, and preslav nakov.
2018. adversar-ial domain adaptation for duplicate question detec-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 1056–1063, brussels, belgium.
associationfor computational linguistics..raphael shu, hideki nakayama, and kyunghyun cho.
2019. generating diverse translations with sentencein proceedings of the 57th annual meet-codes.
ing of the association for computational linguis-tics, pages 1823–1827, florence, italy.
associationfor computational linguistics..hong sun and ming zhou.
2012. joint learning of adual smt system for paraphrase generation.
in pro-ceedings of the 50th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 38–42, jeju island, korea.
associa-tion for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30. curran associates, inc..john wieting and kevin gimpel.
2018. paranmt-50m: pushing the limits of paraphrastic sentence em-beddings with millions of machine translations.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 451–462, melbourne, australia.
association for computational linguistics..ziang xie, sida i. wang, jiwei li, daniel l´evy, aim-ing nie, dan jurafsky, and andrew y. ng.
2017..1415data noising as smoothing in neural network lan-in 5th international conferenceguage models.
on learning representations, iclr 2017, toulon,france, april 24-26, 2017, conference track pro-ceedings.
openreview.net..1416a hyperparameters.
hyperparameters were selected by manual tuning,based on a combination of: (a) validation encodingseparation, (b) validation bleu scores using oracleexemplars, and (c) validation ibleu scores usingpredicted syntactic codes..embedding dimension dencoder layersdecoder layersfeedforward dimensiontransformer headssemantic/syntactic heads hsem, hsynquantizer heads ˜hsynquantizer codebook size koptimizer.
learning ratebatch sizetoken dropout.
decoderbeam widthcommitment weight λcode classiﬁernum.
hidden layershidden layer size.
76855204886/24256adam (kingmaand ba, 2015)0.005640.2 (xie et al.,2017)beam search40.25.
22712.table 6: hyperparameter values used for our experi-ments..b dataset statistics.
summary statistics for our partitions of paralex andqqp are shown in table 7. questions in qqp were9.7 tokens long on average, compared to 8.2 forparalex..we also show the distribution of different ques-tion types in figure 4; qqp contains a higher per-centage of why questions, and we found that thequestions tend to be more subjective compared tothe predominantly factual questions in paralex..paralexclusters questions1,450,759222,223183,27327,778182,81827,778.qqpclusters questions138,96512,55412,225.
55,6115,2555,255.traindevtest.
table 7: summary statistics for our cleaned version of(fader et al., 2013), and our partitioning of qqp..c human evaluation.
annotators were asked to rate the outputs accordingto the following criteria:.
figure 4: distribution of wh- words for the datasetsused in our experiments.
qqp contains a much higherpercentage of why questions..• to what extent is the meaning expressed inthe original question preserved in the rewrit-ten version, with no additional informationadded?
which of the questions generated bya system is likely to have the same answer asthe original?.
• does the rewritten version use different wordsor phrasing to the original?
you should choosethe system that uses the most different wordsor word order..d reproducibility notes.
all experiments were run on a single nvidia rtx2080 ti gpu.
training time for separator wasapproximately 2 days on paralex, and 1 day forqqp.
separator contains a total of 69,139,744trainable parameters..e template dropout.
early experiments showed that, while the modelwas able to separately encode meaning and form,the ‘syntactic’ encoding space showed little order-ing.
that is, local regions of the encoding space didnot necessarily encode templates that co-occurredwith each other in paraphrase clusters.
we there-fore propose template dropout, where exemplarsxsyn are replaced with probability ptd = 0.3 by aquestion with a different template from the sameparaphrase cluster.
this is intended to provide themodel with a signal about which templates are sim-ilar to each other, and thus reduce the distancebetween their encodings..f ordering of the encoding space.
• which system output is the most ﬂuent and.
grammatical?.
figure 5 shows that the semantic encodings zsemare tightly clustered by paraphrase, but the set of.
1417whathowwhenwherewhywhootherwh- word010203040percentage of questionsparalexqqpnumerical errorinputreplace starter on a 1988 ford via?
output how do you replace a starter on a 1992 ford?
repetition.
input.
what brought about the organization of the re-publican political party?.
output what is the political party of the republican.
party?.
ignoring encodingwhat do hondurans do for a living?.
inputoutput what do hondurans eat?.
well documented posterior collapse phenomenon,where the decoder ignores the input encoding andgenerates a generic high probability sequence..(a) semantic encodings.
table 8: examples of failure modes..(b) syntactic encodings.
figure 5: visualisations of zsem and zsyn using t-sne(van der maaten and hinton, 2008), coloured by para-phrase cluster.
the semantic encodings are clusteredby meaning, as expected, but there is little to no localordering in the syntactic space; valid surface forms ofa particular question do not necessarily have syntacticencodings near to each other..valid forms for each cluster overlaps signiﬁcantly.
in other words, regions of licensed templates foreach input are not contiguous, and naively perturb-ing a syntactic encoding for an input question isnot guaranteed to lead to a valid template.
tem-plate dropout, described in appendix e, seems toimprove the arrangement of encoding space, butis not sufﬁcient to allow us to ‘navigate’ encodingspace directly.
the ability to induce an orderedencoding space and introduce syntactic diversityby simply perturbing the encoding, would allow usto drop the template prediction network, and wehope that future work will build on this idea..g failure cases.
a downside of our approach is the use of an infor-mation bottleneck; the model must learn to com-press a full question into a single, ﬁxed-length vec-tor.
this can lead to loss of information or corrup-tion, with the output occasionally repeating wordsor generating a number that is slightly different tothe correct one, as shown in table 8..we also occasionally observe instances of the.
1418