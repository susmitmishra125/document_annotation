towards robustness of text-to-sql models against synonymsubstitution.
yujian gan1 xinyun chen2 qiuping huang3 matthew purver1jinxia xie4.
john r. woodward1.
pengsheng huang52uc berkeley.
1queen mary university of london.
3nanning central sub-branch of the people’s bank of china.
4guangxi university of finance and economics{y.gan,m.purver,j.woodward}@qmul.ac.uk.
5beijing meiyilab co.,ltd.
xinyun.chen@berkeley.edu.
qiuping h@foxmail.com.
jinxia xie@hotmail.com.
huangpengsheng@pku.edu.cn.
abstract.
recently, there has been signiﬁcant progress instudying neural networks to translate text de-scriptions into sql queries.
despite achiev-ing good performance on some public bench-marks, existing text-to-sql models typicallyrely on the lexical matching between wordsin natural language (nl) questions and tokensin table schemas, which may render the mod-els vulnerable to attacks that break the schemalinking mechanism.
in this work, we investi-gate the robustness of text-to-sql models toin particular, we in-synonym substitution.
troduce spider-syn, a human-curated datasetbased on the spider benchmark for text-to-sql translation.
nl questions in spider-synare modiﬁed from spider, by replacing theirschema-related words with manually selectedsynonyms that reﬂect real-world question para-phrases.
we observe that the accuracy dramat-ically drops by eliminating such explicit cor-respondence between nl questions and tableschemas, even if the synonyms are not adver-sarially selected to conduct worst-case adver-sarial attacks 1. finally, we present two cate-gories of approaches to improve the model ro-bustness.
the ﬁrst category of approaches uti-lizes additional synonym annotations for tableschemas by modifying the model input, whilethe second category is based on adversarialtraining.
we demonstrate that both categoriesof approaches signiﬁcantly outperform theircounterparts without the defense, and the ﬁrstcategory of approaches are more effective.
2.
1.introduction.
neural networks have become the defacto approachfor various natural language processing tasks, in-.
1following the prior work on adversarial learning, worst-case adversarial attacks mean adversarial examples generatedby attacking speciﬁc models.
code.
available.
datasethttps://github.com/ygan/spider-syn.
2our.
and.
at.
is.
figure 1: sample spider questions that include thesame tokens as the table schema annotations, and suchquestions constitute the majority of the spider bench-mark.
in our spider-syn benchmark, we replace someschema words in the nl question with their synonyms,without changing the sql query to synthesize..cluding text-to-sql translation.
various bench-marks have been proposed for this task, includingearlier small-scale single-domain datasets such asatis and geoquery (yaghmazadeh et al., 2017;iyer et al., 2017; zelle and mooney, 1996), andrecent large-scale cross-domain datasets such aswikisql (zhong et al., 2017) and spider (yu et al.,2018b).
while wikisql only contains simple sqlqueries executed on single tables, spider coversmore complex sql structures, e.g., joining of mul-tiple tables and nested queries..the state-of-the-art models have achieved im-pressive performance on text-to-sql tasks, e.g.,around 70% accuracy on the spider test set, evenif the model is tested on databases that are unseenin training.
however, we suspect that such cross-domain generalization heavily relies on the exactlexical matching between the nl question and thetable schema.
as shown in figure 1, names of ta-bles and columns in the sql query are explicitlystated in the nl question.
such questions con-stitute the majority of cross-domain text-to-sqlbenchmarks including both spider and wikisql..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2505–2515august1–6,2021.©2021associationforcomputationallinguistics2505what is the type of the filenamed "david cv"?
what is the type of the documentnamed "david cv"?
"document", "users", ……selectdocument_type fromdocuments……spider question:spider-syn question:schema annotations:sql:what is the average powerfor all automobilesproduced before 1980?what is the average horsepowerfor all carsproduced before 1980?
"horsepower", "cars data", ……selectavg(horsepower) fromcars_data……spider question:spider-syn question:schema annotations:sql:differentdifferentmodified tomodified tomodified todifferentalthough assuming exact lexical matching is agood starting point to solving the text-to-sql prob-lem, this assumption usually does not hold in real-world scenarios.
speciﬁcally, it requires that usershave precise knowledge of the table schemas to beincluded in the sql query, which could be tediousfor synthesizing complex sql queries..in this work, we investigate whether state-of-the-art text-to-sql models preserve good predictionperformance without the assumption of exact lexi-cal matching, where nl questions use synonyms torefer to tables or columns in sql queries.
we callsuch nl questions synonym substitution questions.
although some existing approaches can automati-cally generate synonymous substitution examples,these examples may deviate from real-world sce-narios, e.g., they may not follow common humanwriting styles, or even accidentally becomes incon-sistent with the annotated sql query.
to providea reliable benchmark for evaluating model perfor-mance on synonym substitution questions, we in-troduce spider-syn, a human-curated dataset con-structed by modifying nl questions in the spiderdataset.
speciﬁcally, we replace the schema annota-tions in the nl question with synonyms, manuallyselected so as not to change the corresponding sqlquery, as shown in figure 1. we demonstrate thatwhen models are only trained on the original spi-der dataset, they suffer a signiﬁcant performancedrop on spider-syn, even though the spider-synbenchmark is not constructed to exploit the worst-case attacks for text-to-sql models.
it is thereforeclear that the performance of these models will suf-fer in real-world use, particularly in cross-domainscenarios..to improve the robustness of text-to-sql mod-els, we utilize synonyms of table schema words,which are either manually annotated, or automati-cally generated when no annotation is available.
we investigate two categories of approaches toincorporate these synonyms.
the ﬁrst categoryof approaches modify the schema annotations ofthe model input, so that they align better with thenl question.
no additional training is requiredfor these approaches.
the second category of ap-proaches are based on adversarial training, wherewe augment the training set with nl questionsmodiﬁed by synonym substitution.
both categoriesof approaches signiﬁcantly improve the robustness,and the ﬁrst category is both effective and requiresless computational resources..figure 2: synonym substitution occurs in cell valuewords in both spider and spider-syn..in short, we make the following contributions:.
• we conduct a comprehensive study to evaluatethe robustness of text-to-sql models against syn-onym substitution..• besides worst-case adversarial attacks, we fur-ther introduce spider-syn, a human-curateddataset built upon spider, to evaluate synonymsubstitution for real-world question paraphrases.
• we propose a simple yet effective approach toutilize multiple schema annotations, without theneed of additional training.
we show that our ap-proach outperforms adversarial training methodson spider-syn, and achieves competitive perfor-mance on worst-case adversarial attacks..2 spider-syn dataset.
2.1 overview.
we construct the spider-syn benchmark by manu-ally modifying nl questions in the spider datasetusing synonym substitution.
the purpose of build-ing spider-syn is to simulate the scenario whereusers do not call the exact schema words in theutterances, e.g., users may not have the knowledgeof table schemas.
in particular, we focus on syn-onym substitution for words related to databases,including table schemas and cell values.
consis-tent with spider, spider-syn contains 7000 trainingand 1034 development examples, but spider-syndoes not contain a test set since the spider test setis not public.
figure 1 presents two examples inspider-syn and how they are modiﬁed from spider..2506"france", "germany" ……what is the average, minimum, and maximum age for all frenchsingers?selectavg(age) ,  min(age) ,  max(age) fromsinger wherecountry  =  'france'spider question:cell values in country column: sql:"dog", "cat" ……how many dogpets are raised by femalestudents?select...... wherestudent.sex  =  'f' and pet.pettype  =  'dog'spider question:cell values in pettype column: sql:how many puppypets are raised by femalestudents?spider-syn question:spider example:spider-syn example:"f", "m" ……cell values in sex column: differentmodified todifferentdifferentfigure 4: examples of synonym substitutions in the‘world’ domain from spider-syn..a subset of dog, the corresponding sql for thespider-syn question should still use the word ‘dog’instead of the word ‘puppy’ because there is onlydog type in the database and no puppy type.
similarreasoning is needed to infer that the word ‘female’corresponds to ‘f’ in figure 2..in some cases, words are replaced by syn-onymous phrases (rather than single words), asshown in figure 3. besides, some substitutionsare also based on the database contents.
for ex-ample, a column ‘location’ of the database ‘em-ployee hire evaluation’ in spider only stores citynames as cell values.
without knowing the tableschema, users are more likely to call ‘city’ insteadof ‘location’ in their nl questions..to summarize, we construct spider-syn with the.
following principles:• spider-syn is not constructed to exploit the worst-case adversarial attacks, but to represent real-world use scenarios; it therefore uses only rela-tively common words as substitutions..• we conduct synonym substitution only for words.
related to schema items and cell values..• synonym substitution includes both single words.
and phrases with multiple words..2.3 annotation steps.
before annotation, we ﬁrst separate original spidersamples based on their domains.
for each domain,we only utilize synonyms that are suitable for thatdomain.
we recruit four graduate students major incomputer science to annotate the dataset manually.
they are trained with a detailed annotation guide-line, principles, and some samples.
one is allowedto start after his trial samples are approved by thewhole team..as synonyms can be freely chosen by annotators,standard inter-annotator agreement metrics are notsufﬁcient to conﬁrm the data quality.
instead, weconduct the quality control with two rounds of re-.
figure 3: samples of replacing the original words orphrases by synonymous phrases..2.2 conduct principle.
the goal of constructing the spider-syn datasetis not to perform worst-case adversarial attacksagainst existing text-to-sql models, but to investi-gate the model robustness for paraphrasing schema-related words, which is particularly important whenusers do not have the knowledge of table schemas.
we carefully select the synonyms to replace theoriginal text to ensure that new words will notcause ambiguity in some domains.
for example,the word ‘country’ can often be used to replace theword ‘nationality’.
however, we did not replaceit in the domain whose ‘country’ means people’s‘born country’ different from its other schema item,‘nationality’.
besides, some synonym substitutionsare only valid in the speciﬁc domain.
for example,the word ‘number’ and ‘code’ are not generallysynonymous, but ‘ﬂight number’ can be replacedby ‘ﬂight code’ in the aviation domain..most synonym substitutions use relatively com-mon words3 to replace the schema item words.
be-sides, we denote ‘id’, ‘age’, ‘name’, and ‘year’ asreserved words, which are the most standard wordsto represent their meanings.
under this principle,we keep some original spider examples unchangedin spider-syn.
our synonym substitution does notguarantee that the modiﬁed nl question has theexact same meaning as the original question, butguarantees that its corresponding sql is consis-tent.
in figure 2, spider-syn replaces the cell valueword ‘dog’ with ‘puppy’.
although puppy is only.
3accordingwordsingoogle-10000-english..to.
20,000 most.
common englishhttps://github.com/first20hours/.
2507schemaannotations:schemaannotations:spider examples:sql:selectnamefromteacher……sql:selectidfromhighschoolerexcept……"teacher", "name",  ……,"highschooler",  "friend",  ……,what are the names of peoplewhoteachmath courses ?show ids of all studentswho do not have any friends.what is the name and capacityof the stadium with the most concerts?
"capacity", "stadium", ……spider-syn example:spiderquestion:schemaannotations:sql:selectname , capacityfrom……differentdifferentdifferentwhat is the name and number of seats of the stadium with the most concerts?spider-syn  question:modified tospiderquestion:spiderquestion:originalsubstituted  bytimescountrystate11nation35citytown11headleader2greatestpercentageofmost1populationnumberofpeople13numberofresidents15……worlddomainview.
the ﬁrst round is the cross-review betweenannotations.
we require the annotators to discusstheir disagreed annotations and come up with a ﬁ-nal result out of consensus.
to improve the workefﬁciency, we extract all synonym substitutions as areport without the nl questions from the annotateddata, as shown in figure 4. then, the annotatorsdo not have to go through the nl questions one byone.
the second round of review is similar to theﬁrst round but is done by native english speakers..2.4 dataset statistics.
in spider-syn, 5672 questions are modiﬁed com-pared to the original spider dataset.
in 5634 casesthe schema item words are modiﬁed, with the cellvalue words modiﬁed in only 27 cases.we use 273synonymous words and 189 synonymous phrasesto replace approximately 492 different words orphrases in these questions.
in all spider-syn ex-amples, there is an average of 0.997 change perquestion and 7.7 words or phrases modiﬁed perdomain..besides, spider-syn keeps 2201 and 161 originalspider questions in the training and developmentset, respectively.
in the modiﬁcation between thetraining and development sets, 52 modiﬁed wordsor phrases were the same, accounting for 35% ofthe modiﬁcation in the development set..3 defense approaches.
we present two categories of approaches for im-proving model robustness to synonym substitution.
we ﬁrst introduce our multiple annotation selectionapproach, which could utilize multiple annotationsfor one schema item.
then we present an adver-sarial training method based on analysis of the nlquestion and domain information..3.1 multi-annotation selection (mas).
the synonym substitution problem emerges whenusers do not call the exact names in table schemasto query the database.
therefore, one defenseagainst synonym substitution is utilizing multipleannotation words to represent the table schema, sothat the schema linking mechanism is still effective.
for example, for a database table with the name‘country’, we annotate additional table names withsimilar meanings, e.g., ‘nation’, ‘state’, etc.
in thisway, we explicitly inform the text-to-sql modelsthat all these words refer to the same table, thus thetable should be called in the sql query when the.
nl question includes any of the annotated words.
we design a simple yet effective mechanismto incorporate multiple annotation words, calledmultiple-annotation selection (mas).
for eachschema item, we check whether any annotationsappear in the nl question, and we select such an-notations as the model input.
when no annota-tion appears in the question, we select the defaultschema annotation, i.e., the same as the originalspider dataset.
in this way, we could utilize mul-tiple schema annotations simultaneously, withoutchanging the model input format..the main advantage of this method is that it doesnot require additional training, and could apply toexisting models trained without synonym substitu-tion questions.
annotating multiple schema wordscould be done automatically or manually, and wecompare them in section 4..3.2 adversarial training.
motivated by the idea of adversarial training thatcan improve the robustness of machine learningmodels against adversarial attacks (madry et al.,2018; morris et al., 2020), we implement adversar-ial training using the current open-source sotamodel rat-sql (wang et al., 2020).
we usethe bert-attack model (li et al., 2020) to gen-erate adversarial examples, and implement theentire training process based on the textattackframework (morris et al., 2020).
textattack pro-vides 82 pre-trained models, including word-levellstm, word-level cnn, bert-attack, and otherpre-trained transformer-based models..we follow the standard adversarial trainingpipeline that iteratively generates adversarial ex-amples, and trains the model on the dataset aug-mented with these adversarial examples.
whengenerating adversarial examples for training, weaim to generate samples that align with the spider-syn principles, rather than arbitrary adversarial per-turbations.
we describe the details of adversarialexample generation below..3.2.1 generating adversarial examples.
we choose bert-attack to generate the adversarialexamples.
different from other word substitutionmethods (mrkˇsi´c et al., 2016; ebrahimi et al., 2018;wei and zou, 2019), bert-attack model consid-ers the entire nl question when generating wordsfor synonym substitution.
such a sentence-basedmethod can generate different synonyms for thesame word in different context.
for example, the.
2508figure 5: input the bert-attack with and without domain information..word ‘head’ in ‘the head of a department’ and‘the head of a body’ should correspond to differentsynonyms.
making such distinctions requires ananalysis of the entire sentence, since the keywords’positions may not be close, such as that the word‘head’ and ‘department’ are not close in ‘give methe info of heads whose name is mike in each de-partment’..in addition to the original question, we add extradomain information into the bert-attack model,as shown in figure 5. without the domain informa-tion, on the right side of the figure 5, the bert-attack model conjectures the word ‘head’ representthe head of a body, since there are multiple feasibleinterpretations for the word ‘head’ if you only lookat the question.
to eliminate the ambiguity, wefeed questions with its domain information into thebert-attack model, as shown on the left side ofthe figure 5..instead of using schema annotations, we selectseveral other questions from the same domain asdomain information.
these questions should con-tain the schema item words we plan to replace, andother distinct schema item words in the same do-main.
the beneﬁts of using sentences instead ofschema annotations as domain information include:1) avoiding many unrelated schema annotations,which could include hundreds of words; 2) the sen-tence format is closer to the pre-training data ofbert.
as shown on the left side of the figure 5,our method improves the quality of data generation..since our work focuses on the synonym substitu-tion of schema item words, we make two additionalconstraints to limit the generation of adversarial ex-amples: 1) only words about schema items and cellvalues can be replaced; and 2) do not replace thereserved words discussed in section 2.2. theseconstraints make sure that the adversarial examplesonly perform the synonym substitution for wordsrelated to database tables..4 experiments.
4.1 experimental setup.
we compare our approaches against baseline meth-ods on both the spider (yu et al., 2018b) andspider-syn development sets.
as discussed in sec-tion 2.1, the spider test set is not publicly accessi-ble, and thus spider-syn does not contain a test set.
both spider and spider-syn contain 7000 trainingand 1034 development samples respectively, wherethere are 146 databases for training and 20 for de-velopment.
the sql queries and schema annota-tions between spider and spider-syn are the same;the difference is that the questions in spider-synare modiﬁed from spider by synonym substitution.
models are evaluated using the ofﬁcial exact match-ing accuracy metric of spider..we ﬁrst evaluate open-source models that reachcompetitive performance on spider: gnn (boginet al., 2019a), irnet (guo et al., 2019) and rat-sql (wang et al., 2020), on the spider-syn devel-opment set.
we then evaluate our approaches withrat-sql+bert model (denoted as rat-sqlb)on both spider-syn and spider development set..we examine the robustness of following ap-.
proaches for synonym substitution:• spr: indicate that the model is trained on the.
spider dataset..• sprsyn: indicate that the model is trained on.
the spider-syn dataset ..the model.
• sprspr&syn: indicate that.
istrained on both spider and spider-syn datasets.
• advbert: to improve the robustness of text-to-sql models, we use adversarial training methodsto deal with synonym substitution.
this variantmeans that we use bert-attack following the de-sign introduced in section 3.2. note that we onlyuse the spider dataset for adversarial training.
• advglove: to demonstrate the effectivenessof our advbert method, we also evaluate asimpler adversarial training method based on the.
2509[cls] which           's name has the substring ' ha ' ?
[sep] how many heads of the departments are older than 56 ?
[sep][cls] which 's name has the substring ' ha ' ?
[sep] which  chief's name has the substring ' ha ' ?
which  rain's name has the substring ' ha ' ?bert-attackchiefbrainheadheadinputwithdomaininformation：input withoutdomaininformation：bert-attackmodelgnn + spr (bogin et al., 2019a)irnet + spr (guo et al., 2019)rat-sql + spr (wang et al., 2020)rat-sqlb + spr (wang et al., 2020).
spider spider-syn48.5%53.2%62.7%69.7%.
23.6%28.4%33.6%48.2%.
table 1: exact match accuracy on the spider andspider-syn development set, where models are trainedon the original spider training set..sql componentselectselect (no agg)wherewhere (no op)group by (no having)group byorder byand/oriuekeywords.
spider spider-syn0.9100.9260.7720.8240.8460.8160.8310.9790.5500.897.
0.6990.7120.7150.7570.5750.5530.7680.9770.3440.876.table 2:rat-sqlb+spr on development sets..f1 scores of component matching of.
nearest glove word vector (pennington et al.,2014; mrkˇsi´c et al., 2016).
this method only con-siders the meaning of a single word, dispensingwith domain information and question context..standsas.
forintroduced.
• manualmas: masselection’,.
‘multi-annotationinsection 3.1. manualmas means that we collectmultiple annotations of schema item words,which are synonyms used in spider-syn.
after-ward, mas selects the appropriate annotationfor each schema item as the model input..• automas: in contrast to manualmas, in au-tomas we collect multiple annotations basedon the nearest glove word vector, as used inadvglove.
in this way, compared to manual-mas, there are much more synonyms to be se-lected from for automas.
both manualmasand automas are to demonstrate the effective-ness of mas in an ideal case.
this experimentaldesign principle is similar to evaluating adver-sarially trained models on the same adversarialattack used for training, which aims to show thegeneralization to in-distribution test samples..4.2 results of models trained on spider.
table 1 presents the exact matching accuracy ofmodels trained on the spider training set, and weevaluate them on development sets of spider andspider-syn.
although spider-syn is not designed.
approachsprsprsynsprspr&synadvgloveadvbertspr + manualmasspr + automas.
spider spider-syn69.7%67.8%68.1%48.7%68.7%67.4%68.7%.
48.2%59.9%58.0%27.7%58.5%62.6%56.0%.
table 3: exact match accuracy on the spider andspider-syn development set.
all approaches use therat-sqlb model..to exploit the worst-case attacks of text-to-sqlmodels, compared to spider, the performance ofall models has clearly dropped by about 20% to30% on spider-syn.
using bert for input em-bedding suffers less performance degradation thanmodels without bert, but the drop is still signiﬁ-cant.
these experiments demonstrate that trainingon spider alone is insufﬁcient for achieving goodperformance on synonym substitutions, becausethe spider dataset only contains a few questionswith synonym substitution..to obtain a better understanding of predic-tion results, we compare the f1 scores ofrat-sqlb+spr on different sql components onboth the spider and spider-syn development set.
as shown in table 2, the performance degrada-tion mainly comes from the components includ-ing schema items, while the decline in the ‘key-words’ and the ‘and/or’ that do not includeschema items is marginal.
this observation is con-sistent with the design of spider-syn, which fo-cuses on the substitution of schema item words..4.3 comparison of different approaches.
table 3 presents the results of rat-sqlb trainedwith different approaches.
we focus on rat-sqlbsince it achieves the best performance on both spi-der and spider-syn, as shown in table 1. our masapproaches signiﬁcantly improve the performanceon spider-syn, with only 1-2% performance degra-dation on the spider.
with manualmas, we seean accuracy of 62.6%, which outperforms all otherapproaches evaluated on spider-syn..we compare the result of rat-sqlb trained onspider (spr) as a baseline with other approaches.
rat-sqlb trained on spider-syn (sprsyn) ob-tains 11.7% accuracy improvement when evaluatedon spider-syn, while only suffers 1.9% accuracy.
2510advglove advbert.
approachsprsprsynsprspr&synadvgloveadvbertspr + manualmasspr + automas.
38.0%49.6%47.7%29.7%55.7%34.2%61.2%.
48.8%54.9%55.7%33.8%59.2%44.5%52.5%.
table 4: exact match accuracy on the worst-case devel-opment sets generated by advglove and advbert.
all approaches use the rat-sqlb model..drop when evaluated on spider.
meanwhile, ouradversarial training method based on bert-attack(advbert) improves the accuracy by 10.3% onspider-syn.
we observe that advbert performsmuch better than adversarial training based onglove (advglove), and we provide more ex-planation in section 4.4. both of our multiple anno-tation methods (manualmas and automas) im-prove the baseline model evaluated on spider-syn.
the performance of manualmas is better becausethe synonyms in manualmas are exactly the sameas the synonym substitution in spider-syn.
we dis-cuss more results about multi-annotation selectionin section 4.5..4.4 evaluation on adversarial attacks.
observing the dramatic performance drop onspider-syn, we then study the model robustnessunder worst-case attacks.
we use the adversarialexamples generation module in advglove andadvbert to attack the rat-sqlb+spr to gener-ate two worst-case development sets..table 4 presents the results on two worst-casedevelopment sets.
the advglove and advbertattacks cause the accuracy of rat-sqlb+sprrespectively.
to drop by 31.7% and 20.9%,rat-sqlb+spr+automas achieve the bestperformance on defending the advglove at-tack.
because the annotations in automascover the synonym substitutions generated byadvglove.
the relation between automas andadvglove is similar to that between manual-mas and spider-syn.
similarly, manualmashelps rat-sqlb+spr get the best accuracy asshown in table 3..as to advbert attack, rat-sqlb+advbertoutperforms other approaches.
this result isnot surprising, because rat-sqlb+advbert istrained based on defense advbert attack.
how-.
ever, why does rat-sqlb+advglove performso poorly in defending advglove attack?.
we conjecture that this is because the word em-bedding from bert is based on the context: if youreplace a word with a so-called synonym that isirrelevant to the context, bert may give this syn-onym a vector with low similarity to the original.
inthe ﬁrst example of table 6, advglove replacesthe word ‘courses’ with ‘trajectory’.
we observethat, based on the cosine similarity of bert em-bedding, the schema item most similar to ‘trajec-tory’ changes from ‘courses’ to ‘grade conversion’.
this problem does not appear in the spider-synand advbert examples, and some advgloveexamples do not have this problem, such as thesecond example in table 6. some examples rewardthe model for ﬁnding the schema item that is mostsimilar to the question token, while others penalizethis pattern, which causes the model to fail to learn.
thus the model with advglove neither defendsagainst advglove attack nor even obtains goodperformance on the spider..4.5 ablation study.
to analyze the individual contribution of ourproposed techniques, we have run some addi-tional experiments and show their results in ta-speciﬁcally, we use rat-sqlb+spr,ble 5.rat-sqlb+sprsyn, rat-sqlb+sprspr&syn,and rat-sqlb+advbert as base models, thenwe apply different schema annotation methods tothese model and evaluate their performance in dif-ferent development sets.
note that all base modelsuse the spider original schema annotations..first, for all base models, we found that masconsistently improves the model performance whenquestions are modiﬁed by synonym substitution.
speciﬁcally, when evaluating on spider-syn, us-ing manualmas achieves the best performance,because the manualmas contains the synonymsubstitutions of spider-syn.
meanwhile, whenevaluating on worst-case adversarial attacks, au-tomas mostly outperforms manualmas.
consid-ering that the automas is automatically generated,automas would be a simple and efﬁcient way toimprove the robustness of text-to-sql models..4.6 further discussion on mas.
manualmas utilizes the same synonym annota-tions on spider-syn, the same relationship as au-tomas with advglove, and we design this mech-anism to demonstrate the effectiveness of mas in.
2511approachsprspr + manualmasspr + automassprsynsprsyn + manualmassprsyn + automasspr&sprsynspr&sprsyn + manualmasspr&sprsyn + automasadvbertadvbert + manualmasadvbert + automas.
spider spider-syn advglove advbert69.7%67.4%68.7%67.8%65.7%67.0%68.1%65.6%66.8%68.7%66.7%67.5%.
48.8%44.5%52.5%54.9%52.1%54.4%55.7%51.7%55.7%59.2%56.7%58.0%.
48.2%62.6%56.0%59.9%62.9%61.7%58.0%59.5%57.5%58.5%62.2%59.6%.
38.0%34.2%61.2%49.6%47.8%63.3%47.7%46.9%61.0%55.7%53.4%62.4%.
table 5: ablation study results using rat-sqlb..which courses are taught on days mtw?.
spider:spider-syn: which curriculum are taught on days mtw?
advglove: which trajectory are taught on jour mtw ?
advbert: which classes are taught on times mtw ?.
show the name and phone for customers with a mailshot with outcome code ‘no response’show the name and telephone for clients with a mailshot with outcome code ‘no response’..spider:spider-syn:advglove: show the name and telephones for customers with a mailshot with outcome code ‘no response’.
show the name and telephone for customers with a mailbox with result code ‘no response’.
advbert:.
table 6: two questions in spider with corresponding versions of spider-syn, advglove and advbert..an ideal case.
by showing the superior performanceof manualmas on spider-syn, we conﬁrm that thefailure of existing models on spider-syn is largelybecause they rely on the lexical correspondence,and mas improves the performance by repairingthe lexical link.
besides, mas has the followingadvantages:.
• compared to adversarial training, mas does notneed any additional training.
therefore, by in-cluding different annotations for mas, the samepre-trained model could be applied to applicationscenarios with different requirements of robust-ness to synonym substitutions..• mas could also be combined with existing de-fenses, e.g., on adversarially trained models, asshown in our evaluation..we add the evaluation on the combination ofmas with gnn and irnet respectively, shown intable 7. the conclusions are similar to rat-sql:(1) mas signiﬁcantly improves the performanceon spider-syn, and manualmas achieves the bestperformance.
(2) automas also considerably im-proves the performance on adversarial attacks..5 related work.
text-to-sql translation.
text-to-sql transla-tion has been a long-standing challenge, and vari-ous benchmarks are constructed for this task (iyeret al., 2017; ana-maria popescu et al., 2003; tangand mooney, 2000; giordani and moschitti, 2012;li and jagadish, 2014; yaghmazadeh et al., 2017;zhong et al., 2017; yu et al., 2018b).
in particular,most recent works aim to improve the performanceon spider benchmark (yu et al., 2018b), wheremodels are required to synthesize sql querieswith complex structures, e.g., join clauses andnested queries, and they need to generalize acrossdatabases of different domains.
among variousmodel architectures (yu et al., 2018a; bogin et al.,2019a; guo et al., 2019; zhang et al., 2019b; boginet al., 2019b; wang et al., 2020), latest state-of-the-art models have implemented a schema linkingmethod, which is based on the exact lexical match-ing between the nl question and the table schemaitems (guo et al., 2019; bogin et al., 2019a; wanget al., 2020).
schema linking is essential for thesemodels, and causes a huge performance drop when.
2512approachgnngnn + manualmasgnn + automasirnetirnet + manualmasirnet + automas.
spider spider-syn advglove advbert48.5%44.0%44.0%53.2%49.7%53.1%.
23.6%38.2%29.5%28.4%39.3%35.1%.
25.4%22.9%39.8%26.4%24.0%44.3%.
28.9%26.2%31.8%29.0%27.2%35.6%.
table 7: evaluation on the combination of mas with gnn and irnet respectively..removing it.
based on this observation, we inves-tigate the robustness of such models to synonymsubstitution in this work..data augmentation for text-to-sql models.
existing works have proposed some data augmenta-tion and adversarial training techniques to improvethe performance of text-to-sql models.
xiong andsun (2019) propose an augmentgan model togenerate samples in the target domain for data aug-mentation, so as to improve the cross-domain gen-eralization.
however, this approach only supportssql queries executed on a single table, e.g., wik-isql.
li et al.
(2019) propose to use data augmenta-tion specialized for learning the spatial informationin databases, which improves the performance onsingle-domain geoquery and restaurants datasets.
some recent works study data augmentation to im-prove the model performance on variants of exist-ing sql benchmarks.
speciﬁcally, radhakrish-nan et al.
(2020) focus on search-style questionsthat are short and colloquial, and zhu et al.
(2020)study adversarial training to improve the adversar-ial robustness.
however, both of them are basedon wikisql.
zeng et al.
(2020) study the modelrobustness when the nl questions are untranslat-able and ambiguous, where they construct a datasetof such questions based on the spider benchmark,and perform data augmentation to detect confusingspans in the question.
on the contrary, our workinvestigate the robustness against synonym sub-stitution for cross-domain text-to-sql translation,supporting complex sql structures..synonym substitution for other nlp problems.
the study of synonym substitution can be tracedback to the 1970s (waltz, 1978; lehmann and sta-chowitz, 1972).
with the rise of machine learning,synonym substitution is widely used in nlp fordata augment and adversarial attacks (rizos et al.,2019; wei and zou, 2019; ebrahimi et al., 2018; al-shemali and kalita, 2020; ren et al., 2019).
many.
adversarial attacks based on synonym substitutionhave successfully compromised the performance ofexisting models (alzantot et al., 2018; zhang et al.,2019a; ren et al., 2019; jin et al., 2020).
recently,(morris et al., 2020) integrate many above worksinto their textattack framework for ease of use..6 conclusion.
we introduce spider-syn, a human-curated datasetbased on the spider benchmark for evaluating therobustness of text-to-sql models for synonym sub-stitution.
we found that the performance of pre-vious text-to-sql models drop dramatically onspider-syn, as well as other adversarial attacks per-forming the synonym substitution.
we design twocategories of approaches to improve the model ro-bustness, i.e., multi-anotation selection and adver-sarial training, and demonstrate the effectivenessof our approaches..acknowledgements.
we would like to thank the anonymous review-ers for their helpful comments.
matthew purveris partially supported by the epsrc under grantep/s033564/1, and by the european union’shorizon 2020 programme under grant agree-ments 769661 (saam, supporting active ageingthrough multimodal coaching) and 825153 (em-beddia, cross-lingual embeddings for less-represented languages in european news media).
xinyun chen is supported by the facebook fellow-ship.
the results of this publication reﬂect only theauthors’ views and the commission is not responsi-ble for any use that may be made of the informationit contains..references.
basemah alshemali and jugal kalita.
2020. general-ization to mitigate synonym substitution attacks..2513in proceedings of deep learning inside out (dee-lio): the first workshop on knowledge extractionand integration for deep learning architectures,pages 20–28, online.
association for computationallinguistics..moustafa alzantot, yash sharma, ahmed elgohary,bo-jhang ho, mani srivastava, and kai-wei chang.
2018. generating natural language adversarial ex-amples.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 2890–2896..ana-maria popescu, oren etzioni, and henry kautz.
2003. towards a theory of natural language inter-faces to databases.
in proceedings of the 8th inter-national conference on intelligent user interfaces,pages 149–157..ben bogin, jonathan berant, and matt gardner.
2019a.
representing schema structure with graph neuralnetworks for text-to-sql parsing.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 4560–4565, florence,italy.
association for computational linguistics..ben bogin, matt gardner, and jonathan berant.
2019b.
global reasoning over database structures for text-in proceedings of the 2019 con-to-sql parsing.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3659–3664, hong kong, china.
as-sociation for computational linguistics..javid ebrahimi, anyi rao, daniel lowd, and dejingdou.
2018. hotflip: white-box adversarial exam-in proceedings of theples for text classiﬁcation.
56th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages31–36, melbourne, australia.
association for com-putational linguistics..alessandra giordani and alessandro moschitti.
2012.automatic generation and reranking of sql-derived answers to nl questions.
in proceedingsof the second international conference on trustwor-thy eternal systems via evolving software, data andknowledge, pages 59–76..jiaqi guo, zecheng zhan, yan gao, yan xiao, jian-guang lou, ting liu, and dongmei zhang.
2019.towards complex text-to-sql in cross-domaindatabase with intermediate representation.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4524–4535, florence, italy.
association for computationallinguistics..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
2020. is bert really robust?
a strong base-line for natural language attack on text classiﬁcationin proceedings of the aaai con-and entailment.
ference on artiﬁcial intelligence, volume 34, pages8018–8025..winfred philipp lehmann and ra stachowitz.
1972.normalization of natural language for informationretrieval.
technical report, texas univ austinlinguistics research center..fei li and h. v. jagadish.
2014. constructing aninteractive natural language interface for relationaldatabases.
proceedings of the vldb endowment,8(1):73–84..jingjing li, wenlu wang, wei shinn ku, yingtao tian,and haixun wang.
2019. spatialnli: a spatial do-main natural language interface to databases usingspatial comprehension.
in gis: proceedings of theacm international symposium on advances in ge-ographic information systems, pages 339–348, newyork, ny, usa.
association for computing machin-ery..linyang li, ruotian ma, qipeng guo, xiangyang xue,and xipeng qiu.
2020. bert-attack: adversar-ial attack against bert using bert.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages6193–6202, stroudsburg, pa, usa.
association forcomputational linguistics..aleksander madry, aleksandar makelov, ludwigschmidt, dimitris tsipras, and adrian vladu.
2018.towards deep learning models resistant to adversar-ial attacks.
in international conference on learningrepresentations..john morris, eli liﬂand, jin yong yoo, jake grigsby,di jin, and yanjun qi.
2020. textattack: a frame-work for adversarial attacks, data augmentation,and adversarial training in nlp.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing: system demonstrations,pages 119–126, stroudsburg, pa, usa.
associationfor computational linguistics..nikola mrkˇsi´c, diarmuid ´o s´eaghdha, blaise thom-son, milica gaˇsi´c, lina m. rojas-barahona, pei-hao su, david vandyke, tsung-hsien wen, andsteve young.
2016. counter-ﬁtting word vectors toin proceedings of the 2016linguistic constraints.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 142–148, san diego,california.
association for computational linguis-tics..srinivasan iyer, ioannis konstas, alvin cheung, jayantkrishnamurthy, and luke zettlemoyer.
2017. learn-ing a neural semantic parser from user feedback.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 963–973..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543, doha, qatar.
asso-ciation for computational linguistics..2514complex and cross-domain text-to-sql task.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1653–1663, brussels, belgium.
association for computa-tional linguistics..tao yu, rui zhang, kai yang, michihiro yasunaga,dongxu wang, zifan li, james ma,irene li,qingning yao, shanelle roman, zilin zhang,and dragomir radev.
2018b.
spider: a large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages3911–3921, brussels, belgium.
association forcomputational linguistics..john m zelle and raymond j mooney.
1996. learn-ing to parse database queries using inductive logicprogramming.
in proceedings of the thirteenth na-tional conference on artiﬁcial intelligence - volume2, pages 1050–1055..jichuan zeng, xi victoria lin, steven c.h.
hoi,richard socher, caiming xiong, michael lyu, andirwin king.
2020. photon: a robust cross-domainin proceedings of the 58thtext-to-sql system.
annual meeting of the association for computa-tional linguistics: system demonstrations, pages204–214, stroudsburg, pa, usa.
association forcomputational linguistics..huangzhao zhang, hao zhou, ning miao, and lei li.
2019a.
generating ﬂuent adversarial examples forin proceedings of the 57th an-natural languages.
nual meeting of the association for computationallinguistics, pages 5564–5569, florence, italy.
asso-ciation for computational linguistics..rui zhang, tao yu, he yang er, sungrok shim,eric xue, xi victoria lin, tianze shi, caimingxiong, richard socher, and dragomir radev.
2019b.
editing-based sql query generation for cross-domain context-dependent questions..victor zhong, caiming xiong, and richard socher.
2017.seq2sql: generating structured queriesfrom natural language using reinforcement learn-ing.
corr, abs/1709.0..yi zhu, yiwei zhou, and menglin xia.
2020. gener-ating semantically valid adversarial questions fortableqa..karthik radhakrishnan, arvind srikantan, and xi vic-toria lin.
2020. colloql: robust cross-domaintext-to-sql over search queries..shuhuai ren, yihe deng, kun he, and wanxiangche.
2019. generating natural language adver-sarial examples through probability weighted wordsaliency.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 1085–1097, florence, italy.
association forcomputational linguistics..georgios rizos, konstantin hemker,.
and bj¨ornschuller.
2019. augment to prevent: short-textdata augmentation in deep learning for hate-speechclassiﬁcation.
in proceedings of the 28th acm in-ternational conference on information and knowl-edge management, cikm ’19, page 991–1000, newyork, ny, usa.
association for computing machin-ery..lappoon r tang and raymond j mooney.
2000. au-tomated construction of database interfaces: inter-grating statistical and relational learning for se-mantic parsing.
in 2000 joint sigdat conferenceon empirical methods in natural language process-ing and very large corpora, pages 133–141..david l waltz.
1978. an english language question an-swering system for a large relational database.
com-munications of the acm, 21(7):526–539..bailin wang, richard shin, xiaodong liu, oleksandrpolozov, and matthew richardson.
2020. rat-sql:relation-aware schema encoding and linking fortext-to-sql parsers.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7567–7578, online.
associationfor computational linguistics..jason wei and kai zou.
2019. eda: easy data aug-mentation techniques for boosting performance onin proceedings of thetext classiﬁcation tasks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 6382–6388, hong kong,china.
association for computational linguistics..hongvu xiong and ruixiao sun.
2019. transferablenatural language interface to structured queriesin 2019 ieeeaided by adversarial generation.
13th international conference on semantic comput-ing (icsc), pages 255–262.
ieee..navid yaghmazadeh, yuepeng wang, isil dillig, andthomas dillig.
2017.sqlizer: query synthe-in international con-sis from natural language.
ference on object-oriented programming, systems,languages, and applications, acm, pages 63:1—-63:26..tao yu, michihiro yasunaga, kai yang, rui zhang,dongxu wang, zifan li, and dragomir radev.
2018a.
syntaxsqlnet: syntax tree networks for.
2515