do neural dialog systems use the conversation history effectively?
an empirical study.
chinnadhurai sankar1,2,4∗.
sandeep subramanian1,2,5.
christopher pal1,3,5.
sarath chandar1,2,4.
yoshua bengio 1,2.
1mila.
2universit´e de montr´eal4google research, brain team.
3 ´ecole polytechnique de montr´eal.
5element ai, montr´eal.
abstract.
neural generative models have been becomeincreasingly popular when building conversa-tional agents.
they offer ﬂexibility, can be eas-ily adapted to new domains, and require min-imal domain engineering.
a common criti-cism of these systems is that they seldom un-derstand or use the available dialog history ef-in this paper, we take an empiri-fectively.
cal approach to understanding how these mod-els use the available dialog history by study-ing the sensitivity of the models to artiﬁciallyintroduced unnatural changes or perturbationsto their context at test time.
we experimentwith 10 different types of perturbations on 4multi-turn dialog datasets and ﬁnd that com-monly used neural dialog architectures like re-current and transformer-based seq2seq modelsare rarely sensitive to most perturbations suchas missing or reordering utterances, shufﬂingwords, etc.
also, by open-sourcing our code,we believe that it will serve as a useful diag-nostic tool for evaluating dialog systems in thefuture 1..1.introduction.
with recent advancements in generative models oftext (wu et al., 2016; vaswani et al., 2017; rad-ford et al., 2018), neural approaches to buildingchit-chat and goal-oriented conversational agents(sordoni et al., 2015; vinyals and le, 2015; ser-ban et al., 2016; bordes and weston, 2016; serbanet al., 2017b) has gained popularity with the hopethat advancements in tasks like machine transla-tion (bahdanau et al., 2015), abstractive summa-rization (see et al., 2017) should translate to dialogsystems as well.
while these models have demon-strated the ability to generate ﬂuent responses,.
∗corresponding author: chinnadhurai@gmail.com1code.
at https://github.com/.
available.
is.
chinnadhurai/parlai/.
they still lack the ability to “understand” and pro-cess the dialog history to produce coherent andinteresting responses.
they often produce bor-ing and repetitive responses like “thank you.” (liet al., 2015; serban et al., 2017a) or meander awayfrom the topic of conversation.
this has been oftenattributed to the manner and extent to which thesemodels use the dialog history when generating re-sponses.
however, there has been little empiricalinvestigation to validate these speculations..in this work, we take a step in that direction andconﬁrm some of these speculations, showing thatmodels do not make use of a lot of the informa-tion available to it, by subjecting the dialog his-tory to a variety of synthetic perturbations.
wethen empirically observe how recurrent (sutskeveret al., 2014) and transformer-based (vaswani et al.,2017) sequence-to-sequence (seq2seq) models re-spond to these changes.
the central premise ofthis work is that models make minimal use of cer-tain types of information if they are insensitive toperturbations that destroy them.
worryingly, weﬁnd that 1) both recurrent and transformer-basedseq2seq models are insensitive to most kinds ofperturbations considered in this work 2) both areparticularly insensitive even to extreme pertur-bations such as randomly shufﬂing or reversingwords within every utterance in the conversationhistory (see table 1) and 3) recurrent models aremore sensitive to the ordering of utterances withinthe dialog history, suggesting that they could bemodeling conversation dynamics better than trans-formers..2 related work.
since this work aims at investigating and gain-ing an understanding of the kinds of informationa generative neural response model learns to use,the most relevant pieces of work are where sim-.
proceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages32–37florence,italy,july28-august2,2019.c(cid:13)2019associationforcomputationallinguistics32no perturbations.
1 good afternoon !
can i help you ?
2 could you show me where the chinesc-style clothing is.
located ?
i want to buy a silk coatthis way , please .
here they are .
they’re all handmade ..34 model response: how much is it ?.
token shufﬂingi afternoon help you good ?
!
canthe located chinesc-style where is show a .
buy you ?
iclothing want coat silk me could toare handmade .
way please this all here they .
, they’re .
model response: how much is it ?.
table 1: an example of an lstm seq2seq model with attention’s insensitivity to shufﬂing of words in the dialoghistory on the dailydialog dataset..ilar analyses have been carried out to understandthe behavior of neural models in other settings.
an investigation into how lstm based uncondi-tional language models use available context wascarried out by khandelwal et al.
(2018).
theyempirically demonstrate that models are sensitiveto perturbations only in the nearby context andtypically use only about 150 words of context.
on the other hand, in conditional language mod-eling tasks like machine translation, models areadversely affected by both synthetic and naturalnoise introduced anywhere in the input (belinkovand bisk, 2017).
understanding what informationis learned or contained in the representations ofneural networks has also been studied by “prob-ing” them with linear or deep models (adi et al.,2016; subramanian et al., 2018; conneau et al.,2018)..several works have recently pointed out thepresence of annotation artifacts in common textand multi-modal benchmarks.
for example, guru-rangan et al.
(2018) demonstrate that hypothesis-only baselines for natural language inference ob-tain results signiﬁcantly better than random guess-ing.
kaushik and lipton (2018) report that readingcomprehension systems can often ignore the entirequestion or use only the last sentence of a doc-ument to answer questions.
anand et al.
(2018)show that an agent that does not navigate or evensee the world around it can answer questions aboutit as well as one that does.
these pieces of worksuggest that while neural methods have the poten-tial to learn the task speciﬁed, its design could leadthem to do so in a manner that doesn’t use all ofthe available information within the task..recent work has also investigated the induc-tive biases that different sequence models learn.
for example, tran et al.
(2018) ﬁnd that recurrentmodels are better at modeling hierarchical struc-ture while tang et al.
(2018) ﬁnd that feedfor-ward architectures like the transformer and con-volutional models are not better than rnns atmodeling long-distance agreement.
transformers.
however excel at word-sense disambiguation.
weanalyze whether the choice of architecture and theuse of an attention mechanism affect the way inwhich dialog systems use information available tothem..3 experimental setup.
following the recent line of work on generativedialog systems, we treat the problem of generat-ing an appropriate response given a conversationhistory as a conditional language modeling prob-lem.
speciﬁcally we want to learn a conditionalprobability distribution pθ(y|x) where y is a rea-sonable response given the conversation history x.the conversation history is typically representedas a sequence of utterances x1, x2, .
.
.
xn, whereeach utterance xi itself is comprised of a sequenceof words xi1, xi2 .
.
.
xik .
the response y is a singleutterance also comprised of a sequence of wordsy1, y2 .
.
.
ym.
the overall conditional probabilityis factorized autoregressively as.
pθ(y|x) =.
pθ(yi|y<i, x1 .
.
.
xn).
n(cid:89).
i=1.
pθ, in this work, is parameterized by a recurrentor transformer-based seq2seq model.
the crux ofthis work is to study how the learned probabilitydistribution behaves as we artiﬁcially perturb theconversation history x1, .
.
.
xn.
we measure be-havior by looking at how much the per-token per-plexity increases under these changes.
for exam-ple, one could think of shufﬂing the order in whichx1 .
.
.
xn is presented to the model and observehow much the perplexity of y under the model in-creases.
if the increase is only minimal, we canconclude that the ordering of x1 .
.
.
xn isn’t infor-mative to the model.
for a complete list of per-turbations considered in this work, please refer tosection 3.2. all models are trained without anyperturbations and sensitivity is studied only at testtime..33figure 1: the increase in perplexity for different models when only presented with the k most recent utterancesfrom the dialog history for dailydialog (left) and babi dialog (right) datasets.
recurrent models with attentionfare better than transformers, since they use more of the conversation history..3.1 datasets.
we experiment with four multi-turn dialogdatasets..babi dialog is a synthetic goal-oriented multi-turn dataset (bordes and weston, 2016) consistingof 5 different tasks for restaurant booking with in-creasing levels of complexity.
we consider task 5in our experiments since it is the hardest and is aunion of all four tasks.
it contains 1k dialogs withan average of 13 user utterances per dialog..persona chatis an open domain dataset (zhanget al., 2018) with multi-turn chit-chat conversa-tions between turkers who are each assigned ait comprises of 10.9k di-“persona” at random.
alogs with an average of 14.8 turns per dialog..dailydialog is an open domain dataset (li et al.,2017) which consists of dialogs that resemble day-itto-day conversations across multiple topics.
comprises of 13k dialogs with an average of 7.9turns per dialog..mutualfriendsis a multi-turn goal-orienteddataset (he et al., 2017) where two agents mustdiscover which friend of theirs is mutual based onthe friends’ attributes.
it contains 11k dialogs withan average of 11.41 utterances per dialog..3.2 types of perturbations.
we experimented with several types of perturba-tion operations at the utterance and word (token)levels.
all perturbations are applied in isolation..utterance-level perturbations we consider thefollowing operations 1) shuf that shufﬂes the se-quence of utterances in the dialog history, 2) revthat reverses the order of utterances in the history.
(but maintains word order within each utterance)3) drop that completely drops certain utterancesand 4) truncate that truncates the dialog historyto contain only the k most recent utterances wherek ≤ n, where n is the length of dialog history..word-level perturbations we consider similaroperations but at the word level within every ut-terance 1) word-shufﬂe that randomly shufﬂes thewords within an utterance 2) reverse that reversesthe ordering of words, 3) word-drop that drops30% of the words uniformly 4) noun-drop thatdrops all nouns, 5) verb-drop that drops all verbs..3.3 models.
we experimented with two different classesrecurrent and transformer-basedof models -sequence-to-sequence generative models.
all dataloading, model implementations and evaluationswere done using the parlai framework.
we usedthe default hyper-parameters for all the models asspeciﬁed in parlai..trained a.recurrent models weseq2seq(seq2seq lstm) model where the encoder anddecoder are parameterized as lstms (hochreiterand schmidhuber, 1997).
we also experimentwith using decoders that use an attention mecha-nism (seq2seq lstm att) (bahdanau et al., 2015).
the encoder and decoder lstms have 2 layerswith 128 dimensional hidden states with a dropoutrate of 0.1..transformer our transformer (vaswani et al.,2017) model uses 300 dimensional embeddingsand hidden states, 2 layers and 2 attention headswith no dropout.
this model is signiﬁcantlysmaller than the ones typically used in machine.
34models.
test ppl.
shuf.
rev.
onlylast.
utterance level perturbations.
droplast.
dropfirst( ∆ p p l[σ] )dailydialog.
worddrop.
verbdrop.
noundrop.
wordshuf.
wordrev.
word level perturbations.
( ∆ p p l[σ] ).
seq2seq lstmseq2seq lstm atttransformer.
seq2seq lstmseq2seq lstm atttransformer.
seq2seq lstmseq2seq lstm atttransformer.
32.90[1.40]29.65[1.10]28.73[1.30].
43.24[0.99]42.90[1.76]40.78[0.31].
14.17[0.29]10.60[0.21]10.63[0.03].
1.70[0.41] 3.35[0.38] 4.04[0.28] 0.13[0.04] 5.08[0.79] 1.58[0.15] 0.87[0.08] 1.06[0.28] 3.37[0.33] 3.10[0.45]4.76[0.39] 2.54[0.24] 3.31[0.49] 0.32[0.03] 4.84[0.42] 2.03[0.25] 1.37[0.29] 2.22[0.22] 2.82[0.31] 3.29[0.25]3.28[1.37] 0.82[0.40] 1.25[0.62] 0.27[0.19] 2.43[0.83] 1.20[0.69] 0.63[0.17] 2.60[0.98] 0.15[0.08] 0.26[0.18].
3.27[0.13] 6.29[0.48] 13.11[1.22] 0.47[0.21] 6.10[0.46] 1.81[0.25] 0.68[0.19] 0.75[0.15] 1.29[0.17] 1.95[0.20]4.44[0.81] 6.70[0.67] 11.61[0.75] 2.99[2.24] 5.58[0.45] 2.47[0.67] 1.11[0.27] 1.20[0.23] 2.03[0.46] 2.39[0.31]1.90[0.08] 1.22[0.22] 1.41[0.54] −0.1[0.07] 1.59[0.39] 0.54[0.08] 0.40[0.00] 0.32[0.18] 0.01[0.01] 0.00[0.06].
1.44[0.86] 1.42[0.25] 1.24[0.34] 0.00[0.00] 0.76[0.10] 0.28[0.11] 0.00[0.03] 0.61[0.39] 0.31[0.25] 0.56[0.39]32.13[4.08] 1.24[0.19] 1.06[0.24] 0.08[0.03] 1.35[0.15] 1.56[0.20] 0.15[0.07] 3.28[0.38] 2.35[0.22] 4.59[0.46]20.11[0.67] 1.06[0.16] 1.62[0.44] 0.12[0.03] 0.81[0.09] 0.75[0.05] 0.16[0.02] 1.50[0.12] 0.07[0.01] 0.13[0.04].
persona chat.
mutualfriends.
babi dailog: task5.
seq2seq lstmseq2seq lstm atttransformer.
1.28[0.02]1.06[0.02]1.07[0.00].
1.31[0.50] 43.61[15.9] 40.99[9.38] 0.00[0.00] 4.28[1.90] 0.38[0.11] 0.01[0.00] 0.10[0.06] 0.09[0.02] 0.42[0.38]9.14[1.28] 41.21[8.03] 34.32[10.7] 0.00[0.00] 6.75[1.86] 0.64[0.07] 0.03[0.03] 0.22[0.04] 0.25[0.01] 1.10[0.80]4.06[0.33] 0.38[0.02] 0.62[0.02] 0.00[0.00] 0.21[0.02] 0.36[0.02] 0.25[0.06] 0.37[0.06] 0.00[0.00] 0.00[0.00].
table 2: model performance across multiple datasets and sensitivity to different perturbations.
columns 1 & 2report the test set perplexity (without perturbations) of different models.
columns 3-12 report the increase inperplexity when models are subjected to different perturbations.
the mean (µ) and standard deviation [σ] across5 runs are reported.
the only last column presents models with only the last utterance from the dialog history.
the model that exhibits the highest sensitivity (higher the better) to a particular perturbation on a dataset is in bold.
seq2seq lstm att are the most sensitive models 24/40 times, while transformers are the least with 6/40 times..translation since we found that the model that re-sembled vaswani et al.
(2017) signiﬁcantly overﬁton all our datasets..while the models considered in this work mightnot be state-of-the-art on the datasets considered,we believe these models are still competitive andused commonly enough at least as baselines, thatthe community will beneﬁt by understanding theirbehavior.
in this paper, we use early stopping witha patience of 10 on the validation set to save ourbest model.
all models achieve close to the per-plexity numbers reported for generative seq2seqmodels in their respective papers..4 results & discussion.
our results are presented in table 2 and figure 1.table 2 reports the perplexities of different mod-els on test set in the second column, followed bythe increase in perplexity when the dialog historyis perturbed using the method speciﬁed in the col-umn header.
rows correspond to models trainedon different datasets.
figure 1 presents the changein perplexity for models when presented only withthe k most recent utterances from the dialog his-tory..we make the following observations:.
1. models tend to show only tiny changes inperplexity in most cases, even under extremechanges to the dialog history, suggesting thatthey use far from all the information that isavailable to them..2. transformersreordering,indicating thatlearning bag-of-words like representations..to word-they could be.
insensitive.
are.
3. the use of an attention mechanism inseq2seq lstm att and transformers makesthese models use more information from ear-lier parts of the conversation than vanillaseq2seq models as seen from increases in per-plexity when using only the last utterance..4. while transformers converge faster and tolower test perplexities, they don’t seem tocapture the conversational dynamics acrossutterances in the dialog history and are lesssensitive to perturbations that scramble thisstructure than recurrent models..5 conclusion.
this work studies the behaviour of generative neu-ral dialog systems in the presence of syntheticallyintroduced perturbations to the dialog history, thatit conditions on.
we ﬁnd that both recurrent andtransformer-based seq2seq models are not signiﬁ-cantly affected even by drastic and unnatural mod-iﬁcations to the dialog history.
we also ﬁnd sub-tle differences between the way in which recurrentand transformer-based models use available con-text.
by open-sourcing our code, we believe thisparadigm of studying model behavior by intro-ducing perturbations that destroys different kindsof structure present within the dialog history can.
35be a useful diagnostic tool.
we also foresee thisparadigm being useful when building new dialogdatasets to understand the kinds of informationmodels use to solve them..acknowledgements.
we would like to acknowledge nvidia for do-nating gpus and a dgx-1 computer used in thiswork.
we would also like to thank the anonymousreviewers for their constructive feedback.
ourcode is available at https://github.com/chinnadhurai/parlai/..references.
yossi adi, einat kermany, yonatan belinkov, oferlavi, and yoav goldberg.
2016. fine-grained anal-ysis of sentence embeddings using auxiliary predic-tion tasks.
arxiv preprint arxiv:1608.04207..divyansh kaushik and zachary c lipton.
2018. howmuch reading does reading comprehension require?
a criticalinvestigation of popular benchmarks.
arxiv preprint arxiv:1808.04926..urvashi khandelwal, he he, peng qi, and dan ju-rafsky.
2018. sharp nearby, fuzzy far away: howneural language models use context.
arxiv preprintarxiv:1805.04623..j. li, m. galley, c. brockett, j. gao, and b. dolan.
2015. a diversity-promoting objective functionfor neural conversation models.
arxiv e-prints..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manuallylabelled multi-turn dialogue dataset.
arxiv preprintarxiv:1710.03957..alec radford, karthik narasimhan, tim salimans, andimproving language under-ilya sutskever.
2018.standing by generative pre-training.
url https://s3-us-west-2.
amazonaws.
com/openai-assets/research-covers/languageunsupervised/languageunder-standing paper.
pdf..ankesh anand, eugene belilovsky, kyle kastner,hugo larochelle, and aaron courville.
2018.blindfold baselines for embodied qa.
arxiv preprintarxiv:1811.05013..abigail see, peter j liu, and christopher d man-to the point: summarizationarxiv preprint.
ning.
2017. getwith pointer-generator networks.
arxiv:1704.04368..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin proceedingslearning to align and translate.
of the international conference on representationlearning (iclr 2015)..yonatan belinkov and yonatan bisk.
2017. syntheticand natural noise both break neural machine transla-tion.
arxiv preprint arxiv:1711.02173..antoine bordes and jason weston.
2016.ing end-to-end goal-oriented dialog.
abs/1605.07683..learn-corr,.
alexis conneau, german kruszewski, guillaumelample, lo¨ıc barrault, and marco baroni.
2018.what you can cram into a single vector: probingsentence embeddings for linguistic properties.
arxivpreprint arxiv:1805.01070..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel r bowman, andannotation artifacts innoah a smith.
2018.arxiv preprintnatural language inference data.
arxiv:1803.02324..h. he, a. balakrishnan, m. eric, and p. liang.
2017. learning symmetric collaborative dialogueagents with dynamic knowledge graph embed-dings.
arxiv e-prints..sepp hochreiter and j¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..i. v. serban, a. sordoni, r. lowe, l. charlin,j. pineau, a. courville, and y. bengio.
2017a.
ahierarchical latent variable encoder-decoder modelfor generating dialogues.
in thirty-first aaai con-ference (aaai)..iulian v serban, chinnadhurai sankar, mathieu ger-main, saizheng zhang, zhouhan lin, sandeep sub-ramanian, taesup kim, michael pieper, sarathchandar, nan rosemary ke, et al.
2017b.
adeep reinforcement learning chatbot.
arxiv preprintarxiv:1709.02349..iulian vlad serban, alessandro sordoni, yoshua ben-gio, aaron c. courville, and joelle pineau.
2016.building end-to-end dialogue systems using gener-in pro-ative hierarchical neural network models.
ceedings of aaai..alessandro sordoni, michel galley, michael auli,chris brockett, yangfeng ji, margaret mitchell,jian-yun nie, jianfeng gao, and bill dolan.
2015.a neural network approach to context-sensitive gen-eration of conversational responses.
arxiv preprintarxiv:1506.06714..sandeep subramanian, adam trischler, yoshua ben-gio, and christopher j pal.
2018. learning gen-eral purpose distributed sentence representations viaarxiv preprintlarge scale multi-task learning.
arxiv:1804.00079..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural net-works.
in advances in neural information process-ing systems, pages 3104–3112..36gongbo tang, mathias m¨uller, annette rios, and ricosennrich.
2018. why self-attention?
a targeted eval-uation of neural machine translation architectures.
arxiv preprint arxiv:1808.08946..ke tran, arianna bisazza, and christof monz.
the importance of being recurrent forarxiv preprint.
2018.modeling hierarchical structure.
arxiv:1803.03585..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..oriol vinyals and quoc le.
2015. a neural conversa-tional model.
arxiv preprint arxiv:1506.05869..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016.google’s neural ma-chine translation system: bridging the gap betweenarxiv preprinthuman and machine translation.
arxiv:1609.08144..saizheng zhang, emily dinan, jack urbanek, arthurszlam, douwe kiela, and jason weston.
2018. per-sonalizing dialogue agents: i have a dog, do youhave pets too?
arxiv preprint arxiv:1801.07243..37